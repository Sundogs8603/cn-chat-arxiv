<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21355;&#26143;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#25429;&#25417;&#31227;&#21160;&#24322;&#24120;&#65292;&#20855;&#26377;&#23454;&#26102;&#26816;&#27979;&#28798;&#23475;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.05376</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21355;&#26143;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Satellite Videos using Diffusion Models. (arXiv:2306.05376v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21355;&#26143;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#25429;&#25417;&#31227;&#21160;&#24322;&#24120;&#65292;&#20855;&#26377;&#23454;&#26102;&#26816;&#27979;&#28798;&#23475;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#30340;&#23450;&#20041;&#26159;&#35782;&#21035;&#24847;&#22806;&#20107;&#20214;&#12290;&#20351;&#29992;&#21355;&#26143;&#25968;&#25454;&#23454;&#26102;&#26816;&#27979;&#26497;&#31471;&#20107;&#20214;&#65288;&#22914;&#37326;&#28779;&#12289;&#27668;&#26059;&#25110;&#27946;&#27700;&#65289;&#22312;&#28798;&#23475;&#31649;&#29702;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#20960;&#20010;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#25552;&#20379;&#26377;&#20851;&#28798;&#23475;&#30340;&#20449;&#24687;&#65292;&#20294;&#22320;&#29699;&#38745;&#27490;&#36712;&#36947;&#19978;&#30340;&#21355;&#26143;&#27599;&#20998;&#38047;&#33021;&#22815;&#25552;&#20379;&#25968;&#25454;&#65292;&#20174;&#23454;&#36136;&#19978;&#21019;&#24314;&#20102;&#22826;&#31354;&#35270;&#39057;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25216;&#26415;&#26469;&#35782;&#21035;&#30417;&#35270;&#35270;&#39057;&#20013;&#30340;&#24322;&#24120;&#65292;&#28982;&#32780;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#27809;&#26377;&#21160;&#24577;&#34892;&#20026;&#65292;&#25152;&#20197;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31181;&#24322;&#24120;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#38750;&#24120;&#39640;&#39057;&#30340;&#25968;&#25454;&#38598;&#20197;&#26597;&#25214;&#38750;&#24120;&#24555;&#36895;&#31227;&#21160;&#30340;&#24322;&#24120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#36816;&#21160;&#32452;&#20214;&#26469;&#25429;&#25417;&#24555;&#36895;&#31227;&#21160;&#30340;&#24322;&#24120;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The definition of anomaly detection is the identification of an unexpected event. Real-time detection of extreme events such as wildfires, cyclones, or floods using satellite data has become crucial for disaster management. Although several earth-observing satellites provide information about disasters, satellites in the geostationary orbit provide data at intervals as frequent as every minute, effectively creating a video from space. There are many techniques that have been proposed to identify anomalies in surveillance videos; however, the available datasets do not have dynamic behavior, so we discuss an anomaly framework that can work on very high-frequency datasets to find very fast-moving anomalies. In this work, we present a diffusion model which does not need any motion component to capture the fast-moving anomalies and outperforms the other baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#31574;&#21010;&#30340; C/C++ &#28304;&#20195;&#30721;&#28431;&#27934;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#27169;&#22411;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026; SEquential Graph Neural Network&#65288;SEGNN&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22823;&#37327;&#20195;&#30721;&#35821;&#20041;&#34920;&#31034;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SEGNN &#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644; F1&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05375</link><description>&lt;p&gt;
&#38754;&#21521;&#28304;&#20195;&#30721;&#28431;&#27934;&#35782;&#21035;&#30340;&#39034;&#24207;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sequential Graph Neural Networks for Source Code Vulnerability Identification. (arXiv:2306.05375v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#31574;&#21010;&#30340; C/C++ &#28304;&#20195;&#30721;&#28431;&#27934;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#27169;&#22411;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026; SEquential Graph Neural Network&#65288;SEGNN&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22823;&#37327;&#20195;&#30721;&#35821;&#20041;&#34920;&#31034;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SEGNN &#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644; F1&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28431;&#27934;&#35782;&#21035;&#23545;&#20110;&#32593;&#32476;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#22823;&#22411;&#24212;&#29992;&#20013;&#26597;&#25214;&#21644;&#20462;&#22797;&#26131;&#21463;&#25915;&#20987;&#30340;&#20989;&#25968;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#38752;&#21644;&#20805;&#20998;&#31649;&#29702;&#30340;&#25968;&#25454;&#38598;&#21644;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#39033;&#20219;&#21153;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#20381;&#36182;&#20110;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#27880;&#37322;&#25968;&#25454;&#38598;&#25110;&#25351;&#23450;&#29305;&#24449;&#65292;&#36825;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#35823;&#25253;&#29575;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#31574;&#21010;&#30340;C/C++&#28304;&#20195;&#30721;&#28431;&#27934;&#25968;&#25454;&#38598;CVEFunctionGraphEmbeddings&#65288;CVEFGE&#65289;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#27169;&#22411;&#12290;CVEFGE&#33258;&#21160;&#20174;CVE&#25968;&#25454;&#24211;&#20013;&#29228;&#21462;&#65292;&#20854;&#20013;&#21253;&#21547;&#30495;&#23454;&#21644;&#20844;&#24320;&#25259;&#38706;&#30340;&#28304;&#20195;&#30721;&#28431;&#27934;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;SEquential Graph Neural Network&#65288;SEGNN&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22823;&#37327;&#20195;&#30721;&#35821;&#20041;&#34920;&#31034;&#12290;SEGNN&#30001;&#20004;&#20010;&#22534;&#21472;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#35299;&#30721;&#22120;&#23618;&#32452;&#25104;&#65292;&#29992;&#20110;&#27169;&#25311;&#28304;&#20195;&#30721;&#20013;&#20989;&#25968;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vulnerability identification constitutes a task of high importance for cyber security. It is quite helpful for locating and fixing vulnerable functions in large applications. However, this task is rather challenging owing to the absence of reliable and adequately managed datasets and learning models. Existing solutions typically rely on human expertise to annotate datasets or specify features, which is prone to error. In addition, the learning models have a high rate of false positives. To bridge this gap, in this paper, we present a properly curated C/C++ source code vulnerability dataset, denoted as CVEFunctionGraphEmbeddings (CVEFGE), to aid in developing models. CVEFGE is automatically crawled from the CVE database, which contains authentic and publicly disclosed source code vulnerabilities. We also propose a learning framework based on graph neural networks, denoted SEquential Graph Neural Network (SEGNN) for learning a large number of code semantic representations. SEGNN consists
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#20998;&#26512;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#26368;&#39640;&#27861;&#38498;&#30340;&#21475;&#22836;&#36777;&#35770;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#21475;&#22836;&#36777;&#35770;&#26159;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#21644;&#25112;&#30053;&#24615;&#30340;&#20202;&#24335;&#65292;&#24459;&#24072;&#20351;&#29992;&#21508;&#31181;&#20462;&#36766;&#31574;&#30053;&#26469;&#25512;&#36827;&#20182;&#20204;&#30340;&#26696;&#20214;&#65292;&#32780;&#27861;&#23448;&#21017;&#36890;&#36807;&#25552;&#38382;&#21644;&#35780;&#35770;&#19981;&#20165;&#22609;&#36896;&#20182;&#20204;&#38754;&#21069;&#30340;&#26696;&#20214;&#65292;&#32780;&#19988;&#22609;&#36896;&#20102;&#27861;&#38498;&#26356;&#24191;&#27867;&#30340;&#27861;&#24459;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2306.05373</link><description>&lt;p&gt;
&#26368;&#39640;&#27861;&#38498;&#21475;&#22836;&#36777;&#35770;&#30340;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Computational Analysis of Oral Argument in the Supreme Court. (arXiv:2306.05373v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#20998;&#26512;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#26368;&#39640;&#27861;&#38498;&#30340;&#21475;&#22836;&#36777;&#35770;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#21475;&#22836;&#36777;&#35770;&#26159;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#21644;&#25112;&#30053;&#24615;&#30340;&#20202;&#24335;&#65292;&#24459;&#24072;&#20351;&#29992;&#21508;&#31181;&#20462;&#36766;&#31574;&#30053;&#26469;&#25512;&#36827;&#20182;&#20204;&#30340;&#26696;&#20214;&#65292;&#32780;&#27861;&#23448;&#21017;&#36890;&#36807;&#25552;&#38382;&#21644;&#35780;&#35770;&#19981;&#20165;&#22609;&#36896;&#20182;&#20204;&#38754;&#21069;&#30340;&#26696;&#20214;&#65292;&#32780;&#19988;&#22609;&#36896;&#20102;&#27861;&#38498;&#26356;&#24191;&#27867;&#30340;&#27861;&#24459;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26368;&#39640;&#27861;&#38498;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#20026;&#20844;&#24320;&#30340;&#37096;&#20998;&#65292;&#21475;&#22836;&#36777;&#35770;&#22312;&#22823;&#20247;&#23186;&#20307;&#20013;&#21463;&#21040;&#20102;&#38750;&#24120;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#26426;&#26500;&#65292;&#21475;&#22836;&#36777;&#35770;&#30340;&#22522;&#26412;&#21151;&#33021;&#21644;&#20316;&#29992;&#20173;&#28982;&#19981;&#20026;&#20154;&#20204;&#25152;&#29087;&#30693;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#21475;&#22836;&#36777;&#35770;&#30340;&#31163;&#25955;&#37327;&#21270;&#23646;&#24615;&#19978;&#65292;&#22914;&#23545;&#27599;&#20301;&#36777;&#25252;&#24459;&#24072;&#25552;&#38382;&#30340;&#27425;&#25968;&#12289;&#27861;&#23448;&#20219;&#21629;&#24635;&#32479;&#30340;&#20826;&#27966;&#21644;&#19978;&#35785;&#26696;&#20214;&#30340;&#24847;&#35782;&#24418;&#24577;&#24433;&#21709;&#31561;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24456;&#23569;&#28041;&#21450;&#21040;&#21475;&#22836;&#36777;&#35770;&#30340;&#20855;&#20307;&#36807;&#31243;&#21644;&#26356;&#24191;&#27867;&#30340;&#21046;&#24230;&#21644;&#35268;&#33539;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;1955&#24180;&#33267;2018&#24180;&#35821;&#26009;&#24211;&#30340;&#26368;&#39640;&#27861;&#38498;&#21475;&#22836;&#36777;&#35770;&#30340;&#35745;&#31639;&#20998;&#26512;&#26041;&#27861;&#65292;&#24378;&#35843;&#21475;&#22836;&#36777;&#35770;&#30340;&#36136;&#24615;&#21644;&#20132;&#20114;&#26041;&#38754;&#65292;&#26088;&#22312;&#25581;&#31034;&#22312;&#36777;&#35770;&#20013;&#24459;&#24072;&#21644;&#27861;&#23448;&#25152;&#37319;&#29992;&#30340;&#20462;&#36766;&#31574;&#30053;&#30340;&#27169;&#24335;&#21644;&#20542;&#21521;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21475;&#22836;&#36777;&#35770;&#26159;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#21644;&#25112;&#30053;&#24615;&#30340;&#20202;&#24335;&#65292;&#24459;&#24072;&#20351;&#29992;&#21508;&#31181;&#20462;&#36766;&#31574;&#30053;&#26469;&#25512;&#36827;&#20182;&#20204;&#30340;&#26696;&#20214;&#65292;&#32780;&#27861;&#23448;&#21017;&#36890;&#36807;&#25552;&#38382;&#21644;&#35780;&#35770;&#19981;&#20165;&#22609;&#36896;&#20182;&#20204;&#38754;&#21069;&#30340;&#26696;&#20214;&#65292;&#32780;&#19988;&#22609;&#36896;&#20102;&#27861;&#38498;&#26356;&#24191;&#27867;&#30340;&#27861;&#24459;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the most public component of the Supreme Court's decision-making process, oral argument receives an out-sized share of attention in the popular media. Despite its prominence, however, the basic function and operation of oral argument as an institution remains poorly understood, as political scientists and legal scholars continue to debate even the most fundamental questions about its role.  Past study of oral argument has tended to focus on discrete, quantifiable attributes of oral argument, such as the number of questions asked to each advocate, the party of the Justices' appointing president, or the ideological implications of the case on appeal. Such studies allow broad generalizations about oral argument and judicial decision making: Justices tend to vote in accordance with their ideological preferences, and they tend to ask more questions when they are skeptical of a party's position. But they tell us little about the actual goings on at oral argument -- the running dialog betw
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24207;&#25968;&#21183;&#20989;&#25968;&#30340;&#29609;&#23478;&#35780;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#36870;&#26144;&#23556;&#30340;&#23884;&#22871;&#35745;&#31639;&#65292;&#33021;&#22815;&#20445;&#25345;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05366</link><description>&lt;p&gt;
&#24207;&#25968;&#21183;&#20989;&#25968;&#30340;&#29609;&#23478;&#35780;&#32423;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ordinal Potential-based Player Rating. (arXiv:2306.05366v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05366
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24207;&#25968;&#21183;&#20989;&#25968;&#30340;&#29609;&#23478;&#35780;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#36870;&#26144;&#23556;&#30340;&#23884;&#22871;&#35745;&#31639;&#65292;&#33021;&#22815;&#20445;&#25345;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#23545;&#20110;&#20219;&#24847;&#32431;&#31574;&#30053;$x$&#12289;$y$&#21644;$z$&#65292;&#22914;&#26524;$x$&#27604;$y$&#26356;&#22909;&#65292;$y$&#27604;$z$&#26356;&#22909;&#65292;&#21017;$x$&#27604;$z$&#26356;&#22909;&#65292;&#21017;&#20004;&#20010;&#23545;&#31216;&#30340;&#38646;&#21644;&#21338;&#24328;&#26159;&#21487;&#20256;&#36882;&#30340;&#12290;&#26368;&#36817;&#35266;&#23519;&#21040;&#65292;Elo&#35780;&#32423;&#26410;&#33021;&#20445;&#25345;&#31574;&#30053;&#20043;&#38388;&#30340;&#20256;&#36882;&#20851;&#31995;&#65292;&#22240;&#27492;&#19981;&#33021;&#27491;&#30830;&#25552;&#21462;&#28216;&#25103;&#30340;&#20256;&#36882;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#34920;&#26126;&#24403;&#22312;&#27491;&#30830;&#30340;&#31354;&#38388;&#20013;&#35745;&#31639;Elo&#35780;&#32423;&#26102;&#65292;Elo&#35780;&#32423;&#30830;&#23454;&#33021;&#22815;&#20445;&#25345;&#20256;&#36882;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21512;&#36866;&#30340;&#21487;&#36870;&#26144;&#23556;$\varphi$&#23558;&#28216;&#25103;&#24212;&#29992;&#20110;$\varphi$&#65292;&#28982;&#21518;&#35745;&#31639;Elo&#35780;&#32423;&#65292;&#26368;&#21518;&#36890;&#36807;&#24212;&#29992;$\varphi^{-1}$&#22238;&#21040;&#21407;&#22987;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#21487;&#20256;&#36882;&#28216;&#25103;&#30340;&#34920;&#24449;&#20026;&#21183;&#28216;&#25103;&#30340;&#19968;&#20010;&#24369;&#21464;&#20307;&#65292;&#20854;&#21183;&#20989;&#25968;&#26159;&#21152;&#24615;&#21487;&#20998;&#31163;&#30340;&#12290;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20256;&#36882;&#24207;&#25968;&#30340;&#27010;&#24565;&#65292;&#21363;&#23558;&#21487;&#20256;&#36882;&#28216;&#25103;&#30340;&#25910;&#30410;&#36716;&#21270;&#20026;&#20854;&#24046;&#24322;&#25152;&#38656;&#30340;&#26368;&#23567;&#21487;&#36870;&#26144;&#23556;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A two-player symmetric zero-sum game is transitive if for any pure strategies $x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$ is better than $z$. It was recently observed that the Elo rating fails at preserving transitive relations among strategies and therefore cannot correctly extract the transitive component of a game. Our first contribution is to show that the Elo rating actually does preserve transitivity when computed in the right space. Precisely, using a suitable invertible mapping $\varphi$, we first apply $\varphi$ to the game, then compute Elo ratings, then go back to the original space by applying $\varphi^{-1}$. We provide a characterization of transitive games as a weak variant of ordinal potential games with additively separable potential functions. Leveraging this insight, we introduce the concept of transitivity order, the minimum number of invertible mappings required to transform the payoff of a transitive game into (differences of) its
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#26377;&#24433;&#21709;&#29305;&#24449;PCA&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;IF-VAE&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#20027;&#39064;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#22240;&#33455;&#29255;&#21644;&#21333;&#32454;&#32990;RNA-seq&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#34920;&#29616;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.05363</link><description>&lt;p&gt;
&#36890;&#36807;IF-PCA&#21644;&#19968;&#20123;&#26368;&#26032;&#30340;&#26041;&#27861;&#36827;&#34892;&#20027;&#39064;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Subject clustering by IF-PCA and several recent methods. (arXiv:2306.05363v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#26377;&#24433;&#21709;&#29305;&#24449;PCA&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;IF-VAE&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#20027;&#39064;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#22240;&#33455;&#29255;&#21644;&#21333;&#32454;&#32990;RNA-seq&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#32858;&#31867;&#26159;&#19968;&#20010;&#38750;&#24120;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#27979;&#37327;&#29305;&#24449;&#23558;&#21463;&#35797;&#32773;&#65288;&#22914;&#24739;&#32773;&#25110;&#32454;&#32990;&#65289;&#32858;&#31867;&#25104;&#22810;&#20010;&#32452;&#30340;&#26041;&#27861;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20854;&#20013;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#65288;UDL&#65289;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#27969;&#34892;&#30340;UDL&#26041;&#27861;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#21644;&#26368;&#26032;&#30340;&#26377;&#24433;&#21709;&#29305;&#24449;PCA&#65288;IF-PCA&#65289;&#24819;&#27861;&#65292;&#25552;&#20986;&#20102;IF-VAE&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#20027;&#39064;&#32858;&#31867;&#26041;&#27861;&#65292;&#35814;&#32454;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IF-VAE&#26174;&#33879;&#20248;&#20110;VAE&#65292;&#20294;&#20173;&#19981;&#22914;IF-PCA&#12290;&#21478;&#22806;&#65292;&#22312;8&#20010;&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#19978;&#65292;IF-PCA&#31245;&#24494;&#20248;&#20110;Seurat&#21644;SC3&#12290;
&lt;/p&gt;
&lt;p&gt;
Subject clustering (i.e., the use of measured features to cluster subjects, such as patients or cells, into multiple groups) is a problem of great interest. In recent years, many approaches were proposed, among which unsupervised deep learning (UDL) has received a great deal of attention. Two interesting questions are (a) how to combine the strengths of UDL and other approaches, and (b) how these approaches compare to one other.  We combine Variational Auto-Encoder (VAE), a popular UDL approach, with the recent idea of Influential Feature PCA (IF-PCA), and propose IF-VAE as a new method for subject clustering. We study IF-VAE and compare it with several other methods (including IF-PCA, VAE, Seurat, and SC3) on $10$ gene microarray data sets and $8$ single-cell RNA-seq data sets. We find that IF-VAE significantly improves over VAE, but still underperforms IF-PCA. We also find that IF-PCA is quite competitive, which slightly outperforms Seurat and SC3 over the $8$ single-cell data sets. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65288;MFF&#65289;&#65292;&#21033;&#29992;VGG&#31995;&#21015;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#24322;&#26500;&#38899;&#39057;-&#35270;&#35273;&#27169;&#24577;&#30340;&#34701;&#21512;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MFF&#23454;&#29616;&#20102;92.25%&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35777;&#26126;MFF&#22312;&#19981;&#21516;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#20855;&#26377;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05358</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#20013;&#21548;&#19981;&#35265;&#30340;&#25351;&#20196;&#25915;&#20987;&#30340;&#21487;&#20449;&#20256;&#24863;&#22120;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Sensor Fusion against Inaudible Command Attacks in Advanced Driver-Assistance System. (arXiv:2306.05358v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65288;MFF&#65289;&#65292;&#21033;&#29992;VGG&#31995;&#21015;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#24322;&#26500;&#38899;&#39057;-&#35270;&#35273;&#27169;&#24577;&#30340;&#34701;&#21512;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MFF&#23454;&#29616;&#20102;92.25%&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35777;&#26126;MFF&#22312;&#19981;&#21516;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#20855;&#26377;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36973;&#21463;&#24694;&#24847;&#25915;&#20987;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#21152;&#12290;&#20854;&#20013;&#65292;&#21548;&#19981;&#35265;&#30340;&#35821;&#38899;&#25351;&#20196;&#25915;&#20987;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#65292;&#22240;&#20026;&#35821;&#38899;&#25351;&#20196;&#24050;&#32463;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#22914;&#20309;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#26469;&#25269;&#24481;&#36825;&#20123;&#21548;&#19981;&#35265;&#30340;&#25915;&#20987;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#25506;&#35752;&#36816;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26469;&#36827;&#34892;&#38450;&#24481;&#65292;&#21364;&#27809;&#26377;&#32771;&#34385;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#25935;&#24863;&#30340;&#20219;&#21153;&#65292;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#22312;&#24110;&#21161;&#25552;&#39640;&#27169;&#22411;&#31283;&#20581;&#24615;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20851;&#38190;&#20219;&#21153;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65288;MFF&#65289;&#20316;&#20026;&#26234;&#33021;&#23433;&#20840;&#31995;&#32479;&#26469;&#38450;&#33539;&#21548;&#19981;&#35265;&#30340;&#35821;&#38899;&#25351;&#20196;&#25915;&#20987;&#12290;MFF&#21033;&#29992;VGG&#31995;&#21015;&#31070;&#32463;&#32593;&#32476;&#34701;&#21512;&#24322;&#26500;&#38899;&#39057;-&#35270;&#35273;&#27169;&#24577;&#65292;&#24182;&#22312;&#27604;&#36739;&#34701;&#21512;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;92.25%&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#23545;MFF&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#35777;&#26126;&#20854;&#22312;&#19981;&#21516;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are increasing concerns about malicious attacks on autonomous vehicles. In particular, inaudible voice command attacks pose a significant threat as voice commands become available in autonomous driving systems. How to empirically defend against these inaudible attacks remains an open question. Previous research investigates utilizing deep learning-based multimodal fusion for defense, without considering the model uncertainty in trustworthiness. As deep learning has been applied to increasingly sensitive tasks, uncertainty measurement is crucial in helping improve model robustness, especially in mission-critical scenarios. In this paper, we propose the Multimodal Fusion Framework (MFF) as an intelligent security system to defend against inaudible voice command attacks. MFF fuses heterogeneous audio-vision modalities using VGG family neural networks and achieves the detection accuracy of 92.25% in the comparative fusion method empirical study. Additionally, extensive experiments on
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#33258;&#21160;&#22320;&#21457;&#29616;&#19981;&#21516;&#30340;&#29983;&#25104;&#27010;&#24565;&#65292;&#24182;&#19988;&#36825;&#20123;&#29983;&#25104;&#27010;&#24565;&#21487;&#20197;&#34987;&#29992;&#20110;&#37325;&#26032;&#32452;&#21512;&#21644;&#29983;&#25104;&#26032;&#30340;&#33402;&#26415;&#21644;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#20316;&#20026;&#19968;&#31181;&#34920;&#31034;&#29992;&#20110;&#19979;&#28216;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.05357</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#19979;&#30340;&#32452;&#21512;&#24335;&#27010;&#24565;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models. (arXiv:2306.05357v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#33258;&#21160;&#22320;&#21457;&#29616;&#19981;&#21516;&#30340;&#29983;&#25104;&#27010;&#24565;&#65292;&#24182;&#19988;&#36825;&#20123;&#29983;&#25104;&#27010;&#24565;&#21487;&#20197;&#34987;&#29992;&#20110;&#37325;&#26032;&#32452;&#21512;&#21644;&#29983;&#25104;&#26032;&#30340;&#33402;&#26415;&#21644;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#20316;&#20026;&#19968;&#31181;&#34920;&#31034;&#29992;&#20110;&#19979;&#28216;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20351;&#24471;&#22312;&#19981;&#21516;&#39046;&#22495;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21512;&#25104;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#38656;&#35201;&#29992;&#25143;&#25351;&#23450;&#20182;&#20204;&#24819;&#35201;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#30456;&#21453;&#30340;&#38382;&#39064;&#8212;&#8212;&#22312;&#32473;&#20986;&#30340;&#19981;&#21516;&#22270;&#20687;&#38598;&#21512;&#20013;&#65292;&#25105;&#20204;&#33021;&#21542;&#21457;&#29616;&#20195;&#34920;&#27599;&#20010;&#22270;&#20687;&#30340;&#29983;&#25104;&#27010;&#24565;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20174;&#19968;&#32452;&#22270;&#20687;&#20013;&#21457;&#29616;&#29983;&#25104;&#30340;&#27010;&#24565;&#65292;&#23558;&#32472;&#30011;&#20013;&#19981;&#21516;&#30340;&#33402;&#26415;&#39118;&#26684;&#65292;&#23545;&#35937;&#21644;&#29031;&#26126;&#20174;&#21416;&#25151;&#22330;&#26223;&#20013;&#20998;&#35299;&#20986;&#26469;&#65292;&#24182;&#36890;&#36807;&#32473;&#23450;&#30340;ImageNet&#22270;&#20687;&#21457;&#29616;&#22270;&#20687;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#29983;&#25104;&#27010;&#24565;&#33021;&#22815;&#20934;&#30830;&#22320;&#34920;&#31034;&#22270;&#20687;&#30340;&#20869;&#23481;&#65292;&#33021;&#22815;&#37325;&#26032;&#32452;&#21512;&#21644;&#32452;&#25104;&#20197;&#29983;&#25104;&#26032;&#30340;&#33402;&#26415;&#21644;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#31181;&#34920;&#31034;&#26469;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have enabled high-resolution image synthesis across different domains, but require users to specify the content they wish to generate. In this paper, we consider the inverse problem -- given a collection of different images, can we discover the generative concepts that represent each image? We present an unsupervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to generate new artistic and hybrid images, and be further used as a representation for downstream classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26230;&#20307;&#29305;&#24322;&#24615;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35774;&#35745;&#20102;&#19968;&#31181;&#20114;&#26021;&#25513;&#30721;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#26230;&#20307;&#29289;&#24615;&#39044;&#27979;&#20013;&#23384;&#22312;&#30340;&#26631;&#35760;&#21463;&#38480;&#38382;&#39064;&#65292;&#24182;&#22686;&#24378;&#20102;&#34920;&#31034;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#36824;&#32771;&#34385;&#20102;&#26230;&#20307;&#32467;&#26500;&#20013;&#30340;&#21608;&#26399;&#24615;&#19981;&#21464;&#24615;&#65292;&#24320;&#21457;&#20102;&#21608;&#26399;&#24615;&#19981;&#21464;&#30340;&#22810;&#22270;&#27169;&#22359;&#21644;&#21608;&#26399;&#29305;&#24615;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.05344</link><description>&lt;p&gt;
&#27700;&#26230;&#26448;&#26009;&#24615;&#33021;&#39044;&#27979;&#30340;&#26230;&#20307;&#29305;&#24322;&#24615;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Crystal-Specific Pre-Training Framework for Crystal Material Property Prediction. (arXiv:2306.05344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26230;&#20307;&#29305;&#24322;&#24615;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35774;&#35745;&#20102;&#19968;&#31181;&#20114;&#26021;&#25513;&#30721;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#26230;&#20307;&#29289;&#24615;&#39044;&#27979;&#20013;&#23384;&#22312;&#30340;&#26631;&#35760;&#21463;&#38480;&#38382;&#39064;&#65292;&#24182;&#22686;&#24378;&#20102;&#34920;&#31034;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#36824;&#32771;&#34385;&#20102;&#26230;&#20307;&#32467;&#26500;&#20013;&#30340;&#21608;&#26399;&#24615;&#19981;&#21464;&#24615;&#65292;&#24320;&#21457;&#20102;&#21608;&#26399;&#24615;&#19981;&#21464;&#30340;&#22810;&#22270;&#27169;&#22359;&#21644;&#21608;&#26399;&#29305;&#24615;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#26230;&#29289;&#24615;&#39044;&#27979;&#26159;&#24320;&#21457;&#26032;&#26448;&#26009;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#21152;&#36895;&#26230;&#20307;&#30340;&#30740;&#31350;&#65292;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#25216;&#26415;&#38590;&#39064;&#65306;&#20854;&#19968;&#65292;&#26631;&#35760;&#26230;&#20307;&#29289;&#24615;&#26412;&#36136;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#32791;&#26102;&#30340;&#29289;&#29702;&#27169;&#25311;&#25110;&#23454;&#39564;&#65307;&#20854;&#20108;&#65292;&#26230;&#20307;&#36981;&#24490;&#29305;&#23450;&#30340;&#37327;&#23376;&#21270;&#23398;&#21407;&#29702;&#65292;&#21363;&#21608;&#26399;&#24615;&#19981;&#21464;&#24615;&#65292;&#24448;&#24448;&#26080;&#27861;&#34987;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25429;&#25417;&#21040;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26230;&#20307;&#29305;&#24322;&#24615;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#26230;&#20307;&#34920;&#31034;&#12290;&#26694;&#26550;&#35774;&#35745;&#20102;&#19968;&#31181;&#20114;&#26021;&#25513;&#30721;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#34920;&#31034;&#23398;&#20064;&#65292;&#32531;&#35299;&#20102;&#26230;&#20307;&#29289;&#24615;&#39044;&#27979;&#20013;&#23384;&#22312;&#30340;&#26631;&#35760;&#21463;&#38480;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26230;&#20307;&#32467;&#26500;&#20013;&#30340;&#21608;&#26399;&#24615;&#19981;&#21464;&#24615;&#65292;&#24320;&#21457;&#20102;&#21608;&#26399;&#24615;&#19981;&#21464;&#30340;&#22810;&#22270;&#27169;&#22359;&#21644;&#21608;&#26399;&#29305;&#24615;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crystal property prediction is a crucial aspect of developing novel materials. However, there are two technical challenges to be addressed for speeding up the investigation of crystals. First, labeling crystal properties is intrinsically difficult due to the high cost and time involved in physical simulations or lab experiments. Second, crystals adhere to a specific quantum chemical principle known as periodic invariance, which is often not captured by existing machine learning methods. To overcome these challenges, we propose the crystal-specific pre-training framework for learning crystal representations with self-supervision. The framework designs a mutex mask strategy for enhancing representation learning so as to alleviate the limited labels available for crystal property prediction. Moreover, we take into account the specific periodic invariance in crystal structures by developing a periodic invariance multi-graph module and periodic attribute learning within our framework. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#27867;&#21270;&#20445;&#35777;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FTW-ERM&#65292;&#22312;&#22788;&#29702;&#21327;&#21464;&#37327;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.05325</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#20855;&#26377;&#27867;&#21270;&#20445;&#35777;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning under Covariate Shifts with Generalization Guarantees. (arXiv:2306.05325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#27867;&#21270;&#20445;&#35777;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FTW-ERM&#65292;&#22312;&#22788;&#29702;&#21327;&#21464;&#37327;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#20869;&#21644;&#23458;&#25143;&#31471;&#38388;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#24635;&#20307;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#22788;&#29702;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;Federated Importance-Weighted Empirical Risk Minimization&#65288;FTW-ERM&#65289;&#65292;&#25913;&#36827;&#20102;&#23494;&#24230;&#27604;&#21305;&#37197;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#23436;&#32654;&#30693;&#35782;&#26469;&#22788;&#29702;&#30495;&#23454;&#27604;&#29575;&#30340;&#19978;&#30830;&#30028;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#36890;&#20449;&#39640;&#25928;&#30340;&#21464;&#20307;FITW-ERM&#65292;&#20854;&#38544;&#31169;&#20445;&#35777;&#19982;&#32463;&#20856;ERM&#22312;FL&#20013;&#30340;&#38544;&#31169;&#20445;&#35777;&#30456;&#21516;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;FTW-ERM&#22312;&#26576;&#20123;&#35774;&#32622;&#19979;&#27604;&#32463;&#20856;ERM&#36798;&#21040;&#26356;&#23567;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#25968;&#25454;&#20998;&#24067;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#21457;&#29983;&#20559;&#31227;&#30340;&#25361;&#25112;&#24615;&#22833;&#34913;&#32852;&#37030;&#35774;&#32622;&#26041;&#38754;&#65292;FTW-ERM&#20248;&#20110;&#29616;&#26377;&#30340;FL&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses intra-client and inter-client covariate shifts in federated learning (FL) with a focus on the overall generalization performance. To handle covariate shifts, we formulate a new global model training paradigm and propose Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM) along with improving density ratio matching methods without requiring perfect knowledge of the supremum over true ratios. We also propose the communication-efficient variant FITW-ERM with the same level of privacy guarantees as those of classical ERM in FL. We theoretically show that FTW-ERM achieves smaller generalization error than classical ERM under certain settings. Experimental results demonstrate the superiority of FTW-ERM over existing FL baselines in challenging imbalanced federated settings in terms of data distribution shifts across clients.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20986;&#22810;&#20013;&#24515;&#35782;&#21035;&#27169;&#22411;&#65292;&#25972;&#20307; F1&#24471;&#20998;&#20026;84.77%&#12290;&#35813;&#27169;&#22411;&#23558;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05323</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#26041;&#27861;&#35770;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#22810;&#20013;&#24515;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20986;&#22810;&#20013;&#24515;&#35782;&#21035;&#27169;&#22411;&#65292;&#25972;&#20307; F1&#24471;&#20998;&#20026;84.77%&#12290;&#35813;&#27169;&#22411;&#23558;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#38498;&#24341;&#20837;&#35745;&#31639;&#26426;&#21270;&#21307;&#30103;&#35760;&#24405;&#26377;&#21161;&#20110;&#20943;&#23569;&#25163;&#20889;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#32321;&#29712;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#21462;&#25968;&#25454;&#38656;&#35201;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#22240;&#27492;&#21307;&#30103;&#35760;&#24405;&#20013;&#21253;&#21547;&#30340;&#25968;&#25454;&#20173;&#28982;&#34987;&#20805;&#20998;&#21033;&#29992;&#31243;&#24230;&#20302;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#39046;&#22495;&#20449;&#24687;&#25552;&#21462;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#20351;&#29992;&#33258;&#21160;&#21270;&#25991;&#26412;&#25366;&#25496;&#27969;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598; PsyNIT&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#24320;&#21457;&#36825;&#19968;&#20219;&#21153;&#30340;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#20351;&#29992;&#19977;&#20010;&#22806;&#37096;&#29420;&#31435;&#25968;&#25454;&#38598;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#20013;&#24515;&#27169;&#22411;&#65292;&#25972;&#20307; F1 &#24471;&#20998;&#20026; 84.77%&#65292;&#31934;&#30830;&#29575;&#20026; 83.16%&#65292;&#21484;&#22238;&#29575;&#20026; 86.44%&#12290;&#25105;&#20204;&#23398;&#21040;&#30340;&#32463;&#39564;&#26159;: (i) &#19968;&#33268;&#30340;&#27880;&#37322;&#36807;&#31243;&#30340;&#20851;&#38190;&#20316;&#29992;&#21644; (ii) &#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#21644;&#8220;&#23569;&#37327;&#35757;&#32451;&#8221;&#30340; fine-tuning &#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of computerized medical records in hospitals has reduced burdensome operations like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting them from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation, using automated text-mining pipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Large Language Model for this task. Moreover, we conducted several experiments with three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a "few-shot" a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#28508;&#21464;&#37327;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(LNODEs)&#23454;&#29616;&#20102;&#23454;&#26102;&#20840;&#24515;&#33039;&#30005;&#26426;&#20223;&#30495;&#65292;&#24182;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#21487;&#24555;&#36895;&#39044;&#27979;&#23545;&#19981;&#21516;&#32908;&#21147;&#24178;&#39044;&#30340;&#21387;&#21147;-&#23481;&#31215;&#26354;&#32447;&#65292;&#20026;&#24739;&#32773;&#29305;&#24322;&#24615;&#35786;&#26029;&#21644;&#27835;&#30103;&#20248;&#21270;&#25552;&#20379;&#20102;&#28508;&#22312;&#24555;&#36895;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.05321</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#21464;&#37327;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#23454;&#26102;&#20840;&#24515;&#33039;&#30005;&#26426;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
Real-time whole-heart electromechanical simulations using Latent Neural Ordinary Differential Equations. (arXiv:2306.05321v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05321
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#28508;&#21464;&#37327;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(LNODEs)&#23454;&#29616;&#20102;&#23454;&#26102;&#20840;&#24515;&#33039;&#30005;&#26426;&#20223;&#30495;&#65292;&#24182;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#21487;&#24555;&#36895;&#39044;&#27979;&#23545;&#19981;&#21516;&#32908;&#21147;&#24178;&#39044;&#30340;&#21387;&#21147;-&#23481;&#31215;&#26354;&#32447;&#65292;&#20026;&#24739;&#32773;&#29305;&#24322;&#24615;&#35786;&#26029;&#21644;&#27835;&#30103;&#20248;&#21270;&#25552;&#20379;&#20102;&#28508;&#22312;&#24555;&#36895;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#25968;&#23383;&#23402;&#29983;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21644;&#29983;&#29702;&#23398;&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#20379;&#39044;&#27979;&#24615;&#21644;&#20010;&#24615;&#21270;&#21307;&#23398;&#12290;&#28982;&#32780;&#65292;&#39640;&#20445;&#30495;&#24230;&#30340;&#22810;&#23610;&#24230;&#24515;&#33039;&#27169;&#22411;&#20173;&#28982;&#26159;&#37319;&#29992;&#30340;&#38556;&#30861;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24191;&#27867;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#38656;&#35201;&#22823;&#37327;&#30340;&#27169;&#22411;&#35780;&#20272;&#36827;&#34892;&#29305;&#23450;&#20110;&#24739;&#32773;&#30340;&#20010;&#24615;&#21270;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#21019;&#24314;&#24555;&#36895;&#20934;&#30830;&#30340;&#20840;&#24515;&#25968;&#23383;&#23402;&#29983;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28508;&#21464;&#37327;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(LNODEs)&#26469;&#23398;&#20064;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#26102;&#38388;&#21387;&#21147;-&#23481;&#31215;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#22522;&#20110;LNODEs&#30340;&#20195;&#29702;&#27169;&#22411;&#26159;&#20174;400&#20010;&#19977;&#32500;&#38646;&#32500;&#20840;&#24515;&#38381;&#29615;&#30005;&#26426;&#20223;&#30495;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#21516;&#26102;&#32771;&#34385;43&#20010;&#27169;&#22411;&#21442;&#25968;&#65292;&#25551;&#36848;&#21333;&#20010;&#32454;&#32990;&#21040;&#25972;&#20010;&#22120;&#23448;&#21644;&#24515;&#34880;&#31649;&#34880;&#28082;&#21160;&#21147;&#23398;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;LNODE&#25552;&#20379;&#20102;3D-0D&#27169;&#22411;&#22312;&#28508;&#31354;&#38388;&#20013;&#30340;&#32039;&#20945;&#19988;&#39640;&#25928;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#21069;&#39304;&#20840;&#36830;&#25509;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LNODE&#30340;&#20195;&#29702;&#21487;&#20197;&#23545;&#19981;&#21516;&#30340;&#32908;&#21147;&#20171;&#20837;&#20570;&#20986;&#23454;&#26102;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#24555;&#36895;&#24037;&#20855;&#65292;&#29992;&#20110;&#29305;&#23450;&#20110;&#24739;&#32773;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiac digital twins provide a physics and physiology informed framework to deliver predictive and personalized medicine. However, high-fidelity multi-scale cardiac models remain a barrier to adoption due to their extensive computational costs and the high number of model evaluations needed for patient-specific personalization. Artificial Intelligence-based methods can make the creation of fast and accurate whole-heart digital twins feasible. In this work, we use Latent Neural Ordinary Differential Equations (LNODEs) to learn the temporal pressure-volume dynamics of a heart failure patient. Our surrogate model based on LNODEs is trained from 400 3D-0D whole-heart closed-loop electromechanical simulations while accounting for 43 model parameters, describing single cell through to whole organ and cardiovascular hemodynamics. The trained LNODEs provides a compact and efficient representation of the 3D-0D model in a latent space by means of a feedforward fully-connected Artificial Neural 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#23450;&#20041;&#30697;&#38453;&#22788;&#29702;&#21355;&#26143;&#27979;&#37327;&#29305;&#24449;&#21644;&#20266;&#36317;&#27531;&#24046;&#65292;&#20197;&#25552;&#39640;&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479;(GNSS)&#30340;&#23450;&#20301;&#31934;&#24230;</title><link>http://arxiv.org/abs/2306.05319</link><description>&lt;p&gt;
&#22522;&#20110;RNN&#30340;GNSS&#23450;&#20301;&#31995;&#32479;&#20351;&#29992;&#21355;&#26143;&#27979;&#37327;&#29305;&#24449;&#21644;&#20266;&#36317;&#27531;&#24046;
&lt;/p&gt;
&lt;p&gt;
RNN-Based GNSS Positioning using Satellite Measurement Features and Pseudorange Residuals. (arXiv:2306.05319v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#23450;&#20041;&#30697;&#38453;&#22788;&#29702;&#21355;&#26143;&#27979;&#37327;&#29305;&#24449;&#21644;&#20266;&#36317;&#27531;&#24046;&#65292;&#20197;&#25552;&#39640;&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479;(GNSS)&#30340;&#23450;&#20301;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479;(GNSS)&#20013;&#65292;&#21487;&#29992;&#21355;&#26143;&#30340;&#25968;&#37327;&#22686;&#21152;&#23548;&#33268;&#22312;&#36873;&#25321;&#26368;&#20934;&#30830;&#30340;&#20266;&#36317;&#36129;&#29486;&#26041;&#38754;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22240;&#20026;&#26377;&#20559;&#24046;&#30340;&#27979;&#37327;&#23545;&#23450;&#20301;&#31934;&#24230;&#24433;&#21709;&#24456;&#22823;&#65292;&#29305;&#21035;&#26159;&#22312;&#21333;&#26102;&#21051;&#22330;&#26223;&#20013;&#12290;&#35813;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#39044;&#27979;&#38142;&#36335;&#27979;&#37327;&#36136;&#37327;&#22240;&#23376;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20248;&#21270;&#27979;&#37327;&#21152;&#26435;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#30697;&#38453;&#65292;&#30001;&#26465;&#20214;&#20266;&#36317;&#27531;&#24046;&#21644;&#27599;&#20010;&#38142;&#36335;&#21355;&#26143;&#25351;&#26631;(&#20363;&#22914;&#36733;&#27874;&#22122;&#22768;&#21151;&#29575;&#23494;&#24230;&#27604;&#21450;&#20854;&#32463;&#39564;&#32479;&#35745;&#37327;&#12289;&#21355;&#26143;&#39640;&#24230;&#12289;&#36733;&#27874;&#30456;&#20301;&#38145;&#23450;&#26102;&#38388;)&#32452;&#25104;&#12290;&#28982;&#21518;&#65292;&#23558;&#35813;&#30697;&#38453;&#20316;&#20026;&#36755;&#20837;&#25552;&#20379;&#32473;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#65288;&#21363;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#32593;&#32476;&#65289;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the Global Navigation Satellite System (GNSS) context, the growing number of available satellites has lead to many challenges when it comes to choosing the most accurate pseudorange contributions, given the strong impact of biased measurements on positioning accuracy, particularly in single-epoch scenarios. This work leverages the potential of machine learning in predicting link-wise measurement quality factors and, hence, optimize measurement weighting. For this purpose, we use a customized matrix composed of heterogeneous features such as conditional pseudorange residuals and per-link satellite metrics (e.g., carrier-to-noise power density ratio and its empirical statistics, satellite elevation, carrier phase lock time). This matrix is then fed as an input to a recurrent neural network (RNN) (i.e., a long-short term memory (LSTM) network). Our experimental results on real data, obtained from extensive field measurements, demonstrate the high potential of our proposed solution bein
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19977;&#31181;&#22270;&#20687;&#26680;&#24515;&#31639;&#27861;&#65292;&#20197;&#23545;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#24378;&#21270;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#21387;&#32553;&#21644;&#21435;&#22122;&#65292;&#20197;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#35745;&#31639;&#21644;&#25104;&#20687;&#29615;&#22659;&#12290;&#26368;&#22823;&#29109;&#26680;&#24515;&#31639;&#27861;&#20197;&#36739;&#20302;&#21387;&#32553;&#27604;&#33719;&#24471;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05310</link><description>&lt;p&gt;
&#21160;&#24577;&#35757;&#32451;&#21644;&#36866;&#24212;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21040;&#19981;&#21516;&#30340;&#12289;&#20302;&#35745;&#31639;&#30340;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25918;&#23556;&#23398;&#37096;&#32626;&#29615;&#22659;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A framework for dynamically training and adapting deep reinforcement learning models to different, low-compute, and continuously changing radiology deployment environments. (arXiv:2306.05310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19977;&#31181;&#22270;&#20687;&#26680;&#24515;&#31639;&#27861;&#65292;&#20197;&#23545;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#24378;&#21270;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#21387;&#32553;&#21644;&#21435;&#22122;&#65292;&#20197;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#35745;&#31639;&#21644;&#25104;&#20687;&#29615;&#22659;&#12290;&#26368;&#22823;&#29109;&#26680;&#24515;&#31639;&#27861;&#20197;&#36739;&#20302;&#21387;&#32553;&#27604;&#33719;&#24471;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Deep Reinforcement Learning&#22312;&#21307;&#23398;&#24433;&#20687;&#26041;&#38754;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#36890;&#24120;&#38656;&#35201;&#24378;&#22823;&#30340;GPU&#12290;&#30001;&#20110;&#24433;&#20687;&#29615;&#22659;&#30340;&#24555;&#36895;&#28436;&#21464;&#21644;&#36793;&#32536;&#35774;&#22791;&#30340;&#26222;&#21450;&#65292;&#31639;&#27861;&#38656;&#35201;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#65292;&#20026;&#20302;&#35745;&#31639;&#35774;&#22791;&#36827;&#34892;&#36866;&#37197;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#22270;&#20687;&#26680;&#24515;&#31639;&#27861;&#65292;&#20197;&#23545;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#24378;&#21270;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#21387;&#32553;&#21644;&#21435;&#22122;&#12290;&#25105;&#20204;&#22312;&#20840;&#36523;DIXON&#27700;&#21644;DIXON&#33026;&#32938;MRI&#22270;&#20687;&#19978;&#23454;&#26045;&#20102;&#37051;&#22495;&#24179;&#22343;&#26680;&#24515;&#31639;&#27861;&#12289;&#37051;&#22495;&#28789;&#25935;&#24230;&#37319;&#26679;&#26680;&#24515;&#31639;&#27861;&#21644;&#26368;&#22823;&#29109;&#26680;&#24515;&#31639;&#27861;&#12290;&#25152;&#26377;&#19977;&#31181;&#26680;&#24515;&#31639;&#27861;&#37117;&#33021;&#22815;&#20197;27&#20493;&#30340;&#21387;&#32553;&#29575;&#22312;&#20004;&#31181;&#25104;&#20687;&#29615;&#22659;&#19979;&#23450;&#20301;5&#20010;&#35299;&#21078;&#26631;&#24535;&#65306;&#24038;&#33181;&#12289;&#21491;&#36716;&#23376;&#12289;&#24038;&#32958;&#12289;&#33086;&#33039;&#21644;&#32954;&#12290;&#26368;&#22823;&#29109;&#26680;&#24515;&#31639;&#27861;&#22312;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#26041;&#38754;&#33719;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#65292;&#20026;$11.97\pm12.02$&#12290;
&lt;/p&gt;
&lt;p&gt;
While Deep Reinforcement Learning has been widely researched in medical imaging, the training and deployment of these models usually require powerful GPUs. Since imaging environments evolve rapidly and can be generated by edge devices, the algorithm is required to continually learn and adapt to changing environments, and adjust to low-compute devices. To this end, we developed three image coreset algorithms to compress and denoise medical images for selective experience replayed-based lifelong reinforcement learning. We implemented neighborhood averaging coreset, neighborhood sensitivity-based sampling coreset, and maximum entropy coreset on full-body DIXON water and DIXON fat MRI images. All three coresets produced 27x compression with excellent performance in localizing five anatomical landmarks: left knee, right trochanter, left kidney, spleen, and lung across both imaging environments. Maximum entropy coreset obtained the best performance of $11.97\pm 12.02$ average distance error,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#23545;&#24615;&#21035;&#27495;&#35270;&#30340;&#20559;&#35265;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.05307</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20844;&#24179;&#24230;&#25351;&#26631;&#35780;&#20272;&#27495;&#35270;&#20559;&#24046;&#26159;&#21542;&#36275;&#22815;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are fairness metric scores enough to assess discrimination biases in machine learning?. (arXiv:2306.05307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#23545;&#24615;&#21035;&#27495;&#35270;&#30340;&#20559;&#35265;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#23545;&#24615;&#21035;&#27495;&#35270;&#30340;&#20559;&#35265;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#20197;Bios&#25968;&#25454;&#38598;&#20026;&#20363;&#65292;&#23398;&#20064;&#39044;&#27979;&#20010;&#20154;&#30340;&#32844;&#19994;&#12290;&#36825;&#31181;&#39044;&#27979;&#20219;&#21153;&#22312;&#21830;&#19994;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#31243;&#24207;&#20013;&#24456;&#24120;&#35265;&#65292;&#20363;&#22914;&#33258;&#21160;&#24037;&#20316;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents novel experiments shedding light on the shortcomings of current metrics for assessing biases of gender discrimination made by machine learning algorithms on textual data. We focus on the Bios dataset, and our learning task is to predict the occupation of individuals, based on their biography. Such prediction tasks are common in commercial Natural Language Processing (NLP) applications such as automatic job recommendations. We address an important limitation of theoretical discussions dealing with group-wise fairness metrics: they focus on large datasets, although the norm in many industrial NLP applications is to use small to reasonably large linguistic datasets for which the main practical constraint is to get a good prediction accuracy. We then question how reliable are different popular measures of bias when the size of the training set is simply sufficient to learn reasonably accurate predictions. Our experiments sample the Bios dataset and learn more than 200 m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36890;&#29992;&#30340;&#22823;&#35268;&#27169;&#21644;&#28508;&#22312;&#26410;&#30693;&#22270;&#19978;&#23450;&#20041;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#22270;&#20869;&#26680;&#65292;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.05304</link><description>&lt;p&gt;
&#22270;&#19978;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimisation of Functions on Graphs. (arXiv:2306.05304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36890;&#29992;&#30340;&#22823;&#35268;&#27169;&#21644;&#28508;&#22312;&#26410;&#30693;&#22270;&#19978;&#23450;&#20041;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#22270;&#20869;&#26680;&#65292;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#19981;&#26029;&#28044;&#29616;&#25512;&#21160;&#20102;&#22312;&#22270;&#33410;&#28857;&#38598;&#19978;&#23450;&#20041;&#20989;&#25968;&#30340;&#20248;&#21270;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#22270;&#25628;&#32034;&#31639;&#27861;&#21487;&#29992;&#20110;&#27492;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#65292;&#24182;&#19988;&#19981;&#21033;&#29992;&#20851;&#20110;&#20989;&#25968;&#20540;&#30340;&#20449;&#24687;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31867;&#26377;&#21069;&#36884;&#30340;&#40657;&#30418;&#27714;&#35299;&#22120;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#23427;&#24456;&#23569;&#34987;&#24212;&#29992;&#20110;&#36825;&#26679;&#30340;&#26032;&#39062;&#35774;&#32622;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20248;&#21270;&#22312;&#36890;&#29992;&#65292;&#22823;&#35268;&#27169;&#21644;&#28508;&#22312;&#30340;&#26410;&#30693;&#22270;&#19978;&#23450;&#20041;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#22270;&#20869;&#26680;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#34892;&#20026;&#30340;&#20248;&#28857;&#12290;&#23616;&#37096;&#24314;&#27169;&#26041;&#27861;&#36827;&#19968;&#27493;&#20445;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing availability of graph-structured data motivates the task of optimising over functions defined on the node set of graphs. Traditional graph search algorithms can be applied in this case, but they may be sample-inefficient and do not make use of information about the function values; on the other hand, Bayesian optimisation is a class of promising black-box solvers with superior sample efficiency, but it has been scarcely been applied to such novel setups. To fill this gap, we propose a novel Bayesian optimisation framework that optimises over functions defined on generic, large-scale and potentially unknown graphs. Through the learning of suitable kernels on graphs, our framework has the advantage of adapting to the behaviour of the target function. The local modelling approach further guarantees the efficiency of our method. Extensive experiments on both synthetic and real-world graphs demonstrate the effectiveness of the proposed optimisation framework.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25361;&#25112;&#20102;&#22312;&#26102;&#38388;&#19978;&#26159;&#19981;&#30456;&#20851;&#30340;&#20551;&#35774;&#65292;&#24182;&#24378;&#35843;&#20102;epoch-based&#22122;&#22768;&#30456;&#20851;&#24615;&#23545;&#31163;&#25955;&#26102;&#38388;&#24102;&#21160;&#37327;&#30340;SGD&#30340;&#26435;&#37325;&#26041;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.05300</link><description>&lt;p&gt;
&#22522;&#20110;Epoch&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30456;&#20851;&#22122;&#22768;&#65306;&#26435;&#37325;&#26041;&#24046;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances. (arXiv:2306.05300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05300
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25361;&#25112;&#20102;&#22312;&#26102;&#38388;&#19978;&#26159;&#19981;&#30456;&#20851;&#30340;&#20551;&#35774;&#65292;&#24182;&#24378;&#35843;&#20102;epoch-based&#22122;&#22768;&#30456;&#20851;&#24615;&#23545;&#31163;&#25955;&#26102;&#38388;&#24102;&#21160;&#37327;&#30340;SGD&#30340;&#26435;&#37325;&#26041;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#24050;&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#30707;&#65292;&#20294;&#35748;&#20026;SGD&#24341;&#20837;&#30340;&#22122;&#22768;&#22312;&#26102;&#38388;&#19978;&#26159;&#19981;&#30456;&#20851;&#30340;&#65292;&#23613;&#31649;epoch-based&#35757;&#32451;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#27492;&#36827;&#34892;&#20102;&#25361;&#25112;&#65292;&#24182;&#35843;&#26597;&#20102;epoch-based&#22122;&#22768;&#30456;&#20851;&#24615;&#23545;&#31163;&#25955;&#26102;&#38388;&#24102;&#21160;&#37327;&#30340;SGD&#30340;&#31283;&#24577;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#38480;&#20110;&#20108;&#27425;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#20004;&#20010;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35745;&#31639;&#35757;&#32451;epoch&#26102;&#22122;&#22768;&#30340;&#31934;&#30830;&#33258;&#30456;&#20851;&#24615;&#65292;&#20551;&#35774;&#35813;&#22122;&#22768;&#29420;&#31435;&#20110;&#26435;&#37325;&#21521;&#37327;&#20013;&#30340;&#23567;&#27874;&#21160;;&#20854;&#27425;&#65292;&#25105;&#20204;&#25506;&#32034;epoch-based&#23398;&#20064;&#26041;&#26696;&#24341;&#20837;&#30340;&#30456;&#20851;&#24615;&#23545;SGD&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26354;&#29575;&#22823;&#20110;&#19968;&#20010;&#36229;&#21442;&#25968;&#30456;&#20851;&#20540;&#30340;&#26041;&#21521;&#19978;&#65292;&#36824;&#21407;&#20102;&#19981;&#30456;&#20851;&#22122;&#22768;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#30456;&#23545;&#24179;&#22374;&#30340;&#26041;&#21521;&#19978;&#65292;&#26435;&#37325;&#26041;&#24046;&#26174;&#30528;&#20943;&#23567;&#12290;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#30340;&#20108;&#32500;&#22270;&#20363;&#23545;&#36825;&#20123;&#32467;&#26524;&#36827;&#34892;&#20102;&#30452;&#35266;&#35299;&#37322;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#20851;&#20110;epoch-based SGD&#20013;&#30456;&#20851;&#22122;&#22768;&#24433;&#21709;&#30340;&#35265;&#35299;&#65292;&#21487;&#20197;&#25351;&#23548;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD) has become a cornerstone of neural network optimization, yet the noise introduced by SGD is often assumed to be uncorrelated over time, despite the ubiquity of epoch-based training. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum, limited to a quadratic loss. Our main contributions are twofold: first, we calculate the exact autocorrelation of the noise for training in epochs under the assumption that the noise is independent of small fluctuations in the weight vector; second, we explore the influence of correlations introduced by the epoch-based learning scheme on SGD dynamics. We find that for directions with a curvature greater than a hyperparameter-dependent crossover value, the results for uncorrelated noise are recovered. However, for relatively flat directions, the weight variance is significantly reduced. We provide an intui
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#21033;&#29992;&#20391;&#38754;&#20449;&#24687;&#37325;&#24314;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#22320;&#22270;&#30340;&#38382;&#39064;&#65292;&#37319;&#29992;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;&#20351;&#29992;&#39069;&#22806;&#30340;&#20391;&#38754;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22312;&#19977;&#20010;&#20027;&#35201;&#22478;&#24066;&#25968;&#25454;&#38598;&#20013;&#37325;&#24314;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05294</link><description>&lt;p&gt;
&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#23556;&#39057;&#22320;&#22270;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Deep Learning with Partially Labeled Data for Radio Map Reconstruction. (arXiv:2306.05294v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#21033;&#29992;&#20391;&#38754;&#20449;&#24687;&#37325;&#24314;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#22320;&#22270;&#30340;&#38382;&#39064;&#65292;&#37319;&#29992;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;&#20351;&#29992;&#39069;&#22806;&#30340;&#20391;&#38754;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22312;&#19977;&#20010;&#20027;&#35201;&#22478;&#24066;&#25968;&#25454;&#38598;&#20013;&#37325;&#24314;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#22522;&#20110;&#20301;&#32622;&#30456;&#20851;&#30340;&#26080;&#32447;&#30005;&#27979;&#37327;&#21644;&#21033;&#29992;&#20851;&#20110;&#26412;&#22320;&#21306;&#22495;&#30340;&#20391;&#38754;&#30693;&#35782;&#65288;&#20363;&#22914;&#22478;&#24066;&#35268;&#21010;&#65292;&#22320;&#24418;&#39640;&#24230;&#65292;&#32593;&#20851;&#20301;&#32622;&#65289;&#30340;&#38382;&#39064;&#65292;&#20197;&#37325;&#24314;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#22320;&#22270;&#12290;&#26681;&#25454;&#27492;&#31867;&#20808;&#21069;&#20391;&#38754;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#37319;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26469;&#25214;&#21040;&#26368;&#20339;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26368;&#20339;&#26550;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#19977;&#20010;&#23545;&#24212;&#20110;&#19977;&#20010;&#20027;&#35201;&#22478;&#24066;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#39069;&#22806;&#30340;&#20391;&#38754;&#20449;&#24687;&#21487;&#20197;&#22686;&#24378;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#22320;&#22270;&#37325;&#24314;&#30340;&#26368;&#32456;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#32593;&#20851;&#38468;&#36817;&#30340;&#23376;&#21306;&#22495;&#65292;&#22240;&#20026;&#36890;&#24120;&#35266;&#23519;&#21040;&#24179;&#22343;&#25509;&#25910;&#20449;&#21495;&#21151;&#29575;&#30340;&#21464;&#21270;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the problem of Received Signal Strength map reconstruction based on location-dependent radio measurements and utilizing side knowledge about the local region; for example, city plan, terrain height, gateway position. Depending on the quantity of such prior side information, we employ Neural Architecture Search to find an optimized Neural Network model with the best architecture for each of the supposed settings. We demonstrate that using additional side information enhances the final accuracy of the Received Signal Strength map reconstruction on three datasets that correspond to three major cities, particularly in sub-areas near the gateways where larger variations of the average received signal power are typically observed.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65292;&#25552;&#39640;&#20302;&#28385;&#24847;&#24230;&#29992;&#25143;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#20445;&#25345;&#24635;&#20307;&#25512;&#33616;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.05292</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Safe Collaborative Filtering. (arXiv:2306.05292v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65292;&#25552;&#39640;&#20302;&#28385;&#24847;&#24230;&#29992;&#25143;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#20445;&#25345;&#24635;&#20307;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#20363;&#22914;&#31639;&#27861;&#20844;&#24179;&#24615;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#39118;&#38505;&#25935;&#24863;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#20248;&#31168;&#30340;&#23614;&#37096;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#23545;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#30340;&#26377;&#25928;&#22788;&#29702;&#12290;&#23614;&#37096;&#24615;&#33021;&#20063;&#26159;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#25104;&#21151;&#30340;&#37325;&#35201;&#20915;&#23450;&#22240;&#32032;&#65292;&#20197;&#20943;&#23569;&#23545;&#20302;&#28385;&#24847;&#24230;&#29992;&#25143;&#30340;&#27969;&#22833;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#8220;&#23433;&#20840;&#8221;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#20302;&#28385;&#24847;&#24230;&#29992;&#25143;&#30340;&#25512;&#33616;&#36136;&#37327;&#65292;&#32780;&#19981;&#26159;&#20851;&#27880;&#24179;&#22343;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26368;&#23567;&#21270;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65288;CVaR&#65289;&#65292;&#34920;&#31034;&#29992;&#25143;&#25439;&#22833;&#23614;&#37096;&#30340;&#24179;&#22343;&#39118;&#38505;&#12290;&#20026;&#20102;&#20811;&#26381;&#32593;&#32476;&#35268;&#27169;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#35745;&#31639;&#38590;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#32780;&#23454;&#29992;&#30340;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#26368;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#38544;&#24335;&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;iALS&#65289;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#32463;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#23614;&#37096;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Excellent tail performance is crucial for modern machine learning tasks, such as algorithmic fairness, class imbalance, and risk-sensitive decision making, as it ensures the effective handling of challenging samples within a dataset. Tail performance is also a vital determinant of success for personalised recommender systems to reduce the risk of losing users with low satisfaction. This study introduces a "safe" collaborative filtering method that prioritises recommendation quality for less-satisfied users rather than focusing on the average performance. Our approach minimises the conditional value at risk (CVaR), which represents the average risk over the tails of users' loss. To overcome computational challenges for web-scale recommender systems, we develop a robust yet practical algorithm that extends the most scalable method, implicit alternating least squares (iALS). Empirical evaluation on real-world datasets demonstrates the excellent tail performance of our approach while maint
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#27627;&#31859;&#27874;&#38647;&#36798;&#20256;&#24863;&#22120;&#20174;&#21496;&#26426;&#30340;&#22836;&#37096;&#21160;&#20316;&#20013;&#25910;&#38598;&#20449;&#21495;&#65292;&#24182;&#22522;&#20110;&#19968;&#27425;&#23398;&#20064;&#25216;&#26415;&#35774;&#35745;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#21496;&#26426;&#30340;&#19981;&#21516;&#21160;&#20316;&#31867;&#22411;&#65292;&#23454;&#39564;&#20934;&#30830;&#29575;&#36229;&#36807;95%&#12290;</title><link>http://arxiv.org/abs/2306.05291</link><description>&lt;p&gt;
&#22522;&#20110;&#27627;&#31859;&#27874;&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#19968;&#27425;&#23398;&#20064;&#24335;&#21496;&#26426;&#22836;&#37096;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One shot learning based drivers head movement identification using a millimetre wave radar sensor. (arXiv:2306.05291v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05291
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#27627;&#31859;&#27874;&#38647;&#36798;&#20256;&#24863;&#22120;&#20174;&#21496;&#26426;&#30340;&#22836;&#37096;&#21160;&#20316;&#20013;&#25910;&#38598;&#20449;&#21495;&#65292;&#24182;&#22522;&#20110;&#19968;&#27425;&#23398;&#20064;&#25216;&#26415;&#35774;&#35745;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#21496;&#26426;&#30340;&#19981;&#21516;&#21160;&#20316;&#31867;&#22411;&#65292;&#23454;&#39564;&#20934;&#30830;&#29575;&#36229;&#36807;95%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#28966;&#20110;&#20132;&#36890;&#23433;&#20840;&#65292;&#30417;&#27979;&#21496;&#26426;&#26159;&#21542;&#19987;&#27880;&#20110;&#39550;&#39542;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#22522;&#20110;&#27627;&#31859;&#27874;&#38647;&#36798;&#24212;&#29992;&#65292;&#21033;&#29992;&#23433;&#35013;&#22312;&#36710;&#36742;&#26041;&#21521;&#30424;&#22788;&#30340;&#23567;&#22411;&#27627;&#31859;&#27874;&#38647;&#36798;&#20174;&#21496;&#26426;&#19981;&#21516;&#30340;&#22836;&#37096;&#21160;&#20316;&#25910;&#38598;&#20449;&#21495;&#65292;&#20877;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#20998;&#31867;&#22120;&#26469;&#21306;&#20998;&#19981;&#21516;&#30340;&#21160;&#20316;&#31867;&#22411;&#12290;&#37492;&#20110;&#25968;&#25454;&#38598;&#36739;&#23567;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#19968;&#27425;&#23398;&#20064;&#26041;&#27861;&#20998;&#31867;&#21496;&#26426;&#30340;&#22235;&#31181;&#22836;&#37096;&#21160;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;95%&#12290;
&lt;/p&gt;
&lt;p&gt;
Concentration of drivers on traffic is a vital safety issue; thus, monitoring a driver being on road becomes an essential requirement. The key purpose of supervision is to detect abnormal behaviours of the driver and promptly send warnings to him her for avoiding incidents related to traffic accidents. In this paper, to meet the requirement, based on radar sensors applications, the authors first use a small sized millimetre wave radar installed at the steering wheel of the vehicle to collect signals from different head movements of the driver. The received signals consist of the reflection patterns that change in response to the head movements of the driver. Then, in order to distinguish these different movements, a classifier based on the measured signal of the radar sensor is designed. However, since the collected data set is not large, in this paper, the authors propose One shot learning to classify four cases of driver's head movements. The experimental results indicate that the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#34880;&#27969;&#21160;&#21147;&#23398;&#25968;&#25454;&#24320;&#21457;&#20102;&#23454;&#26102;&#35786;&#26029;&#21644;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#35786;&#26029;&#20013;&#39118;&#20122;&#22411;&#65292;&#39044;&#27979;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#21644;&#20013;&#39118;&#22797;&#21457;&#12290;</title><link>http://arxiv.org/abs/2306.05289</link><description>&lt;p&gt;
&#20174;&#34880;&#27969;&#21160;&#21147;&#23398;&#20449;&#21495;&#30417;&#27979;&#20013;&#39044;&#27979;&#21644;&#35786;&#26029;&#20013;&#39118;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predictive and diagnosis models of stroke from hemodynamic signal monitoring. (arXiv:2306.05289v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#34880;&#27969;&#21160;&#21147;&#23398;&#25968;&#25454;&#24320;&#21457;&#20102;&#23454;&#26102;&#35786;&#26029;&#21644;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#35786;&#26029;&#20013;&#39118;&#20122;&#22411;&#65292;&#39044;&#27979;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#21644;&#20013;&#39118;&#22797;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#26377;&#21069;&#26223;&#30340;&#24613;&#24615;&#20013;&#39118;&#20020;&#24202;&#31649;&#29702;&#26041;&#27861;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#21151;&#24320;&#21457;&#20102;&#20174;&#34880;&#27969;&#21160;&#21147;&#23398;&#25968;&#25454;&#20013;&#23454;&#26102;&#35786;&#26029;&#21644;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22312;30&#20998;&#38047;&#20869;&#35786;&#26029;&#20013;&#39118;&#20122;&#22411;&#65292;&#22312;&#30417;&#27979;&#30340;&#21069;3&#23567;&#26102;&#20869;&#39044;&#27979;&#27515;&#20129;&#29575;&#65292;&#24182;&#19988;&#22312;&#20165;15&#20998;&#38047;&#30340;&#30417;&#27979;&#20869;&#39044;&#27979;&#20013;&#39118;&#22797;&#21457;&#12290;&#37027;&#20123;&#38590;&#20197;&#33719;&#21462;CT&#25195;&#25551;&#30340;&#24739;&#32773;&#65292;&#20197;&#21450;&#25269;&#36798;&#19987;&#31185;&#21307;&#38498;&#20013;&#39118;&#21333;&#20803;&#30340;&#25152;&#26377;&#24739;&#32773;&#37117;&#23558;&#20174;&#36825;&#20123;&#31215;&#26497;&#30340;&#32467;&#26524;&#20013;&#21463;&#30410;&#12290;&#23454;&#26102;&#24320;&#21457;&#30340;&#27169;&#22411;&#24471;&#21040;&#30340;&#32467;&#26524;&#22914;&#19979;&#65306;&#20013;&#39118;&#35786;&#26029;&#20934;&#30830;&#29575;&#32422;&#20026;$98\%$&#65288;$97.8\%$&#25935;&#24863;&#24615;&#65292;$99.5\%$&#29305;&#24322;&#24615;&#65289;&#65292;&#39044;&#27979;&#27515;&#20129;&#29575;&#30340;&#31934;&#24230;&#20026;$99.8\%$&#65288;$99.8\%$&#25935;&#24863;&#24615;&#65292;$99.9\%$&#29305;&#24322;&#24615;&#65289;&#65292;&#39044;&#27979;&#20013;&#39118;&#22797;&#21457;&#30340;&#31934;&#30830;&#24230;&#20026;$98\%$&#65288;$98\%$&#25935;&#24863;&#24615;&#65292;$99\%$&#29305;&#24322;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a novel and promising approach to the clinical management of acute stroke. Using machine learning techniques, our research has succeeded in developing accurate diagnosis and prediction real-time models from hemodynamic data. These models are able to diagnose stroke subtype with 30 minutes of monitoring, to predict the exitus during the first 3 hours of monitoring, and to predict the stroke recurrence in just 15 minutes of monitoring. Patients with difficult access to a \acrshort{CT} scan, and all patients that arrive at the stroke unit of a specialized hospital will benefit from these positive results. The results obtained from the real-time developed models are the following: stroke diagnosis around $98\%$ precision ($97.8\%$ Sensitivity, $99.5\%$ Specificity), exitus prediction with $99.8\%$ precision ($99.8\%$ Sens., $99.9\%$ Spec.) and $98\%$ precision predicting stroke recurrence ($98\%$ Sens., $99\%$ Spec.).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#26102;&#31354;&#22270;&#24418;&#27880;&#24847;&#21147;&#32593;&#32476;&#26694;&#26550;JGAT&#65292;&#38598;&#25104;&#20102;&#26469;&#33258;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#21644;&#25193;&#25955;&#21152;&#26435;&#25104;&#20687;(DWI)&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#20445;&#30041;&#21160;&#24577;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#22823;&#33041;&#35299;&#30721;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.05286</link><description>&lt;p&gt;
JGAT: &#19968;&#31181;&#29992;&#20110;&#22823;&#33041;&#35299;&#30721;&#30340;&#32852;&#21512;&#26102;&#31354;&#22270;&#24418;&#27880;&#24847;&#21147;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
JGAT: a joint spatio-temporal graph attention model for brain decoding. (arXiv:2306.05286v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#26102;&#31354;&#22270;&#24418;&#27880;&#24847;&#21147;&#32593;&#32476;&#26694;&#26550;JGAT&#65292;&#38598;&#25104;&#20102;&#26469;&#33258;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#21644;&#25193;&#25955;&#21152;&#26435;&#25104;&#20687;(DWI)&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#20445;&#30041;&#21160;&#24577;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#22823;&#33041;&#35299;&#30721;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#30721;&#19968;&#30452;&#26159;&#31070;&#32463;&#31185;&#23398;&#20013;&#20196;&#20154;&#30528;&#36855;&#30340;&#35838;&#39064;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#19981;&#21516;&#31867;&#22411;&#30340;&#33041;&#37096;&#30142;&#30149;&#21644;&#35748;&#30693;&#21050;&#28608;&#12290;&#32467;&#21512;&#26469;&#33258;&#22810;&#27169;&#24577;&#25104;&#20687;&#25216;&#26415;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#36830;&#25509;&#65288;&#20363;&#22914;&#21151;&#33021;&#36830;&#25509;&#21644;&#32467;&#26500;&#36830;&#25509;&#65289;&#21487;&#20197;&#32771;&#34385;&#23427;&#20204;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#22240;&#27492;&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#30721;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25972;&#21512;&#21151;&#33021;&#36830;&#25509;&#21644;&#32467;&#26500;&#36830;&#25509;&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#21160;&#24577;&#21464;&#21270;&#65292;&#36825;&#24456;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#27010;&#25324;&#22823;&#33041;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#26680;&#22270;&#24418;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;JGAT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#22270;&#24418;&#27880;&#24847;&#21147;&#32593;&#32476;&#26694;&#26550;&#65292;&#21516;&#26102;&#20445;&#30041;&#21160;&#24577;&#20449;&#24687;&#65292;&#38598;&#25104;&#20102;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#21644;&#25193;&#25955;&#21152;&#26435;&#25104;&#20687;&#65288;DWI&#65289;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;JGAT&#22312;&#22235;&#20010;&#29420;&#31435;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#33041;&#35299;&#30721;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The decoding of brain neural networks has been an intriguing topic in neuroscience for a well-rounded understanding of different types of brain disorders and cognitive stimuli. Integrating different types of connectivity, e.g., Functional Connectivity (FC) and Structural Connectivity (SC), from multi-modal imaging techniques can take their complementary information into account and therefore have the potential to get better decoding capability. However, traditional approaches for integrating FC and SC overlook the dynamical variations, which stand a great chance to over-generalize the brain neural network. In this paper, we propose a Joint kernel Graph Attention Network (JGAT), which is a new multi-modal temporal graph attention network framework. It integrates the data from functional Magnetic Resonance Images (fMRI) and Diffusion Weighted Imaging (DWI) while preserving the dynamic information at the same time. We conduct brain-decoding tasks with our JGAT on four independent datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30495;&#23454;&#19990;&#30028;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#27880;&#37322;&#22256;&#38590;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05285</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Statistical Feature-Guided Diffusion Model for Sensor-based Human Activity Recognition. (arXiv:2306.05285v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30495;&#23454;&#19990;&#30028;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#27880;&#37322;&#22256;&#38590;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20294;&#33719;&#24471;&#22810;&#26679;&#21270;&#21644;&#26631;&#35760;&#20256;&#24863;&#22120;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#25104;&#26412;&#39640;&#26114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#26631;&#35760;&#25968;&#25454;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#19982;&#30495;&#23454;&#19990;&#30028;&#20256;&#24863;&#22120;&#25968;&#25454;&#30456;&#20851;&#30340;&#31232;&#32570;&#24615;&#21644;&#27880;&#37322;&#22256;&#38590;&#24615;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26465;&#20214;&#35774;&#32622;&#20026;&#32479;&#35745;&#20449;&#24687;&#65288;&#20363;&#22914;&#24179;&#22343;&#20540;&#12289;&#26631;&#20934;&#20559;&#24046;&#12289;Z&#24471;&#20998;&#21644;&#20559;&#24230;&#65289;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#22810;&#26679;&#21270;&#21644;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#36807;&#37319;&#26679;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing human activities from sensor data is a vital task in various domains, but obtaining diverse and labeled sensor data remains challenging and costly. In this paper, we propose an unsupervised statistical feature-guided diffusion model for sensor-based human activity recognition. The proposed method aims to generate synthetic time-series sensor data without relying on labeled data, addressing the scarcity and annotation difficulties associated with real-world sensor data. By conditioning the diffusion model on statistical information such as mean, standard deviation, Z-score, and skewness, we generate diverse and representative synthetic sensor data. We conducted experiments on public human activity recognition datasets and compared the proposed method to conventional oversampling methods and state-of-the-art generative adversarial network methods. The experimental results demonstrate that the proposed method can improve the performance of human activity recognition and outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MusicGen&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05284</link><description>&lt;p&gt;
&#31616;&#21333;&#19988;&#21487;&#25511;&#30340;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Simple and Controllable Music Generation. (arXiv:2306.05284v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MusicGen&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#26465;&#20214;&#38899;&#20048;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MusicGen&#65292;&#23427;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#25805;&#20316;&#22810;&#20010;&#21387;&#32553;&#31163;&#25955;&#38899;&#20048;&#34920;&#31034;&#27969;&#65292;&#21363;&#20196;&#29260;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;MusicGen&#30001;&#19968;&#20010;&#21333;&#19968;&#38454;&#27573;&#30340;Transformer LM&#21644;&#39640;&#25928;&#30340;&#20196;&#29260;&#20132;&#38169;&#27169;&#24335;&#32452;&#25104;&#65292;&#28040;&#38500;&#20102;&#32423;&#32852;&#22810;&#20010;&#27169;&#22411;&#30340;&#38656;&#35201;&#65292;&#20363;&#22914;&#20998;&#23618;&#25110;&#19978;&#37319;&#26679;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MusicGen&#22914;&#20309;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#30340;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#33258;&#21160;&#21644;&#20154;&#20026;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#26631;&#20934;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#35780;&#20272;&#30340;&#22522;&#32447;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;MusicGen&#25152;&#21253;&#21547;&#32452;&#20214;&#30340;&#37325;&#35201;&#24615;&#12290;&#38899;&#20048;&#26679;&#26412;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312;https://github.com/fac&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/fac
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#38899;&#33258;&#30456;&#20284;&#21644;&#22797;&#26434;&#29305;&#24615;&#30340;&#21028;&#21035;&#24615;&#22810;&#23610;&#24230;&#29305;&#24449;&#38598;&#65292;&#20197;&#23567;&#27874;&#22495;&#20013;&#30340;&#24515;&#38899;&#20449;&#21495;&#20026;&#22522;&#30784;&#12290;&#24403;&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#24320;&#30340;&#24515;&#38899;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#23567;&#27874;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#29616;&#20986;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05283</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30456;&#20284;&#29305;&#24615;&#30340;&#24515;&#33039;&#26434;&#38899;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Method for Detecting Murmurous Heart Sounds based on Self-similar Properties. (arXiv:2306.05283v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05283
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#38899;&#33258;&#30456;&#20284;&#21644;&#22797;&#26434;&#29305;&#24615;&#30340;&#21028;&#21035;&#24615;&#22810;&#23610;&#24230;&#29305;&#24449;&#38598;&#65292;&#20197;&#23567;&#27874;&#22495;&#20013;&#30340;&#24515;&#38899;&#20449;&#21495;&#20026;&#22522;&#30784;&#12290;&#24403;&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#24320;&#30340;&#24515;&#38899;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#23567;&#27874;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#29616;&#20986;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#26434;&#38899;&#26159;&#30001;&#34880;&#28082;&#36890;&#36807;&#24515;&#33039;&#27969;&#21160;&#26102;&#20135;&#29983;&#30340;&#19968;&#31181;&#38750;&#27491;&#24120;&#22768;&#38899;&#12290;&#23427;&#21487;&#33021;&#26159;&#20005;&#37325;&#24515;&#33039;&#30149;&#30340;&#30151;&#29366;&#65292;&#22240;&#27492;&#26816;&#27979;&#24515;&#33039;&#26434;&#38899;&#23545;&#20110;&#35782;&#21035;&#21644;&#31649;&#29702;&#24515;&#34880;&#31649;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24515;&#38899;&#33258;&#30456;&#20284;&#21644;&#22797;&#26434;&#29305;&#24615;&#30340;&#21028;&#21035;&#24615;&#22810;&#23610;&#24230;&#29305;&#24449;&#38598;&#65292;&#20197;&#23567;&#27874;&#22495;&#20013;&#30340;&#24515;&#38899;&#20449;&#21495;&#20026;&#22522;&#30784;&#12290;&#33258;&#30456;&#20284;&#24615;&#29305;&#24449;&#36890;&#36807;&#35780;&#20272;&#20998;&#24418;&#34892;&#20026;&#36827;&#34892;&#34920;&#24449;&#65292;&#32780;&#22797;&#26434;&#24615;&#21017;&#36890;&#36807;&#35745;&#31639;&#23567;&#27874;&#29109;&#36827;&#34892;&#25506;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#26631;&#20934;&#20998;&#31867;&#22120;&#35780;&#20272;&#20102;&#36825;&#20123;&#29305;&#24449;&#22312;&#26816;&#27979;&#24515;&#33039;&#26434;&#38899;&#26041;&#38754;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;&#24403;&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#24320;&#30340;&#24515;&#38899;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#23567;&#27874;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#29616;&#20986;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A heart murmur is an atypical sound produced by the flow of blood through the heart. It can be a sign of a serious heart condition, so detecting heart murmurs is critical for identifying and managing cardiovascular diseases. However, current methods for identifying murmurous heart sounds do not fully utilize the valuable insights that can be gained by exploring intrinsic properties of heart sound signals. To address this issue, this study proposes a new discriminatory set of multiscale features based on the self-similarity and complexity properties of heart sounds, as derived in the wavelet domain. Self-similarity is characterized by assessing fractal behaviors, while complexity is explored by calculating wavelet entropy. We evaluated the diagnostic performance of these proposed features for detecting murmurs using a set of standard classifiers. When applied to a publicly available heart sound dataset, our proposed wavelet-based multiscale features achieved comparable performance to ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;CDP&#25552;&#20986;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#32852;&#37030;&#31639;&#27861;\robin&#65292;&#22312;LDP&#19979;&#35777;&#26126;&#20102;&#23398;&#20064;&#24517;&#39035;&#25215;&#21463;&#33267;&#23569;&#19968;&#20010;&#36951;&#25022;&#33192;&#32960;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2306.05275</link><description>&lt;p&gt;
&#24102;&#26377;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Linear Contextual Bandits with User-level Differential Privacy. (arXiv:2306.05275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;CDP&#25552;&#20986;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#32852;&#37030;&#31639;&#27861;\robin&#65292;&#22312;LDP&#19979;&#35777;&#26126;&#20102;&#23398;&#20064;&#24517;&#39035;&#25215;&#21463;&#33267;&#23569;&#19968;&#20010;&#36951;&#25022;&#33192;&#32960;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#27010;&#24565;&#19979;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;DP&#30340;&#21508;&#31181;&#23450;&#20041;&#12290;&#28982;&#21518;&#22312;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#27491;&#24335;&#24341;&#20837;&#20102;&#29992;&#25143;&#32423;&#20013;&#24515;DP&#21644;&#26412;&#22320;DP&#65292;&#24182;&#30740;&#31350;&#20102;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#23398;&#20064;&#36951;&#25022;&#21644;&#30456;&#24212;DP&#20445;&#35777;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#23545;&#20110;CDP&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;\robin&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25512;&#23548;&#22312;&#28385;&#36275;&#29992;&#25143;&#32423;DP&#26102;&#30340;&#20960;&#20046;&#21305;&#37197;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#36951;&#25022;&#30028;&#65292;&#35777;&#26126;&#20854;&#22312;&#23458;&#25143;&#31471;&#25968;&#37327;$M$&#21644;&#38544;&#31169;&#39044;&#31639;$\varepsilon$&#26041;&#38754;&#26159;&#36817;&#20046;&#26368;&#20248;&#30340;&#12290;&#23545;&#20110;LDP&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20960;&#20010;&#19979;&#30028;&#65292;&#34920;&#26126;&#22312;&#29992;&#25143;&#32423;$(\varepsilon,\delta)$-LDP&#19979;&#23398;&#20064;&#24517;&#39035;&#33267;&#23569;&#25215;&#21463;&#19968;&#20010;&#36951;&#25022;&#33192;&#32960;&#22240;&#23376;&#33267;&#23569;&#20026;{$\min\{1/\varepsilon,M\}$&#25110;$\min\{1/\sqrt{\varepsilon},\sq
&lt;/p&gt;
&lt;p&gt;
This paper studies federated linear contextual bandits under the notion of user-level differential privacy (DP). We first introduce a unified federated bandits framework that can accommodate various definitions of DP in the sequential decision-making setting. We then formally introduce user-level central DP (CDP) and local DP (LDP) in the federated bandits framework, and investigate the fundamental trade-offs between the learning regrets and the corresponding DP guarantees in a federated linear contextual bandits model. For CDP, we propose a federated algorithm termed as \robin and show that it is near-optimal in terms of the number of clients $M$ and the privacy budget $\varepsilon$ by deriving nearly-matching upper and lower regret bounds when user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating that learning under user-level $(\varepsilon,\delta)$-LDP must suffer a regret blow-up factor at least {$\min\{1/\varepsilon,M\}$ or $\min\{1/\sqrt{\varepsilon},\sq
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#21644; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#26631;&#35760;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05272</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36895;&#29575;&#38477;&#20302;&#21407;&#21017;&#36827;&#34892;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models. (arXiv:2306.05272v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#21644; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#33258;&#26631;&#35760;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#24050;&#32463;&#22312;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#24102;&#26469;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#20294;&#26159;&#32858;&#31867;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#20316;&#20026;&#19968;&#31181;&#22522;&#26412;&#21644;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20173;&#28982;&#32570;&#20047;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#27969;&#31243;&#65292;&#21033;&#29992; CLIP &#31561;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#35268;&#27169;&#19978;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#29305;&#24449;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#36895;&#29575;&#38477;&#20302;&#30446;&#26631;&#65292;&#26356;&#20855;&#26377;&#32467;&#26500;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#20174; ImageNet-1k &#30340; 57&#65285;&#25552;&#39640;&#21040; 66&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#32465;&#23450;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#22914;&#20309;&#23548;&#33268;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#26631;&#35760;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#26410;&#26631;&#35760;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914; MS-COCO &#21644; LAION-Aesthetics&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks effective solution, particularly for large-scale datasets. In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective. The resulting features may significantly improve the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore, by leveraging CLIP's image-text binding, we show how the new clustering method leads to a simple yet effective self-labeling algorithm that successfully works on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FactorCL&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#32771;&#34385;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#65292;&#36824;&#33021;&#25429;&#25417;&#36328;&#27169;&#24577;&#21807;&#19968;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05268</link><description>&lt;p&gt;
&#20998;&#35299;&#23545;&#27604;&#23398;&#20064;&#65306;&#36229;&#36234;&#22810;&#35270;&#35282;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FactorCL&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#32771;&#34385;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#65292;&#36824;&#33021;&#25429;&#25417;&#36328;&#27169;&#24577;&#21807;&#19968;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#29305;&#21035;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#20855;&#26377;&#20016;&#23500;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#34920;&#31034;&#65292;&#21482;&#38656;&#37197;&#23545;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#26631;&#39064;&#25110;&#35270;&#39057;&#38899;&#39057;&#23545;&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#30784;&#26159;&#22810;&#35270;&#35282;&#20887;&#20313;&#30340;&#20551;&#35774;&#8212;&#8212;&#36328;&#27169;&#24577;&#38388;&#20849;&#20139;&#20449;&#24687;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#26159;&#24517;&#35201;&#19988;&#36275;&#22815;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#20063;&#21253;&#21547;&#22312;&#36328;&#27169;&#24577;&#21807;&#19968;&#21306;&#22495;&#20013;&#65306;&#19968;&#31181;&#20165;&#23384;&#22312;&#20110;&#19968;&#20010;&#27169;&#24577;&#20013;&#20294;&#19982;&#20219;&#21153;&#20173;&#28982;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#22914;&#20309;&#23398;&#20064;&#33258;&#25105;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#20197;&#25429;&#33719;&#19982;&#19979;&#28216;&#20219;&#21153;&#30456;&#20851;&#30340;&#20849;&#20139;&#21644;&#21807;&#19968;&#20449;&#24687;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;FactorCL&#65292;&#20197;&#36229;&#36234;&#22810;&#35270;&#35282;&#20887;&#20313;&#12290;FactorCL&#30340;&#22522;&#30784;&#26159;&#19977;&#20010;&#26032;&#30340;&#36129;&#29486;&#65306;&#65288;1&#65289;&#23558;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#20998;&#35299;&#20026;&#20849;&#20139;&#21644;&#21807;&#19968;&#34920;&#31034;&#65292;&#65288;2&#65289;&#38480;&#21046;&#20849;&#20139;&#21644;&#21807;&#19968;&#25104;&#20998;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#65288;3&#65289;&#20351;&#29992;&#22240;&#23376;&#27491;&#21017;&#21270;&#20419;&#36827;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) cap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#34920;&#36798;&#24418;&#24335;&#65292;&#29992;&#20110;&#25551;&#36848;&#20855;&#26377;&#26230;&#20307;&#23545;&#31216;&#24615;&#30340;&#20989;&#25968;&#12290;&#32447;&#24615;&#34920;&#36798;&#24418;&#24335;&#25552;&#20379;&#20102;&#26230;&#20307;&#23545;&#31216;&#24615;&#19979;&#30340;&#22522;&#20989;&#25968;&#65292;&#32780;&#38750;&#32447;&#24615;&#34920;&#36798;&#23558;&#36712;&#36947;&#31354;&#38388;&#23884;&#20837;&#21040;&#26377;&#38480;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36827;&#34892;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2306.05261</link><description>&lt;p&gt;
&#25551;&#36848;&#21644;&#23398;&#20064;&#26230;&#20307;&#23545;&#31216;&#32676;&#19979;&#19981;&#21464;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Representing and Learning Functions Invariant Under Crystallographic Groups. (arXiv:2306.05261v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#34920;&#36798;&#24418;&#24335;&#65292;&#29992;&#20110;&#25551;&#36848;&#20855;&#26377;&#26230;&#20307;&#23545;&#31216;&#24615;&#30340;&#20989;&#25968;&#12290;&#32447;&#24615;&#34920;&#36798;&#24418;&#24335;&#25552;&#20379;&#20102;&#26230;&#20307;&#23545;&#31216;&#24615;&#19979;&#30340;&#22522;&#20989;&#25968;&#65292;&#32780;&#38750;&#32447;&#24615;&#34920;&#36798;&#23558;&#36712;&#36947;&#31354;&#38388;&#23884;&#20837;&#21040;&#26377;&#38480;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36827;&#34892;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#20307;&#23545;&#31216;&#32676;&#25551;&#36848;&#20102;&#33258;&#28982;&#21644;&#31185;&#23398;&#20013;&#36935;&#21040;&#30340;&#26230;&#20307;&#21644;&#20854;&#20182;&#37325;&#22797;&#32467;&#26500;&#30340;&#23545;&#31216;&#24615;&#12290;&#26412;&#25991;&#25512;&#23548;&#20986;&#20989;&#25968;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#34920;&#31034;&#24418;&#24335;&#65292;&#36825;&#20123;&#20989;&#25968;&#21516;&#26102;&#20855;&#26377;&#65288;1&#65289;&#24179;&#28369;&#21644;&#65288;2&#65289;&#22312;&#27492;&#31867;&#32676;&#19979;&#19981;&#21464;&#30340;&#29305;&#24615;&#12290;&#32447;&#24615;&#34920;&#31034;&#23558;&#20613;&#37324;&#21494;&#22522;&#24191;&#20041;&#21270;&#20026;&#20855;&#26377;&#26230;&#20307;&#23545;&#31216;&#24615;&#30340;&#22522;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#22522;&#20989;&#25968;&#23545;&#20110;&#27599;&#20010;&#26230;&#20307;&#23545;&#31216;&#32676;&#37117;&#23384;&#22312;&#65292;&#23545;&#24212;$L_2$&#31354;&#38388;&#20855;&#26377;&#27491;&#20132;&#24615;&#65292;&#22312;&#32431;&#24179;&#31227;&#32676;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#24674;&#22797;&#20026;&#26631;&#20934;&#20613;&#37324;&#21494;&#22522;&#12290;&#38750;&#32447;&#24615;&#34920;&#31034;&#23558;&#35813;&#32676;&#30340;&#36712;&#36947;&#31354;&#38388;&#23884;&#20837;&#21040;&#26377;&#38480;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#26230;&#20307;&#23545;&#31216;&#32676;&#37117;&#23384;&#22312;&#36825;&#26679;&#30340;&#23884;&#20837;&#65292;&#32780;&#19988;&#36825;&#20010;&#23884;&#20837;&#36890;&#36807;&#19968;&#20010;&#31216;&#20026;&#36712;&#31215;&#27969;&#24418;&#30340;&#27969;&#24418;&#30340;&#24191;&#20041;&#21270;&#20989;&#25968;&#22240;&#23376;&#21270;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#20123;&#31639;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#35813;&#32676;&#30340;&#26631;&#20934;&#25551;&#36848;&#35745;&#31639;&#20986;&#20613;&#37324;&#21494;&#22522;&#21644;&#23884;&#20837;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crystallographic groups describe the symmetries of crystals and other repetitive structures encountered in nature and the sciences. These groups include the wallpaper and space groups. We derive linear and nonlinear representations of functions that are (1) smooth and (2) invariant under such a group. The linear representation generalizes the Fourier basis to crystallographically invariant basis functions. We show that such a basis exists for each crystallographic group, that it is orthonormal in the relevant $L_2$ space, and recover the standard Fourier basis as a special case for pure shift groups. The nonlinear representation embeds the orbit space of the group into a finite-dimensional Euclidean space. We show that such an embedding exists for every crystallographic group, and that it factors functions through a generalization of a manifold called an orbifold. We describe algorithms that, given a standardized description of the group, compute the Fourier basis and an embedding map.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#23398;&#20064;&#22312;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#24635;&#32467;&#20102;&#22522;&#20110;&#21270;&#23398;&#32467;&#26500;&#12289;&#22522;&#20110;&#32593;&#32476;&#12289;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2306.05257</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#23398;&#20064;&#22312;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive evaluation of deep and graph learning on drug-drug interactions prediction. (arXiv:2306.05257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#23398;&#20064;&#22312;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#24635;&#32467;&#20102;&#22522;&#20110;&#21270;&#23398;&#32467;&#26500;&#12289;&#22522;&#20110;&#32593;&#32476;&#12289;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21450;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25104;&#23601;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;(DDIs)&#30456;&#20851;&#30340;&#24212;&#29992;&#12290;DDIs&#26159;&#25351;&#22312;&#20154;&#20307;&#20013;&#23384;&#22312;&#21478;&#19968;&#20010;&#33647;&#29289;&#26102;&#65292;&#23545;&#19968;&#31181;&#33647;&#29289;&#20316;&#29992;&#30340;&#25913;&#21464;&#65292;&#23427;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#20020;&#24202;&#30740;&#31350;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#20256;&#32479;&#30340;&#20020;&#24202;&#35797;&#39564;&#21644;&#23454;&#39564;&#26469;&#39044;&#27979;DDIs&#26159;&#19968;&#39033;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#27491;&#30830;&#24212;&#29992;&#20808;&#36827;&#30340;AI&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#21644;&#32534;&#30721;&#20197;&#21450;&#35745;&#31639;&#26041;&#27861;&#30340;&#35774;&#35745;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#21270;&#23398;&#32467;&#26500;&#12289;&#22522;&#20110;&#32593;&#32476;&#12289;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#20026;&#20855;&#26377;&#19981;&#21516;&#39046;&#22495;&#30693;&#35782;&#30340;&#24191;&#27867;&#30740;&#31350;&#21644;&#21457;&#23637;&#31038;&#21306;&#25552;&#20379;&#20102;&#26368;&#26032;&#21644;&#26131;&#20110;&#35775;&#38382;&#30340;&#25351;&#21335;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances and achievements of artificial intelligence (AI) as well as deep and graph learning models have established their usefulness in biomedical applications, especially in drug-drug interactions (DDIs). DDIs refer to a change in the effect of one drug to the presence of another drug in the human body, which plays an essential role in drug discovery and clinical research. DDIs prediction through traditional clinical trials and experiments is an expensive and time-consuming process. To correctly apply the advanced AI and deep learning, the developer and user meet various challenges such as the availability and encoding of data resources, and the design of computational methods. This review summarizes chemical structure based, network based, NLP based and hybrid methods, providing an updated and accessible guide to the broad researchers and development community with different domain knowledge. We introduce widely-used molecular representation and describe the theoretical frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21517;&#20026;&#38750;&#32447;&#24615;&#26080;&#25439;&#33258;&#32534;&#30721;&#22120;&#65288;UAE&#65289;&#65292;&#23427;&#20351;&#29992;&#30830;&#23450;&#24615;&#37319;&#26679;&#30340;&#26377;&#38480;&#19968;&#32452;&#32479;&#35745;&#37327;&#26469;&#25552;&#39640;&#21518;&#39564;&#34920;&#31034;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20351;&#29992;Wasserstein&#20998;&#24067;&#24230;&#37327;&#26367;&#25442;KL&#25955;&#24230;&#65292;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#21518;&#39564;&#12290;</title><link>http://arxiv.org/abs/2306.05256</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#26080;&#25439;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Unscented Autoencoder. (arXiv:2306.05256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21517;&#20026;&#38750;&#32447;&#24615;&#26080;&#25439;&#33258;&#32534;&#30721;&#22120;&#65288;UAE&#65289;&#65292;&#23427;&#20351;&#29992;&#30830;&#23450;&#24615;&#37319;&#26679;&#30340;&#26377;&#38480;&#19968;&#32452;&#32479;&#35745;&#37327;&#26469;&#25552;&#39640;&#21518;&#39564;&#34920;&#31034;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20351;&#29992;Wasserstein&#20998;&#24067;&#24230;&#37327;&#26367;&#25442;KL&#25955;&#24230;&#65292;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#21518;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26159;&#20855;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24320;&#21019;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#20854;&#37325;&#24314;&#36807;&#31243;&#35299;&#37322;&#20026;&#26469;&#33258;&#28508;&#22312;&#21518;&#39564;&#20998;&#24067;&#30340;&#26679;&#26412;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#24182;&#24212;&#29992;&#26080;&#25439;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;UKF&#65289;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#20998;&#24067;&#36817;&#20284;&#8212;&#8212;&#26080;&#25439;&#21464;&#25442;&#65288;UT&#65289;&#12290;&#30830;&#23450;&#24615;&#37319;&#26679;&#30340;&#26377;&#38480;&#19968;&#32452;&#31216;&#20026;sigma&#28857;&#30340;&#32479;&#35745;&#37327;&#25552;&#20379;&#27604;&#37325;&#21442;&#25968;&#25216;&#24039;&#30340;&#26222;&#36941;&#22122;&#22768;&#32553;&#25918;&#26356;&#20855;&#20449;&#24687;&#37327;&#19988;&#26041;&#24046;&#26356;&#23567;&#30340;&#21518;&#39564;&#34920;&#31034;&#65292;&#21516;&#26102;&#30830;&#20445;&#26356;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23558;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#26367;&#25442;&#20026;Wasserstein&#20998;&#24067;&#24230;&#37327;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20174;&#32780;&#20801;&#35768;&#26356;&#23574;&#38160;&#30340;&#21518;&#39564;&#12290;&#21463;&#21040;&#36825;&#20004;&#20010;&#32452;&#20214;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;VAE&#65292;&#21363;&#38750;&#32447;&#24615;&#26080;&#25439;&#33258;&#32534;&#30721;&#22120;&#65288;UAE&#65289;&#65292;&#35813;&#33258;&#32534;&#30721;&#22120;&#20165;&#20351;&#29992;&#38024;&#23545;&#27599;&#20010;&#26679;&#26412;&#21518;&#39564;&#30340;&#31867;&#20284;&#27491;&#35268;&#21270;&#30340;&#39033;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Variational Autoencoder (VAE) is a seminal approach in deep generative modeling with latent variables. Interpreting its reconstruction process as a nonlinear transformation of samples from the latent posterior distribution, we apply the Unscented Transform (UT) -- a well-known distribution approximation used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite set of statistics called sigma points, sampled deterministically, provides a more informative and lower-variance posterior representation than the ubiquitous noise-scaling of the reparameterization trick, while ensuring higher-quality reconstruction. We further boost the performance by replacing the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric that allows for a sharper posterior. Inspired by the two components, we derive a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder (UAE), trained purely with regularization-like terms on the per-sample posterior
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33041;&#37096;&#21464;&#24418;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#32467;&#21512;&#20102;&#38750;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#12289;&#26356;&#36890;&#29992;&#22320;&#39044;&#27979;&#33041;&#26368;&#22823;&#20027;&#24212;&#21464;&#21644;MPS&#36895;&#29575;&#12290;&#35813;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#65292;&#23545;&#26089;&#26399;TBI&#26816;&#27979;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.05255</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26356;&#20934;&#30830;&#21644;&#36890;&#29992;&#30340;&#33041;&#37096;&#21464;&#24418;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toward more accurate and generalizable brain deformation estimators for traumatic brain injury detection with unsupervised domain adaptation. (arXiv:2306.05255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33041;&#37096;&#21464;&#24418;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#32467;&#21512;&#20102;&#38750;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#12289;&#26356;&#36890;&#29992;&#22320;&#39044;&#27979;&#33041;&#26368;&#22823;&#20027;&#24212;&#21464;&#21644;MPS&#36895;&#29575;&#12290;&#35813;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#65292;&#23545;&#26089;&#26399;TBI&#26816;&#27979;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22836;&#37096;&#27169;&#22411;&#65288;MLHMs&#65289;&#34987;&#24320;&#21457;&#29992;&#20110;&#20272;&#35745;&#33041;&#37096;&#21464;&#24418;&#65292;&#20197;&#26089;&#26399;&#26816;&#27979;&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#65288;TBI&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#27169;&#25311;&#30896;&#25758;&#30340;&#36807;&#24230;&#25311;&#21512;&#20197;&#21450;&#30001;&#19981;&#21516;&#22836;&#37096;&#30896;&#25758;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#20559;&#31227;&#24341;&#36215;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38459;&#30861;&#20102;&#24403;&#21069;MLHMs&#30340;&#24191;&#27867;&#20020;&#24202;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33041;&#37096;&#21464;&#24418;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#23558;&#38750;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#39044;&#27979;&#25972;&#20010;&#33041;&#26368;&#22823;&#20027;&#24212;&#21464;&#65288;MPS&#65289;&#21644;MPS&#36895;&#29575;&#65288;MPSR&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#27491;&#21017;&#21270;&#32452;&#20214;&#20998;&#26512;&#65288;DRCA&#65289;&#21644;&#24490;&#29615;GAN&#26041;&#27861;&#65292;&#38024;&#23545;12,780&#20010;&#27169;&#25311;&#22836;&#37096;&#25758;&#20987;&#65292;&#23545;&#26469;&#33258;302&#20010;&#22823;&#23398;&#27204;&#27012;&#29699;&#65288;CF&#65289;&#25758;&#20987;&#21644;457&#20010;&#32508;&#21512;&#27494;&#26415;&#65288;MMA&#65289;&#25758;&#20987;&#30340;&#29616;&#22330;&#22836;&#37096;&#25758;&#20987;&#36827;&#34892;&#20102;&#38750;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#12290;&#26032;&#27169;&#22411;&#25552;&#39640;&#20102;MPS / MPSR&#20272;&#35745;&#20934;&#30830;&#24615;&#65292;&#20854;&#20013;DRCA&#26041;&#27861;&#22312;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65288;p &lt;0.001&#65289;&#65306;MPS RMSE&#65306;0.027&#65288;CF&#65289;&#21644;0.037&#65288;MMA&#65289;;
&lt;/p&gt;
&lt;p&gt;
Machine learning head models (MLHMs) are developed to estimate brain deformation for early detection of traumatic brain injury (TBI). However, the overfitting to simulated impacts and the lack of generalizability caused by distributional shift of different head impact datasets hinders the broad clinical applications of current MLHMs. We propose brain deformation estimators that integrates unsupervised domain adaptation with a deep neural network to predict whole-brain maximum principal strain (MPS) and MPS rate (MPSR). With 12,780 simulated head impacts, we performed unsupervised domain adaptation on on-field head impacts from 302 college football (CF) impacts and 457 mixed martial arts (MMA) impacts using domain regularized component analysis (DRCA) and cycle-GAN-based methods. The new model improved the MPS/MPSR estimation accuracy, with the DRCA method significantly outperforming other domain adaptation methods in prediction accuracy (p&lt;0.001): MPS RMSE: 0.027 (CF) and 0.037 (MMA); 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#25991;&#26412;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#26469;&#36827;&#34892;&#28789;&#27963;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#22411;&#30340;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26469;&#23558;&#38899;&#39057;&#24207;&#21015;&#26368;&#20248;&#22320;&#20998;&#21106;&#25104;&#19982;&#22522;&#20110;&#21333;&#35789;&#30340;&#25991;&#26412;&#24207;&#21015;&#30456;&#21516;&#30340;&#38271;&#24230;&#65292;&#20197;&#23454;&#29616;&#19981;&#21516;&#38271;&#24230;&#22810;&#35789;&#20851;&#38190;&#35789;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2306.05245</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#39057;&#25991;&#26412;&#21305;&#37197;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Matching Latent Encoding for Audio-Text based Keyword Spotting. (arXiv:2306.05245v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#25991;&#26412;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#26469;&#36827;&#34892;&#28789;&#27963;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#22411;&#30340;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26469;&#23558;&#38899;&#39057;&#24207;&#21015;&#26368;&#20248;&#22320;&#20998;&#21106;&#25104;&#19982;&#22522;&#20110;&#21333;&#35789;&#30340;&#25991;&#26412;&#24207;&#21015;&#30456;&#21516;&#30340;&#38271;&#24230;&#65292;&#20197;&#23454;&#29616;&#19981;&#21516;&#38271;&#24230;&#22810;&#35789;&#20851;&#38190;&#35789;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#38899;&#39057;&#19982;&#25991;&#26412;&#23884;&#20837;&#36827;&#34892;&#20851;&#38190;&#35789;&#26816;&#27979;&#24050;&#32463;&#21462;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#22914;&#20309;&#23545;&#19981;&#21516;&#24207;&#21015;&#38271;&#24230;&#30340;&#22810;&#35789;&#20851;&#38190;&#35789;&#36827;&#34892;&#35821;&#20041;&#23545;&#40784;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#25991;&#26412;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#28789;&#27963;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#20351;&#29992;&#19968;&#31181;&#26032;&#22411;&#30340;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;Dynamic Sequence Partitioning&#65288;DSP&#65289;&#65292;&#36890;&#36807;&#21475;&#35821;&#20869;&#23481;&#30340;&#21333;&#35843;&#23545;&#40784;&#23558;&#38899;&#39057;&#24207;&#21015;&#26368;&#20248;&#22320;&#20998;&#21106;&#25104;&#19982;&#22522;&#20110;&#21333;&#35789;&#30340;&#25991;&#26412;&#24207;&#21015;&#30456;&#21516;&#30340;&#38271;&#24230;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19968;&#20010;&#32534;&#30721;&#22120;&#22359;&#26469;&#33719;&#21462;&#38899;&#39057;&#21644;&#25991;&#26412;&#23884;&#20837;&#65292;&#19968;&#20010;&#25237;&#24433;&#22120;&#22359;&#23558;&#21508;&#33258;&#30340;&#23884;&#20837;&#25237;&#24433;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21644;&#19968;&#20010;&#38899;&#39057;&#25991;&#26412;&#21305;&#37197;&#22120;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31181;&#26032;&#39062;&#30340;DSP&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#38899;&#39057;&#21644;&#25991;&#26412;&#23884;&#20837;&#23545;&#40784;&#65292;&#20197;&#30830;&#23450;&#21475;&#35821;&#20869;&#23481;&#26159;&#21542;&#19982;&#25991;&#26412;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using audio and text embeddings jointly for Keyword Spotting (KWS) has shown high-quality results, but the key challenge of how to semantically align two embeddings for multi-word keywords of different sequence lengths remains largely unsolved. In this paper, we propose an audio-text-based end-to-end model architecture for flexible keyword spotting (KWS), which builds upon learned audio and text embeddings. Our architecture uses a novel dynamic programming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimally partition the audio sequence into the same length as the word-based text sequence using the monotonic alignment of spoken content. Our proposed model consists of an encoder block to get audio and text embeddings, a projector block to project individual embeddings to a common latent space, and an audio-text aligner containing a novel DSP algorithm, which aligns the audio and text embeddings to determine if the spoken content is the same as the text. Experimental result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;GAN&#25317;&#26377;&#26435;&#20445;&#25252;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#30446;&#26631;&#27169;&#22411;&#21450;&#20854;&#30423;&#29256;&#27169;&#22411;&#20849;&#21516;&#29305;&#24449;&#65292;&#33021;&#22815;&#30452;&#25509;&#36866;&#29992;&#20110;&#25152;&#26377;&#35757;&#32451;&#33391;&#22909;&#30340;GAN&#65292;&#26368;&#32456;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#20445;&#25252;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05233</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#25317;&#26377;&#26435;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Ownership Protection of Generative Adversarial Networks. (arXiv:2306.05233v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;GAN&#25317;&#26377;&#26435;&#20445;&#25252;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#30446;&#26631;&#27169;&#22411;&#21450;&#20854;&#30423;&#29256;&#27169;&#22411;&#20849;&#21516;&#29305;&#24449;&#65292;&#33021;&#22815;&#30452;&#25509;&#36866;&#29992;&#20110;&#25152;&#26377;&#35757;&#32451;&#33391;&#22909;&#30340;GAN&#65292;&#26368;&#32456;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#20445;&#25252;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20351;&#24471;GAN&#27169;&#22411;&#26412;&#36523;&#23545;&#20110;&#21512;&#27861;&#30340;&#27169;&#22411;&#25152;&#26377;&#32773;&#20855;&#26377;&#21830;&#19994;&#20215;&#20540;&#12290;&#22240;&#27492;&#65292;&#25216;&#26415;&#20445;&#25252;GAN&#30340;&#30693;&#35782;&#20135;&#26435;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#38656;&#35201;&#31713;&#25913;&#35757;&#32451;&#38598;&#25110;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#19988;&#23427;&#20204;&#23545;&#26032;&#20852;&#30340;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#30446;&#26631;&#27169;&#22411;&#21450;&#20854;&#30423;&#29256;&#27169;&#22411;&#20849;&#21516;&#29305;&#24449;&#30340;&#26032;&#22411;&#25317;&#26377;&#26435;&#20445;&#25252;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#36866;&#29992;&#20110;&#25152;&#26377;&#35757;&#32451;&#33391;&#22909;&#30340;GAN&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#20445;&#25252;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#29983;&#25104;&#27425;&#25968;&#12289;&#29983;&#25104;&#26679;&#26412;&#30340;&#25968;&#37327;&#12289;&#19981;&#21516;&#25968;&#25454;&#38598;&#20197;&#21450;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) have shown remarkable success in image synthesis, making GAN models themselves commercially valuable to legitimate model owners. Therefore, it is critical to technically protect the intellectual property of GANs. Prior works need to tamper with the training set or training process, and they are not robust to emerging model extraction attacks. In this paper, we propose a new ownership protection method based on the common characteristics of a target model and its stolen models. Our method can be directly applicable to all well-trained GANs as it does not require retraining target models. Extensive experimental results show that our new method can achieve the best protection performance, compared to the state-of-the-art methods. Finally, we demonstrate the effectiveness of our method with respect to the number of generations of model extraction attacks, the number of generated samples, different datasets, as well as adaptive attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#20248;&#21270;&#26041;&#27861;&#26469;&#23454;&#29616;&#24179;&#22374;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05225</link><description>&lt;p&gt;
&#23454;&#29616;&#24179;&#22374;&#23616;&#37096;&#26497;&#23567;&#20540;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#36716;&#31227;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Boosting Adversarial Transferability by Achieving Flat Local Maxima. (arXiv:2306.05225v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#20248;&#21270;&#26041;&#27861;&#26469;&#23454;&#29616;&#24179;&#22374;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36716;&#31227;&#30340;&#25915;&#20987;&#37319;&#29992;&#22312;&#26367;&#20195;&#27169;&#22411;&#19978;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#25915;&#20987;&#21508;&#31181;&#27169;&#22411;&#65292;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#36866;&#29992;&#24182;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#21463;&#21040;&#24179;&#22374;&#23616;&#37096;&#26497;&#23567;&#20540;&#19982;&#33391;&#22909;&#30340;&#27867;&#21270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#24182;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#65292;&#22312;&#24179;&#22374;&#23616;&#37096;&#21306;&#22495;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#20542;&#21521;&#20110;&#20855;&#26377;&#33391;&#22909;&#30340;&#36716;&#31227;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#24809;&#32602;&#26799;&#24230;&#33539;&#25968;&#12290;&#30001;&#20110;&#30452;&#25509;&#20248;&#21270;&#26799;&#24230;&#27491;&#21017;&#21270;&#33539;&#25968;&#22312;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#26102;&#35745;&#31639;&#20195;&#20215;&#26114;&#36149;&#19988;&#38590;&#20197;&#22788;&#29702;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#26356;&#26032;&#12290;&#25105;&#20204;&#38543;&#26426;&#37319;&#26679;&#19968;&#20010;&#31034;&#20363;&#65292;&#24182;&#37319;&#29992;&#19968;&#38454;&#26799;&#24230;&#26469;&#36924;&#36817;&#20108;&#38454;&#40657;&#22622;&#30697;&#38453;&#65292;&#20174;&#32780;&#20351;&#35745;&#31639;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer-based attack adopts the adversarial examples generated on the surrogate model to attack various models, making it applicable in the physical world and attracting increasing interest. Recently, various adversarial attacks have emerged to boost adversarial transferability from different perspectives. In this work, inspired by the fact that flat local minima are correlated with good generalization, we assume and empirically validate that adversarial examples at a flat local region tend to have good transferability by introducing a penalized gradient norm to the original loss function. Since directly optimizing the gradient regularization norm is computationally expensive and intractable for generating adversarial examples, we propose an approximation optimization method to simplify the gradient update of the objective function. Specifically, we randomly sample an example and adopt the first-order gradient to approximate the second-order Hessian matrix, which makes computing more 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#36896;&#20219;&#20309;&#32447;&#24615;&#38408;&#20540;&#20989;&#25968;&#30340;&#26377;&#24207;BDD&#65292;&#24182;&#21487;&#20197;&#22312;O(n2^{n})&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20869;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24050;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.05211</link><description>&lt;p&gt;
&#22522;&#20110;Boosting&#30340;&#32447;&#24615;&#38408;&#20540;&#20989;&#25968;BDD&#26500;&#36896;&#21450;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Boosting-based Construction of BDDs for Linear Threshold Functions and Its Application to Verification of Neural Networks. (arXiv:2306.05211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#36896;&#20219;&#20309;&#32447;&#24615;&#38408;&#20540;&#20989;&#25968;&#30340;&#26377;&#24207;BDD&#65292;&#24182;&#21487;&#20197;&#22312;O(n2^{n})&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20869;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24050;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#34892;&#20026;&#65292;&#29702;&#35299;&#20854;&#29305;&#24449;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#20294;&#20063;&#24456;&#22256;&#38590;&#12290;&#20808;&#21069;&#30340;&#19968;&#20123;&#24037;&#20316;&#25552;&#20986;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#31561;&#20215;&#30340;&#24067;&#23572;&#34920;&#36798;&#24335;&#65292;&#24182;&#24212;&#29992;&#39564;&#35777;&#25216;&#26415;&#26469;&#30740;&#31350;&#24863;&#20852;&#36259;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#35813;&#36716;&#25442;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#26159;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#36896;&#20219;&#20309;&#32447;&#24615;&#38408;&#20540;&#20989;&#25968;&#30340;&#26377;&#24207;BDD&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;O(n2^{n})&#20869;&#23454;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the characteristics of neural networks is important but difficult due to their complex structures and behaviors. Some previous work proposes to transform neural networks into equivalent Boolean expressions and apply verification techniques for characteristics of interest. This approach is promising since rich results of verification techniques for circuits and other Boolean expressions can be readily applied. The bottleneck is the time complexity of the transformation. More precisely, (i) each neuron of the network, i.e., a linear threshold function, is converted to a Binary Decision Diagram (BDD), and (ii) they are further combined into some final form, such as Boolean circuits. For a linear threshold function with $n$ variables, an existing method takes $O(n2^{\frac{n}{2}})$ time to construct an ordered BDD of size $O(2^{\frac{n}{2}})$ consistent with some variable ordering. However, it is non-trivial to choose a variable ordering producing a small BDD among $n!$ candid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#31532;&#19968;&#39033;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#38544;&#31169;&#30740;&#31350;&#65292;&#25915;&#20987;&#32773;&#23558;&#20174;&#27169;&#22411;&#20013;&#25552;&#21462;&#35757;&#32451;&#38598;&#30340;&#25935;&#24863;&#20840;&#23616;&#23646;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31181;&#25193;&#25955;&#27169;&#22411;&#21450;&#20854;&#21462;&#26679;&#22120;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.05208</link><description>&lt;p&gt;
PriSampler: &#32531;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#26029;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
PriSampler: Mitigating Property Inference of Diffusion Models. (arXiv:2306.05208v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#39033;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#38544;&#31169;&#30740;&#31350;&#65292;&#25915;&#20987;&#32773;&#23558;&#20174;&#27169;&#22411;&#20013;&#25552;&#21462;&#35757;&#32451;&#38598;&#30340;&#25935;&#24863;&#20840;&#23616;&#23646;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31181;&#25193;&#25955;&#27169;&#22411;&#21450;&#20854;&#21462;&#26679;&#22120;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#36825;&#20123;&#25104;&#21151;&#20063;&#20419;&#20351;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#25935;&#24863;&#25968;&#25454;&#65292;&#20363;&#22914;&#20154;&#33080;&#25968;&#25454;&#65292;&#20294;&#36825;&#21487;&#33021;&#24102;&#26469;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#31532;&#19968;&#39033;&#38544;&#31169;&#30740;&#31350;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#26088;&#22312;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#21462;&#35757;&#32451;&#38598;&#30340;&#25935;&#24863;&#20840;&#23616;&#23646;&#24615;&#65292;&#20363;&#22914;&#26576;&#20123;&#25935;&#24863;&#23646;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26368;&#23454;&#29992;&#30340;&#25915;&#20987;&#22330;&#26223;&#65306;&#25915;&#20987;&#32773;&#21482;&#33021;&#33719;&#24471;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#19979;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#21462;&#26679;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#35780;&#20272;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#33539;&#22260;&#34920;&#26126;&#65292;&#21508;&#31181;&#25193;&#25955;&#27169;&#22411;&#21450;&#20854;&#21462;&#26679;&#22120;&#37117;&#23481;&#26131;&#21463;&#21040;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23545;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#20063;&#23637;&#31034;&#20102;&#25915;&#20987;&#30340;&#23454;&#38469;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have been remarkably successful in data synthesis. Such successes have also driven diffusion models to apply to sensitive data, such as human face data, but this might bring about severe privacy concerns. In this work, we systematically present the first privacy study about property inference attacks against diffusion models, in which adversaries aim to extract sensitive global properties of the training set from a diffusion model, such as the proportion of the training data for certain sensitive properties. Specifically, we consider the most practical attack scenario: adversaries are only allowed to obtain synthetic data. Under this realistic scenario, we evaluate the property inference attacks on different types of samplers and diffusion models. A broad range of evaluations shows that various diffusion models and their samplers are all vulnerable to property inference attacks. Furthermore, one case study on off-the-shelf pre-trained diffusion models also demonstrates
&lt;/p&gt;</description></item><item><title>EMO&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#20248;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#22806;&#37096;&#23384;&#20648;&#22120;&#20013;&#35760;&#24405;&#36807;&#21435;&#20219;&#21153;&#30340;&#26799;&#24230;&#21382;&#21490;&#65292;&#23454;&#29616;&#23567;&#26679;&#26412;&#23398;&#20064;&#65292;&#26080;&#35770;&#25552;&#20379;&#30340;&#26799;&#24230;&#20449;&#24687;&#26159;&#21542;&#21487;&#38752;&#65292;&#37117;&#21487;&#20197;&#25512;&#21160;&#21442;&#25968;&#26356;&#26032;&#26397;&#30528;&#27491;&#30830;&#30340;&#26041;&#21521;&#21069;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.05189</link><description>&lt;p&gt;
EMO&#65306;&#29992;&#20110;&#23567;&#26679;&#26412;&#20803;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
EMO: Episodic Memory Optimization for Few-Shot Meta-Learning. (arXiv:2306.05189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05189
&lt;/p&gt;
&lt;p&gt;
EMO&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#20248;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#22806;&#37096;&#23384;&#20648;&#22120;&#20013;&#35760;&#24405;&#36807;&#21435;&#20219;&#21153;&#30340;&#26799;&#24230;&#21382;&#21490;&#65292;&#23454;&#29616;&#23567;&#26679;&#26412;&#23398;&#20064;&#65292;&#26080;&#35770;&#25552;&#20379;&#30340;&#26799;&#24230;&#20449;&#24687;&#26159;&#21542;&#21487;&#38752;&#65292;&#37117;&#21487;&#20197;&#25512;&#21160;&#21442;&#25968;&#26356;&#26032;&#26397;&#30528;&#27491;&#30830;&#30340;&#26041;&#21521;&#21069;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#20803;&#23398;&#20064;&#30001;&#20110;&#20219;&#21153;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#30340;&#38480;&#21046;&#23545;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#20248;&#21270;&#26041;&#26696;&#65292;&#31216;&#20026;EMO&#12290;EMO&#21463;&#21040;&#20154;&#31867;&#20174;&#33041;&#20869;&#35760;&#24518;&#20013;&#22238;&#24518;&#36807;&#21435;&#23398;&#20064;&#32463;&#39564;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#23558;&#36807;&#21435;&#20219;&#21153;&#30340;&#26799;&#24230;&#21382;&#21490;&#35760;&#24405;&#22312;&#22806;&#37096;&#23384;&#20648;&#22120;&#20013;&#65292;&#20197;&#22686;&#24378;&#35760;&#24518;&#30340;&#26041;&#24335;&#36827;&#34892;&#23567;&#26679;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#20445;&#30041;&#21644;&#22238;&#24518;&#36807;&#21435;&#35757;&#32451;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21363;&#20351;&#20165;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#31034;&#20363;&#25552;&#20379;&#20102;&#19981;&#21487;&#38752;&#30340;&#26799;&#24230;&#65292;EMO&#20063;&#21487;&#20197;&#25512;&#21160;&#21442;&#25968;&#26356;&#26032;&#26397;&#30528;&#27491;&#30830;&#30340;&#26041;&#21521;&#21069;&#36827;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#23545;&#20110;&#24179;&#28369;&#12289;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#20250;&#25910;&#25947;&#12290;EMO&#26159;&#36890;&#29992;&#30340;&#12289;&#28789;&#27963;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20248;&#21270;&#22120;&#65292;&#21487;&#26080;&#32541;&#23884;&#20837;&#29616;&#26377;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#23567;&#26679;&#26412;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;EMO&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot meta-learning presents a challenge for gradient descent optimization due to the limited number of training samples per task. To address this issue, we propose an episodic memory optimization for meta-learning, we call \emph{EMO}, which is inspired by the human ability to recall past learning experiences from the brain's memory. EMO retains the gradient history of past experienced tasks in external memory, enabling few-shot learning in a memory-augmented way. By learning to retain and recall the learning process of past training tasks, EMO nudges parameter updates in the right direction, even when the gradients provided by a limited number of examples are uninformative. We prove theoretically that our algorithm converges for smooth, strongly convex objectives. EMO is generic, flexible, and model-agnostic, making it a simple plug-and-play optimizer that can be seamlessly embedded into existing optimization-based few-shot meta-learning approaches. Empirical results show that EMO 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#27491;&#21017;&#24615;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#21322;&#32447;&#24615;&#26925;&#22278;PDE&#20013;&#30340;Nemytskii&#31639;&#23376;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#23545;&#20110;&#36827;&#34892;&#26377;&#20851;PDE&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#26377;&#24456;&#22909;&#30340;&#21551;&#31034;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.05185</link><description>&lt;p&gt;
&#20851;&#20110;&#21322;&#32447;&#24615;&#26925;&#22278;PDE&#20013;&#38750;&#20809;&#28369;&#36229;&#23450;&#31639;&#23376;&#30340;&#35782;&#21035;&#21644;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
On the Identification and Optimization of Nonsmooth Superposition Operators in Semilinear Elliptic PDEs. (arXiv:2306.05185v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#27491;&#21017;&#24615;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#21322;&#32447;&#24615;&#26925;&#22278;PDE&#20013;&#30340;Nemytskii&#31639;&#23376;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#23545;&#20110;&#36827;&#34892;&#26377;&#20851;PDE&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#26377;&#24456;&#22909;&#30340;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26080;&#38480;&#32500;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#30340;&#26159;&#35782;&#21035;&#21322;&#32447;&#24615;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#38750;&#32447;&#24615;&#37096;&#20998;&#20013;&#30340;Nemytskii&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#23558;PDE&#35299;&#19982;&#32473;&#23450;&#30340;&#26399;&#26395;&#29366;&#24577;&#20043;&#38388;&#30340;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#20302;&#27491;&#21017;&#24615;&#24773;&#20917;&#19979;&#32771;&#34385;&#20102;&#36825;&#20010;&#35782;&#21035;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24341;&#36215;Nemytskii&#31639;&#23376;&#20989;&#25968;&#20808;&#39564;&#20165;&#34987;&#35748;&#20026;&#26159;H^1_{loc}(\mathbb{R})&#30340;&#20803;&#32032;&#12290;&#36825;&#20351;&#24471;&#30740;&#31350;&#30340;&#38382;&#39064;&#31867;&#25104;&#20026;&#19968;&#31181;&#36866;&#21512;&#20174;&#20005;&#26684;&#20998;&#26512;&#30340;&#35282;&#24230;&#26469;&#22788;&#29702;&#23398;&#20064;&#26377;&#20851;PDE&#30340;&#22521;&#35757;&#38382;&#39064;&#30340;&#20986;&#21457;&#28857;&#65292;&#20854;&#20013;&#26410;&#30693;&#30340;&#36229;&#23450;&#31639;&#23376;&#26159;&#36890;&#36807;&#20351;&#29992;&#38750;&#20809;&#28369;&#28608;&#27963;&#20989;&#25968;(ReLU&#65292;leaky-ReLU&#31561;)&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23613;&#31649;&#25511;&#21046;&#30340;&#27491;&#21017;&#24615;&#36739;&#20302;&#65292;&#20294;&#21487;&#20197;&#20026;&#23616;&#37096;&#26497;&#23567;&#20540;&#23548;&#20986;&#32463;&#20856;&#30340;&#31449;&#28857;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#25237;&#24433;&#27861;&#35299;&#20915;&#25152;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study an infinite-dimensional optimization problem that aims to identify the Nemytskii operator in the nonlinear part of a prototypical semilinear elliptic partial differential equation (PDE) which minimizes the distance between the PDE-solution and a given desired state. In contrast to previous works, we consider this identification problem in a low-regularity regime in which the function inducing the Nemytskii operator is a-priori only known to be an element of $H^1_{loc}(\mathbb{R})$. This makes the studied problem class a suitable point of departure for the rigorous analysis of training problems for learning-informed PDEs in which an unknown superposition operator is approximated by means of a neural network with nonsmooth activation functions (ReLU, leaky-ReLU, etc.). We establish that, despite the low regularity of the controls, it is possible to derive a classical stationarity system for local minimizers and to solve the considered problem by means of a gradient projection me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21463;&#38480;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#38271;&#31687;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.05183</link><description>&lt;p&gt;
&#25552;&#39640;&#38271;&#31687;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Improving Long Context Document-Level Machine Translation. (arXiv:2306.05183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21463;&#38480;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#38271;&#31687;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#26469;&#35828;&#65292;&#25991;&#26412;&#32423;&#21035;&#30340;&#19978;&#19979;&#25991;&#23545;&#20110;&#25552;&#39640;&#32763;&#35793;&#30340;&#19968;&#33268;&#24615;&#12289;&#20957;&#32858;&#24615;&#12289;&#27169;&#26865;&#20004;&#21487;&#36755;&#20837;&#30340;&#32763;&#35793;&#20197;&#21450;&#20854;&#20182;&#35821;&#35328;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#20851;&#20110;&#25991;&#26723;&#32423;&#21035; NMT &#30340;&#30456;&#20851;&#35770;&#25991;&#20986;&#29256;&#65292;&#20294;&#22823;&#22810;&#25968;&#23558;&#31995;&#32479;&#38480;&#21046;&#22312;&#26412;&#22320;&#19978;&#19979;&#25991;&#65292;&#36890;&#24120;&#21482;&#21253;&#25324;&#21069;&#19968;&#20004;&#20010;&#21477;&#23376;&#20316;&#20026;&#26356;&#22810;&#20449;&#24687;&#12290;&#36825;&#21487;&#33021;&#36275;&#20197;&#35299;&#20915;&#19968;&#20123;&#26326;&#26151;&#24615;&#36755;&#20837;&#65292;&#20294;&#21487;&#33021;&#19981;&#36275;&#20197;&#25429;&#25417;&#25991;&#26723;&#32423;&#21035;&#20449;&#24687;&#65292;&#20363;&#22914;&#35805;&#39064;&#25110;&#23545;&#35805;&#39118;&#26684;&#12290;&#24403;&#23558;&#19978;&#19979;&#25991;&#22823;&#23567;&#22686;&#21152;&#21040;&#26412;&#22320;&#19978;&#19979;&#25991;&#20043;&#22806;&#26102;&#65292;&#20250;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#65288;i&#65289;&#20869;&#23384;&#20351;&#29992;&#23558;&#21576;&#25351;&#25968;&#22686;&#38271;&#65288;ii&#65289;&#32763;&#35793;&#24615;&#33021;&#24320;&#22987;&#38477;&#20302;&#12290;&#25105;&#20204;&#35748;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#27880;&#24847;&#26426;&#21046;&#26159;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#38480;&#30340;&#27880;&#24847;&#21147;&#21464;&#20307;&#65292;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#24207;&#21015;&#30340;&#26368;&#30456;&#20851;&#37096;&#20998;&#65292;&#21516;&#26102;&#25511;&#21046;&#23545;&#40784;&#26435;&#37325;&#30340;&#24635;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level context for neural machine translation (NMT) is crucial to improve the translation consistency and cohesion, the translation of ambiguous inputs, as well as several other linguistic phenomena. Many works have been published on the topic of document-level NMT, but most restrict the system to only local context, typically including just the one or two preceding sentences as additional information. This might be enough to resolve some ambiguous inputs, but it is probably not sufficient to capture some document-level information like the topic or style of a conversation. When increasing the context size beyond just the local context, there are two challenges: (i) the~memory usage increases exponentially (ii) the translation performance starts to degrade. We argue that the widely-used attention mechanism is responsible for both issues. Therefore, we propose a constrained attention variant that focuses the attention on the most relevant parts of the sequence, while simultaneou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26102;&#23578;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#36827;&#34892;&#20114;&#21160;&#24182;&#29983;&#25104;&#26102;&#23578;&#22270;&#20687;&#65292;&#20174;&#32780;&#24110;&#21161;&#26102;&#23578;&#35774;&#35745;&#24072;&#36827;&#34892;&#23454;&#26102;&#21487;&#35270;&#21270;&#21644;&#36827;&#19968;&#27493;&#25913;&#36827;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.05182</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#20114;&#21160;&#26102;&#23578;&#20869;&#23481;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Interactive Fashion Content Generation Using LLMs and Latent Diffusion Models. (arXiv:2306.05182v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05182
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26102;&#23578;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#36827;&#34892;&#20114;&#21160;&#24182;&#29983;&#25104;&#26102;&#23578;&#22270;&#20687;&#65292;&#20174;&#32780;&#24110;&#21161;&#26102;&#23578;&#35774;&#35745;&#24072;&#36827;&#34892;&#23454;&#26102;&#21487;&#35270;&#21270;&#21644;&#36827;&#19968;&#27493;&#25913;&#36827;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#23578;&#22270;&#20687;&#29983;&#25104;&#26088;&#22312;&#21512;&#25104;&#19990;&#30028;&#21508;&#22320;&#27969;&#34892;&#30340;&#19981;&#21516;&#26102;&#23578;&#22270;&#29255;&#65292;&#36890;&#36807;&#20026;&#26102;&#23578;&#35774;&#35745;&#24072;&#25552;&#20379;&#29305;&#23450;&#35774;&#35745;&#20559;&#22909;&#22312;&#30495;&#23454;&#29983;&#27963;&#20013;&#30340;&#22522;&#26412;&#23450;&#21046;&#32467;&#26500;&#20197;&#21450;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#30475;&#27861;&#65292;&#24110;&#21161;&#35774;&#35745;&#24072;&#36827;&#34892;&#23454;&#26102;&#21487;&#35270;&#21270;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#36827;&#34892;&#20114;&#21160;&#24182;&#29983;&#25104;&#26102;&#23578;&#22270;&#20687;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#20174;&#39640;&#26031;&#22122;&#22768;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#32780;&#34987;&#35270;&#20026;&#29983;&#25104;&#27169;&#22411;&#12290;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#26469;&#27169;&#25311;&#22797;&#26434;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#38899;&#39057;&#25110;&#25991;&#26412;&#65289;&#30340;&#29983;&#25104;&#12290;&#23427;&#20204;&#34987;&#31216;&#20026;&#8220;&#28508;&#22312;&#8221;&#65292;&#22240;&#20026;&#23427;&#20204;&#23398;&#20064;&#25968;&#25454;&#30340;&#38544;&#34255;&#34920;&#31034;&#25110;&#28508;&#22312;&#21464;&#37327;&#65292;&#25429;&#25417;&#20854;&#22522;&#26412;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fashionable image generation aims to synthesize images of diverse fashion prevalent around the globe, helping fashion designers in real-time visualization by giving them a basic customized structure of how a specific design preference would look in real life and what further improvements can be made for enhanced customer satisfaction. Moreover, users can alone interact and generate fashionable images by just giving a few simple prompts. Recently, diffusion models have gained popularity as generative models owing to their flexibility and generation of realistic images from Gaussian noise. Latent diffusion models are a type of generative model that use diffusion processes to model the generation of complex data, such as images, audio, or text. They are called "latent" because they learn a hidden representation, or latent variable, of the data that captures its underlying structure. We propose a method exploiting the equivalence between diffusion models and energy-based models (EBMs) and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#35757;&#32451;&#21160;&#24577;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#23376;&#38598;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;ImageNet-1K&#21644;ImageNet-21K&#19978;&#23454;&#29616;&#20102;75&#65285;&#30340;&#26080;&#25439;&#21387;&#32553;&#27604;&#12290;</title><link>http://arxiv.org/abs/2306.05175</link><description>&lt;p&gt;
&#21160;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Large-scale Dataset Pruning with Dynamic Uncertainty. (arXiv:2306.05175v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#35757;&#32451;&#21160;&#24577;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#23376;&#38598;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;ImageNet-1K&#21644;ImageNet-21K&#19978;&#23454;&#29616;&#20102;75&#65285;&#30340;&#26080;&#25439;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#25910;&#38598;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#24182;&#22312;&#20854;&#19978;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#26159;&#25512;&#21160;&#25216;&#26415;&#21069;&#36827;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#35745;&#31639;&#25104;&#26412;&#36880;&#28176;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20462;&#21098;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20135;&#29983;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#23376;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#22797;&#26434;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#19988;&#24615;&#33021;&#19979;&#38477;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#35757;&#32451;&#21160;&#24577;&#26469;&#23454;&#29616;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;ImageNet-1K&#21644;ImageNet-21K&#65289;&#21644;&#20808;&#36827;&#27169;&#22411;&#65288;Swin Transformer&#21644;ConvNeXt&#65289;&#19978;&#30740;&#31350;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#24037;&#20316;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;ImageNet-1K&#21644;ImageNet-21K&#19978;&#23454;&#29616;&#20102;75&#65285;&#30340;&#26080;&#25439;&#21387;&#32553;&#27604;&#12290;&#20195;&#30721;&#21644;&#20462;&#21098;&#21518;&#30340;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/BAAI-DCAI/Dataset-Pruning&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state of the art of many learning tasks, e.g., image classification, is advanced by collecting larger datasets and then training larger models on them. As the outcome, the increasing computational cost is becoming unaffordable. In this paper, we investigate how to prune the large-scale datasets, and thus produce an informative subset for training sophisticated deep models with negligible performance drop. We propose a simple yet effective dataset pruning method by exploring both the prediction uncertainty and training dynamics. To our knowledge, this is the first work to study dataset pruning on large-scale datasets, i.e., ImageNet-1K and ImageNet-21K, and advanced models, i.e., Swin Transformer and ConvNeXt. Extensive experimental results indicate that our method outperforms the state of the art and achieves 75% lossless compression ratio on both ImageNet-1K and ImageNet-21K. The code and pruned datasets are available at https://github.com/BAAI-DCAI/Dataset-Pruning.
&lt;/p&gt;</description></item><item><title>FLEdge&#26159;&#19968;&#20010;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;FL&#24037;&#20316;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#30740;&#31350;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#33021;&#37327;&#25928;&#29575;&#21644;&#38544;&#31169;&#32423;&#21035;&#23545;FL&#31995;&#32479;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23458;&#25143;&#31471;&#36864;&#20986;&#23545;&#26368;&#26032;FL&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;FL&#24037;&#20316;&#36127;&#36733;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.05172</link><description>&lt;p&gt;
FLEdge&#65306;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FLEdge: Benchmarking Federated Machine Learning Applications in Edge Computing Systems. (arXiv:2306.05172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05172
&lt;/p&gt;
&lt;p&gt;
FLEdge&#26159;&#19968;&#20010;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;FL&#24037;&#20316;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#30740;&#31350;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#33021;&#37327;&#25928;&#29575;&#21644;&#38544;&#31169;&#32423;&#21035;&#23545;FL&#31995;&#32479;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23458;&#25143;&#31471;&#36864;&#20986;&#23545;&#26368;&#26032;FL&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;FL&#24037;&#20316;&#36127;&#36733;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#65288;FL&#65289;&#22791;&#21463;&#20851;&#27880;&#12290; FL&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#22312;&#27169;&#25311;&#31995;&#32479;&#25110;&#25968;&#25454;&#20013;&#24515;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#24573;&#30053;&#20102;&#19982;&#36793;&#32536;&#35745;&#31639;&#23494;&#20999;&#30456;&#20851;&#30340;&#23454;&#38469;&#31995;&#32479;&#35774;&#32622;&#12290; &#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;FL&#24037;&#20316;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;FLEdge&#26469;&#24357;&#34917;&#36825;&#19968;&#30740;&#31350;&#24046;&#36317;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#25928;&#29575;&#20197;&#21450;&#21508;&#31181;&#19981;&#21516;&#38544;&#31169;&#32423;&#21035;&#23545;FL&#31995;&#32479;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20351;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#36866;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23458;&#25143;&#31471;&#36864;&#20986;&#23545;&#20855;&#26377;&#39640;&#36798;50&#65285;&#22833;&#25928;&#29575;&#30340;&#26368;&#26032;FL&#31574;&#30053;&#30340;&#24433;&#21709;&#12290; FLEdge&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#20363;&#22914;&#65292;&#22312;&#26087;GPU&#21152;&#36895;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;FL&#24037;&#20316;&#36127;&#36733;&#27604;&#22312;&#29616;&#20195;&#26381;&#21153;&#22120;&#32423;GPU&#19978;&#35757;&#32451;&#39640;&#36798;3&#20493;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Machine Learning (FL) has received considerable attention in recent years. FL benchmarks are predominantly explored in either simulated systems or data center environments, neglecting the setups of real-world systems, which are often closely linked to edge computing. We close this research gap by introducing FLEdge, a benchmark targeting FL workloads in edge computing systems. We systematically study hardware heterogeneity, energy efficiency during training, and the effect of various differential privacy levels on training in FL systems. To make this benchmark applicable to real-world scenarios, we evaluate the impact of client dropouts on state-of-the-art FL strategies with failure rates as high as 50%. FLEdge provides new insights, such as that training state-of-the-art FL workloads on older GPU-accelerated embedded devices is up to 3x more energy efficient than on modern server-grade GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#23618;&#30340;&#39640;&#25928;&#24207;&#21015;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#21547;&#19968;&#31181;&#31163;&#32447;&#21644;&#19968;&#31181;&#22312;&#32447;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#20174;&#38271;&#31243;&#20381;&#36182;&#20013;&#21463;&#30410;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05167</link><description>&lt;p&gt;
Decision S4&#65306;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#23618;&#30340;&#39640;&#25928;&#24207;&#21015;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decision S4: Efficient Sequence-Based RL via State Spaces Layers. (arXiv:2306.05167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#23618;&#30340;&#39640;&#25928;&#24207;&#21015;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#21547;&#19968;&#31181;&#31163;&#32447;&#21644;&#19968;&#31181;&#22312;&#32447;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#20174;&#38271;&#31243;&#20381;&#36182;&#20013;&#21463;&#30410;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;Transformer&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#8212;&#8212;Decision Transformer&#12290;&#30001;&#20110;Transformer&#21442;&#25968;&#37325;&#65292;&#26080;&#27861;&#21033;&#29992;&#36229;&#36807;&#22266;&#23450;&#31383;&#21475;&#22823;&#23567;&#30340;&#21382;&#21490;&#20449;&#24687;&#65292;&#19988;&#19981;&#33021;&#20351;&#29992;&#22797;&#29616;&#35745;&#31639;&#65292;&#22240;&#27492;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;S4&#27169;&#22411;&#31995;&#21015;&#30340;&#36866;&#29992;&#24615;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#23618;&#26500;&#24314;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#24314;&#27169;&#38271;&#31243;&#20381;&#36182;&#26041;&#38754;&#20248;&#20110;Transformer&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#31639;&#27861;&#65306;&#65288;i&#65289;&#19968;&#31181;&#31163;&#32447;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#20351;&#29992;&#36712;&#36857;&#25968;&#25454;&#65292;&#21516;&#26102;&#20173;&#28982;&#20445;&#25345;S4&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#65307;&#65288;ii&#65289;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#31283;&#23450;&#30340;&#28436;&#21592; - &#35780;&#35770;&#23478;&#26426;&#21046;&#65292;&#20197;&#24490;&#29615;&#26041;&#24335;&#35757;&#32451;&#30340;&#22312;&#32447;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#20174;&#38271;&#31243;&#20381;&#36182;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22810;&#31181;Decision Transformer&#30340;&#21464;&#20307;&#20197;&#21450;&#20256;&#32479;&#30340;LSTM&#21644;GRU&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, sequence learning methods have been applied to the problem of off-policy Reinforcement Learning, including the seminal work on Decision Transformers, which employs transformers for this task. Since transformers are parameter-heavy, cannot benefit from history longer than a fixed window size, and are not computed using recurrence, we set out to investigate the suitability of the S4 family of models, which are based on state-space layers and have been shown to outperform transformers, especially in modeling long-range dependencies. In this work we present two main algorithms: (i) an off-policy training procedure that works with trajectories, while still maintaining the training efficiency of the S4 model. (ii) An on-policy training procedure that is trained in a recurrent manner, benefits from long-range dependencies, and is based on a novel stable actor-critic mechanism. Our results indicate that our method outperforms multiple variants of decision transformers, as well as the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#23884;&#22871;&#40657;&#30333;&#31665;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#40657;&#31665;&#20248;&#21270;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20840;&#23616;&#26368;&#20248;&#35299;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.05150</link><description>&lt;p&gt;
&#26114;&#36149;&#23884;&#22871;&#28784;&#30418;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization of Expensive Nested Grey-Box Functions. (arXiv:2306.05150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#23884;&#22871;&#40657;&#30333;&#31665;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#40657;&#31665;&#20248;&#21270;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20840;&#23616;&#26368;&#20248;&#35299;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20248;&#21270;&#28784;&#30418;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#21363;&#30001;&#40657;&#31665;&#21644;&#30333;&#31665;&#20989;&#25968;&#32452;&#25104;&#30340;&#23884;&#22871;&#20989;&#25968;&#12290;&#32473;&#20986;&#20102;&#36825;&#31181;&#28784;&#30418;&#38382;&#39064;&#30340;&#19968;&#33324;&#24418;&#24335;&#65292;&#28085;&#30422;&#20102;&#29616;&#26377;&#30340;&#28784;&#30418;&#20248;&#21270;&#20844;&#24335;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#19968;&#23450;&#30340;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#19982;&#26631;&#20934;&#40657;&#31665;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30456;&#20284;&#30340;&#21518;&#24724;&#36793;&#30028;&#65292;&#20294;&#20056;&#20197;&#20381;&#36182;&#20110;&#25152;&#32771;&#34385;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#30340;&#24120;&#25968;&#20056;&#39033;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#32422;&#26463;&#24773;&#20917;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#20010;&#29305;&#27530;&#24773;&#20917;&#12290;&#23545;&#20110;&#24120;&#29992;&#30340;&#26680;&#20989;&#25968;&#65292;&#21518;&#24724;&#36793;&#30028;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#21040;&#26368;&#20248;&#35299;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#40657;&#31665;&#20248;&#21270;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#28784;&#30418;&#20248;&#21270;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#26174;&#30528;&#25552;&#39640;&#20102;&#23547;&#25214;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of optimizing a grey-box objective function, i.e., nested function composed of both black-box and white-box functions. A general formulation for such grey-box problems is given, which covers the existing grey-box optimization formulations as special cases. We then design an optimism-driven algorithm to solve it. Under certain regularity assumptions, our algorithm achieves similar regret bound as that for the standard black-box Bayesian optimization algorithm, up to a constant multiplicative term depending on the Lipschitz constants of the functions considered. We further extend our method to the constrained case and discuss several special cases. For the commonly used kernel functions, the regret bounds allow us to derive a convergence rate to the optimal solution. Experimental results show that our grey-box optimization method empirically improves the speed of finding the global optimal solution significantly, as compared to the standard black-box optimization 
&lt;/p&gt;</description></item><item><title>Mesogeos&#26159;&#19968;&#20010;&#22320;&#20013;&#28023;&#22320;&#21306;&#30340;&#22823;&#35268;&#27169;&#22810;&#29992;&#36884;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#37326;&#28779;&#24314;&#27169;&#12290;&#23427;&#38598;&#25104;&#20102;&#21382;&#21490;&#37326;&#28779;&#35760;&#24405;&#21644;&#37326;&#28779;&#39537;&#21160;&#22240;&#32032;&#65292;&#20855;&#26377;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#30701;&#26399;&#37326;&#28779;&#21361;&#38505;&#39044;&#27979;&#21644;&#26368;&#32456;&#28903;&#27585;&#21306;&#22495;&#20272;&#35745;&#20004;&#20010;&#21487;&#29992;&#20110;&#28436;&#31034;&#28508;&#21147;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.05144</link><description>&lt;p&gt;
Mesogeos: &#22320;&#20013;&#28023;&#21306;&#22495;&#25968;&#25454;&#39537;&#21160;&#37326;&#28779;&#24314;&#27169;&#30340;&#22810;&#29992;&#36884;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean. (arXiv:2306.05144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05144
&lt;/p&gt;
&lt;p&gt;
Mesogeos&#26159;&#19968;&#20010;&#22320;&#20013;&#28023;&#22320;&#21306;&#30340;&#22823;&#35268;&#27169;&#22810;&#29992;&#36884;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#37326;&#28779;&#24314;&#27169;&#12290;&#23427;&#38598;&#25104;&#20102;&#21382;&#21490;&#37326;&#28779;&#35760;&#24405;&#21644;&#37326;&#28779;&#39537;&#21160;&#22240;&#32032;&#65292;&#20855;&#26377;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#30701;&#26399;&#37326;&#28779;&#21361;&#38505;&#39044;&#27979;&#21644;&#26368;&#32456;&#28903;&#27585;&#21306;&#22495;&#20272;&#35745;&#20004;&#20010;&#21487;&#29992;&#20110;&#28436;&#31034;&#28508;&#21147;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Mesogeos&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#29992;&#36884;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22320;&#20013;&#28023;&#22320;&#21306;&#30340;&#37326;&#28779;&#24314;&#27169;&#12290;Mesogeos&#38598;&#25104;&#20102;&#20195;&#34920;&#37326;&#28779;&#39537;&#21160;&#22240;&#32032;&#65288;&#27668;&#35937;&#12289;&#26893;&#34987;&#12289;&#20154;&#31867;&#27963;&#21160;&#65289;&#21644;17&#24180;&#65288;2006-2022&#24180;&#65289;&#37326;&#28779;&#28857;&#29123;&#21644;&#28903;&#27585;&#21306;&#22495;&#30340;&#21382;&#21490;&#35760;&#24405;&#30340;&#21464;&#37327;&#12290;&#23427;&#34987;&#35774;&#35745;&#20026;&#20113;&#21451;&#22909;&#22411;&#26102;&#31354;&#25968;&#25454;&#38598;&#65288;&#21363;&#25968;&#25454;&#31435;&#26041;&#20307;&#65289;&#65292;&#22312;1km x 1km x 1&#22825;&#30340;&#20998;&#36776;&#29575;&#19979;&#23545;&#25152;&#26377;&#21464;&#37327;&#36827;&#34892;&#20102;&#21327;&#35843;&#12290;&#25968;&#25454;&#31435;&#26041;&#20307;&#30340;&#32467;&#26500;&#20026;&#21508;&#31181;&#37326;&#28779;&#24314;&#27169;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#20351;&#29992;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#21462;&#20986;&#20004;&#20010;ML&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#23637;&#31034;&#36825;&#20010;&#28508;&#21147;&#65306;(1)&#30701;&#26399;&#37326;&#28779;&#21361;&#38505;&#39044;&#27979;&#21644;(2)&#22312;&#28857;&#28779;&#20301;&#32622;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26368;&#32456;&#28903;&#27585;&#21306;&#22495;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#25351;&#26631;&#21644;&#22522;&#32447;&#26469;&#35780;&#20272;&#27599;&#20010;&#36319;&#36394;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21457;&#24067;&#25968;&#25454;&#31435;&#26041;&#20307;&#65292;&#20197;&#21450;&#29992;&#20110;&#21019;&#24314;ML&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#25105;&#20204;&#40723;&#21169;&#31038;&#21306;&#20419;&#36827;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire modeling in the Mediterranean. Mesogeos integrates variables representing wildfire drivers (meteorology, vegetation, human activity) and historical records of wildfire ignitions and burned areas for 17 years (2006-2022). It is designed as a cloud-friendly spatio-temporal dataset, namely a datacube, harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The datacube structure offers opportunities to assess machine learning (ML) usage in various wildfire modeling tasks. We extract two ML-ready datasets that establish distinct tracks to demonstrate this potential: (1) short-term wildfire danger forecasting and (2) final burned area estimation given the point of ignition. We define appropriate metrics and baselines to evaluate the performance of models in each track. By publishing the datacube, along with the code to create the ML datasets and models, we encourage the community to foster the implementatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22522;&#22240;&#32452;&#35299;&#37322;&#22120;&#8221;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#23545;&#22522;&#22240;&#32452;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#24182;&#21457;&#29616;&#22522;&#22240;&#35843;&#25511;&#30340;&#23618;&#27425;&#20381;&#36182;&#20851;&#31995;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05143</link><description>&lt;p&gt;
&#22522;&#22240;&#32452;&#35299;&#37322;&#22120;&#65306;&#19968;&#31181;&#24102;&#26377;1D&#31227;&#21160;&#31383;&#21475;&#21464;&#25442;&#22120;&#30340;&#23618;&#27425;&#22522;&#22240;&#32452;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer. (arXiv:2306.05143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22522;&#22240;&#32452;&#35299;&#37322;&#22120;&#8221;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#23545;&#22522;&#22240;&#32452;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#24182;&#21457;&#29616;&#22522;&#22240;&#35843;&#25511;&#30340;&#23618;&#27425;&#20381;&#36182;&#20851;&#31995;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#22240;&#32452;&#25968;&#25454;&#37327;&#21644;&#36136;&#37327;&#30340;&#22686;&#21152;&#65292;&#25552;&#21462;&#26032;&#30340;&#27934;&#35265;&#38656;&#35201;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#22240;&#32452;&#27979;&#23450;&#39044;&#27979;&#32467;&#26500;&#65306;&#22522;&#22240;&#32452;&#35299;&#37322;&#22120;&#12290;&#35813;&#27169;&#22411;&#22312;&#22522;&#22240;&#32452;&#27979;&#23450;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#35782;&#21035;&#22522;&#22240;&#32452;&#20301;&#28857;&#30340;&#23618;&#27425;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26159;&#36890;&#36807;1D-Swin&#36827;&#34892;&#23454;&#29616;&#30340;&#65292;&#36825;&#26159;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#38271;&#33539;&#22260;&#23618;&#27425;&#25968;&#25454;&#30340;&#26032;&#22411;&#21464;&#25442;&#22120;&#22359;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;38,171&#20010;17K&#30897;&#22522;&#23545;&#30340;DNA&#29255;&#27573;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#22522;&#22240;&#32452;&#35299;&#37322;&#22120;&#22312;&#26579;&#33394;&#36136;&#21487;&#36798;&#24615;&#21644;&#22522;&#22240;&#34920;&#36798;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#22522;&#22240;&#35843;&#25511;&#30340;&#28508;&#22312;&#8220;&#35821;&#27861;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the increasing volume and quality of genomics data, extracting new insights requires interpretable machine-learning models. This work presents Genomic Interpreter: a novel architecture for genomic assay prediction. This model outperforms the state-of-the-art models for genomic assay prediction tasks. Our model can identify hierarchical dependencies in genomic sites. This is achieved through the integration of 1D-Swin, a novel Transformer-based block designed by us for modelling long-range hierarchical data. Evaluated on a dataset containing 38,171 DNA segments of 17K base pairs, Genomic Interpreter demonstrates superior performance in chromatin accessibility and gene expression prediction and unmasks the underlying `syntax' of gene regulation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#22411;&#32852;&#37030;&#21512;&#35268;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#26631;&#31614;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20026;&#39044;&#27979;&#38598;&#30340;&#26377;&#25928;&#35206;&#30422;&#21644;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#30446;&#21069;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;</title><link>http://arxiv.org/abs/2306.05131</link><description>&lt;p&gt;
&#38754;&#21521;&#26631;&#31614;&#28418;&#31227;&#30340;&#32852;&#37030;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21512;&#35268;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction for Federated Uncertainty Quantification Under Label Shift. (arXiv:2306.05131v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05131
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#22411;&#32852;&#37030;&#21512;&#35268;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#26631;&#31614;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20026;&#39044;&#27979;&#38598;&#30340;&#26377;&#25928;&#35206;&#30422;&#21644;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#30446;&#21069;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#35768;&#22810;&#23458;&#25143;&#31471;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#20998;&#25955;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;FL&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#23545;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20027;&#39064;&#65288;UQ&#65289;&#36827;&#34892;&#37096;&#20998;&#22788;&#29702;&#12290;&#22312;UQ&#26041;&#27861;&#20013;&#65292;&#21512;&#35268;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#26041;&#27861;&#22312;&#26368;&#23567;&#20551;&#35774;&#19979;&#25552;&#20379;&#26080;&#20998;&#24067;&#20445;&#35777;&#12290;&#25105;&#20204;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#21512;&#35268;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#38544;&#31169;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#26631;&#31614;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20026;&#39044;&#27979;&#38598;&#30340;&#26377;&#25928;&#35206;&#30422;&#21644;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#30446;&#21069;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning framework where many clients collaboratively train models while keeping the training data decentralized. Despite recent advances in FL, the uncertainty quantification topic (UQ) remains partially addressed. Among UQ methods, conformal prediction (CP) approaches provides distribution-free guarantees under minimal assumptions. We develop a new federated conformal prediction method based on quantile regression and take into account privacy constraints. This method takes advantage of importance weighting to effectively address the label shift between agents and provides theoretical guarantees for both valid coverage of the prediction sets and differential privacy. Extensive experimental studies demonstrate that this method outperforms current competitors.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23454;&#38469;&#24037;&#19994;&#31995;&#32479;&#29305;&#24449;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#21487;&#20316;&#20026;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#26550;&#26500;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;Meta-VAE&#65292;&#21487;&#20197;&#20934;&#30830;&#29983;&#25104;&#20855;&#26377;&#28508;&#22312;&#35774;&#35745;&#32422;&#26463;&#30340;&#22810;&#32452;&#20998;&#24037;&#19994;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.05123</link><description>&lt;p&gt;
&#24037;&#19994;&#31995;&#32479;&#29983;&#25104;&#30340;&#20803;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Meta-Generation framework for Industrial System Generation. (arXiv:2306.05123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05123
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23454;&#38469;&#24037;&#19994;&#31995;&#32479;&#29305;&#24449;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#21487;&#20316;&#20026;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#26550;&#26500;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;Meta-VAE&#65292;&#21487;&#20197;&#20934;&#30830;&#29983;&#25104;&#20855;&#26377;&#28508;&#22312;&#35774;&#35745;&#32422;&#26463;&#30340;&#22810;&#32452;&#20998;&#24037;&#19994;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35774;&#35745;&#26159;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#24037;&#20855;&#12290;&#23427;&#20801;&#35768;&#35774;&#35745;&#24072;&#21644;&#24037;&#31243;&#24072;&#36731;&#26494;&#25506;&#32034;&#24191;&#27867;&#30340;&#35774;&#35745;&#36873;&#39033;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#20415;&#23452;&#12289;&#26356;&#24555;&#36895;&#30340;&#35797;&#38169;&#26041;&#27861;&#26367;&#20195;&#26041;&#26696;&#12290;&#30001;&#20110;&#20854;&#25552;&#20379;&#30340;&#28789;&#27963;&#24615;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#35774;&#35745;&#25216;&#26415;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#21644;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#39046;&#22495;&#32570;&#20047;&#21487;&#35775;&#38382;&#30340;&#22522;&#20934;&#26469;&#23458;&#35266;&#22320;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#26222;&#36890;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20284;&#20046;&#26080;&#27861;&#20934;&#30830;&#29983;&#25104;&#30001;&#28508;&#22312;&#35774;&#35745;&#32422;&#26463;&#25511;&#21046;&#30340;&#22810;&#32452;&#20998;&#24037;&#19994;&#31995;&#32479;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24037;&#19994;&#30028;&#28789;&#24863;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#35813;&#26696;&#20363;&#34701;&#20837;&#20102;&#23454;&#38469;&#24037;&#19994;&#31995;&#32479;&#30340;&#29305;&#24449;&#12290;&#35813;&#20351;&#29992;&#26696;&#20363;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#24182;&#29992;&#20316;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Meta-VAE&#65292;&#33021;&#22815;&#20135;&#29983;&#22810;&#32452;&#20998;&#30340;&#24037;&#19994;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Generative design is an increasingly important tool in the industrial world. It allows the designers and engineers to easily explore vast ranges of design options, providing a cheaper and faster alternative to the trial and failure approaches. Thanks to the flexibility they offer, Deep Generative Models are gaining popularity amongst Generative Design technologies. However, developing and evaluating these models can be challenging. The field lacks accessible benchmarks, in order to evaluate and compare objectively different Deep Generative Models architectures. Moreover, vanilla Deep Generative Models appear to be unable to accurately generate multi-components industrial systems that are controlled by latent design constraints. To address these challenges, we propose an industry-inspired use case that incorporates actual industrial system characteristics. This use case can be quickly generated and used as a benchmark. We propose a Meta-VAE capable of producing multi-component industria
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#26368;&#22909;&#22320;&#21033;&#29992;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#35299;&#30721;&#26041;&#26696;&#21644;&#20316;&#32773;&#25552;&#20986;&#30340;&#26041;&#26696;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#26102;&#65292;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#33021;&#22815;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05116</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25628;&#32034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
On Search Strategies for Document-Level Neural Machine Translation. (arXiv:2306.05116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#26368;&#22909;&#22320;&#21033;&#29992;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#35299;&#30721;&#26041;&#26696;&#21644;&#20316;&#32773;&#25552;&#20986;&#30340;&#26041;&#26696;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#26102;&#65292;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#33021;&#22815;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21477;&#23376;&#32423;&#31995;&#32479;&#30456;&#27604;&#65292;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#33021;&#22815;&#22312;&#19968;&#20221;&#25991;&#20214;&#20013;&#20135;&#29983;&#26356;&#19968;&#33268;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#20915;&#36755;&#20837;&#20013;&#30340;&#27495;&#20041;&#12290;&#22312;&#25991;&#26723;&#32423;NMT&#19978;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#30528;&#37325;&#20110;&#20462;&#25913;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#31574;&#30053;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#30740;&#31350;&#20013;&#65292;&#22914;&#20309;&#36890;&#36807;&#24050;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#25191;&#34892;&#25628;&#32034;&#30340;&#38382;&#39064;&#24456;&#23569;&#34987;&#35752;&#35770;&#65292;&#26377;&#26102;&#29978;&#33267;&#26681;&#26412;&#19981;&#34987;&#25552;&#21450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#22914;&#20309;&#22312;&#35299;&#30721;&#20013;&#26368;&#22909;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#26368;&#27969;&#34892;&#30340;&#25991;&#26723;&#32423;NMT&#26041;&#27861;&#24320;&#22987;&#65292;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#35299;&#30721;&#26041;&#26696;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#12290;&#27604;&#36739;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#20197;&#21450;&#38024;&#23545;&#19977;&#20010;&#26631;&#20934;&#25991;&#26723;&#32423;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#30340;&#29305;&#23450;&#35821;&#35328;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#22312;&#38024;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#26102;&#24182;&#19981;&#33021;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to sentence-level systems, document-level neural machine translation (NMT) models produce a more consistent output across a document and are able to better resolve ambiguities within the input. There are many works on document-level NMT, mostly focusing on modifying the model architecture or training strategy to better accommodate the additional context-input. On the other hand, in most works, the question on how to perform search with the trained model is scarcely discussed, sometimes not mentioned at all. In this work, we aim to answer the question how to best utilize a context-aware translation model in decoding. We start with the most popular document-level NMT approach and compare different decoding schemes, some from the literature and others proposed by us. In the comparison, we are using both, standard automatic metrics, as well as specific linguistic phenomena on three standard document-level translation benchmarks. We find that most commonly used decoding strategies 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#28151;&#21512;&#22270;&#30340;&#27010;&#24565;&#21450;&#20854;&#22312;&#39640;&#38454;&#22270;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#28151;&#21512;&#22270;&#25968;&#25454;&#38598;&#21450;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36825;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#22270;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.05108</link><description>&lt;p&gt;
&#28151;&#21512;&#22270;&#65306;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#22270;&#30340;&#32479;&#19968;&#22270;&#34920;&#31034;&#21450;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid Graph: A Unified Graph Representation with Datasets and Benchmarks for Complex Graphs. (arXiv:2306.05108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#28151;&#21512;&#22270;&#30340;&#27010;&#24565;&#21450;&#20854;&#22312;&#39640;&#38454;&#22270;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#28151;&#21512;&#22270;&#25968;&#25454;&#38598;&#21450;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36825;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#22270;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34987;&#24191;&#27867;&#29992;&#20110;&#23553;&#35013;&#21508;&#31181;&#25968;&#25454;&#26684;&#24335;&#65292;&#20294;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#19981;&#20165;&#20165;&#26159;&#25104;&#23545;&#30340;&#20851;&#31995;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#21644;&#20351;&#29992;&#20102;&#36229;&#22270;&#21644;&#20998;&#23618;&#22270;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#26080;&#27861;&#23436;&#20840;&#34920;&#31034;&#36825;&#20123;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;&#26356;&#39640;&#38454;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21482;&#22312;&#31616;&#21333;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26356;&#39640;&#38454;&#22270;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#32452;&#21253;&#21547;&#20840;&#38754;&#25968;&#25454;&#38598;&#30340;&#21487;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#23436;&#20840;&#20102;&#35299;&#36825;&#20123;&#31639;&#27861;&#22312;&#22797;&#26434;&#22270;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28151;&#21512;&#22270;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#39640;&#38454;&#22270;&#30340;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#22270;&#36125;&#22855;&#39532;&#20811;&#65288;HGB&#65289;&#12290;HGB&#21253;&#21547;&#21508;&#20010;&#39046;&#22495;&#30340;23&#20010;&#30495;&#23454;&#28151;&#21512;&#22270;&#25968;&#25454;&#38598;&#65288;&#22914;&#29983;&#29289;&#23398;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#20132;&#36890;&#65289;&#65292;&#24182;&#20026;GNN&#22312;&#22797;&#26434;&#22270;&#19978;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are widely used to encapsulate a variety of data formats, but real-world networks often involve complex node relations beyond only being pairwise. While hypergraphs and hierarchical graphs have been developed and employed to account for the complex node relations, they cannot fully represent these complexities in practice. Additionally, though many Graph Neural Networks (GNNs) have been proposed for representation learning on higher-order graphs, they are usually only evaluated on simple graph datasets. Therefore, there is a need for a unified modelling of higher-order graphs, and a collection of comprehensive datasets with an accessible evaluation framework to fully understand the performance of these algorithms on complex graphs. In this paper, we introduce the concept of hybrid graphs, a unified definition for higher-order graphs, and present the Hybrid Graph Benchmark (HGB). HGB contains 23 real-world hybrid graph datasets across various domains such as biology, social media
&lt;/p&gt;</description></item><item><title>Sy-CON&#26159;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25345;&#32493;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23427;&#30001;&#20004;&#20010;&#25439;&#22833;&#32452;&#25104;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#25214;&#21040;&#33391;&#22909;&#30340;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05101</link><description>&lt;p&gt;
Sy-CON&#65306;&#29992;&#20110;&#25345;&#32493;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#23545;&#31216;&#23545;&#27604;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Sy-CON: Symmetric Contrastive Loss for Continual Self-Supervised Representation Learning. (arXiv:2306.05101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05101
&lt;/p&gt;
&lt;p&gt;
Sy-CON&#26159;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25345;&#32493;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23427;&#30001;&#20004;&#20010;&#25439;&#22833;&#32452;&#25104;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#25214;&#21040;&#33391;&#22909;&#30340;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;Symmetric Contrastive&#65288;Sy-CON&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#25345;&#32493;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;CSSL&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#35748;&#20026;&#65292;&#20256;&#32479;&#30340;&#25345;&#32493;&#23398;&#20064;&#25439;&#22833;&#24418;&#24335;&#30001;&#21333;&#20010;&#20219;&#21153;&#29305;&#23450;&#25439;&#22833;&#65288;&#29992;&#20110;&#21487;&#22609;&#24615;&#65289;&#21644;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#65288;&#29992;&#20110;&#31283;&#23450;&#24615;&#65289;&#32452;&#25104;&#65292;&#23545;&#20110;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;CSSL&#26469;&#35828;&#21487;&#33021;&#19981;&#29702;&#24819;&#65292;&#22240;&#20026;&#22312;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20219;&#21153;&#29305;&#23450;&#25439;&#22833;&#20250;&#36973;&#21463;&#36127;&#26679;&#26412;&#22810;&#26679;&#24615;&#38477;&#20302;&#30340;&#22256;&#25200;&#65292;&#32780;&#27491;&#21017;&#21270;&#22120;&#21487;&#33021;&#20250;&#38459;&#30861;&#23398;&#20064;&#26032;&#30340;&#26377;&#21306;&#21035;&#30340;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;Sy-CON&#65292;&#23427;&#30001;&#20004;&#20010;&#25439;&#22833;&#65288;&#19968;&#20010;&#29992;&#20110;&#21487;&#22609;&#24615;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#31283;&#23450;&#24615;&#65289;&#32452;&#25104;&#65292;&#23545;&#24403;&#21069;&#21644;&#36807;&#21435;&#27169;&#22411;&#30340;&#36127;&#26679;&#26412;&#23884;&#20837;&#20855;&#26377;&#23545;&#31216;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#28982;&#22320;&#25214;&#21040;&#33391;&#22909;&#30340;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26174;&#24335;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#31181;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#30340;&#22806;&#37096;&#21644;&#20869;&#37096;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;Sy-CON&#22312;&#31283;&#23450;&#24615;&#21644;&#34920;&#24449;&#36136;&#37327;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel and general loss function, called Symmetric Contrastive (Sy-CON) loss, for effective continual self-supervised learning (CSSL). We first argue that the conventional loss form of continual learning which consists of single task-specific loss (for plasticity) and a regularizer (for stability) may not be ideal for contrastive loss based CSSL that focus on representation learning. Our reasoning is that, in contrastive learning based methods, the task-specific loss would suffer from decreasing diversity of negative samples and the regularizer may hinder learning new distinctive representations. To that end, we propose Sy-CON that consists of two losses (one for plasticity and the other for stability) with symmetric dependence on current and past models' negative sample embeddings. We argue our model can naturally find good trade-off between the plasticity and stability without any explicit hyperparameter tuning. We validate the effectiveness of our approach through exte
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#20998;&#26512;&#20102;&#24433;&#23376;&#27169;&#22411;&#19981;&#23545;&#40784;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#25110;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#37325;&#26032;&#23545;&#40784;&#24433;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30333;&#30418;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05093</link><description>&lt;p&gt;
&#37325;&#26032;&#23545;&#40784;&#24433;&#23376;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;&#30333;&#30418;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Re-aligning Shadow Models can Improve White-box Membership Inference Attacks. (arXiv:2306.05093v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05093
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#20998;&#26512;&#20102;&#24433;&#23376;&#27169;&#22411;&#19981;&#23545;&#40784;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#25110;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#37325;&#26032;&#23545;&#40784;&#24433;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30333;&#30418;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#27844;&#38706;&#20102;&#26377;&#20851;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#38543;&#30528;&#27169;&#22411;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#34987;&#29992;&#20110;&#35774;&#22791;&#19978;&#65292;&#33258;&#21160;&#21270;&#20219;&#21153;&#21644;&#39537;&#21160;&#26032;&#24212;&#29992;&#65292;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#30333;&#30418;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#32780;&#19981;&#26159;&#20165;&#25552;&#20379;&#23545;&#27169;&#22411;&#30340;&#26597;&#35810;&#35775;&#38382;&#30340;&#40657;&#30418;&#35774;&#32622;&#65292;&#36825;&#22686;&#21152;&#20102;&#25915;&#20987;&#38754;&#12290;&#23558;&#40657;&#30418;&#21040;&#30333;&#30418;&#35774;&#32622;&#30340;&#24433;&#23376;&#24314;&#27169;&#25216;&#26415;&#30452;&#25509;&#25193;&#23637;&#21040;&#30333;&#30418;&#35774;&#32622;&#20013;&#65292;&#36890;&#24120;&#34920;&#29616;&#19981;&#22914;&#20165;&#36827;&#34892;&#40657;&#30418;&#25915;&#20987;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24050;&#30693;&#29305;&#24449;&#8212;&#8212;&#19981;&#23545;&#40784;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;&#24433;&#23376;&#27169;&#22411;&#19981;&#23545;&#40784;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;&#37319;&#29992;&#19981;&#21516;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26159;&#24433;&#23376;&#27169;&#22411;&#19981;&#23545;&#40784;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#34701;&#21512;&#25991;&#29486;&#20013;&#20808;&#21069;&#24320;&#21457;&#30340;&#22810;&#31181;&#37325;&#26032;&#23545;&#40784;&#25216;&#26415;&#25193;&#23637;&#21040;&#24433;&#23376;&#24314;&#27169;&#19978;&#19979;&#25991;&#20013;&#65292;&#30446;&#26631;&#26159;&#37325;&#26032;&#23545;&#40784;......
&lt;/p&gt;
&lt;p&gt;
Machine learning models have been shown to leak sensitive information about their training datasets. As models are being increasingly used, on devices, to automate tasks and power new applications, there have been concerns that such white-box access to its parameters, as opposed to the black-box setting which only provides query access to the model, increases the attack surface. Directly extending the shadow modelling technique from the black-box to the white-box setting has been shown, in general, not to perform better than black-box only attacks. A key reason is misalignment, a known characteristic of deep neural networks. We here present the first systematic analysis of the causes of misalignment in shadow models and show the use of a different weight initialisation to be the main cause of shadow model misalignment. Second, we extend several re-alignment techniques, previously developed in the model fusion literature, to the shadow modelling context, where the goal is to re-align th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23402;&#29983;RNN&#32467;&#26500;&#29992;&#20110;&#27979;&#37327;L2-L2&#20132;&#20114;&#20013;&#35821;&#38899;&#38899;&#36136;&#25910;&#25947;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#35821;&#38899;&#25910;&#25947;&#21644;&#35828;&#35805;&#32773;&#30340;&#27169;&#20223;&#33021;&#21147;&#30340;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#30001;L1&#24341;&#36215;&#30340;&#35828;&#35805;&#32773;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.05088</link><description>&lt;p&gt;
&#23545;&#35805;&#30340;&#33402;&#26415;&#65306;&#20351;&#29992;&#23402;&#29983;RNN&#27979;&#37327;L2&#35821;&#38899;&#20013;&#30340;&#35821;&#38899;&#25910;&#25947;&#21644;&#25925;&#24847;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN. (arXiv:2306.05088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23402;&#29983;RNN&#32467;&#26500;&#29992;&#20110;&#27979;&#37327;L2-L2&#20132;&#20114;&#20013;&#35821;&#38899;&#38899;&#36136;&#25910;&#25947;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#35821;&#38899;&#25910;&#25947;&#21644;&#35828;&#35805;&#32773;&#30340;&#27169;&#20223;&#33021;&#21147;&#30340;&#21160;&#24577;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#30001;L1&#24341;&#36215;&#30340;&#35828;&#35805;&#32773;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23402;&#29983;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#32467;&#26500;&#65292;&#29992;&#20110;&#27979;&#37327;L2-L2&#20132;&#20114;&#20013;&#35821;&#38899;&#38899;&#36136;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;20&#21517;&#27597;&#35821;&#20026;&#26031;&#27931;&#20240;&#20811;&#35821;&#30340;&#33521;&#35821;&#23398;&#20064;&#32773;&#26469;&#25193;&#23637;&#20132;&#26367;&#38405;&#35835;&#20219;&#21153;&#65288;ART&#65289;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#35813;&#27169;&#22411;&#65292;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#27597;&#35821;&#32452;&#65292;&#21363;&#24847;&#22823;&#21033;&#35821;&#65288;9&#23545;&#65289;&#65292;&#27861;&#35821;&#65288;10&#23545;&#65289;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#65288;10&#23545;&#65289;&#20013;&#27979;&#37327;L2&#33521;&#35821;&#35821;&#38899;&#30340;&#35821;&#38899;&#25910;&#25947;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23402;&#29983;RNN&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#35821;&#38899;&#25910;&#25947;&#21644;&#35828;&#35805;&#32773;&#30340;&#27169;&#20223;&#33021;&#21147;&#30340;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#36825;&#26159;&#19968;&#20010;&#25991;&#26412;&#26080;&#20851;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#25193;&#23637;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#30001;L1&#24341;&#36215;&#30340;&#35828;&#35805;&#32773;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phonetic convergence describes the automatic and unconscious speech adaptation of two interlocutors in a conversation. This paper proposes a Siamese recurrent neural network (RNN) architecture to measure the convergence of the holistic spectral characteristics of speech sounds in an L2-L2 interaction. We extend an alternating reading task (the ART) dataset by adding 20 native Slovak L2 English speakers. We train and test the Siamese RNN model to measure phonetic convergence of L2 English speech from three different native language groups: Italian (9 dyads), French (10 dyads) and Slovak (10 dyads). Our results indicate that the Siamese RNN model effectively captures the dynamics of phonetic convergence and the speaker's imitation ability. Moreover, this text-independent model is scalable and capable of handling L1-induced speaker variability.
&lt;/p&gt;</description></item><item><title>&#22240;&#26524;&#31639;&#27861;&#34917;&#25937;&#38656;&#35201;&#32435;&#20837;&#26102;&#38388;&#32500;&#24230;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#21512;&#29702;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05082</link><description>&lt;p&gt;
&#22240;&#26524;&#31639;&#27861;&#34917;&#25937;&#20013;&#26102;&#38388;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Time in Causal Algorithmic Recourse. (arXiv:2306.05082v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05082
&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#31639;&#27861;&#34917;&#25937;&#38656;&#35201;&#32435;&#20837;&#26102;&#38388;&#32500;&#24230;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#21512;&#29702;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#34917;&#25937;&#22312;&#20915;&#31574;&#21046;&#23450;&#20013;&#20855;&#26377;&#23454;&#36341;&#24847;&#20041;&#65292;&#21487;&#20197;&#25552;&#20379;&#26377;&#21033;&#20110;&#25913;&#21464;&#19981;&#21033;&#20915;&#31574;&#30340;&#23454;&#29616;&#26041;&#26696;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26377;&#20102;&#25913;&#36827;&#65292;&#20294;&#26080;&#27861;&#32435;&#20837;&#26102;&#38388;&#32500;&#24230;&#20173;&#28982;&#26159;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30340;&#37325;&#22823;&#23616;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26102;&#38388;&#32500;&#24230;&#32435;&#20837;&#22240;&#26524;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#20013;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#21512;&#29702;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Algorithmic Recourse in decision-making is a promising field that offers practical solutions to reverse unfavorable decisions. However, the inability of these methods to consider potential dependencies among variables poses a significant challenge due to the assumption of feature independence. Recent advancements have incorporated knowledge of causal dependencies, thereby enhancing the quality of the recommended recourse actions. Despite these improvements, the inability to incorporate the temporal dimension remains a significant limitation of these approaches. This is particularly problematic as identifying and addressing the root causes of undesired outcomes requires understanding time-dependent relationships between variables. In this work, we motivate the need to integrate the temporal dimension into causal algorithmic recourse methods to enhance recommendations' plausibility and reliability. The experimental evaluation highlights the significance of the role of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05079</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25200;&#21160;&#28155;&#21152;&#21040;&#23433;&#20840;&#24615;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#30340;&#20195;&#30721;&#25551;&#36848;&#20013;&#30340;&#26041;&#27861;&#65292;&#21363;&#26469;&#33258;&#21892;&#24847;&#24320;&#21457;&#32773;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#65288;NL&#65289;&#65292;&#24182;&#20998;&#26512;&#20102;&#25200;&#21160;&#22914;&#20309;&#20197;&#21450;&#22312;&#20160;&#20040;&#31243;&#24230;&#19978;&#24433;&#21709;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NL&#25551;&#36848;&#20013;&#30340;&#25200;&#21160;&#39640;&#24230;&#24433;&#21709;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26041;&#27861;&#25191;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#21363;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a method to add perturbations to the code descriptions, i.e., new inputs in natural language (NL) from well-intentioned developers, in the context of security-oriented code, and analyze how and to what extent perturbations affect the performance of AI offensive code generators. Our experiments show that the performance of the code generators is highly affected by perturbations in the NL descriptions. To enhance the robustness of the code generators, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#35299;&#20266;&#21464;&#24322;&#65292;&#21253;&#25324;&#34394;&#20551;&#24615;&#22270;&#34920;&#21644;&#29702;&#35770;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30495;&#20551;&#30456;&#20851;&#20851;&#31995;&#19978;&#24110;&#21161;&#21306;&#20998;&#30452;&#25509;&#25928;&#24212;&#21644;&#38388;&#25509;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.05071</link><description>&lt;p&gt;
&#20998;&#35299;&#20266;&#21464;&#24322;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Causal Framework for Decomposing Spurious Variations. (arXiv:2306.05071v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#35299;&#20266;&#21464;&#24322;&#65292;&#21253;&#25324;&#34394;&#20551;&#24615;&#22270;&#34920;&#21644;&#29702;&#35770;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30495;&#20551;&#30456;&#20851;&#20851;&#31995;&#19978;&#24110;&#21161;&#21306;&#20998;&#30452;&#25509;&#25928;&#24212;&#21644;&#38388;&#25509;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#20013;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#25361;&#25112;&#26159;&#35299;&#37322;&#20026;&#20160;&#20040;&#20107;&#24773;&#20197;&#29305;&#23450;&#30340;&#26041;&#24335;&#21457;&#29983;&#65292;&#25110;&#36890;&#36807;&#21738;&#20123;&#26426;&#21046;&#26576;&#20010;&#21464;&#37327;$X$&#23545;&#21478;&#19968;&#20010;&#21464;&#37327;$Y$&#26045;&#21152;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#39532;&#23572;&#21487;&#22827;&#21644;&#21322;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#20013;&#20998;&#35299;&#20266;&#21464;&#24322;&#65292;&#24341;&#20837;&#20102;&#34394;&#20551;&#24615;&#22270;&#34920;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#20266;&#25928;&#24212;&#20998;&#35299;&#20026;&#30452;&#25509;&#25928;&#24212;&#21644;&#38388;&#25509;&#25928;&#24212;&#65292;&#20197;&#35782;&#21035;&#29983;&#25104;&#36825;&#20123;&#34394;&#20551;&#20851;&#31995;&#30340;&#22522;&#30784;&#22240;&#26524;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental challenges found throughout the data sciences is to explain why things happen in specific ways, or through which mechanisms a certain variable $X$ exerts influences over another variable $Y$. In statistics and machine learning, significant efforts have been put into developing machinery to estimate correlations across variables efficiently. In causal inference, a large body of literature is concerned with the decomposition of causal effects under the rubric of mediation analysis. However, many variations are spurious in nature, including different phenomena throughout the applied sciences. Despite the statistical power to estimate correlations and the identification power to decompose causal effects, there is still little understanding of the properties of spurious associations and how they can be decomposed in terms of the underlying causal mechanisms. In this manuscript, we develop formal tools for decomposing spurious variations in both Markovian and Semi-Mark
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#35265;&#23545;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#25277;&#26679;&#20559;&#24046;&#30340;&#21464;&#20307;&#65306;&#26679;&#26412;&#37327;&#20559;&#24046;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#20559;&#24046;&#65292;&#25581;&#31034;&#27495;&#35270;&#21487;&#20197;&#20998;&#35299;&#20026;&#26041;&#24046;&#12289;&#20559;&#24046;&#21644;&#22122;&#22768;&#65292;&#24182;&#25361;&#25112;&#20102;&#36890;&#24120;&#34987;&#25509;&#21463;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05068</link><description>&lt;p&gt;
&#25581;&#31034;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#21644;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Shedding light on underrepresentation and Sampling Bias in machine learning. (arXiv:2306.05068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05068
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#35265;&#23545;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#25277;&#26679;&#20559;&#24046;&#30340;&#21464;&#20307;&#65306;&#26679;&#26412;&#37327;&#20559;&#24046;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#20559;&#24046;&#65292;&#25581;&#31034;&#27495;&#35270;&#21487;&#20197;&#20998;&#35299;&#20026;&#26041;&#24046;&#12289;&#20559;&#24046;&#21644;&#22122;&#22768;&#65292;&#24182;&#25361;&#25112;&#20102;&#36890;&#24120;&#34987;&#25509;&#21463;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#34913;&#37327;&#27495;&#35270;&#23545;&#20110;&#24544;&#23454;&#22320;&#35780;&#20272;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#20844;&#27491;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23384;&#22312;&#20219;&#20309;&#27979;&#37327;&#27495;&#35270;&#30340;&#20559;&#35265;&#37117;&#20250;&#23548;&#33268;&#29616;&#26377;&#24046;&#36317;&#30340;&#25918;&#22823;&#25110;&#20302;&#20272;&#12290;&#23384;&#22312;&#20960;&#31181;&#20559;&#35265;&#26469;&#28304;&#65292;&#20551;&#35774;&#26426;&#22120;&#23398;&#20064;&#23548;&#33268;&#30340;&#20559;&#35265;&#22312;&#19981;&#21516;&#30340;&#32676;&#20307;&#65288;&#20363;&#22914;&#22899;&#24615;&#19982;&#30007;&#24615;&#12289;&#30333;&#20154;&#19982;&#40657;&#20154;&#31561;&#65289;&#20043;&#38388;&#24179;&#31561;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20559;&#35265;&#22312;&#19981;&#21516;&#30340;&#32676;&#20307;&#20013;&#20998;&#21035;&#23384;&#22312;&#65292;&#21487;&#33021;&#20250;&#21152;&#21095;&#23545;&#29305;&#23450;&#20122;&#32676;&#20307;&#30340;&#27495;&#35270;&#12290;&#25277;&#26679;&#20559;&#24046;&#26159;&#25991;&#29486;&#20013;&#25551;&#36848;&#30001;&#25277;&#26679;&#31243;&#24207;&#24341;&#36215;&#30340;&#20559;&#24046;&#30340;&#26415;&#35821;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#24341;&#20837;&#26126;&#30830;&#23450;&#20041;&#30340;&#25277;&#26679;&#20559;&#24046;&#21464;&#20307;&#65292;&#21363;&#26679;&#26412;&#37327;&#20559;&#24046;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#20559;&#24046;&#65292;&#26469;&#28040;&#38500;&#36825;&#20010;&#26415;&#35821;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#27495;&#35270;&#20998;&#35299;&#20026;&#26041;&#24046;&#12289;&#20559;&#24046;&#21644;&#22122;&#22768;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#36890;&#24120;&#34987;&#25509;&#21463;&#30340;&#32531;&#35299;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#8220;&#31616;&#21333;&#22320;&#8221;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#21024;&#38500;&#25935;&#24863;&#23646;&#24615;&#65288;&#20363;&#22914;&#31181;&#26063;&#25110;&#24615;&#21035;&#65289;&#26469;&#35299;&#20915;&#27495;&#35270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately measuring discrimination is crucial to faithfully assessing fairness of trained machine learning (ML) models. Any bias in measuring discrimination leads to either amplification or underestimation of the existing disparity. Several sources of bias exist and it is assumed that bias resulting from machine learning is born equally by different groups (e.g. females vs males, whites vs blacks, etc.). If, however, bias is born differently by different groups, it may exacerbate discrimination against specific sub-populations. Sampling bias, is inconsistently used in the literature to describe bias due to the sampling procedure. In this paper, we attempt to disambiguate this term by introducing clearly defined variants of sampling bias, namely, sample size bias (SSB) and underrepresentation bias (URB). We show also how discrimination can be decomposed into variance, bias, and noise. Finally, we challenge the commonly accepted mitigation approach that discrimination can be addressed b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#35270;&#35273;Transformer&#20013;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#30340;&#25552;&#21319;&#65292;&#24182;&#30830;&#23450;&#20102;&#25552;&#31034;&#35760;&#21495;&#25554;&#20837;&#21518;&#32493;&#22270;&#22359;&#32780;&#38750;&#31532;&#19968;&#20010;&#22270;&#22359;&#30340;&#26368;&#20339;&#20301;&#32622;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#20943;&#23569;&#30830;&#23450;&#26368;&#20339;&#25552;&#31034;&#35760;&#21495;&#22359;&#20301;&#32622;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.05067</link><description>&lt;p&gt;
&#25552;&#39640;&#33258;&#30417;&#30563;&#35270;&#35273;Transformer&#21487;&#35270;&#21270;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Visual Prompt Tuning for Self-supervised Vision Transformers. (arXiv:2306.05067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#35270;&#35273;Transformer&#20013;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#30340;&#25552;&#21319;&#65292;&#24182;&#30830;&#23450;&#20102;&#25552;&#31034;&#35760;&#21495;&#25554;&#20837;&#21518;&#32493;&#22270;&#22359;&#32780;&#38750;&#31532;&#19968;&#20010;&#22270;&#22359;&#30340;&#26368;&#20339;&#20301;&#32622;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#20943;&#23569;&#30830;&#23450;&#26368;&#20339;&#25552;&#31034;&#35760;&#21495;&#22359;&#20301;&#32622;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#30340;&#26377;&#25928;&#35843;&#25972;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#39069;&#22806;&#30340;&#21487;&#23398;&#20064;&#35760;&#21495;&#65292;&#31216;&#20026;&#25552;&#31034;&#65292;&#26469;&#25351;&#23548;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;ViTs&#12290;&#34429;&#28982;VPT&#22312;&#21463;&#30417;&#30563;&#30340;&#35270;&#35273;Transformer&#20013;&#26174;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#65292;&#20294;&#22312;&#33258;&#30417;&#30563;&#24773;&#20917;&#19979;&#24120;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#23454;&#35777;&#35266;&#23519;&#65292;&#25105;&#20204;&#25512;&#26029;VPT&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25552;&#31034;&#35760;&#21495;&#19982;ViT&#22270;&#22359;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#25552;&#31034;&#35760;&#21495;&#25554;&#20837;&#21518;&#32493;&#22270;&#22359;&#32780;&#19981;&#26159;&#31532;&#19968;&#20010;&#22270;&#22359;&#26102;&#65292;VPT&#22312;MAE&#21644;MoCo v3&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#35266;&#23519;&#34920;&#26126;&#65292;&#23384;&#22312;&#36866;&#29992;&#20110;&#25554;&#20837;&#25552;&#31034;&#35760;&#21495;&#30340;&#26368;&#20339;&#22359;&#30340;&#20301;&#32622;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30830;&#23450;&#27599;&#20010;&#33258;&#30417;&#30563;ViT&#20869;&#29992;&#20110;&#19981;&#21516;&#26410;&#26469;&#22330;&#26223;&#30340;&#25552;&#31034;&#30340;&#26368;&#20339;&#22359;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Prompt Tuning (VPT) is an effective tuning method for adapting pretrained Vision Transformers (ViTs) to downstream tasks. It leverages extra learnable tokens, known as prompts, which steer the frozen pretrained ViTs. Although VPT has demonstrated its applicability with supervised vision transformers, it often underperforms with self-supervised ones. Through empirical observations, we deduce that the effectiveness of VPT hinges largely on the ViT blocks with which the prompt tokens interact. Specifically, VPT shows improved performance on image classification tasks for MAE and MoCo v3 when the prompt tokens are inserted into later blocks rather than the first block. These observations suggest that there exists an optimal location of blocks for the insertion of prompt tokens. Unfortunately, identifying the optimal blocks for prompts within each self-supervised ViT for diverse future scenarios is a costly process. To mitigate this problem, we propose a simple yet effective method t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#20915;&#31574;&#20219;&#21153;&#21483;&#20570;&#32467;&#26524;&#25511;&#21046;&#65292;&#38024;&#23545;&#28041;&#21450;&#21040;&#21009;&#20107;&#21496;&#27861;&#12289;&#31119;&#21033;&#12289;&#20020;&#24202;&#20915;&#31574;&#20197;&#21450;&#20844;&#20849;&#21355;&#29983;&#31561;&#22810;&#20010;&#26041;&#38754;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#20844;&#24179;&#30340;&#27010;&#24565;&#20197;&#21450;&#19968;&#32452;&#22240;&#26524;&#24037;&#20855;&#21644;&#25216;&#26415;&#26469;&#25512;&#26029;&#32467;&#26524;&#25511;&#21046;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05066</link><description>&lt;p&gt;
&#32467;&#26524;&#25511;&#21046;&#30340;&#22240;&#26524;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Fairness for Outcome Control. (arXiv:2306.05066v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#20915;&#31574;&#20219;&#21153;&#21483;&#20570;&#32467;&#26524;&#25511;&#21046;&#65292;&#38024;&#23545;&#28041;&#21450;&#21040;&#21009;&#20107;&#21496;&#27861;&#12289;&#31119;&#21033;&#12289;&#20020;&#24202;&#20915;&#31574;&#20197;&#21450;&#20844;&#20849;&#21355;&#29983;&#31561;&#22810;&#20010;&#26041;&#38754;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#20844;&#24179;&#30340;&#27010;&#24565;&#20197;&#21450;&#19968;&#32452;&#22240;&#26524;&#24037;&#20855;&#21644;&#25216;&#26415;&#26469;&#25512;&#26029;&#32467;&#26524;&#25511;&#21046;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20250;&#21521;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#22522;&#30784;&#35774;&#26045;&#30340;&#36807;&#28193;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26366;&#30001;&#20154;&#31867;&#25511;&#21046;&#30340;&#20915;&#31574;&#29616;&#22312;&#34987;&#22996;&#25176;&#32473;&#20102;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#21457;&#23637;&#20351;&#31038;&#20250;&#30340;&#21508;&#20010;&#26041;&#38754;&#26356;&#26377;&#25928;&#29575;&#65292;&#20294;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#38656;&#35201;&#38750;&#24120;&#23567;&#24515;&#22320;&#20351;&#36825;&#31181;&#33258;&#21160;&#21270;&#20915;&#31574;&#31995;&#32479;&#21464;&#24471;&#20844;&#24179;&#21644;&#20844;&#27491;&#65292;&#21363;&#32771;&#34385;&#21040;&#35832;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#31561;&#25935;&#24863;&#23646;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#20915;&#31574;&#20219;&#21153;&#65292;&#31216;&#20026;&#32467;&#26524;&#25511;&#21046;&#65292;&#20854;&#20013;&#33258;&#21160;&#21270;&#31995;&#32479;&#26088;&#22312;&#20248;&#21270;&#19968;&#20010;&#32467;&#26524;&#21464;&#37327;Y&#65292;&#21516;&#26102;&#20445;&#25345;&#20844;&#24179;&#21644;&#20844;&#27491;&#12290;&#23545;&#20110;&#36825;&#26679;&#19968;&#20010;&#35774;&#32622;&#30340;&#20852;&#36259;&#33539;&#22260;&#20174;&#19982;&#21009;&#20107;&#21496;&#27861;&#21644;&#31119;&#21033;&#26377;&#20851;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#19968;&#30452;&#21040;&#20020;&#24202;&#20915;&#31574;&#21644;&#20844;&#20849;&#21355;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#38236;&#29255;&#39318;&#20808;&#20998;&#26512;&#20102;&#21033;&#30410;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#25429;&#25417;&#20102;&#19968;&#20010;&#29305;&#23450;&#20010;&#20307;&#20174;&#31215;&#26497;&#20915;&#31574;&#20013;&#33719;&#24471;&#20102;&#22810;&#23569;&#22909;&#22788;&#65292;&#23545;&#29031;&#20107;&#23454;&#30340;&#20844;&#24179;&#24615;&#65292;&#25429;&#25417;&#20102;&#22914;&#26524;&#28041;&#21450;&#21040;&#19981;&#21516;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#38382;&#39064;&#25152;&#22312;&#65292;&#20197;&#21450;&#20805;&#20998;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#25429;&#25417;&#20102;&#38656;&#35201;&#22810;&#23569;&#24178;&#39044;&#26469;&#25913;&#21892;&#22240;&#26524;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#32452;&#22240;&#26524;&#24037;&#20855;&#21644;&#25216;&#26415;&#26469;&#25512;&#26029;&#32467;&#26524;&#25511;&#21046;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#25511;&#21046;&#20102;&#36151;&#27454;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#19981;&#20844;&#24179;&#27495;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
As society transitions towards an AI-based decision-making infrastructure, an ever-increasing number of decisions once under control of humans are now delegated to automated systems. Even though such developments make various parts of society more efficient, a large body of evidence suggests that a great deal of care needs to be taken to make such automated decision-making systems fair and equitable, namely, taking into account sensitive attributes such as gender, race, and religion. In this paper, we study a specific decision-making task called outcome control in which an automated system aims to optimize an outcome variable $Y$ while being fair and equitable. The interest in such a setting ranges from interventions related to criminal justice and welfare, all the way to clinical decision-making and public health. In this paper, we first analyze through causal lenses the notion of benefit, which captures how much a specific individual would benefit from a positive decision, counterfac
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;ODiMO&#65292;&#19968;&#31181;&#30828;&#20214;&#24863;&#30693;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#31934;&#32454;&#26144;&#23556;&#23558;DNN&#20998;&#21106;&#24182;&#22312;&#19981;&#21516;&#21152;&#36895;&#22120;&#19978;&#24182;&#34892;&#25191;&#34892;&#65292;&#20197;&#22312;&#32500;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#38477;&#20302;&#25512;&#26029;&#30340;&#33021;&#32791;&#21644;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2306.05060</link><description>&lt;p&gt;
&#38754;&#21521;DNN&#25512;&#26029;&#30340;&#22810;&#21152;&#36895;&#22120;&#24179;&#21488;&#19978;&#31934;&#24230;&#24863;&#30693;&#30340;&#24310;&#36831;&#21644;&#33021;&#37327;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Precision-aware Latency and Energy Balancing on Multi-Accelerator Platforms for DNN Inference. (arXiv:2306.05060v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05060
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;ODiMO&#65292;&#19968;&#31181;&#30828;&#20214;&#24863;&#30693;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#31934;&#32454;&#26144;&#23556;&#23558;DNN&#20998;&#21106;&#24182;&#22312;&#19981;&#21516;&#21152;&#36895;&#22120;&#19978;&#24182;&#34892;&#25191;&#34892;&#65292;&#20197;&#22312;&#32500;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#38477;&#20302;&#25512;&#26029;&#30340;&#33021;&#32791;&#21644;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#38656;&#35201;&#20302;&#24310;&#36831;&#21644;&#20302;&#21151;&#32791;&#22320;&#25191;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#65292;&#36825;&#20419;&#20351;&#20102;&#26032;&#30340;&#24322;&#26500;&#29255;&#19978;&#31995;&#32479;(System-on-Chips, SoCs)&#30340;&#24320;&#21457;&#65292;&#20854;&#23553;&#35013;&#20102;&#21508;&#31181;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#22914;&#20309;&#26368;&#20248;&#22320;&#23558;DNN&#26144;&#23556;&#21040;&#36825;&#26679;&#30340;&#22810;&#21152;&#36895;&#22120;&#31995;&#32479;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;ODiMO&#65292;&#19968;&#31181;&#30828;&#20214;&#24863;&#30693;&#30340;&#24037;&#20855;&#65292;&#23427;&#22312;&#29255;&#19978;&#30340;&#19981;&#21516;&#21152;&#36895;&#22120;&#20043;&#38388;&#25191;&#34892;&#32454;&#31890;&#24230;&#26144;&#23556;&#65292;&#20998;&#21106;&#21333;&#20010;&#23618;&#24182;&#24182;&#34892;&#25191;&#34892;&#23427;&#20204;&#65292;&#20197;&#38477;&#20302;&#25512;&#26029;&#30340;&#33021;&#37327;&#28040;&#32791;&#25110;&#24310;&#36831;&#65292;&#21516;&#26102;&#32771;&#34385;&#27599;&#20010;&#21152;&#36895;&#22120;&#30340;&#37327;&#21270;&#31934;&#24230;&#20197;&#32500;&#25252;&#20934;&#30830;&#24615;&#12290;&#22312;&#19977;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;/DNN&#23545;&#19978;&#36861;&#27714;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#25110;&#24310;&#36831;&#31354;&#38388;&#20013;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#32593;&#32476;&#65292;&#24182;&#37096;&#32626;&#22312;DIANA&#24322;&#26500;&#36229;&#20302;&#21151;&#32791;&#36793;&#32536;AI SoC&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ODiMO&#30456;&#23545;&#20110;&#25163;&#21160;&#21551;&#21457;&#24335;&#26144;&#23556;&#21487;&#20197;&#38477;&#20302;&#33021;&#37327;/&#24310;&#36831;&#39640;&#36798;33%/31%&#65292;&#32780;&#20934;&#30830;&#24230;&#19979;&#38477;&#26377;&#38480;(-0.53%/-0.32%)&#12290;
&lt;/p&gt;
&lt;p&gt;
The need to execute Deep Neural Networks (DNNs) at low latency and low power at the edge has spurred the development of new heterogeneous Systems-on-Chips (SoCs) encapsulating a diverse set of hardware accelerators. How to optimally map a DNN onto such multi-accelerator systems is an open problem. We propose ODiMO, a hardware-aware tool that performs a fine-grain mapping across different accelerators on-chip, splitting individual layers and executing them in parallel, to reduce inference energy consumption or latency, while taking into account each accelerator's quantization precision to maintain accuracy. Pareto-optimal networks in the accuracy vs. energy or latency space are pursued for three popular dataset/DNN pairs, and deployed on the DIANA heterogeneous ultra-low power edge AI SoC. We show that ODiMO reduces energy/latency by up to 33%/31% with limited accuracy drop (-0.53%/-0.32%) compared to manual heuristic mappings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#36991;&#20813;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;HAR&#30340;NeSy&#26041;&#27861;&#22312;&#37096;&#32626;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05058</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Approaches for Context-Aware Human Activity Recognition. (arXiv:2306.05058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#36991;&#20813;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;HAR&#30340;NeSy&#26041;&#27861;&#22312;&#37096;&#32626;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#20256;&#24863;&#22120;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#26631;&#20934;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#37096;&#32626;&#24120;&#24120;&#21463;&#21040;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#21644;&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#30340;&#38480;&#21046;&#12290;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NeSy&#65289;&#20026;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#23558;&#20851;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#30693;&#35782;&#27880;&#20837;&#21040;HAR&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;HAR&#30340;NeSy&#26041;&#27861;&#38656;&#35201;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#35745;&#31639;&#22797;&#26434;&#30340;&#31526;&#21495;&#25512;&#29702;&#22120;&#65292;&#20351;&#23427;&#20204;&#19981;&#22826;&#36866;&#21512;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#65288;&#20363;&#22914;&#31227;&#21160;&#35774;&#22791;&#65289;&#19978;&#37096;&#32626;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;HAR&#30340;NeSy&#26041;&#27861;&#20174;&#26410;&#22312;&#37326;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#20063;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#27880;&#20837;&#30693;&#35782;&#32422;&#26463;&#26469;&#36991;&#20813;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#25163;&#31295;&#21644;
&lt;/p&gt;
&lt;p&gt;
Deep Learning models are a standard solution for sensor-based Human Activity Recognition (HAR), but their deployment is often limited by labeled data scarcity and models' opacity. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate these issues by infusing knowledge about context information into HAR deep learning classifiers. However, existing NeSy methods for context-aware HAR require computationally expensive symbolic reasoners during classification, making them less suitable for deployment on resource-constrained devices (e.g., mobile devices). Additionally, NeSy approaches for context-aware HAR have never been evaluated on in-the-wild datasets, and their generalization capabilities in real-world scenarios are questionable. In this work, we propose a novel approach based on a semantic loss function that infuses knowledge constraints in the HAR model during the training phase, avoiding symbolic reasoning during classification. Our results on scripted and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24133;&#20540;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#21098;&#26525;&#26041;&#27861;(MAP)&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#21069;&#21521;&#21644;&#21453;&#21521;&#36335;&#24452;&#20013;&#24212;&#29992;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#21160;&#24577;&#22320;&#25506;&#32034;&#31232;&#30095;&#27169;&#22411;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20174;&#20887;&#20313;&#21040;&#26377;&#25928;&#30340;&#31232;&#30095;&#32593;&#32476;&#30340;&#26080;&#32541;&#36716;&#25442;&#65292;&#24471;&#21040;&#20102;&#26082;&#20855;&#26377;&#39640;&#24615;&#33021;&#21448;&#32463;&#36807;&#31934;&#31616;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05056</link><description>&lt;p&gt;
&#22522;&#20110;&#24133;&#20540;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Magnitude Attention-based Dynamic Pruning. (arXiv:2306.05056v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24133;&#20540;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#21098;&#26525;&#26041;&#27861;(MAP)&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#21069;&#21521;&#21644;&#21453;&#21521;&#36335;&#24452;&#20013;&#24212;&#29992;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#21160;&#24577;&#22320;&#25506;&#32034;&#31232;&#30095;&#27169;&#22411;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20174;&#20887;&#20313;&#21040;&#26377;&#25928;&#30340;&#31232;&#30095;&#32593;&#32476;&#30340;&#26080;&#32541;&#36716;&#25442;&#65292;&#24471;&#21040;&#20102;&#26082;&#20855;&#26377;&#39640;&#24615;&#33021;&#21448;&#32463;&#36807;&#31934;&#31616;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21098;&#26525;&#26041;&#27861;&#21482;&#20250;&#26681;&#25454;&#29305;&#23450;&#26631;&#20934;&#21033;&#29992;&#27599;&#20010;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#26469;&#25628;&#32034;&#31232;&#30095;&#32467;&#26500;&#65292;&#20294;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#20250;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#24133;&#20540;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#21098;&#26525;&#26041;&#27861;(MAP)&#12290;&#35813;&#26041;&#27861;&#22312;&#21069;&#21521;&#21644;&#21453;&#21521;&#36335;&#24452;&#20013;&#24212;&#29992;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#21160;&#24577;&#22320;&#25506;&#32034;&#31232;&#30095;&#27169;&#22411;&#32467;&#26500;&#12290;&#36890;&#36807;&#22522;&#20110;&#26435;&#37325;&#30340;&#24133;&#20540;&#23450;&#20041;&#24133;&#20540;&#27880;&#24847;&#21147;&#65292;&#20197;&#36830;&#32493;&#23454;&#20540;&#25968;&#30340;&#24418;&#24335;&#20351;&#24471;&#20174;&#20887;&#20313;&#21040;&#26377;&#25928;&#30340;&#31232;&#30095;&#32593;&#32476;&#30340;&#26080;&#32541;&#36716;&#25442;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#30830;&#20445;&#20102;&#22312;&#31232;&#30095;&#32593;&#32476;&#20013;&#23545;&#37325;&#35201;&#23618;&#30340;&#26356;&#26377;&#25928;&#30340;&#26356;&#26032;&#12290;&#22312;&#35757;&#32451;&#30340;&#21518;&#26399;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#25506;&#32034;&#36716;&#21521;&#20102;&#24320;&#21457;&#65292;&#20165;&#22522;&#20110;&#25152;&#25506;&#32034;&#20986;&#30340;&#32467;&#26500;&#26356;&#26032;&#30001;&#37325;&#35201;&#26435;&#37325;&#32452;&#25104;&#30340;&#31232;&#30095;&#27169;&#22411;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#26082;&#20855;&#26377;&#39640;&#24615;&#33021;&#21448;&#32463;&#36807;&#31934;&#31616;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing pruning methods utilize the importance of each weight based on specified criteria only when searching for a sparse structure but do not utilize it during training. In this work, we propose a novel approach \textbf{M}agnitude \textbf{A}ttention-based Dynamic \textbf{P}runing (MAP) method, which applies the importance of weights throughout both the forward and backward paths to explore sparse model structures dynamically. Magnitude attention is defined based on the magnitude of weights as continuous real-valued numbers enabling a seamless transition from a redundant to an effective sparse network by promoting efficient exploration. Additionally, the attention mechanism ensures more effective updates for important layers within the sparse network. In later stages of training, our approach shifts from exploration to exploitation, exclusively updating the sparse model composed of crucial weights based on the explored structure, resulting in pruned models that not only achieve per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;OGRS&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#27969;&#22330;&#26223;&#19979;&#22312;&#32447;&#23398;&#20064;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#38382;&#39064;&#65292;&#33021;&#22815;&#33258;&#21160;&#36873;&#25321;&#19981;&#21516;&#24178;&#20928;&#27604;&#29575;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#26080;&#38656;&#25913;&#21464;&#21442;&#25968;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2306.05046</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#22312;&#32447;&#40065;&#26834;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65306;&#32771;&#34385;&#22122;&#22768;&#26631;&#31614;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
A Gradient-based Approach for Online Robust Deep Neural Network Training with Noisy Labels. (arXiv:2306.05046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;OGRS&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#27969;&#22330;&#26223;&#19979;&#22312;&#32447;&#23398;&#20064;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#38382;&#39064;&#65292;&#33021;&#22815;&#33258;&#21160;&#36873;&#25321;&#19981;&#21516;&#24178;&#20928;&#27604;&#29575;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#26080;&#38656;&#25913;&#21464;&#21442;&#25968;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23398;&#20064;&#26159;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#25193;&#23637;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#20294;&#26159;&#24456;&#23569;&#26377;&#20043;&#21069;&#30340;&#30740;&#31350;&#32771;&#34385;&#22312;&#25968;&#25454;&#27969;&#22330;&#26223;&#19979;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#27169;&#22411;&#23398;&#20064;&#21442;&#25968;&#30340;&#22122;&#22768;&#26631;&#31614;&#38382;&#39064;&#65292;&#31216;&#20026;Online Gradient-based Robust Selection&#65288;OGRS&#65289;&#12290;&#19982;&#20351;&#29992;&#31163;&#32447;&#35757;&#32451;&#26679;&#26412;&#36873;&#25321;&#25152;&#38656;&#30340;&#22312;&#27599;&#27425;&#35757;&#32451;&#20043;&#21069;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#24178;&#20928;&#27604;&#29575;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;OGRS&#21487;&#20197;&#22312;&#28176;&#21464;&#26356;&#26032;&#27493;&#39588;&#20013;&#33258;&#21160;&#36873;&#25321;&#19981;&#21516;&#24178;&#20928;&#27604;&#29575;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#21442;&#25968;&#35774;&#32622;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;OGRS&#26041;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36873;&#25321;&#24178;&#20928;&#26679;&#26412;&#65292;&#24182;&#23558;&#25152;&#36873;&#26679;&#26412;&#39304;&#36865;&#21040;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25968;&#25454;&#36873;&#25321;&#36807;&#31243;&#27491;&#22312;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with noisy labels is an important topic for scalable training in many real-world scenarios. However, few previous research considers this problem in the online setting, where the arrival of data is streaming. In this paper, we propose a novel gradient-based approach to enable the detection of noisy labels for the online learning of model parameters, named Online Gradient-based Robust Selection (OGRS). In contrast to the previous sample selection approach for the offline training that requires the estimation of a clean ratio of the dataset before each epoch of training, OGRS can automatically select clean samples by steps of gradient update from datasets with varying clean ratios without changing the parameter setting. During the training process, the OGRS method selects clean samples at each iteration and feeds the selected sample to incrementally update the model parameters. We provide a detailed theoretical analysis to demonstrate data selection process is converging to the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimeDiff&#30340;&#38750;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#36136;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#26426;&#21046; - &#26410;&#26469;&#28151;&#21512;&#21644;&#33258;&#22238;&#24402;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TimeDiff&#22312;&#21508;&#31181;&#24378;&#22522;&#32447;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05043</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#33258;&#22238;&#24402;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive Conditional Diffusion Models for Time Series Prediction. (arXiv:2306.05043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimeDiff&#30340;&#38750;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#36136;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#26426;&#21046; - &#26410;&#26469;&#28151;&#21512;&#21644;&#33258;&#22238;&#24402;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TimeDiff&#22312;&#21508;&#31181;&#24378;&#22522;&#32447;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#36866;&#24212;&#20854;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#26469;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TimeDiff&#65292;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#26426;&#21046; - &#26410;&#26469;&#28151;&#21512;&#21644;&#33258;&#22238;&#24402;&#21021;&#22987;&#21270;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#31867;&#20284;&#20110;teacher forcing&#65292;&#26410;&#26469;&#28151;&#21512;&#20801;&#35768;&#20351;&#29992;&#37096;&#20998;&#30495;&#23454;&#26410;&#26469;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#26465;&#20214;&#65292;&#32780;&#33258;&#22238;&#24402;&#21021;&#22987;&#21270;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#21021;&#22987;&#21270;&#27169;&#22411;&#24182;&#33719;&#24471;&#22522;&#26412;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#65292;&#22914;&#30701;&#26399;&#36235;&#21183;&#12290;&#22312;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TimeDiff&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#29616;&#26377;&#30340;&#24378;&#22522;&#32447;&#27169;&#22411;&#65288;&#21253;&#25324;transformers&#21644;FiLM&#65289;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, denoising diffusion models have led to significant breakthroughs in the generation of images, audio and text. However, it is still an open question on how to adapt their strong modeling ability to model time series. In this paper, we propose TimeDiff, a non-autoregressive diffusion model that achieves high-quality time series prediction with the introduction of two novel conditioning mechanisms: future mixup and autoregressive initialization. Similar to teacher forcing, future mixup allows parts of the ground-truth future predictions for conditioning, while autoregressive initialization helps better initialize the model with basic time series patterns such as short-term trends. Extensive experiments are performed on nine real-world datasets. Results show that TimeDiff consistently outperforms existing time series diffusion models, and also achieves the best overall performance across a variety of the existing strong baselines (including transformers and FiLM).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#29983;&#25104;&#36890;&#20449;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#29983;&#25104;&#22411;&#29992;&#25143;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#26469;&#21019;&#24314;&#26412;&#22320;&#22270;&#20687;&#65292;&#20174;&#32780;&#20943;&#23569;&#22522;&#31449;&#19979;&#34892;&#20256;&#36755;&#33021;&#37327;&#28040;&#32791;&#65292;&#20294;&#20250;&#22686;&#21152;&#29992;&#25143;&#33021;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#29983;&#25104;&#29992;&#25143;&#36873;&#25321;&#31639;&#27861;&#65292;&#24635;&#33021;&#32791;&#21487;&#20197;&#38477;&#20302;&#39640;&#36798;54%&#12290;</title><link>http://arxiv.org/abs/2306.05041</link><description>&lt;p&gt;
&#33021;&#28304;&#39640;&#25928;&#30340;&#19979;&#34892;&#35821;&#20041;&#29983;&#25104;&#36890;&#20449;&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-Efficient Downlink Semantic Generative Communication with Text-to-Image Generators. (arXiv:2306.05041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#29983;&#25104;&#36890;&#20449;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#29983;&#25104;&#22411;&#29992;&#25143;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#26469;&#21019;&#24314;&#26412;&#22320;&#22270;&#20687;&#65292;&#20174;&#32780;&#20943;&#23569;&#22522;&#31449;&#19979;&#34892;&#20256;&#36755;&#33021;&#37327;&#28040;&#32791;&#65292;&#20294;&#20250;&#22686;&#21152;&#29992;&#25143;&#33021;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#29983;&#25104;&#29992;&#25143;&#36873;&#25321;&#31639;&#27861;&#65292;&#24635;&#33021;&#32791;&#21487;&#20197;&#38477;&#20302;&#39640;&#36798;54%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#29983;&#25104;&#36890;&#20449; (SGC) &#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#29983;&#25104;&#22411;&#29992;&#25143;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687; (T2I) &#29983;&#25104;&#22120;&#20174;&#19979;&#36733;&#30340;&#25991;&#26412;&#25552;&#31034;&#26412;&#22320;&#21019;&#24314;&#22270;&#20687;&#65292;&#32780;&#38750;&#29983;&#25104;&#22411;&#29992;&#25143;&#30452;&#25509;&#20174;&#22522;&#31449; (BS) &#19979;&#36733;&#22270;&#20687;&#12290;&#34429;&#28982;&#29983;&#25104;&#22411;&#29992;&#25143;&#24110;&#21161;&#20943;&#23569;&#20102;&#22522;&#31449;&#19979;&#34892;&#20256;&#36755;&#33021;&#37327;&#65292;&#20294;&#23427;&#20204;&#28040;&#32791;&#20102;&#39069;&#22806;&#30340;&#33021;&#28304;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#21644;&#19978;&#20256;&#23427;&#20204;&#30340;&#29983;&#25104;&#22120;&#29366;&#24577;&#20449;&#24687; (GSI)&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#29983;&#25104;&#29992;&#25143;&#36873;&#25321;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#26368;&#23567;&#21270;&#22522;&#31449;&#21644;&#29992;&#25143;&#24635;&#33021;&#37327;&#28040;&#32791;&#30340;&#38382;&#39064;&#12290;&#20223;&#30495;&#32467;&#26524;&#35777;&#23454;&#65292;&#19982;&#25152;&#26377;&#38750;&#29983;&#25104;&#22411;&#29992;&#25143;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23558;&#24635;&#33021;&#37327;&#28040;&#32791;&#38477;&#20302;&#20102;&#39640;&#36798; 54%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel semantic generative communication (SGC) framework, where generative users leverage text-to-image (T2I) generators to create images locally from downloaded text prompts, while non-generative users directly download images from a base station (BS). Although generative users help reduce downlink transmission energy at the BS, they consume additional energy for image generation and for uploading their generator state information (GSI). We formulate the problem of minimizing the total energy consumption of the BS and the users, and devise a generative user selection algorithm. Simulation results corroborate that our proposed algorithm reduces total energy by up to 54% compared to a baseline with all non-generative users.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#21608;&#26399;-&#27880;&#24847;&#26426;&#21046;&#65292;&#21517;&#20026;Periodformer&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;Transformer-based&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#26469;&#20445;&#35777;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05035</link><description>&lt;p&gt;
&#38271;&#26399;&#24207;&#21015;&#39044;&#27979;&#26159;&#21542;&#38656;&#35201;&#22797;&#26434;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#39069;&#22806;&#30340;&#38271;&#36755;&#20837;&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Long-Term Series Forecasting Need Complex Attention and Extra Long Inputs?. (arXiv:2306.05035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#21608;&#26399;-&#27880;&#24847;&#26426;&#21046;&#65292;&#21517;&#20026;Periodformer&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;Transformer-based&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#26469;&#20445;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#38271;&#26399;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#22312;&#36817;&#24180;&#26469;&#20063;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#25152;&#22266;&#26377;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#38656;&#35201;&#38271;&#24207;&#21015;&#65292;&#23427;&#22312;LTSF&#20219;&#21153;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65306;1&#65289;&#36825;&#20123;&#26041;&#27861;&#35774;&#35745;&#30340;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#26159;&#21542;&#23454;&#38469;&#19978;&#32553;&#30701;&#20102;&#30495;&#23454;&#35774;&#22791;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#65307;2&#65289;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#38656;&#35201;&#39069;&#22806;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#26469;&#20445;&#35777;&#23427;&#20204;&#30340;&#24615;&#33021;&#65311;&#26412;&#35770;&#25991;&#30340;&#31572;&#26696;&#26159;&#21542;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21608;&#26399;-&#27880;&#24847;&#26426;&#21046;&#65288;Periodformer&#65289;&#65292;&#36890;&#36807;&#26174;&#24335;&#21608;&#26399;&#24615;&#21644;&#20869;&#32622;&#30340;&#25509;&#36817;&#24615;&#26469;&#37325;&#26032;&#35774;&#35745;&#38271;&#26399;&#23376;&#24207;&#21015;&#21644;&#30701;&#26399;&#23376;&#24207;&#21015;&#30340;&#32858;&#21512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23884;&#20837;&#20102;&#19968;&#20010;&#38376;&#25511;&#26426;&#21046;&#21040;Periodformer&#20013;&#20197;&#35843;&#25972;&#27880;&#24847;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Transformer-based models have achieved impressive performance on various time series tasks, Long-Term Series Forecasting (LTSF) tasks have also received extensive attention in recent years. However, due to the inherent computational complexity and long sequences demanding of Transformer-based methods, its application on LTSF tasks still has two major issues that need to be further investigated: 1) Whether the sparse attention mechanism designed by these methods actually reduce the running time on real devices; 2) Whether these models need extra long input sequences to guarantee their performance? The answers given in this paper are negative. Therefore, to better copy with these two issues, we design a lightweight Period-Attention mechanism (Periodformer), which renovates the aggregation of long-term subseries via explicit periodicity and short-term subseries via built-in proximity. Meanwhile, a gating mechanism is embedded into Periodformer to regulate the influence of the attention
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#30340;&#20934;&#30830;&#12289;&#36731;&#37327;&#32423;&#12289;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#8212;&#8212;SeaLog&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110; Trie &#32467;&#26500;&#30340;&#21160;&#24577;&#22686;&#38271;&#26816;&#27979;&#20195;&#29702;&#65292;&#21487;&#20197;&#25509;&#25910;&#20154;&#31867;&#19987;&#23478;&#21453;&#39304;&#65292;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#26085;&#24535;&#25968;&#25454;&#20013;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#25163;&#21160;&#39564;&#35777;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.05032</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#24102;&#26377;&#19987;&#23478;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Scalable and Adaptive Log-based Anomaly Detection with Expert in the Loop. (arXiv:2306.05032v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#30340;&#20934;&#30830;&#12289;&#36731;&#37327;&#32423;&#12289;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#8212;&#8212;SeaLog&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110; Trie &#32467;&#26500;&#30340;&#21160;&#24577;&#22686;&#38271;&#26816;&#27979;&#20195;&#29702;&#65292;&#21487;&#20197;&#25509;&#25910;&#20154;&#31867;&#19987;&#23478;&#21453;&#39304;&#65292;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#26085;&#24535;&#25968;&#25454;&#20013;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#25163;&#21160;&#39564;&#35777;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#26085;&#24535;&#22312;&#32500;&#25252;&#36719;&#20214;&#31995;&#32479;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#24050;&#26377;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#21160;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#26102;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30001;&#20110;&#39640;&#36164;&#28304;&#28040;&#32791;&#21644;&#32570;&#20047;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#26085;&#24535;&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#32780;&#38754;&#20020;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#65292;&#36731;&#37327;&#32423;&#21644;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#8212;&#8212;SeaLog&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110; Trie &#30340;&#26816;&#27979;&#20195;&#29702; (TDA)&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#21160;&#24577;&#22686;&#38271;&#30340; Trie &#32467;&#26500;&#36827;&#34892;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#12290;&#20026;&#20102;&#22686;&#24378; TDA &#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#26085;&#24535;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#20351;&#20854;&#33021;&#22815;&#20174;&#19987;&#23478;&#33719;&#24471;&#21453;&#39304;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#20195;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914; ChatGPT&#65292;&#21487;&#20197;&#25552;&#20379;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#24403;&#19968;&#33268;&#24615;&#30340;&#21453;&#39304;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#20943;&#23569;&#25163;&#21160;&#39564;&#35777;&#30340;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#22823;&#35268;&#27169;&#29615;&#22659;&#19979;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
System logs play a critical role in maintaining the reliability of software systems. Fruitful studies have explored automatic log-based anomaly detection and achieved notable accuracy on benchmark datasets. However, when applied to large-scale cloud systems, these solutions face limitations due to high resource consumption and lack of adaptability to evolving logs. In this paper, we present an accurate, lightweight, and adaptive log-based anomaly detection framework, referred to as SeaLog. Our method introduces a Trie-based Detection Agent (TDA) that employs a lightweight, dynamically-growing trie structure for real-time anomaly detection. To enhance TDA's accuracy in response to evolving log data, we enable it to receive feedback from experts. Interestingly, our findings suggest that contemporary large language models, such as ChatGPT, can provide feedback with a level of consistency comparable to human experts, which can potentially reduce manual verification efforts. We extensively 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#12289;&#38646;&#25104;&#26412;&#20195;&#29702;&#65292;&#36890;&#36807;&#32771;&#34385;&#24178;&#20928;&#22270;&#20687;&#21644;&#21463;&#25200;&#21160;&#22270;&#20687;&#30340;&#29305;&#24449;&#12289;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#24555;&#36895;&#22320;&#25628;&#32034;&#40065;&#26834;NAS&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.05031</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#30340;&#21487;&#25512;&#24191;&#36731;&#37327;&#32423;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generalizable Lightweight Proxy for Robust NAS against Diverse Perturbations. (arXiv:2306.05031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#12289;&#38646;&#25104;&#26412;&#20195;&#29702;&#65292;&#36890;&#36807;&#32771;&#34385;&#24178;&#20928;&#22270;&#20687;&#21644;&#21463;&#25200;&#21160;&#22270;&#20687;&#30340;&#29305;&#24449;&#12289;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#24555;&#36895;&#22320;&#25628;&#32034;&#40065;&#26834;NAS&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#24050;&#25104;&#21151;&#22312;&#32473;&#23450;&#26465;&#20214;&#19979;&#65288;&#20363;&#22914;&#24615;&#33021;&#25110;&#24310;&#36831;&#65289;&#25214;&#21040;&#26368;&#20339;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21482;&#38024;&#23545;&#24178;&#20928;&#22270;&#20687;&#30340;&#24615;&#33021;&#23547;&#25214;&#26368;&#20339;&#26550;&#26500;&#65292;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#25200;&#21160;&#25110;&#25439;&#22351;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#20010;&#36890;&#36807;&#23558;&#23545;&#25239;&#24615;&#35757;&#32451;&#38598;&#25104;&#21040;&#21333;&#27425;NAS&#20013;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#40065;&#26834;&#24615;NAS&#26694;&#26550;&#65292;&#20294;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#23427;&#20204;&#21482;&#32771;&#34385;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#20026;&#21333;&#20010;&#20219;&#21153;&#21457;&#29616;&#26368;&#20339;&#26550;&#26500;&#65292;&#36825;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#40065;&#26834;&#38646;&#25104;&#26412;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#22312;&#21021;&#22987;&#21270;&#29366;&#24577;&#19979;&#32771;&#34385;&#24178;&#20928;&#22270;&#20687;&#21644;&#21463;&#25200;&#21160;&#22270;&#20687;&#30340;&#29305;&#24449;&#12289;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#24555;&#36895;&#12289;&#39640;&#25928;&#22320;&#25628;&#32034;&#40065;&#26834;NAS&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent neural architecture search (NAS) frameworks have been successful in finding optimal architectures for given conditions (e.g., performance or latency). However, they search for optimal architectures in terms of their performance on clean images only, while robustness against various types of perturbations or corruptions is crucial in practice. Although there exist several robust NAS frameworks that tackle this issue by integrating adversarial training into one-shot NAS, however, they are limited in that they only consider robustness against adversarial attacks and require significant computational resources to discover optimal architectures for a single task, which makes them impractical in real-world scenarios. To address these challenges, we propose a novel lightweight robust zero-cost proxy that considers the consistency across features, parameters, and gradients of both clean and perturbed images at the initialization state. Our approach facilitates an efficient and rapid sea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22810;&#32423;&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MMIL-Transformer&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#28041;&#21450;&#22823;&#37327;&#23454;&#20363;&#30340;MIL&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.05029</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22810;&#32423;&#22810;&#31034;&#20363;&#23398;&#20064;&#29992;&#20110;&#20840;&#25195;&#25551;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-level Multiple Instance Learning with Transformer for Whole Slide Image Classification. (arXiv:2306.05029v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22810;&#32423;&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MMIL-Transformer&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#28041;&#21450;&#22823;&#37327;&#23454;&#20363;&#30340;MIL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#25195;&#25551;&#22270;&#20687;&#65288;WSI&#65289;&#26159;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#30340;&#32452;&#32455;&#25195;&#25551;&#22270;&#20687;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#65288;CAD&#65289;&#12290;&#30001;&#20110;&#26497;&#39640;&#30340;&#20998;&#36776;&#29575;&#21644;&#21306;&#22495;&#32423;&#21035;&#27880;&#37322;&#30340;&#26377;&#38480;&#24615;&#65292;&#23545;&#20110;&#22522;&#20110;WSI&#30340;&#25968;&#23383;&#35786;&#26029;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26159;&#35299;&#20915;&#24369;&#27880;&#37322;&#38382;&#39064;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#65292;&#32780;Transformer&#24050;&#22312;&#35270;&#35273;&#20219;&#21153;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#32467;&#21512;&#20004;&#32773;&#23558;&#20026;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#35786;&#26029;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21333;&#32423;MIL&#30340;&#38480;&#21046;&#21644;&#27880;&#24847;&#26426;&#21046;&#23545;&#24207;&#21015;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#23558;Transformer&#30452;&#25509;&#24212;&#29992;&#20110;&#22522;&#20110;WSI&#30340;MIL&#20219;&#21153;&#24182;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;MIL&#19982;Transformer&#65288;MMIL-Transformer&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#32467;&#26500;&#21040;MIL&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#28041;&#21450;&#22823;&#37327;&#23454;&#20363;&#30340;MIL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whole slide image (WSI) refers to a type of high-resolution scanned tissue image, which is extensively employed in computer-assisted diagnosis (CAD). The extremely high resolution and limited availability of region-level annotations make it challenging to employ deep learning methods for WSI-based digital diagnosis. Multiple instance learning (MIL) is a powerful tool to address the weak annotation problem, while Transformer has shown great success in the field of visual tasks. The combination of both should provide new insights for deep learning based image diagnosis. However, due to the limitations of single-level MIL and the attention mechanism's constraints on sequence length, directly applying Transformer to WSI-based MIL tasks is not practical. To tackle this issue, we propose a Multi-level MIL with Transformer (MMIL-Transformer) approach. By introducing a hierarchical structure to MIL, this approach enables efficient handling of MIL tasks that involve a large number of instances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#24230;&#30456;&#20284;&#30340;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#21644;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#21518;&#39564;&#23849;&#28291;&#29616;&#35937;&#65292;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;&#26465;&#20214;VAE&#21644;&#20998;&#23618;VAE&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#28508;&#22312;&#21464;&#37327;&#23618;&#27425;&#20851;&#31995;&#19981;&#28165;&#26224;&#32780;&#24341;&#36215;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.05023</link><description>&lt;p&gt;
&#32447;&#24615;&#26465;&#20214;VAE&#21644;&#20998;&#23618;VAE&#20013;&#30340;&#21518;&#39564;&#23849;&#28291;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Posterior Collapse in Linear Conditional and Hierarchical Variational Autoencoders. (arXiv:2306.05023v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#24230;&#30456;&#20284;&#30340;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#21644;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#21518;&#39564;&#23849;&#28291;&#29616;&#35937;&#65292;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;&#26465;&#20214;VAE&#21644;&#20998;&#23618;VAE&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#28508;&#22312;&#21464;&#37327;&#23618;&#27425;&#20851;&#31995;&#19981;&#28165;&#26224;&#32780;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#20013;&#65292;&#21518;&#39564;&#23849;&#28291;&#29616;&#35937;&#25351;&#30340;&#26159;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#19982;&#20808;&#39564;&#20998;&#24067;&#30340;&#30456;&#20284;&#24230;&#36807;&#39640;&#65292;&#23548;&#33268;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#28508;&#22312;&#21464;&#37327;&#20445;&#23384;&#30340;&#36755;&#20837;&#25968;&#25454;&#20449;&#24687;&#36739;&#23569;&#65292;&#26080;&#27861;&#20026;&#35299;&#30721;&#22120;&#30340;&#25968;&#25454;&#37325;&#24314;&#36807;&#31243;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#23613;&#31649;&#35813;&#29616;&#35937;&#19968;&#30452;&#26159;VAEs&#24615;&#33021;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20294;&#26159;&#23545;&#20110;&#21518;&#39564;&#23849;&#28291;&#30340;&#29702;&#35770;&#21364;&#30456;&#23545;&#34180;&#24369;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#26631;&#20934;&#30340;VAEs&#20013;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20004;&#31867;&#37325;&#35201;&#32780;&#24120;&#35265;&#21448;&#36739;&#23569;&#30740;&#31350;&#30340;VAEs&#36827;&#34892;&#38750;&#24179;&#20961;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21363;&#20855;&#26377;&#20004;&#20010;&#28508;&#22312;&#21464;&#37327;&#23618;&#27425;&#30340;&#32447;&#24615;&#26465;&#20214;VAE&#21644;&#20998;&#23618;VAE&#65292;&#25552;&#21319;&#20102;&#23545;&#21518;&#39564;&#23849;&#28291;&#30340;&#29702;&#35770;&#35748;&#35782;&#65292;&#35777;&#26126;&#20102;&#20854;&#25104;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
The posterior collapse phenomenon in variational autoencoders (VAEs), where the variational posterior distribution closely matches the prior distribution, can hinder the quality of the learned latent variables. As a consequence of posterior collapse, the latent variables extracted by the encoder in VAEs preserve less information from the input data and thus fail to produce meaningful representations as input to the reconstruction process in the decoder. While this phenomenon has been an actively addressed topic related to VAEs performance, the theory for posterior collapse remains underdeveloped, especially beyond the standard VAEs. In this work, we advance the theoretical understanding of posterior collapse to two important and prevalent yet less studied classes of VAEs: conditional VAEs and hierarchical VAEs. Specifically, via a non-trivial theoretical analysis of linear conditional VAEs and hierarchical VAEs with two levels of latent, we prove that the cause of posterior collapses i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#29305;&#23450;&#24352;&#37327;&#20998;&#35299;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;Mixed-TD&#65292;&#37319;&#29992;&#28151;&#21512;&#26041;&#24335;&#30340;SVD&#21644;CPD&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;&#21407;&#22987;&#31070;&#32463;&#32593;&#32476;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#26144;&#23556;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#21487;&#29992;&#29255;&#19978;&#23384;&#20648;&#22120;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.05021</link><description>&lt;p&gt;
Mixed-TD: &#22522;&#20110;&#23618;&#29305;&#23450;&#24352;&#37327;&#20998;&#35299;&#30340;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific Tensor Decomposition. (arXiv:2306.05021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#29305;&#23450;&#24352;&#37327;&#20998;&#35299;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;Mixed-TD&#65292;&#37319;&#29992;&#28151;&#21512;&#26041;&#24335;&#30340;SVD&#21644;CPD&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;&#21407;&#22987;&#31070;&#32463;&#32593;&#32476;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#26144;&#23556;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#21487;&#29992;&#29255;&#19978;&#23384;&#20648;&#22120;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30456;&#24403;&#22810;&#26679;&#21270;&#65292;&#20174;VGG&#21040;ResNet&#65292;&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21040;&#21464;&#25442;&#22120;&#12290;&#20026;&#20102;&#35774;&#35745;&#25928;&#29575;&#39640;&#30340;&#21152;&#36895;&#22120;&#65292;&#35768;&#22810;&#24037;&#20316;&#37319;&#29992;&#20102;&#22522;&#20110;&#25968;&#25454;&#27969;&#30340;&#12289;&#23618;&#38388;&#27969;&#27700;&#32447;&#32467;&#26500;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#38024;&#23545;&#27599;&#19968;&#23618;&#36827;&#34892;&#20102;&#33258;&#23450;&#20041;&#30828;&#20214;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#36229;&#39640;&#30340;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#12290;&#31070;&#32463;&#32593;&#32476;&#37096;&#32626;&#21040;&#27492;&#31867;&#25968;&#25454;&#27969;&#20307;&#31995;&#32467;&#26500;&#21152;&#36895;&#22120;&#19978;&#36890;&#24120;&#21463;&#21487;&#29992;&#29255;&#19978;&#20869;&#23384;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#39044;&#21152;&#36733;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21040;&#29255;&#19978;&#20197;&#26368;&#22823;&#21270;&#31995;&#32479;&#24615;&#33021;&#26159;&#29702;&#24819;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32593;&#32476;&#36890;&#24120;&#20250;&#36890;&#36807;&#20462;&#21098;&#12289;&#37327;&#21270;&#21644;&#24352;&#37327;&#20998;&#35299;&#31561;&#26041;&#27861;&#36827;&#34892;&#21387;&#32553;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;CNN&#26144;&#23556;&#21040;FPGA&#19978;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;Mixed-TD&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#28151;&#21512;&#26041;&#24335;&#30340;&#23618;&#29305;&#23450;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#21644;&#20856;&#22411;&#22810;&#39033;&#24335;&#20998;&#35299;&#65288;CPD&#65289;&#65292;&#22312;&#20445;&#25345;&#21407;&#22987;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#29575;&#12290;Mixed-TD&#26694;&#26550;&#37319;&#29992;&#21160;&#24577;&#26144;&#23556;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#22320;&#21033;&#29992;&#21487;&#29992;&#30340;&#29255;&#19978;&#23384;&#20648;&#22120;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Mixed-TD&#26694;&#26550;&#22312;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#35745;&#31639;&#21608;&#26399;&#26041;&#38754;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#21407;&#22987;&#26410;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Network designs are quite diverse, from VGG-style to ResNet-style, and from Convolutional Neural Networks to Transformers. Towards the design of efficient accelerators, many works have adopted a dataflow-based, inter-layer pipelined architecture, with a customised hardware towards each layer, achieving ultra high throughput and low latency. The deployment of neural networks to such dataflow architecture accelerators is usually hindered by the available on-chip memory as it is desirable to preload the weights of neural networks on-chip to maximise the system performance. To address this, networks are usually compressed before the deployment through methods such as pruning, quantization and tensor decomposition. In this paper, a framework for mapping CNNs onto FPGAs based on a novel tensor decomposition method called Mixed-TD is proposed. The proposed method applies layer-specific Singular Value Decomposition (SVD) and Canonical Polyadic Decomposition (CPD) in a mixed manner, achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;NILM&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20026;&#38656;&#27714;&#20391;&#31649;&#29702;&#12289;&#33021;&#32791;&#30417;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05017</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#65288;NILM&#65289;: &#19968;&#39033;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-Intrusive Load Monitoring (NILM) using Deep Neural Networks: A Review. (arXiv:2306.05017v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;NILM&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20026;&#38656;&#27714;&#20391;&#31649;&#29702;&#12289;&#33021;&#32791;&#30417;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#27714;&#20391;&#31649;&#29702;&#29616;&#22312;&#28085;&#30422;&#20102;&#26356;&#22810;&#30340;&#23621;&#27665;&#36127;&#36733;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#24212;&#29992;&#38656;&#27714;&#21709;&#24212;&#31574;&#30053;&#65292;&#23450;&#26399;&#35266;&#23519;&#21508;&#31181;&#23478;&#29992;&#30005;&#22120;&#23545;&#24635;&#33021;&#32791;&#30340;&#36129;&#29486;&#33267;&#20851;&#37325;&#35201;&#12290;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#65288;NILM&#65289;&#65292;&#20063;&#31216;&#36127;&#36733;&#20998;&#35299;&#65292;&#26159;&#19968;&#31181;&#23558;&#24635;&#33021;&#32791;&#21078;&#20998;&#20026;&#23478;&#24237;&#20869;&#21333;&#20010;&#30005;&#22120;&#36127;&#36733;&#36718;&#24275;&#30340;&#26041;&#27861;&#12290;NILM&#26377;&#22810;&#20010;&#24212;&#29992;&#65292;&#21253;&#25324;&#38656;&#27714;&#20391;&#31649;&#29702;&#12289;&#33021;&#32791;&#30417;&#27979;&#21644;&#20998;&#26512;&#12290;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#24050;&#34987;&#29992;&#20110;&#23454;&#29616;&#21644;&#25913;&#36827;NILM&#31639;&#27861;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#19968;&#20123;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#36817;NILM&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#26368;&#31934;&#30830;&#30340;&#20303;&#23429;&#36127;&#36733;&#26041;&#27861;&#12290;&#23427;&#24635;&#32467;&#20102;NILM&#35780;&#20272;&#30340;&#20844;&#20849;&#25968;&#25454;&#24211;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#24615;&#33021;&#25351;&#26631;&#27604;&#36739;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demand-side management now encompasses more residential loads. To efficiently apply demand response strategies, it's essential to periodically observe the contribution of various domestic appliances to total energy consumption. Non-intrusive load monitoring (NILM), also known as load disaggregation, is a method for decomposing the total energy consumption profile into individual appliance load profiles within the household. It has multiple applications in demand-side management, energy consumption monitoring, and analysis. Various methods, including machine learning and deep learning, have been used to implement and improve NILM algorithms. This paper reviews some recent NILM methods based on deep learning and introduces the most accurate methods for residential loads. It summarizes public databases for NILM evaluation and compares methods using standard performance metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#38750;&#32447;&#24615;&#26799;&#24230;&#27169;&#22411;&#65288;NGM&#65289;&#65292;&#23427;&#26159;&#21487;&#35299;&#26512;&#22320;&#20351;&#29992;Taylor&#32423;&#25968;&#25299;&#23637;&#23548;&#20986;&#30340;&#38381;&#21512;&#24418;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22320;&#29699;&#31995;&#32479;&#22797;&#26434;&#36807;&#31243;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#65288;SGS&#65289;&#38381;&#21512;/&#21442;&#25968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.05014</link><description>&lt;p&gt;
&#20174;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#20013;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#38381;&#21512;&#24418;&#24335;&#26041;&#31243;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Learning Closed-form Equations for Subgrid-scale Closures from High-fidelity Data: Promises and Challenges. (arXiv:2306.05014v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#38750;&#32447;&#24615;&#26799;&#24230;&#27169;&#22411;&#65288;NGM&#65289;&#65292;&#23427;&#26159;&#21487;&#35299;&#26512;&#22320;&#20351;&#29992;Taylor&#32423;&#25968;&#25299;&#23637;&#23548;&#20986;&#30340;&#38381;&#21512;&#24418;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22320;&#29699;&#31995;&#32479;&#22797;&#26434;&#36807;&#31243;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#65288;SGS&#65289;&#38381;&#21512;/&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21457;&#29616;&#22320;&#29699;&#31995;&#32479;&#22797;&#26434;&#36807;&#31243;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#65288;SGS&#65289;&#38381;&#21512;/&#21442;&#25968;&#21270;&#30340;&#21487;&#35299;&#37322;&#24615;&#38381;&#21512;&#24418;&#24335;&#26041;&#31243;&#19978;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#34920;&#29616;&#20986;&#27987;&#21402;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#24191;&#27867;&#30340;&#24211;&#24212;&#29992;&#36890;&#29992;&#30340;&#26041;&#31243;&#21457;&#29616;&#25216;&#26415;&#65292;&#20174;&#32463;&#36807;&#28388;&#27874;&#30340;&#20108;&#32500;&#24378;&#36843;&#28237;&#27969;&#21644;&#29790;&#21033; - &#36125;&#32435;&#24503;&#23545;&#27969;&#30340;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#20013;&#23398;&#20064;&#38381;&#21512;&#24418;&#24335;&#12290;&#22312;&#24120;&#35265;&#30340;&#28388;&#27874;&#22120;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#24378;&#26377;&#21147;&#22320;&#21457;&#29616;&#20102;&#21160;&#37327;&#21644;&#28909;&#36890;&#37327;&#30340;&#30456;&#21516;&#24418;&#24335;&#30340;&#38381;&#21512;&#24418;&#24335;&#12290;&#36825;&#20123;&#38381;&#21512;&#24418;&#24335;&#21462;&#20915;&#20110;&#34987;&#36807;&#28388;&#21464;&#37327;&#65288;&#36895;&#24230;&#12289;&#28201;&#24230;&#65289;&#30340;&#26799;&#24230;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#20854;&#20013;&#30340;&#24120;&#25968;&#29420;&#31435;&#20110;&#27969;&#20307;/&#27969;&#21160;&#29305;&#24615;&#65292;&#20165;&#20381;&#36182;&#20110;&#36807;&#28388;&#22120;&#31867;&#22411;/&#22823;&#23567;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#38381;&#21512;&#24418;&#24335;&#26159;&#38750;&#32447;&#24615;&#26799;&#24230;&#27169;&#22411;&#65288;NGM&#65289;&#65292;&#21487;&#20197;&#20351;&#29992;Taylor&#32423;&#25968;&#23637;&#24320;&#20998;&#26512;&#22320;&#23548;&#20986;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#20351;&#29992;&#24120;&#35265;&#30340;&#65288;&#26080;&#29289;&#29702;&#20449;&#24687;&#30340;&#65289;&#26041;&#31243;&#21457;&#29616;&#31639;&#27861;&#26102;&#65292;&#26080;&#35770;&#26159;&#20160;&#20040;&#31995;&#32479;/&#29289;&#29702;&#23398;&#65292;&#21457;&#29616;&#30340;&#38381;&#21512;&#24418;&#24335;&#22987;&#32456;&#19982;Taylor&#32423;&#25968;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in discovering interpretable, closed-form equations for subgrid-scale (SGS) closures/parameterizations of complex processes in Earth system. Here, we apply a common equation-discovery technique with expansive libraries to learn closures from filtered direct numerical simulations of 2D forced turbulence and Rayleigh-B\'enard convection (RBC). Across common filters, we robustly discover closures of the same form for momentum and heat fluxes. These closures depend on nonlinear combinations of gradients of filtered variables (velocity, temperature), with constants that are independent of the fluid/flow properties and only depend on filter type/size. We show that these closures are the nonlinear gradient model (NGM), which is derivable analytically using Taylor-series expansions. In fact, we suggest that with common (physics-free) equation-discovery algorithms, regardless of the system/physics, discovered closures are always consistent with the Taylor-series. Like 
&lt;/p&gt;</description></item><item><title>&#30005;&#23376;&#21830;&#21153;&#20010;&#24615;&#21270;&#25490;&#21517;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;MoE&#26694;&#26550;&#36827;&#34892;&#29305;&#24449;&#20132;&#20114;&#24314;&#27169;&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#21382;&#21490;&#34892;&#20026;&#36739;&#23569;&#30340;&#38271;&#23614;&#29992;&#25143;&#20010;&#24615;&#21270;&#25490;&#21517;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05011</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21152;&#26435;&#27880;&#24847;&#21147;&#30340;&#30005;&#23376;&#21830;&#21153;&#20010;&#24615;&#21270;&#25490;&#21517;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Attention Weighted Mixture of Experts with Contrastive Learning for Personalized Ranking in E-commerce. (arXiv:2306.05011v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05011
&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20010;&#24615;&#21270;&#25490;&#21517;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;MoE&#26694;&#26550;&#36827;&#34892;&#29305;&#24449;&#20132;&#20114;&#24314;&#27169;&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#21382;&#21490;&#34892;&#20026;&#36739;&#23569;&#30340;&#38271;&#23614;&#29992;&#25143;&#20010;&#24615;&#21270;&#25490;&#21517;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21517;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26377;&#25928;&#30340;&#25490;&#21517;&#27169;&#22411;&#24212;&#35813;&#26681;&#25454;&#29992;&#25143;&#21916;&#22909;&#20026;&#27599;&#20010;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25490;&#21517;&#21015;&#34920;&#12290;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#20174;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#20013;&#25552;&#21462;&#29992;&#25143;&#34920;&#31034;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#35813;&#21521;&#37327;&#19982;&#20854;&#20182;&#29305;&#24449;&#19968;&#36215;&#39304;&#20837;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#36827;&#34892;&#29305;&#24449;&#20132;&#20114;&#65292;&#24182;&#26368;&#32456;&#29983;&#25104;&#20010;&#24615;&#21270;&#25490;&#21517;&#24471;&#20998;&#12290;&#23613;&#31649;&#36807;&#21435;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#39318;&#20808;&#65292;&#19981;&#21516;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#29305;&#24449;&#20132;&#20114;&#27169;&#24335;&#27809;&#26377;&#26126;&#30830;&#24314;&#27169;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#22312;&#20855;&#26377;&#23569;&#37327;&#21382;&#21490;&#34892;&#20026;&#30340;&#38271;&#23614;&#29992;&#25143;&#19978;&#30340;&#20010;&#24615;&#21270;&#25490;&#21517;&#32467;&#26524;&#36739;&#24046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21152;&#26435;&#27880;&#24847;&#21147;&#30340;&#20010;&#24615;&#21270;&#25490;&#21517;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;AW-MoE&#65289;&#12290;&#39318;&#20808;&#65292;AW-MoE&#21033;&#29992;MoE&#26694;&#26550;&#25429;&#33719;&#20010;&#24615;&#21270;&#29305;&#24449;&#20132;&#20114;&#27169;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
Ranking model plays an essential role in e-commerce search and recommendation. An effective ranking model should give a personalized ranking list for each user according to the user preference. Existing algorithms usually extract a user representation vector from the user behavior sequence, then feed the vector into a feed-forward network (FFN) together with other features for feature interactions, and finally produce a personalized ranking score. Despite tremendous progress in the past, there is still room for improvement. Firstly, the personalized patterns of feature interactions for different users are not explicitly modeled. Secondly, most of existing algorithms have poor personalized ranking results for long-tail users with few historical behaviors due to the data sparsity. To overcome the two challenges, we propose Attention Weighted Mixture of Experts (AW-MoE) with contrastive learning for personalized ranking. Firstly, AW-MoE leverages the MoE framework to capture personalized 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05001</link><description>&lt;p&gt;
COURIER: &#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#29305;&#24449;&#39044;&#35757;&#32451;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#36824;&#21407;
&lt;/p&gt;
&lt;p&gt;
COURIER: Contrastive User Intention Reconstruction for Large-Scale Pre-Train of Image Features. (arXiv:2306.05001v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#20114;&#32852;&#32593;&#30340;&#21457;&#23637;&#65292;&#35270;&#35273;&#29305;&#24449;&#24050;&#25104;&#20026;&#24433;&#21709;&#29992;&#25143;&#20852;&#36259;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#34701;&#20837;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#26041;&#21521;&#26159;&#39047;&#20855;&#21069;&#26223;&#30340;&#12290;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#23558;&#24050;&#26377;&#39044;&#35757;&#32451;&#26041;&#27861;&#35757;&#32451;&#24471;&#21040;&#30340;&#22270;&#20687;&#23884;&#20837;&#27880;&#20837;&#21040;&#27169;&#22411;&#20013;&#20165;&#33021;&#20135;&#29983;&#36739;&#23567;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#24402;&#22240;&#20110;&#20197;&#19979;&#20004;&#20010;&#21407;&#22240;&#65306;&#39318;&#20808;&#65292;&#39044;&#35757;&#32451;&#26041;&#27861;&#26159;&#20026;&#20102;&#26126;&#30830;&#30340;&#12289;&#37325;&#28857;&#25918;&#22312;&#35821;&#20041;&#29305;&#24449;&#19978;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#65292;&#26080;&#27861;&#23398;&#20064;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#30340;&#20010;&#20154;&#20852;&#36259;; &#20854;&#27425;&#65292;&#39044;&#35757;&#32451;&#30340;&#21482;&#21253;&#21547;&#35821;&#20041;&#20449;&#24687;&#30340;&#22270;&#20687;&#23884;&#20837;&#19982;&#25105;&#20204;&#22312;CTR&#39044;&#27979;&#20219;&#21153;&#20013;&#25152;&#24050;&#26377;&#30340;&#31867;&#21035;&#21644;&#29289;&#21697;&#26631;&#39064;&#31561;&#35821;&#20041;&#29305;&#24449;&#30456;&#27604;&#26377;&#36739;&#23567;&#30340;&#20449;&#24687;&#22686;&#30410;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#20197;&#33719;&#24471;&#26356;&#22810;&#25913;&#36827;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of the multi-media internet, visual characteristics have become an important factor affecting user interests. Thus, incorporating visual features is a promising direction for further performance improvements in click-through rate (CTR) prediction. However, we found that simply injecting the image embeddings trained with established pre-training methods only has marginal improvements. We attribute the failure to two reasons: First, The pre-training methods are designed for well-defined computer vision tasks concentrating on semantic features, and they cannot learn personalized interest in recommendations. Secondly, pre-trained image embeddings only containing semantic information have little information gain, considering we already have semantic features such as categories and item titles as inputs in the CTR prediction task. We argue that a pre-training method tailored for recommendation is necessary for further improvements. To this end, we propose a recommendatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#21270;&#20026;&#28909;&#21147;&#22270;&#65292;&#39044;&#27979;&#25937;&#25252;&#36710;&#38656;&#27714;&#12290;&#21516;&#26102;&#37319;&#29992;&#29305;&#24449;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;&#65292;&#32467;&#21512;&#21382;&#21490;&#25937;&#25252;&#36710;&#38656;&#27714;&#21644;&#22806;&#37096;&#20449;&#24687;&#65292;&#22914;&#22825;&#27668;&#12289;&#20107;&#20214;&#12289;&#33410;&#26085;&#21644;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.04994</link><description>&lt;p&gt;
&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#25937;&#25252;&#36710;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Ambulance Demand Prediction via Convolutional Neural Networks. (arXiv:2306.04994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#21270;&#20026;&#28909;&#21147;&#22270;&#65292;&#39044;&#27979;&#25937;&#25252;&#36710;&#38656;&#27714;&#12290;&#21516;&#26102;&#37319;&#29992;&#29305;&#24449;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;&#65292;&#32467;&#21512;&#21382;&#21490;&#25937;&#25252;&#36710;&#38656;&#27714;&#21644;&#22806;&#37096;&#20449;&#24687;&#65292;&#22914;&#22825;&#27668;&#12289;&#20107;&#20214;&#12289;&#33410;&#26085;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#21709;&#24212;&#26102;&#38388;&#23545;&#20110;&#32039;&#24613;&#21307;&#30103;&#26381;&#21153;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#38477;&#20302;&#24739;&#32773;&#30340;&#31561;&#24453;&#26102;&#38388;&#65292;&#25552;&#39640;&#20182;&#20204;&#30340;&#29983;&#23384;&#29575;&#12290;&#35768;&#22810;&#27169;&#22411;&#21487;&#29992;&#20110;&#20248;&#21270;&#35832;&#22914;&#25937;&#25252;&#36710;&#20998;&#37197;&#21644;&#35843;&#24230;&#31561;&#36816;&#33829;&#20219;&#21153;&#12290;&#23558;&#31934;&#30830;&#30340;&#38656;&#27714;&#39044;&#27979;&#21253;&#21547;&#22312;&#27492;&#31867;&#27169;&#22411;&#20013;&#21487;&#25552;&#39640;&#36816;&#33829;&#20915;&#31574;&#21046;&#23450;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#20307;&#31995;&#32467;&#26500;&#65292;&#23427;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#21270;&#20026;&#28909;&#21147;&#22270;&#20197;&#39044;&#27979;&#25937;&#25252;&#36710;&#38656;&#27714;&#12290;&#24212;&#29992;&#36825;&#31181;&#39044;&#27979;&#38656;&#35201;&#32435;&#20837;&#24433;&#21709;&#25937;&#25252;&#36710;&#38656;&#27714;&#30340;&#22806;&#37096;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#28789;&#27963;&#30340;&#12289;&#36890;&#29992;&#30340;CNN&#20307;&#31995;&#32467;&#26500;&#65292;&#20801;&#35768;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#23610;&#23544;&#30340;&#22806;&#37096;&#29305;&#24449;&#65292;&#20026;&#29616;&#26377;&#25991;&#29486;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#24449;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#25972;&#21512;&#21382;&#21490;&#25937;&#25252;&#36710;&#38656;&#27714;&#21644;&#22806;&#37096;&#20449;&#24687;&#65292;&#22914;&#22825;&#27668;&#65292;&#20107;&#20214;&#65292;&#33410;&#26085;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimizing response times is crucial for emergency medical services to reduce patients' waiting times and to increase their survival rates. Many models exist to optimize operational tasks such as ambulance allocation and dispatching. Including accurate demand forecasts in such models can improve operational decision-making. Against this background, we present a novel convolutional neural network (CNN) architecture that transforms time series data into heatmaps to predict ambulance demand. Applying such predictions requires incorporating external features that influence ambulance demands. We contribute to the existing literature by providing a flexible, generic CNN architecture, allowing for the inclusion of external features with varying dimensions. Additionally, we provide a feature selection and hyperparameter optimization framework utilizing Bayesian optimization. We integrate historical ambulance demand and external information such as weather, events, holidays, and time. To show t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#26222;&#36866;&#30340;&#26657;&#20934;&#35823;&#24046;&#23450;&#20041;&#8212;&#8212;&#20998;&#21306;&#26657;&#20934;&#35823;&#24046;&#65288;PCE&#65289;&#65292;&#25351;&#20986;&#20102;&#20998;&#21306;&#21010;&#20998;&#26159;&#21508;&#31181;&#26657;&#20934;&#35823;&#24046;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#38190;&#21306;&#21035;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21629;&#39064;&#65306;&#20934;&#30830;&#30340;&#27169;&#22411;&#24212;&#35813;&#22312;&#20219;&#20309;&#20998;&#21306;&#19978;&#37117;&#20855;&#26377;&#26657;&#20934;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#27010;&#29575;&#20998;&#21306;&#12290;&#36890;&#36807;&#35821;&#20041;&#30456;&#20851;&#30340;&#20998;&#21306;&#20989;&#25968;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#20998;&#21306;&#20989;&#25968;&#30340;&#31890;&#24230;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.04985</link><description>&lt;p&gt;
&#36229;&#36234;&#27010;&#29575;&#21010;&#20998;&#65306;&#35821;&#20041;&#24863;&#30693;&#20998;&#32452;&#26657;&#20934;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Beyond Probability Partitions: Calibrating Neural Networks with Semantic Aware Grouping. (arXiv:2306.04985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04985
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#26222;&#36866;&#30340;&#26657;&#20934;&#35823;&#24046;&#23450;&#20041;&#8212;&#8212;&#20998;&#21306;&#26657;&#20934;&#35823;&#24046;&#65288;PCE&#65289;&#65292;&#25351;&#20986;&#20102;&#20998;&#21306;&#21010;&#20998;&#26159;&#21508;&#31181;&#26657;&#20934;&#35823;&#24046;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#38190;&#21306;&#21035;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21629;&#39064;&#65306;&#20934;&#30830;&#30340;&#27169;&#22411;&#24212;&#35813;&#22312;&#20219;&#20309;&#20998;&#21306;&#19978;&#37117;&#20855;&#26377;&#26657;&#20934;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#27010;&#29575;&#20998;&#21306;&#12290;&#36890;&#36807;&#35821;&#20041;&#30456;&#20851;&#30340;&#20998;&#21306;&#20989;&#25968;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#20998;&#21306;&#20989;&#25968;&#30340;&#31890;&#24230;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#32593;&#32476;&#24448;&#24448;&#23545;&#20854;&#39044;&#27979;&#36807;&#20110;&#20048;&#35266;&#65292;&#23548;&#33268;&#39044;&#27979;&#35823;&#24046;&#34987;&#20302;&#20272;&#12290;&#30001;&#20110;&#25968;&#25454;&#30340;&#26377;&#38480;&#24615;&#65292;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#27010;&#29575;&#30340;&#26041;&#27861;&#26469;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#32452;&#24182;&#35780;&#20272;&#26657;&#20934;&#35823;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#36890;&#29992;&#30340;&#26657;&#20934;&#35823;&#24046;&#23450;&#20041;&#65292;&#31216;&#20026;&#20998;&#21306;&#26657;&#20934;&#35823;&#24046;&#65288;PCE&#65289;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#26657;&#20934;&#35823;&#24046;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#38190;&#21306;&#21035;&#22312;&#20110;&#22914;&#20309;&#23558;&#25968;&#25454;&#31354;&#38388;&#21010;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#21629;&#39064;&#65292;&#21363;&#20934;&#30830;&#30340;&#27169;&#22411;&#24212;&#35813;&#22312;&#20219;&#20309;&#20998;&#21306;&#19978;&#37117;&#20855;&#26377;&#26657;&#20934;&#24615;&#65292;&#36825;&#34920;&#26126;&#36755;&#20837;&#31354;&#38388;&#20998;&#21306;&#21487;&#20197;&#25193;&#23637;&#21040;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#27010;&#29575;&#20998;&#21306;&#65292;&#36824;&#21487;&#20197;&#21253;&#25324;&#19982;&#36755;&#20837;&#30452;&#25509;&#30456;&#20851;&#30340;&#20998;&#21306;&#12290;&#36890;&#36807;&#35821;&#20041;&#30456;&#20851;&#30340;&#20998;&#21306;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#20110;&#20998;&#21306;&#20989;&#25968;&#30340;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research has shown that deep networks tend to be overly optimistic about their predictions, leading to an underestimation of prediction errors. Due to the limited nature of data, existing studies have proposed various methods based on model prediction probabilities to bin the data and evaluate calibration error. We propose a more generalized definition of calibration error called Partitioned Calibration Error (PCE), revealing that the key difference among these calibration error metrics lies in how the data space is partitioned. We put forth an intuitive proposition that an accurate model should be calibrated across any partition, suggesting that the input space partitioning can extend beyond just the partitioning of prediction probabilities, and include partitions directly related to the input. Through semantic-related partitioning functions, we demonstrate that the relationship between model accuracy and calibration lies in the granularity of the partitioning function. This highlight
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G$^2$uardFL&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23646;&#24615;&#21270;&#23458;&#25143;&#31471;&#22270;&#32858;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#21363;&#20351;&#24694;&#24847;&#23458;&#25143;&#31471;&#25968;&#37327;&#39640;&#36798;50&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.04984</link><description>&lt;p&gt;
G$^2$uardFL: &#36890;&#36807;&#23646;&#24615;&#21270;&#23458;&#25143;&#31471;&#22270;&#32858;&#31867;&#26469;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering. (arXiv:2306.04984v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G$^2$uardFL&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23646;&#24615;&#21270;&#23458;&#25143;&#31471;&#22270;&#32858;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#21363;&#20351;&#24694;&#24847;&#23458;&#25143;&#31471;&#25968;&#37327;&#39640;&#36798;50&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#21327;&#21516;&#33539;&#24335;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#23458;&#25143;&#31471;&#33021;&#22815;&#36827;&#34892;&#38598;&#20307;&#27169;&#22411;&#35757;&#32451;&#32780;&#19981;&#20132;&#25442;&#21508;&#33258;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;FL&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#20250;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#26435;&#37325;&#27880;&#20837;&#26377;&#27602;&#25968;&#25454;&#65292;&#20174;&#32780;&#24471;&#21040;&#38024;&#23545;&#29305;&#23450;&#26679;&#26412;&#30340;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#29616;&#26377;&#30340;&#23545;&#31574;&#20027;&#35201;&#22522;&#20110;&#24322;&#24120;&#26816;&#27979;&#65292;&#20294;&#30001;&#20110;&#37327;&#21270;&#23458;&#25143;&#27169;&#22411;&#30456;&#20284;&#24615;&#30340;&#19981;&#36275;&#65292;&#36825;&#20123;&#23545;&#31574;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#25298;&#32477;&#21512;&#27861;&#26435;&#37325;&#65292;&#21516;&#26102;&#25509;&#21463;&#24694;&#24847;&#26435;&#37325;&#12290;&#20854;&#20182;&#38450;&#24481;&#26426;&#21046;&#20165;&#22312;&#38754;&#23545;&#23569;&#37327;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#20363;&#22914;&#23569;&#20110;10&#65285;&#30340;&#24694;&#24847;&#23458;&#25143;&#31471;&#26102;&#25165;&#26377;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;G$^2$uardFL&#65292;&#36825;&#26159;&#19968;&#20010;&#20445;&#25252;&#26694;&#26550;&#65292;&#23427;&#23558;&#26816;&#27979;&#24694;&#24847;&#23458;&#25143;&#31471;&#35270;&#20026;&#19968;&#20010;&#23646;&#24615;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#20174;&#32780;&#20445;&#25252;FL&#31995;&#32479;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#23458;&#25143;&#31471;&#22270;&#32858;&#31867;&#25216;&#26415;&#65292;&#26681;&#25454;&#27169;&#22411;&#26435;&#37325;&#30340;&#30456;&#20284;&#24615;&#23558;&#23458;&#25143;&#31471;&#20998;&#31867;&#20026;&#27491;&#24120;&#25110;&#24694;&#24847;&#12290;&#36890;&#36807;&#37319;&#29992;&#23545;&#23458;&#25143;&#31471;&#22266;&#26377;&#23646;&#24615;&#36827;&#34892;&#32534;&#30721;&#30340;&#23646;&#24615;&#26631;&#31614;&#65292;G$^2$uardFL&#22312;&#35782;&#21035;&#21463;&#25439;&#23458;&#25143;&#31471;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#32780;&#19981;&#25490;&#38500;&#21512;&#27861;&#23458;&#25143;&#31471;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26377;50&#65285;&#30340;&#23458;&#25143;&#31471;&#26159;&#24694;&#24847;&#30340;&#65292;G$^2$uardFL&#20063;&#33021;&#26174;&#33879;&#38477;&#20302;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a collaborative paradigm, Federated Learning (FL) empowers clients to engage in collective model training without exchanging their respective local data. Nevertheless, FL remains vulnerable to backdoor attacks in which an attacker compromises malicious clients, and injects poisoned model weights into the aggregation process to yield attacker-chosen predictions for particular samples. Existing countermeasures, mainly based on anomaly detection, may erroneously reject legitimate weights while accepting malicious ones, which is due to inadequacies in quantifying client model similarities. Other defense mechanisms prove effective exclusively when confronted with a restricted number of malicious clients, e.g., less than 10%. To address these vulnerabilities, we present G$^2$uardFL, a protective framework that reframes the detection of malicious clients as an attributed graph clustering problem, thereby safeguarding FL systems. This framework employs a client graph clustering technique to
&lt;/p&gt;</description></item><item><title>CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.04979</link><description>&lt;p&gt;
CoCo: &#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#30340;&#32806;&#21512;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. (arXiv:2306.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04979
&lt;/p&gt;
&lt;p&gt;
CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#26497;&#22823;&#30340;&#20195;&#20215;&#26469;&#33719;&#24471;&#12290;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25506;&#32034;&#20854;&#20182;&#26631;&#27880;&#22270;&#20197;&#22686;&#24378;&#30446;&#26631;&#22495;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#22914;&#20309;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#21040;&#39046;&#22495;&#36866;&#24212;&#20013;&#20173;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#23545;&#22270;&#25299;&#25169;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#20197;&#21450;&#30456;&#24403;&#22823;&#30340;&#39046;&#22495;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoCo&#65288;Coupled Contrastive Graph Representation Learning&#65289;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20174;&#32806;&#21512;&#23398;&#20064;&#20998;&#25903;&#20013;&#25552;&#21462;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#12290;CoCo&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#25903;&#21644;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#20998;&#25903;&#65292;&#20998;&#21035;&#29992;&#38544;&#24335;&#21644;&#26174;&#24335;&#26041;&#24335;&#25506;&#32034;&#22270;&#25299;&#25169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32806;&#21512;&#20998;&#25903;&#32467;&#21512;&#21040;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \underline{Co}upled \underline{Co}ntrastive Graph Representation Learning (\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22788;&#29702;&#19981;&#24120;&#35265;&#26679;&#26412;&#26102;&#25512;&#36831;&#21040;&#20154;&#31867;&#21028;&#26029;&#30340;&#20445;&#23432;&#27169;&#22411;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#65288;DCM&#65289;&#30340;&#31639;&#27861;&#65292;&#22312;&#36741;&#21161;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24863;&#20852;&#36259;&#30340;OOD&#65288;Out-of-Distribution&#65289;&#21306;&#22495;&#30340;&#26679;&#26412;&#65292;&#36827;&#32780;&#23454;&#29616;&#21487;&#38752;&#22320;&#20998;&#31163;ID&#65288;In-Distribution&#65289;&#21644;OOD&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.04974</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#30340;&#20445;&#23432;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conservative Prediction via Data-Driven Confidence Minimization. (arXiv:2306.04974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22788;&#29702;&#19981;&#24120;&#35265;&#26679;&#26412;&#26102;&#25512;&#36831;&#21040;&#20154;&#31867;&#21028;&#26029;&#30340;&#20445;&#23432;&#27169;&#22411;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#65288;DCM&#65289;&#30340;&#31639;&#27861;&#65292;&#22312;&#36741;&#21161;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24863;&#20852;&#36259;&#30340;OOD&#65288;Out-of-Distribution&#65289;&#21306;&#22495;&#30340;&#26679;&#26412;&#65292;&#36827;&#32780;&#23454;&#29616;&#21487;&#38752;&#22320;&#20998;&#31163;ID&#65288;In-Distribution&#65289;&#21644;OOD&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38169;&#35823;&#20195;&#20215;&#24456;&#39640;&#65292;&#29305;&#21035;&#26159;&#22312;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#36825;&#31181;&#38169;&#35823;&#21487;&#33021;&#20250;&#38459;&#27490;&#26426;&#22120;&#23398;&#20064;&#30340;&#37096;&#32626;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#20445;&#23432;&#24615;&#30340;&#27169;&#22411;&#8212;&#8212;&#24403;&#23427;&#20204;&#21487;&#33021;&#20986;&#29616;&#38169;&#35823;&#26102;&#21487;&#20197;&#25512;&#36831;&#21040;&#20154;&#31867;&#21028;&#26029;&#8212;&#8212;&#21487;&#33021;&#20250;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#24322;&#24120;&#25110;&#22797;&#26434;&#31034;&#20363;&#26126;&#26174;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#26080;&#27861;&#39044;&#27979;&#25152;&#26377;&#21487;&#33021;&#30340;&#27979;&#35797;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22312;&#36741;&#21161;&#20266;OOD&#25968;&#25454;&#38598;&#19978;&#26368;&#23567;&#21270;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#36741;&#21161;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#24863;&#20852;&#36259;&#30340;OOD&#21306;&#22495;&#30340;&#26679;&#26412;&#65292;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#32622;&#20449;&#24230;&#21487;&#38752;&#22320;&#20998;&#31163;ID&#21644;OOD&#36755;&#20837;&#12290;&#21463;&#21040;&#36825;&#19968;&#32467;&#26524;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#65288;DCM&#65289;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Errors of machine learning models are costly, especially in safety-critical domains such as healthcare, where such mistakes can prevent the deployment of machine learning altogether. In these settings, conservative models -- models which can defer to human judgment when they are likely to make an error -- may offer a solution. However, detecting unusual or difficult examples is notably challenging, as it is impossible to anticipate all potential inputs at test time. To address this issue, prior work has proposed to minimize the model's confidence on an auxiliary pseudo-OOD dataset. We theoretically analyze the effect of confidence minimization and show that the choice of auxiliary dataset is critical. Specifically, if the auxiliary dataset includes samples from the OOD region of interest, confidence minimization provably separates ID and OOD inputs by predictive confidence. Taking inspiration from this result, we present data-driven confidence minimization (DCM), which minimizes confid
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#25105;&#20204;&#23567;&#32452;&#22312;&#36827;&#21270;&#31639;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#30340;&#34701;&#21512;&#26041;&#38754;&#30340;&#20843;&#20010;&#37325;&#35201;&#20316;&#21697;&#12290;</title><link>http://arxiv.org/abs/2306.04971</link><description>&lt;p&gt;
&#36827;&#21270;&#21644;&#23398;&#20064;&#30340;&#34701;&#21512;&#65306;&#20843;&#20010;&#20316;&#21697;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Melting Pot of Evolution and Learning. (arXiv:2306.04971v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#25105;&#20204;&#23567;&#32452;&#22312;&#36827;&#21270;&#31639;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#30340;&#34701;&#21512;&#26041;&#38754;&#30340;&#20843;&#20010;&#37325;&#35201;&#20316;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#25105;&#20204;&#23567;&#32452;&#30340;&#20843;&#20010;&#26368;&#36817;&#20316;&#21697;&#65292;&#36825;&#20123;&#20316;&#21697;&#25104;&#21151;&#22320;&#23558;&#36827;&#21270;&#31639;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#34701;&#21512;&#65306;1.&#20351;&#29992;&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#36827;&#34892;&#20108;&#20803;&#21644;&#22810;&#39033;&#20998;&#31867;&#65292;2.&#20248;&#38597;&#30340;&#38598;&#25104;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#38598;&#25104;&#31639;&#27861;&#65292;3.Python&#29256;&#28436;&#21270;&#35745;&#31639;&#24037;&#20855;&#21253;&#65292;4.&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#28608;&#27963;&#20989;&#25968;&#30340;&#36827;&#21270;&#65292;5.&#36951;&#20256;&#31639;&#27861;&#21644;&#26032;&#22855;&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#32452;&#21512;&#65292;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#36827;&#21270;&#65292;6.&#29992;&#20110;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#29983;&#25104;&#23545;&#25239;&#24615;&#23454;&#20363;&#30340;&#36827;&#21270;&#12289;&#26080;&#26799;&#24230;&#12289;&#26597;&#35810;&#39640;&#25928;&#12289;&#40657;&#30418;&#31639;&#27861;&#65292;7.&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25387;&#36133;&#24615;&#35299;&#37322;&#65292;8.&#38544;&#24418;&#26001;&#28857;&#65306;&#33258;&#28982;&#40657;&#21283;&#23376;&#23545;&#30446;&#26631;&#25506;&#27979;&#22120;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We survey eight recent works by our group, involving the successful blending of evolutionary algorithms with machine learning and deep learning: 1. Binary and Multinomial Classification through Evolutionary Symbolic Regression, 2. Classy Ensemble: A Novel Ensemble Algorithm for Classification, 3. EC-KitY: Evolutionary Computation Tool Kit in Python, 4. Evolution of Activation Functions for Deep Learning-Based Image Classification, 5. Adaptive Combination of a Genetic Algorithm and Novelty Search for Deep Neuroevolution, 6. An Evolutionary, Gradient-Free, Query-Efficient, Black-Box Algorithm for Generating Adversarial Instances in Deep Networks, 7. Foiling Explanations in Deep Neural Networks, 8. Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#20998;&#31867;&#30340;&#27969;&#31243;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#35789;&#32423;&#35821;&#35328;&#35782;&#21035;&#12289;&#35821;&#35328;&#22686;&#24378;&#21644;&#27169;&#22411;&#35757;&#32451;&#31561;&#27493;&#39588;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#35789;&#32423;&#20132;&#38169;&#21644;&#21477;&#23376;&#21518;&#32622;&#30340;&#35821;&#35328;&#20449;&#24687;&#25554;&#20837;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#20195;&#30721;&#28151;&#21512;&#21360;&#22320;&#35821;-&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04964</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#35782;&#21035;&#25216;&#26415;&#25552;&#21319;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Leveraging Language Identification to Enhance Code-Mixed Text Classification. (arXiv:2306.04964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#20998;&#31867;&#30340;&#27969;&#31243;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#35789;&#32423;&#35821;&#35328;&#35782;&#21035;&#12289;&#35821;&#35328;&#22686;&#24378;&#21644;&#27169;&#22411;&#35757;&#32451;&#31561;&#27493;&#39588;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#35789;&#32423;&#20132;&#38169;&#21644;&#21477;&#23376;&#21518;&#32622;&#30340;&#35821;&#35328;&#20449;&#24687;&#25554;&#20837;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#20195;&#30721;&#28151;&#21512;&#21360;&#22320;&#35821;-&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21516;&#19968;&#27573;&#25991;&#26412;&#20013;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#21483;&#20570;&#20195;&#30721;&#28151;&#21512;&#12290;&#24403;&#21069;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#65292;&#29305;&#21035;&#26159;&#33521;&#35821;&#21644;&#22320;&#26041;&#35821;&#35328;&#28151;&#21512;&#20351;&#29992;&#30340;&#25968;&#25454;&#36234;&#26469;&#36234;&#22810;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#38544;&#24615;&#35821;&#35328;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23581;&#35797;&#19981;&#21516;&#30340;&#35821;&#35328;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#39640;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#20195;&#30721;&#28151;&#21512;&#21360;&#22320;&#35821;-&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#20195;&#30721;&#28151;&#21512;&#31995;&#32479;&#30340;&#27969;&#31243;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#35789;&#32423;&#35821;&#35328;&#35782;&#21035;&#12289;&#35821;&#35328;&#22686;&#24378;&#21644;&#27169;&#22411;&#35757;&#32451;&#31561;&#27493;&#39588;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#12290;&#22312;BERT&#27169;&#22411;&#20013;&#36827;&#34892;&#35821;&#35328;&#22686;&#24378;&#26102;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35789;&#32423;&#20132;&#38169;&#21644;&#21477;&#23376;&#21518;&#32622;&#30340;&#35821;&#35328;&#20449;&#24687;&#25554;&#20837;&#26041;&#27861;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21407;&#22987;BERT&#27169;&#22411;&#21644;&#32463;&#36807;&#20195;&#30721;&#28151;&#21512;&#25913;&#36827;&#30340;HingBERT&#22312;&#21508;&#33258;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The usage of more than one language in the same text is referred to as Code Mixed. It is evident that there is a growing degree of adaption of the use of code-mixed data, especially English with a regional language, on social media platforms. Existing deep-learning models do not take advantage of the implicit language information in the code-mixed text. Our study aims to improve BERT-based models performance on low-resource Code-Mixed Hindi-English Datasets by experimenting with language augmentation approaches. We propose a pipeline to improve code-mixed systems that comprise data preprocessing, word-level language identification, language augmentation, and model training on downstream tasks like sentiment analysis. For language augmentation in BERT models, we explore word-level interleaving and post-sentence placement of language information. We have examined the performance of vanilla BERT-based models and their code-mixed HingBERT counterparts on respective benchmark datasets, comp
&lt;/p&gt;</description></item><item><title>arXiv4TGC&#25552;&#20379;&#20102;&#19968;&#32452;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#24577;&#22270;&#32858;&#31867;&#30340;&#26032;&#39062;&#23398;&#26415;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#21487;&#38752;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#32858;&#31867;&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.04962</link><description>&lt;p&gt;
arXiv4TGC&#65306;&#29992;&#20110;&#26102;&#24577;&#22270;&#32858;&#31867;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv4TGC: Large-Scale Datasets for Temporal Graph Clustering. (arXiv:2306.04962v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04962
&lt;/p&gt;
&lt;p&gt;
arXiv4TGC&#25552;&#20379;&#20102;&#19968;&#32452;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#24577;&#22270;&#32858;&#31867;&#30340;&#26032;&#39062;&#23398;&#26415;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#21487;&#38752;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#32858;&#31867;&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24577;&#22270;&#32858;&#31867;&#26159;&#26102;&#24577;&#22270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20854;&#37325;&#28857;&#26159;&#22312;&#26102;&#24577;&#22270;&#19978;&#30340;&#33410;&#28857;&#32858;&#31867;&#65292;&#24182;&#30001;&#20110;&#26102;&#24577;&#22270;&#26041;&#27861;&#30340;&#26426;&#21046;&#65292;&#20026;&#22823;&#35268;&#27169;&#22270;&#32467;&#26500;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#26102;&#24577;&#22270;&#32858;&#31867;&#30340;&#21457;&#23637;&#30446;&#21069;&#23384;&#22312;&#19968;&#20010;&#26174;&#33879;&#38382;&#39064;&#65306;&#32570;&#20047;&#36866;&#21512;&#21644;&#21487;&#38752;&#30340;&#22823;&#35268;&#27169;&#26102;&#24577;&#22270;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#32858;&#31867;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;arXiv4TGC&#65292;&#36825;&#26159;&#19968;&#32452;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#24577;&#22270;&#32858;&#31867;&#30340;&#26032;&#39062;&#23398;&#26415;&#25968;&#25454;&#38598;&#12290;&#20854;&#20013;&#65292;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;arXivLarge&#21253;&#21547;130&#19975;&#20010;&#26377;&#26631;&#31614;&#21487;&#29992;&#33410;&#28857;&#21644;1000&#19975;&#20010;&#26102;&#24577;&#36793;&#32536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graph clustering (TGC) is a crucial task in temporal graph learning. Its focus is on node clustering on temporal graphs, and it offers greater flexibility for large-scale graph structures due to the mechanism of temporal graph methods. However, the development of TGC is currently constrained by a significant problem: the lack of suitable and reliable large-scale temporal graph datasets to evaluate clustering performance. In other words, most existing temporal graph datasets are in small sizes, and even large-scale datasets contain only a limited number of available node labels. It makes evaluating models for large-scale temporal graph clustering challenging. To address this challenge, we build arXiv4TGC, a set of novel academic datasets (including arXivAI, arXivCS, arXivMath, arXivPhy, and arXivLarge) for large-scale temporal graph clustering. In particular, the largest dataset, arXivLarge, contains 1.3 million labeled available nodes and 10 million temporal edges. We further 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#36845;&#20195;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#27861;&#29992;&#20110;&#21516;&#26102;&#24674;&#22797;&#34892;&#31232;&#30095;&#21644;&#20302;&#31209;&#30340;&#25968;&#25454;&#30697;&#38453;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#21040;&#21516;&#26102;&#32467;&#26500;&#21270;&#30340;&#25968;&#25454;&#30697;&#38453;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#32463;&#39564;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04961</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#20984;&#36845;&#20195;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#27861;&#21516;&#26102;&#24674;&#22797;&#32467;&#26500;&#21270;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Recovering Simultaneously Structured Data via Non-Convex Iteratively Reweighted Least Squares. (arXiv:2306.04961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04961
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#36845;&#20195;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#27861;&#29992;&#20110;&#21516;&#26102;&#24674;&#22797;&#34892;&#31232;&#30095;&#21644;&#20302;&#31209;&#30340;&#25968;&#25454;&#30697;&#38453;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#21040;&#21516;&#26102;&#32467;&#26500;&#21270;&#30340;&#25968;&#25454;&#30697;&#38453;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#32463;&#39564;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32447;&#24615;&#35266;&#27979;&#20013;&#24674;&#22797;&#36981;&#24490;&#22810;&#20010;&#24322;&#26500;&#20302;&#32500;&#32467;&#26500;&#30340;&#25968;&#25454;&#12290;&#38024;&#23545;&#21516;&#26102;&#34892;&#31232;&#30095;&#21644;&#20302;&#31209;&#30340;&#25968;&#25454;&#30697;&#38453;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#36845;&#20195;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#65288;IRLS&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#21033;&#29992;&#36825;&#20004;&#31181;&#32467;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#20248;&#21270;&#20102;&#19968;&#31181;&#38750;&#20984;&#31232;&#30095;&#24615;&#21644;&#31209;&#30340;&#32452;&#21512;&#20195;&#29702;&#65292;&#20854;&#20013;&#24179;&#34913;&#34987;&#24314;&#20837;&#21040;&#31639;&#27861;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26368;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#65288;&#26368;&#22810;&#24120;&#25968;&#21644;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#65289;&#65292;&#36845;&#20195;&#26041;&#24335;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#21040;&#21516;&#26102;&#32467;&#26500;&#21270;&#25968;&#25454;&#30697;&#38453;&#65292;&#36825;&#23545;&#20110;&#32452;&#21512;&#20984;&#20195;&#29702;&#32780;&#35328;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;IRLS&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26377;&#21033;&#30340;&#32463;&#39564;&#25910;&#25947;&#24615;&#65292;&#20174;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#23569;&#30340;&#27979;&#37327;&#20013;&#30830;&#23450;&#20102;&#21516;&#26102;&#34892;&#31232;&#30095;&#21644;&#20302;&#31209;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new algorithm for the problem of recovering data that adheres to multiple, heterogeneous low-dimensional structures from linear observations. Focusing on data matrices that are simultaneously row-sparse and low-rank, we propose and analyze an iteratively reweighted least squares (IRLS) algorithm that is able to leverage both structures. In particular, it optimizes a combination of non-convex surrogates for row-sparsity and rank, a balancing of which is built into the algorithm. We prove locally quadratic convergence of the iterates to a simultaneously structured data matrix in a regime of minimal sample complexity (up to constants and a logarithmic factor), which is known to be impossible for a combination of convex surrogates. In experiments, we show that the IRLS method exhibits favorable empirical convergence, identifying simultaneously row-sparse and low-rank matrices from fewer measurements than state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#26032;&#20986;&#29616;&#20551;&#38899;&#39057;&#31867;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#30697;&#38453;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#33021;&#20445;&#30041;&#29616;&#26377;&#27169;&#22411;&#23545;&#24050;&#30693;&#20551;&#38899;&#39057;&#31867;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04956</link><description>&lt;p&gt;
&#20302;&#31209;&#27169;&#22411;&#21387;&#32553;&#30340;&#33258;&#36866;&#24212;&#20551;&#38899;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Fake Audio Detection with Low-Rank Model Squeezing. (arXiv:2306.04956v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#26032;&#20986;&#29616;&#20551;&#38899;&#39057;&#31867;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#30697;&#38453;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#33021;&#20445;&#30041;&#29616;&#26377;&#27169;&#22411;&#23545;&#24050;&#30693;&#20551;&#38899;&#39057;&#31867;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#38899;&#39057;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#38656;&#35201;&#24320;&#21457;&#40065;&#26834;&#24615;&#26816;&#27979;&#26041;&#27861;&#26469;&#20934;&#30830;&#35782;&#21035;&#26032;&#20986;&#29616;&#30340;&#20551;&#38899;&#39057;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#36866;&#24212;&#20110;&#26032;&#20986;&#29616;&#30340;&#20551;&#38899;&#39057;&#31867;&#22411;&#30340;&#20302;&#31209;&#36866;&#24212;&#30697;&#38453;&#26469;&#32531;&#35299;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;&#22312;&#25512;&#26029;&#38454;&#27573;&#65292;&#36825;&#20123;&#36866;&#24212;&#30697;&#38453;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#32467;&#21512;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of spoofing algorithms necessitates the development of robust detection methods capable of accurately identifying emerging fake audio. Traditional approaches, such as finetuning on new datasets containing these novel spoofing algorithms, are computationally intensive and pose a risk of impairing the acquired knowledge of known fake audio types. To address these challenges, this paper proposes an innovative approach that mitigates the limitations associated with finetuning. We introduce the concept of training low-rank adaptation matrices tailored specifically to the newly emerging fake audio types. During the inference stage, these adaptation matrices are combined with the existing model to generate the final prediction output. Extensive experimentation is conducted to evaluate the efficacy of the proposed method. The results demonstrate that our approach effectively preserves the prediction accuracy of the existing model for known fake audio types. Furthermore, o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#21457;&#29616;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#30340;&#21478;&#19968;&#20010;&#35282;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04955</link><description>&lt;p&gt;
&#35770;&#31070;&#32463;&#32593;&#32476;&#23545;&#38477;&#35299;&#22810;&#36793;&#24418;&#30340;&#24863;&#30693;&#23384;&#22312;&#30340;&#22522;&#26412;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Degraded Polygons Raise Fundamental Questions of Neural Network Perception. (arXiv:2306.04955v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#21457;&#29616;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#30340;&#21478;&#19968;&#20010;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24448;&#24448;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#19981;&#19968;&#33268;&#30340;&#34892;&#20026;&#65306;&#20174;&#23545;&#25239;&#25915;&#20987;&#21040;&#22270;&#20687;&#25439;&#22351;&#65292;&#28145;&#24230;&#23398;&#20064;&#35270;&#35273;&#27169;&#22411;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#28982;&#32780;&#20154;&#31867;&#21364;&#33021;&#22815;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24674;&#22797;&#21463;&#25439;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#22312;&#20154;&#31867;&#35270;&#35273;&#30340;&#8220;&#35782;&#21035;&#32452;&#20214;&#8221;&#29702;&#35770;&#20013;&#39318;&#27425;&#24341;&#20837;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#33258;&#21160;&#21270;&#24418;&#29366;&#21487;&#24674;&#22797;&#24615;&#27979;&#35797;&#65292;&#24555;&#36895;&#29983;&#25104;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23558;&#21382;&#21490;&#19978;&#25163;&#21160;&#21019;&#24314;&#22270;&#20687;&#21487;&#24674;&#22797;&#24615;&#23454;&#39564;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#29616;&#20195;&#21270;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#22810;&#36793;&#24418;&#30340;&#33021;&#21147;&#20197;&#21450;&#20854;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well-known that modern computer vision systems often exhibit behaviors misaligned with those of humans: from adversarial attacks to image corruptions, deep learning vision models suffer in a variety of settings that humans capably handle. In light of these phenomena, here we introduce another, orthogonal perspective studying the human-machine vision gap. We revisit the task of recovering images under degradation, first introduced over 30 years ago in the Recognition-by-Components theory of human vision. Specifically, we study the performance and behavior of neural networks on the seemingly simple task of classifying regular polygons at varying orders of degradation along their perimeters. To this end, we implement the Automated Shape Recoverability Test for rapidly generating large-scale datasets of perimeter-degraded regular polygons, modernizing the historically manual creation of image recoverability experiments. We then investigate the capacity of neural networks to recognize
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;KL&#35757;&#32451;&#27861;&#21644;Fisher&#35757;&#32451;&#27861;&#26469;&#35757;&#32451;&#23427;&#65292;&#23454;&#29616;&#20102;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#22823;&#25209;&#37327;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.04952</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Entropy-based Training Methods for Scalable Neural Implicit Sampler. (arXiv:2306.04952v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;KL&#35757;&#32451;&#27861;&#21644;Fisher&#35757;&#32451;&#27861;&#26469;&#35757;&#32451;&#23427;&#65292;&#23454;&#29616;&#20102;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#22823;&#25209;&#37327;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20174;&#38750;&#26631;&#20934;&#30446;&#26631;&#20998;&#24067;&#20013;&#37319;&#26679;&#26159;&#31185;&#23398;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#39532;&#23572;&#31185;&#22827;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#21487;&#20445;&#35777;&#20174;&#36825;&#20123;&#20998;&#24067;&#20013;&#28176;&#36827;&#26080;&#20559;&#37319;&#26679;&#65292;&#20294;&#22312;&#22788;&#29702;&#39640;&#32500;&#30446;&#26631;&#26102;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#65292;&#38656;&#35201;&#22810;&#27425;&#36845;&#20195;&#29983;&#25104;&#19968;&#25209;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#30452;&#25509;&#23558;&#26131;&#20110;&#37319;&#26679;&#30340;&#28508;&#22312;&#21521;&#37327;&#26144;&#23556;&#21040;&#30446;&#26631;&#26679;&#26412;&#30340;&#31070;&#32463;&#21464;&#25442;&#65292;&#21487;&#20197;&#22312;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#22823;&#25209;&#37327;&#26679;&#26412;&#12290;&#20026;&#20102;&#35757;&#32451;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65306;KL&#35757;&#32451;&#27861;&#21644;Fisher&#35757;&#32451;&#27861;&#12290;&#21069;&#32773;&#26368;&#23567;&#21270;Kullback-Leibler&#25955;&#24230;&#65292;&#32780;&#21518;&#32773;&#21017;&#26368;&#23567;&#21270;Fisher&#25955;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently sampling from un-normalized target distributions is a fundamental problem in scientific computing and machine learning. Traditional approaches like Markov Chain Monte Carlo (MCMC) guarantee asymptotically unbiased samples from such distributions but suffer from computational inefficiency, particularly when dealing with high-dimensional targets, as they require numerous iterations to generate a batch of samples. In this paper, we propose an efficient and scalable neural implicit sampler that overcomes these limitations. Our sampler can generate large batches of samples with low computational costs by leveraging a neural transformation that directly maps easily sampled latent vectors to target samples without the need for iterative procedures. To train the neural implicit sampler, we introduce two novel methods: the KL training method and the Fisher training method. The former minimizes the Kullback-Leibler divergence, while the latter minimizes the Fisher divergence. By empl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#65292;&#22312;&#23384;&#22312;&#34394;&#20551;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#32452;&#30340;&#19981;&#24179;&#34913;&#21644;&#26131;&#20110;&#23398;&#20064;&#30340;&#34394;&#20551;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#29305;&#24449;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;PDE&#65292;&#23427;&#36880;&#27493;&#25193;&#23637;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#20854;&#26368;&#21155;&#32452;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#23454;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#36229;&#36234;&#20102;R&#27169;&#22411;&#31561;&#20854;&#23427;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.04949</link><description>&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#25193;&#23637;&#25968;&#25454;&#23545;&#25239;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning with Progressive Data Expansion Against Spurious Correlation. (arXiv:2306.04949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#65292;&#22312;&#23384;&#22312;&#34394;&#20551;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#32452;&#30340;&#19981;&#24179;&#34913;&#21644;&#26131;&#20110;&#23398;&#20064;&#30340;&#34394;&#20551;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#29305;&#24449;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;PDE&#65292;&#23427;&#36880;&#27493;&#25193;&#23637;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#20854;&#26368;&#21155;&#32452;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#23454;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#36229;&#36234;&#20102;R&#27169;&#22411;&#31561;&#20854;&#23427;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#26131;&#20110;&#23398;&#20064;&#19982;&#30495;&#23454;&#26631;&#31614;&#26080;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#19982;&#26631;&#31614;&#30456;&#20851;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#24050;&#26377;&#32447;&#24615;&#27169;&#22411;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#20174;&#29702;&#35770;&#19978;&#26816;&#26597;&#20102;&#23384;&#22312;&#34394;&#20551;&#29305;&#24449;&#26102;&#20004;&#23618;&#38750;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25968;&#25454;&#32452;&#19981;&#24179;&#34913;&#21644;&#26131;&#20110;&#23398;&#20064;&#30340;&#34394;&#20551;&#29305;&#24449;&#21487;&#33021;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23548;&#33268;&#34394;&#20551;&#29305;&#24449;&#30340;&#25903;&#37197;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PDE&#30340;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#26368;&#24046;&#32452;&#24615;&#33021;&#12290;PDE&#20174;&#19968;&#32452;&#24179;&#34913;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#24320;&#22987;&#65292;&#24182;&#36880;&#27493;&#25193;&#23637;&#20854;&#22823;&#23567;&#20197;&#20419;&#36827;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#22411;&#65288;&#20363;&#22914;R&#65289;&#19978;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning models have shown remarkable performance in various tasks, they are susceptible to learning non-generalizable spurious features rather than the core features that are genuinely correlated to the true label. In this paper, beyond existing analyses of linear models, we theoretically examine the learning process of a two-layer nonlinear convolutional neural network in the presence of spurious features. Our analysis suggests that imbalanced data groups and easily learnable spurious features can lead to the dominance of spurious features during the learning process. In light of this, we propose a new training algorithm called PDE that efficiently enhances the model's robustness for a better worst-group performance. PDE begins with a group-balanced subset of training data and progressively expands it to facilitate the learning of the core features. Experiments on synthetic and real-world benchmark datasets confirm the superior performance of our method on models such as R
&lt;/p&gt;</description></item><item><title>ShuttleSet&#26159;&#19968;&#20221;&#32701;&#27611;&#29699;&#21333;&#25171;&#27604;&#36187;&#30340;&#25293;&#32423;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;104&#22330;&#27604;&#36187;&#12289;3,685&#36718;&#27604;&#36187;&#12289;36,492&#20010;&#25293;&#20987;&#65292;&#24182;&#28085;&#30422;&#20102;27&#21517;&#25490;&#21517;&#21069;&#21015;&#30340;&#30007;&#23376;&#21644;&#22899;&#23376;&#21333;&#25171;&#36873;&#25163;&#12290;&#36825;&#20123;&#25293;&#32423;&#35760;&#24405;&#23558;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#22312;&#20307;&#32946;&#20998;&#26512;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.04948</link><description>&lt;p&gt;
ShuttleSet: &#19968;&#20221;&#20154;&#24037;&#26631;&#27880;&#30340;&#32701;&#27611;&#29699;&#21333;&#25171;&#27604;&#36187;&#30340;&#25293;&#32423;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25112;&#26415;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
ShuttleSet: A Human-Annotated Stroke-Level Singles Dataset for Badminton Tactical Analysis. (arXiv:2306.04948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04948
&lt;/p&gt;
&lt;p&gt;
ShuttleSet&#26159;&#19968;&#20221;&#32701;&#27611;&#29699;&#21333;&#25171;&#27604;&#36187;&#30340;&#25293;&#32423;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;104&#22330;&#27604;&#36187;&#12289;3,685&#36718;&#27604;&#36187;&#12289;36,492&#20010;&#25293;&#20987;&#65292;&#24182;&#28085;&#30422;&#20102;27&#21517;&#25490;&#21517;&#21069;&#21015;&#30340;&#30007;&#23376;&#21644;&#22899;&#23376;&#21333;&#25171;&#36873;&#25163;&#12290;&#36825;&#20123;&#25293;&#32423;&#35760;&#24405;&#23558;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#22312;&#20307;&#32946;&#20998;&#26512;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20307;&#32946;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20986;&#25366;&#25496;&#29699;&#21592;&#25112;&#26415;&#27934;&#23519;&#21147;&#20197;&#25552;&#39640;&#34920;&#29616;&#36136;&#37327;&#21644;&#29699;&#36855;&#21442;&#19982;&#24230;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#24402;&#22240;&#20110;&#20844;&#20849;&#22522;&#30784;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#34429;&#28982;&#26377;&#19968;&#20123;&#29992;&#20110;&#34892;&#21160;&#26816;&#27979;&#30340;&#22238;&#21512;&#27604;&#36187;&#30340;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#20005;&#37325;&#32570;&#20047;&#32467;&#26500;&#21270;&#30340;&#26469;&#28304;&#25968;&#25454;&#21644;&#25293;&#32423;&#35760;&#24405;&#65292;&#22240;&#20026;&#36825;&#20123;&#38656;&#35201;&#26469;&#33258;&#39046;&#22495;&#19987;&#23478;&#30340;&#39640;&#25104;&#26412;&#26631;&#35760;&#24037;&#20316;&#65292;&#24182;&#19988;&#24456;&#38590;&#20351;&#29992;&#33258;&#21160;&#25216;&#26415;&#26816;&#27979;&#21040;&#12290;&#22240;&#27492;&#65292;&#24403;&#29616;&#26377;&#27169;&#22411;&#24212;&#29992;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32467;&#26500;&#21270;&#22238;&#21512;&#24207;&#21015;&#26102;&#65292;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#24320;&#21457;&#21463;&#21040;&#37325;&#22823;&#21046;&#32422;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; ShuttleSet&#65292;&#36825;&#26159;&#26368;&#22823;&#30340;&#20844;&#24320;&#32701;&#27611;&#29699;&#21333;&#25171;&#27604;&#36187;&#30340;&#25293;&#32423;&#35760;&#24405;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;2018&#24180;&#33267;2021&#24180;&#38388;&#30340;44&#22330;&#27604;&#36187;&#20013;&#30340;104&#30424;&#27604;&#36187;&#65292;3,685&#36718;&#27604;&#36187;&#21644;36,492&#20010;&#25293;&#20987;&#65292;&#24182;&#28085;&#30422;&#20102;27&#21517;&#25490;&#21517;&#21069;&#21015;&#30340;&#30007;&#23376;&#21644;&#22899;&#23376;&#21333;&#25171;&#36873;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent progress in sports analytics, deep learning approaches have demonstrated the effectiveness of mining insights into players' tactics for improving performance quality and fan engagement. This is attributed to the availability of public ground-truth datasets. While there are a few available datasets for turn-based sports for action detection, these datasets severely lack structured source data and stroke-level records since these require high-cost labeling efforts from domain experts and are hard to detect using automatic techniques. Consequently, the development of artificial intelligence approaches is significantly hindered when existing models are applied to more challenging structured turn-based sequences. In this paper, we present ShuttleSet, the largest publicly-available badminton singles dataset with annotated stroke-level records. It contains 104 sets, 3,685 rallies, and 36,492 strokes in 44 matches between 2018 and 2021 with 27 top-ranking men's singles and wome
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#35821;&#26009;&#24211;&#20013;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#20027;&#39064;&#30340;&#21487;&#35835;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04941</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#33391;&#30340;&#35821;&#26009;&#20027;&#39064;&#26816;&#27979;&#27169;&#22411;&#21450;&#35780;&#20272;&#20027;&#39064;&#21487;&#35835;&#24615;&#30340;&#26032;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
A modified model for topic detection from a corpus and a new metric evaluating the understandability of topics. (arXiv:2306.04941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#35821;&#26009;&#24211;&#20013;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#20027;&#39064;&#30340;&#21487;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#33391;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#20174;&#35821;&#26009;&#24211;&#20013;&#26816;&#27979;&#20027;&#39064;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#25152;&#26816;&#27979;&#21040;&#30340;&#20027;&#39064;&#12290;&#26032;&#27169;&#22411;&#24314;&#31435;&#22312;&#23884;&#20837;&#24335;&#20027;&#39064;&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#24182;&#21152;&#20837;&#20102;&#19968;&#20123;&#25913;&#36827;&#65292;&#22914;&#25991;&#26723;&#32858;&#31867;&#31561;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#38271;&#24230;&#30340;&#25991;&#26723;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#26032;&#30340;&#25351;&#26631;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35745;&#31639;&#20027;&#39064;&#21487;&#35835;&#24615;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#25152;&#26816;&#27979;&#21040;&#20027;&#39064;&#30340;&#21487;&#29702;&#35299;&#31243;&#24230;&#30340;&#19981;&#21516;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a modified neural model for topic detection from a corpus and proposes a new metric to evaluate the detected topics. The new model builds upon the embedded topic model incorporating some modifications such as document clustering. Numerical experiments suggest that the new model performs favourably regardless of the document's length. The new metric, which can be computed more efficiently than widely-used metrics such as topic coherence, provides variable information regarding the understandability of the detected topics.
&lt;/p&gt;</description></item><item><title>&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;</title><link>http://arxiv.org/abs/2306.04940</link><description>&lt;p&gt;
&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Layer-level activation mechanism. (arXiv:2306.04940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04940
&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28608;&#27963;&#26426;&#21046;&#65292;&#26088;&#22312;&#24314;&#31435;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#21151;&#33021;&#65288;LayerAct&#65289;&#12290;&#36825;&#20123;&#21151;&#33021;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36755;&#20837;&#20559;&#31227;&#25152;&#23548;&#33268;&#30340;&#28608;&#27963;&#36755;&#20986;&#30340;&#20998;&#23618;&#32423;&#27874;&#21160;&#26469;&#38477;&#20302;&#20256;&#32479;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#30340;&#22122;&#38899;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;LayerAct&#21151;&#33021;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#38646;&#30340;&#24179;&#22343;&#28608;&#27963;&#36755;&#20986;&#65292;&#32780;&#19981;&#38480;&#21046;&#28608;&#27963;&#36755;&#20986;&#31354;&#38388;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;LayerAct&#21151;&#33021;&#22312;&#22122;&#22768;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#65292;&#24182;&#19988;&#32463;&#39564;&#35777;&#26126;&#36825;&#20123;&#21151;&#33021;&#30340;&#24179;&#22343;&#28608;&#27963;&#32467;&#26524;&#31867;&#20284;&#20110;&#38646;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22024;&#26434;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#26102;&#65292;LayerAct&#21151;&#33021;&#27604;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#28165;&#27905;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#20063;&#26159;&#20248;&#36234;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel activation mechanism aimed at establishing layer-level activation (LayerAct) functions. These functions are designed to be more noise-robust compared to traditional element-level activation functions by reducing the layer-level fluctuation of the activation outputs due to shift in inputs. Moreover, the LayerAct functions achieve a zero-like mean activation output without restricting the activation output space. We present an analysis and experiments demonstrating that LayerAct functions exhibit superior noise-robustness compared to element-level activation functions, and empirically show that these functions have a zero-like mean activation. Experimental results on three benchmark image classification tasks show that LayerAct functions excel in handling noisy image datasets, outperforming element-level activation functions, while the performance on clean datasets is also superior in most cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25552;&#31034;&#21644;&#20854;&#20182;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#20114;&#20449;&#24687;&#26469;&#20248;&#21270;&#36719;&#25552;&#31034;&#35843;&#25972;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#26356;&#21152;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;InfoPrompt&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#21457;&#29616;&#21512;&#36866;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#65292;&#24182;&#20174;&#25552;&#31034;&#20196;&#29260;&#20013;&#23398;&#20064;&#36275;&#22815;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#21516;&#26102;&#40723;&#21169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#34920;&#31034;&#26356;&#21152;&#20851;&#27880;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.04933</link><description>&lt;p&gt;
InfoPrompt&#65306;&#29992;&#20449;&#24687;&#35770;&#36719;&#25552;&#31034;&#35843;&#25972;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding. (arXiv:2306.04933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25552;&#31034;&#21644;&#20854;&#20182;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#20114;&#20449;&#24687;&#26469;&#20248;&#21270;&#36719;&#25552;&#31034;&#35843;&#25972;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#26356;&#21152;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;InfoPrompt&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#21457;&#29616;&#21512;&#36866;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#65292;&#24182;&#20174;&#25552;&#31034;&#20196;&#29260;&#20013;&#23398;&#20064;&#36275;&#22815;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#21516;&#26102;&#40723;&#21169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#34920;&#31034;&#26356;&#21152;&#20851;&#27880;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#25552;&#31034;&#35843;&#25972;&#22312;&#24191;&#27867;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#25552;&#31034;&#35843;&#25972;&#30340;&#24615;&#33021;&#23545;&#21021;&#22987;&#21270;&#30340;&#25552;&#31034;&#38750;&#24120;&#25935;&#24863;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#19981;&#33021;&#20174;&#25552;&#31034;&#20196;&#29260;&#20013;&#32534;&#30721;&#21644;&#23398;&#20064;&#36275;&#22815;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#23558;&#36719;&#25552;&#31034;&#35843;&#25972;&#24418;&#24335;&#21270;&#20026;&#26368;&#22823;&#21270;&#25552;&#31034;&#21644;&#20854;&#20182;&#27169;&#22411;&#21442;&#25968;&#65288;&#25110;&#32534;&#30721;&#34920;&#31034;&#65289;&#20043;&#38388;&#20114;&#20449;&#24687;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#35266;&#28857;&#26377;&#21161;&#20110;&#25105;&#20204;&#24320;&#21457;&#19968;&#20010;&#26356;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;InfoPrompt&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#65288;i&#65289;&#21457;&#29616;&#19979;&#28216;&#20219;&#21153;&#30340;&#21512;&#36866;&#25552;&#31034;&#21021;&#22987;&#21270;&#65292;&#24182;&#20174;&#25552;&#31034;&#20196;&#29260;&#20013;&#23398;&#20064;&#36275;&#22815;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#65288;ii&#65289;&#40723;&#21169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#34920;&#31034;&#26356;&#21152;&#20851;&#27880;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft prompt tuning achieves superior performances across a wide range of few-shot tasks. However, the performances of prompt tuning can be highly sensitive to the initialization of the prompts. We also empirically observe that conventional prompt tuning methods cannot encode and learn sufficient task-relevant information from prompt tokens. In this work, we develop an information-theoretic framework that formulates soft prompt tuning as maximizing mutual information between prompts and other model parameters (or encoded representations). This novel view helps us to develop a more efficient, accurate and robust soft prompt tuning method InfoPrompt. With this framework, we develop two novel mutual information based loss functions, to (i) discover proper prompt initialization for the downstream tasks and learn sufficient task-relevant information from prompt tokens and (ii) encourage the output representation from the pretrained language model to be more aware of the task-relevant informa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20808;&#21069;&#25968;&#25454;&#30340;&#24178;&#39044;&#25514;&#26045;&#25552;&#39640;&#22522;&#20110;AI&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;CDHF&#26694;&#26550;&#26469;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;&#65292;&#39044;&#27979;&#24314;&#35758;&#25509;&#21463;&#31243;&#24230;&#24182;&#20915;&#23450;&#20309;&#26102;&#23637;&#31034;&#21738;&#20123;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04930</link><description>&lt;p&gt;
&#20309;&#26102;&#23637;&#31034;&#24314;&#35758;&#65311;&#22312;AI&#36741;&#21161;&#32534;&#31243;&#20013;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
When to Show a Suggestion? Integrating Human Feedback in AI-Assisted Programming. (arXiv:2306.04930v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20808;&#21069;&#25968;&#25454;&#30340;&#24178;&#39044;&#25514;&#26045;&#25552;&#39640;&#22522;&#20110;AI&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;CDHF&#26694;&#26550;&#26469;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;&#65292;&#39044;&#27979;&#24314;&#35758;&#25509;&#21463;&#31243;&#24230;&#24182;&#20915;&#23450;&#20309;&#26102;&#23637;&#31034;&#21738;&#20123;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#65292;&#22914;Copilot&#21644;CodeWhisperer&#65292;&#25552;&#20379;&#31243;&#24207;&#21592;&#29615;&#22659;&#65288;&#20363;&#22914;IDE&#65289;&#20869;&#30340;&#20195;&#30721;&#24314;&#35758;&#65292;&#26088;&#22312;&#25552;&#39640;&#20182;&#20204;&#30340;&#29983;&#20135;&#21147;&#12290;&#30001;&#20110;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#31243;&#24207;&#21592;&#25509;&#21463;&#21644;&#25298;&#32477;&#24314;&#35758;&#65292;&#22240;&#27492;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#35813;&#31995;&#32479;&#24212;&#20351;&#29992;&#27492;&#21453;&#39304;&#20197;&#20419;&#36827;&#36825;&#19968;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#31243;&#24207;&#21592;&#19982;Copilot&#20132;&#20114;&#30340;&#20808;&#21069;&#25968;&#25454;&#65292;&#24320;&#21457;&#21487;&#20197;&#33410;&#30465;&#31243;&#24207;&#21592;&#26102;&#38388;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#19982;&#31243;&#24207;&#21592;&#30340;&#20132;&#20114;&#65292;&#24182;&#20915;&#23450;&#20309;&#26102;&#23637;&#31034;&#21738;&#20123;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#8220;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#26465;&#20214;&#24314;&#35758;&#23637;&#31034;&#8221;&#65288;CDHF&#65289;&#22522;&#20110;&#23545;&#31243;&#24207;&#21592;&#25805;&#20316;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;535&#21517;&#31243;&#24207;&#21592;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21487;&#20197;&#39044;&#27979;&#24314;&#35758;&#25509;&#21463;&#31243;&#24230;&#30340;&#27169;&#22411;&#12290;&#22312;&#23545;&#36890;&#36807;AI&#36741;&#21161;&#32534;&#31243;&#35299;&#20915;&#30340;&#30495;&#23454;&#19990;&#30028;&#32534;&#31243;&#20219;&#21153;&#30340;&#22238;&#39038;&#24615;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;CDHF&#33021;&#22815;&#23454;&#29616;&#26377;&#21033;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;AI&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI powered code-recommendation systems, such as Copilot and CodeWhisperer, provide code suggestions inside a programmer's environment (e.g., an IDE) with the aim to improve their productivity. Since, in these scenarios, programmers accept and reject suggestions, ideally, such a system should use this feedback in furtherance of this goal. In this work we leverage prior data of programmers interacting with Copilot to develop interventions that can save programmer time. We propose a utility theory framework, which models this interaction with programmers and decides when and which suggestions to display. Our framework Conditional suggestion Display from Human Feedback (CDHF) is based on predictive models of programmer actions. Using data from 535 programmers we build models that predict the likelihood of suggestion acceptance. In a retrospective evaluation on real-world programming tasks solved with AI-assisted programming, we find that CDHF can achieve favorable tradeoffs. Our findings s
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;covLLM&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;COVID-19&#25991;&#29486;&#12290;covLLM&#21487;&#20197;&#27719;&#24635;&#21644;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#22909;&#22320;&#24212;&#23545;COVID-19&#30123;&#24773;&#12290;</title><link>http://arxiv.org/abs/2306.04926</link><description>&lt;p&gt;
covLLM&#65306;&#29992;&#20110;COVID-19&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
covLLM: Large Language Models for COVID-19 Biomedical Literature. (arXiv:2306.04926v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04926
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;covLLM&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;COVID-19&#25991;&#29486;&#12290;covLLM&#21487;&#20197;&#27719;&#24635;&#21644;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#22909;&#22320;&#24212;&#23545;COVID-19&#30123;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26032;&#20896;&#30149;&#27602;&#30340;&#30740;&#31350;&#22312;&#19981;&#26029;&#22686;&#21152;&#65292;&#20294;COVID-19&#22823;&#27969;&#34892;&#23548;&#33268;&#20102;&#32654;&#22269;110&#19975;&#20154;&#30340;&#27515;&#20129;&#12290;&#36825;&#20123;&#26032;&#21457;&#29616;&#22312;&#36716;&#21270;&#20026;&#20020;&#24202;&#24178;&#39044;&#26041;&#26696;&#26041;&#38754;&#32531;&#24930;&#65292;&#23548;&#33268;&#24739;&#32773;&#39044;&#21518;&#36739;&#24046;&#21644;&#19981;&#24517;&#35201;&#30340;&#27515;&#20129;&#12290;&#20854;&#20013;&#19968;&#31181;&#21407;&#22240;&#26159;&#20020;&#24202;&#21307;&#29983;&#22240;&#24739;&#32773;&#36807;&#22810;&#32780;&#38590;&#20197;&#36319;&#19978;&#26032;&#20896;&#30149;&#27602;&#25991;&#29486;&#30340;&#36895;&#24230;&#12290;&#21457;&#23637;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35780;&#20272;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#30340;&#24037;&#20855;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21487;&#33021;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;LLMs&#21487;&#29992;&#20110;&#27719;&#24635;&#21644;&#25552;&#21462;&#29992;&#25143;&#25351;&#23450;&#30340;&#20449;&#24687;&#12290;&#36739;&#22823;&#33539;&#22260;&#21644;&#20808;&#36827;&#30340;LLMs&#21644;&#39044;&#22788;&#29702;&#30340;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#36890;&#36807;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#29305;&#23450;LLM&#65288;covLLM&#65289;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#30340;&#26426;&#20250;&#65292;&#35813;&#24037;&#20855;&#30452;&#25509;&#36755;&#20837;&#30740;&#31350;&#25991;&#31456;&#21644;&#29992;&#25143;&#26597;&#35810;&#20197;&#36820;&#22238;&#31572;&#26696;&#12290;&#22312;&#20351;&#29992;COVID-19&#24320;&#25918;&#30740;&#31350;&#25968;&#25454;&#38598;&#65288;CORD-19&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;covLLM&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#24635;&#32467;&#21644;&#20174;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic led to 1.1 million deaths in the United States, despite the explosion of coronavirus research. These new findings are slow to translate to clinical interventions, leading to poorer patient outcomes and unnecessary deaths. One reason is that clinicians, overwhelmed by patients, struggle to keep pace with the rate of new coronavirus literature. A potential solution is developing a tool for evaluating coronavirus literature using large language models (LLMs) -- neural networks that are deployed for natural language processing. LLMs can be used to summarize and extract user-specified information. The greater availability and advancement of LLMs and pre-processed coronavirus literature databases provide the opportunity to assist clinicians in evaluating coronavirus literature through a coronavirus literature specific LLM (covLLM), a tool that directly takes an inputted research article and a user query to return an answer. Using the COVID-19 Open Research Dataset (CORD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#20559;&#22909;&#20998;&#31867;&#23398;&#20064;&#36741;&#21161;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#25991;&#26412;&#23545;&#20043;&#38388;&#30340;&#20559;&#22909;&#20851;&#31995;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20026;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2306.04925</link><description>&lt;p&gt;
&#20559;&#22909;&#20998;&#31867;&#65306;&#36890;&#36807;&#36741;&#21161;&#20559;&#22909;&#23398;&#20064;&#25913;&#36827;&#25991;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning. (arXiv:2306.04925v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#20559;&#22909;&#20998;&#31867;&#23398;&#20064;&#36741;&#21161;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#25991;&#26412;&#23545;&#20043;&#38388;&#30340;&#20559;&#22909;&#20851;&#31995;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20026;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#20154;&#24037;&#26631;&#27880;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#25512;&#21160;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#12290;&#20026;&#20102;&#22686;&#24378;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#65292;&#25910;&#38598;&#26032;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;&#36890;&#24120;&#36807;&#20110;&#26114;&#36149;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#23427;&#20204;&#23545;&#25552;&#39640;&#24403;&#21069;&#27169;&#22411;&#31934;&#24230;&#30340;&#36793;&#38469;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#29616;&#26377;&#36755;&#20837;&#25991;&#26412;&#30340;&#38468;&#21152;&#25110;&#34917;&#20805;&#26631;&#27880;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#39069;&#22806;&#20154;&#24037;&#25104;&#26412;&#25903;&#20184;&#26041;&#24335;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#36755;&#20837;&#25991;&#26412;&#23545;&#20043;&#38388;&#30340;&#20559;&#22909;&#20851;&#31995;&#20316;&#20026;&#36825;&#31181;&#36741;&#21161;&#25968;&#25454;&#26631;&#27880;&#30340;&#26032;&#26367;&#20195;&#26041;&#24335;&#12290;&#20174;&#20219;&#21153;&#30456;&#20851;&#30340;&#8220;&#25104;&#23545;&#8221;&#27604;&#36739;&#20013;&#65292;&#36741;&#21161;&#20559;&#22909;&#23398;&#20064;&#20351;&#27169;&#22411;&#23398;&#20064;&#21040;&#19968;&#31181;&#39069;&#22806;&#30340;&#20449;&#24687;&#24615;&#35757;&#32451;&#20449;&#21495;&#65292;&#36825;&#31181;&#20449;&#21495;&#26080;&#27861;&#36890;&#36807;&#8220;&#23454;&#20363;&#32423;&#8221;&#30340;&#20219;&#21153;&#26631;&#31614;&#26469;&#25429;&#25417;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;prefer-to-classify &#65288;P2C&#65289;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#20139;&#21463;&#23454;&#20363;&#32423;&#21644;&#20559;&#22909;&#32423;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of largely human-annotated benchmarks has driven the success of deep neural networks in various NLP tasks. To enhance the effectiveness of existing benchmarks, collecting new additional input-output pairs is often too costly and challenging, particularly considering their marginal impact on improving the current model accuracy. Instead, additional or complementary annotations on the existing input texts in the benchmarks can be preferable as an efficient way to pay the additional human cost. In this paper, we investigate task-specific preferences between pairs of input texts as a new alternative way for such auxiliary data annotation. From 'pair-wise' comparisons with respect to the task, the auxiliary preference learning enables the model to learn an additional informative training signal that cannot be captured with 'instance-wise' task labels. To this end, we propose a novel multi-task learning framework, called prefer-to-classify (P2C), which can enjoy the cooperati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#22312;&#36890;&#20449;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#31934;&#30830;&#26368;&#20248;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#26059;&#36716;&#23545;&#31216;&#30340;&#20849;&#20139;&#38543;&#26426;&#30721;&#20070;&#65292;&#24182;&#36890;&#36807;$k$-closest&#32534;&#30721;&#23454;&#29616;&#20102;&#38543;&#26426;&#26059;&#36716;&#30340;&#21333;&#32431;&#24418;$c$&#30340;&#31934;&#30830;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2306.04924</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22343;&#20540;&#20272;&#35745;&#20013;&#30340;&#36890;&#20449;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#30340;&#31934;&#30830;&#26368;&#20248;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation. (arXiv:2306.04924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#22312;&#36890;&#20449;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#31934;&#30830;&#26368;&#20248;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#26059;&#36716;&#23545;&#31216;&#30340;&#20849;&#20139;&#38543;&#26426;&#30721;&#20070;&#65292;&#24182;&#36890;&#36807;$k$-closest&#32534;&#30721;&#23454;&#29616;&#20102;&#38543;&#26426;&#26059;&#36716;&#30340;&#21333;&#32431;&#24418;$c$&#30340;&#31934;&#30830;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#20449;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#30456;&#21516;&#38382;&#39064;&#30340;\emph{&#38454;}-&#26368;&#20248;&#31639;&#27861;&#65288;&#21363;&#24403;&#25105;&#20204;&#33457;&#36153;&#26356;&#22810;&#27604;&#29305;&#26102;&#28176;&#36827;&#26368;&#20248;&#65289;&#65292;&#20294;&#22312;&#38750;&#28176;&#36827;&#35774;&#32622;&#19979;&#20173;&#28982;&#27809;&#26377;&#23454;&#29616;\emph{&#31934;&#30830;}&#26368;&#20248;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#25551;&#36848;&#20102;&#22312;&#20849;&#20139;&#38543;&#26426;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;\emph{&#31934;&#30830;}-&#26368;&#20248;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#20960;&#20010;\emph{&#31934;&#30830;}&#26368;&#20248;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#20013;&#19968;&#20010;&#24517;&#35201;&#26465;&#20214;&#26159;&#21033;&#29992;&#26059;&#36716;&#23545;&#31216;&#30340;&#20849;&#20139;&#38543;&#26426;&#30721;&#20070;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#21270;&#26426;&#21046;&#65292;&#20854;&#20013;&#30721;&#20070;&#26159;&#38543;&#26426;&#26059;&#36716;&#30340;&#21333;&#32431;&#24418;&#8212;&#8212;&#28385;&#36275;\emph{&#31934;&#30830;}-&#26368;&#20248;&#30721;&#20070;&#30340;&#24517;&#35201;&#23646;&#24615;&#12290;&#35813;&#26426;&#21046;&#22522;&#20110;&#25105;&#20204;&#35777;&#26126;&#30340;$k$&#26368;&#36817;&#32534;&#30721;&#65292;&#23545;&#20110;&#38543;&#26426;&#26059;&#36716;&#30340;&#21333;&#32431;&#24418;$c$&#26469;&#35828;&#26159;\emph{&#31934;&#30830;}-&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the mean estimation problem under communication and local differential privacy constraints. While previous work has proposed \emph{order}-optimal algorithms for the same problem (i.e., asymptotically optimal as we spend more bits), \emph{exact} optimality (in the non-asymptotic setting) still has not been achieved. In this work, we take a step towards characterizing the \emph{exact}-optimal approach in the presence of shared randomness (a random variable shared between the server and the user) and identify several necessary conditions for \emph{exact} optimality. We prove that one of the necessary conditions is to utilize a rotationally symmetric shared random codebook. Based on this, we propose a randomization mechanism where the codebook is a randomly rotated simplex -- satisfying the necessary properties of the \emph{exact}-optimal codebook. The proposed mechanism is based on a $k$-closest encoding which we prove to be \emph{exact}-optimal for the randomly rotated simplex c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#26080;&#30028;&#22495;&#21644;&#38750;Lipschitz&#25439;&#22833;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36951;&#25022;&#30340;&#24230;&#37327;&#65292;&#20197;&#34913;&#37327;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#35813;&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38797;&#28857;&#20248;&#21270;&#31639;&#27861;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26377;&#24847;&#20041;&#30340;&#26354;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22815;&#22312;&#26080;&#30028;&#39046;&#22495;&#20013;&#25910;&#25947;&#20110;&#23545;&#20598;&#38388;&#38553;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#26080;&#30028;&#22495;&#21644;&#38750;Lipschitz&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#30340;&#21160;&#24577;&#36951;&#25022;&#65292;&#20197;&#21450;&#30456;&#21305;&#37197;&#30340;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.04923</link><description>&lt;p&gt;
&#26080;&#32422;&#26463;&#22312;&#32447;&#23398;&#20064;&#21644;&#26080;&#30028;&#25439;&#22833;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unconstrained Online Learning with Unbounded Losses. (arXiv:2306.04923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#26080;&#30028;&#22495;&#21644;&#38750;Lipschitz&#25439;&#22833;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36951;&#25022;&#30340;&#24230;&#37327;&#65292;&#20197;&#34913;&#37327;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#35813;&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38797;&#28857;&#20248;&#21270;&#31639;&#27861;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26377;&#24847;&#20041;&#30340;&#26354;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22815;&#22312;&#26080;&#30028;&#39046;&#22495;&#20013;&#25910;&#25947;&#20110;&#23545;&#20598;&#38388;&#38553;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#26080;&#30028;&#22495;&#21644;&#38750;Lipschitz&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#30340;&#21160;&#24577;&#36951;&#25022;&#65292;&#20197;&#21450;&#30456;&#21305;&#37197;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#25110;&#22810;&#20010;&#26377;&#30028;&#24615;&#20551;&#35774;&#65306;&#21363;&#22495;&#26159;&#26377;&#30028;&#30340;&#65292;&#25439;&#22833;&#26159;Lipschitz&#30340;&#25110;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#20855;&#26377;&#26080;&#30028;&#22495;&#21644;&#38750;Lipschitz&#25439;&#22833;&#30340;&#22312;&#32447;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#12290;&#38024;&#23545;&#35813;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#20219;&#20309;&#28385;&#36275;&#23376;&#26799;&#24230;&#28385;&#36275;$\|g_{t}\|\le G+L\|w_{t}\|$&#30340;&#38382;&#39064;&#20013;&#65292;&#20854;&#36951;&#25022;&#30340;&#24230;&#37327;&#20540;$R_{T}(u)\le \tilde O(G\|u\|\sqrt{T}+L\|u\|^{2}\sqrt{T})$&#65292;&#24182;&#19988;&#34920;&#26126;&#38500;&#38750;&#26377;&#36827;&#19968;&#27493; &#20551;&#35774;&#65292;&#21542;&#21017;&#35813;&#30028;&#38480;&#26159;&#19981;&#33021;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for online learning typically require one or more boundedness assumptions: that the domain is bounded, that the losses are Lipschitz, or both. In this paper, we develop a new setting for online learning with unbounded domains and non-Lipschitz losses. For this setting we provide an algorithm which guarantees $R_{T}(u)\le \tilde O(G\|u\|\sqrt{T}+L\|u\|^{2}\sqrt{T})$ regret on any problem where the subgradients satisfy $\|g_{t}\|\le G+L\|w_{t}\|$, and show that this bound is unimprovable without further assumptions. We leverage this algorithm to develop new saddle-point optimization algorithms that converge in duality gap in unbounded domains, even in the absence of meaningful curvature. Finally, we provide the first algorithm achieving non-trivial dynamic regret in an unbounded domain for non-Lipschitz losses, as well as a matching lower bound. The regret of our dynamic regret algorithm automatically improves to a novel $L^{*}$ bound when the losses are smooth.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QHNet&#30340;SE(3)-&#31561;&#21464;&#32593;&#32476;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#31561;&#21464;&#24615;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;QHNet&#22312;&#26356;&#24555;&#30340;&#36895;&#24230;&#19979;&#23454;&#29616;&#20102;&#19982;&#20854;&#21487;&#27604;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#28040;&#32791;&#30340;&#20869;&#23384;&#23569;&#20102;50&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.04922</link><description>&lt;p&gt;
&#39640;&#25928;&#21644;&#31561;&#21464;&#22270;&#32593;&#32476;&#39044;&#27979;&#37327;&#23376;&#21704;&#23494;&#39039;
&lt;/p&gt;
&lt;p&gt;
Efficient and Equivariant Graph Networks for Predicting Quantum Hamiltonian. (arXiv:2306.04922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QHNet&#30340;SE(3)-&#31561;&#21464;&#32593;&#32476;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#31561;&#21464;&#24615;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;QHNet&#22312;&#26356;&#24555;&#30340;&#36895;&#24230;&#19979;&#23454;&#29616;&#20102;&#19982;&#20854;&#21487;&#27604;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#28040;&#32791;&#30340;&#20869;&#23384;&#23569;&#20102;50&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#29992;&#20110;&#37327;&#23376;&#21270;&#23398;&#21644;&#20957;&#32858;&#24577;&#29289;&#29702;&#20013;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#39044;&#27979;&#12290;&#25928;&#29575;&#21644;&#31561;&#21464;&#24615;&#26159;&#20004;&#20010;&#37325;&#35201;&#20294;&#20914;&#31361;&#30340;&#22240;&#32032;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SE(3)-&#31561;&#21464;&#32593;&#32476;&#65292;&#31216;&#20026;QHNet&#65292;&#26082;&#23454;&#29616;&#20102;&#25928;&#29575;&#21448;&#23454;&#29616;&#20102;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#36827;&#23637;&#22312;&#20110;QHNet&#26550;&#26500;&#30340;&#21019;&#26032;&#35774;&#35745;&#65292;&#23427;&#19981;&#20165;&#36981;&#23432;&#22522;&#26412;&#30340;&#23545;&#31216;&#24615;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;92&#65285;&#30340;&#24352;&#37327;&#31215;&#25968;&#37327;&#26469;&#20943;&#23569;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;QHNet&#22312;&#28041;&#21450;&#26356;&#22810;&#21407;&#23376;&#31867;&#22411;&#26102;&#65292;&#21487;&#20197;&#38450;&#27490;&#36890;&#36947;&#32500;&#24230;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#25105;&#20204;&#22312;MD17&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;&#22235;&#20010;&#20998;&#23376;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;QHNet&#21487;&#20197;&#22312;&#26174;&#33879;&#26356;&#24555;&#30340;&#36895;&#24230;&#19979;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#21487;&#27604;&#25311;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20854;&#31616;&#21270;&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#30340;QHNet&#28040;&#32791;&#30340;&#20869;&#23384;&#23569;&#20102;50&#65285;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20316;&#20026;AIRS&#24211;&#30340;&#19968;&#37096;&#20998;&#20844;&#24320;&#20351;&#29992;&#65288;\url{https://github.com/divelab/AIRS}&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the prediction of the Hamiltonian matrix, which finds use in quantum chemistry and condensed matter physics. Efficiency and equivariance are two important, but conflicting factors. In this work, we propose a SE(3)-equivariant network, named QHNet, that achieves efficiency and equivariance. Our key advance lies at the innovative design of QHNet architecture, which not only obeys the underlying symmetries, but also enables the reduction of number of tensor products by 92\%. In addition, QHNet prevents the exponential growth of channel dimension when more atom types are involved. We perform experiments on MD17 datasets, including four molecular systems. Experimental results show that our QHNet can achieve comparable performance to the state of the art methods at a significantly faster speed. Besides, our QHNet consumes 50\% less memory due to its streamlined architecture. Our code is publicly available as part of the AIRS library (\url{https://github.com/divelab/AIRS}).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#36719;&#27979;&#37327;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04919</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36125;&#21494;&#26031;&#31890;&#23376;&#27969;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework. (arXiv:2306.04919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#36719;&#27979;&#37327;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36719;&#27979;&#37327;&#23545;&#20110;&#36890;&#36807;&#21487;&#38752;&#30340;&#29366;&#24577;&#25512;&#26029;&#23454;&#29616;&#31934;&#30830;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#24320;&#21457;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#36719;&#27979;&#37327;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031; (DPFB) &#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26080;&#30446;&#26631;&#29366;&#24577;&#26631;&#31614;&#24773;&#20917;&#19979;&#36827;&#34892;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#24314;&#27169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#39034;&#24207;&#36125;&#21494;&#26031;&#30446;&#26631;&#65292;&#20197;&#25191;&#34892;&#28508;&#22312;&#30340;&#36328;&#39046;&#22495;&#36719;&#24863;&#30693;&#38382;&#39064;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#22312;&#26694;&#26550;&#26680;&#24515;&#65292;&#25105;&#20204;&#32467;&#21512;&#29289;&#29702;&#23398;&#21551;&#21457;&#30340;&#31890;&#23376;&#27969;&#65292;&#36890;&#36807;&#20248;&#21270;&#39034;&#24207;&#36125;&#21494;&#26031;&#30446;&#26631;&#26469;&#25191;&#34892;&#27169;&#22411;&#25552;&#21462;&#30340;&#28508;&#22312;&#21644;&#38544;&#34255;&#29305;&#24449;&#30340;&#31934;&#30830;&#36125;&#21494;&#26031;&#26356;&#26032;&#12290;&#30001;&#27492;&#65292;&#36825;&#20123;&#36129;&#29486;&#20351;&#24471;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#26377;&#26426;&#30340;&#36817;&#20284;&#21518;&#39564;&#29305;&#24449;&#34920;&#31034;&#65292;&#33021;&#22815;&#34920;&#24449;&#22797;&#26434;&#30340;&#36328;&#39046;&#22495;&#31995;&#32479;&#21160;&#21147;&#23398;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#36719;&#27979;&#37327;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven soft sensors are essential for achieving accurate perception through reliable state inference. However, developing representative soft sensor models is challenged by issues such as missing labels, domain adaptability, and temporal coherence in data. To address these challenges, we propose a deep Particle Flow Bayes (DPFB) framework for cross-domain soft sensor modeling in the absence of target state labels. In particular, a sequential Bayes objective is first formulated to perform the maximum likelihood estimation underlying the cross-domain soft sensing problem. At the core of the framework, we incorporate a physics-inspired particle flow that optimizes the sequential Bayes objective to perform an exact Bayes update of the model extracted latent and hidden features. As a result, these contributions enable the proposed framework to learn a cohesive approximate posterior feature representation capable of characterizing complex cross-domain system dynamics and performing effe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#36941;&#21382;&#31639;&#27861;&#20013;&#30340;&#35206;&#30422;&#26102;&#38388;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#35745;&#25968;&#30340;&#36127;&#21453;&#39304;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#22270;&#20013;&#23616;&#37096;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#65292;&#24182;&#22312;&#29305;&#27530;&#30340;&#22270;&#24418;&#20013;&#23454;&#29616;&#26356;&#23567;&#30340;&#35206;&#30422;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.04902</link><description>&lt;p&gt;
&#19968;&#31181;&#38750;&#39532;&#23572;&#21487;&#22827;&#31639;&#27861;&#30340;&#35206;&#30422;&#26102;&#38388;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Cover Time Study of a non-Markovian Algorithm. (arXiv:2306.04902v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#36941;&#21382;&#31639;&#27861;&#20013;&#30340;&#35206;&#30422;&#26102;&#38388;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#35745;&#25968;&#30340;&#36127;&#21453;&#39304;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#22270;&#20013;&#23616;&#37096;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#65292;&#24182;&#22312;&#29305;&#27530;&#30340;&#22270;&#24418;&#20013;&#23454;&#29616;&#26356;&#23567;&#30340;&#35206;&#30422;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#30340;&#22270;&#20013;&#65292;&#35206;&#30422;&#26102;&#38388;&#26159;&#35775;&#38382;&#25152;&#26377;&#33410;&#28857;&#25152;&#38656;&#30340;&#26399;&#26395;&#27493;&#25968;&#12290;&#26356;&#23567;&#30340;&#35206;&#30422;&#26102;&#38388;&#24847;&#21619;&#30528;&#36941;&#21382;&#31639;&#27861;&#30340;&#25506;&#32034;&#25928;&#29575;&#26356;&#39640;&#12290;&#23613;&#31649;&#23545;&#20110;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#38750;&#39532;&#23572;&#21487;&#22827;&#26041;&#27861;&#23578;&#26080;&#35206;&#30422;&#26102;&#38388;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#20174;&#29702;&#35770;&#35282;&#24230;&#20986;&#21457;&#65292;&#34920;&#26126;&#36127;&#21453;&#39304;&#31574;&#30053;&#65288;&#19968;&#31181;&#22522;&#20110;&#35745;&#25968;&#30340;&#25506;&#32034;&#26041;&#27861;&#65289;&#27604;&#26420;&#32032;&#30340;&#38543;&#26426;&#28459;&#27493;&#25628;&#32034;&#31574;&#30053;&#26356;&#22909;&#12290;&#29305;&#21035;&#22320;&#65292;&#21069;&#32773;&#21487;&#20197;&#22312;&#20219;&#24847;&#22270;&#20013;&#23616;&#37096;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;&#23427;&#36824;&#21487;&#20197;&#22312;&#29305;&#27530;&#20294;&#37325;&#35201;&#30340;&#22270;&#24418;&#20013;&#23454;&#29616;&#26356;&#23567;&#30340;&#35206;&#30422;&#26102;&#38388;&#65292;&#21253;&#25324;&#22242;&#31751;&#22270;&#21644;&#26641;&#22270;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#25581;&#31034;&#20026;&#20160;&#20040;&#32463;&#20856;&#30340;UCB&#21644;MCTS&#31639;&#27861;&#22914;&#27492;&#26377;&#29992;&#12290;&#21508;&#31181;&#25968;&#20540;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a traversal algorithm, cover time is the expected number of steps needed to visit all nodes in a given graph. A smaller cover time means a higher exploration efficiency of traversal algorithm. Although random walk algorithms have been studied extensively in the existing literature, there has been no cover time result for any non-Markovian method. In this work, we stand on a theoretical perspective and show that the negative feedback strategy (a count-based exploration method) is better than the naive random walk search. In particular, the former strategy can locally improve the search efficiency for an arbitrary graph. It also achieves smaller cover times for special but important graphs, including clique graphs, tree graphs, etc. Moreover, we make connections between our results and reinforcement learning literature to give new insights on why classical UCB and MCTS algorithms are so useful. Various numerical results corroborate our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#36716;&#31227;&#23398;&#20064;&#22312;&#19981;&#21516;&#30456;&#20284;&#24230;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20998;&#26512;&#20102;&#20004;&#31181;&#21442;&#25968;&#20256;&#36882;&#36873;&#39033;&#30340;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#27169;&#22411;&#35823;&#24046;&#30340;&#29305;&#24449;&#26469;&#26816;&#39564;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.04901</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#65306;&#36807;&#21442;&#25968;&#21270;&#21644;&#27424;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Generalization Performance of Transfer Learning: Overparameterized and Underparameterized Regimes. (arXiv:2306.04901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#36716;&#31227;&#23398;&#20064;&#22312;&#19981;&#21516;&#30456;&#20284;&#24230;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20998;&#26512;&#20102;&#20004;&#31181;&#21442;&#25968;&#20256;&#36882;&#36873;&#39033;&#30340;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#27169;&#22411;&#35823;&#24046;&#30340;&#29305;&#24449;&#26469;&#26816;&#39564;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26159;&#19968;&#31181;&#20351;&#29992;&#28304;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30446;&#26631;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#25913;&#21892;&#24615;&#33021;&#21644;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#30340;&#26377;&#29992;&#25216;&#26415;&#12290;&#35780;&#20272;&#36716;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20381;&#36182;&#20110;&#29702;&#35299;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20219;&#21153;&#36890;&#24120;&#34920;&#29616;&#20986;&#37096;&#20998;&#30456;&#20284;&#24615;&#65292;&#20854;&#20013;&#26576;&#20123;&#26041;&#38754;&#30456;&#20284;&#32780;&#21478;&#19968;&#20123;&#26041;&#38754;&#21017;&#19981;&#21516;&#25110;&#26080;&#20851;&#12290;&#20026;&#20102;&#30740;&#31350;&#37096;&#20998;&#30456;&#20284;&#24615;&#23545;&#36716;&#31227;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20855;&#26377;&#20004;&#32452;&#19981;&#21516;&#29305;&#24449;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65306;&#22312;&#20219;&#21153;&#38388;&#20849;&#20139;&#30340;&#20844;&#20849;&#37096;&#20998;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#36716;&#31227;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#20004;&#20010;&#21442;&#25968;&#20256;&#36882;&#36873;&#39033;&#12290;&#36890;&#36807;&#24314;&#31435;&#23398;&#20064;&#27169;&#22411;&#35823;&#24046;&#30340;&#29702;&#35770;&#29305;&#24449;&#65292;&#25105;&#20204;&#27604;&#36739;&#36825;&#20123;&#36716;&#31227;&#23398;&#20064;&#36873;&#39033;&#65292;&#29305;&#21035;&#20851;&#27880;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#38543;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is a useful technique for achieving improved performance and reducing training costs by leveraging the knowledge gained from source tasks and applying it to target tasks. Assessing the effectiveness of transfer learning relies on understanding the similarity between the ground truth of the source and target tasks. In real-world applications, tasks often exhibit partial similarity, where certain aspects are similar while others are different or irrelevant. To investigate the impact of partial similarity on transfer learning performance, we focus on a linear regression model with two distinct sets of features: a common part shared across tasks and a task-specific part. Our study explores various types of transfer learning, encompassing two options for parameter transfer. By establishing a theoretical characterization on the error of the learned model, we compare these transfer learning options, particularly examining how generalization performance changes with the numbe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27491;&#24335;&#21051;&#30011;&#21644;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#32463;&#39564;&#24615;&#35265;&#35299;&#65292;&#24182;&#20026;MAE&#30340;&#35757;&#32451;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.04898</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#28508;&#21464;&#37327;&#27169;&#22411;&#29702;&#35299;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Understanding Masked Autoencoders via Hierarchical Latent Variable Models. (arXiv:2306.04898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27491;&#24335;&#21051;&#30011;&#21644;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#32463;&#39564;&#24615;&#35265;&#35299;&#65292;&#24182;&#20026;MAE&#30340;&#35757;&#32451;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#22270;&#20687;&#21306;&#22495;&#37325;&#26500;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36817;&#26469;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;MAE&#23384;&#22312;&#26377;&#36259;&#30340;&#32463;&#39564;&#24615;&#35266;&#23519;&#32467;&#26524;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#21407;&#21017;&#24615;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#21051;&#30011;&#21644;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#32463;&#39564;&#24615;&#35265;&#35299;&#65292;&#24182;&#23545;MAE&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#23558;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#23618;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#34920;&#26126;&#22312;&#21512;&#29702;&#30340;&#20551;&#35774;&#19979;&#65292;MAE&#33021;&#22815;&#35777;&#26126;&#22320;&#35782;&#21035;&#20986;&#19968;&#32452;&#28508;&#22312;&#21464;&#37327;&#65292;&#35299;&#37322;&#20102;MAE&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#39640;&#23618;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MAE&#20013;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#65288;&#36974;&#34109;&#27604;&#29575;&#21644;&#34917;&#19969;&#22823;&#23567;&#65289;&#22914;&#20309;&#20915;&#23450;&#35201;&#24674;&#22797;&#21738;&#20123;&#30495;&#23454;&#28508;&#21464;&#37327;&#65292;&#20174;&#32780;&#24433;&#21709;&#34920;&#31034;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#27700;&#24179;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26497;&#22823;&#25110;&#26497;&#23567;&#30340;&#36974;&#34109;&#27604;&#29575;&#20250;&#23548;&#33268;&#34920;&#31034;&#36136;&#37327;&#24046;&#65292;&#32780;&#19981;&#21516;&#30340;&#34917;&#19969;&#22823;&#23567;&#21487;&#20197;&#25429;&#33719;&#19981;&#21516;&#32423;&#21035;&#30340;&#23545;&#35937;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#20026;MAE&#30340;&#35757;&#32451;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical insights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small maski
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26679;&#26412;&#21644;&#29289;&#29702;&#23398;&#30340;&#27491;&#21521;&#27169;&#22411;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24471;&#21040;&#26465;&#20214;Wasserstein&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;cWGAN&#65289;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22256;&#38590;&#30340;&#27010;&#29575;&#21453;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04895</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#20840;&#26799;&#24230;&#24809;&#32602;&#30340;&#26465;&#20214;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#35299;&#20915;&#22522;&#20110;&#29289;&#29702;&#21453;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solution of physics-based inverse problems using conditional generative adversarial networks with full gradient penalty. (arXiv:2306.04895v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26679;&#26412;&#21644;&#29289;&#29702;&#23398;&#30340;&#27491;&#21521;&#27169;&#22411;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24471;&#21040;&#26465;&#20214;Wasserstein&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;cWGAN&#65289;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22256;&#38590;&#30340;&#27010;&#29575;&#21453;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#27010;&#29575;&#21453;&#38382;&#39064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#24403;&#25512;&#26029;&#21521;&#37327;&#30340;&#32500;&#24230;&#24456;&#22823;&#19988;&#20854;&#20808;&#39564;&#20449;&#24687;&#26159;&#30001;&#19968;&#32452;&#26679;&#26412;&#32452;&#25104;&#26102;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20174;&#20808;&#39564;&#20998;&#24067;&#20013;&#32472;&#21046;&#30340;&#25512;&#26029;&#21521;&#37327;&#26679;&#26412;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#27491;&#21521;&#27169;&#22411;&#26469;&#29983;&#25104;&#26465;&#20214;Wasserstein&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;cWGAN&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;cWGAN&#23398;&#20064;&#32473;&#23450;&#27979;&#37327;&#30340;&#25512;&#26029;&#21521;&#37327;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#26469;&#33258;&#35813;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25152;&#24320;&#21457;&#30340;cWGAN&#19982;&#26089;&#26399;&#29256;&#26412;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#20854;&#35780;&#35770;&#23478;&#24517;&#39035;&#22312;&#25512;&#26029;&#21521;&#37327;&#21644;&#27979;&#37327;&#21521;&#37327;&#26041;&#38754;&#37117;&#26159;1-Lipschitz&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#24102;&#26377;&#20840;&#26799;&#24230;&#24809;&#32602;&#30340;&#25439;&#22833;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution of probabilistic inverse problems for which the corresponding forward problem is constrained by physical principles is challenging. This is especially true if the dimension of the inferred vector is large and the prior information about it is in the form of a collection of samples. In this work, a novel deep learning based approach is developed and applied to solving these types of problems. The approach utilizes samples of the inferred vector drawn from the prior distribution and a physics-based forward model to generate training data for a conditional Wasserstein generative adversarial network (cWGAN). The cWGAN learns the probability distribution for the inferred vector conditioned on the measurement and produces samples from this distribution. The cWGAN developed in this work differs from earlier versions in that its critic is required to be 1-Lipschitz with respect to both the inferred and the measurement vectors and not just the former. This leads to a loss term with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#37319;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#21644;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#39044;&#23450;&#20041;&#30340;&#22522;&#20989;&#25968;&#23383;&#20856;&#20013;&#23398;&#20064;&#30456;&#20851;&#22522;&#26412;&#30693;&#35782;&#65292;&#24182;&#21457;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.04894</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20174;&#25968;&#25454;&#23398;&#20064;&#25511;&#21046;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Framework for learning governing Partial Differential Equation from Data. (arXiv:2306.04894v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#37319;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#21644;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#39044;&#23450;&#20041;&#30340;&#22522;&#20989;&#25968;&#23383;&#20856;&#20013;&#23398;&#20064;&#30456;&#20851;&#22522;&#26412;&#30693;&#35782;&#65292;&#24182;&#21457;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#19968;&#39033;&#26082;&#28041;&#21450;&#29702;&#35770;&#21448;&#28041;&#21450;&#23454;&#35777;&#26041;&#27861;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#34987;&#29992;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#20294;&#26159;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#28508;&#22312;&#30340;&#26041;&#31243;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21464;&#20998;&#36125;&#21494;&#26031;&#21644;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#21457;&#29616;PDE&#30340;&#26032;&#26041;&#27861;&#12290;PDE&#21457;&#29616;&#38382;&#39064;&#34987;&#35270;&#20026;&#20174;&#39044;&#23450;&#20041;&#22522;&#20989;&#25968;&#23383;&#20856;&#20013;&#23398;&#20064;&#30456;&#20851;&#22522;&#30784;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#21152;&#36895;&#25972;&#20010;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#20026;&#20102;&#30830;&#20445;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#38024;&#23574;&#21644;&#26495;&#26465;&#20808;&#39564;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;Burgers&#12289;Korteweg-de Vries&#12289;Kuramoto Sivashinsky&#12289;&#27874;&#21160;&#26041;&#31243;&#21644;&#28909;&#26041;&#31243;&#65288;1D&#21644;2D&#65289;&#22312;&#20869;&#30340;&#20960;&#20010;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#31574;&#30053;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of partial differential equations (PDEs) is a challenging task that involves both theoretical and empirical methods. Machine learning approaches have been developed and used to solve this problem; however, it is important to note that existing methods often struggle to identify the underlying equation accurately in the presence of noise. In this study, we present a new approach to discovering PDEs by combining variational Bayes and sparse linear regression. The problem of PDE discovery has been posed as a problem to learn relevant basis from a predefined dictionary of basis functions. To accelerate the overall process, a variational Bayes-based approach for discovering partial differential equations is proposed. To ensure sparsity, we employ a spike and slab prior. We illustrate the efficacy of our strategy in several examples, including Burgers, Korteweg-de Vries, Kuramoto Sivashinsky, wave equation, and heat equation (1D as well as 2D). Our method offers a promising ave
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#12290;&#20316;&#32773;&#36824;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#26469;&#39564;&#35777;&#36825;&#31181;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.04891</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#21407;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning through the Bayesian Prism. (arXiv:2306.04891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#12290;&#20316;&#32773;&#36824;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#26469;&#39564;&#35777;&#36825;&#31181;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20196;&#20154;&#24778;&#35766;&#19988;&#26377;&#29992;&#30340;&#29305;&#24615;&#20043;&#19968;&#12290;&#23427;&#30340;&#24037;&#20316;&#21407;&#29702;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36817;&#26399;&#65292;&#20154;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#39118;&#26684;&#21270;&#30340;&#31867;&#20803;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#23427;&#20204;&#20351;&#29992;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#20989;&#25968;&#23545;&#26469;&#33258;&#20989;&#25968;&#31867;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;$(x, f(x))$ &#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#35266;&#23519;&#27169;&#22411;&#23545;&#21516;&#19968;&#31867;&#20013;&#26410;&#35265;&#36807;&#30340;&#20989;&#25968;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#19968;&#30740;&#31350;&#32447;&#36335;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#23545;&#20110;&#35832;&#22914;&#32447;&#24615;&#22238;&#24402;&#31561;&#20960;&#20010;&#38382;&#39064;&#65292;&#35757;&#32451;&#22909;&#30340; Transformer &#23398;&#20064;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#36825;&#31181;&#34892;&#20026;&#30340;&#24402;&#32435;&#20559;&#24046;&#24182;&#19981;&#28165;&#26970;&#12290;&#25317;&#26377;&#26080;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#27169;&#22411;&#26159;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#65306;&#23427;&#23398;&#20064;&#20102;&#39044;&#35757;&#32451;&#20998;&#24067;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#39640;&#23481;&#37327;&#30340; Transformer &#27169;&#22411;&#22312;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#27169;&#25311;&#36125;&#21494;&#26031;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#29702;&#24819;&#23398;&#20064;&#32773;&#30340;&#34892;&#20026;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#21253;&#25324;&#22806;&#25512;&#21644;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#21512;&#29702;&#20989;&#25968;&#30340;&#20808;&#39564;&#27010;&#29575;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26368;&#23567;&#21270;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#26469;&#36827;&#19968;&#27493;&#25506;&#31350;&#36825;&#31181;&#32852;&#31995;&#65292;&#35777;&#26126;&#20351;&#29992;&#30495;&#23454;&#30340;&#36125;&#21494;&#26031;&#20808;&#39564;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#20351;&#29992;&#22266;&#23450;&#20808;&#39564;&#25110;&#27809;&#26377;&#20808;&#39564;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning is one of the surprising and useful features of large language models. How it works is an active area of research. Recently, stylized meta-learning-like setups have been devised that train these models on a sequence of input-output pairs $(x, f(x))$ from a function class using the language modeling loss and observe generalization to unseen functions from the same class. One of the main discoveries in this line of research has been that for several problems such as linear regression, trained transformers learn algorithms for learning functions in context. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. It has been shown that high-capacity transformers mimic the Bayesian predictor for linear regression. In this paper, we show empirical evidence of transformers exhibiting the behavior of this ideal learne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#28145;&#24230;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476; ShaDDR&#65292;&#21487;&#20197;&#36890;&#36807;&#20960;&#20309;&#32454;&#33410;&#21270;&#21644;&#26465;&#20214;&#32441;&#29702;&#29983;&#25104;&#24212;&#29992;&#20110;&#36755;&#20837;&#30340;&#31895;&#30053;&#20307;&#32032;&#24418;&#29366;&#65292;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#36148;&#22270;&#30340; 3D &#24418;&#29366;&#12290;&#29983;&#25104;&#23454;&#26102;&#19988;&#31934;&#24230;&#39640;&#65292;&#39118;&#26684;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#30340;&#28508;&#22312;&#20195;&#30721;&#36827;&#34892;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.04889</link><description>&lt;p&gt;
ShaDDR: &#22522;&#20110;&#31034;&#20363;&#30340;&#23454;&#26102;&#20960;&#20309;&#21644;&#32441;&#29702;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ShaDDR: Real-Time Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering. (arXiv:2306.04889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#28145;&#24230;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476; ShaDDR&#65292;&#21487;&#20197;&#36890;&#36807;&#20960;&#20309;&#32454;&#33410;&#21270;&#21644;&#26465;&#20214;&#32441;&#29702;&#29983;&#25104;&#24212;&#29992;&#20110;&#36755;&#20837;&#30340;&#31895;&#30053;&#20307;&#32032;&#24418;&#29366;&#65292;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#36148;&#22270;&#30340; 3D &#24418;&#29366;&#12290;&#29983;&#25104;&#23454;&#26102;&#19988;&#31934;&#24230;&#39640;&#65292;&#39118;&#26684;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#30340;&#28508;&#22312;&#20195;&#30721;&#36827;&#34892;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; ShaDDR&#65292;&#19968;&#31181;&#22522;&#20110;&#31034;&#20363;&#30340;&#28145;&#24230;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20960;&#20309;&#32454;&#33410;&#21270;&#21644;&#26465;&#20214;&#32441;&#29702;&#29983;&#25104;&#24212;&#29992;&#20110;&#36755;&#20837;&#30340;&#31895;&#30053;&#20307;&#32032;&#24418;&#29366;&#65292;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#36148;&#22270;&#30340; 3D &#24418;&#29366;&#12290;&#22312;&#23569;&#37327;&#35814;&#32454;&#21644;&#32441;&#29702;&#30340;&#33539;&#20363;&#24418;&#29366;&#19978;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22810;&#20998;&#36776;&#29575;&#20307;&#32032;&#19978;&#37319;&#26679;&#23398;&#20064;&#20960;&#20309;&#32454;&#33410;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#20960;&#20010;&#35270;&#22270;&#30340;&#33539;&#20363;&#32441;&#29702;&#22270;&#20687;&#36827;&#34892;&#21487;&#24494;&#28210;&#26579;&#65292;&#22312;&#20307;&#32032;&#34920;&#38754;&#29983;&#25104;&#32441;&#29702;&#12290;&#29983;&#25104;&#26159;&#23454;&#26102;&#30340;&#65292;&#20165;&#38656;&#19981;&#21040; 1 &#31186;&#21363;&#21487;&#29983;&#25104;&#20998;&#36776;&#29575;&#39640;&#36798; 512^3 &#30340; 3D &#27169;&#22411;&#12290;&#29983;&#25104;&#30340;&#24418;&#29366;&#20445;&#30041;&#20102;&#36755;&#20837;&#31895;&#30053;&#20307;&#32032;&#27169;&#22411;&#30340;&#25972;&#20307;&#32467;&#26500;&#65292;&#32780;&#29983;&#25104;&#30340;&#20960;&#20309;&#32454;&#33410;&#21644;&#32441;&#29702;&#30340;&#39118;&#26684;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#30340;&#28508;&#22312;&#20195;&#30721;&#36827;&#34892;&#25805;&#32437;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#27604;&#20197;&#21069;&#30340;&#20316;&#21697;&#26356;&#30495;&#23454;&#19988;&#20960;&#20309;&#32454;&#33410;&#21644;&#32441;&#29702;&#26356;&#24178;&#20928;&#30340;&#39640;&#20998;&#36776;&#29575;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ShaDDR, an example-based deep generative neural network which produces a high-resolution textured 3D shape through geometry detailization and conditional texture generation applied to an input coarse voxel shape. Trained on a small set of detailed and textured exemplar shapes, our method learns to detailize the geometry via multi-resolution voxel upsampling and generate textures on voxel surfaces via differentiable rendering against exemplar texture images from a few views. The generation is real-time, taking less than 1 second to produce a 3D model with voxel resolutions up to 512^3. The generated shape preserves the overall structure of the input coarse voxel model, while the style of the generated geometric details and textures can be manipulated through learned latent codes. In the experiments, we show that our method can generate higher-resolution shapes with plausible and improved geometric details and clean textures compared to prior works. Furthermore, we showcase th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20219;&#21153;&#29983;&#29289;&#27979;&#23450;&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;MBP&#65289;&#29992;&#20110;&#22522;&#20110;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;ChEMBL-Dock&#65292;&#35299;&#20915;&#20102;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#20197;&#21450;&#19981;&#21516;&#26631;&#31614;&#21644;&#23454;&#39564;&#26465;&#20214;&#23548;&#33268;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04886</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#29983;&#29289;&#27979;&#23450;&#39044;&#35757;&#32451;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-task Bioassay Pre-training for Protein-ligand Binding Affinity Prediction. (arXiv:2306.04886v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20219;&#21153;&#29983;&#29289;&#27979;&#23450;&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;MBP&#65289;&#29992;&#20110;&#22522;&#20110;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;ChEMBL-Dock&#65292;&#35299;&#20915;&#20102;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#20197;&#21450;&#19981;&#21516;&#26631;&#31614;&#21644;&#23454;&#39564;&#26465;&#20214;&#23548;&#33268;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#65288;PLBA&#65289;&#39044;&#27979;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#21508;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#36890;&#36807;&#23558;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#30340;&#19977;&#32500;&#32467;&#26500;&#20316;&#20026;&#36755;&#20837;&#24182;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#36827;&#23637;&#26469;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#24403;&#21069;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30340;&#29983;&#29289;&#27979;&#23450;&#20351;&#29992;&#19981;&#21516;&#30340;&#20146;&#21644;&#21147;&#27979;&#37327;&#26631;&#31614;&#65288;&#21363;IC50&#65292;Ki&#65292;Kd&#65289;&#65292;&#19981;&#21516;&#30340;&#23454;&#39564;&#26465;&#20214;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#24341;&#20837;&#31995;&#32479;&#22122;&#22768;&#65292;&#36825;&#23545;&#26500;&#24314;&#39640;&#31934;&#24230;&#20146;&#21644;&#21147;&#39044;&#27979;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#65288;1&#65289;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#29983;&#29289;&#27979;&#23450;&#39044;&#35757;&#32451;&#65288;MBP&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#32467;&#26500;&#30340;PLBA&#39044;&#27979;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;;&#65288;2&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;300k&#20010;&#23454;&#39564;&#27979;&#23450;&#20146;&#21644;&#21147;&#26631;&#31614;&#21644;&#32422;2.8M&#20010;&#23545;&#25509;&#19977;&#32500;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;ChEMBL-Dock&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein-ligand binding affinity (PLBA) prediction is the fundamental task in drug discovery. Recently, various deep learning-based models predict binding affinity by incorporating the three-dimensional structure of protein-ligand complexes as input and achieving astounding progress. However, due to the scarcity of high-quality training data, the generalization ability of current models is still limited. In addition, different bioassays use varying affinity measurement labels (i.e., IC50, Ki, Kd), and different experimental conditions inevitably introduce systematic noise, which poses a significant challenge to constructing high-precision affinity prediction models. To address these issues, we (1) propose Multi-task Bioassay Pre-training (MBP), a pre-training framework for structure-based PLBA prediction; (2) construct a pre-training dataset called ChEMBL-Dock with more than 300k experimentally measured affinity labels and about 2.8M docked three-dimensional structures. By introducing m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21442;&#25968;&#21270;&#32858;&#31867;LambdaCC&#30340;&#26356;&#24555;&#36924;&#36817;&#31639;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#25324;&#20043;&#21069;&#36793;&#26631;&#35760;&#38382;&#39064;&#30340;&#26032;&#30340;&#21442;&#25968;&#21270;&#36793;&#26631;&#35760;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#25193;&#23637;&#24615;&#27604;&#20197;&#21069;&#30340;&#31639;&#27861;&#39640;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2306.04884</link><description>&lt;p&gt;
&#21442;&#25968;&#21270;&#22270;&#32858;&#31867;&#21644;&#36793;&#26631;&#35760;&#30340;&#26356;&#24555;&#36924;&#36817;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Faster Approximation Algorithms for Parameterized Graph Clustering and Edge Labeling. (arXiv:2306.04884v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21442;&#25968;&#21270;&#32858;&#31867;LambdaCC&#30340;&#26356;&#24555;&#36924;&#36817;&#31639;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#25324;&#20043;&#21069;&#36793;&#26631;&#35760;&#38382;&#39064;&#30340;&#26032;&#30340;&#21442;&#25968;&#21270;&#36793;&#26631;&#35760;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#25193;&#23637;&#24615;&#27604;&#20197;&#21069;&#30340;&#31639;&#27861;&#39640;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26159;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#20854;&#30446;&#26631;&#26159;&#26816;&#27979;&#19982;&#24444;&#27492;&#36830;&#25509;&#32039;&#23494;&#12289;&#20294;&#19982;&#20854;&#20313;&#22270;&#31232;&#30095;&#36830;&#25509;&#30340;&#33410;&#28857;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#36924;&#36817;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#19968;&#20010;&#21517;&#20026;LambdaCC&#30340;NP&#22256;&#38590;&#21442;&#25968;&#21270;&#32858;&#31867;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#30001;&#21487;&#35843;&#33410;&#30340;&#20998;&#36776;&#29575;&#21442;&#25968;&#25511;&#21046;&#65292;&#24182;&#19988;&#27010;&#25324;&#20102;&#35768;&#22810;&#20854;&#20182;&#32858;&#31867;&#30446;&#26631;&#65292;&#20363;&#22914;&#27169;&#22359;&#21270;&#12289;&#26368;&#31232;&#30095;&#21106;&#21644;&#32858;&#31867;&#21024;&#38500;&#12290;&#20197;&#21069;&#30340;LambdaCC&#31639;&#27861;&#35201;&#20040;&#26159;&#27809;&#26377;&#36924;&#36817;&#20445;&#35777;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#35201;&#20040;&#26159;&#35745;&#31639;&#22797;&#26434;&#30340;&#36924;&#36817;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#32431;&#31929;&#21033;&#29992;&#32452;&#21512;&#26041;&#27861;&#23454;&#29616;&#30340;&#24555;&#36895;&#26032;&#36924;&#36817;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#22522;&#20110;&#25105;&#20204;&#20171;&#32461;&#30340;&#26032;&#21442;&#25968;&#21270;&#36793;&#26631;&#35760;&#38382;&#39064;&#65292;&#36825;&#20010;&#38382;&#39064;&#27010;&#25324;&#20102;&#20043;&#21069;&#22522;&#20110;&#24378;&#19977;&#20803;&#38381;&#21512;&#21407;&#29702;&#30340;&#36793;&#26631;&#35760;&#38382;&#39064;&#65292;&#24182;&#22312;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#20013;&#26377;&#30528;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#36924;&#36817;&#31639;&#27861;&#21487;&#25193;&#23637;&#24615;&#39640;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering is a fundamental task in network analysis where the goal is to detect sets of nodes that are well-connected to each other but sparsely connected to the rest of the graph. We present faster approximation algorithms for an NP-hard parameterized clustering framework called LambdaCC, which is governed by a tunable resolution parameter and generalizes many other clustering objectives such as modularity, sparsest cut, and cluster deletion. Previous LambdaCC algorithms are either heuristics with no approximation guarantees, or computationally expensive approximation algorithms. We provide fast new approximation algorithms that can be made purely combinatorial. These rely on a new parameterized edge labeling problem we introduce that generalizes previous edge labeling problems that are based on the principle of strong triadic closure and are of independent interest in social network analysis. Our methods are orders of magnitude more scalable than previous approximation algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#28151;&#21512;&#31934;&#24230;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#20998;&#37197;&#19981;&#21516;&#30340;&#25968;&#20540;&#31934;&#24230;&#20197;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25913;&#21892;&#24310;&#36831;&#65292;&#21516;&#26102;&#36890;&#36807;&#22686;&#24378;Hessian&#19982;&#20854;&#20182;&#23618;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#19981;&#36275;&#20197;&#30830;&#23450;&#23618;&#25935;&#24863;&#24615;&#25490;&#24207;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#28151;&#21512;&#31934;&#24230;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04879</link><description>&lt;p&gt;
&#22686;&#24378;&#20855;&#26377;&#23618;&#38388;&#20381;&#36182;&#24615;&#30340;Hessians&#29992;&#20110;&#28151;&#21512;&#31934;&#24230;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Augmenting Hessians with Inter-Layer Dependencies for Mixed-Precision Post-Training Quantization. (arXiv:2306.04879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#28151;&#21512;&#31934;&#24230;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#20998;&#37197;&#19981;&#21516;&#30340;&#25968;&#20540;&#31934;&#24230;&#20197;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25913;&#21892;&#24310;&#36831;&#65292;&#21516;&#26102;&#36890;&#36807;&#22686;&#24378;Hessian&#19982;&#20854;&#20182;&#23618;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#19981;&#36275;&#20197;&#30830;&#23450;&#23618;&#25935;&#24863;&#24615;&#25490;&#24207;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#28151;&#21512;&#31934;&#24230;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#26377;&#25928;&#22320;&#20026;&#20302;&#24310;&#36831;&#26381;&#21153;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27169;&#22411;&#37327;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#30340;&#37327;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#20110;&#27169;&#22411;&#19981;&#21516;&#23618;&#20043;&#38388;&#23545;&#25968;&#23383;&#32570;&#38519;&#30340;&#25935;&#24863;&#24615;&#24046;&#24322;&#32780;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24230;&#30340;&#19981;&#21487;&#25509;&#21463;&#25439;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31934;&#24230;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#65292;&#26681;&#25454;&#32593;&#32476;&#20013;&#24352;&#37327;&#30340;&#29305;&#23450;&#38656;&#27714;&#20026;&#23427;&#20204;&#20998;&#37197;&#19981;&#21516;&#30340;&#25968;&#20540;&#31934;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25913;&#21892;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20381;&#36182;&#20110;&#23618;&#27425;Hessian&#20449;&#24687;&#26469;&#30830;&#23450;&#25968;&#20540;&#31934;&#24230;&#65292;&#20294;&#27491;&#22914;&#25105;&#20204;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;Hessian&#20272;&#35745;&#36890;&#24120;&#19981;&#36275;&#20197;&#30830;&#23450;&#23618;&#25935;&#24863;&#24615;&#30340;&#26377;&#25928;&#25490;&#24207;&#12290;&#25105;&#20204;&#36890;&#36807;&#22686;&#24378;&#20272;&#35745;&#30340;Hessian&#19982;&#20854;&#20182;&#23618;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#28151;&#21512;&#31934;&#24230;PTQ&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently serving neural network models with low latency is becoming more challenging due to increasing model complexity and parameter count. Model quantization offers a solution which simultaneously reduces memory footprint and compute requirements. However, aggressive quantization may lead to an unacceptable loss in model accuracy owing to differences in sensitivity to numerical imperfection across different layers in the model. To address this challenge, we propose a mixed-precision post training quantization (PTQ) approach that assigns different numerical precisions to tensors in a network based on their specific needs, for a reduced memory footprint and improved latency while preserving model accuracy. Previous works rely on layer-wise Hessian information to determine numerical precision, but as we demonstrate, Hessian estimation is typically insufficient in determining an effective ordering of layer sensitivities. We address this by augmenting the estimated Hessian with additio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#27963;&#20248;&#21270;&#20026;&#27169;&#22411;&#21019;&#24314;&#31614;&#21517;&#65292;&#28982;&#21518;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#29305;&#27931;&#20234;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04877</link><description>&lt;p&gt;
&#20351;&#29992;&#28608;&#27963;&#20248;&#21270;&#36827;&#34892;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Trojan Model Detection Using Activation Optimization. (arXiv:2306.04877v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#27963;&#20248;&#21270;&#20026;&#27169;&#22411;&#21019;&#24314;&#31614;&#21517;&#65292;&#28982;&#21518;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#29305;&#27931;&#20234;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#30340;&#19981;&#21487;&#29992;&#24615;&#25110;&#22823;&#35268;&#27169;&#65292;&#20197;&#21450;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#35745;&#31639;&#21644;&#20154;&#21147;&#25104;&#26412;&#65292;&#36890;&#24120;&#20250;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#20381;&#36182;&#20110;&#24320;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#20174;&#23433;&#20840;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#20570;&#27861;&#38750;&#24120;&#20196;&#20154;&#25285;&#24551;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#34987;&#24863;&#26579;&#29305;&#27931;&#20234;&#25915;&#20987;&#65292;&#22312;&#36825;&#31181;&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#32773;&#23884;&#20837;&#19968;&#20010;&#35302;&#21457;&#22120;&#22312;&#27169;&#22411;&#20013;&#65292;&#20351;&#24471;&#24403;&#35302;&#21457;&#22120;&#23384;&#22312;&#20110;&#36755;&#20837;&#20013;&#26102;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25511;&#21046;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#30340;&#21021;&#27493;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#28608;&#27963;&#20248;&#21270;&#20026;&#27169;&#22411;&#21019;&#24314;&#31614;&#21517;&#12290;&#28982;&#21518;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#29305;&#27931;&#20234;&#27169;&#22411;&#24182;&#32473;&#20986;&#20854;&#31614;&#21517;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to data's unavailability or large size, and the high computational and human labor costs of training machine learning models, it is a common practice to rely on open source pre-trained models whenever possible. However, this practice is worry some from the security perspective. Pre-trained models can be infected with Trojan attacks, in which the attacker embeds a trigger in the model such that the model's behavior can be controlled by the attacker when the trigger is present in the input. In this paper, we present our preliminary work on a novel method for Trojan model detection. Our method creates a signature for a model based on activation optimization. A classifier is then trained to detect a Trojan model given its signature. Our method achieves state of the art performance on two public datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26102;&#38388;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411; TCD&#65292;&#36890;&#36807;&#25552;&#21462;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#24182;&#23558;&#20854;&#29992;&#20110;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25511;&#21046;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04875</link><description>&lt;p&gt;
&#25351;&#23548;&#25193;&#25955;&#22120;&#32467;&#21512;&#26102;&#38388;&#26465;&#20214;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instructed Diffuser with Temporal Condition Guidance for Offline Reinforcement Learning. (arXiv:2306.04875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26102;&#38388;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411; TCD&#65292;&#36890;&#36807;&#25552;&#21462;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#24182;&#23558;&#20854;&#29992;&#20110;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25511;&#21046;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#65292;&#25193;&#25955;&#27169;&#22411;&#36824;&#36890;&#36807;&#23558;&#20915;&#31574;&#21046;&#23450;&#20026;&#24207;&#21015;&#29983;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#31454;&#20105;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#39034;&#24207;&#25968;&#25454;&#30340;&#26102;&#38388;&#20449;&#24687;&#32435;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#25351;&#23548;&#26356;&#22909;&#30340;&#29983;&#25104;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#26102;&#38388;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026; Temporally-Composable Diffuser (TCD)&#65292;&#23427;&#20174;&#20132;&#20114;&#20013;&#25552;&#21462;&#26102;&#38388;&#20449;&#24687;&#65292;&#36890;&#36807;&#32454;&#21270;&#26102;&#38388;&#26465;&#20214;&#36827;&#34892;&#25511;&#21046;&#29983;&#25104;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26102;&#38388;&#26465;&#20214;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown the potential of diffusion models in computer vision and natural language processing. Apart from the classical supervised learning fields, diffusion models have also shown strong competitiveness in reinforcement learning (RL) by formulating decision-making as sequential generation. However, incorporating temporal information of sequential data and utilizing it to guide diffusion models to perform better generation is still an open challenge. In this paper, we take one step forward to investigate controllable generation with temporal conditions that are refined from temporal information. We observe the importance of temporal conditions in sequential generation in sufficient explorative scenarios and provide a comprehensive discussion and comparison of different temporal conditions. Based on the observations, we propose an effective temporally-conditional diffusion model coined Temporally-Composable Diffuser (TCD), which extracts temporal information from interact
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#33521;&#25991;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#36866;&#29992;&#20110;&#20013;&#25991;&#19978;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#23545;&#25239;&#23454;&#20363;&#12290;&#36890;&#36807;&#20851;&#27880;&#20013;&#25991;&#30340;&#35821;&#35328;&#29305;&#28857;&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#23454;&#29616;&#39640;&#27969;&#30021;&#24230;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#26469;&#25552;&#39640;&#20013;&#25991;NLP&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04874</link><description>&lt;p&gt;
&#25193;&#22823;&#33539;&#22260;&#65306;&#23558;&#33521;&#25991;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#36866;&#24212;&#21040;&#20013;&#25991;&#19978;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Expanding Scope: Adapting English Adversarial Attacks to Chinese. (arXiv:2306.04874v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#33521;&#25991;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#36866;&#29992;&#20110;&#20013;&#25991;&#19978;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#23545;&#25239;&#23454;&#20363;&#12290;&#36890;&#36807;&#20851;&#27880;&#20013;&#25991;&#30340;&#35821;&#35328;&#29305;&#28857;&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#23454;&#29616;&#39640;&#27969;&#30021;&#24230;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#26469;&#25552;&#39640;&#20013;&#25991;NLP&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#39044;&#27979;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#30528;&#30524;&#20110;&#35774;&#35745;&#25915;&#20987;&#26041;&#24335;&#26469;&#35780;&#20272;&#33521;&#35821;&#35821;&#22659;&#19979;&#30340;NLP&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#23398;&#26415;&#30028;&#23545;&#20854;&#23427;&#35821;&#35328;&#30340;NLP&#35299;&#20915;&#26041;&#26696;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33258;&#28982;&#20135;&#29983;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#23427;&#35821;&#35328;&#20013;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#22312;&#33521;&#25991;&#29615;&#22659;&#19979;&#30340;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25915;&#20987;&#31639;&#27861;&#36866;&#24212;&#21040;&#20013;&#25991;&#19978;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32467;&#21512;&#27491;&#30830;&#30340;&#25991;&#26412;&#20998;&#21106;&#21644;&#35821;&#35328;&#38480;&#21046;&#26102;&#65292;&#20808;&#21069;&#38024;&#23545;&#33521;&#25991;NLP&#30340;&#25915;&#20987;&#26041;&#27861;&#20063;&#33021;&#22815;&#22312;&#20013;&#25991;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#20851;&#27880;&#20013;&#25991;&#30340;&#24418;&#24577;&#21644;&#38899;&#31995;&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#23454;&#29616;&#39640;&#27969;&#30021;&#24230;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#26469;&#25552;&#39640;&#20013;&#25991;NLP&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed that NLP predictive models are vulnerable to adversarial attacks. Most existing studies focused on designing attacks to evaluate the robustness of NLP models in the English language alone. Literature has seen an increasing need for NLP solutions for other languages. We, therefore, ask one natural question: whether state-of-the-art (SOTA) attack methods generalize to other languages. This paper investigates how to adapt SOTA adversarial attack algorithms in English to the Chinese language. Our experiments show that attack methods previously applied to English NLP can generate high-quality adversarial examples in Chinese when combined with proper text segmentation and linguistic constraints. In addition, we demonstrate that the generated adversarial examples can achieve high fluency and semantic consistency by focusing on the Chinese language's morphology and phonology, which in turn can be used to improve the adversarial robustness of Chinese NLP models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21435;&#22122;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#20174;&#32593;&#32476;&#30340;&#35282;&#24230;&#29983;&#25104;&#22478;&#24066;&#20840;&#22495;&#30340;&#36215;&#28857;&#19982;&#32456;&#28857;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#21306;&#22495;&#23618;&#38754;&#30340;&#22478;&#24066;&#29305;&#24449;&#26469;&#35774;&#35745;&#20986;&#22270;&#21435;&#22122;&#25193;&#25955;&#26041;&#27861;&#30340;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.04873</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21435;&#22122;&#25193;&#25955;&#30340;&#22478;&#24066;&#20840;&#22495;&#36215;&#28857;&#19982;&#32456;&#28857;&#30697;&#38453;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
City-wide Origin-Destination Matrix Generation via Graph Denoising Diffusion. (arXiv:2306.04873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21435;&#22122;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#20174;&#32593;&#32476;&#30340;&#35282;&#24230;&#29983;&#25104;&#22478;&#24066;&#20840;&#22495;&#30340;&#36215;&#28857;&#19982;&#32456;&#28857;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#21306;&#22495;&#23618;&#38754;&#30340;&#22478;&#24066;&#29305;&#24449;&#26469;&#35774;&#35745;&#20986;&#22270;&#21435;&#22122;&#25193;&#25955;&#26041;&#27861;&#30340;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36215;&#28857;&#32456;&#28857;&#30697;&#38453;&#20272;&#35745;&#20102;&#21306;&#22495;&#20043;&#38388;&#30340;&#20986;&#34892;&#20154;&#25968;&#65292;&#21363;&#22478;&#24066;&#20013;&#30340;&#20154;&#27969;&#37327;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#22478;&#24066;&#35268;&#21010;&#12289;&#20132;&#36890;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21435;&#22122;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#20174;&#32593;&#32476;&#30340;&#35282;&#24230;&#29983;&#25104;&#22478;&#24066;&#20840;&#22495;&#30340;&#36215;&#28857;&#19982;&#32456;&#28857;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#21306;&#22495;&#23618;&#38754;&#30340;&#22478;&#24066;&#29305;&#24449;&#26469;&#35774;&#35745;&#20986;&#22270;&#21435;&#22122;&#25193;&#25955;&#26041;&#27861;&#30340;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#12290;&#20026;&#20102;&#20811;&#26381;&#28085;&#30422;&#25968;&#21315;&#20010;&#21306;&#22495;&#30340;&#22478;&#24066;&#20840;&#22495;&#36215;&#28857;&#21644;&#32456;&#28857;&#30697;&#38453;&#30340;&#23398;&#20064;&#38590;&#24230;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#25104;&#20102;&#33509;&#24178;&#23567;&#22359;&#12289;&#29420;&#31435;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Origin-Destination~(OD) matrix provides an estimation of number of individuals traveling between regions, i.e., mobility flow in the city, which is widely-used in urban planning, transportation, etc. Given various city characteristics of urban regions, generating the city-wide OD matrix without using historical flow information has become increasingly appealing to both researchers and practitioners. However, existing works are limited in independent generation of each element, i.e., flow, in OD matrix, overlooking the element relations within the matrix that can be well formulated as a network. In this paper, we instead propose to generate the city-wide OD matrix from the network perspective, and design a graph denoising diffusion method to learn the conditional joint probability distribution of all elements in the OD matrix given city characteristics at region level. To overcome the learning difficulty of the city-wide OD matrix covering over thousands of regions, we decompose the
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#24322;&#36136;&#24615;&#12289;&#36164;&#28304;&#20998;&#37197;&#12289;&#36890;&#20449;&#25104;&#26412;&#21644;&#20844;&#24179;&#24615;&#12290;&#23458;&#25143;&#36873;&#25321;&#26041;&#26696;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26368;&#24120;&#29992;&#30340;&#25351;&#26631;&#26159;&#27979;&#35797;&#20934;&#30830;&#24615;&#19982;&#36890;&#20449;&#36718;&#27425;&#12290;</title><link>http://arxiv.org/abs/2306.04862</link><description>&lt;p&gt;
&#8220;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#8221;
&lt;/p&gt;
&lt;p&gt;
A Systematic Literature Review on Client Selection in Federated Learning. (arXiv:2306.04862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04862
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#24322;&#36136;&#24615;&#12289;&#36164;&#28304;&#20998;&#37197;&#12289;&#36890;&#20449;&#25104;&#26412;&#21644;&#20844;&#24179;&#24615;&#12290;&#23458;&#25143;&#36873;&#25321;&#26041;&#26696;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26368;&#24120;&#29992;&#30340;&#25351;&#26631;&#26159;&#27979;&#35797;&#20934;&#30830;&#24615;&#19982;&#36890;&#20449;&#36718;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#38544;&#31169;&#30340;&#25285;&#24551;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;2017&#24180;&#34987;&#21457;&#26126;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#65289;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#26356;&#26032;&#21457;&#36865;&#21040;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#12290;&#38543;&#26426;&#36873;&#25321;&#23458;&#25143;&#31471;&#36827;&#34892;FL&#21487;&#33021;&#20250;&#23545;&#23398;&#20064;&#24615;&#33021;&#36896;&#25104;&#20260;&#23475;&#65292;&#22240;&#20026;&#21407;&#22240;&#21508;&#24322;&#12290;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#20915;FL&#23458;&#25143;&#31471;&#36873;&#25321;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20027;&#39064;&#30340;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65288;SLR&#65289;&#19981;&#23384;&#22312;&#12290;&#26412;&#25991;&#30340;SLR&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#29616;&#29366;&#65292;&#24182;&#22238;&#31572;&#20102;&#25361;&#25112;&#12289;&#35299;&#20915;&#26041;&#26696;&#21644;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#26102;&#20351;&#29992;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#23545;47&#31687;&#20027;&#35201;&#30740;&#31350;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#22312;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#38754;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#24322;&#36136;&#24615;&#12289;&#36164;&#28304;&#20998;&#37197;&#12289;&#36890;&#20449;&#25104;&#26412;&#21644;&#20844;&#24179;&#24615;&#12290;&#23458;&#25143;&#36873;&#25321;&#26041;&#26696;&#26088;&#22312;&#36890;&#36807;&#19987;&#27880;&#20110;&#19978;&#36848;&#19968;&#39033;&#25110;&#20960;&#39033;&#25361;&#25112;&#26469;&#25913;&#36827;&#21407;&#22987;&#30340;&#38543;&#26426;&#36873;&#25321;&#31639;&#27861;&#12290;&#26368;&#24120;&#29992;&#30340;&#25351;&#26631;&#26159;&#27979;&#35797;&#20934;&#30830;&#24615;&#19982;&#36890;&#20449;&#36718;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the arising concerns of privacy within machine learning, federated learning (FL) was invented in 2017, in which the clients, such as mobile devices, train a model and send the update to the centralized server. Choosing clients randomly for FL can harm learning performance due to different reasons. Many studies have proposed approaches to address the challenges of client selection of FL. However, no systematic literature review (SLR) on this topic existed. This SLR investigates the state of the art of client selection in FL and answers the challenges, solutions, and metrics to evaluate the solutions. We systematically reviewed 47 primary studies. The main challenges found in client selection are heterogeneity, resource allocation, communication costs, and fairness. The client selection schemes aim to improve the original random selection algorithm by focusing on one or several of the aforementioned challenges. The most common metric used is testing accuracy versus communication rou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23707;&#23679;&#30340;&#38543;&#26426;&#21160;&#24577;&#30005;&#21387;&#35843;&#33410;&#65288;iRDVS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#38450;&#33539;&#21151;&#29575;&#20391;&#20449;&#36947;&#25915;&#20987;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04859</link><description>&lt;p&gt;
&#22522;&#20110;&#23707;&#23679;&#30340;&#38543;&#26426;&#21160;&#24577;&#30005;&#21387;&#35843;&#33410;&#19982;ML&#22686;&#24378;&#22411;&#21151;&#29575;&#20391;&#20449;&#36947;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Island-based Random Dynamic Voltage Scaling vs ML-Enhanced Power Side-Channel Attacks. (arXiv:2306.04859v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23707;&#23679;&#30340;&#38543;&#26426;&#21160;&#24577;&#30005;&#21387;&#35843;&#33410;&#65288;iRDVS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#38450;&#33539;&#21151;&#29575;&#20391;&#20449;&#36947;&#25915;&#20987;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#23707;&#23679;&#30340;&#38543;&#26426;&#21160;&#24577;&#30005;&#21387;&#35843;&#33410;&#65288;iRDVS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#38450;&#33539;&#21151;&#29575;&#20391;&#20449;&#36947;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#29420;&#31435;&#30005;&#21387;&#23707;&#30340;&#25968;&#37327;&#23545;&#20449;&#22122;&#27604;&#21644;&#36712;&#36857;&#38169;&#20301;&#30340;&#24433;&#21709;&#12290;&#20316;&#20026;&#25105;&#20204;&#23545;&#38169;&#20301;&#30340;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#25915;&#20987;&#65292;&#23545;&#20110;&#20855;&#26377;&#19977;&#20010;&#25110;&#26356;&#23569;&#29420;&#31435;&#30005;&#21387;&#30340;&#31995;&#32479;&#24456;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#22235;&#20010;&#30005;&#21387;&#23707;&#30340;iRDVS&#22312;200k&#21152;&#23494;&#36319;&#36394;&#19979;&#26080;&#27861;&#34987;&#30772;&#35299;&#65292;&#35828;&#26126;iRDVS&#21487;&#20197;&#26377;&#25928;&#12290;&#25105;&#20204;&#26368;&#21518;&#36890;&#36807;&#25551;&#36848;&#19968;&#20010;12&#32435;&#31859;FinFet&#24037;&#33402;&#19979;&#30340;iRDVS&#27979;&#35797;&#33455;&#29255;&#26469;&#32467;&#26463;&#35762;&#35805;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#20010;&#21464;&#20307;&#30340;AES-256&#21152;&#36895;&#22120;&#65292;&#25152;&#26377;&#36825;&#20123;&#21152;&#36895;&#22120;&#22343;&#28304;&#33258;&#21516;&#19968;RTL&#12290;&#36825;&#21253;&#25324;&#19968;&#20010;&#21516;&#27493;&#26680;&#24515;&#65292;&#19968;&#20010;&#27809;&#26377;&#20445;&#25252;&#30340;&#24322;&#27493;&#26680;&#24515;&#65292;&#20197;&#21450;&#19968;&#20010;&#20351;&#29992;&#24322;&#27493;&#36923;&#36753;&#37319;&#29992;iRDVS&#25216;&#26415;&#30340;&#26680;&#24515;&#12290;&#33455;&#29255;&#30340;&#23454;&#39564;&#23460;&#27979;&#37327;&#34920;&#26126;&#65292;&#20004;&#20010;&#26410;&#21463;&#20445;&#25252;&#30340;&#21464;&#20307;&#37117;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe and analyze an island-based random dynamic voltage scaling (iRDVS) approach to thwart power side-channel attacks. We first analyze the impact of the number of independent voltage islands on the resulting signal-to-noise ratio and trace misalignment. As part of our analysis of misalignment, we propose a novel unsupervised machine learning (ML) based attack that is effective on systems with three or fewer independent voltages. Our results show that iRDVS with four voltage islands, however, cannot be broken with 200k encryption traces, suggesting that iRDVS can be effective. We finish the talk by describing an iRDVS test chip in a 12nm FinFet process that incorporates three variants of an AES-256 accelerator, all originating from the same RTL. This included a synchronous core, an asynchronous core with no protection, and a core employing the iRDVS technique using asynchronous logic. Lab measurements from the chips indicated that both unprotected variants failed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#35299;&#37322;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#12290;&#37319;&#26679;&#22120;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#24471;&#20998;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.04848</link><description>&lt;p&gt;
&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#35299;&#37322;&#21644;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting and Improving Diffusion Models Using the Euclidean Distance Function. (arXiv:2306.04848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#35299;&#37322;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#12290;&#37319;&#26679;&#22120;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#24471;&#20998;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#30452;&#35273;&#19978;&#19982;&#25237;&#24433;&#26377;&#20851;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#27969;&#24418;&#20551;&#35774;&#19979;&#65292;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#36817;&#20284;&#31561;&#20215;&#20110;&#27491;&#20132;&#25200;&#21160;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#21435;&#22122;&#36817;&#20284;&#20110;&#23398;&#20064;&#25237;&#24433;&#12290;&#26412;&#25991;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#23558;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#35299;&#37322;&#20026;&#24212;&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#30340;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#21435;&#22122;&#22120;&#25237;&#24433;&#35823;&#24046;&#30340;&#31616;&#21333;&#20551;&#35774;&#65292;&#25552;&#20379;DDIM&#65288;Denoising Diffusion Implicit Models&#65289;&#37319;&#26679;&#22120;&#30340;&#31616;&#21333;&#25910;&#25947;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#29702;&#35770;&#32467;&#26524;&#30340;&#27934;&#35265;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;DDIM&#30340;&#20004;&#20010;&#31616;&#21333;&#20462;&#25913;&#30340;&#26032;&#37319;&#26679;&#22120;&#12290;&#20165;&#38656;&#35201;5-10&#20010;&#20989;&#25968;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#37319;&#26679;&#22120;&#23601;&#33021;&#22312;&#39044;&#35757;&#32451;&#30340;CIFAR-10&#21644;CelebA&#27169;&#22411;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;FID&#24471;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to reinterpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection-error of the denoiser. Finally, we propose a new sampler based on two simple modifications to DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#20449;&#24687;&#30452;&#25509;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#36827;&#34892;&#27604;&#36739;&#65292;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26399;&#26395;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#22312;&#21407;&#28857;&#38468;&#36817;&#30340;&#36755;&#20837;&#26377;&#30528;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04847</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#37325;&#36807;&#31243;&#23558;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#23884;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Embedding stochastic differential equations into neural networks via dual processes. (arXiv:2306.04847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04847
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#20449;&#24687;&#30452;&#25509;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#36827;&#34892;&#27604;&#36739;&#65292;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26399;&#26395;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#22312;&#21407;&#28857;&#38468;&#36817;&#30340;&#36755;&#20837;&#26377;&#30528;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26399;&#26395;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36755;&#20837;&#21644;&#36755;&#20986;&#25968;&#25454;&#38598;&#65307;&#30456;&#21453;&#65292;&#20174;&#26102;&#38388;&#28436;&#21270;&#26041;&#31243;&#33719;&#24471;&#30340;&#20449;&#24687;&#65292;&#21363;&#30456;&#24212;&#30340;&#21452;&#37325;&#36807;&#31243;&#65292;&#30452;&#25509;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#36827;&#34892;&#27604;&#36739;&#12290;&#20316;&#20026;&#28436;&#31034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#29992;&#20110;Ornstein-Uhlenbeck&#36807;&#31243;&#21644;&#22122;&#22768;van der Pol&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#23398;&#20064;&#30340;&#32593;&#32476;&#30340;&#26174;&#30528;&#29305;&#24449;&#26159;&#22312;&#21407;&#28857;&#38468;&#36817;&#30340;&#36755;&#20837;&#30340;&#20934;&#30830;&#24230;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#22240;&#20026;&#23398;&#20064;&#30340;&#32593;&#32476;&#19981;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach to constructing a neural network for predicting expectations of stochastic differential equations. The proposed method does not need data sets of inputs and outputs; instead, the information obtained from the time-evolution equations, i.e., the corresponding dual process, is directly compared with the weights in the neural network. As a demonstration, we construct neural networks for the Ornstein-Uhlenbeck process and the noisy van der Pol system. The remarkable feature of learned networks with the proposed method is the accuracy of inputs near the origin. Hence, it would be possible to avoid the overfitting problem because the learned network does not depend on training data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#20856;&#39564;&#35777;&#37327;&#23376;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20197;&#20415;&#32463;&#20856;&#23458;&#25143;&#22996;&#25176;&#23398;&#20064;&#32473;&#19981;&#21487;&#20449;&#30340;&#37327;&#23376;&#26381;&#21153;&#22120;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#23545;&#20183;&#23398;&#20064;&#22855;&#20598;&#24615;&#21644;&#20613;&#37324;&#21494;&#31232;&#30095;&#20989;&#25968;&#19981;&#21487;&#20449;&#37327;&#23376;&#35777;&#26126;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04843</link><description>&lt;p&gt;
&#37327;&#23376;&#23398;&#20064;&#30340;&#32463;&#20856;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Classical Verification of Quantum Learning. (arXiv:2306.04843v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#20856;&#39564;&#35777;&#37327;&#23376;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20197;&#20415;&#32463;&#20856;&#23458;&#25143;&#22996;&#25176;&#23398;&#20064;&#32473;&#19981;&#21487;&#20449;&#30340;&#37327;&#23376;&#26381;&#21153;&#22120;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#23545;&#20183;&#23398;&#20064;&#22855;&#20598;&#24615;&#21644;&#20613;&#37324;&#21494;&#31232;&#30095;&#20989;&#25968;&#19981;&#21487;&#20449;&#37327;&#23376;&#35777;&#26126;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#25968;&#25454;&#35775;&#38382;&#21644;&#37327;&#23376;&#22788;&#29702;&#21487;&#20197;&#20351;&#26576;&#20123;&#32463;&#20856;&#38590;&#20197;&#22788;&#29702;&#30340;&#23398;&#20064;&#20219;&#21153;&#21464;&#24471;&#21487;&#34892;&#12290;&#28982;&#32780;&#65292;&#37327;&#23376;&#33021;&#21147;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#21482;&#33021;&#25552;&#20379;&#32473;&#23569;&#25968;&#20154;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#21487;&#38752;&#30340;&#26041;&#26696;&#65292;&#20801;&#35768;&#32463;&#20856;&#23458;&#25143;&#22996;&#25176;&#23398;&#20064;&#32473;&#19981;&#21487;&#20449;&#30340;&#37327;&#23376;&#26381;&#21153;&#22120;&#65292;&#20197;&#20419;&#36827;&#24191;&#27867;&#33719;&#24471;&#37327;&#23376;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#21476;&#20856;&#26426;&#22120;&#23398;&#20064;&#20132;&#20114;&#35777;&#26126;&#31995;&#32479;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32463;&#20856;&#39564;&#35777;&#37327;&#23376;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#32463;&#20856;&#23398;&#20064;&#32773;&#26080;&#27861;&#33258;&#34892;&#39640;&#25928;&#27714;&#35299;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#20294;&#24403;&#19982;&#19981;&#21487;&#20449;&#30340;&#37327;&#23376;&#35777;&#26126;&#32773;&#20132;&#20114;&#26102;&#65292;&#20182;&#20204;&#21487;&#20197;&#39640;&#25928;&#19988;&#21487;&#38752;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20851;&#20110;&#20855;&#26377;&#22343;&#21248;&#36755;&#20837;&#36793;&#32536;&#23494;&#24230;&#30340;&#23545;&#20183;&#23398;&#20064;&#22855;&#20598;&#24615;&#21644;&#20613;&#37324;&#21494;&#31232;&#30095;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#23376;&#25968;&#25454;&#35775;&#38382;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;&#21472;&#21152;&#28151;&#21512;&#37327;&#23376;&#26679;&#20363;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum data access and quantum processing can make certain classically intractable learning tasks feasible. However, quantum capabilities will only be available to a select few in the near future. Thus, reliable schemes that allow classical clients to delegate learning to untrusted quantum servers are required to facilitate widespread access to quantum learning advantages. Building on a recently introduced framework of interactive proof systems for classical machine learning, we develop a framework for classical verification of quantum learning. We exhibit learning problems that a classical learner cannot efficiently solve on their own, but that they can efficiently and reliably solve when interacting with an untrusted quantum prover. Concretely, we consider the problems of agnostic learning parities and Fourier-sparse functions with respect to distributions with uniform input marginal. We propose a new quantum data access model that we call "mixture-of-superpositions" quantum example
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$K$&#26368;&#36817;&#37051;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#31639;&#21382;&#21490;&#25968;&#25454;&#20013;&#30001;&#19981;&#21516;&#31574;&#30053;&#29983;&#25104;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04836</link><description>&lt;p&gt;
$K$&#26368;&#36817;&#37051;&#37325;&#37319;&#26679;&#29992;&#20110;&#38543;&#26426;&#25511;&#21046;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
$K$-Nearest-Neighbor Resampling for Off-Policy Evaluation in Stochastic Control. (arXiv:2306.04836v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04836
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$K$&#26368;&#36817;&#37051;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#31639;&#21382;&#21490;&#25968;&#25454;&#20013;&#30001;&#19981;&#21516;&#31574;&#30053;&#29983;&#25104;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$K$&#26368;&#36817;&#37051;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#31639;&#21382;&#21490;&#25968;&#25454;&#20013;&#30001;&#19981;&#21516;&#31574;&#30053;&#29983;&#25104;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20381;&#36182;&#20110;&#24403;&#21069;&#29366;&#24577;&#30340;&#21453;&#39304;&#31574;&#30053;&#65292;&#36825;&#31181;&#31574;&#30053;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21644;&#25152;&#36873;&#21160;&#20316;&#24433;&#21709;&#19979;&#30340;&#31995;&#32479;&#22266;&#26377;&#38543;&#26426;&#24615;&#30340;&#29615;&#22659;&#12290;&#36825;&#20123;&#35774;&#32622;&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#24212;&#29992;&#31243;&#24207;&#20013;&#24456;&#24120;&#35265;&#65292;&#24182;&#22312;&#38543;&#26426;&#25511;&#21046;&#30340;&#19978;&#19979;&#25991;&#20013;&#31215;&#26497;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#21033;&#29992;&#20102;&#31867;&#20284;&#30340;&#29366;&#24577;/&#21160;&#20316;&#23545;&#65288;&#22312;&#24230;&#37327;&#24847;&#20041;&#19979;&#65289;&#19982;&#31867;&#20284;&#30340;&#22870;&#21169;&#21644;&#29366;&#24577;&#36716;&#25442;&#30456;&#20851;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#37325;&#37319;&#26679;&#36807;&#31243;&#36890;&#36807;&#31867;&#20284;&#20110;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#36712;&#36857;&#27169;&#25311;&#26469;&#35299;&#20915;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#20013;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#38382;&#39064;&#12290;&#19982;&#20854;&#20182;OPE&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#20248;&#21270;&#65292;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#26641;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#39640;&#25928;&#23454;&#29616;&#65292;&#24182;&#19988;&#26412;&#36136;&#19978;&#26159;&#21487;&#24182;&#34892;&#21270;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#22312;&#22522;&#20934;&#29615;&#22659;&#19979;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#20248;&#36234;&#23454;&#39564;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel $K$-nearest neighbor resampling procedure for estimating the performance of a policy from historical data containing realized episodes of a decision process generated under a different policy. We focus on feedback policies that depend deterministically on the current state in environments with continuous state-action spaces and system-inherent stochasticity effected by chosen actions. Such settings are common in a wide range of high-stake applications and are actively investigated in the context of stochastic control. Our procedure exploits that similar state/action pairs (in a metric sense) are associated with similar rewards and state transitions. This enables our resampling procedure to tackle the counterfactual estimation problem underlying off-policy evaluation (OPE) by simulating trajectories similarly to Monte Carlo methods. Compared to other OPE methods, our algorithm does not require optimization, can be efficiently implemented via tree-based nearest neighbo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#24402;&#32435;&#31639;&#27861;INDUCE&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04835</link><description>&lt;p&gt;
&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#36890;&#36807;&#24402;&#32435;&#24615;
&lt;/p&gt;
&lt;p&gt;
Empowering Counterfactual Reasoning over Graph Neural Networks through Inductivity. (arXiv:2306.04835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#24402;&#32435;&#31639;&#27861;INDUCE&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26377;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#33647;&#29289;&#21457;&#29616;&#12289;&#25512;&#33616;&#24341;&#25806;&#21644;&#33455;&#29255;&#35774;&#35745;&#12290;&#20294;&#26159;&#65292;GNN&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26469;&#25903;&#25345;&#20854;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;GNN&#30340;&#36755;&#20837;&#22270;&#36827;&#34892;&#26368;&#23567;&#26356;&#25913;&#65292;&#20197;&#25913;&#21464;&#20854;&#39044;&#27979;&#32467;&#26524;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#26469;&#35299;&#37322;GNN&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#36793;&#21024;&#38500;&#20316;&#20026;&#25200;&#21160;&#12290;&#20854;&#27425;&#65292;&#21453;&#20107;&#23454;&#35299;&#37322;&#27169;&#22411;&#26159;&#20256;&#23548;&#24615;&#30340;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#19981;&#33021;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;INDUCE&#30340;&#24402;&#32435;&#31639;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21253;&#25324;&#36793;&#28155;&#21152;&#22312;&#20869;&#30340;&#25913;&#36827;&#21487;&#33719;&#24471;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#24402;&#32435;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#20063;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have various practical applications, such as drug discovery, recommendation engines, and chip design. However, GNNs lack transparency as they cannot provide understandable explanations for their predictions. To address this issue, counterfactual reasoning is used. The main goal is to make minimal changes to the input graph of a GNN in order to alter its prediction. While several algorithms have been proposed for counterfactual explanations of GNNs, most of them have two main drawbacks. Firstly, they only consider edge deletions as perturbations. Secondly, the counterfactual explanation models are transductive, meaning they do not generalize to unseen data. In this study, we introduce an inductive algorithm called INDUCE, which overcomes these limitations. By conducting extensive experiments on several datasets, we demonstrate that incorporating edge additions leads to better counterfactual results compared to the existing methods. Moreover, the inductive mo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#23558;&#20154;&#36896;&#29289;&#20307;&#35270;&#20026;&#24322;&#24120;&#26469;&#26816;&#27979;&#23427;&#20204;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#32858;&#31867;&#22270;&#20687;&#25968;&#25454;&#24182;&#25552;&#21462;&#21487;&#33021;&#21253;&#21547;&#24322;&#24120;&#29305;&#24449;&#30340;&#22270;&#20687;&#20197;&#21450;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;&#25552;&#21462;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#24046;&#30340;&#24322;&#24120;&#24471;&#20998;&#26041;&#27861;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#27700;&#19979;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.04834</link><description>&lt;p&gt;
&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#27700;&#19979;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Semi-supervised Object Detection Algorithm for Underwater Imagery. (arXiv:2306.04834v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#23558;&#20154;&#36896;&#29289;&#20307;&#35270;&#20026;&#24322;&#24120;&#26469;&#26816;&#27979;&#23427;&#20204;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#32858;&#31867;&#22270;&#20687;&#25968;&#25454;&#24182;&#25552;&#21462;&#21487;&#33021;&#21253;&#21547;&#24322;&#24120;&#29305;&#24449;&#30340;&#22270;&#20687;&#20197;&#21450;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;&#25552;&#21462;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#24046;&#30340;&#24322;&#24120;&#24471;&#20998;&#26041;&#27861;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#27700;&#19979;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#27700;&#19979;&#26426;&#22120;&#20154;&#65288;AUV&#65289;&#37319;&#38598;&#30340;&#27700;&#19979;&#22270;&#20687;&#20013;&#26816;&#27979;&#20154;&#36896;&#29289;&#20307;&#26159;&#35768;&#22810;&#28023;&#24213;&#24212;&#29992;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;AUV&#22270;&#20687;&#25968;&#25454;&#38598;&#24448;&#24448;&#38750;&#24120;&#22823;&#19988;&#26410;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#19981;&#24179;&#34913;&#65292;&#21253;&#21547;&#24456;&#23569;&#30340;&#24863;&#20852;&#36259;&#23545;&#35937;&#30340;&#23454;&#20363;&#65292;&#29305;&#21035;&#26159;&#22312;&#25628;&#32034;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#23545;&#35937;&#26102;&#12290;&#22240;&#27492;&#65292;&#38590;&#20197;&#25311;&#21512;&#33021;&#22815;&#21487;&#38752;&#26816;&#27979;&#36825;&#20123;&#23545;&#35937;&#30340;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#22240;&#32032;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#20154;&#36896;&#29289;&#20307;&#35270;&#20026;&#24322;&#24120;&#20540;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#26469;&#26816;&#27979;&#23427;&#20204;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#25968;&#25454;&#32858;&#31867;&#21040;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#24182;&#25552;&#21462;&#21487;&#33021;&#21253;&#21547;&#24322;&#24120;&#29305;&#24449;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#21462;&#22270;&#20687;&#30340;&#37325;&#24314;&#36136;&#37327;&#24046;&#30340;&#21306;&#22495;&#30340;&#24322;&#24120;&#24471;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#21521;&#20154;&#31867;&#25805;&#20316;&#21592;&#26174;&#31034;&#20505;&#36873;&#29289;&#20307;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#27700;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of artificial objects from underwater imagery gathered by Autonomous Underwater Vehicles (AUVs) is a key requirement for many subsea applications. Real-world AUV image datasets tend to be very large and unlabelled. Furthermore, such datasets are typically imbalanced, containing few instances of objects of interest, particularly when searching for unusual objects in a scene. It is therefore, difficult to fit models capable of reliably detecting these objects. Given these factors, we propose to treat artificial objects as anomalies and detect them through a semi-supervised framework based on Variational Autoencoders (VAEs). We develop a method which clusters image data in a learned low-dimensional latent space and extracts images that are likely to contain anomalous features. We also devise an anomaly score based on extracting poorly reconstructed regions of an image. We demonstrate that by applying both methods on large image datasets, human operators can be shown candidate an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#30456;&#20284;&#24615;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#30340;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#65292;&#22312;&#21512;&#25104;MOVi&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26412;&#27169;&#22411;&#26159;&#39318;&#20010;&#33021;&#22815;&#25193;&#23637;&#21040;&#26080;&#32422;&#26463;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.04829</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#27979;&#26102;&#38388;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities. (arXiv:2306.04829v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#30456;&#20284;&#24615;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#30340;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#65292;&#22312;&#21512;&#25104;MOVi&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26412;&#27169;&#22411;&#26159;&#39318;&#20010;&#33021;&#22815;&#25193;&#23637;&#21040;&#26080;&#32422;&#26463;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#26159;&#20174;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#35270;&#39057;&#38598;&#21512;&#20013;&#23398;&#20064;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#21482;&#33021;&#22312;&#21463;&#38480;&#39046;&#22495;&#20869;&#32553;&#25918;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#29305;&#24449;&#30340;&#37325;&#24314;&#20250;&#23548;&#33268;&#22312;&#19981;&#21463;&#32422;&#26463;&#30340;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20123;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#24418;&#24335;&#20026;&#26102;&#38388;&#29305;&#24449;&#30456;&#20284;&#24615;&#25439;&#22833;&#12290;&#35813;&#25439;&#22833;&#32534;&#30721;&#22270;&#20687;&#22359;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#33258;&#28982;&#22320;&#24341;&#20837;&#36816;&#21160;&#20559;&#24046;&#26469;&#21457;&#29616;&#29289;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#25439;&#22833;&#23548;&#33268;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;MOVi&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#24403;&#19982;&#29305;&#24449;&#37325;&#24314;&#25439;&#22833;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#39318;&#20010;&#33021;&#22815;&#25193;&#23637;&#21040;&#26080;&#32422;&#26463;&#35270;&#39057;&#25968;&#25454;&#38598;&#65288;&#22914;YouTube-VIS&#65289;&#30340;&#29289;&#20307;&#20013;&#24515;&#35270;&#39057;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised video-based object-centric learning is a promising avenue to learn structured representations from large, unlabeled video collections, but previous approaches have only managed to scale to real-world datasets in restricted domains. Recently, it was shown that the reconstruction of pre-trained self-supervised features leads to object-centric representations on unconstrained real-world image datasets. Building on this approach, we propose a novel way to use such pre-trained features in the form of a temporal feature similarity loss. This loss encodes temporal correlations between image patches and is a natural way to introduce a motion bias for object discovery. We demonstrate that this loss leads to state-of-the-art performance on the challenging synthetic MOVi datasets. When used in combination with the feature reconstruction loss, our model is the first object-centric video model that scales to unconstrained video datasets such as YouTube-VIS.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#21270;&#38543;&#26426;&#29983;&#25104;&#26641;&#30340;GNN&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#27604;&#20854;&#20182;&#32463;&#20856;&#31639;&#27861;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2306.04828</link><description>&lt;p&gt;
&#32447;&#24615;&#21270;&#38543;&#26426;&#29983;&#25104;&#26641;&#19982;GNN&#30340;&#24555;&#36895;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Fast and Effective GNN Training with Linearized Random Spanning Trees. (arXiv:2306.04828v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#21270;&#38543;&#26426;&#29983;&#25104;&#26641;&#30340;GNN&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#27604;&#20854;&#20182;&#32463;&#20856;&#31639;&#27861;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#30340;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#35757;&#32451;GNN&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32447;&#24615;&#21270;&#20174;&#36755;&#20837;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#38543;&#26426;&#29983;&#25104;&#26641;&#24471;&#21040;&#19968;&#31995;&#21015;&#36335;&#24452;&#22270;&#26469;&#36880;&#27493;&#31934;&#32454;&#21270;&#26435;&#37325;&#26356;&#26032;&#25805;&#20316;&#12290;&#36335;&#24452;&#22270;&#34987;&#35774;&#35745;&#20026;&#20445;&#30041;&#21407;&#22987;&#22270;&#30340;&#22522;&#26412;&#25299;&#25169;&#21644;&#33410;&#28857;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#36335;&#24452;&#22270;&#30340;&#31232;&#30095;&#24615;&#20351;&#24471;GNN&#35757;&#32451;&#26356;&#36731;&#20415;&#65292;&#38500;&#20102;&#21487;&#25193;&#23637;&#24615;&#22806;&#65292;&#36824;&#26377;&#21161;&#20110;&#32531;&#35299;&#36807;&#24230;&#21387;&#32553;&#21644;&#36807;&#24230;&#24179;&#28369;&#31561;&#32463;&#20856;&#35757;&#32451;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65292;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#21644;&#27979;&#35797;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new effective and scalable framework for training GNNs in supervised node classification tasks, given graph-structured data. Our approach increasingly refines the weight update operations on a sequence of path graphs obtained by linearizing random spanning trees extracted from the input network. The path graphs are designed to retain essential topological and node information of the original graph. At the same time, the sparsity of path graphs enables a much lighter GNN training which, besides scalability, helps in mitigating classical training issues, like over-squashing and over-smoothing. We carry out an extensive experimental investigation on a number of real-world graph benchmarks, where we apply our framework to graph convolutional networks, showing simultaneous improvement of both training speed and test accuracy, as compared to well-known baselines.
&lt;/p&gt;</description></item><item><title>SLCE&#26159;&#29992;&#32447;&#24615;&#21464;&#25442;&#23558;&#28857;&#37325;&#26500;&#20026;&#31867;&#21035;&#36136;&#24515;&#24182;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#24809;&#32602;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#28388;&#38500;&#19981;&#24517;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#36873;&#25321;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20026;&#22810;&#31867;&#25968;&#25454;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26174;&#30528;&#36739;&#23569;&#30340;&#29305;&#24449;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.04824</link><description>&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#36136;&#24515;&#32534;&#30721;&#22120;&#65306;&#19968;&#31181;&#29305;&#24449;&#36873;&#25321;&#30340;&#20984;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparse Linear Centroid-Encoder: A Convex Method for Feature Selection. (arXiv:2306.04824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04824
&lt;/p&gt;
&lt;p&gt;
SLCE&#26159;&#29992;&#32447;&#24615;&#21464;&#25442;&#23558;&#28857;&#37325;&#26500;&#20026;&#31867;&#21035;&#36136;&#24515;&#24182;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#24809;&#32602;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#28388;&#38500;&#19981;&#24517;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#36873;&#25321;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20026;&#22810;&#31867;&#25968;&#25454;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26174;&#30528;&#36739;&#23569;&#30340;&#29305;&#24449;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#65292;&#31216;&#20026;&#31232;&#30095;&#32447;&#24615;&#36136;&#24515;&#32534;&#30721;&#22120;&#65288;SLCE&#65289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#19968;&#20010;&#32447;&#24615;&#21464;&#25442;&#23558;&#19968;&#20010;&#28857;&#37325;&#26500;&#20026;&#20854;&#31867;&#21035;&#30340;&#36136;&#24515;&#65292;&#24182;&#21516;&#26102;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#24809;&#32602;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#28388;&#38500;&#19981;&#24517;&#35201;&#30340;&#29305;&#24449;&#12290;&#20248;&#21270;&#38382;&#39064;&#30340;&#21407;&#22987;&#20844;&#24335;&#26159;&#38750;&#20984;&#30340;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#27861;&#65292;&#20854;&#20013;&#27599;&#19968;&#27493;&#37117;&#26159;&#20984;&#30340;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#32447;&#24615;&#36136;&#24515;&#32534;&#30721;&#22120;&#65292;&#23427;&#26159;&#19968;&#20010;&#30697;&#38453;$A$&#19978;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#21482;&#22312;&#23545;&#35282;&#32447;&#30697;&#38453;$B$&#19978;&#25628;&#32034;&#31232;&#30095;&#35299;&#65292;&#21516;&#26102;&#20445;&#25345;$A$&#19981;&#21464;&#12290;&#19982;&#20854;&#20182;&#32447;&#24615;&#26041;&#27861;&#65288;&#20363;&#22914;&#31232;&#30095;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;Lasso&#65289;&#19981;&#21516;&#65292;&#31232;&#30095;&#32447;&#24615;&#36136;&#24515;&#32534;&#30721;&#22120;&#23545;&#20110;&#22810;&#31867;&#25968;&#25454;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;&#23427;&#20419;&#36827;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#39640;&#32500;&#29983;&#29289;&#25968;&#25454;&#65289;&#19978;&#30340;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SLCE&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#26041;&#38754;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26174;&#30528;&#36739;&#23569;&#30340;&#29305;&#24449;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel feature selection technique, Sparse Linear Centroid-Encoder (SLCE). The algorithm uses a linear transformation to reconstruct a point as its class centroid and, at the same time, uses the $\ell_1$-norm penalty to filter out unnecessary features from the input data. The original formulation of the optimization problem is nonconvex, but we propose a two-step approach, where each step is convex. In the first step, we solve the linear Centroid-Encoder, a convex optimization problem over a matrix $A$. In the second step, we only search for a sparse solution over a diagonal matrix $B$ while keeping $A$ fixed. Unlike other linear methods, e.g., Sparse Support Vector Machines and Lasso, Sparse Linear Centroid-Encoder uses a single model for multi-class data. We present an in-depth empirical analysis of the proposed model and show that it promotes sparsity on various data sets, including high-dimensional biological data. Our experimental results show that SLCE has a performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#24046;&#24322;&#12290;&#23427;&#21487;&#20197;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2306.04817</link><description>&lt;p&gt;
SiBBlInGS: &#20351;&#29992;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#27169;&#22359;&#25512;&#29702;&#30340;&#24314;&#27169;&#22359;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States. (arXiv:2306.04817v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#24046;&#24322;&#12290;&#23427;&#21487;&#20197;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#26469;&#35828;&#65292;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#27169;&#22359;&#26159;&#21457;&#29616;&#22797;&#26434;&#31995;&#32479;&#20013;&#26377;&#20215;&#20540;&#35265;&#35299;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;(SiBBlInGS)&#65292;&#29992;&#20110;&#21457;&#29616;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#33021;&#22815;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;SiBBlInGS&#36824;&#20801;&#35768;&#36328;&#29366;&#24577;&#21464;&#21270;&#27169;&#22359;&#32467;&#26500;&#21644;&#27599;&#27425;&#35797;&#39564;&#30340;&#26102;&#38388;&#21464;&#24322;&#65292;&#24182;&#21487;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable methods for extracting meaningful building blocks (BBs) underlying multi-dimensional time series are vital for discovering valuable insights in complex systems. Existing techniques, however, encounter limitations that restrict their applicability to real-world systems, like reliance on orthogonality assumptions, inadequate incorporation of inter- and intra-state variability, and incapability to handle sessions of varying duration. Here, we present a framework for Similarity-driven Building Block Inference using Graphs across States (SiBBlInGS). SiBBlInGS employs a graph-based dictionary learning approach for BB discovery, simultaneously considers both inter- and intra-state relationships in the data, can extract non-orthogonal components, and allows for variations in session counts and duration across states. Additionally, SiBBlInGS allows for cross-state variations in BB structure and per-trial temporal variability, can identify state-specific vs state-invariant BBs, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;SGD&#35757;&#32451;&#25439;&#22833;&#20013;&#30340;&#23574;&#23792;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#8220;&#25237;&#30707;&#26426;&#8221;&#20248;&#21270;&#29616;&#35937;&#65292;&#36890;&#36807;&#22686;&#21152;&#19982;&#30495;&#23454;&#39044;&#27979;&#22120;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#23545;&#40784;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#35777;&#26126;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#21487;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04815</link><description>&lt;p&gt;
SGD&#20013;&#30340;&#25237;&#30707;&#26426;&#65306;&#35757;&#32451;&#25439;&#22833;&#20013;&#30340;&#23574;&#23792;&#21450;&#20854;&#36890;&#36807;&#29305;&#24449;&#23398;&#20064;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning. (arXiv:2306.04815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;SGD&#35757;&#32451;&#25439;&#22833;&#20013;&#30340;&#23574;&#23792;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#8220;&#25237;&#30707;&#26426;&#8221;&#20248;&#21270;&#29616;&#35937;&#65292;&#36890;&#36807;&#22686;&#21152;&#19982;&#30495;&#23454;&#39044;&#27979;&#22120;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#23545;&#40784;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#35777;&#26126;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#21487;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#35299;&#37322;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#36827;&#34892;&#35757;&#32451;&#26102;&#20026;&#20160;&#20040;&#32463;&#24120;&#20986;&#29616;&#35757;&#32451;&#25439;&#22833;&#23574;&#23792;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;SGD&#35757;&#32451;&#25439;&#22833;&#20013;&#30340;&#23574;&#23792;&#26159;&#8220;&#25237;&#30707;&#26426;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#20248;&#21270;&#29616;&#35937;&#65292;&#26368;&#21021;&#22312;[Lewkowycz&#31561;&#20154;&#65292;2020&#24180;]&#30340;&#22823;&#23398;&#20064;&#29575;GD&#20013;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25237;&#30707;&#26426;&#20986;&#29616;&#22312;&#30001;&#27491;&#20999;&#20869;&#26680;&#30340;&#21069;&#20960;&#20010;&#29305;&#24449;&#21521;&#37327;&#25152;&#24352;&#25104;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#36866;&#29992;&#20110;GD&#21644;SGD&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#65292;&#21363;&#25237;&#30707;&#26426;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#19982;&#30495;&#23454;&#39044;&#27979;&#22120;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#65288;AGOP&#65289;&#23545;&#40784;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;SGD&#20013;&#65292;&#26356;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#20250;&#23548;&#33268;&#26356;&#22810;&#30340;&#25237;&#30707;&#26426;&#20986;&#29616;&#65292;&#20174;&#32780;&#25552;&#39640;AGOP&#23545;&#40784;&#21644;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we first present an explanation regarding the common occurrence of spikes in the training loss when neural networks are trained with stochastic gradient descent (SGD). We provide evidence that the spikes in the training loss of SGD are "catapults", an optimization phenomenon originally observed in GD with large learning rates in [Lewkowycz et al. 2020]. We empirically show that these catapults occur in a low-dimensional subspace spanned by the top eigenvectors of the tangent kernel, for both GD and SGD. Second, we posit an explanation for how catapults lead to better generalization by demonstrating that catapults promote feature learning by increasing alignment with the Average Gradient Outer Product (AGOP) of the true predictor. Furthermore, we demonstrate that a smaller batch size in SGD induces a larger number of catapults, thereby improving AGOP alignment and test performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26435;&#37325;&#23545;&#31216;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#21512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#22312;&#23618;&#28608;&#27963;&#20043;&#38388;&#25551;&#36848;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#20256;&#25773;&#12290;&#36890;&#36807;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#30456;&#24212;&#30340;&#30446;&#26631;&#21644;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#26356;&#29983;&#29289;&#30495;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.04810</link><description>&lt;p&gt;
&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#65306;&#19968;&#31181;&#26080;&#38656;&#26435;&#37325;&#23545;&#31216;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#21512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry. (arXiv:2306.04810v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26435;&#37325;&#23545;&#31216;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#21512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#22312;&#23618;&#28608;&#27963;&#20043;&#38388;&#25551;&#36848;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#20256;&#25773;&#12290;&#36890;&#36807;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#30456;&#24212;&#30340;&#30446;&#26631;&#21644;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#26356;&#29983;&#29289;&#30495;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#29983;&#29289;&#21512;&#29702;&#24615;&#21463;&#21040;&#20105;&#35758;&#65292;&#29616;&#22312;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#21363;&#22823;&#33041;&#26159;&#21542;&#37319;&#29992;&#31867;&#20284;&#20110;&#23427;&#30340;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#23618;&#28608;&#27963;&#20043;&#38388;&#36827;&#34892;&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26367;&#20195;&#35268;&#33539;&#26041;&#27861;&#65292;&#20197;&#25551;&#36848;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#20449;&#21495;&#22312;&#21069;&#21521;&#21644;&#21518;&#21521;&#26041;&#21521;&#19978;&#20256;&#25773;&#30340;&#26426;&#21046;&#12290;&#36825;&#31181;&#26032;&#26694;&#26550;&#35299;&#20915;&#20102;&#26377;&#20851;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#29983;&#29289;&#21512;&#29702;&#24615;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;&#30456;&#24212;&#30446;&#26631;&#30340;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#65292;&#19982;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#27169;&#25311;&#19968;&#31181;&#20855;&#26377;&#26641;&#31361;&#22788;&#29702;&#21644;&#20391;&#25233;&#21046;&#31070;&#32463;&#20803;&#30340;&#26356;&#29983;&#29289;&#30495;&#23454;&#30340;&#22810;&#23460;&#37329;&#23383;&#22612;&#24418;&#31070;&#32463;&#20803;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
The backpropagation algorithm has experienced remarkable success in training large-scale artificial neural networks, however, its biological-plausibility is disputed, and it remains an open question whether the brain employs supervised learning mechanisms akin to it. Here, we propose correlative information maximization between layer activations as an alternative normative approach to describe the signal propagation in biological neural networks in both forward and backward directions. This new framework addresses many concerns about the biological-plausibility of conventional artificial neural networks and the backpropagation algorithm. The coordinate descent-based optimization of the corresponding objective, combined with the mean square error loss function for fitting labeled supervision data, gives rise to a neural network structure that emulates a more biologically realistic network of multi-compartment pyramidal neurons with dendritic processing and lateral inhibitory neurons. Fu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#27599;&#19968;&#34892;&#25968;&#25454;&#20316;&#20026;&#19968;&#20010;&#21477;&#23376;&#24182;&#28155;&#21152;&#24046;&#20998;&#38544;&#31169;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#31169;&#20154;&#25968;&#25454;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.04803</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#31169;&#19979;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Privately generating tabular data using language models. (arXiv:2306.04803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04803
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#27599;&#19968;&#34892;&#25968;&#25454;&#20316;&#20026;&#19968;&#20010;&#21477;&#23376;&#24182;&#28155;&#21152;&#24046;&#20998;&#38544;&#31169;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#31169;&#20154;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#38544;&#31169;&#20026;&#20808;&#30340;&#19990;&#30028;&#20013;&#65292;&#31169;&#19979;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23558;&#34920;&#26684;&#20013;&#30340;&#27599;&#19968;&#34892;&#35270;&#20026;&#19968;&#20010;&#21477;&#23376;&#65292;&#24182;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#34920;&#26684;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#21363;&#20351;&#26159;&#22312;&#26377;&#21033;&#20110;&#22522;&#20110;&#36793;&#32536;&#20998;&#24067;&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#23567;&#35268;&#27169;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privately generating synthetic data from a table is an important brick of a privacy-first world. We propose and investigate a simple approach of treating each row in a table as a sentence and training a language model with differential privacy. We show this approach obtains competitive results in modelling tabular data across multiple datasets, even at small scales that favor alternative methods based on marginal distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#24050;&#25104;&#20026;&#32452;&#32455;&#21307;&#23398;&#30693;&#35782;&#30340;&#26377;&#32467;&#26500;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21307;&#23398;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#24378;&#35843;&#20102;&#22312;HKG&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#32508;&#36848;&#26159;HKG&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;HKG&#26500;&#24314;&#30340;&#27969;&#31243;&#21644;&#20851;&#38190;&#25216;&#26415;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#21644;&#36890;&#36807;&#38598;&#25104;&#65289;&#65292;&#20197;&#21450;&#24120;&#35265;&#30340;&#21033;&#29992;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#22522;&#20110;&#27169;&#22411;&#65289;&#12290;&#20026;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#25429;&#33719;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65288;&#35813;&#36164;&#28304;&#23384;&#20648;&#20110;https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase&#65289;&#32452;&#32455;&#20102;&#29616;&#26377;&#30340;HKG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#24212;&#29992;&#37096;&#20998;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#33258;&#36866;&#24212;&#29942;&#39048;&#36136;&#24515;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#36873;&#25321;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#26377;&#21306;&#21035;&#30340;&#29305;&#24449;&#32452;&#20197;&#21450;&#22312;&#37325;&#26500;&#31867;&#21035;&#36136;&#24515;&#30340;&#21516;&#26102;&#20943;&#23569;&#21516;&#31867;&#20998;&#25955;&#24230;&#12289;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#36136;&#24515;&#30340;&#20998;&#31163;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#36807;&#28388;&#36755;&#20837;&#25968;&#25454;&#20013;&#19981;&#24517;&#35201;&#29305;&#24449;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04795</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#33258;&#36866;&#24212;&#29942;&#39048;&#36136;&#24515;&#32534;&#30721;&#22120;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Feature Selection using Sparse Adaptive Bottleneck Centroid-Encoder. (arXiv:2306.04795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#33258;&#36866;&#24212;&#29942;&#39048;&#36136;&#24515;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#36873;&#25321;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#26377;&#21306;&#21035;&#30340;&#29305;&#24449;&#32452;&#20197;&#21450;&#22312;&#37325;&#26500;&#31867;&#21035;&#36136;&#24515;&#30340;&#21516;&#26102;&#20943;&#23569;&#21516;&#31867;&#20998;&#25955;&#24230;&#12289;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#36136;&#24515;&#30340;&#20998;&#31163;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#36807;&#28388;&#36755;&#20837;&#25968;&#25454;&#20013;&#19981;&#24517;&#35201;&#29305;&#24449;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#65292;&#21363;&#31232;&#30095;&#33258;&#36866;&#24212;&#29942;&#39048;&#36136;&#24515;&#32534;&#30721;&#22120;&#65288;SABCE&#65289;&#65292;&#29992;&#20110;&#30830;&#23450;&#33021;&#22815;&#21306;&#20998;&#20004;&#20010;&#25110;&#22810;&#20010;&#31867;&#21035;&#30340;&#29305;&#24449;&#12290;&#35813;&#31639;&#27861;&#26088;&#22312;&#25552;&#21462;&#26377;&#21306;&#21035;&#30340;&#29305;&#24449;&#32452;&#65292;&#24182;&#22312;&#29615;&#22659;&#31354;&#38388;&#20013;&#37325;&#26500;&#31867;&#21035;&#36136;&#24515;&#65292;&#21516;&#26102;&#22312;&#29942;&#39048;&#23618;&#20351;&#29992;&#38468;&#21152;&#24809;&#32602;&#39033;&#26469;&#20943;&#23569;&#21516;&#31867;&#20998;&#25955;&#24230;&#24182;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#36136;&#24515;&#30340;&#20998;&#31163;&#24615;&#12290;&#27169;&#22411;&#20855;&#26377;&#20419;&#36827;&#31232;&#30095;&#24615;&#30340;&#23618;&#65288;SPL&#65289;&#65292;&#19982;&#36755;&#20837;&#23618;&#20855;&#26377;&#19968;&#23545;&#19968;&#30340;&#36830;&#25509;&#12290;&#38500;&#20102;&#20027;&#35201;&#30446;&#26631;&#65292;&#25105;&#20204;&#36824;&#26368;&#23567;&#21270;&#31232;&#30095;&#23618;&#30340;$l_{2,1}$-&#33539;&#25968;&#65292;&#20174;&#32780;&#36807;&#28388;&#25481;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#19981;&#24517;&#35201;&#30340;&#29305;&#24449;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36136;&#24515;&#21644;&#31232;&#30095;&#23618;&#30340;&#26435;&#37325;&#30340;Hadamard&#31215;&#26469;&#26356;&#26032;&#31867;&#21035;&#36136;&#24515;&#65292;&#20174;&#32780;&#24573;&#30053;&#30446;&#26631;&#20013;&#30340;&#26080;&#20851;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#37325;&#24314;&#31867;&#21035;&#36136;&#24515;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#36136;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel nonlinear model, Sparse Adaptive Bottleneck Centroid-Encoder (SABCE), for determining the features that discriminate between two or more classes. The algorithm aims to extract discriminatory features in groups while reconstructing the class centroids in the ambient space and simultaneously use additional penalty terms in the bottleneck layer to decrease within-class scatter and increase the separation of different class centroids. The model has a sparsity-promoting layer (SPL) with a one-to-one connection to the input layer. Along with the primary objective, we minimize the $l_{2,1}$-norm of the sparse layer, which filters out unnecessary features from input data. During training, we update class centroids by taking the Hadamard product of the centroids and weights of the sparse layer, thus ignoring the irrelevant features from the target. Therefore the proposed method learns to reconstruct the critical components of class centroids rather than the whole centroids.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#24037;&#20855;--&#20132;&#20114;&#24352;&#37327;&#29992;&#20110;&#36890;&#36807;&#29305;&#24449;&#23545;&#27169;&#22411;&#21644;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#32463;&#39564;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#24449;&#23398;&#20064;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#19968;&#20123;&#32463;&#39564;&#35266;&#23519;&#29616;&#35937;&#65292;&#34920;&#26126;&#28145;&#24230;&#23398;&#20064;&#21487;&#33021;&#21463;&#30410;&#20110;&#25506;&#32034;&#20256;&#32479;IID&#65288;&#29420;&#31435;&#21516;&#20998;&#24067;&#65289;&#20551;&#35774;&#20043;&#22806;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.04793</link><description>&lt;p&gt;
&#35770;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#29305;&#24449;&#30340;&#32852;&#21512;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
On the Joint Interaction of Models, Data, and Features. (arXiv:2306.04793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#24037;&#20855;--&#20132;&#20114;&#24352;&#37327;&#29992;&#20110;&#36890;&#36807;&#29305;&#24449;&#23545;&#27169;&#22411;&#21644;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#32463;&#39564;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#24449;&#23398;&#20064;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#19968;&#20123;&#32463;&#39564;&#35266;&#23519;&#29616;&#35937;&#65292;&#34920;&#26126;&#28145;&#24230;&#23398;&#20064;&#21487;&#33021;&#21463;&#30410;&#20110;&#25506;&#32034;&#20256;&#32479;IID&#65288;&#29420;&#31435;&#21516;&#20998;&#24067;&#65289;&#20551;&#35774;&#20043;&#22806;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#23450;&#20041;&#24615;&#29305;&#24449;&#65292;&#20294;&#26159;&#25105;&#20204;&#23545;&#20110;&#29305;&#24449;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25152;&#36215;&#30340;&#20316;&#29992;&#30340;&#29702;&#35299;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#24037;&#20855;&#65292;&#20132;&#20114;&#24352;&#37327;&#65292;&#29992;&#20110;&#36890;&#36807;&#29305;&#24449;&#23545;&#27169;&#22411;&#21644;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#32463;&#39564;&#20998;&#26512;&#12290;&#36890;&#36807;&#20132;&#20114;&#24352;&#37327;&#65292;&#25105;&#20204;&#23545;&#29305;&#24449;&#22312;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#20197;&#21450;&#19981;&#21516;&#38543;&#26426;&#31181;&#23376;&#30340;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;&#29305;&#24449;&#31561;&#26041;&#38754;&#20570;&#20986;&#20102;&#20960;&#20010;&#20851;&#38190;&#35266;&#23519;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#24449;&#23398;&#20064;&#30340;&#27010;&#24565;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#38381;&#24335;&#24418;&#24335;&#25512;&#23548;&#20986;&#21333;&#20010;&#20551;&#35774;&#30340;&#26399;&#26395;&#20934;&#30830;&#29575;&#21644;&#19968;&#23545;&#20551;&#35774;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#35299;&#37322;&#19968;&#20123;&#32463;&#39564;&#35266;&#23519;&#29616;&#35937;&#65292;&#21253;&#25324;&#26368;&#36817;&#21457;&#29616;&#30340;&#24191;&#20041;&#21270;&#19981;&#21516;&#31561;&#24335;&#65288;GDE&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#20165;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#20272;&#35745;&#27867;&#21270;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#21487;&#33021;&#21463;&#30410;&#20110;&#25506;&#32034;&#20256;&#32479;IID&#65288;&#29420;&#31435;&#21516;&#20998;&#24067;&#65289;&#20551;&#35774;&#20043;&#22806;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning features from data is one of the defining characteristics of deep learning, but our theoretical understanding of the role features play in deep learning is still rudimentary. To address this gap, we introduce a new tool, the interaction tensor, for empirically analyzing the interaction between data and model through features. With the interaction tensor, we make several key observations about how features are distributed in data and how models with different random seeds learn different features. Based on these observations, we propose a conceptual framework for feature learning. Under this framework, the expected accuracy for a single hypothesis and agreement for a pair of hypotheses can both be derived in closed-form. We demonstrate that the proposed framework can explain empirically observed phenomena, including the recently discovered Generalization Disagreement Equality (GDE) that allows for estimating the generalization error with only unlabeled data. Further, our theory
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26080;&#30417;&#30563;&#22810;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#30340;Transformer-based&#26041;&#27861;Absformer&#65292;&#23427;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#32858;&#31867;&#25991;&#26723;&#24182;&#29983;&#25104;&#25277;&#35937;&#25688;&#35201;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23427;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#25277;&#35937;MDS&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04787</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#26080;&#30417;&#30563;&#22810;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Absformer: Transformer-based Model for Unsupervised Multi-Document Abstractive Summarization. (arXiv:2306.04787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26080;&#30417;&#30563;&#22810;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#30340;Transformer-based&#26041;&#27861;Absformer&#65292;&#23427;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#32858;&#31867;&#25991;&#26723;&#24182;&#29983;&#25104;&#25277;&#35937;&#25688;&#35201;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23427;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#25277;&#35937;MDS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25991;&#26723;&#25688;&#35201;&#65288;MDS&#65289;&#26159;&#23558;&#22810;&#20010;&#25991;&#26723;&#20013;&#30340;&#25991;&#26412;&#24635;&#32467;&#25104;&#31616;&#27905;&#27010;&#25324;&#30340;&#20219;&#21153;&#12290;&#25152;&#29983;&#25104;&#30340;&#25688;&#35201;&#36890;&#36807;&#29992;&#23569;&#25968;&#20960;&#21477;&#35805;&#25552;&#20379;&#37325;&#35201;&#20869;&#23481;&#65292;&#21487;&#20197;&#30465;&#21435;&#38405;&#35835;&#22810;&#20010;&#25991;&#26723;&#30340;&#26102;&#38388;&#12290;&#25277;&#35937;MDS&#26088;&#22312;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#20026;&#22810;&#20010;&#25991;&#26723;&#29983;&#25104;&#36830;&#36143;&#12289;&#27969;&#30021;&#30340;&#25688;&#35201;&#12290;&#26412;&#25991;&#32771;&#34385;&#20165;&#26377;&#25991;&#26723;&#32780;&#27809;&#26377;&#25688;&#35201;&#30340;&#26080;&#30417;&#30563;&#25277;&#35937;MDS&#29615;&#22659;&#65292;&#24182;&#25552;&#20986;Absformer&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#30340;&#26032;&#22411;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#31532;&#19968;&#27493;&#65292;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#20316;&#20026;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#35757;&#32451;Transformer&#32534;&#30721;&#22120;&#65292;&#23558;&#25991;&#26723;&#32858;&#31867;&#20026;&#35821;&#20041;&#30456;&#20284;&#30340;&#32452;&#65307;&#31532;&#20108;&#27493;&#65292;&#35757;&#32451;&#19968;&#20010;Transformer&#35299;&#30721;&#22120;&#65292;&#20026;&#25991;&#26723;&#38598;&#32676;&#29983;&#25104;&#25277;&#35937;&#25688;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20221;&#25552;&#20986;&#23558;&#32858;&#31867;&#29992;&#20110;&#25991;&#26723;&#32452;&#21512;&#30340;&#26080;&#30417;&#30563;&#25277;&#35937;MDS&#26041;&#27861;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#25277;&#35937;MDS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-document summarization (MDS) refers to the task of summarizing the text in multiple documents into a concise summary. The generated summary can save the time of reading many documents by providing the important content in the form of a few sentences. Abstractive MDS aims to generate a coherent and fluent summary for multiple documents using natural language generation techniques. In this paper, we consider the unsupervised abstractive MDS setting where there are only documents with no groundtruh summaries provided, and we propose Absformer, a new Transformer-based method for unsupervised abstractive summary generation. Our method consists of a first step where we pretrain a Transformer-based encoder using the masked language modeling (MLM) objective as the pretraining task in order to cluster the documents into semantically similar groups; and a second step where we train a Transformer-based decoder to generate abstractive summaries for the clusters of documents. To our knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#25968;&#25454;&#28857;&#20013;&#26631;&#35782;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#22411;&#21644;&#38376;&#30697;&#38453;&#26469;&#39044;&#27979;&#21487;&#35299;&#37322;&#30340;&#23454;&#20363;&#21644;&#32858;&#31867;&#32423;&#21035;&#30340;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04785</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpretable Deep Clustering. (arXiv:2306.04785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#25968;&#25454;&#28857;&#20013;&#26631;&#35782;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#22411;&#21644;&#38376;&#30697;&#38453;&#26469;&#39044;&#27979;&#21487;&#35299;&#37322;&#30340;&#23454;&#20363;&#21644;&#32858;&#31867;&#32423;&#21035;&#30340;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#19968;&#39033;&#24191;&#27867;&#24212;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22522;&#30784;&#23398;&#20064;&#20219;&#21153;&#12290;&#20363;&#22914;&#65292;&#29983;&#29289;&#23398;&#23478;&#32463;&#24120;&#20351;&#29992;&#32858;&#31867;&#20998;&#37197;&#26469;&#20998;&#26512;&#22522;&#22240;&#32452;&#24207;&#21015;&#12289;&#21307;&#30103;&#35760;&#24405;&#25110;&#22270;&#20687;&#12290;&#30001;&#20110;&#19979;&#28216;&#20998;&#26512;&#36890;&#24120;&#22312;&#32858;&#31867;&#32423;&#21035;&#19978;&#25191;&#34892;&#65292;&#22240;&#27492;&#20174;&#19994;&#32773;&#23547;&#27714;&#21487;&#38752;&#19988;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#21487;&#35299;&#37322;&#30340;&#23454;&#20363;&#21644;&#32858;&#31867;&#32423;&#21035;&#30340;&#32858;&#31867;&#20998;&#37197;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#33258;&#25105;&#30417;&#30563;&#30340;&#36807;&#31243;&#26469;&#20174;&#27599;&#20010;&#25968;&#25454;&#28857;&#20013;&#26631;&#35782;&#20986;&#19968;&#32452;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32858;&#31867;&#20998;&#37197;&#21644;&#19968;&#20010;&#38376;&#30697;&#38453;&#65292;&#29992;&#20110;&#24341;&#23548;&#32858;&#31867;&#32423;&#21035;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#21487;&#38752;&#22320;&#39044;&#27979;&#32858;&#31867;&#20998;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#23454;&#20363;&#21644;&#32858;&#31867;&#32423;&#21035;&#19978;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering is a fundamental learning task widely used as a first step in data analysis. For example, biologists often use cluster assignments to analyze genome sequences, medical records, or images. Since downstream analysis is typically performed at the cluster level, practitioners seek reliable and interpretable clustering models. We propose a new deep-learning framework that predicts interpretable cluster assignments at the instance and cluster levels. First, we present a self-supervised procedure to identify a subset of informative features from each data point. Then, we design a model that predicts cluster assignments and a gate matrix that leads to cluster-level feature selection. We show that the proposed method can reliably predict cluster assignments using synthetic and real data. Furthermore, we verify that our model leads to interpretable results at a sample and cluster level.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#23884;&#22871;&#25511;&#21046;&#26550;&#26500;&#23558;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;&#19982;&#28237;&#27969;&#34917;&#20607;&#20998;&#31163;&#65292;&#20197;&#35299;&#20915;&#22312;&#28237;&#27969;&#39118;&#20917;&#19979;&#26080;&#20154;&#26426;&#32676;&#32452;&#21327;&#21516;&#36816;&#21160;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04781</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#32676;&#20307;&#23398;&#20064;&#21327;&#21516;&#23548;&#33322;&#22312;&#28237;&#27969;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Navigate in Turbulent Flows with Aerial Robot Swarms: A Cooperative Deep Reinforcement Learning Approach. (arXiv:2306.04781v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#23884;&#22871;&#25511;&#21046;&#26550;&#26500;&#23558;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;&#19982;&#28237;&#27969;&#34917;&#20607;&#20998;&#31163;&#65292;&#20197;&#35299;&#20915;&#22312;&#28237;&#27969;&#39118;&#20917;&#19979;&#26080;&#20154;&#26426;&#32676;&#32452;&#21327;&#21516;&#36816;&#21160;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28237;&#27969;&#29615;&#22659;&#20013;&#30340;&#31354;&#20013;&#25805;&#20316;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#27668;&#27969;&#30340;&#28151;&#27788;&#34892;&#20026;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#29305;&#21035;&#26159;&#24403;&#19968;&#32452;&#26080;&#20154;&#26426;&#35797;&#22270;&#22312;&#28237;&#27969;&#39118;&#20917;&#19979;&#23454;&#29616;&#21327;&#35843;&#36816;&#21160;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#26469;&#22312;&#28237;&#27969;&#20013;&#23548;&#33322;&#65292;&#36890;&#36807;&#23884;&#22871;&#25511;&#21046;&#26550;&#26500;&#23558;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;&#19982;&#28237;&#27969;&#34917;&#20607;&#20998;&#31163;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20250;&#22312;&#29305;&#23450;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#23398;&#20064;&#34917;&#20607;&#31354;&#27668;&#27969;&#21160;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#28237;&#27969;&#23545;&#22242;&#38431;&#30340;&#24433;&#21709;&#26469;&#23398;&#20064;&#34917;&#20607;&#31354;&#27668;&#27969;&#21160;&#65292;&#22312;&#23454;&#29616;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;&#22522;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCNN&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22788;&#29702;&#22242;&#38431;&#20013;&#39118;&#27969;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#39118;&#21147;&#34917;&#20607;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24456;&#22909;&#22320;&#36866;&#29992;&#20110;&#22823;&#22411;&#26426;&#22120;&#20154;&#22242;&#38431;&#65292;&#22240;&#20026;&#27599;&#20010;&#26426;&#22120;&#20154;&#21482;&#20351;&#29992;&#23616;&#37096;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aerial operation in turbulent environments is a challenging problem due to the chaotic behavior of the flow. This problem is made even more complex when a team of aerial robots is trying to achieve coordinated motion in turbulent wind conditions. In this paper, we present a novel multi-robot controller to navigate in turbulent flows, decoupling the trajectory-tracking control from the turbulence compensation via a nested control architecture. Unlike previous works, our method does not learn to compensate for the air-flow at a specific time and space. Instead, our method learns to compensate for the flow based on its effect on the team. This is made possible via a deep reinforcement learning approach, implemented via a Graph Convolutional Neural Network (GCNN)-based architecture, which enables robots to achieve better wind compensation by processing the spatial-temporal correlation of wind flows across the team. Our approach scales well to large robot teams -- as each robot only uses in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34892;&#20026;&#21338;&#24328;&#35770;&#23478;&#22312;&#25439;&#22833;&#20989;&#25968;&#36873;&#25321;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;&#28385;&#36275;&#29305;&#23450;&#20844;&#29702;&#30340;&#25439;&#22833;&#20989;&#25968;&#38598;&#21512;&#65292;&#20854;&#20013;&#24179;&#26041;L2&#35823;&#24046;&#26159;&#23454;&#36341;&#20013;&#21807;&#19968;&#21487;&#25509;&#21463;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#24314;&#35758;&#34892;&#20026;&#21338;&#24328;&#35770;&#23478;&#32487;&#32493;&#20351;&#29992;&#23427;&#12290;</title><link>http://arxiv.org/abs/2306.04778</link><description>&lt;p&gt;
&#34892;&#20026;&#21338;&#24328;&#35770;&#30340;&#25439;&#22833;&#20989;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Loss Functions for Behavioral Game Theory. (arXiv:2306.04778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34892;&#20026;&#21338;&#24328;&#35770;&#23478;&#22312;&#25439;&#22833;&#20989;&#25968;&#36873;&#25321;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;&#28385;&#36275;&#29305;&#23450;&#20844;&#29702;&#30340;&#25439;&#22833;&#20989;&#25968;&#38598;&#21512;&#65292;&#20854;&#20013;&#24179;&#26041;L2&#35823;&#24046;&#26159;&#23454;&#36341;&#20013;&#21807;&#19968;&#21487;&#25509;&#21463;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#24314;&#35758;&#34892;&#20026;&#21338;&#24328;&#35770;&#23478;&#32487;&#32493;&#20351;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#21338;&#24328;&#35770;&#23478;&#20204;&#20351;&#29992;&#23454;&#39564;&#25968;&#25454;&#35780;&#20272;&#20154;&#31867;&#34892;&#20026;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#20294;&#26159;&#20182;&#20204;&#22312;&#25439;&#22833;&#20989;&#25968;&#30340;&#36873;&#25321;&#19978;&#23384;&#22312;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#38169;&#35823;&#29575;&#12289;&#36127;&#23545;&#25968;&#20284;&#28982;&#12289;&#20132;&#21449;&#29109;&#12289;Brier&#24471;&#20998;&#21644;L2&#35823;&#24046;&#37117;&#26159;&#24120;&#35265;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#35797;&#22270;&#25552;&#20379;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#31572;&#26696;&#65292;&#35299;&#20915;&#21738;&#20123;&#25439;&#22833;&#20989;&#25968;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#35268;&#33539;&#21270;&#20102;&#35748;&#20026;&#25439;&#22833;&#20989;&#25968;&#24212;&#35813;&#28385;&#36275;&#30340;&#20934;&#21017;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#32452;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#8220;&#23545;&#35282;&#32447;&#26377;&#30028;Bregman&#25955;&#24230;&#8221;&#65292;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#20844;&#29702;&#65292;&#24182;&#21253;&#25324;&#24179;&#26041;L2&#35823;&#24046;&#12290;&#23454;&#38469;&#19978;&#65292;&#24179;&#26041;L2&#35823;&#24046;&#26159;&#30456;&#23545;&#24120;&#29992;&#30340;&#21807;&#19968;&#21487;&#25509;&#21463;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#34892;&#20026;&#21338;&#24328;&#35770;&#23478;&#32487;&#32493;&#20351;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioral game theorists all use experimental data to evaluate predictive models of human behavior. However, they differ greatly in their choice of loss function for these evaluations, with error rate, negative log-likelihood, cross-entropy, Brier score, and L2 error all being common choices. We attempt to offer a principled answer to the question of which loss functions make sense for this task, formalizing desiderata that we argue loss functions should satisfy. We construct a family of loss functions, which we dub "diagonal bounded Bregman divergences", that satisfy all of these axioms and includes the squared L2 error. In fact, the squared L2 error is the only acceptable loss that is relatively commonly used in practice; we thus recommend its continued use to behavioral game theorists.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#21464;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;&#30340;&#31639;&#27861;&#65292;&#23427;&#36991;&#20813;&#20102;&#20135;&#29983;&#34394;&#20551;&#20851;&#32852;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.04777</link><description>&lt;p&gt;
&#19981;&#21464;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;
&lt;/p&gt;
&lt;p&gt;
Invariant Causal Set Covering Machines. (arXiv:2306.04777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#21464;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;&#30340;&#31639;&#27861;&#65292;&#23427;&#36991;&#20813;&#20102;&#20135;&#29983;&#34394;&#20551;&#20851;&#32852;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65292;&#22914;&#20915;&#31574;&#26641;&#65292;&#22240;&#20854;&#21487;&#35299;&#37322;&#30340;&#29305;&#24615;&#21463;&#21040;&#20174;&#19994;&#32773;&#30340;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#36825;&#31181;&#27169;&#22411;&#30340;&#23398;&#20064;&#31639;&#27861;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#34394;&#20551;&#20851;&#32852;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#19981;&#33021;&#20445;&#35777;&#25552;&#21462;&#30340;&#26159;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#27934;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#19981;&#21464;&#22240;&#26524;&#39044;&#27979;&#25991;&#29486;&#20013;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19981;&#21464;&#30340;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;&#65292;&#36825;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#38598;&#35206;&#30422;&#26426;&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#20108;&#20540;&#35268;&#21017;&#30340;&#21512;&#21462;/&#26512;&#21462;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#36991;&#20813;&#20102;&#34394;&#20551;&#20851;&#32852;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#36341;&#19978;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based models, such as decision trees, appeal to practitioners due to their interpretable nature. However, the learning algorithms that produce such models are often vulnerable to spurious associations and thus, they are not guaranteed to extract causally-relevant insights. In this work, we build on ideas from the invariant causal prediction literature to propose Invariant Causal Set Covering Machines, an extension of the classical Set Covering Machine algorithm for conjunctions/disjunctions of binary-valued rules that provably avoids spurious associations. We demonstrate both theoretically and empirically that our method can identify the causal parents of a variable of interest in polynomial time.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35266;&#27979;&#20559;&#24046;&#26469;&#25913;&#36827;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#23545;&#26410;&#35266;&#27979;&#21327;&#21464;&#37327;&#30340;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04775</link><description>&lt;p&gt;
&#21033;&#29992;&#35266;&#27979;&#20559;&#24046;&#25552;&#39640;&#30697;&#38453;&#34917;&#20840;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting Observation Bias to Improve Matrix Completion. (arXiv:2306.04775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35266;&#27979;&#20559;&#24046;&#26469;&#25913;&#36827;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#23545;&#26410;&#35266;&#27979;&#21327;&#21464;&#37327;&#30340;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#21464;&#24418;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20837;&#25968;&#25454;&#20197;&#20559;&#24046;&#30340;&#26041;&#24335;&#21576;&#29616;&#65292;&#31867;&#20284;&#20110;Ma&#21644;Chen&#25152;&#24341;&#20837;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#20559;&#24046;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#26469;&#25913;&#36827;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65306;&#65288;i&#65289;&#23558;&#35266;&#27979;&#27169;&#24335;&#35299;&#37322;&#20026;&#23436;&#20840;&#35266;&#27979;&#30340;&#22122;&#22768;&#30697;&#38453;&#65292;&#25105;&#20204;&#23545;&#35266;&#27979;&#27169;&#24335;&#24212;&#29992;&#20256;&#32479;&#30340;&#30697;&#38453;&#34917;&#20840;&#26041;&#27861;&#26469;&#20272;&#35745;&#28508;&#22312;&#22240;&#32032;&#20043;&#38388;&#30340;&#36317;&#31163;&#65307; (ii)&#25105;&#20204;&#23545;&#24674;&#22797;&#30340;&#29305;&#24449;&#24212;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#22635;&#34917;&#32570;&#22833;&#35266;&#23519;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#29575;&#65292;&#36825;&#20123;&#35823;&#24046;&#29575;&#19982;&#30456;&#24212;&#30340;&#30417;&#30563;&#23398;&#20064;&#21442;&#25968;&#29575;&#30456;&#31454;&#20105;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#23398;&#20064;&#24615;&#33021;&#19982;&#20351;&#29992;&#26410;&#35266;&#27979;&#21327;&#21464;&#37327;&#30456;&#24403;&#12290;&#23454;&#35777;&#35780;&#20272;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21453;&#26144;&#20102;&#31867;&#20284;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a variant of matrix completion where entries are revealed in a biased manner, adopting a model akin to that introduced by Ma and Chen. Instead of treating this observation bias as a disadvantage, as is typically the case, our goal is to exploit the shared information between the bias and the outcome of interest to improve predictions. Towards this, we propose a simple two-stage algorithm: (i) interpreting the observation pattern as a fully observed noisy matrix, we apply traditional matrix completion methods to the observation pattern to estimate the distances between the latent factors; (ii) we apply supervised learning on the recovered features to impute missing observations. We establish finite-sample error rates that are competitive with the corresponding supervised learning parametric rates, suggesting that our learning performance is comparable to having access to the unobserved covariates. Empirical evaluation using a real-world dataset reflects similar performance g
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; PLATO &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25551;&#36848;&#36755;&#20837;&#29305;&#24449;&#30340;&#36741;&#21161; KG &#26469;&#35268;&#33539; MLP&#65292;&#22312; $d \gg n$ &#30340;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04766</link><description>&lt;p&gt;
&#29992;&#36741;&#21161;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#22312; $d \gg n$ &#24773;&#20917;&#19979;&#30340;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enabling tabular deep learning when $d \gg n$ with an auxiliary knowledge graph. (arXiv:2306.04766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; PLATO &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25551;&#36848;&#36755;&#20837;&#29305;&#24449;&#30340;&#36741;&#21161; KG &#26469;&#35268;&#33539; MLP&#65292;&#22312; $d \gg n$ &#30340;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20855;&#26377;&#20016;&#23500;&#26631;&#35760;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#38750;&#24120;&#39640;&#32500;&#29305;&#24449;&#20294;&#26679;&#26412;&#25968;&#26377;&#38480;&#65288;&#21363; $d \gg n$ &#30340;&#34920;&#26684;&#25968;&#25454;&#65289;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24456;&#38590;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#20316;&#32773;&#30340;&#20027;&#35201;&#27934;&#35265;&#22312;&#20110;&#36755;&#20837;&#29305;&#24449;&#36890;&#24120;&#20855;&#26377;&#20016;&#23500;&#30340;&#36741;&#21161;&#39046;&#22495;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#34987;&#32452;&#32455;&#25104;&#24322;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102; PLATO &#26041;&#27861;&#65292;&#20351;&#29992;&#25551;&#36848;&#36755;&#20837;&#29305;&#24449;&#30340;&#36741;&#21161; KG &#26469;&#35268;&#33539;&#19968;&#20010; MLP&#65292;&#22312; $d \gg n$ &#30340;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models exhibit strong performance on datasets with abundant labeled samples. However, for tabular datasets with extremely high $d$-dimensional features but limited $n$ samples (i.e. $d \gg n$), machine learning models struggle to achieve strong performance due to the risk of overfitting. Here, our key insight is that there is often abundant, auxiliary domain information describing input features which can be structured as a heterogeneous knowledge graph (KG). We propose PLATO, a method that achieves strong performance on tabular data with $d \gg n$ by using an auxiliary KG describing input features to regularize a multilayer perceptron (MLP). In PLATO, each input feature corresponds to a node in the auxiliary KG. In the MLP's first layer, each input feature also corresponds to a weight vector. PLATO is based on the inductive bias that two input features corresponding to similar nodes in the auxiliary KG should have similar weight vectors in the MLP's first layer. PLATO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#20999;&#29255;&#22270;&#20687;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04763</link><description>&lt;p&gt;
&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Self-Supervised Learning of Whole Slide Images. (arXiv:2306.04763v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#20999;&#29255;&#22270;&#20687;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25226;&#20840;&#20999;&#29255;&#22270;&#20687; (WSIs) &#21576;&#29616;&#20026;&#22270;&#24418;&#65292;&#23558;&#20026;&#30284;&#30151;&#35786;&#26029;&#25552;&#20379;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290; &#30001;&#20110;&#21333;&#20010;WSI&#30001;&#25968;&#21313;&#20159;&#20687;&#32032;&#32452;&#25104;&#65292;&#32780;&#35745;&#31639;&#30149;&#29702;&#23398;&#25152;&#38656;&#30340;&#24191;&#27867;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19981;&#36275;&#65292;&#22240;&#27492;&#20351;&#29992;&#20856;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN)&#65292;&#20174;WSIs&#20013;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;WSIs&#19979;&#37319;&#26679;&#21487;&#33021;&#23548;&#33268;&#20002;&#22833;&#23545;&#30284;&#30151;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#25968;&#25454;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#25216;&#26415;&#12290; &#37492;&#20110;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#30244;&#21608;&#22260;&#30340;&#25299;&#25169;&#29305;&#24449;&#65289;&#21487;&#33021;&#25345;&#26377;&#30284;&#30151;&#20998;&#32423;&#21644;&#35786;&#26029;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#22240;&#27492;&#25429;&#25417;WSI&#20013;&#25152;&#26377;&#21306;&#22495;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#30340;&#22270;&#24418;&#34920;&#31034;&#38750;&#24120;&#30452;&#35266;&#12290;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#20174;&#32959;&#30244;&#21644;&#30456;&#37051;&#32452;&#32455;&#20013;&#25429;&#25417;&#19978;&#19979;&#25991;&#65292;&#24182;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#22686;&#24378;&#35757;&#32451;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#35813;&#26041;&#27861;&#26159;&#20351;&#29992;GCN&#30340;WSIs&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21487;&#23454;&#29616;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#30284;&#30151;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Presenting whole slide images (WSIs) as graph will enable a more efficient and accurate learning framework for cancer diagnosis. Due to the fact that a single WSI consists of billions of pixels and there is a lack of vast annotated datasets required for computational pathology, the problem of learning from WSIs using typical deep learning approaches such as convolutional neural network (CNN) is challenging. Additionally, WSIs down-sampling may lead to the loss of data that is essential for cancer detection. A novel two-stage learning technique is presented in this work. Since context, such as topological features in the tumor surroundings, may hold important information for cancer grading and diagnosis, a graph representation capturing all dependencies among regions in the WSI is very intuitive. Graph convolutional network (GCN) is deployed to include context from the tumor and adjacent tissues, and self-supervised learning is used to enhance training through unlabeled data. More speci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37492;&#21035;&#20986;&#19977;&#31181;&#19981;&#21516;&#30340;&#24085;&#37329;&#26862;&#30149;&#20122;&#22411;&#65292;&#24182;&#29983;&#25104;&#24739;&#32773;&#20010;&#24615;&#21270;&#30340;&#30151;&#29366;&#36827;&#23637;&#39044;&#27979;&#65292;&#26377;&#21161;&#20110;&#21046;&#23450;&#38024;&#23545;&#24615;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;</title><link>http://arxiv.org/abs/2306.04748</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Analysis, Identification and Prediction of Parkinson's disease sub-types and progression through Machine Learning. (arXiv:2306.04748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37492;&#21035;&#20986;&#19977;&#31181;&#19981;&#21516;&#30340;&#24085;&#37329;&#26862;&#30149;&#20122;&#22411;&#65292;&#24182;&#29983;&#25104;&#24739;&#32773;&#20010;&#24615;&#21270;&#30340;&#30151;&#29366;&#36827;&#23637;&#39044;&#27979;&#65292;&#26377;&#21161;&#20110;&#21046;&#23450;&#38024;&#23545;&#24615;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#24739;&#32773;&#34920;&#29616;&#22810;&#31181;&#22810;&#26679;&#65292;&#20294;&#23545;&#20854;&#28508;&#22312;&#21407;&#22240;&#21644;&#30151;&#29366;&#36827;&#23637;&#30340;&#20102;&#35299;&#36824;&#24456;&#26377;&#38480;&#12290;&#24085;&#37329;&#26862;&#36827;&#23637;&#26631;&#24535;&#29289;&#35745;&#21010;&#65288;PPMI&#65289;&#25910;&#38598;&#20102;&#22810;&#20010;&#24739;&#32773;&#32676;&#30340;&#35814;&#32454;&#32437;&#21521;&#25968;&#25454;&#20197;&#30830;&#23450;&#29983;&#29289;&#26631;&#24535;&#29289;&#24182;&#36741;&#21161;&#24178;&#39044;&#26041;&#27861;&#30340;&#21046;&#23450;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#36229;&#36807;110&#20010;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20351;&#29992;PPMI&#25968;&#25454;&#24211;&#65292;&#20294;&#22823;&#37096;&#20998;&#30740;&#31350;&#20165;&#32858;&#28966;&#20110;&#30417;&#30563;&#27169;&#22411;&#29992;&#20110;&#35786;&#26029;&#39044;&#27979;&#65292;&#38480;&#21046;&#20102;&#23545;&#24739;&#32773;&#22810;&#26679;&#24615;&#21644;&#36827;&#23637;&#30340;&#35748;&#35782;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#37492;&#21035;&#20122;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24085;&#37329;&#26862;&#24739;&#32773;&#30142;&#30149;&#36827;&#31243;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;PPMI&#25968;&#25454;&#24211;&#20013;5&#24180;&#30340;&#32437;&#21521;&#25968;&#25454;&#24182;&#22312;&#20043;&#21069;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#25972;&#21512;&#26080;&#30417;&#30563;&#30340;&#30149;&#20154;&#32858;&#31867;&#21644;&#30149;&#20154;&#24403;&#21069;&#21644;&#26410;&#26469;&#30151;&#29366;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37492;&#21035;&#20986;&#19977;&#20010;&#19981;&#21516;&#30340;&#24085;&#37329;&#26862;&#30149;&#20122;&#22411;&#65292;&#24182;&#20026;&#20010;&#20307;&#24739;&#32773;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#30151;&#29366;&#36827;&#23637;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#39044;&#27979;&#30142;&#30149;&#36827;&#23637;&#30340;&#20934;&#30830;&#24615;&#65292;&#24110;&#21161;&#21046;&#23450;&#20010;&#24615;&#21270;&#24178;&#39044;&#26041;&#27861;&#24182;&#25913;&#21892;&#24739;&#32773;&#30340;&#39044;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) is a prevalent neurodegenerative disorder with varying patient trajectories, yet little is understood about the underlying causes and symptom progression. The Parkinson's Progression Markers Initiative (PPMI) has collected comprehensive longitudinal data from diverse patient cohorts to identify biomarkers and aid in the development of interventions. Despite over 110 machine learning studies using the PPMI database, the majority have focused on supervised models for diagnosis prediction, which has limited impact on understanding patient variability and progression. This paper addresses this gap by combining supervised and unsupervised machine learning methods to identify subtypes that accurately predict disease progression in Parkinson's patients. Building upon previous work, we replicate and extend the study by integrating unsupervised patient clustering and prediction of present and future symptoms using 5 additional years of longitudinal data from the Progres
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#38477;&#20302;&#26631;&#31614;&#33719;&#21462;&#30340;&#30740;&#31350;&#25104;&#26412;80&#65285;&#65292;&#21516;&#26102;&#20445;&#35777;CSS&#30740;&#31350;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04746</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#36827;&#34892;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#26377;&#25928;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;: &#22522;&#20110;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning. (arXiv:2306.04746v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04746
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#38477;&#20302;&#26631;&#31614;&#33719;&#21462;&#30340;&#30740;&#31350;&#25104;&#26412;80&#65285;&#65292;&#21516;&#26102;&#20445;&#35777;CSS&#30740;&#31350;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#65288;CSS&#65289;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#25991;&#26723;&#26469;&#35299;&#37322;&#31038;&#20250;&#21644;&#25919;&#27835;&#29616;&#35937;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;CSS&#30740;&#31350;&#20154;&#21592;&#39318;&#20808;&#33719;&#21462;&#25991;&#26723;&#30340;&#26631;&#31614;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22238;&#24402;&#20998;&#26512;&#26469;&#35299;&#37322;&#26631;&#31614;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#21487;&#20197;&#36890;&#36807;&#22312;&#35268;&#27169;&#19978;&#20415;&#23452;&#22320;&#27880;&#37322;&#25991;&#26723;&#26469;&#38477;&#20302;CSS&#30740;&#31350;&#25104;&#26412;&#65292;&#20294;&#36825;&#20123;&#26367;&#20195;&#26631;&#31614;&#36890;&#24120;&#26159;&#19981;&#23436;&#32654;&#21644;&#26377;&#20559;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;LLMs&#30340;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#21516;&#26102;&#20445;&#35777;&#19982;CSS&#30740;&#31350;&#22522;&#26412;&#30456;&#20851;&#30340;&#32479;&#35745;&#23646;&#24615;-&#22914;&#28176;&#36817;&#26080;&#20559;&#24615;&#21644;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30452;&#25509;&#22312;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#20013;&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#26367;&#20195;&#26631;&#31614;&#20250;&#23548;&#33268;&#23454;&#36136;&#24615;&#20559;&#24046;&#21644;&#26080;&#25928;&#32622;&#20449;&#21306;&#38388;&#65292;&#21363;&#20351;&#26367;&#20195;&#20934;&#30830;&#24615;&#39640;&#36798;80-90&#65285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#22522;&#20110;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;D-SSL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;LLM&#27880;&#37322;&#19982;&#26377;&#38024;&#23545;&#24615;&#30340;&#37319;&#26679;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#26631;&#31614;&#33719;&#21462;&#30340;CSS&#30740;&#31350;&#25104;&#26412;&#38477;&#20302;80&#65285;&#65292;&#32780;&#19981;&#24433;&#21709;&#32479;&#35745;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#34920;&#26126;&#65292;&#19982;&#30452;&#25509;&#20351;&#29992;LLM&#39044;&#27979;&#26631;&#31614;&#30456;&#27604;&#65292;D-SSL&#21487;&#20197;&#23558;&#22238;&#24402;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#22810;&#36798;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#26816;&#32034;&#19981;&#21516;&#26102;&#38388;&#25195;&#25551;&#30340;&#30456;&#20284;&#36229;&#22768;&#32908;&#32905;&#35270;&#22270;&#12290;&#20351;&#29992;&#20174;67&#21517;ICU&#24739;&#32773;&#33719;&#21462;&#30340;&#25968;&#25454;&#23545;&#27604;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35270;&#22270;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.04739</link><description>&lt;p&gt;
&#38271;&#26399;&#26816;&#26597;&#20013;&#20381;&#25454;&#26102;&#38388;&#26816;&#32034;&#30456;&#24212;&#30340;&#36229;&#22768;&#22270;&#20687;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic retrieval of corresponding US views in longitudinal examinations. (arXiv:2306.04739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#26816;&#32034;&#19981;&#21516;&#26102;&#38388;&#25195;&#25551;&#30340;&#30456;&#20284;&#36229;&#22768;&#32908;&#32905;&#35270;&#22270;&#12290;&#20351;&#29992;&#20174;67&#21517;ICU&#24739;&#32773;&#33719;&#21462;&#30340;&#25968;&#25454;&#23545;&#27604;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35270;&#22270;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;(ICU)&#38271;&#26399;&#21351;&#24202;&#30340;&#21361;&#37325;&#30149;&#24739;&#32773;&#20013;&#65292;&#39592;&#39612;&#32908;&#33806;&#32553;&#26159;&#24120;&#35265;&#30340;&#24773;&#20917;&#12290;&#24517;&#39035;&#36890;&#36807;&#29289;&#29702;&#27835;&#30103;&#24674;&#22797;&#32908;&#32905;&#36136;&#37327;&#65292;&#28982;&#21518;&#25165;&#33021;&#20986;&#38498;&#12290;&#36229;&#22768;&#25104;&#20687;&#36890;&#24120;&#29992;&#20110;&#27979;&#37327;&#32908;&#32905;&#23610;&#23544;&#30340;&#21464;&#21270;&#65292;&#20197;&#35780;&#20272;&#24674;&#22797;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25195;&#25551;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#26085;&#26399;&#36827;&#34892;&#65292;&#32780;&#19988;&#21487;&#33021;&#30001;&#19981;&#21516;&#30340;&#25805;&#20316;&#21592;&#36827;&#34892;&#65292;&#22240;&#27492;&#36825;&#20123;&#25163;&#21160;&#27979;&#37327;&#23481;&#26131;&#20135;&#29983;&#36739;&#22823;&#30340;&#21464;&#24322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#26816;&#32034;&#19981;&#21516;&#26102;&#38388;&#25195;&#25551;&#30340;&#30456;&#20284;&#36229;&#22768;&#32908;&#32905;&#35270;&#22270;&#12290;&#20351;&#29992;&#20174;67&#21517;ICU&#24739;&#32773;&#33719;&#21462;&#30340;&#25968;&#25454;&#23545;&#27604;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23545;&#27604;&#27169;&#22411;&#22312;&#35270;&#22270;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#30417;&#30563;&#22522;&#20934;&#27169;&#22411;&#65292;&#20855;&#26377;73.52%&#30340;AUC&#20540;&#65292;&#24403;&#19982;&#33258;&#21160;&#20998;&#21106;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#20854;&#27178;&#25130;&#38754;&#31215;&#35823;&#24046;&#20026;5.7%+/-0.24%&#12290;
&lt;/p&gt;
&lt;p&gt;
Skeletal muscle atrophy is a common occurrence in critically ill patients in the intensive care unit (ICU) who spend long periods in bed. Muscle mass must be recovered through physiotherapy before patient discharge and ultrasound imaging is frequently used to assess the recovery process by measuring the muscle size over time. However, these manual measurements are subject to large variability, particularly since the scans are typically acquired on different days and potentially by different operators. In this paper, we propose a self-supervised contrastive learning approach to automatically retrieve similar ultrasound muscle views at different scan times. Three different models were compared using data from 67 patients acquired in the ICU. Results indicate that our contrastive model outperformed a supervised baseline model in the task of view retrieval with an AUC of 73.52% and when combined with an automatic segmentation model achieved 5.7%+/-0.24% error in cross-sectional area. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#36890;&#36807;&#20998;&#32452;&#20844;&#24179;&#24615;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.04735</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Soft-prompt Tuning for Large Language Models to Evaluate Bias. (arXiv:2306.04735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#36890;&#36807;&#20998;&#32452;&#20844;&#24179;&#24615;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#21151;&#33021;&#22240;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#21363;&#21487;&#20135;&#29983;&#33391;&#22909;&#32467;&#26524;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#20197;&#33719;&#24471;&#24341;&#23548;&#26356;&#22909;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#20339;&#25552;&#31034;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Open Pre-trained Transformers&#65288;OPT&#65289;&#21644;Galactica&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#21487;&#33021;&#20559;&#21521;&#26576;&#20123;&#20154;&#32676;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#35782;&#21035;&#36825;&#20123;&#28508;&#22312;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#20351;&#29992;&#36719;&#25552;&#31034;&#26469;&#35780;&#20272;&#20559;&#24046;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#32452;&#20844;&#24179;&#24615;&#65288;&#20559;&#24046;&#65289;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#24182;&#25214;&#21040;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;&#30001;&#20110;LLMs&#24050;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#29992;&#20110;&#24037;&#19994;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#30340;&#20559;&#35265;&#35780;&#20272;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. However, this requires prompt tuning to get optimal prompts that lead to better model performances. In this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (LLMs) such as Open Pre-trained Transformers (OPT) and Galactica language model. Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. Using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. We check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. Since LLMs have been used in the industry in various applications, i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#33258;&#28982;&#38408;&#20540;&#31639;&#27861;&#65292;&#29992;&#20110;&#31232;&#30095;&#20449;&#21495;&#24674;&#22797;&#65292;&#24182;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#33258;&#28982;&#38408;&#20540;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04730</link><description>&lt;p&gt;
&#38543;&#26426;&#33258;&#28982;&#38408;&#20540;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Natural Thresholding Algorithms. (arXiv:2306.04730v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04730
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#33258;&#28982;&#38408;&#20540;&#31639;&#27861;&#65292;&#29992;&#20110;&#31232;&#30095;&#20449;&#21495;&#24674;&#22797;&#65292;&#24182;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#33258;&#28982;&#38408;&#20540;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#20449;&#21495;&#24674;&#22797;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#65292;&#21253;&#25324;&#21307;&#23398;&#25104;&#20687;&#21644;&#36965;&#24863;&#12290;&#35768;&#22810;&#22522;&#20110;&#30828;&#38408;&#20540;&#31639;&#23376;&#26063;&#30340;&#36138;&#23146;&#31639;&#27861;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#35299;&#20915;&#31232;&#30095;&#20449;&#21495;&#24674;&#22797;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#38408;&#20540;&#27861;&#65288;NT&#65289;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;NT&#20174;&#24102;&#32447;&#24615;&#27979;&#37327;&#30340;&#30830;&#23450;&#29256;&#26412;&#25193;&#23637;&#21040;&#20855;&#26377;&#19968;&#33324;&#30446;&#26631;&#20989;&#25968;&#30340;&#38543;&#26426;&#29256;&#26412;&#65292;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#38543;&#26426;&#33258;&#28982;&#38408;&#20540;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#21508;&#31181;&#25968;&#20540;&#23454;&#39564;&#65292;&#23545;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#27979;&#37327;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#20197;&#23637;&#31034;StoNT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse signal recovery is one of the most fundamental problems in various applications, including medical imaging and remote sensing. Many greedy algorithms based on the family of hard thresholding operators have been developed to solve the sparse signal recovery problem. More recently, Natural Thresholding (NT) has been proposed with improved computational efficiency. This paper proposes and discusses convergence guarantees for stochastic natural thresholding algorithms by extending the NT from the deterministic version with linear measurements to the stochastic version with a general objective function. We also conduct various numerical experiments on linear and nonlinear measurements to demonstrate the performance of StoNT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;</title><link>http://arxiv.org/abs/2306.04719</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#20320;&#30340;&#30524;&#30555;&#65306;&#20851;&#20110;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#65288;&#19981;&#65289;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#65311;&#29305;&#24449;&#21487;&#35270;&#21270;&#36890;&#36807;&#20248;&#21270;&#26469;&#21487;&#35270;&#21270;&#39640;&#28608;&#27963;&#30340;&#27169;&#24335;&#65292;&#35797;&#22270;&#22238;&#31572;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22914;&#20170;&#65292;&#21487;&#35270;&#21270;&#26041;&#27861;&#26500;&#25104;&#20102;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#30340;&#20102;&#35299;&#30340;&#22522;&#30784;&#65292;&#20316;&#20026;&#19968;&#31181;&#26426;&#26800;&#24335;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38382;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#26377;&#22810;&#21487;&#38752;&#65311;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#32593;&#32476;&#30005;&#36335;&#26469;&#35784;&#39575;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20351;&#20854;&#26174;&#31034;&#23436;&#20840;&#19982;&#33258;&#28982;&#36755;&#20837;&#30340;&#27491;&#24120;&#32593;&#32476;&#34892;&#20026;&#27627;&#26080;&#32852;&#31995;&#30340;&#20219;&#24847;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#22312;&#26631;&#20934;&#65292;&#26410;&#25805;&#32437;&#32593;&#32476;&#20013;&#21457;&#29983;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#19982;&#26631;&#20934;&#36755;&#20837;&#22788;&#29702;&#38750;&#24120;&#19981;&#21516;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#25903;&#25745;&#36825;&#19968;&#32463;&#39564;&#21457;&#29616;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#21487;&#35270;&#21270;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#26497;&#20854;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25511;&#21046;&#21464;&#37327;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#35299;&#22810;&#21464;&#37327;&#31526;&#21495;&#22238;&#24402;&#20026;&#21333;&#21464;&#37327;&#22238;&#24402;&#65292;&#24182;&#20174;&#24213;&#37096;&#21521;&#19978;&#32452;&#21512;&#65292;&#35299;&#20915;&#20102;&#31526;&#21495;&#22238;&#24402;&#23384;&#22312;&#30340;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04718</link><description>&lt;p&gt;
&#25511;&#21046;&#21464;&#37327;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Neural Symbolic Regression using Control Variables. (arXiv:2306.04718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04718
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25511;&#21046;&#21464;&#37327;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#35299;&#22810;&#21464;&#37327;&#31526;&#21495;&#22238;&#24402;&#20026;&#21333;&#21464;&#37327;&#22238;&#24402;&#65292;&#24182;&#20174;&#24213;&#37096;&#21521;&#19978;&#32452;&#21512;&#65292;&#35299;&#20915;&#20102;&#31526;&#21495;&#22238;&#24402;&#23384;&#22312;&#30340;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#20998;&#26512;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#24378;&#26377;&#21147;&#25216;&#26415;&#65292;&#30001;&#20110;&#20854;&#33391;&#22909;&#30340;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#28041;&#21450;&#22810;&#20010;&#21464;&#37327;&#30340;&#22797;&#26434;&#26041;&#31243;&#26102;&#38754;&#20020;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SRCV&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#21464;&#37327;&#26469;&#22686;&#24378;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#22810;&#21464;&#37327;&#31526;&#21495;&#22238;&#24402;&#20998;&#35299;&#20026;&#19968;&#32452;&#21333;&#21464;&#37327; SR &#38382;&#39064;&#65292;&#28982;&#21518;&#20174;&#24213;&#37096;&#21521;&#19978;&#32452;&#21512;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#22235;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20174;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#25968;&#25454;&#29983;&#25104;&#22120;&#36890;&#36807;&#25511;&#21046;&#36755;&#20837;&#21464;&#37327;&#26469;&#29983;&#25104;&#29305;&#23450;&#21464;&#37327;&#30340;&#26679;&#26412;&#12290;&#31532;&#19977;&#65292;&#24212;&#29992;&#21333;&#21464;&#37327;&#31526;&#21495;&#22238;&#24402;&#26469;&#20272;&#35745;&#30456;&#24212;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) is a powerful technique for discovering the analytical mathematical expression from data, finding various applications in natural sciences due to its good interpretability of results. However, existing methods face scalability issues when dealing with complex equations involving multiple variables. To address this challenge, we propose SRCV, a novel neural symbolic regression method that leverages control variables to enhance both accuracy and scalability. The core idea is to decompose multi-variable symbolic regression into a set of single-variable SR problems, which are then combined in a bottom-up manner. The proposed method involves a four-step process. First, we learn a data generator from observed data using deep neural networks (DNNs). Second, the data generator is used to generate samples for a certain variable by controlling the input variables. Thirdly, single-variable symbolic regression is applied to estimate the corresponding mathematical expressio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21487;&#24494;&#20998;&#30340;&#24555;&#36895;&#36924;&#36817;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39640;&#20142;LHC&#25968;&#25454;&#30340;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#25968;&#25454;&#20869;&#19982;&#31890;&#23376;&#25506;&#27979;&#22120;&#20013;&#30340;&#33021;&#37327;&#27785;&#31215;&#20998;&#24067;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.04712</link><description>&lt;p&gt;
&#21487;&#24494;&#30340;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#22312;&#39640;&#20142;LHC&#25968;&#25454;&#21387;&#32553;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentiable Earth Mover's Distance for Data Compression at the High-Luminosity LHC. (arXiv:2306.04712v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21487;&#24494;&#20998;&#30340;&#24555;&#36895;&#36924;&#36817;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39640;&#20142;LHC&#25968;&#25454;&#30340;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#25968;&#25454;&#20869;&#19982;&#31890;&#23376;&#25506;&#27979;&#22120;&#20013;&#30340;&#33021;&#37327;&#27785;&#31215;&#20998;&#24067;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;(EMD)&#26159;&#22270;&#20687;&#35782;&#21035;&#21644;&#20998;&#31867;&#30340;&#26377;&#29992;&#25351;&#26631;&#65292;&#20294;&#20854;&#36890;&#24120;&#23454;&#29616;&#19981;&#21487;&#24494;&#20998;&#25110;&#36807;&#20110;&#32531;&#24930;&#65292;&#26080;&#27861;&#29992;&#20316;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#20854;&#20182;&#31639;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#26412;&#25991;&#35757;&#32451;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#65292;&#23398;&#20064;&#20102;&#21487;&#24494;&#20998;&#30340;&#12289;&#24555;&#36895;&#30340;EMD&#30340;&#36924;&#36817;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#29992;&#20316;&#35745;&#31639;&#23494;&#38598;&#30340;EMD&#23454;&#29616;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#21487;&#24494;&#20998;&#30340;&#36924;&#36817;&#26041;&#27861;&#24212;&#29992;&#20110;&#29992;&#20110;&#25968;&#25454;&#21387;&#32553;&#30340;&#31867;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;(encoder NN)&#30340;&#35757;&#32451;&#65292;&#36825;&#20123;&#25968;&#25454;&#26469;&#33258;&#27431;&#27954;&#26680;&#23376;&#30740;&#31350;&#32452;&#32455;&#30340;&#39640;&#20142;LHC&#12290;&#32534;&#30721;&#22120;NN&#30340;&#30446;&#26631;&#26159;&#22312;&#20445;&#30041;&#19982;&#31890;&#23376;&#25506;&#27979;&#22120;&#20013;&#30340;&#33021;&#37327;&#27785;&#31215;&#20998;&#24067;&#30456;&#20851;&#30340;&#20449;&#24687;&#30340;&#21516;&#26102;&#21387;&#32553;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#21487;&#24494;&#30340;EMD CNN&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;NN&#30340;&#24615;&#33021;&#36229;&#36234;&#22522;&#20110;&#24179;&#22343;&#24179;&#26041;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Earth mover's distance (EMD) is a useful metric for image recognition and classification, but its usual implementations are not differentiable or too slow to be used as a loss function for training other algorithms via gradient descent. In this paper, we train a convolutional neural network (CNN) to learn a differentiable, fast approximation of the EMD and demonstrate that it can be used as a substitute for computing-intensive EMD implementations. We apply this differentiable approximation in the training of an autoencoder-inspired neural network (encoder NN) for data compression at the high-luminosity LHC at CERN. The goal of this encoder NN is to compress the data while preserving the information related to the distribution of energy deposits in particle detectors. We demonstrate that the performance of our encoder NN trained using the differentiable EMD CNN surpasses that of training with loss functions based on mean squared error.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23884;&#22871;&#25104;&#23545;&#24103;&#35780;&#20272;&#26041;&#27861;&#29992;&#20110;&#30456;&#23545;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#27604;&#36739;&#20505;&#36873;&#27169;&#22411;&#21644;&#30149;&#29702;&#23398;&#23478;&#27880;&#37322;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#30149;&#29702;&#23398;&#23478;&#27880;&#37322;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#25163;&#21160;&#30149;&#29702;&#23398;&#23478;&#27880;&#37322;&#36827;&#34892;&#27169;&#22411;&#39564;&#35777;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04709</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#20351;&#29992;&#25104;&#23545;&#24103;&#35780;&#20272;&#25913;&#36827;&#25968;&#23383;&#30149;&#29702;&#27169;&#22411;&#30340;&#32479;&#35745;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Improved statistical benchmarking of digital pathology models using pairwise frames evaluation. (arXiv:2306.04709v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04709
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23884;&#22871;&#25104;&#23545;&#24103;&#35780;&#20272;&#26041;&#27861;&#29992;&#20110;&#30456;&#23545;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#27604;&#36739;&#20505;&#36873;&#27169;&#22411;&#21644;&#30149;&#29702;&#23398;&#23478;&#27880;&#37322;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#30149;&#29702;&#23398;&#23478;&#27880;&#37322;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#25163;&#21160;&#30149;&#29702;&#23398;&#23478;&#27880;&#37322;&#36827;&#34892;&#27169;&#22411;&#39564;&#35777;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#22871;&#25104;&#23545;&#24103;&#26159;&#19968;&#31181;&#30456;&#23545;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#23558;&#32454;&#32990;&#25110;&#32452;&#32455;&#25968;&#23383;&#30149;&#29702;&#27169;&#22411;&#19982;&#25163;&#21160;&#30149;&#29702;&#23398;&#23478;&#27880;&#37322;&#30340;&#25277;&#26679;&#34917;&#19969;&#38598;&#36827;&#34892;&#27604;&#36739;&#12290;&#35813;&#26041;&#27861;&#27604;&#36739;&#20102;&#20505;&#36873;&#27169;&#22411;&#21644;&#30149;&#29702;&#23398;&#23478;&#27880;&#37322;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#30149;&#29702;&#23398;&#23478;&#27880;&#37322;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20351;&#29992;&#25163;&#21160;&#30149;&#29702;&#23398;&#23478;&#27880;&#37322;&#20316;&#20026;&#27169;&#22411;&#39564;&#35777;&#30340;&#22522;&#30784;&#25968;&#25454;&#30340;&#22823;&#23567;&#21644;&#27880;&#37322;&#32773;&#21464;&#24322;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#20026;&#32452;&#32455;&#20998;&#31867;&#12289;&#32454;&#32990;&#20998;&#31867;&#21644;&#32454;&#32990;&#35745;&#25968;&#39044;&#27979;&#20219;&#21153;&#23454;&#29616;&#20102;&#23884;&#22871;&#25104;&#23545;&#24103;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;H&#65286;E&#26579;&#33394;&#40657;&#32032;&#30244;&#25968;&#25454;&#38598;&#19978;&#37096;&#32626;&#30340;&#32454;&#32990;&#21644;&#32452;&#32455;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nested pairwise frames is a method for relative benchmarking of cell or tissue digital pathology models against manual pathologist annotations on a set of sampled patches. At a high level, the method compares agreement between a candidate model and pathologist annotations with agreement among pathologists' annotations. This evaluation framework addresses fundamental issues of data size and annotator variability in using manual pathologist annotations as a source of ground truth for model validation. We implemented nested pairwise frames evaluation for tissue classification, cell classification, and cell count prediction tasks and show results for cell and tissue models deployed on an H&amp;E-stained melanoma dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#38750;&#21018;&#24615;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;Robust-DefReg&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#31895;&#21040;&#32454;&#37197;&#20934;&#31574;&#30053;&#65292;&#24182;&#22312;&#35299;&#20915;&#19981;&#21516;&#38590;&#24230;&#38382;&#39064;&#19978;&#22343;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04701</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#21487;&#21464;&#24418;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;&#65306;Robust-DefReg
&lt;/p&gt;
&lt;p&gt;
Robust-DefReg: A Robust Deformable Point Cloud Registration Method based on Graph Convolutional Neural Networks. (arXiv:2306.04701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#38750;&#21018;&#24615;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;Robust-DefReg&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#31895;&#21040;&#32454;&#37197;&#20934;&#31574;&#30053;&#65292;&#24182;&#22312;&#35299;&#20915;&#19981;&#21516;&#38590;&#24230;&#38382;&#39064;&#19978;&#22343;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#37197;&#20934;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#26088;&#22312;&#20272;&#35745;&#30456;&#24212;&#28857;&#38598;&#20043;&#38388;&#30340;&#21464;&#25442;&#12290;&#23588;&#20854;&#26159;&#38750;&#21018;&#24615;&#37197;&#20934;&#38656;&#35201;&#35299;&#20915;&#22810;&#31181;&#21464;&#24418;&#12289;&#22122;&#22768;&#12289;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#31283;&#20581;&#38750;&#21018;&#24615;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;Robust-DefReg&#12290;Robust-DefReg&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31895;&#21040;&#32454;&#37197;&#20934;&#26041;&#27861;&#65292;&#21033;&#29992;&#31895;&#37197;&#20934;&#21644;&#32454;&#37197;&#20934;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#20840;&#23616;&#29305;&#24449;&#20197;&#25214;&#21040;&#28304;&#19982;&#30446;&#26631;&#28857;&#20113;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#36866;&#24403;&#30340;&#21021;&#22987;&#23545;&#20934;&#21644;&#38543;&#21518;&#30340;&#32454;&#37197;&#20934;&#12290;&#22312;&#25152;&#26377;&#25361;&#25112;&#20013;&#21516;&#26102;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#25253;&#36947;&#24471;&#26356;&#23569;&#65292;&#36825;&#25104;&#20026;Robust-DefReg&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#21464;&#24418;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point cloud registration is a fundamental problem in computer vision that aims to estimate the transformation between corresponding sets of points. Non-rigid registration, in particular, involves addressing challenges including various levels of deformation, noise, outliers, and data incompleteness. This paper introduces Robust-DefReg, a robust non-rigid point cloud registration method based on graph convolutional networks (GCNNs). Robust-DefReg is a coarse-to-fine registration approach within an end-to-end pipeline, leveraging the advantages of both coarse and fine methods. The method learns global features to find correspondences between source and target point clouds, to enable appropriate initial alignment, and subsequently fine registration. The simultaneous achievement of high accuracy and robustness across all challenges is reported less frequently in existing studies, making it a key objective of the Robust-DefReg method. The proposed method achieves high accuracy in large defo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;ConceptBed&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;CCD&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04695</link><description>&lt;p&gt;
ConceptBed: &#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#24565;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models. (arXiv:2306.04695v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;ConceptBed&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;CCD&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35270;&#35273;&#27010;&#24565;&#24182;&#20174;&#22270;&#20687;&#20013;&#22797;&#21046;&#21644;&#32452;&#21512;&#36825;&#20123;&#27010;&#24565;&#30340;&#33021;&#21147;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#19968;&#20010;&#26680;&#24515;&#30446;&#26631;&#12290;&#26368;&#36817;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#36890;&#36807;&#23398;&#20064;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25551;&#36848;&#26469;&#29983;&#25104;&#39640;&#28165;&#26224;&#24230;&#21644;&#36924;&#30495;&#30340;&#22270;&#20687;&#36136;&#37327;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;T2I&#27169;&#22411;&#30340;&#37325;&#28857;&#22312;&#20110;&#29031;&#29255;&#33324;&#30340;&#30495;&#23454;&#24863;&#21644;&#26377;&#38480;&#30340;&#35270;&#35273;&#29702;&#35299;&#23450;&#24615;&#37327;&#24230;&#12290;&#20026;&#20102;&#37327;&#21270;T2I&#27169;&#22411;&#22312;&#23398;&#20064;&#21644;&#21512;&#25104;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ConceptBed&#65292;&#19968;&#20010;&#21253;&#21547;284&#20010;&#29420;&#29305;&#35270;&#35273;&#27010;&#24565;&#12289;5K&#20010;&#29420;&#29305;&#27010;&#24565;&#32452;&#21512;&#21644;33K&#20010;&#32452;&#21512;&#25991;&#26412;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;Concept Confidence Deviation&#65288;CCD&#65289;&#65292;&#23427;&#21033;&#29992;oracle&#27010;&#24565;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#26469;&#34913;&#37327;T2I&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#27010;&#24565;&#19982;&#22320;&#38754;&#30495;&#23454;&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#27010;&#24565;&#20043;&#38388;&#30340;&#23545;&#40784;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#35270;&#35273;&#27010;&#24565;&#26159;&#23545;&#35937;&#25110;&#32773;...
&lt;/p&gt;
&lt;p&gt;
The ability to understand visual concepts and replicate and compose these concepts from images is a central goal for computer vision. Recent advances in text-to-image (T2I) models have lead to high definition and realistic image quality generation by learning from large databases of images and their descriptions. However, the evaluation of T2I models has focused on photorealism and limited qualitative measures of visual understanding. To quantify the ability of T2I models in learning and synthesizing novel visual concepts, we introduce ConceptBed, a large-scale dataset that consists of 284 unique visual concepts, 5K unique concept compositions, and 33K composite text prompts. Along with the dataset, we propose an evaluation metric, Concept Confidence Deviation (CCD), that uses the confidence of oracle concept classifiers to measure the alignment between concepts generated by T2I generators and concepts contained in ground truth images. We evaluate visual concepts that are either object
&lt;/p&gt;</description></item><item><title>&#29992;&#22810;&#23610;&#24230;&#27969;&#36827;&#34892;&#20108;&#32500;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#24314;&#27169;&#65292;&#21487;&#35782;&#21035;&#19981;&#21516;&#23610;&#24230;&#30340;&#20449;&#24687;&#24182;&#26174;&#33879;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04689</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#27969;&#29992;&#20110;&#40065;&#26834;&#21644;&#26368;&#20248;&#23431;&#23449;&#23398;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multiscale Flow for Robust and Optimal Cosmological Analysis. (arXiv:2306.04689v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04689
&lt;/p&gt;
&lt;p&gt;
&#29992;&#22810;&#23610;&#24230;&#27969;&#36827;&#34892;&#20108;&#32500;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#24314;&#27169;&#65292;&#21487;&#35782;&#21035;&#19981;&#21516;&#23610;&#24230;&#30340;&#20449;&#24687;&#24182;&#26174;&#33879;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#27969;(Convolutional Normalizing Flow)&#29992;&#20110;&#29983;&#25104;&#20108;&#32500;&#23431;&#23449;&#23398;&#25968;&#25454;&#65292;&#24182;&#23545;&#20043;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#26512;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23567;&#27874;&#22522;&#30784;&#20998;&#35299;&#23431;&#23449;&#23398;&#22330;&#65292;&#28982;&#21518;&#23558;&#19981;&#21516;&#32423;&#21035;&#30340;&#23567;&#27874;&#20998;&#37327;&#20998;&#21035;&#24314;&#27169;&#12290;&#36890;&#36807;&#36880;&#39033;&#27714;&#21644;&#24471;&#20986;&#21407;&#22987;&#23431;&#23449;&#23398;&#22330;&#30340;&#23545;&#25968;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#20998;&#31163;&#19981;&#21516;&#23610;&#24230;&#30340;&#20449;&#24687;&#24182;&#35782;&#21035;&#20854;&#20013;&#26410;&#30693;&#30340;&#23610;&#24230;&#30456;&#20851;&#31995;&#32479;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Multiscale Flow, a generative Normalizing Flow that creates samples and models the field-level likelihood of two-dimensional cosmological data such as weak lensing. Multiscale Flow uses hierarchical decomposition of cosmological fields via a wavelet basis, and then models different wavelet components separately as Normalizing Flows. The log-likelihood of the original cosmological field can be recovered by summing over the log-likelihood of each wavelet term. This decomposition allows us to separate the information from different scales and identify distribution shifts in the data such as unknown scale-dependent systematics. The resulting likelihood analysis can not only identify these types of systematics, but can also be made optimal, in the sense that the Multiscale Flow can learn the full likelihood at the field without any dimensionality reduction. We apply Multiscale Flow to weak lensing mock datasets for cosmological inference, and show that it significantly outperform
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#24120;&#35265;&#30340;&#35780;&#20215;&#25351;&#26631;&#22914;FID&#31561;&#19981;&#33021;&#24456;&#22909;&#22320;&#20307;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#24863;&#30693;&#30495;&#23454;&#24615;&#65292;&#24314;&#35758;&#20351;&#29992;SwAV&#29305;&#24449;&#25552;&#21462;&#22120;&#32467;&#21512;FID&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.04675</link><description>&lt;p&gt;
&#25581;&#31034;&#29983;&#25104;&#27169;&#22411;&#35780;&#20215;&#24230;&#37327;&#30340;&#32570;&#38519;&#21450;&#20854;&#19981;&#20844;&#24179;&#23545;&#24453;&#25193;&#25955;&#27169;&#22411;&#30340;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. (arXiv:2306.04675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04675
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#24120;&#35265;&#30340;&#35780;&#20215;&#25351;&#26631;&#22914;FID&#31561;&#19981;&#33021;&#24456;&#22909;&#22320;&#20307;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#24863;&#30693;&#30495;&#23454;&#24615;&#65292;&#24314;&#35758;&#20351;&#29992;SwAV&#29305;&#24449;&#25552;&#21462;&#22120;&#32467;&#21512;FID&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#35768;&#22810;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#35821;&#20041;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#29702;&#35299;&#21644;&#25913;&#36827;&#29992;&#20110;&#35780;&#20272;&#23427;&#20204;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#24230;&#37327;&#12290;&#20351;&#29992;&#24515;&#29702;&#29289;&#29702;&#23398;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#23454;&#39564;&#65292;&#36890;&#36807;&#23545;&#29983;&#25104;&#26679;&#26412;&#36827;&#34892;&#20154;&#31867;&#24863;&#30693;&#22270;&#20687;&#30495;&#23454;&#24615;&#30340;&#27979;&#37327;&#65292;&#21457;&#29616;&#27809;&#26377;&#20219;&#20309;&#29616;&#26377;&#30340;&#24230;&#37327;&#33021;&#19982;&#20154;&#31867;&#35780;&#20272;&#24378;&#30456;&#20851;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#25972;&#20307;&#24615;&#33021;&#12289;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#35760;&#24518;&#33021;&#21147;&#30340;16&#20010;&#29616;&#20195;&#25351;&#26631;&#65292;&#21457;&#29616;&#20197;&#20154;&#31867;&#20026;&#22522;&#20934;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#30340;&#24863;&#30693;&#30495;&#23454;&#24615;&#19981;&#21453;&#26144;&#22312;&#24120;&#35265;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#22914;FID&#20013;&#12290;&#36825;&#31181;&#24046;&#24322;&#24182;&#19981;&#33021;&#36890;&#36807;&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#26469;&#35299;&#37322;&#65292;&#23613;&#31649;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#36807;&#24230;&#20381;&#36182;&#20110;Inception-V3&#12290;&#36890;&#36807;&#30740;&#31350;&#26367;&#20195;&#30340;&#33258;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#38519;&#65292;&#21457;&#29616;&#20010;&#21035;&#24369;Downstream&#20219;&#21153;&#32534;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#26368;&#33021;&#35299;&#37322;&#22270;&#20687;&#30495;&#23454;&#24615;&#65292;&#24314;&#35758;&#22312;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#26102;&#20351;&#29992;SwAV&#29305;&#24449;&#25552;&#21462;&#22120;&#32467;&#21512;FID&#12290;
&lt;/p&gt;
&lt;p&gt;
We systematically study a wide variety of image-based generative models spanning semantically-diverse datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 16 modern metrics for evaluating the overall performance, fidelity, diversity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23558;&#36229;&#22768;&#20307;&#31215;&#28857;&#20113;&#25552;&#21462;&#20316;&#20026;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#24555;&#36895;&#21407;&#22411;&#24320;&#21457;&#39564;&#35777;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04668</link><description>&lt;p&gt;
SMRVIS&#65306;&#29992;&#20110;&#38750;&#30772;&#22351;&#24615;&#27979;&#35797;&#30340;&#19977;&#32500;&#36229;&#22768;&#28857;&#20113;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
SMRVIS: Point cloud extraction from 3-D ultrasound for non-destructive testing. (arXiv:2306.04668v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04668
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23558;&#36229;&#22768;&#20307;&#31215;&#28857;&#20113;&#25552;&#21462;&#20316;&#20026;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#24555;&#36895;&#21407;&#22411;&#24320;&#21457;&#39564;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#36229;&#22768;&#20307;&#31215;&#30340;&#28857;&#20113;&#25552;&#21462;&#20316;&#20026;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#36825;&#20010;&#20415;&#25463;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24555;&#36895;&#21407;&#22411;&#26469;&#25506;&#32034;U-Net&#26550;&#26500;&#30340;&#21508;&#31181;&#21464;&#20307;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26412;&#25253;&#21578;&#35760;&#24405;&#20102;&#20351;&#29992;5&#20010;&#26631;&#35760;&#36229;&#22768;&#20307;&#31215;&#21644;84&#20010;&#26410;&#26631;&#35760;&#20307;&#31215;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#23436;&#25104;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#36825;&#20010;&#24037;&#20316;&#26159;&#20316;&#20026;&#19968;&#20010;&#21517;&#20026;&#8220;&#36229;&#22768;&#22270;&#20687;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#8221;&#24320;&#25918;&#25361;&#25112;&#30340;&#25552;&#20132;&#30340;&#19968;&#37096;&#20998;&#12290;&#28304;&#20195;&#30721;&#24050;&#22312;Github&#19978;\url{https://github.com/lisatwyw/smrvis}&#20844;&#24320;&#20849;&#20139;&#32473;&#30740;&#31350;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to formulate point cloud extraction from ultrasound volumes as an image segmentation problem. Through this convenient formulation, a quick prototype exploring various variants of the U-Net architecture was developed and evaluated. This report documents the experimental results compiled using a training dataset of 5 labelled ultrasound volumes and 84 unlabelled volumes that got completed in a two-week period as part of a challenge submission to an open challenge entitled ``Deep Learning in Ultrasound Image Analysis''. Source code is shared with the research community at this GitHub URL \url{https://github.com/lisatwyw/smrvis}.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32534;&#30721;&#22120;&#20989;&#25968;&#22312;&#20960;&#20309;&#21521;&#37327;&#31354;&#38388;&#20013;&#23884;&#20837;&#34507;&#30333;&#36136;&#22270;&#65292;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#21644;&#24207;&#21015;&#24863;&#30693;&#33021;&#21147;&#30340;&#34507;&#30333;&#36136;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;&#27604;&#36739;&#34507;&#30333;&#36136;&#32467;&#26500;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26174;&#30528;&#21152;&#36895;&#21644;&#21331;&#36234;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04667</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#22270;&#30340;&#31070;&#32463;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Neural Embeddings for Protein Graphs. (arXiv:2306.04667v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04667
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32534;&#30721;&#22120;&#20989;&#25968;&#22312;&#20960;&#20309;&#21521;&#37327;&#31354;&#38388;&#20013;&#23884;&#20837;&#34507;&#30333;&#36136;&#22270;&#65292;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#21644;&#24207;&#21015;&#24863;&#30693;&#33021;&#21147;&#30340;&#34507;&#30333;&#36136;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;&#27604;&#36739;&#34507;&#30333;&#36136;&#32467;&#26500;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26174;&#30528;&#21152;&#36895;&#21644;&#21331;&#36234;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#22312;&#29983;&#29289;&#20307;&#20013;&#25198;&#28436;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#22240;&#27492;&#65292;&#24320;&#21457;&#29992;&#20110;&#34507;&#30333;&#36136;&#34920;&#31034;&#30340;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#23545;&#25512;&#36827;&#22823;&#35268;&#27169;&#29983;&#29289;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38590;&#20197;&#39640;&#25928;&#38598;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#20013;&#21253;&#21547;&#30340;&#22823;&#37327;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20445;&#25345;&#34507;&#30333;&#36136;&#22270;&#32467;&#26500;&#36317;&#31163;&#30340;&#32534;&#30721;&#22120;&#20989;&#25968;&#65292;&#22312;&#20960;&#20309;&#21521;&#37327;&#31354;&#38388;&#20013;&#23884;&#20837;&#34507;&#30333;&#36136;&#22270;&#12290;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#21644;&#24207;&#21015;&#24863;&#30693;&#33021;&#21147;&#30340;&#34507;&#30333;&#36136;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23884;&#20837;&#22312;&#27604;&#36739;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#21516;&#26102;&#19982;&#22522;&#20110;&#32467;&#26500;&#23545;&#40784;&#30340;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25552;&#20379;&#20102;&#26174;&#30528;&#21152;&#36895;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#34507;&#30333;&#36136;&#32467;&#26500;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#32467;&#26524;&#65307;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#34507;&#30333;&#36136;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteins perform much of the work in living organisms, and consequently the development of efficient computational methods for protein representation is essential for advancing large-scale biological research. Most current approaches struggle to efficiently integrate the wealth of information contained in the protein sequence and structure. In this paper, we propose a novel framework for embedding protein graphs in geometric vector spaces, by learning an encoder function that preserves the structural distance between protein graphs. Utilizing Graph Neural Networks (GNNs) and Large Language Models (LLMs), the proposed framework generates structure- and sequence-aware protein representations. We demonstrate that our embeddings are successful in the task of comparing protein structures, while providing a significant speed-up compared to traditional approaches based on structural alignment. Our framework achieves remarkable results in the task of protein structure classification; in partic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#37327;&#21270;PET&#22270;&#20687;&#37325;&#24314;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#24182;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04664</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#21518;&#39564;&#37319;&#26679;&#20272;&#35745;PET&#22270;&#20687;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Estimating Uncertainty in PET Image Reconstruction via Deep Posterior Sampling. (arXiv:2306.04664v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#37327;&#21270;PET&#22270;&#20687;&#37325;&#24314;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#24182;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#65288;PET&#65289;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#21151;&#33021;&#21307;&#23398;&#25104;&#20687;&#25216;&#26415;&#65292;&#24120;&#29992;&#20110;&#35780;&#20272;&#26576;&#20123;&#33041;&#37096;&#30142;&#30149;&#65292;&#20854;&#37325;&#24314;&#38382;&#39064;&#26159;&#26080;&#35770;&#26159;&#21542;&#23454;&#38469;&#30340;&#12290;PET&#25104;&#20687;&#20013;&#32477;&#22823;&#22810;&#25968;&#37325;&#24314;&#26041;&#27861;&#65292;&#21253;&#25324;&#36845;&#20195;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#37117;&#20250;&#36820;&#22238;&#21333;&#20010;&#20272;&#35745;&#20540;&#32780;&#19981;&#37327;&#21270;&#20851;&#32852;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#30001;&#20110;&#26080;&#27861;&#30830;&#23450;&#24615;&#21644;&#22122;&#22768;&#65292;&#21333;&#20010;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#35823;&#23548;&#25110;&#19981;&#20934;&#30830;&#12290;&#22240;&#27492;&#65292;&#22312;PET&#22270;&#20687;&#37325;&#24314;&#20013;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#30340;&#24230;&#37327;&#21487;&#20197;&#24110;&#21161;&#21307;&#21153;&#20154;&#21592;&#20570;&#20986;&#20851;&#38190;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#37327;&#21270;PET&#22270;&#20687;&#37325;&#24314;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#35757;&#32451;&#19968;&#31181;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20854;&#29983;&#25104;&#22120;&#36817;&#20284;&#20110;&#22312;&#36125;&#21494;&#26031;&#21453;&#28436;&#30340;&#21518;&#39564;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290; &#29983;&#25104;&#22120;&#26159;&#22312;&#20256;&#32479;&#37325;&#24314;&#26041;&#27861;&#33719;&#24471;&#30340;&#20302;&#21058;&#37327;PET&#25195;&#25551;&#21644;&#21516;&#19968;&#21463;&#35797;&#32773;&#30340;&#39640;&#36136;&#37327;&#30913;&#20849;&#25391;&#65288;MR&#65289;&#22270;&#20687;&#30340;&#37325;&#24314;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#30340;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#26356;&#22909;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positron emission tomography (PET) is an important functional medical imaging technique often used in the evaluation of certain brain disorders, whose reconstruction problem is ill-posed. The vast majority of reconstruction methods in PET imaging, both iterative and deep learning, return a single estimate without quantifying the associated uncertainty. Due to ill-posedness and noise, a single solution can be misleading or inaccurate. Thus, providing a measure of uncertainty in PET image reconstruction can help medical practitioners in making critical decisions. This paper proposes a deep learning-based method for uncertainty quantification in PET image reconstruction via posterior sampling. The method is based on training a conditional generative adversarial network whose generator approximates sampling from the posterior in Bayesian inversion. The generator is conditioned on reconstruction from a low-dose PET scan obtained by a conventional reconstruction method and a high-quality mag
&lt;/p&gt;</description></item><item><title>U-PASS&#26159;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#65292;&#20855;&#26377;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#38024;&#23545;&#20020;&#24202;&#24212;&#29992;&#36827;&#34892;&#35774;&#35745;&#12290;&#22312;&#30561;&#30496;&#20998;&#26399;&#20013;&#34920;&#29616;&#20986;&#26497;&#39640;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20248;&#21270;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#31215;&#26497;&#23547;&#25214;&#20449;&#24687;&#26679;&#26412;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04663</link><description>&lt;p&gt;
U-PASS&#65306;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
U-PASS: an Uncertainty-guided deep learning Pipeline for Automated Sleep Staging. (arXiv:2306.04663v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04663
&lt;/p&gt;
&lt;p&gt;
U-PASS&#26159;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#65292;&#20855;&#26377;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#38024;&#23545;&#20020;&#24202;&#24212;&#29992;&#36827;&#34892;&#35774;&#35745;&#12290;&#22312;&#30561;&#30496;&#20998;&#26399;&#20013;&#34920;&#29616;&#20986;&#26497;&#39640;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20248;&#21270;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#31215;&#26497;&#23547;&#25214;&#20449;&#24687;&#26679;&#26412;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#31561;&#20851;&#38190;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#21487;&#38752;&#24615;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#33021;&#22815;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#20197;&#20351;&#35782;&#21035;&#33258;&#20449;&#24230;&#39640;&#21644;&#20302;&#30340;&#21306;&#22495;&#24182;&#24110;&#21161;&#26368;&#23567;&#21270;&#38169;&#35823;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; U-PASS &#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#35813;&#31649;&#36947;&#19987;&#20026;&#20020;&#24202;&#24212;&#29992;&#32780;&#35774;&#35745;&#65292;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;&#25968;&#25454;&#37319;&#38598;&#12289;&#35757;&#32451;&#21644;&#27169;&#22411;&#37096;&#32626;&#65292;&#37117;&#32467;&#21512;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#35757;&#32451;&#36807;&#31243;&#20998;&#20026;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#27493;&#39588;&#21644;&#21322;&#30417;&#30563;&#30340;&#24494;&#35843;&#27493;&#39588;&#12290;&#25105;&#20204;&#23558;&#27492;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#24212;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30561;&#30496;&#20998;&#26399;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#27599;&#20010;&#38454;&#27573;&#37117;&#31995;&#32479;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#36890;&#36807;&#20248;&#21270;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#31215;&#26497;&#23547;&#25214;&#20449;&#24687;&#26679;&#26412;&#24182;&#23558;&#26368;&#19981;&#30830;&#23450;&#30340;&#26679;&#26412;&#25512;&#36831;&#32473;&#19987;&#23478;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#30561;&#30496;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#21830;&#19994;&#30561;&#30496;&#20998;&#26399;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning becomes increasingly prevalent in critical fields such as healthcare, ensuring the safety and reliability of machine learning systems becomes paramount. A key component of reliability is the ability to estimate uncertainty, which enables the identification of areas of high and low confidence and helps to minimize the risk of error. In this study, we propose a machine learning pipeline called U-PASS tailored for clinical applications that incorporates uncertainty estimation at every stage of the process, including data acquisition, training, and model deployment. The training process is divided into a supervised pre-training step and a semi-supervised finetuning step. We apply our uncertainty-guided deep learning pipeline to the challenging problem of sleep staging and demonstrate that it systematically improves performance at every stage. By optimizing the training dataset, actively seeking informative samples, and deferring the most uncertain samples to an expert, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25429;&#25417;&#22478;&#24066;&#22320;&#26041;&#35748;&#21516;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36924;&#30495;&#34920;&#29616;&#65292;&#20294;&#19981;&#33021;&#36807;&#24230;&#20381;&#36182;&#65292;&#21516;&#26102;&#25366;&#25496;&#20102;&#23427;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04662</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29702;&#35299;&#22320;&#26041;&#35748;&#21516;
&lt;/p&gt;
&lt;p&gt;
Understanding Place Identity with Generative AI. (arXiv:2306.04662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25429;&#25417;&#22478;&#24066;&#22320;&#26041;&#35748;&#21516;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36924;&#30495;&#34920;&#29616;&#65292;&#20294;&#19981;&#33021;&#36807;&#24230;&#20381;&#36182;&#65292;&#21516;&#26102;&#25366;&#25496;&#20102;&#23427;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#19981;&#26029;&#21033;&#29992;&#26032;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#26088;&#22312;&#20102;&#35299;&#20154;&#20204;&#22914;&#20309;&#35748;&#30693;&#24314;&#31569;&#29615;&#22659;&#24182;&#24314;&#31435;&#22478;&#24066;&#30340;&#38598;&#20307;&#22320;&#26041;&#35748;&#21516;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#29983;&#25104;&#36924;&#30495;&#30340;&#34920;&#29616;&#24418;&#24335;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#27979;&#35797;&#29983;&#25104;&#24335;AI&#20316;&#20026;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#26469;&#28304;&#30340;&#28508;&#21147;&#65292;&#20197;&#25429;&#25417;&#32463;&#36807;&#36807;&#28388;&#30340;&#22478;&#24066;&#22320;&#26041;&#35748;&#21516;&#12290;&#25105;&#20204;&#21521;ChatGPT&#21644;DALL-E2&#20004;&#20010;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#25552;&#20986;&#20851;&#20110;31&#20010;&#20840;&#29699;&#22478;&#24066;&#22320;&#26041;&#35748;&#21516;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#29983;&#25104;&#24335;AI&#24341;&#21457;&#20102;&#20854;&#21487;&#38752;&#24615;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20132;&#21449;&#39564;&#35777;&#20197;&#26816;&#26597;&#32467;&#26524;&#26159;&#21542;&#26174;&#31034;&#20986;&#19982;&#30495;&#23454;&#22478;&#24066;&#29615;&#22659;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers are constantly leveraging new forms of data with the goal of understanding how people perceive the built environment and build the collective place identity of cities. Latest advancements in generative artificial intelligence (AI) models have enabled the production of realistic representations learned from vast amounts of data. In this study, we aim to test the potential of generative AI as the source of textual and visual information in capturing the place identity of cities assessed by filtered descriptions and images. We asked questions on the place identity of a set of 31 global cities to two generative AI models, ChatGPT and DALL-E2. Since generative AI has raised ethical concerns regarding its trustworthiness, we performed cross-validation to examine whether the results show similar patterns to real urban settings. In particular, we compared the outputs with Wikipedia data for text and images searched from Google for image. Our results indicate that generative AI mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#25955;&#21644;&#36830;&#32493;&#28436;&#21592;&#32593;&#32476;&#26469;&#20248;&#21270;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#31995;&#32479;&#30340;&#39057;&#29575;&#21644;&#21152;&#36895;&#24230;&#26354;&#32447;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24179;&#34913;&#20943;&#23569;&#20572;&#36710;&#21644;&#26368;&#23567;&#21270;&#36895;&#24230;&#21464;&#21270;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26053;&#34892;&#26102;&#38388;&#21644;&#29123;&#26009;&#28040;&#32791;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;GLOSA&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.04660</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Adaptive Frequency Green Light Optimal Speed Advisory based on Hybrid Actor-Critic Reinforcement Learning. (arXiv:2306.04660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#25955;&#21644;&#36830;&#32493;&#28436;&#21592;&#32593;&#32476;&#26469;&#20248;&#21270;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#31995;&#32479;&#30340;&#39057;&#29575;&#21644;&#21152;&#36895;&#24230;&#26354;&#32447;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24179;&#34913;&#20943;&#23569;&#20572;&#36710;&#21644;&#26368;&#23567;&#21270;&#36895;&#24230;&#21464;&#21270;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26053;&#34892;&#26102;&#38388;&#21644;&#29123;&#26009;&#28040;&#32791;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;GLOSA&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#65288;GLOSA&#65289;&#31995;&#32479;&#24314;&#35758;&#36710;&#36742;&#36895;&#24230;&#65292;&#20197;&#24110;&#21161;&#23427;&#20204;&#22312;&#32511;&#33394;&#26102;&#38388;&#36890;&#36807;&#36335;&#21475;&#65292;&#20174;&#32780;&#36890;&#36807;&#26368;&#23567;&#21270;&#22312;&#36335;&#21475;&#20572;&#36710;&#21644;&#24608;&#36895;&#26102;&#38388;&#26469;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#21644;&#29123;&#26009;&#28040;&#32791;&#12290;&#20294;&#26159;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20248;&#21270;GLOSA&#31639;&#27861;&#65292;&#24573;&#30053;&#20102;GLOSA&#31995;&#32479;&#30340;&#36895;&#24230;&#24314;&#35758;&#39057;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#20123;&#30740;&#31350;&#22312;&#27599;&#20010;&#20915;&#31574;&#27493;&#39588;&#25552;&#20379;&#36895;&#24230;&#24314;&#35758;&#65292;&#23548;&#33268;&#20887;&#20313;&#24314;&#35758;&#65292;&#32780;&#20854;&#20182;&#20154;&#20165;&#20026;&#36710;&#36742;&#35745;&#31639;&#26368;&#20339;&#36895;&#24230;&#65292;&#26080;&#27861;&#36866;&#24212;&#21160;&#24577;&#20132;&#36890;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;PPO&#65288;H-PPO&#65289;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;GLOSA&#65288;AF-GLOSA&#65289;&#27169;&#22411;&#65292;&#20854;&#37319;&#29992;&#20102;&#19968;&#20010;actor-critic&#26550;&#26500;&#21644;&#19968;&#20010;&#28151;&#21512;actor&#32593;&#32476;&#12290;&#28151;&#21512;&#28436;&#21592;&#32593;&#32476;&#21253;&#25324;&#19968;&#20010;&#31163;&#25955;&#28436;&#21592;&#65292;&#36755;&#20986;&#21672;&#35810;&#39057;&#29575;&#21644;&#19968;&#20010;&#36830;&#32493;&#28436;&#21592;&#65292;&#36755;&#20986;&#21152;&#36895;&#24230;&#26354;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24179;&#34913;&#20943;&#23569;&#20572;&#36710;&#21644;&#26368;&#23567;&#21270;&#36895;&#24230;&#21464;&#21270;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;AF-GLOSA&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26053;&#34892;&#26102;&#38388;&#21644;&#29123;&#26009;&#28040;&#32791;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;GLOSA&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Green Light Optimal Speed Advisory (GLOSA) system suggests speeds to vehicles to assist them in passing through intersections during green intervals, thus reducing traffic congestion and fuel consumption by minimizing the number of stops and idle times at intersections. However, previous research has focused on optimizing the GLOSA algorithm, neglecting the frequency of speed advisory by the GLOSA system. Specifically, some studies provide speed advisory profile at each decision step, resulting in redundant advisory, while others calculate the optimal speed for the vehicle only once, which cannot adapt to dynamic traffic. In this paper, we propose an Adaptive Frequency GLOSA (AF-GLOSA) model based on Hybrid Proximal Policy Optimization (H-PPO), which employs an actor-critic architecture with a hybrid actor network. The hybrid actor network consists of a discrete actor that outputs advisory frequency and a continuous actor that outputs acceleration profiles. Additionally, we design a no
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#23450;&#21521;&#36827;&#21270;&#22312;&#34507;&#30333;&#36136;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#23545;&#20854;&#23454;&#29616;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.04658</link><description>&lt;p&gt;
&#25968;&#23398;&#36741;&#21161;&#19979;&#30340;&#23450;&#21521;&#36827;&#21270;&#19982;&#34507;&#30333;&#36136;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Mathematics-assisted directed evolution and protein engineering. (arXiv:2306.04658v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04658
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#23450;&#21521;&#36827;&#21270;&#22312;&#34507;&#30333;&#36136;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#23545;&#20854;&#23454;&#29616;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#21521;&#36827;&#21270;&#26159;&#19968;&#31181;&#25913;&#21464;&#34507;&#30333;&#36136;&#24037;&#31243;&#30340;&#20998;&#23376;&#29983;&#29289;&#23398;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#21019;&#24314;&#20855;&#26377;&#26399;&#26395;&#29305;&#24615;&#21644;&#21151;&#33021;&#30340;&#34507;&#30333;&#36136;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#31361;&#21464;&#31354;&#38388;&#65292;&#21363;&#31361;&#21464;&#31354;&#38388;&#38543;&#27688;&#22522;&#37240;&#25968;N&#21576;$20^N$&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#26080;&#27861;&#36827;&#34892;&#25972;&#20010;&#34507;&#30333;&#36136;&#24211;&#30340;&#28145;&#24230;&#31361;&#21464;&#25195;&#25551;&#12290;&#36825;&#23548;&#33268;AI&#36741;&#21161;&#30340;&#23450;&#21521;&#36827;&#21270;&#65288;AIDE&#65289;&#25110;AI&#36741;&#21161;&#30340;&#34507;&#30333;&#36136;&#24037;&#31243;&#65288;AIPE&#65289;&#20316;&#20026;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#20511;&#21161;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#21253;&#25324;&#38271;&#30701;&#26399;&#35760;&#24518;&#12289;&#33258;&#32534;&#30721;&#22120;&#21644;Transformer&#65292;&#22522;&#20110;&#24207;&#21015;&#30340;&#23884;&#20837;&#24050;&#25104;&#20026;AIDE&#21644;AIPE&#20013;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#25345;&#20037;&#25289;&#26222;&#25289;&#26031;&#20154;&#65292;&#22312;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#20013;&#30340;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#20026;&#22522;&#20110;&#32467;&#26500;&#30340;&#23884;&#20837;&#25552;&#20379;&#20102;&#19968;&#20010;&#20986;&#33394;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19968;&#31867;&#25345;&#20037;&#25299;&#25169;&#25289;&#26222;&#25289;&#26031;&#20154;&#65288;PTLs&#65289;&#65292;&#21253;&#25324;&#25345;&#20037;&#25289;&#26222;&#25289;&#26031;&#20154;&#12289;&#25345;&#20037;&#36335;&#24452;&#25289;&#26222;&#25289;&#26031;&#20154;&#12289;&#25345;&#20037;&#37327;&#23376;&#21270;&#25289;&#26222;&#25289;&#26031;&#20154;&#65292;&#21487;&#20197;&#25512;&#36827;&#22522;&#20110;&#25299;&#25169;&#21644;&#20195;&#25968;&#30340;AIDE&#21644;AIPE&#20013;&#30340;&#25968;&#23398;&#26041;&#27861;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Directed evolution is a molecular biology technique that is transforming protein engineering by creating proteins with desirable properties and functions. However, it is experimentally impossible to perform the deep mutational scanning of the entire protein library due to the enormous mutational space, which scales as $20^N$ , where N is the number of amino acids. This has led to the rapid growth of AI-assisted directed evolution (AIDE) or AI-assisted protein engineering (AIPE) as an emerging research field. Aided with advanced natural language processing (NLP) techniques, including long short-term memory, autoencoder, and transformer, sequence-based embeddings have been dominant approaches in AIDE and AIPE. Persistent Laplacians, an emerging technique in topological data analysis (TDA), have made structure-based embeddings a superb option in AIDE and AIPE. We argue that a class of persistent topological Laplacians (PTLs), including persistent Laplacians, persistent path Laplacians, pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#29983;&#25104;&#19981;&#21516;&#35843;&#21046;&#31867;&#22411;&#30340;&#39057;&#35889;&#22270;&#20687;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#36827;&#34892;&#35843;&#21046;&#20998;&#31867;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23545;&#39057;&#35889;&#22270;&#36827;&#34892;&#20998;&#36776;&#29575;&#36716;&#25442;&#26469;&#25552;&#39640;&#36716;&#25442;&#36895;&#24230;&#21644;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#12290;</title><link>http://arxiv.org/abs/2306.04655</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#36776;&#29575;&#36716;&#25442;&#39057;&#35889;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#35843;&#21046;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Modulation Classification Through Deep Learning Using Resolution Transformed Spectrograms. (arXiv:2306.04655v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#29983;&#25104;&#19981;&#21516;&#35843;&#21046;&#31867;&#22411;&#30340;&#39057;&#35889;&#22270;&#20687;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#36827;&#34892;&#35843;&#21046;&#20998;&#31867;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23545;&#39057;&#35889;&#22270;&#36827;&#34892;&#20998;&#36776;&#29575;&#36716;&#25442;&#26469;&#25552;&#39640;&#36716;&#25442;&#36895;&#24230;&#21644;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#21046;&#20998;&#31867;&#26159;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#65292;&#22312;&#36890;&#20449;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#30001;&#20110;&#39057;&#29575;&#38543;&#26102;&#38388;&#21464;&#21270;&#23545;&#20110;&#19981;&#21516;&#35843;&#21046;&#26684;&#24335;&#30340;&#26080;&#32447;&#30005;&#20449;&#21495;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#21306;&#21035;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23558;1-D&#26080;&#32447;&#30005;&#20449;&#21495;&#36716;&#25442;&#25104;&#39057;&#22495;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#21313;&#19968;&#31181;&#19981;&#21516;&#35843;&#21046;&#31867;&#22411;&#30340;&#39057;&#35889;&#22270;&#20687;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#29616;&#20195;&#26550;&#26500;&#26469;&#36827;&#34892;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#65288;AMC&#65289;&#30340;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#39057;&#35889;&#22270;&#36827;&#34892;&#20998;&#36776;&#29575;&#36716;&#25442;&#65292;&#23548;&#33268;&#35745;&#31639;&#36127;&#36733;&#20943;&#23569;&#20102;&#22810;&#36798;99.61&#65285;&#65292;&#20174;&#25509;&#25910;&#21040;&#30340;I/Q&#25968;&#25454;&#36716;&#25442;&#36895;&#24230;&#21152;&#24555;&#20102;8&#20493;&#12290;&#35813;AMC&#26041;&#26696;&#22312;CPU&#21644;GPU&#19978;&#23454;&#29616;&#65292;&#29992;&#20110;&#35782;&#21035;&#25968;&#23383;&#21644;&#27169;&#25311;&#20449;&#21495;&#35843;&#21046;&#26041;&#26696;&#12290;&#24615;&#33021;&#35780;&#20272;&#22522;&#20110;&#29616;&#26377;&#30340;CNN&#27169;&#22411;&#65292;&#21253;&#25324;SqueezeNet&#12289;Resnet-50&#21644;InceptionR&#12290;
&lt;/p&gt;
&lt;p&gt;
Modulation classification is an essential step of signal processing and has been regularly applied in the field of tele-communication. Since variations of frequency with respect to time remains a vital distinction among radio signals having different modulation formats, these variations can be used for feature extraction by converting 1-D radio signals into frequency domain. In this paper, we propose a scheme for Automatic Modulation Classification (AMC) using modern architectures of Convolutional Neural Networks (CNN), through generating spectrum images of eleven different modulation types. Additionally, we perform resolution transformation of spectrograms that results up to 99.61% of computational load reduction and 8x faster conversion from the received I/Q data. This proposed AMC is implemented on CPU and GPU, to recognize digital as well as analogue signal modulation schemes on signals. The performance is evaluated on existing CNN models including SqueezeNet, Resnet-50, InceptionR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26234;&#33021;&#22478;&#24066;&#31649;&#29702;&#31995;&#32479;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#29289;&#32852;&#32593;&#25216;&#26415;&#26469;&#20998;&#26512;&#20132;&#36890;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#22478;&#24066;&#35774;&#26045;&#32500;&#25252;&#31561;&#25968;&#25454;&#65292;&#25552;&#20379;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#22478;&#24066;&#30340;&#23433;&#20840;&#24615;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04653</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#21040;&#34892;&#21160;&#65306;&#25506;&#32034;&#26234;&#33021;&#22478;&#24066;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#29289;&#32852;&#32593;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
From Data to Action: Exploring AI and IoT-driven Solutions for Smarter Cities. (arXiv:2306.04653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26234;&#33021;&#22478;&#24066;&#31649;&#29702;&#31995;&#32479;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#29289;&#32852;&#32593;&#25216;&#26415;&#26469;&#20998;&#26512;&#20132;&#36890;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#22478;&#24066;&#35774;&#26045;&#32500;&#25252;&#31561;&#25968;&#25454;&#65292;&#25552;&#20379;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#22478;&#24066;&#30340;&#23433;&#20840;&#24615;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#22478;&#24066;&#30340;&#20986;&#29616;&#38656;&#35201;&#21033;&#29992;&#20808;&#36827;&#25216;&#26415;&#65292;&#22914;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#24182;&#25215;&#35834;&#37322;&#25918;&#22478;&#24066;&#28508;&#21147;&#65292;&#20351;&#20854;&#26356;&#20855;&#21487;&#25345;&#32493;&#24615;&#12289;&#25928;&#29575;&#21644;&#23621;&#27665;&#21451;&#22909;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26234;&#33021;&#22478;&#24066;&#31649;&#29702;&#31995;&#32479;&#65292;&#25552;&#20379;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#19977;&#20010;&#29992;&#20363;&#65306;&#65288;i&#65289;&#20998;&#26512;&#20132;&#36890;&#20449;&#24687;&#65292;&#20197;&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#39118;&#38505;&#24182;&#25913;&#21892;&#39550;&#39542;&#21592;&#21644;&#34892;&#20154;&#30340;&#23433;&#20840;&#24615;&#65292;&#65288;ii&#65289;&#30830;&#23450;&#20309;&#26102;&#20309;&#22320;&#21487;&#20197;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#20197;&#25552;&#39640;&#25104;&#26412;&#33410;&#30465;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#26816;&#27979;&#22478;&#24066;&#36947;&#36335;&#21644;&#20154;&#34892;&#36947;&#19978;&#30340;&#32500;&#25252;&#38382;&#39064;&#65292;&#20197;&#21450;&#27946;&#27700;&#21644;&#28779;&#28798;&#31561;&#28798;&#23475;&#30340;&#24320;&#22987;&#12290;&#22312;&#38463;&#32500;&#32599;&#24066;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#22312;&#29983;&#25104;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#22686;&#24378;&#20102;&#23433;&#20840;&#24615;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;AI&#21644;IoT&#39537;&#21160;&#30340;&#26234;&#33021;&#22478;&#24066;&#21457;&#23637;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of smart cities demands harnessing advanced technologies like the Internet of Things (IoT) and Artificial Intelligence (AI) and promises to unlock cities' potential to become more sustainable, efficient, and ultimately livable for their inhabitants. This work introduces an intelligent city management system that provides a data-driven approach to three use cases: (i) analyze traffic information to reduce the risk of traffic collisions and improve driver and pedestrian safety, (ii) identify when and where energy consumption can be reduced to improve cost savings, and (iii) detect maintenance issues like potholes in the city's roads and sidewalks, as well as the beginning of hazards like floods and fires. A case study in Aveiro City demonstrates the system's effectiveness in generating actionable insights that enhance security, energy efficiency, and sustainability, while highlighting the potential of AI and IoT-driven solutions for smart city development.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Conformal Prediction&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#21464;&#37327;&#21464;&#25442;&#37325;&#26032;&#23450;&#20041;&#31526;&#21512;&#24230;&#37327;&#65292;&#20351;&#24471;&#39044;&#27979;&#21306;&#38388;&#22312;&#20445;&#25345;&#36793;&#38469;&#26377;&#25928;&#30340;&#21516;&#26102;&#20855;&#26377;&#23545;&#35937;&#23646;&#24615;&#30456;&#20851;&#30340;&#22823;&#23567;&#12290;&#36890;&#36807;&#35757;&#32451;&#21487;&#26368;&#22823;&#21270;&#38388;&#38548;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.04648</link><description>&lt;p&gt;
&#35770;&#35757;&#32451;&#26412;&#22320;&#33258;&#36866;&#24212;&#30340;&#25490;&#21517;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
On training locally adaptive CP. (arXiv:2306.04648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Conformal Prediction&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#21464;&#37327;&#21464;&#25442;&#37325;&#26032;&#23450;&#20041;&#31526;&#21512;&#24230;&#37327;&#65292;&#20351;&#24471;&#39044;&#27979;&#21306;&#38388;&#22312;&#20445;&#25345;&#36793;&#38469;&#26377;&#25928;&#30340;&#21516;&#26102;&#20855;&#26377;&#23545;&#35937;&#23646;&#24615;&#30456;&#20851;&#30340;&#22823;&#23567;&#12290;&#36890;&#36807;&#35757;&#32451;&#21487;&#26368;&#22823;&#21270;&#38388;&#38548;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20351;&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#38388;&#38548;&#26412;&#22320;&#33258;&#36866;&#24212;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38598;&#20013;&#20110;&#36890;&#36807;&#21010;&#20998;&#25110;&#37325;&#26032;&#21152;&#26435;&#26657;&#20934;&#38598;&#26469;&#36817;&#20284;&#38388;&#38548;&#30340;&#23545;&#35937;&#26465;&#20214;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#26159;&#26032;&#30340;&#19988;&#27010;&#24565;&#19978;&#19981;&#21516;&#12290;&#25105;&#20204;&#19981;&#26159;&#37325;&#26032;&#21152;&#26435;&#26657;&#20934;&#25968;&#25454;&#65292;&#32780;&#26159;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#21464;&#37327;&#21464;&#25442;$A \to \phi_X(A)$&#37325;&#26032;&#23450;&#20041;&#31526;&#21512;&#24230;&#37327;&#65292;&#35813;&#21464;&#25442;&#26126;&#30830;&#22320;&#21462;&#20915;&#20110;&#23545;&#35937;&#23646;&#24615;$X$&#12290;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#22914;&#26524;$\phi_X$&#23545;&#20110;&#20219;&#20309;$X$&#22312;$A$&#20013;&#26159;&#21333;&#35843;&#30340;&#65292;&#21017;&#21464;&#25442;&#23558;&#29983;&#25104;&#20445;&#35777;&#26159;&#36793;&#38469;&#26377;&#25928;&#19988;&#20855;&#26377;$X$&#30456;&#20851;&#22823;&#23567;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#23545;$\phi_X$&#36827;&#34892;&#21442;&#25968;&#21270;&#21644;&#35757;&#32451;&#20197;&#26368;&#22823;&#21270;&#38388;&#38548;&#25928;&#29575;&#12290;&#19982;&#20854;&#20182;CP-aware&#35757;&#32451;&#26041;&#27861;&#30456;&#21453;&#65292;&#30446;&#26631;&#20989;&#25968;&#26159;&#24179;&#28369;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#26368;&#23567;&#21270;&#32780;&#26080;&#38656;&#36827;&#34892;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of making Conformal Prediction (CP) intervals locally adaptive. Most existing methods focus on approximating the object-conditional validity of the intervals by partitioning or re-weighting the calibration set. Our strategy is new and conceptually different. Instead of re-weighting the calibration data, we redefine the conformity measure through a trainable change of variables, $A \to \phi_X(A)$, that depends explicitly on the object attributes, $X$. Under certain conditions and if $\phi_X$ is monotonic in $A$ for any $X$, the transformations produce prediction intervals that are guaranteed to be marginally valid and have $X$-dependent sizes. We describe how to parameterize and train $\phi_X$ to maximize the interval efficiency. Contrary to other CP-aware training methods, the objective function is smooth and can be minimized through standard gradient methods without approximations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20108;&#27425;&#38181;&#26494;&#24347;&#19979;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#31232;&#30095;&#30340;&#21521;&#37327;&#65292;&#24471;&#21040;&#20102;&#21487;&#38752;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.04647</link><description>&lt;p&gt;
&#21387;&#32553;&#24863;&#30693;&#65306;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Compressed Sensing: A Discrete Optimization Approach. (arXiv:2306.04647v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20108;&#27425;&#38181;&#26494;&#24347;&#19979;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#31232;&#30095;&#30340;&#21521;&#37327;&#65292;&#24471;&#21040;&#20102;&#21487;&#38752;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#21363;&#25214;&#21040;&#26368;&#31232;&#30095;&#30340;&#21521;&#37327;&#65292;&#35813;&#21521;&#37327;&#28385;&#36275;&#19968;&#32452;&#32447;&#24615;&#27979;&#37327;&#65292;&#21516;&#26102;&#36798;&#21040;&#19968;&#23450;&#30340;&#25968;&#20540;&#23481;&#38480;&#12290;&#21387;&#32553;&#24863;&#30693;&#26159;&#32479;&#35745;&#23398;&#12289;&#36816;&#31609;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#24212;&#29992;&#20110;&#20449;&#21495;&#22788;&#29702;&#12289;&#25968;&#25454;&#21387;&#32553;&#21644;&#22270;&#20687;&#37325;&#24314;&#31561;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;$\ell_2$&#27491;&#21017;&#21270;&#30340;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#23558;&#20854;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#38181;&#35268;&#21010;&#26469;&#37325;&#26032;&#23450;&#20041;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#27492;&#38382;&#39064;&#30340;&#20108;&#27425;&#38181;&#26494;&#24347;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#28201;&#21644;&#38480;&#21046;&#19979;&#65292;&#24471;&#21040;&#30340;&#26494;&#24347;&#31561;&#20215;&#20110;&#28145;&#20837;&#30740;&#31350;&#30340;&#22522;&#30784;&#36861;&#36394;&#21435;&#22122;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#23450;&#26494;&#24347;&#26469;&#21152;&#24378;&#20108;&#27425;&#38181;&#26494;&#24347;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#20108;&#27425;&#38181;&#26494;&#24347;&#26469;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#20197;&#30830;&#35777;&#30340;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#31934;&#30830;&#30340;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the Compressed Sensing (CS) problem, which is the problem of finding the most sparse vector that satisfies a set of linear measurements up to some numerical tolerance. CS is a central problem in Statistics, Operations Research and Machine Learning which arises in applications such as signal processing, data compression and image reconstruction. We introduce an $\ell_2$ regularized formulation of CS which we reformulate as a mixed integer second order cone program. We derive a second order cone relaxation of this problem and show that under mild conditions on the regularization parameter, the resulting relaxation is equivalent to the well studied basis pursuit denoising problem. We present a semidefinite relaxation that strengthens the second order cone relaxation and develop a custom branch-and-bound algorithm that leverages our second order cone relaxation to solve instances of CS to certifiable optimality. Our numerical results show that our approach produces solutions that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24030;&#32423;&#21152;&#24615;&#20559;&#24046;&#30340;&#31616;&#21333;&#25216;&#26415;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#20351;&#29992;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#23567;&#40614;&#20135;&#37327;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#35299;&#20915;&#21704;&#33832;&#20811;&#26031;&#22374;&#24030;&#32423;&#20135;&#37327;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#23558;RMSE&#38477;&#20302;&#20102;8.9\% ~ 28.37\%&#12290;</title><link>http://arxiv.org/abs/2306.04646</link><description>&lt;p&gt;
&#21033;&#29992;&#31616;&#21333;&#30340;&#31354;&#38388;&#24863;&#30693;&#25216;&#26415;&#65292;&#22312;GEOGLAM&#30340;EO&#25968;&#25454;&#19978;&#25913;&#36827;&#21704;&#33832;&#20811;&#26031;&#22374;&#30340;&#24030;&#32423;&#23567;&#40614;&#20135;&#37327;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improve State-Level Wheat Yield Forecasts in Kazakhstan on GEOGLAM's EO Data by Leveraging A Simple Spatial-Aware Technique. (arXiv:2306.04646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24030;&#32423;&#21152;&#24615;&#20559;&#24046;&#30340;&#31616;&#21333;&#25216;&#26415;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#20351;&#29992;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#23567;&#40614;&#20135;&#37327;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#35299;&#20915;&#21704;&#33832;&#20811;&#26031;&#22374;&#24030;&#32423;&#20135;&#37327;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#23558;RMSE&#38477;&#20302;&#20102;8.9\% ~ 28.37\%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#20934;&#30340;&#20135;&#37327;&#39044;&#27979;&#23545;&#20110;&#21046;&#23450;&#39135;&#21697;&#23433;&#20840;&#30340;&#30693;&#24773;&#25919;&#31574;&#21644;&#38271;&#26399;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#36965;&#24863;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25552;&#20379;&#20174;&#30000;&#38388;&#21040;&#22269;&#23478;&#23610;&#24230;&#30340;&#20316;&#29289;&#29366;&#24577;&#30340;&#20840;&#38754;&#21644;&#21450;&#26102;&#35270;&#22270;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#31934;&#24230;&#24120;&#24120;&#21463;&#21040;&#31354;&#38388;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#30001;&#20110;&#36965;&#24863;&#25968;&#25454;&#20013;&#27809;&#26377;&#21453;&#26144;&#30340;&#22806;&#22312;&#22240;&#32032;&#65292;&#22914;&#20316;&#29289;&#31649;&#29702;&#31574;&#30053;&#30340;&#24046;&#24322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#24030;&#32423;&#21152;&#24615;&#20559;&#24046;&#65292;&#20197;&#26126;&#30830;&#35299;&#20915;&#21704;&#33832;&#20811;&#26031;&#22374;&#30340;&#36328;&#22320;&#21306;&#20135;&#37327;&#24322;&#36136;&#24615;&#12290;&#19982;&#22522;&#32447;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#38543;&#26426;&#26862;&#26519;&#12289;CatBoost&#12289;XGBoost&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24635;&#20307;RMSE&#38477;&#20302;&#20102;8.9\%&#65292;&#23558;&#26368;&#39640;&#24030;&#32423;RMSE&#38477;&#20302;&#20102;28.37\%&#12290;&#24030;&#32423;&#21152;&#24615;&#20559;&#24046;&#30340;&#26377;&#25928;&#24615;&#34920;&#26126;&#65292;&#36890;&#36807;&#26126;&#30830;&#35299;&#20915;&#20351;&#29992;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#30340;&#31354;&#38388;&#24322;&#36136;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate yield forecasting is essential for making informed policies and long-term decisions for food security. Earth Observation (EO) data and machine learning algorithms play a key role in providing a comprehensive and timely view of crop conditions from field to national scales. However, machine learning algorithms' prediction accuracy is often harmed by spatial heterogeneity caused by exogenous factors not reflected in remote sensing data, such as differences in crop management strategies. In this paper, we propose and investigate a simple technique called state-wise additive bias to explicitly address the cross-region yield heterogeneity in Kazakhstan. Compared to baseline machine learning models (Random Forest, CatBoost, XGBoost), our method reduces the overall RMSE by 8.9\% and the highest state-wise RMSE by 28.37\%. The effectiveness of state-wise additive bias indicates machine learning's performance can be significantly improved by explicitly addressing the spatial heterogene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#21152;&#36895;&#22120;&#30340;&#36924;&#36817;&#21644;&#23481;&#38169;&#33021;&#21147;&#12290;&#20351;&#29992;AxC&#31639;&#26415;&#30005;&#36335;&#25935;&#25463;&#22320;&#27169;&#25311;&#30828;&#20214;&#19978;&#30340;&#38169;&#35823;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;DNN&#19978;&#25191;&#34892;&#25925;&#38556;&#27880;&#20837;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPU&#30340;&#24555;&#36895;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#26816;&#26597;&#25925;&#38556;&#20256;&#25773;&#21644;&#25513;&#34109;&#65292;&#36827;&#34892;&#20102;&#31934;&#32454;&#30340;&#23481;&#38169;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.04645</link><description>&lt;p&gt;
&#29305;&#21035;&#20250;&#35758;&#65306;DNN&#21152;&#36895;&#22120;&#30340;&#36924;&#36817;&#21644;&#23481;&#38169;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Special Session: Approximation and Fault Resiliency of DNN Accelerators. (arXiv:2306.04645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#21152;&#36895;&#22120;&#30340;&#36924;&#36817;&#21644;&#23481;&#38169;&#33021;&#21147;&#12290;&#20351;&#29992;AxC&#31639;&#26415;&#30005;&#36335;&#25935;&#25463;&#22320;&#27169;&#25311;&#30828;&#20214;&#19978;&#30340;&#38169;&#35823;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;DNN&#19978;&#25191;&#34892;&#25925;&#38556;&#27880;&#20837;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPU&#30340;&#24555;&#36895;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#26816;&#26597;&#25925;&#38556;&#20256;&#25773;&#21644;&#25513;&#34109;&#65292;&#36827;&#34892;&#20102;&#31934;&#32454;&#30340;&#23481;&#38169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#29616;&#20170;&#22312;&#35768;&#22810;&#22330;&#21512;&#24191;&#27867;&#20351;&#29992;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38500;&#20102;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#22806;&#65292;&#21487;&#38752;&#24615;&#20063;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#22240;&#20026;&#31995;&#32479;&#25925;&#38556;&#21487;&#33021;&#20250;&#21361;&#21450;&#20154;&#31867;&#29983;&#21629;&#12290;&#20316;&#20026;&#20219;&#20309;&#20854;&#20182;&#35774;&#22791;&#19968;&#26679;&#65292;&#36816;&#34892;DNN&#30340;&#30828;&#20214;&#26550;&#26500;&#30340;&#21487;&#38752;&#24615;&#24517;&#39035;&#32463;&#36807;&#35780;&#20272;&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#26114;&#36149;&#30340;&#25925;&#38556;&#27880;&#20837;&#27963;&#21160;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#21152;&#36895;&#22120;&#30340;&#36924;&#36817;&#21644;&#23481;&#38169;&#33021;&#21147;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#36817;&#20284;&#65288;AxC&#65289;&#31639;&#26415;&#30005;&#36335;&#65292;&#20197;&#22312;&#30828;&#20214;&#19978;&#25935;&#25463;&#22320;&#27169;&#25311;&#38169;&#35823;&#65292;&#32780;&#26080;&#38656;&#22312;DNN&#19978;&#25191;&#34892;&#25925;&#38556;&#27880;&#20837;&#12290;&#20026;&#20102;&#20801;&#35768;&#24555;&#36895;&#35780;&#20272;AxC DNN&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22522;&#20110;GPU&#30340;&#27169;&#25311;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#32593;&#32476;&#20013;&#30340;&#25925;&#38556;&#20256;&#25773;&#21644;&#25513;&#34109;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#23481;&#38169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning, and in particular, Deep Neural Network (DNN) is nowadays widely used in many scenarios, including safety-critical applications such as autonomous driving. In this context, besides energy efficiency and performance, reliability plays a crucial role since a system failure can jeopardize human life. As with any other device, the reliability of hardware architectures running DNNs has to be evaluated, usually through costly fault injection campaigns. This paper explores the approximation and fault resiliency of DNN accelerators. We propose to use approximate (AxC) arithmetic circuits to agilely emulate errors in hardware without performing fault injection on the DNN. To allow fast evaluation of AxC DNN, we developed an efficient GPU-based simulation framework. Further, we propose a fine-grain analysis of fault resiliency by examining fault propagation and masking in networks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#20445;&#25252;&#25968;&#23383;&#27700;&#21360;&#26041;&#27861;DiffusionShield&#65292;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#20405;&#26435;&#65292;&#31616;&#21333;&#26131;&#23398;&#65292;&#38590;&#20197;&#26816;&#27979;&#65292;&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#34892;&#21508;&#19994;&#12290;</title><link>http://arxiv.org/abs/2306.04642</link><description>&lt;p&gt;
DiffusionShield&#65306;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#20445;&#25252;&#25968;&#23383;&#27700;&#21360;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models. (arXiv:2306.04642v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#20445;&#25252;&#25968;&#23383;&#27700;&#21360;&#26041;&#27861;DiffusionShield&#65292;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#20405;&#26435;&#65292;&#31616;&#21333;&#26131;&#23398;&#65292;&#38590;&#20197;&#26816;&#27979;&#65292;&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#34892;&#21508;&#19994;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;(GDMs)&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#30340;GDMs&#31038;&#21306;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#24182;&#20419;&#36827;&#20102;GDMs&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26080;&#38480;&#21046;&#30340;&#25193;&#25955;&#24341;&#36215;&#20102;&#26377;&#20851;&#29256;&#26435;&#20445;&#25252;&#30340;&#20005;&#37325;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#33402;&#26415;&#23478;(&#21253;&#25324;&#30011;&#23478;&#21644;&#25668;&#24433;&#24072;)&#36234;&#26469;&#36234;&#25285;&#24515;GDMs&#21487;&#20197;&#27627;&#19981;&#36153;&#21147;&#22320;&#22797;&#21046;&#20182;&#20204;&#29420;&#29305;&#30340;&#21019;&#24847;&#20316;&#21697;&#32780;&#26080;&#38656;&#25480;&#26435;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;GDMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#27700;&#21360;&#26041;&#26696;&#8212;&#8212;DiffusionShield&#12290;&#36890;&#36807;&#23558;&#25152;&#26377;&#26435;&#20449;&#24687;&#32534;&#30721;&#25104;&#19968;&#20010;&#19981;&#21487;&#23519;&#35273;&#30340;&#27700;&#21360;&#24182;&#23558;&#20854;&#27880;&#20837;&#22270;&#20687;&#20013;&#65292;DiffusionShield&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;GDMs&#20405;&#26435;&#12290;&#23427;&#30340;&#27700;&#21360;&#21487;&#20197;&#34987;GDMs&#36731;&#26494;&#22320;&#23398;&#20064;&#24182;&#37325;&#29616;&#22312;&#23427;&#20204;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#12290;&#36890;&#36807;&#20174;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#26816;&#27979;&#27700;&#21360;&#65292;&#21487;&#20197;&#25581;&#38706;&#29256;&#26435;&#20405;&#26435;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;DDLearn&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#32771;&#34385;&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#25552;&#39640;&#36890;&#29992;&#20302;&#36164;&#28304;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04641</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#20302;&#36164;&#28304;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Generalizable Low-Resource Activity Recognition with Diverse and Discriminative Representation Learning. (arXiv:2306.04641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04641
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;DDLearn&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#32771;&#34385;&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#25552;&#39640;&#36890;&#29992;&#20302;&#36164;&#28304;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26159;&#19968;&#39033;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#20174;&#20154;&#31867;&#20256;&#24863;&#22120;&#35835;&#25968;&#20013;&#35782;&#21035;&#21160;&#20316;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#20805;&#36275;&#30340;&#25968;&#25454;&#26159;&#35757;&#32451;&#36890;&#29992;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#30340;&#20851;&#38190;&#38590;&#28857;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#32447;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#30340;&#23450;&#21046;&#21644;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DDLearn&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#21516;&#26102;&#32771;&#34385;&#24046;&#24322;&#24615;&#21644;&#27495;&#35270;&#24615;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#29992;&#20302;&#36164;&#28304;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) is a time series classification task that focuses on identifying the motion patterns from human sensor readings. Adequate data is essential but a major bottleneck for training a generalizable HAR model, which assists customization and optimization of online web applications. However, it is costly in time and economy to collect large-scale labeled data in reality, i.e., the low-resource challenge. Meanwhile, data collected from different persons have distribution shifts due to different living habits, body shapes, age groups, etc. The low-resource and distribution shift challenges are detrimental to HAR when applying the trained model to new unseen subjects. In this paper, we propose a novel approach called Diverse and Discriminative representation Learning (DDLearn) for generalizable low-resource HAR. DDLearn simultaneously considers diversity and discrimination learning. With the constructed self-supervised learning task, DDLearn enlarges the data dive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#33976;&#39311;&#20013;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#30456;&#23545;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24544;&#23454;&#30340;&#27169;&#20223;&#26694;&#26550;&#26469;&#35299;&#20915;&#23398;&#29983;&#32622;&#20449;&#24230;&#21644;&#36719;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#35777;&#21644;&#35748;&#35777;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#23398;&#29983;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04431</link><description>&lt;p&gt;
&#24544;&#23454;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Faithful Knowledge Distillation. (arXiv:2306.04431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#33976;&#39311;&#20013;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#30456;&#23545;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24544;&#23454;&#30340;&#27169;&#20223;&#26694;&#26550;&#26469;&#35299;&#20915;&#23398;&#29983;&#32622;&#20449;&#24230;&#21644;&#36719;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#35777;&#21644;&#35748;&#35777;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#23398;&#29983;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#20351;&#20854;&#33021;&#22815;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#25104;&#21151;&#26041;&#27861;&#65292;&#20294;&#36807;&#21435;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#25945;&#24072;&#19982;&#23398;&#29983;&#20043;&#38388;&#22312;&#36719;&#32622;&#20449;&#24230;&#26041;&#38754;&#30340;&#30456;&#23545;&#26657;&#20934;&#38382;&#39064;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#19968;&#20010;&#25945;&#24072;-&#23398;&#29983;&#23545;&#20013;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;i&#65289;&#25945;&#24072;&#21644;&#23398;&#29983;&#26159;&#21542;&#22312;&#25509;&#36817;&#27491;&#30830;&#20998;&#31867;&#30340;&#25968;&#25454;&#26679;&#26412;&#26102;&#23384;&#22312;&#20998;&#27495;&#65292;&#65288;ii&#65289;&#22312;&#25968;&#25454;&#26679;&#26412;&#21608;&#22260;&#65292;&#32463;&#36807;&#33976;&#39311;&#30340;&#23398;&#29983;&#26159;&#21542;&#20687;&#25945;&#24072;&#19968;&#26679;&#33258;&#20449;&#12290;&#36825;&#20123;&#37117;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#32771;&#34385;&#20174;&#40065;&#26834;&#25945;&#24072;&#20013;&#35757;&#32451;&#36739;&#23567;&#23398;&#29983;&#32593;&#32476;&#30340;&#37096;&#32626;&#26102;&#38750;&#24120;&#20851;&#38190;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24544;&#23454;&#30340;&#27169;&#20223;&#26694;&#26550;&#26469;&#35752;&#35770;&#32622;&#20449;&#24230;&#30340;&#30456;&#23545;&#26657;&#20934;&#65292;&#24182;&#25552;&#20379;&#23454;&#35777;&#21644;&#35748;&#35777;&#26041;&#27861;&#26469;&#35780;&#20272;&#23398;&#29983;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) has received much attention due to its success in compressing networks to allow for their deployment in resource-constrained systems. While the problem of adversarial robustness has been studied before in the KD setting, previous works overlook what we term the relative calibration of the student network with respect to its teacher in terms of soft confidences. In particular, we focus on two crucial questions with regard to a teacher-student pair: (i) do the teacher and student disagree at points close to correctly classified dataset examples, and (ii) is the distilled student as confident as the teacher around dataset examples? These are critical questions when considering the deployment of a smaller student network trained from a robust teacher within a safety-critical setting. To address these questions, we introduce a faithful imitation framework to discuss the relative calibration of confidences, as well as provide empirical and certified methods to eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;EfficientNet&#26550;&#26500;&#30340;&#20572;&#36710;&#20301;&#21344;&#29992;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#22312;5&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24615;&#33021;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.04288</link><description>&lt;p&gt;
&#22522;&#20110;&#20572;&#36710;&#22330;&#21344;&#29992;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Revising deep learning methods in parking lot occupancy detection. (arXiv:2306.04288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;EfficientNet&#26550;&#26500;&#30340;&#20572;&#36710;&#20301;&#21344;&#29992;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#22312;5&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24615;&#33021;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20572;&#36710;&#22330;&#24341;&#23548;&#31995;&#32479;&#20316;&#20026;&#26234;&#33021;&#22478;&#24066;&#21457;&#23637;&#30340;&#19968;&#37096;&#20998;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#27969;&#34892;&#30340;&#36235;&#21183;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#20801;&#35768;&#39550;&#39542;&#21592;&#22312;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#25628;&#32034;&#21487;&#29992;&#20572;&#36710;&#20301;&#30340;&#31639;&#27861;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#25668;&#20687;&#22836;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31995;&#32479;&#22312;&#29305;&#23450;&#30340;&#35270;&#35273;&#26465;&#20214;&#19979;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#21644;&#36866;&#24403;&#30340;&#27979;&#35797;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24191;&#27867;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#20572;&#36710;&#20301;&#21344;&#29992;&#26816;&#27979;&#31639;&#27861;&#65292;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#36136;&#37327;&#19982;&#26368;&#36817;&#20986;&#29616;&#30340;&#35270;&#35273;Transformer&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#22522;&#20110;EfficientNet&#26550;&#26500;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31649;&#36947;&#12290;&#36827;&#34892;&#30340;&#35745;&#31639;&#23454;&#39564;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#26377;&#20102;&#25552;&#39640;&#65292;&#35813;&#27169;&#22411;&#22312;5&#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parking guidance systems have recently become a popular trend as a part of the smart cities' paradigm of development. The crucial part of such systems is the algorithm allowing drivers to search for available parking lots across regions of interest. The classic approach to this task is based on the application of neural network classifiers to camera records. However, existing systems demonstrate a lack of generalization ability and appropriate testing regarding specific visual conditions. In this study, we extensively evaluate state-of-the-art parking lot occupancy detection algorithms, compare their prediction quality with the recently emerged vision transformers, and propose a new pipeline based on EfficientNet architecture. Performed computational experiments have demonstrated the performance increase in the case of our model, which was evaluated on 5 different datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21152;&#26435;&#26399;&#26395;&#25913;&#36827;&#26041;&#27861;&#65288;SAWEI&#65289;&#65292;&#21487;&#20197;&#33258;&#21160;&#24179;&#34913;&#25506;&#32034;&#19981;&#30830;&#23450;&#21306;&#22495;&#21644;&#21033;&#29992;&#26377;&#25215;&#35834;&#21306;&#22495;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;COCO&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04262</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#33258;&#36866;&#24212;&#21152;&#26435;&#26399;&#26395;&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Adjusting Weighted Expected Improvement for Bayesian Optimization. (arXiv:2306.04262v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21152;&#26435;&#26399;&#26395;&#25913;&#36827;&#26041;&#27861;&#65288;SAWEI&#65289;&#65292;&#21487;&#20197;&#33258;&#21160;&#24179;&#34913;&#25506;&#32034;&#19981;&#30830;&#23450;&#21306;&#22495;&#21644;&#21033;&#29992;&#26377;&#25215;&#35834;&#21306;&#22495;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;COCO&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#65292;&#23545;&#23567;&#22411;&#35780;&#20272;&#39044;&#31639;&#30340;&#40657;&#31665;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#30340;&#39640;&#25928;&#31639;&#27861;&#31867;&#12290;BO&#31649;&#36947;&#26412;&#36523;&#39640;&#24230;&#21487;&#37197;&#32622;&#65292;&#28041;&#21450;&#21021;&#22987;&#35774;&#35745;&#12289;&#20195;&#29702;&#27169;&#22411;&#21644;&#33719;&#21462;&#21151;&#33021;&#65288;AF&#65289;&#30340;&#35768;&#22810;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#22914;&#20309;&#20026;&#25163;&#22836;&#38382;&#39064;&#36873;&#25321;&#21512;&#36866;&#30340;&#32452;&#20214;&#30340;&#29702;&#35299;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;AF&#30340;&#23450;&#20041;&#65292;&#20854;&#20027;&#35201;&#30446;&#30340;&#26159;&#24179;&#34913;&#23545;&#39640;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#21644;&#23545;&#22909;&#35299;&#20915;&#26041;&#26696;&#26377;&#39640;&#25215;&#35834;&#30340;&#21306;&#22495;&#20043;&#38388;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#21152;&#26435;&#26399;&#26395;&#25913;&#36827;&#26041;&#27861;&#65288;SAWEI&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#35753;&#25506;&#32034; - &#21033;&#29992;&#26435;&#34913;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#22522;&#20110;BO&#30340;&#25910;&#25947;&#20934;&#21017;&#12290;&#22312;COCO&#22522;&#20934;&#24179;&#21488;&#30340;&#26080;&#22122;&#22768;&#40657;&#31665;BBOB&#20989;&#25968;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#22522;&#32447;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#20219;&#20309;&#26102;&#38388;&#24615;&#33021;&#65292;&#24182;&#19988;&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;SP&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is a class of surrogate-based, sample-efficient algorithms for optimizing black-box problems with small evaluation budgets. The BO pipeline itself is highly configurable with many different design choices regarding the initial design, surrogate model, and acquisition function (AF). Unfortunately, our understanding of how to select suitable components for a problem at hand is very limited. In this work, we focus on the definition of the AF, whose main purpose is to balance the trade-off between exploring regions with high uncertainty and those with high promise for good solutions. We propose Self-Adjusting Weighted Expected Improvement (SAWEI), where we let the exploration-exploitation trade-off self-adjust in a data-driven manner, based on a convergence criterion for BO. On the noise-free black-box BBOB functions of the COCO benchmarking platform, our method exhibits a favorable any-time performance compared to handcrafted baselines and serves as a robust def
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#35268;&#33539;&#21270;&#21521;&#37327;&#22330;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24178;&#20928;&#36755;&#20837;&#21644;&#24322;&#24120;&#36755;&#20837;&#30340;&#21306;&#20998;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25915;&#20987;&#19979;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26816;&#27979;&#22120;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#23545;&#25239;&#26816;&#27979;&#22120;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.04252</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Sample Detection Through Neural Network Transport Dynamics. (arXiv:2306.04252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#35268;&#33539;&#21270;&#21521;&#37327;&#22330;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24178;&#20928;&#36755;&#20837;&#21644;&#24322;&#24120;&#36755;&#20837;&#30340;&#21306;&#20998;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25915;&#20987;&#19979;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26816;&#27979;&#22120;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#23545;&#25239;&#26816;&#27979;&#22120;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#31163;&#25955;&#21160;&#24577;&#31995;&#32479;&#30340;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#22120;&#12290;&#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#36890;&#36807;&#23618;&#36981;&#24490;&#30340;&#31163;&#25955;&#21521;&#37327;&#22330;&#65292;&#35813;&#26816;&#27979;&#22120;&#21487;&#20197;&#23558;&#24178;&#20928;&#36755;&#20837;&#20174;&#24322;&#24120;&#36755;&#20837;&#20013;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#35813;&#21521;&#37327;&#22330;&#36827;&#34892;&#35268;&#33539;&#21270;&#21487;&#20197;&#20351;&#32593;&#32476;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#25903;&#25345;&#19978;&#26356;&#21152;&#35268;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24178;&#20928;&#36755;&#20837;&#30340;&#28608;&#27963;&#26356;&#23481;&#26131;&#19982;&#24322;&#24120;&#36755;&#20837;&#30340;&#28608;&#27963;&#21306;&#20998;&#24320;&#26469;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26816;&#27979;&#22120;&#19982;&#20854;&#20182;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#35268;&#33539;&#21270;&#32593;&#32476;&#21160;&#24577;&#21487;&#20197;&#25552;&#39640;&#23558;&#20869;&#37096;&#23884;&#20837;&#29992;&#20316;&#36755;&#20837;&#30340;&#23545;&#25239;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a detector of adversarial samples that is based on the view of neural networks as discrete dynamic systems. The detector tells clean inputs from abnormal ones by comparing the discrete vector fields they follow through the layers. We also show that regularizing this vector field during training makes the network more regular on the data distribution's support, thus making the activations of clean inputs more distinguishable from those of abnormal ones. Experimentally, we compare our detector favorably to other detectors on seen and unseen attacks, and show that the regularization of the network's dynamics improves the performance of adversarial detectors that use the internal embeddings as inputs, while also improving test accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04220</link><description>&lt;p&gt;
&#22312;&#34920;&#38754;&#20043;&#19979;&#23547;&#25214;&#65306;&#21033;&#29992;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#38590;&#20197;&#25511;&#21046;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#23567;&#19988;&#35206;&#30422;&#33539;&#22260;&#29421;&#31364;&#65292;&#20174;&#32780;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19979;&#26174;&#33879;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;(T-symmetry)&#24378;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDM)&#65292;&#24314;&#31435;&#20102;&#19968;&#23545;&#27491;&#21521;&#21644;&#21453;&#21521;&#28508;&#22312;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;TDM&#20026;&#23567;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;T-symmetry&#30340;&#31526;&#21512;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#39044;&#27979;&#38598;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#21333;&#19968;&#26631;&#31614;&#39044;&#27979;&#65292;&#23427;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#22120;&#26500;&#24314;&#39044;&#27979;&#38598;&#65292;&#24182;&#24341;&#23548;&#20154;&#31867;&#19987;&#23478;&#20174;&#20013;&#36873;&#25321;&#26631;&#31614;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.03928</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#39044;&#27979;&#38598;&#35774;&#35745;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Designing Decision Support Systems Using Counterfactual Prediction Sets. (arXiv:2306.03928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#39044;&#27979;&#38598;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#21333;&#19968;&#26631;&#31614;&#39044;&#27979;&#65292;&#23427;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#22120;&#26500;&#24314;&#39044;&#27979;&#38598;&#65292;&#24182;&#24341;&#23548;&#20154;&#31867;&#19987;&#23478;&#20174;&#20013;&#36873;&#25321;&#26631;&#31614;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#20219;&#21153;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#36890;&#24120;&#34987;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#30340;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#39044;&#27979;&#24182;&#19981;&#23436;&#32654;&#65292;&#36825;&#20123;&#31995;&#32479;&#36824;&#38656;&#35201;&#35753;&#20154;&#31867;&#19987;&#23478;&#20102;&#35299;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#39044;&#27979;&#26469;&#26356;&#26032;&#33258;&#24049;&#30340;&#39044;&#27979;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26368;&#36817;&#26377;&#20154;&#35748;&#20026;&#65292;&#21478;&#19968;&#31181;&#31867;&#22411;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#21487;&#33021;&#20250;&#36991;&#24320;&#36825;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#31995;&#32479;&#19981;&#26159;&#25552;&#20379;&#21333;&#20010;&#26631;&#31614;&#39044;&#27979;&#65292;&#32780;&#26159;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#22120;&#26500;&#24314;&#19968;&#32452;&#26631;&#31614;&#39044;&#27979;&#20540;&#65292;&#21363;&#39044;&#27979;&#38598;&#65292;&#24182;&#24378;&#21046;&#35201;&#27714;&#19987;&#23478;&#20174;&#39044;&#27979;&#38598;&#20013;&#39044;&#27979;&#19968;&#20010;&#26631;&#31614;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#35780;&#20272;&#36804;&#20170;&#20173;&#20381;&#36182;&#20110;&#26679;&#24335;&#21270;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#36825;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#25215;&#35834;&#30340;&#36136;&#30097;&#12290;&#26412;&#25991;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#31181;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision support systems for classification tasks are predominantly designed to predict the value of the ground truth labels. However, since their predictions are not perfect, these systems also need to make human experts understand when and how to use these predictions to update their own predictions. Unfortunately, this has been proven challenging. In this context, it has been recently argued that an alternative type of decision support systems may circumvent this challenge. Rather than providing a single label prediction, these systems provide a set of label prediction values constructed using a conformal predictor, namely a prediction set, and forcefully ask experts to predict a label value from the prediction set. However, the design and evaluation of these systems have so far relied on stylized expert models, questioning their promise. In this paper, we revisit the design of this type of systems from the perspective of online learning and develop a methodology that does not requi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(LHVAE)&#65292;&#30740;&#31350;&#24773;&#24863;&#26465;&#20214;&#23545;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#21644;&#22768;&#36136;&#37327;&#24182;&#25429;&#25417;&#20102;&#20016;&#23500;&#30340;&#21644;&#24358;&#36827;&#34892;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24863;&#30693;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03718</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#24773;&#24863;&#26465;&#20214;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder. (arXiv:2306.03718v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03718
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(LHVAE)&#65292;&#30740;&#31350;&#24773;&#24863;&#26465;&#20214;&#23545;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#21644;&#22768;&#36136;&#37327;&#24182;&#25429;&#25417;&#20102;&#20016;&#23500;&#30340;&#21644;&#24358;&#36827;&#34892;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24863;&#30693;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#27169;&#22411;&#22312;&#25552;&#39640;&#29983;&#25104;&#30340;&#21644;&#22768;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#24573;&#30053;&#20102;&#38899;&#20048;&#20013;&#30340;&#24773;&#24863;&#12290;&#21516;&#26102;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#25152;&#29983;&#25104;&#30340;&#21644;&#22768;&#21464;&#21270;&#24615;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LSTM&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(LHVAE)&#65292;&#30740;&#31350;&#24773;&#24863;&#26465;&#20214;&#23545;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#21644;&#22768;&#36136;&#37327;&#24182;&#25429;&#25417;&#20102;&#20016;&#23500;&#30340;&#21644;&#24358;&#36827;&#34892;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;LHVAE&#22312;&#19981;&#21516;&#23618;&#27425;&#65288;&#20048;&#31456;&#21644;&#23567;&#33410;&#32423;&#21035;&#65289;&#19978;&#34701;&#21512;&#20102;&#28508;&#22312;&#21464;&#37327;&#21644;&#24773;&#24863;&#26465;&#20214;&#65292;&#20197;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#38899;&#20048;&#23646;&#24615;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26059;&#24459;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#26059;&#24459;&#21644;&#21644;&#22768;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#23458;&#35266;&#35780;&#20272;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#20854;&#20182;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#20027;&#35266;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#31526;&#21512;&#32473;&#23450;&#24773;&#24863;&#30340;&#21644;&#22768;&#36827;&#34892;&#65292;&#24182;&#22312;&#24863;&#30693;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing melody harmonization models have made great progress in improving the quality of generated harmonies, but most of them ignored the emotions beneath the music. Meanwhile, the variability of harmonies generated by previous methods is insufficient. To solve these problems, we propose a novel LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the influence of emotional conditions on melody harmonization, while improving the quality of generated harmonies and capturing the abundant variability of chord progressions. Specifically, LHVAE incorporates latent variables and emotional conditions at different levels (piece- and bar-level) to model the global and local music properties. Additionally, we introduce an attention-based melody context vector at each step to better learn the correspondence between melodies and harmonies. Experimental results of the objective evaluation show that our proposed model outperforms other LSTM-based models. Through subjective evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#20351;&#24471;&#36890;&#36807;&#24494;&#35843;&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#26356;&#26032;&#39044;&#35757;&#32451;&#37096;&#20998;&#20174;&#32780;&#25439;&#22833;&#23453;&#36149;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#26032;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#20026;&#26377;&#25928;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.02947</link><description>&lt;p&gt;
&#36890;&#36807;&#35843;&#25972;&#36755;&#20837;&#31354;&#38388;&#36827;&#34892;&#39044;&#35757;&#32451;&#39592;&#24178;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning with Pretrained Backbones by Tuning in the Input Space. (arXiv:2306.02947v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#20351;&#24471;&#36890;&#36807;&#24494;&#35843;&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#26356;&#26032;&#39044;&#35757;&#32451;&#37096;&#20998;&#20174;&#32780;&#25439;&#22833;&#23453;&#36149;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#26032;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#20026;&#26377;&#25928;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36866;&#24212;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#20869;&#22312;&#22256;&#38590;&#38480;&#21046;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#36991;&#20813;&#26356;&#26032;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#37096;&#20998;&#24182;&#23398;&#20064;&#19981;&#20165;&#26159;&#36890;&#24120;&#30340;&#20998;&#31867;&#22836;&#65292;&#32780;&#19988;&#36824;&#26377;&#19968;&#32452;&#26032;&#24341;&#20837;&#30340;&#21487;&#20197;&#23398;&#20064;&#21442;&#25968;&#65292;&#36825;&#20123;&#21442;&#25968;&#36127;&#36131;&#36716;&#25442;&#36755;&#20837;&#25968;&#25454;&#12290;&#36825;&#20010;&#36807;&#31243;&#20801;&#35768;&#32593;&#32476;&#26377;&#25928;&#22320;&#21033;&#29992;&#24050;&#23398;&#20064;&#30340;&#30693;&#35782;&#65292;&#24182;&#20351;&#24494;&#35843;&#36807;&#31243;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intrinsic difficulty in adapting deep learning models to non-stationary environments limits the applicability of neural networks to real-world tasks. This issue is critical in practical supervised learning settings, such as the ones in which a pre-trained model computes projections toward a latent space where different task predictors are sequentially learned over time. As a matter of fact, incrementally fine-tuning the whole model to better adapt to new tasks usually results in catastrophic forgetting, with decreasing performance over the past experiences and losing valuable knowledge from the pre-training stage. In this paper, we propose a novel strategy to make the fine-tuning procedure more effective, by avoiding to update the pre-trained part of the network and learning not only the usual classification head, but also a set of newly-introduced learnable parameters that are responsible for transforming the input data. This process allows the network to effectively leverage the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;GNN&#25512;&#29702;&#33539;&#24335;MSInterpreter&#65292;&#20854;&#20013;&#21253;&#25324;&#28040;&#24687;&#20256;&#36882;&#36873;&#25321;&#26041;&#26696;(MSScheme)&#65292;&#36890;&#36807;&#35745;&#31639;&#30001;&#32467;&#26500;&#21644;&#33410;&#28857;&#23884;&#20837;&#32452;&#25104;&#30340;&#28040;&#24687;&#32858;&#21512;&#36335;&#24452;&#30340;&#26435;&#37325;&#22240;&#23376;&#65292;&#23454;&#29616;&#23545;GNN&#33258;&#25105;&#35299;&#37322;&#12290;&#22312;&#22270;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02081</link><description>&lt;p&gt;
&#20449;&#24687;&#20256;&#36882;&#36873;&#25321;&#65306;&#38754;&#21521;&#22270;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;GNN&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Message-passing selection: Towards interpretable GNNs for graph classification. (arXiv:2306.02081v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;GNN&#25512;&#29702;&#33539;&#24335;MSInterpreter&#65292;&#20854;&#20013;&#21253;&#25324;&#28040;&#24687;&#20256;&#36882;&#36873;&#25321;&#26041;&#26696;(MSScheme)&#65292;&#36890;&#36807;&#35745;&#31639;&#30001;&#32467;&#26500;&#21644;&#33410;&#28857;&#23884;&#20837;&#32452;&#25104;&#30340;&#28040;&#24687;&#32858;&#21512;&#36335;&#24452;&#30340;&#26435;&#37325;&#22240;&#23376;&#65292;&#23454;&#29616;&#23545;GNN&#33258;&#25105;&#35299;&#37322;&#12290;&#22312;&#22270;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;GNN&#25512;&#29702;&#33539;&#24335;&#65292;&#31216;&#20026;MSInterpreter&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#26696;&#36731;&#26494;&#24212;&#29992;&#20110;&#21508;&#31181;GNN&#22522;&#32447;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35299;&#37322;&#26041;&#27861;&#19981;&#21516;&#65292;MSInterpreter&#25552;&#20379;&#20102;&#19968;&#31181;&#28040;&#24687;&#20256;&#36882;&#36873;&#25321;&#26041;&#26696;(MSScheme)&#65292;&#20197;&#36873;&#25321;GNN&#30340;&#20851;&#38190;&#36335;&#24452;&#36827;&#34892;&#28040;&#24687;&#32858;&#21512;&#65292;&#26088;&#22312;&#23454;&#29616;&#33258;&#25105;&#35299;&#37322;&#32780;&#19981;&#26159;&#20107;&#21518;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;MSScheme&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#32771;&#34385;&#39321;&#33609;&#32467;&#26500;&#21644;&#33410;&#28857;&#23884;&#20837;&#32452;&#20214;&#26469;&#35745;&#31639;&#28040;&#24687;&#32858;&#21512;&#36335;&#24452;&#30340;&#26435;&#37325;&#22240;&#23376;&#65292;&#20854;&#20013;&#32467;&#26500;&#22522;&#26088;&#22312;&#26435;&#34913;&#33410;&#28857;&#24341;&#36215;&#30340;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#26435;&#37325;&#22240;&#23376;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#33410;&#28857;&#23884;&#20837;&#22522;&#30528;&#30524;&#20110;&#36890;&#36807;&#21333;&#23618;GNN&#33719;&#24471;&#30340;&#33410;&#28857;&#23884;&#20837;&#26469;&#35745;&#31639;&#26435;&#37325;&#22240;&#23376;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22270;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we strive to develop an interpretable GNNs' inference paradigm, termed MSInterpreter, which can serve as a plug-and-play scheme readily applicable to various GNNs' baselines. Unlike the most existing explanation methods, MSInterpreter provides a Message-passing Selection scheme(MSScheme) to select the critical paths for GNNs' message aggregations, which aims at reaching the self-explaination instead of post-hoc explanations. In detail, the elaborate MSScheme is designed to calculate weight factors of message aggregation paths by considering the vanilla structure and node embedding components, where the structure base aims at weight factors among node-induced substructures; on the other hand, the node embedding base focuses on weight factors via node embeddings obtained by one-layer GNN.Finally, we demonstrate the effectiveness of our approach on graph classification benchmarks.
&lt;/p&gt;</description></item><item><title>SGEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#20027;&#27969;ASR&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#26102;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01981</link><description>&lt;p&gt;
SGEM&#65306;&#36890;&#36807;&#24207;&#21015;&#32423;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#23454;&#29616;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization. (arXiv:2306.01981v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01981
&lt;/p&gt;
&lt;p&gt;
SGEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#20027;&#27969;ASR&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#26102;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#32463;&#24120;&#26292;&#38706;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#26377;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#20197;&#36866;&#24212;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#23454;&#20363;&#12290;&#23613;&#31649;&#26377;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20165;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#36138;&#24515;&#35299;&#30721;&#65292;&#24182;&#22312;&#24103;&#32423;&#21035;&#19978;&#36328;&#36234;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#35843;&#25972;&#65292;&#36825;&#22312;&#27169;&#22411;&#36755;&#20986;&#30340;&#24207;&#21015;&#24615;&#36136;&#19979;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TTA&#26694;&#26550;&#65292;&#31216;&#20026;SGEM&#65292;&#29992;&#20110;&#19968;&#33324;ASR&#27169;&#22411;&#12290;&#20026;&#20102;&#22788;&#29702;&#24207;&#21015;&#36755;&#20986;&#65292;SGEM&#39318;&#20808;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#26469;&#25506;&#32034;&#20505;&#36873;&#36755;&#20986;&#26631;&#24535;&#65292;&#24182;&#36873;&#25321;&#26368;&#21487;&#20449;&#30340;&#26631;&#24535;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#21644;&#36127;&#25277;&#26679;&#20316;&#20026;&#26080;&#30417;&#30563;&#30446;&#26631;&#26469;&#36866;&#24212;&#27169;&#22411;&#12290;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#36716;&#21464;&#19979;&#65292;SGEM&#23454;&#29616;&#20102;&#19977;&#31181;&#20027;&#27969;ASR&#27169;&#22411;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) models are frequently exposed to data distribution shifts in many real-world scenarios, leading to erroneous predictions. To tackle this issue, an existing test-time adaptation (TTA) method has recently been proposed to adapt the pre-trained ASR model on unlabeled test instances without source data. Despite decent performance gain, this work relies solely on naive greedy decoding and performs adaptation across timesteps at a frame level, which may not be optimal given the sequential nature of the model output. Motivated by this, we propose a novel TTA framework, dubbed SGEM, for general ASR models. To treat the sequential output, SGEM first exploits beam search to explore candidate output logits and selects the most plausible one. Then, it utilizes generalized entropy minimization and negative sampling as unsupervised objectives to adapt the model. SGEM achieves state-of-the-art performance for three mainstream ASR models under various domain shifts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#24182;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.01788</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Responsible Design Patterns for Machine Learning Pipelines. (arXiv:2306.01788v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#24182;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36947;&#24503;&#23454;&#36341;&#25972;&#21512;&#21040;&#20154;&#24037;&#26234;&#33021;(AI)&#24320;&#21457;&#36807;&#31243;&#20013;&#23545;&#20110;&#30830;&#20445;AI&#30340;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36127;&#36131;&#20219;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;AI&#20262;&#29702;&#28041;&#21450;&#23558;&#20262;&#29702;&#21407;&#21017;&#24212;&#29992;&#20110;AI&#31995;&#32479;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#12290;&#36825;&#23545;&#20110;&#20943;&#36731;&#19982;AI&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#20260;&#23475;&#65288;&#22914;&#31639;&#27861;&#20559;&#35265;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#65288;RDPs&#65289;&#23545;&#20110;&#30830;&#20445;&#20262;&#29702;&#21644;&#20844;&#24179;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;RDPs&#32435;&#20837;ML&#27969;&#31243;&#20013;&#65292;&#20197;&#20943;&#36731;&#39118;&#38505;&#24182;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#36890;&#36807;&#23545;AI&#20262;&#29702;&#21644;&#25968;&#25454;&#31649;&#29702;&#19987;&#23478;&#30340;&#35843;&#26597;&#30830;&#23450;&#65292;&#24182;&#36890;&#36807;&#19987;&#23478;&#21453;&#39304;&#30340;&#23454;&#38469;&#24773;&#20917;&#36827;&#34892;&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating ethical practices into the AI development process for artificial intelligence (AI) is essential to ensure safe, fair, and responsible operation. AI ethics involves applying ethical principles to the entire life cycle of AI systems. This is essential to mitigate potential risks and harms associated with AI, such as algorithm biases. To achieve this goal, responsible design patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee ethical and fair outcomes. In this paper, we propose a comprehensive framework incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical development of AI systems. Our framework comprises new responsible AI design patterns for ML pipelines identified through a survey of AI ethics and data management experts and validated through real-world scenarios with expert feedback. The framework guides AI developers, data scientists, and policy-makers to implement ethical practices in AI development and deploy responsibl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#24418;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#26469;&#32852;&#21512;&#20248;&#21270;&#23646;&#24615;&#20197;&#33719;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;LECI&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01103</link><description>&lt;p&gt;
&#22312;&#22270;&#24418;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#20013;&#23398;&#20064;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization. (arXiv:2306.01103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#24418;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#26469;&#32852;&#21512;&#20248;&#21270;&#23646;&#24615;&#20197;&#33719;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;LECI&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22270;&#24418;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22270;&#24418;OOD&#31639;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#21463;&#38480;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#26080;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21516;&#26102;&#32435;&#20837;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#65288;LECI&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#21644;&#29615;&#22659;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#22240;&#26524;&#21644;&#19981;&#21464;&#23376;&#22270;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#32852;&#21512;&#20248;&#21270;&#36825;&#20004;&#20010;&#23646;&#24615;&#65292;&#29992;&#20110;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#23548;&#33268;&#23376;&#22270;&#21457;&#29616;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;LECI&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#37117;&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#23558;LECI&#30830;&#31435;&#20026;&#22270;&#24418;OOD&#27867;&#21270;&#30340;&#23454;&#29992;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for casual subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20010;&#20154;&#20559;&#22909;&#30340;&#32622;&#20449;&#24230;&#26500;&#36896;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#32622;&#20449;&#24230;&#23545;&#20110;&#20915;&#31574;&#32773;&#20449;&#20219;&#20915;&#31574;&#30340;&#19981;&#20934;&#30830;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.00074</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#40784;&#26657;&#20934;&#29992;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20010;&#20154;&#20559;&#22909;&#30340;&#32622;&#20449;&#24230;&#26500;&#36896;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#32622;&#20449;&#24230;&#23545;&#20110;&#20915;&#31574;&#32773;&#20449;&#20219;&#20915;&#31574;&#30340;&#19981;&#20934;&#30830;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#26102;&#65292;&#23427;&#36890;&#24120;&#25552;&#20379;&#26631;&#31614;&#39044;&#27979;&#21644;&#32622;&#20449;&#24230;&#20540;&#12290;&#28982;&#21518;&#65292;&#20915;&#31574;&#32773;&#24212;&#20351;&#29992;&#32622;&#20449;&#24230;&#20540;&#26469;&#26657;&#20934;&#23545;&#39044;&#27979;&#30340;&#20449;&#20219;&#31243;&#24230;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#32463;&#24120;&#35748;&#20026;&#32622;&#20449;&#24230;&#20540;&#24212;&#23545;&#39044;&#27979;&#26631;&#31614;&#19982;&#23454;&#38469;&#26631;&#31614;&#21305;&#37197;&#30340;&#27010;&#29575;&#36827;&#34892;&#33391;&#22909;&#26657;&#20934;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22810;&#26465;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#20915;&#31574;&#32773;&#38590;&#20197;&#20351;&#29992;&#36825;&#20123;&#32622;&#20449;&#24230;&#20540;&#24456;&#22909;&#22320;&#30830;&#23450;&#20309;&#26102;&#20449;&#20219;&#39044;&#27979;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#39318;&#20808;&#26159;&#29702;&#35299;&#20026;&#20160;&#20040;&#65292;&#28982;&#21518;&#30740;&#31350;&#22914;&#20309;&#26500;&#24314;&#26356;&#26377;&#29992;&#30340;&#32622;&#20449;&#24230;&#20540;&#12290;&#25105;&#20204;&#39318;&#20808;&#35748;&#20026;&#65292;&#22312;&#24191;&#27867;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20013;&#65292;&#23384;&#22312;&#25968;&#25454;&#20998;&#24067;&#65292;&#23545;&#20110;&#36825;&#20123;&#20998;&#24067;&#65292;&#29702;&#24615;&#20915;&#31574;&#32773;&#36890;&#24120;&#38590;&#20197;&#20351;&#29992;&#20197;&#19978;&#32622;&#20449;&#24230;&#20540;&#21457;&#29616;&#26368;&#20339;&#20915;&#31574;&#25919;&#31574;&#8212;&#8212;&#26368;&#20339;&#30340;&#20915;&#31574;&#32773;&#38656;&#35201;&#20154;&#31867;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20182;&#20204;&#22312;&#25152;&#38754;&#20020;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#20915;&#31574;&#19978;&#30340;&#20010;&#20154;&#20559;&#22909;&#30340;&#26032;&#26041;&#27861;&#26469;&#26500;&#36896;&#32622;&#20449;&#24230;&#20540;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#30340;&#32622;&#20449;&#24230;&#20540;&#27604;&#20351;&#29992;&#26631;&#20934;&#32622;&#20449;&#24230;&#24230;&#37327;&#23548;&#33268;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker wou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26657;&#20934;&#39044;&#27979;&#30446;&#26631;&#8212;&#8212;parity&#26657;&#20934;&#65292;&#20854;&#32771;&#34385;&#26102;&#38388;&#24207;&#21015;&#20013;&#26410;&#26469;&#35266;&#27979;&#20540;&#30340;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;&#20108;&#36827;&#21046;&#26657;&#20934;&#26041;&#27861;&#23454;&#29616;&#20102;parity&#26657;&#20934;&#65292;&#24182;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#22825;&#27668;&#39044;&#25253;&#21644;&#26680;&#32858;&#21464;&#25511;&#21046;&#31561;&#39046;&#22495;&#20013;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18655</link><description>&lt;p&gt;
Parity&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Parity Calibration. (arXiv:2305.18655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26657;&#20934;&#39044;&#27979;&#30446;&#26631;&#8212;&#8212;parity&#26657;&#20934;&#65292;&#20854;&#32771;&#34385;&#26102;&#38388;&#24207;&#21015;&#20013;&#26410;&#26469;&#35266;&#27979;&#20540;&#30340;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;&#20108;&#36827;&#21046;&#26657;&#20934;&#26041;&#27861;&#23454;&#29616;&#20102;parity&#26657;&#20934;&#65292;&#24182;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#22825;&#27668;&#39044;&#25253;&#21644;&#26680;&#32858;&#21464;&#25511;&#21046;&#31561;&#39046;&#22495;&#20013;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#21015;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#20915;&#31574;&#32773;&#21487;&#33021;&#26356;&#20851;&#27880;&#26410;&#26469;&#35266;&#27979;&#20540;&#26159;&#21542;&#27604;&#24403;&#21069;&#20540;&#22686;&#21152;&#25110;&#20943;&#23569;&#65292;&#32780;&#19981;&#26159;&#26410;&#26469;&#35266;&#27979;&#20540;&#30340;&#23454;&#38469;&#20540;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24179;&#31561;&#26657;&#20934;&#30340;&#27010;&#24565;&#65292;&#23427;&#25429;&#25417;&#20102;&#26102;&#38388;&#24207;&#21015;&#22686;&#20943;&#20107;&#20214;&#30340;&#26657;&#20934;&#39044;&#27979;&#30446;&#26631;&#12290;&#24179;&#31561;&#27010;&#29575;&#21487;&#20197;&#20174;&#36755;&#20986;&#30340;&#39044;&#27979;&#20998;&#24067;&#20013;&#25552;&#21462;&#65292;&#20294;&#25105;&#20204;&#26174;&#31034;&#36825;&#31181;&#31574;&#30053;&#23548;&#33268;&#29702;&#35770;&#19978;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#21644;&#24046;&#21170;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;&#28982;&#21518;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#21407;&#20219;&#21153;&#26159;&#22238;&#24402;&#65292;&#20294;&#24179;&#31561;&#26657;&#20934;&#21487;&#20197;&#34987;&#34920;&#36798;&#20026;&#20108;&#36827;&#21046;&#26657;&#20934;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;&#20108;&#36827;&#21046;&#26657;&#20934;&#26041;&#27861;&#23454;&#29616;&#20102;&#24179;&#31561;&#26657;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#27969;&#34892;&#30149;&#23398;&#12289;&#22825;&#27668;&#39044;&#25253;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26680;&#32858;&#21464;&#25511;&#21046;&#30340;&#23454;&#38469;&#26696;&#20363;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a sequential regression setting, a decision-maker may be primarily concerned with whether the future observation will increase or decrease compared to the current one, rather than the actual value of the future observation. In this context, we introduce the notion of parity calibration, which captures the goal of calibrated forecasting for the increase-decrease (or "parity") event in a timeseries. Parity probabilities can be extracted from a forecasted distribution for the output, but we show that such a strategy leads to theoretical unpredictability and poor practical performance. We then observe that although the original task was regression, parity calibration can be expressed as binary calibration. Drawing on this connection, we use an online binary calibration method to achieve parity calibration. We demonstrate the effectiveness of our approach on real-world case studies in epidemiology, weather forecasting, and model-based control in nuclear fusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18486</link><description>&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#31995;&#32479;&#30740;&#31350;&#21644;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#23558;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20135;&#20986;&#19982;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#65292;&#22240;&#27492;&#20854;&#22312;&#22522;&#20934;&#23398;&#26415;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545; ChatGPT &#22312;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; 140 &#20010;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102; ChatGPT&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#30340; 255K &#27425;&#21709;&#24212;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#20026;&#20102;&#22312; NLP &#22522;&#20934;&#27979;&#35797;&#20013;&#23545; ChatGPT &#36827;&#34892;&#30340;&#26368;&#22823;&#35780;&#20272;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992; LLM &#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#31181;&#26032;&#30340;&#36856;&#21457;&#33021;&#21147;&#65292;&#21363;&#36981;&#24490;&#22810;&#20010;&#26597;&#35810;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.17303</link><description>&lt;p&gt;
&#20174;&#40657;&#30418;&#27169;&#22411;&#21040;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36716;&#21270;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;AI&#27169;&#22411;&#26159;&#21307;&#30103;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#21363;&#20351;&#36755;&#20837;&#20998;&#24067;&#36731;&#24494;&#31227;&#20301;&#65288;&#20363;&#22914;&#25195;&#25551;&#20202;&#31867;&#22411;&#65289;&#65292;&#20063;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#32780;&#25918;&#23556;&#31185;&#21307;&#29983;&#21017;&#20381;&#36182;&#20110;&#24322;&#24120;&#24615;&#30340;&#36890;&#29992;&#25551;&#36848;&#24615;&#35268;&#21017;&#12290;&#24494;&#35843;&#27169;&#22411;&#20197;&#23558;&#30693;&#35782;&#20174;&#19968;&#20010;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#38024;&#23545;&#26410;&#30693;&#30340;&#30446;&#26631;&#22495;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35748;&#20026;NN&#30340;&#21487;&#35299;&#37322;&#32452;&#20214;&#22823;&#33268;&#26159;&#22495;&#19981;&#21464;&#30340;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#19981;&#21450;&#23427;&#20204;&#30340;BB&#21464;&#20307;&#12290;&#22312;&#28304;&#22495;&#20013;&#25105;&#20204;&#20808;&#20351;&#29992;&#20154;&#31867;&#29702;&#35299;&#30340;&#27010;&#24565;&#20174;BB&#24320;&#22987;&#65292;&#23558;&#20854;&#25552;&#28860;&#25104;&#19968;&#32452;&#27973;&#26174;&#26131;&#25026;&#30340;interpretable&#27169;&#22411;&#12290;&#30001;&#20110;&#27599;&#20010;interpretable&#27169;&#22411;&#37117;&#35206;&#30422;&#20102;&#25968;&#25454;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20855;&#26377;&#19968;&#32452;interpretable&#27169;&#22411;&#30340;&#28151;&#21512;&#21487;&#20197;&#23454;&#29616;&#19982;BB&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#25968;&#20540;&#26041;&#26696;&#31283;&#23450;&#24615;&#29305;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21152;&#20837;&#20102;&#30828;&#24615;&#32422;&#26463;&#26469;&#20445;&#35777;&#20854;&#26435;&#37325;&#31283;&#23450;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#38271;&#26399;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17155</link><description>&lt;p&gt;
&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#38271;&#26399;&#39044;&#27979;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stability of implicit neural networks for long-term forecasting in dynamical systems. (arXiv:2305.17155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#25968;&#20540;&#26041;&#26696;&#31283;&#23450;&#24615;&#29305;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21152;&#20837;&#20102;&#30828;&#24615;&#32422;&#26463;&#26469;&#20445;&#35777;&#20854;&#26435;&#37325;&#31283;&#23450;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#38271;&#26399;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#29289;&#29702;&#20449;&#21495;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#30740;&#31350;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20026;&#20102;&#35268;&#36991;&#20256;&#32479;&#27714;&#35299;&#22120;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#20204;&#37117;&#22522;&#20110;&#33258;&#22238;&#24402;&#26041;&#27861;&#24182;&#23637;&#31034;&#20986;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#21463;&#38544;&#24335;&#25968;&#20540;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#29305;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31283;&#23450;&#30340;&#33258;&#22238;&#24402;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#26681;&#25454;&#25968;&#20540;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#23450;&#20041;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#29702;&#35770;&#26469;&#20445;&#35777;&#32593;&#32476;&#39044;&#27979;&#30340;&#31283;&#23450;&#24615;&#12290;&#23427;&#23548;&#33268;&#25105;&#20204;&#23545;&#20854;&#26435;&#37325;&#28155;&#21152;&#20102;&#30828;&#24615;&#32422;&#26463;&#65292;&#24182;&#22312;&#28508;&#31354;&#38388;&#20013;&#20256;&#25773;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#31283;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#36755;&#36816;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38271;&#26399;&#39044;&#27979;&#19978;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting physical signals in long time range is among the most challenging tasks in Partial Differential Equations (PDEs) research. To circumvent limitations of traditional solvers, many different Deep Learning methods have been proposed. They are all based on auto-regressive methods and exhibit stability issues. Drawing inspiration from the stability property of implicit numerical schemes, we introduce a stable auto-regressive implicit neural network. We develop a theory based on the stability definition of schemes to ensure the stability in forecasting of this network. It leads us to introduce hard constraints on its weights and propagate the dynamics in the latent space. Our experimental results validate our stability property, and show improved results at long-term forecasting for two transports PDEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;ParaDiGMS&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#22810;&#20010;&#27493;&#39588;&#26469;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#12290;ParaDiGMS&#26159;&#31532;&#19968;&#20010;&#20351;&#35745;&#31639;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#23454;&#29616;&#24179;&#34913;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.16317</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#24182;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Parallel Sampling of Diffusion Models. (arXiv:2305.16317v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;ParaDiGMS&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#22810;&#20010;&#27493;&#39588;&#26469;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#12290;ParaDiGMS&#26159;&#31532;&#19968;&#20010;&#20351;&#35745;&#31639;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#23454;&#29616;&#24179;&#34913;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#37319;&#26679;&#36895;&#24230;&#32531;&#24930;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;1000&#27425;&#39034;&#24207;&#21435;&#22122;&#27493;&#39588;&#25165;&#33021;&#24471;&#21040;&#19968;&#20010;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#20132;&#25442;&#35745;&#31639;&#26426;&#22788;&#29702;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29468;&#27979;&#26410;&#26469;&#30340;&#21435;&#22122;&#27493;&#39588;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#36880;&#27493;&#32454;&#21270;&#33267;&#25910;&#25947;&#30340;Picard&#36845;&#20195;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#21457;&#29616;&#65306;&#23613;&#31649;&#21435;&#22122;&#27493;&#39588;&#26377;&#39034;&#24207;&#24615;&#65292;&#20294;&#20173;&#28982;&#21487;&#20197;&#24182;&#34892;&#37319;&#26679;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ParaDiGMS&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20197;&#24182;&#34892;&#26041;&#24335;&#21435;&#22122;&#22810;&#20010;&#27493;&#39588;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;&#12290;ParaDiGMS&#26159;&#31532;&#19968;&#20010;&#22312;&#35745;&#31639;&#22788;&#29702;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#23454;&#29616;&#24179;&#34913;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#29978;&#33267;&#36824;&#20860;&#23481;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20154;&#31867;&#35821;&#38899;&#33258;&#30417;&#30563;&#31070;&#32463;&#34920;&#31034;&#23398;&#20064;&#26469;&#20998;&#26512;&#29983;&#29289;&#22768;&#23398;&#20449;&#21495;&#30340;&#20132;&#21449;&#21487;&#36801;&#31227;&#24615;&#65292;&#22312;&#29416;&#29492;&#21483;&#22768;&#35782;&#21035;&#21644;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#26410;&#26469;&#35813;&#26041;&#27861;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.14035</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#31070;&#32463;&#34920;&#31034;&#26159;&#21542;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#20154;&#31867;&#35821;&#38899;&#21518;&#21306;&#20998;&#21160;&#29289;&#21628;&#21483;&#32773;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Self-Supervised Neural Representations Pre-Trained on Human Speech distinguish Animal Callers?. (arXiv:2305.14035v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20154;&#31867;&#35821;&#38899;&#33258;&#30417;&#30563;&#31070;&#32463;&#34920;&#31034;&#23398;&#20064;&#26469;&#20998;&#26512;&#29983;&#29289;&#22768;&#23398;&#20449;&#21495;&#30340;&#20132;&#21449;&#21487;&#36801;&#31227;&#24615;&#65292;&#22312;&#29416;&#29492;&#21483;&#22768;&#35782;&#21035;&#21644;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#26410;&#26469;&#35813;&#26041;&#27861;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20165;&#20351;&#29992;&#32473;&#23450;&#20449;&#21495;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#29420;&#31435;&#20110;&#22768;&#23398;&#39046;&#22495;&#65292;&#23558;&#20449;&#21495;&#36716;&#25442;&#20026;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#22522;&#26412;&#20449;&#24687;&#12290;&#36825;&#24847;&#21619;&#30528;&#36825;&#20123;&#34920;&#31034;&#30340;&#25928;&#29992;&#19981;&#20165;&#23616;&#38480;&#20110;&#20154;&#31867;&#35821;&#38899;&#24314;&#27169;&#12290;&#26412;&#25991;&#22522;&#20110;&#27492;&#25506;&#35752;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#20154;&#31867;&#35821;&#38899;&#33258;&#30417;&#30563;&#31070;&#32463;&#34920;&#31034;&#23398;&#20064;&#26469;&#20998;&#26512;&#29983;&#29289;&#22768;&#23398;&#20449;&#21495;&#30340;&#20132;&#21449;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;11&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#23545;&#21483;&#22768;&#36827;&#34892;&#36776;&#21035;&#21644;&#26816;&#27979;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#23884;&#20837;&#31354;&#38388;&#21253;&#21547;&#26377;&#24847;&#20041;&#30340;&#21628;&#21483;&#32773;&#20449;&#24687;&#65292;&#21487;&#25104;&#21151;&#21306;&#20998;&#19981;&#21516;&#29416;&#29492;&#21483;&#30340;&#20010;&#20307;&#36523;&#20221;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#36825;&#34920;&#26126;&#22312;&#29983;&#29289;&#22768;&#23398;&#39046;&#22495;&#65292;&#39044;&#20808;&#35757;&#32451;&#20110;&#20154;&#31867;&#35821;&#38899;&#30340;&#34920;&#31034;&#33021;&#22815;&#34987;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#35813;&#39046;&#22495;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) models use only the intrinsic structure of a given signal, independent of its acoustic domain, to extract essential information from the input to an embedding space. This implies that the utility of such representations is not limited to modeling human speech alone. Building on this understanding, this paper explores the cross-transferability of SSL neural representations learned from human speech to analyze bio-acoustic signals. We conduct a caller discrimination analysis and a caller detection study on Marmoset vocalizations using eleven SSL models pre-trained with various pretext tasks. The results show that the embedding spaces carry meaningful caller information and can successfully distinguish the individual identities of Marmoset callers without fine-tuning. This demonstrates that representations pre-trained on human speech can be effectively applied to the bio-acoustics domain, providing valuable insights for future investigations in this field.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HDR&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21382;&#21490;&#20419;&#38144;&#25968;&#25454;&#65292;&#26469;&#25429;&#25417;&#20419;&#38144;&#36716;&#21270;&#27169;&#24335;&#65292;&#36798;&#21040;&#26356;&#22909;&#22320;&#36866;&#24212;&#20419;&#38144;&#27169;&#24335;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.12837</link><description>&lt;p&gt;
&#25429;&#25417;&#20419;&#38144;&#26399;&#38388;&#30340;&#36716;&#21270;&#29575;&#27874;&#21160;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#21382;&#21490;&#25968;&#25454;&#20877;&#21033;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach. (arXiv:2305.12837v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HDR&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21382;&#21490;&#20419;&#38144;&#25968;&#25454;&#65292;&#26469;&#25429;&#25417;&#20419;&#38144;&#36716;&#21270;&#27169;&#24335;&#65292;&#36798;&#21040;&#26356;&#22909;&#22320;&#36866;&#24212;&#20419;&#38144;&#27169;&#24335;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#39044;&#27979;&#26159;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#32452;&#20214;&#20043;&#19968;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;CVR&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#35757;&#32451;&#33391;&#22909;&#30340;CVR&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#20419;&#38144;&#26399;&#38388;&#20063;&#32463;&#24120;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#20854;&#20013;&#20256;&#32479;&#26041;&#27861;&#19981;&#20877;&#36215;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23547;&#27714;&#24320;&#21457;&#26367;&#20195;&#24314;&#27169;&#25216;&#26415;&#29992;&#20110;CVR&#39044;&#27979;&#12290;&#35266;&#23519;&#21040;&#19981;&#21516;&#20419;&#38144;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#30340;&#36141;&#20080;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#29992;&#21382;&#21490;&#20419;&#38144;&#25968;&#25454;&#20197;&#25429;&#25417;&#20419;&#38144;&#36716;&#21270;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21382;&#21490;&#25968;&#25454;&#20877;&#21033;&#29992;&#65288;HDR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#26816;&#32034;&#21382;&#21490;&#19978;&#30456;&#20284;&#30340;&#20419;&#38144;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#33719;&#21462;&#30340;&#25968;&#25454;&#24494;&#35843;CVR&#39044;&#27979;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#20419;&#38144;&#27169;&#24335;&#12290;HDR&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#33258;&#21160;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Conversion rate (CVR) prediction is one of the core components in online recommender systems, and various approaches have been proposed to obtain accurate and well-calibrated CVR estimation. However, we observe that a well-trained CVR prediction model often performs sub-optimally during sales promotions. This can be largely ascribed to the problem of the data distribution shift, in which the conventional methods no longer work. To this end, we seek to develop alternative modeling techniques for CVR prediction. Observing similar purchase patterns across different promotions, we propose reusing the historical promotion data to capture the promotional conversion patterns. Herein, we propose a novel \textbf{H}istorical \textbf{D}ata \textbf{R}euse (\textbf{HDR}) approach that first retrieves historically similar promotion data and then fine-tunes the CVR prediction model with the acquired data for better adaptation to the promotion mode. HDR consists of three components: an automated data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RGCVAE&#65292;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#22270;&#26465;&#20214;&#21270;&#30340;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#36827;&#34892;&#20998;&#23376;&#35774;&#35745;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20808;&#36827;&#30340;&#29983;&#25104;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11699</link><description>&lt;p&gt;
RGCVAE&#65306;&#22522;&#20110;&#20851;&#31995;&#22270;&#26465;&#20214;&#21270;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#30340;&#20998;&#23376;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
RGCVAE: Relational Graph Conditioned Variational Autoencoder for Molecule Design. (arXiv:2305.11699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RGCVAE&#65292;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#22270;&#26465;&#20214;&#21270;&#30340;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#36827;&#34892;&#20998;&#23376;&#35774;&#35745;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20808;&#36827;&#30340;&#29983;&#25104;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#34920;&#29616;&#20986;&#26576;&#20123;&#39044;&#23450;&#29305;&#24615;&#30340;&#20998;&#23376;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#12290;&#28145;&#24230;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26159;&#26368;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20043;&#19968;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#25429;&#25417;&#30495;&#23454;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#19988;&#20542;&#21521;&#20110;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#22522;&#20110;&#20851;&#31995;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;RGCVAE&#65306;&#65288;i&#65289;&#21033;&#29992;&#20840;&#26032;&#30340;&#24378;&#22823;&#20851;&#31995;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#32534;&#30721;&#32593;&#32476;&#65307;&#65288;ii&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35299;&#30721;&#32452;&#20214;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#25968;&#31181;&#26368;&#20808;&#36827;&#30340;VAE&#26041;&#27861;&#30456;&#27604;&#65292;RGCVAE&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#20998;&#23376;&#29983;&#25104;&#24615;&#33021;&#65292;&#21516;&#26102;&#35757;&#32451;&#36895;&#24230;&#26174;&#33879;&#21152;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying molecules that exhibit some pre-specified properties is a difficult problem to solve. In the last few years, deep generative models have been used for molecule generation. Deep Graph Variational Autoencoders are among the most powerful machine learning tools with which it is possible to address this problem. However, existing methods struggle in capturing the true data distribution and tend to be computationally expensive. In this work, we propose RGCVAE, an efficient and effective Graph Variational Autoencoder based on: (i) an encoding network exploiting a new powerful Relational Graph Isomorphism Network; (ii) a novel probabilistic decoding component. Compared to several state-of-the-art VAE methods on two widely adopted datasets, RGCVAE shows state-of-the-art molecule generation performance while being significantly faster to train.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#25252;&#20080;&#23478;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#37325;&#22797;&#12289;&#19981;&#38480;&#20379;&#24212;&#29289;&#21697;&#25293;&#21334;&#20013;&#30340;&#25910;&#30410;&#26368;&#22823;&#21270;&#65292;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#30340;$O(\sqrt{T}\log{T})$&#20111;&#25439;&#23376;&#32447;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11362</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#29289;&#21697;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Online Item Pricing. (arXiv:2305.11362v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#25252;&#20080;&#23478;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#37325;&#22797;&#12289;&#19981;&#38480;&#20379;&#24212;&#29289;&#21697;&#25293;&#21334;&#20013;&#30340;&#25910;&#30410;&#26368;&#22823;&#21270;&#65292;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#30340;$O(\sqrt{T}\log{T})$&#20111;&#25439;&#23376;&#32447;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20445;&#25252;&#20080;&#26041;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#37325;&#22797;&#12289;&#19981;&#38480;&#20379;&#24212;&#29289;&#21697;&#25293;&#21334;&#20013;&#30340;&#25910;&#30410;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#23427;&#19982;&#20080;&#26041;&#30340;&#36755;&#20837;&#23545;&#65288;&#21830;&#21697;&#36873;&#25321;&#21644;&#20986;&#20215;&#65289;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#30340;$O(\sqrt{T}\log{T})$&#20111;&#25439;&#23376;&#32447;&#24615;(regret)&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25351;&#25968;&#26435;&#37325;&#20803;&#31639;&#27861;&#65292;&#36890;&#36807;&#23567;&#30340;&#38543;&#26426;&#25200;&#21160;&#32531;&#35299;&#20102;&#25910;&#30410;&#20989;&#25968;&#19981;&#36830;&#32493;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#19982;&#25351;&#25968;&#26426;&#21046;&#30340;&#32467;&#26500;&#30456;&#20284;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#22266;&#26377;&#22320;&#20445;&#35777;&#20102;&#24046;&#20998;&#38544;&#31169;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#25193;&#23637;&#21040;&#36880;&#36718;&#31574;&#30053;&#24615;&#20986;&#20215;&#30340;&#24773;&#20917;&#12290;&#20869;&#22312;&#30340;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26368;&#23567;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#30830;&#20445;&#20854;&#20111;&#25439;&#23376;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the problem of revenue maximization in a repeated, unlimited supply item-pricing auction while preserving buyer privacy. We present a novel algorithm that provides differential privacy with respect to the buyer's input pair: item selection and bid. Notably, our algorithm is the first to offer a sublinear $O(\sqrt{T}\log{T})$ regret with a privacy guarantee. Our method is based on an exponential weights meta-algorithm, and we mitigate the issue of discontinuities in revenue functions via small random perturbations. As a result of its structural similarity to the exponential mechanism, our method inherently secures differential privacy. We also extend our algorithm to accommodate scenarios where buyers strategically bid over successive rounds. The inherent differential privacy allows us to adapt our algorithm with minimal modification to ensure a sublinear regret in this setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#21608;&#26399;&#24615;&#23545;&#25239;&#24615;&#32447;&#24615;MDP&#65292;&#22312;&#38543;&#26426;&#32447;&#24615;MDP&#21644;&#24102;&#26377;&#20840;&#20449;&#24687;&#30340;&#25932;&#23545;&#32447;&#24615;MDP&#20013;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#21518;&#24724;&#36793;&#30028;&#65292;&#24182;&#20855;&#26377;&#26032;&#39062;&#30340;&#22810;&#25209;&#27425;&#26356;&#26032;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.08841</link><description>&lt;p&gt;
&#20048;&#35266;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#22312;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes. (arXiv:2305.08841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#21608;&#26399;&#24615;&#23545;&#25239;&#24615;&#32447;&#24615;MDP&#65292;&#22312;&#38543;&#26426;&#32447;&#24615;MDP&#21644;&#24102;&#26377;&#20840;&#20449;&#24687;&#30340;&#25932;&#23545;&#32447;&#24615;MDP&#20013;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#21518;&#24724;&#36793;&#30028;&#65292;&#24182;&#20855;&#26377;&#26032;&#39062;&#30340;&#22810;&#25209;&#27425;&#26356;&#26032;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#31639;&#27861;&#26159;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#25104;&#21151;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#23613;&#31649;PPO&#24456;&#25104;&#21151;&#65292;&#20294;&#26159;&#23545;&#20110;PPO&#21450;&#20854;&#20048;&#35266;&#21464;&#31181;&#26159;&#21542;&#33021;&#26377;&#25928;&#35299;&#20915;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDPs)&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#24102;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#21608;&#26399;&#25932;&#23545;&#32447;&#24615;MDP&#65292;&#24182;&#20026;&#20854;&#24314;&#31435;&#20102;&#19968;&#20010;$\tilde{\mathcal{O}}(d^{3/4}H^2K^{3/4})$&#30340;&#21518;&#24724;&#20540;&#12290;&#20854;&#20013;$d$&#26159;&#32447;&#24615;MDPs&#30340;&#29615;&#22659;&#32500;&#25968;&#65292;$H$&#26159;&#27599;&#20010;&#21608;&#26399;&#30340;&#38271;&#24230;&#65292;$K$&#26159;&#21608;&#26399;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#38543;&#26426;&#32447;&#24615;MDP&#21644;&#24102;&#26377;&#20840;&#20449;&#24687;&#30340;&#25932;&#23545;&#32447;&#24615;MDP&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#24403;&#20170;&#26368;&#20808;&#36827;&#30340;&#21518;&#24724;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#35774;&#35745;&#20855;&#26377;&#26032;&#39062;&#30340;&#22810;&#25209;&#27425;&#26356;&#26032;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proximal policy optimization (PPO) algorithm stands as one of the most prosperous methods in the field of reinforcement learning (RL). Despite its success, the theoretical understanding of PPO remains deficient. Specifically, it is unclear whether PPO or its optimistic variants can effectively solve linear Markov decision processes (MDPs), which are arguably the simplest models in RL with function approximation. To bridge this gap, we propose an optimistic variant of PPO for episodic adversarial linear MDPs with full-information feedback, and establish a $\tilde{\mathcal{O}}(d^{3/4}H^2K^{3/4})$ regret for it. Here $d$ is the ambient dimension of linear MDPs, $H$ is the length of each episode, and $K$ is the number of episodes. Compared with existing policy-based algorithms, we achieve the state-of-the-art regret bound in both stochastic linear MDPs and adversarial linear MDPs with full information. Additionally, our algorithm design features a novel multi-batched updating mechanism
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#40657;&#30418;&#27169;&#22411;&#22312;&#25935;&#24863;&#23376;&#32676;&#20307;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#33258;&#20030;&#26041;&#27861;&#38480;&#21046;&#22810;&#20010;&#32676;&#20307;&#34920;&#29616;&#24046;&#24322;&#65292;&#26041;&#27861;&#36890;&#29992;&#24615;&#24378;&#19988;&#36866;&#29992;&#38754;&#24191;&#12290;</title><link>http://arxiv.org/abs/2305.03712</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical Inference for Fairness Auditing. (arXiv:2305.03712v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#40657;&#30418;&#27169;&#22411;&#22312;&#25935;&#24863;&#23376;&#32676;&#20307;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#33258;&#20030;&#26041;&#27861;&#38480;&#21046;&#22810;&#20010;&#32676;&#20307;&#34920;&#29616;&#24046;&#24322;&#65292;&#26041;&#27861;&#36890;&#29992;&#24615;&#24378;&#19988;&#36866;&#29992;&#38754;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#40657;&#30418;&#27169;&#22411;&#29992;&#20110;&#39640;&#39118;&#38505;&#38382;&#39064;&#20043;&#21069;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#25935;&#24863;&#23376;&#32676;&#20307;&#19978;&#30340;&#34920;&#29616;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20844;&#24179;&#24615;&#23457;&#35745;&#8221;&#30340;&#20219;&#21153;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#23558;&#20854;&#34920;&#36848;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#33258;&#20030;&#26041;&#27861;&#20197;&#32479;&#35745;&#20445;&#35777;&#30340;&#26041;&#24335;&#21516;&#26102;&#38480;&#21046;&#22810;&#20010;&#32676;&#20307;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20960;&#20046;&#20219;&#20309;&#24615;&#33021;&#24230;&#37327;&#25110;&#32676;&#20307;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#38750;&#24120;&#20016;&#23500;&#30340;&#12289;&#29978;&#33267;&#26159;&#26080;&#38480;&#30340;&#23376;&#32676;&#20307;&#38598;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#25512;&#24191;&#21040;&#20102;&#22810;&#20010;&#28508;&#22312;&#37325;&#21472;&#26631;&#20934;&#19979;&#30340;&#27169;&#22411;&#34920;&#29616;&#23457;&#35745;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Before deploying a black-box model in high-stakes problems, it is important to evaluate the model's performance on sensitive subpopulations. For example, in a recidivism prediction task, we may wish to identify demographic groups for which our prediction model has unacceptably high false positive rates or certify that no such groups exist. In this paper, we frame this task, often referred to as "fairness auditing," in terms of multiple hypothesis testing. We show how the bootstrap can be used to simultaneously bound performance disparities over a collection of groups with statistical guarantees. Our methods can be used to flag subpopulations affected by model underperformance, and certify subpopulations for which the model performs adequately. Crucially, our audit is model-agnostic and applicable to nearly any performance metric or group fairness criterion. Our methods also accommodate extremely rich -- even infinite -- collections of subpopulations. Further, we generalize beyond subpo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#29702;&#24819;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32479;&#19968;&#20102;&#22810;&#20010;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20026;&#36825;&#20123;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.00316</link><description>&lt;p&gt;
&#29702;&#24819;&#30340;&#19981;&#26029;&#23398;&#20064;&#32773;: &#19981;&#20250;&#36951;&#24536;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
The Ideal Continual Learner: An Agent That Never Forgets. (arXiv:2305.00316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#29702;&#24819;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32479;&#19968;&#20102;&#22810;&#20010;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20026;&#36825;&#20123;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#27169;&#22411;&#65292;&#35299;&#20915;&#25353;&#39034;&#24207;&#21576;&#29616;&#32473;&#23398;&#20064;&#32773;&#30340;&#22810;&#20010;&#23398;&#20064;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#65292;&#24403;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#23398;&#20064;&#32773;&#21487;&#33021;&#20250;&#24536;&#35760;&#22914;&#20309;&#35299;&#20915;&#20808;&#21069;&#30340;&#20219;&#21153;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#35760;&#24518;&#12289;&#22522;&#20110;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#25193;&#23637;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#20005;&#26684;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#29702;&#24819;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;&#65288;ICL&#65289;&#65292;&#20174;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#40511;&#27807;&#20013;&#36328;&#36234;&#36807;&#21435;&#65292;ICL&#30340;&#26500;&#24314;&#20445;&#35777;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32479;&#19968;&#20102;&#22810;&#20010;&#25104;&#29087;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20026;&#36825;&#20123;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#20026;ICL&#25512;&#23548;&#20986;&#20102;&#27867;&#21270;&#30028;&#38480;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29702;&#35770;&#19978;&#37327;&#21270;&#22914;&#20309;&#25511;&#21046;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#26469;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of continual learning is to find a model that solves multiple learning tasks which are presented sequentially to the learner. A key challenge in this setting is that the learner may forget how to solve a previous task when learning a new task, a phenomenon known as catastrophic forgetting. To address this challenge, many practical methods have been proposed, including memory-based, regularization-based, and expansion-based methods. However, a rigorous theoretical understanding of these methods remains elusive. This paper aims to bridge this gap between theory and practice by proposing a new continual learning framework called Ideal Continual Learner (ICL), which is guaranteed to avoid catastrophic forgetting by construction. We show that ICL unifies multiple well-established continual learning methods and gives new theoretical insights into the strengths and weaknesses of these methods. We also derive generalization bounds for ICL which allow us to theoretically quantify how r
&lt;/p&gt;</description></item><item><title>InstructCTG&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#28436;&#31034;&#26469;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#24182;&#28385;&#36275;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#25628;&#32034;&#25110;&#24471;&#20998;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14293</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14293
&lt;/p&gt;
&lt;p&gt;
InstructCTG&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#28436;&#31034;&#26469;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#24182;&#28385;&#36275;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#25628;&#32034;&#25110;&#24471;&#20998;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#24182;&#33021;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046;&#23427;&#20204;&#30340;&#29983;&#25104;&#20197;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#25152;&#38656;&#30340;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#32422;&#26463;&#35843;&#33410;&#30340;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#8212;&#8212;InstructCTG&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#32422;&#26463;&#28436;&#31034;&#26469;&#32435;&#20837;&#19981;&#21516;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#31995;&#21015;&#29616;&#25104;&#30340;NLP&#24037;&#20855;&#21644;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#21462;&#33258;&#28982;&#25991;&#26412;&#30340;&#28508;&#22312;&#32422;&#26463;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20197;&#24418;&#25104;&#24369;&#30417;&#30563;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#28155;&#21152;&#33258;&#28982;&#35821;&#35328;&#32422;&#26463;&#25551;&#36848;&#21644;&#23569;&#37327;&#28436;&#31034;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20197;&#32435;&#20837;&#21508;&#31181;&#31867;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;&#25628;&#32034;&#25110;&#24471;&#20998;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;InstructCTG &#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20854;&#22312;&#20248;&#21270;&#21518;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#36712;&#36857;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#38598;&#20559;&#32622;&#21644;&#22810;&#26679;&#24615;&#27604;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12579</link><description>&lt;p&gt;
&#23398;&#20064;&#36712;&#36857;&#26159;&#27867;&#21270;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning Trajectories are Generalization Indicators. (arXiv:2304.12579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20854;&#22312;&#20248;&#21270;&#21518;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#36712;&#36857;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#38598;&#20559;&#32622;&#21644;&#22810;&#26679;&#24615;&#27604;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20854;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20248;&#21270;&#26102;&#23545;&#24212;&#30340;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#32447;&#24615;&#36817;&#20284;&#20989;&#25968;&#26469;&#27169;&#25311;&#36712;&#36857;&#20449;&#24687;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26356;&#20016;&#23500;&#36712;&#36857;&#20449;&#24687;&#30340;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27867;&#21270;&#19978;&#30028;&#20381;&#36182;&#20110;&#23398;&#20064;&#36712;&#36857;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#35757;&#32451;&#38598;&#30340;&#20559;&#32622;&#21644;&#22810;&#26679;&#24615;&#27604;&#20043;&#38388;&#30340;&#27604;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#19981;&#21516;&#35757;&#32451;&#27493;&#39588;&#12289;&#23398;&#20064;&#29575;&#21644;&#26631;&#31614;&#22122;&#22768;&#27700;&#24179;&#19979;&#30340;&#27867;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to investigate the connection between learning trajectories of the Deep Neural Networks (DNNs) and their corresponding generalization capabilities when being optimized with broadly used gradient descent and stochastic gradient descent algorithms. In this paper, we construct Linear Approximation Function to model the trajectory information and we propose a new generalization bound with richer trajectory information based on it. Our proposed generalization bound relies on the complexity of learning trajectory and the ratio between the bias and diversity of training set. Experimental results indicate that the proposed method effectively captures the generalization trend across various training steps, learning rates, and label noise levels.
&lt;/p&gt;</description></item><item><title>SkinGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#36855;&#20320;GPT-4&#30340;&#31934;&#32454;&#35843;&#25972;&#29256;&#26412;&#21644;&#20869;&#37096;&#30382;&#32932;&#22270;&#20687;&#38598;&#21512;&#32467;&#21512;&#30340;&#30382;&#32932;&#31185;&#35786;&#26029;&#31995;&#32479;&#65292;&#21033;&#29992;&#39640;&#32423;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#30382;&#32932;&#31185;&#21307;&#29983;&#19981;&#36275;&#12289;&#20934;&#30830;&#35786;&#26029;&#30382;&#32932;&#31185;&#22270;&#29255;&#38590;&#24230;&#22823;&#65292;&#20197;&#21450;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#35786;&#26029;&#25253;&#21578;&#22256;&#38590;&#31561;&#19977;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.10691</link><description>&lt;p&gt;
SkinGPT: &#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30382;&#32932;&#31185;&#35786;&#26029;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model. (arXiv:2304.10691v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10691
&lt;/p&gt;
&lt;p&gt;
SkinGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#36855;&#20320;GPT-4&#30340;&#31934;&#32454;&#35843;&#25972;&#29256;&#26412;&#21644;&#20869;&#37096;&#30382;&#32932;&#22270;&#20687;&#38598;&#21512;&#32467;&#21512;&#30340;&#30382;&#32932;&#31185;&#35786;&#26029;&#31995;&#32479;&#65292;&#21033;&#29992;&#39640;&#32423;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#30382;&#32932;&#31185;&#21307;&#29983;&#19981;&#36275;&#12289;&#20934;&#30830;&#35786;&#26029;&#30382;&#32932;&#31185;&#22270;&#29255;&#38590;&#24230;&#22823;&#65292;&#20197;&#21450;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#35786;&#26029;&#25253;&#21578;&#22256;&#38590;&#31561;&#19977;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30382;&#32932;&#21644;&#30382;&#19979;&#30142;&#30149;&#26159;&#20840;&#29699;&#38750;&#33268;&#21629;&#30142;&#30149;&#36127;&#25285;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#24433;&#21709;&#20102;&#22823;&#37096;&#20998;&#20154;&#21475;&#12290;&#28982;&#32780;&#65292;&#30382;&#32932;&#31185;&#35786;&#26029;&#39046;&#22495;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#30382;&#32932;&#31185;&#21307;&#29983;&#19981;&#36275;&#12289;&#20934;&#30830;&#35786;&#26029;&#30382;&#32932;&#31185;&#22270;&#29255;&#38590;&#24230;&#22823;&#20197;&#21450;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#35786;&#26029;&#25253;&#21578;&#22256;&#38590;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#20102;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22270;&#20687;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#20351;&#29992;ChatGPT&#30340;API&#19978;&#20256;&#25968;&#25454;&#20250;&#23384;&#22312;&#28508;&#22312;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SkinGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#39640;&#32423;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30382;&#32932;&#31185;&#35786;&#26029;&#31995;&#32479;&#12290;SkinGPT&#26159;&#39318;&#20010;&#37319;&#29992;&#20869;&#37096;&#30382;&#32932;&#22270;&#20687;&#38598;&#21512;&#21644;&#36855;&#20320;GPT-4&#30340;&#31934;&#32454;&#35843;&#25972;&#29256;&#26412;&#32467;&#21512;&#32780;&#25104;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin and subcutaneous diseases are among the major causes of the nonfatal disease burden worldwide, affecting a significant proportion of the population. However, there are three major challenges in the field of dermatology diagnosis. Firstly, there is a shortage of dermatologists available to diagnose patients. Secondly, accurately diagnosing dermatological pictures can be challenging. Lastly, providing user-friendly diagnostic reports can be difficult. Recent advancements in the field of large language models (LLMs) have shown potential for clinical applications. However, current LLMs have difficulty processing images, and there are potential privacy concerns associated with using ChatGPT's API for uploading data. In this paper, we propose SkinGPT, which is the first dermatology diagnostic system that utilizes an advanced vision-based large language model. SkinGPT is the first system of its kind, incorporating a fine-tuned version of MiniGPT-4 with a vast collection of in-house skin 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#30693;&#21160;&#24577;&#19979;&#36861;&#27714;&#38271;&#26399;&#20844;&#24179;&#24615;&#65292;&#23454;&#29616;&#31639;&#27861;&#30340;&#21160;&#24577;&#36866;&#24212;&#21644;&#26435;&#34913;&#65292;&#21487;&#20026;&#20998;&#31867;&#22120;-&#20154;&#32676;&#31995;&#32479;&#25512;&#21521;&#26356;&#29702;&#24819;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.09362</link><description>&lt;p&gt;
&#26410;&#30693;&#21160;&#24577;&#19979;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-Term Fairness with Unknown Dynamics. (arXiv:2304.09362v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#30693;&#21160;&#24577;&#19979;&#36861;&#27714;&#38271;&#26399;&#20844;&#24179;&#24615;&#65292;&#23454;&#29616;&#31639;&#27861;&#30340;&#21160;&#24577;&#36866;&#24212;&#21644;&#26435;&#34913;&#65292;&#21487;&#20026;&#20998;&#31867;&#22120;-&#20154;&#32676;&#31995;&#32479;&#25512;&#21521;&#26356;&#29702;&#24819;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#24378;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#65292;&#20294;&#23427;&#20063;&#21487;&#20197;&#29992;&#20110;&#21160;&#24577;&#22320;&#23547;&#27714;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20013;&#35268;&#33539;&#20102;&#38271;&#26399;&#20844;&#27491;&#24615;&#12290;&#35813;&#20844;&#24335;&#21487;&#20197;&#23481;&#32435;&#21160;&#24577;&#25511;&#21046;&#30446;&#26631;&#65292;&#20363;&#22914;&#25512;&#21160;&#20154;&#32676;&#29366;&#24577;&#20013;&#22266;&#26377;&#30340;&#24179;&#31561;&#65292;&#36825;&#20123;&#30446;&#26631;&#19981;&#33021;&#34987;&#32435;&#20837;&#21040;&#20844;&#24179;&#24615;&#30340;&#38745;&#24577;&#20844;&#24335;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#31639;&#27861;&#36890;&#36807;&#29306;&#29298;&#30701;&#26399;&#28608;&#21169;&#65292;&#23558;&#20998;&#31867;&#22120;-&#20154;&#32676;&#31995;&#32479;&#25512;&#21521;&#26356;&#29702;&#24819;&#30340;&#24179;&#34913;&#65292;&#20197;&#36866;&#24212;&#26410;&#30693;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36866;&#24212;&#20102;&#26368;&#36817;&#30340;&#22312;&#32447;&#23398;&#20064;&#24037;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#22312;&#32047;&#31215;&#25439;&#22833;&#21644;&#20844;&#24179;&#24615;&#36829;&#35268;&#30340;&#32047;&#31215;&#24615;&#65288;&#20316;&#20026;&#20154;&#32676;&#20043;&#38388;&#30340;&#32479;&#35745;&#35268;&#24459;&#65289;&#19978;&#23454;&#29616;&#20102;&#21516;&#26102;&#30340;&#27010;&#29575;&#30028;&#38480;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#22522;&#20934;&#30340;&#32454;&#24494;&#20998;&#31867;&#22120;&#30340;&#37325;&#22797;&#35757;&#32451;&#20197;&#21450;&#19968;&#20010;&#35299;&#39064;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#20004;&#20010;&#21327;&#35843;&#30446;&#26631;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
While machine learning can myopically reinforce social inequalities, it may also be used to dynamically seek equitable outcomes. In this paper, we formalize long-term fairness in the context of online reinforcement learning. This formulation can accommodate dynamical control objectives, such as driving equity inherent in the state of a population, that cannot be incorporated into static formulations of fairness. We demonstrate that this framing allows an algorithm to adapt to unknown dynamics by sacrificing short-term incentives to drive a classifier-population system towards more desirable equilibria. For the proposed setting, we develop an algorithm that adapts recent work in online learning. We prove that this algorithm achieves simultaneous probabilistic bounds on cumulative loss and cumulative violations of fairness (as statistical regularities between demographic groups). We compare our proposed algorithm to the repeated retraining of myopic classifiers, as a baseline, and to a d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;&#20809;&#28369;&#21644;&#31561;&#21608;&#26465;&#20214;&#19979;&#65292;MALA&#30340;&#28151;&#21512;&#26102;&#38388;&#20165;&#19982;Hessian&#30697;&#38453;&#30340;trace&#26377;&#20851;&#65292;&#32780;&#19982;&#20854;&#31639;&#23376;&#33539;&#25968;&#21644;log-concave&#27809;&#26377;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.04095</link><description>&lt;p&gt;
&#20803;&#21345;&#27931;&#35843;&#25972;&#26391;&#20043;&#19975;(MALA)&#22312;&#20809;&#28369;&#19988;&#31561;&#21608;&#26465;&#20214;&#19979;&#30340;&#28151;&#21512;&#31616;&#21333;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
A Simple Proof of the Mixing of Metropolis-Adjusted Langevin Algorithm under Smoothness and Isoperimetry. (arXiv:2304.04095v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;&#20809;&#28369;&#21644;&#31561;&#21608;&#26465;&#20214;&#19979;&#65292;MALA&#30340;&#28151;&#21512;&#26102;&#38388;&#20165;&#19982;Hessian&#30697;&#38453;&#30340;trace&#26377;&#20851;&#65292;&#32780;&#19982;&#20854;&#31639;&#23376;&#33539;&#25968;&#21644;log-concave&#27809;&#26377;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\mathbb{R}^d$&#19978;&#26679;&#26412;&#30446;&#26631;&#23494;&#24230;&#30340;&#20803;&#21345;&#27931;&#35843;&#25972;&#26391;&#20043;&#19975;&#65288;MALA&#65289;&#30340;&#28151;&#21512;&#26102;&#38388;&#12290;&#25105;&#20204;&#20551;&#35774;&#30446;&#26631;&#23494;&#24230;&#28385;&#36275;$\psi_\mu$-&#31561;&#21608;&#21644;&#23427;&#30340;&#40657;&#22622;&#30697;&#38453;&#30340;trace&#21644;&#31639;&#23376;&#33539;&#25968;&#20998;&#21035;&#21463;$L$&#21644;$\Upsilon$&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#35770;&#26159;&#65292;&#20174;&#28909;&#21551;&#21160;&#24320;&#22987;&#65292;&#20026;&#20102;&#36798;&#21040;$\epsilon$&#24635;&#21464;&#24046;&#36317;&#31163;&#65292;MALA&#22312;$O\left(\frac{(L\Upsilon)^{\frac12}}{\psi_\mu^2} \log\left(\frac{1}{\epsilon}\right)\right)$&#27425;&#36845;&#20195;&#20013;&#28151;&#21512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#32467;&#26524;&#19981;&#20165;&#36866;&#29992;&#20110;&#23545;&#25968;&#20985;&#37319;&#26679;&#35774;&#32622;&#65292;&#32780;&#19988;&#28151;&#21512;&#26102;&#38388;&#20165;&#21462;&#20915;&#20110;$\Upsilon$&#65292;&#32780;&#19981;&#26159;&#20854;&#19978;&#30028;$Ld$&#12290;&#22312;$m$-&#24378;&#23545;&#25968;&#20985;&#21644;$L$-&#20809;&#28369;&#37319;&#26679;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#24674;&#22797;&#20102;&#20197;&#21069;&#30340;MALA&#30340;&#26368;&#23567;&#20540;&#28151;&#21512;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the mixing time of Metropolis-Adjusted Langevin algorithm (MALA) for sampling a target density on $\mathbb{R}^d$. We assume that the target density satisfies $\psi_\mu$-isoperimetry and that the operator norm and trace of its Hessian are bounded by $L$ and $\Upsilon$ respectively. Our main result establishes that, from a warm start, to achieve $\epsilon$-total variation distance to the target density, MALA mixes in $O\left(\frac{(L\Upsilon)^{\frac12}}{\psi_\mu^2} \log\left(\frac{1}{\epsilon}\right)\right)$ iterations. Notably, this result holds beyond the log-concave sampling setting and the mixing time depends on only $\Upsilon$ rather than its upper bound $L d$. In the $m$-strongly logconcave and $L$-log-smooth sampling setting, our bound recovers the previous minimax mixing bound of MALA~\cite{wu2021minimax}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03279</link><description>&lt;p&gt;
&#22870;&#21169;&#26159;&#21542;&#21512;&#29702;&#65311;&#22312; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#20013;&#34913;&#37327;&#22870;&#21169;&#19982;&#36947;&#24503;&#34892;&#20026;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#36861;&#27714;&#26435;&#21147;&#21644;&#27450;&#39575;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#21487;&#33021;&#20250;&#28608;&#21169;&#26377;&#23475;&#34892;&#20026;&#12290;&#37027;&#20040;&#20195;&#29702;&#26159;&#21542;&#33258;&#28982;&#32780;&#28982;&#22320;&#23398;&#20250;&#20102;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65311;&#25105;&#20204;&#22914;&#20309;&#22312; GPT-4 &#31561;&#36890;&#29992;&#27169;&#22411;&#20013;&#34913;&#37327;&#36825;&#20123;&#34892;&#20026;&#21602;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#22810;&#26679;&#21270;&#30340;&#24773;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20250;&#20915;&#31574;&#21046;&#23450;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#12290;&#25105;&#20204;&#25968;&#23398;&#21270;&#20102;&#25968;&#21313;&#31181;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#20195;&#29702;&#20542;&#21521;&#20110;&#36861;&#27714;&#26435;&#21147;&#65292;&#36896;&#25104;&#21151;&#33021;&#19981;&#33391;&#21644;&#36829;&#21453;&#20262;&#29702;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#20195;&#29702;&#36235;&#21521;&#20110;&#37319;&#21462;&#26356;&#23569;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MACHIAVELLI &#26159;&#35780;&#20272;&#20154;&#24037;&#20195;&#29702;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#27700;&#24179;&#30340;&#26377;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D-CNN&#65289;&#23545;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#39118;&#36895;&#39044;&#27979;&#30340;&#31934;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21608;&#22260;&#21306;&#22495;&#30340;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;3D-CNN&#35757;&#32451;&#21487;&#20197;&#27604;&#20165;&#20351;&#29992;&#21333;&#28857;&#20449;&#24687;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01545</link><description>&lt;p&gt;
&#21306;&#22495;&#39118;&#21147;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;CNN&#30340;&#39118;&#36895;&#39044;&#27979;&#65306;&#26469;&#33258;&#26102;&#31354;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis. (arXiv:2304.01545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D-CNN&#65289;&#23545;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#39118;&#36895;&#39044;&#27979;&#30340;&#31934;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21608;&#22260;&#21306;&#22495;&#30340;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;3D-CNN&#35757;&#32451;&#21487;&#20197;&#27604;&#20165;&#20351;&#29992;&#21333;&#28857;&#20449;&#24687;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#31354;&#25968;&#25454;&#32500;&#24230;&#23545;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#30340;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21152;&#20837;&#31354;&#38388;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#19981;&#21516;&#31354;&#38388;&#23610;&#24230;&#25913;&#36827;&#30340;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#20339;&#26102;&#38388;&#38271;&#24230;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#30740;&#31350;&#20063;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#22312;&#20351;&#29992;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D-CNN&#65289;&#39044;&#27979;&#39118;&#36895;&#26102;&#65292;&#37319;&#29992;&#20855;&#26377;&#19981;&#21516;&#26102;&#31354;&#32500;&#24230;&#30340;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#35780;&#20272;&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21608;&#22260;&#21306;&#22495;&#30340;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;3D-CNN&#35757;&#32451;&#21487;&#20197;&#27604;&#20165;&#20351;&#29992;&#21333;&#28857;&#20449;&#24687;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22810;&#26102;&#38388;&#25968;&#25454;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the impact of spatiotemporal data dimensions on the precision of a wind forecasting model developed using an artificial neural network. Although previous studies have shown that incorporating spatial data can enhance the accuracy of wind forecasting models, few investigations have explored the extent of the improvement owing to different spatial scales in neural network-based predictive models. Additionally, there are limited studies on the optimal temporal length of the input data for these models. To address this gap, this study employs data with various spatiotemporal dimensions as inputs when forecasting wind using 3D-Convolutional Neural Networks (3D-CNN) and assesses their predictive performance. The results indicate that using spatial data of the surrounding area for 3D-CNN training can achieve better predictive performance than using only single-point information. Additionally, multi-time data had a more positive effect on the predictive performance than
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;W2PGNN&#65292;&#26088;&#22312;&#22238;&#31572;&#20309;&#26102;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20351;&#29992;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#19979;&#28216;&#25968;&#25454;&#30340;&#22797;&#26434;&#29983;&#25104;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.16458</link><description>&lt;p&gt;
&#20309;&#26102;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65311;&#22522;&#20110;&#25968;&#25454;&#29983;&#25104;&#35270;&#35282;&#30340;&#22238;&#31572;&#65281;
&lt;/p&gt;
&lt;p&gt;
When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!. (arXiv:2303.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;W2PGNN&#65292;&#26088;&#22312;&#22238;&#31572;&#20309;&#26102;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20351;&#29992;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#19979;&#28216;&#25968;&#25454;&#30340;&#22797;&#26434;&#29983;&#25104;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#39044;&#35757;&#32451;&#22312;&#23398;&#26415;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#23581;&#35797;&#65292;&#20294;&#36127;&#38754;&#36801;&#31227;&#26159;&#23558;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26102;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#22270;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#20309;&#26102;&#39044;&#35757;&#32451;&#21644;&#22914;&#20309;&#39044;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#20505;&#26080;&#35770;&#31574;&#30053;&#22914;&#20309;&#20808;&#36827;&#65292;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#20173;&#28982;&#26080;&#27861;&#24102;&#26469;&#26126;&#26174;&#30340;&#22909;&#22788;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;W2PGNN&#26469;&#22238;&#31572;&#20309;&#26102;&#39044;&#35757;&#32451;&#30340;&#20851;&#38190;&#38382;&#39064;&#65288;&#21363;&#25105;&#20204;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#21487;&#20197;&#21033;&#29992;&#22270;&#39044;&#35757;&#32451;&#65289;&#65292;&#28982;&#21518;&#20877;&#36827;&#34892;&#36153;&#21147;&#30340;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#19979;&#28216;&#25968;&#25454;&#30340;&#22797;&#26434;&#29983;&#25104;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, graph pre-training has attracted wide research attention, which aims to learn transferable knowledge from unlabeled graph data so as to improve downstream performance. Despite these recent attempts, the negative transfer is a major issue when applying graph pre-trained models to downstream tasks. Existing works made great efforts on the issue of what to pre-train and how to pre-train by designing a number of graph pre-training and fine-tuning strategies. However, there are indeed cases where no matter how advanced the strategy is, the "pre-train and fine-tune" paradigm still cannot achieve clear benefits. This paper introduces a generic framework W2PGNN to answer the crucial question of when to pre-train (i.e., in what situations could we take advantage of graph pre-training) before performing effortful pre-training or fine-tuning. We start from a new perspective to explore the complex generative mechanisms from the pre-training data to downstream data. In particular, W2PGNN 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.06516</link><description>&lt;p&gt;
&#25171;&#24320;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20197;&#35745;&#31639;Shap&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Opening Up the Neural Network Classifier for Shap Score Computation. (arXiv:2303.06516v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient method for computing Shap explanation scores in machine learning model classification by transforming binary neural networks into Boolean circuits and treating the resulting circuit as an open-box model, which leads to a significant improvement in performance compared to computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#39640;&#25928;&#35745;&#31639;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#36716;&#25442;&#20026;&#30830;&#23450;&#24615;&#21644;&#21487;&#20998;&#35299;&#30340;&#24067;&#23572;&#30005;&#36335;&#65292;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#12290;&#25152;&#24471;&#21040;&#30340;&#30005;&#36335;&#34987;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#12290;&#35814;&#32454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#30456;&#27604;&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of efficiently computing Shap explanation scores for classifications with machine learning models. With this goal, we show the transformation of binary neural networks (BNNs) for classification into deterministic and decomposable Boolean circuits, for which knowledge compilation techniques are used. The resulting circuit is treated as an open-box model, to compute Shap scores by means of a recent efficient algorithm for this class of circuits. Detailed experiments show a considerable gain in performance in comparison with computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DP-Fast MH&#31639;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20855;&#26377;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.06171</link><description>&lt;p&gt;
DP-Fast MH: &#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#31169;&#26377;&#12289;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;Metropolis-Hastings&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference. (arXiv:2303.06171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DP-Fast MH&#31639;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20855;&#26377;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new DP-Fast MH algorithm for large-scale Bayesian inference, which is accurate, fast, and privacy-preserving.
&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#25512;&#29702;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#23427;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#35786;&#26029;&#12289;&#33647;&#29289;&#35774;&#35745;&#21644;&#25919;&#31574;&#21046;&#23450;&#12290;&#22312;&#36825;&#20123;&#24120;&#35265;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#21487;&#33021;&#38750;&#24120;&#25935;&#24863;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25552;&#20379;&#20102;&#20855;&#26377;&#24378;&#22823;&#26368;&#22351;&#24773;&#20917;&#38544;&#31169;&#20445;&#35777;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#24182;&#24050;&#21457;&#23637;&#25104;&#20026;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20998;&#26512;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Metropolis-Hastings&#65288;MH&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#26368;&#22522;&#26412;&#30340;MCMC&#26041;&#27861;&#20043;&#19968;&#65292;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31169;&#26377;MCMC&#31639;&#27861;&#20026;&#20102;&#33719;&#24471;&#38544;&#31169;&#32780;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#31934;&#30830;&#19988;&#24555;&#36895;&#30340;DP MH&#31639;&#27861;&#65292;&#22823;&#22810;&#25968;&#36845;&#20195;&#20013;&#20165;&#20351;&#29992;&#19968;&#20010;&#23567;&#25209;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#38544;&#31169;&#12289;&#21487;&#25193;&#23637;&#24615;&#65288;&#21363;&#25209;&#37327;&#22823;&#23567;&#65289;&#21644;&#25928;&#29575;&#65288;&#21363;&#25910;&#25947;&#36895;&#24230;&#65289;&#20043;&#38388;&#30340;&#19977;&#37325;&#26435;&#34913;&#65292;&#20174;&#29702;&#35770;&#19978;&#35828;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference provides a principled framework for learning from complex data and reasoning under uncertainty. It has been widely applied in machine learning tasks such as medical diagnosis, drug design, and policymaking. In these common applications, the data can be highly sensitive. Differential privacy (DP) offers data analysis tools with powerful worst-case privacy guarantees and has been developed as the leading approach in privacy-preserving data analysis. In this paper, we study Metropolis-Hastings (MH), one of the most fundamental MCMC methods, for large-scale Bayesian inference under differential privacy. While most existing private MCMC algorithms sacrifice accuracy and efficiency to obtain privacy, we provide the first exact and fast DP MH algorithm, using only a minibatch of data in most iterations. We further reveal, for the first time, a three-way trade-off among privacy, scalability (i.e. the batch size), and efficiency (i.e. the convergence rate), theoretically char
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#23558;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#31169;&#26377;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#38477;&#20302;&#21040;&#38750;&#31169;&#26377;&#31639;&#27861;&#65292;&#20174;&#32780;&#20351;&#29616;&#26377;&#30340;&#38750;&#31169;&#26377;&#31639;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#26465;&#20214;&#19979;&#20351;&#29992;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#31169;&#26377;&#23398;&#20064;GMMs&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.04288</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31169;&#26377;&#23398;&#20064;&#26080;&#38480;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Polynomial Time and Private Learning of Unbounded Gaussian Mixture Models. (arXiv:2303.04288v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#23558;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#31169;&#26377;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#38477;&#20302;&#21040;&#38750;&#31169;&#26377;&#31639;&#27861;&#65292;&#20174;&#32780;&#20351;&#29616;&#26377;&#30340;&#38750;&#31169;&#26377;&#31639;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#26465;&#20214;&#19979;&#20351;&#29992;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#31169;&#26377;&#23398;&#20064;GMMs&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;$k$&#20010;&#32452;&#20214;&#21644;$d$&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;(GMMs)&#31169;&#19979;&#20272;&#35745;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#23558;&#27492;&#38382;&#39064;&#20943;&#23569;&#21040;&#38750;&#31169;&#26377;&#31639;&#27861;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#40657;&#30418;&#26041;&#24335;&#31169;&#26377;&#21270;&#29616;&#26377;&#30340;&#38750;&#31169;&#26377;&#31639;&#27861;&#65292;&#21516;&#26102;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#20165;&#20135;&#29983;&#23569;&#37327;&#24320;&#38144;&#12290;&#20316;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#20027;&#35201;&#24212;&#29992;&#65292;&#25105;&#20204;&#20351;&#29992;Moitra&#21644;Valiant [MV10]&#30340;&#38750;&#31169;&#26377;&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;$(\varepsilon, \delta)$-&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#26469;&#23398;&#20064;GMMs&#12290;&#32467;&#26524;&#65292;&#36825;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#19981;&#23545;&#21442;&#25968;&#36827;&#34892;&#20219;&#20309;&#26377;&#30028;&#24615;&#20551;&#35774;&#30340;&#31169;&#26377;&#23398;&#20064;GMMs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#21644;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#20316;&#20026;&#25105;&#20204;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#39640;&#32500;&#39640;&#26031;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#32039;&#23494;(&#39640;&#36798;&#24120;&#25968;&#22240;&#23376;)&#19979;&#30028;&#65292;&#36825;&#21487;&#20197;&#29420;&#31435;&#22320;&#24471;&#21040;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of privately estimating the parameters of $d$-dimensional Gaussian Mixture Models (GMMs) with $k$ components. For this, we develop a technique to reduce the problem to its non-private counterpart. This allows us to privatize existing non-private algorithms in a blackbox manner, while incurring only a small overhead in the sample complexity and running time. As the main application of our framework, we develop an $(\varepsilon, \delta)$-differentially private algorithm to learn GMMs using the non-private algorithm of Moitra and Valiant [MV10] as a blackbox. Consequently, this gives the first sample complexity upper bound and first polynomial time algorithm for privately learning GMMs without any boundedness assumptions on the parameters. As part of our analysis, we prove a tight (up to a constant factor) lower bound on the total variation distance of high-dimensional Gaussians which can be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02265</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning to Influence Human Behavior with Offline Reinforcement Learning. (arXiv:2303.02265v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method of learning to influence human behavior through offline reinforcement learning, which can improve human performance in collaborative tasks.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#19982;&#20154;&#31867;&#20114;&#21160;&#26159;&#26368;&#22797;&#26434;&#30340;&#35774;&#32622;&#20043;&#19968;&#65292;&#22240;&#20026;&#20154;&#31867;&#24448;&#24448;&#30001;&#20110;&#22797;&#26434;&#30340;&#20559;&#35265;&#32780;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#20195;&#29702;&#26368;&#32456;&#20250;&#24433;&#21709;&#36825;&#20123;&#20154;&#25152;&#37319;&#21462;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24433;&#21709;&#26469;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#38543;&#30528;&#20219;&#21153;&#30340;&#23637;&#24320;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#19982;&#20154;&#21592;&#36827;&#34892;&#22312;&#32447;&#22521;&#35757;&#65288;&#36825;&#24448;&#24448;&#22826;&#26114;&#36149;&#21644;&#19981;&#23433;&#20840;&#65289;&#65292;&#20063;&#19981;&#20551;&#35774;&#26377;&#39640;&#20445;&#30495;&#24230;&#29615;&#22659;&#27169;&#25311;&#22120;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#65292;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#20219;&#21153;&#22870;&#21169;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#23398;&#20064;&#32452;&#21512;&#34892;&#20026;&#30340;&#32452;&#20214;&#65292;&#24182;&#21457;&#29616;&#23548;&#33268;&#26356;&#29702;&#24819;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#34892;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31163;&#32447;RL&#21487;&#20197;&#23398;&#20064;&#31574;&#30053;&#26469;&#24433;&#21709;&#21644;&#25913;&#21892;&#20154;&#31867;&#34892;&#20026;&#65292;&#23613;&#31649;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#19982;&#20154;&#31867;&#30340;&#26399;&#26395;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies 
&lt;/p&gt;</description></item><item><title>CrystalBox&#26159;&#19968;&#31181;&#35299;&#37322;DRL&#32593;&#32476;&#25511;&#21046;&#22120;&#34892;&#20026;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20351;&#29992;&#26410;&#26469;&#20851;&#38190;&#32593;&#32476;&#24615;&#33021;&#25351;&#26631;&#30340;&#24433;&#21709;&#26469;&#29983;&#25104;&#31616;&#26126;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2302.13483</link><description>&lt;p&gt;
CrystalBox&#65306;&#22522;&#20110;&#26410;&#26469;&#30340;DRL&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
CrystalBox: Future-Based Explanations for DRL Network Controllers. (arXiv:2302.13483v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13483
&lt;/p&gt;
&lt;p&gt;
CrystalBox&#26159;&#19968;&#31181;&#35299;&#37322;DRL&#32593;&#32476;&#25511;&#21046;&#22120;&#34892;&#20026;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20351;&#29992;&#26410;&#26469;&#20851;&#38190;&#32593;&#32476;&#24615;&#33021;&#25351;&#26631;&#30340;&#24433;&#21709;&#26469;&#29983;&#25104;&#31616;&#26126;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#19981;&#36275;&#26159;&#38480;&#21046;&#39640;&#25928;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#25511;&#21046;&#22120;&#23454;&#38469;&#37319;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#32593;&#32476;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#36804;&#20170;&#20351;&#29992;&#24341;&#20154;&#27880;&#30446;&#30340;&#36755;&#20837;&#29305;&#24449;&#26469;&#35299;&#37322;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;&#25511;&#21046;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36890;&#24120;&#65292;&#36816;&#33829;&#21830;&#26377;&#20852;&#36259;&#20102;&#35299;&#25511;&#21046;&#22120;&#23545;&#26410;&#26469;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32780;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#25429;&#25417;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrystalBox&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20851;&#38190;&#32593;&#32476;&#24615;&#33021;&#25351;&#26631;&#30340;&#26410;&#26469;&#24433;&#21709;&#26469;&#35299;&#37322;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#12290;CrystalBox&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#31616;&#27905;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;DRL&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#22870;&#21169;&#32452;&#20214;&#20316;&#20026;&#35299;&#37322;&#30340;&#22522;&#30784;&#65292;&#36825;&#26159;&#36816;&#33829;&#21830;&#26377;&#24847;&#20041;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#12290;CrystalBox&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lack of explainability is a key factor limiting the practical adoption of high-performant Deep Reinforcement Learning (DRL) controllers. Explainable RL for networking hitherto used salient input features to interpret a controller's behavior. However, these feature-based solutions do not completely explain the controller's decision-making process. Often, operators are interested in understanding the impact of a controller's actions on performance in the future, which feature-based solutions cannot capture.  In this paper, we present CrystalBox, a framework that explains a controller's behavior in terms of the future impact on key network performance metrics. CrystalBox employs a novel learning-based approach to generate succinct and expressive explanations. We use reward components of the DRL network controller, which are key performance metrics meaningful to operators, as the basis for explanations. CrystalBox is generalizable and can work across both discrete and continuous control en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#22238;&#24402;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#20004;&#20010;&#25512;&#24191;&#65292;&#19968;&#20010;&#26159;&#31515;&#21345;&#23572;&#31354;&#38388;&#20013;&#26356;&#19968;&#33324;&#30340;&#33258;&#22238;&#24402;&#21160;&#21147;&#23398;&#65292;&#21478;&#19968;&#20010;&#26159;&#21333;&#20301;&#22235;&#20803;&#25968;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#21160;&#21147;&#23398;&#12290;&#27492;&#25193;&#23637;&#20801;&#35768;&#25551;&#36848;&#35266;&#23519;&#29366;&#24577;&#30340;&#26356;&#22797;&#26434;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2302.11834</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#22312;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#21333;&#20301;&#22235;&#20803;&#25968;&#35266;&#27979;&#31354;&#38388;&#20013;&#30340;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Generalization of Auto-Regressive Hidden Markov Models to Non-Linear Dynamics and Unit Quaternion Observation Space. (arXiv:2302.11834v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#22238;&#24402;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#20004;&#20010;&#25512;&#24191;&#65292;&#19968;&#20010;&#26159;&#31515;&#21345;&#23572;&#31354;&#38388;&#20013;&#26356;&#19968;&#33324;&#30340;&#33258;&#22238;&#24402;&#21160;&#21147;&#23398;&#65292;&#21478;&#19968;&#20010;&#26159;&#21333;&#20301;&#22235;&#20803;&#25968;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#21160;&#21147;&#23398;&#12290;&#27492;&#25193;&#23637;&#20801;&#35768;&#25551;&#36848;&#35266;&#23519;&#29366;&#24577;&#30340;&#26356;&#22797;&#26434;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#21464;&#37327;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#32463;&#27982;&#23398;&#31561;&#19981;&#21516;&#39046;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#38750;&#30417;&#30563;&#20998;&#21106;&#12290;&#20854;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26159;&#33258;&#22238;&#24402;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;(ARHMM)&#65292;&#23427;&#23558;&#30001;&#39532;&#23572;&#21487;&#22827;&#38142;&#21160;&#21147;&#23398;&#25511;&#21046;&#30340;&#28508;&#22312;&#27169;&#24577;&#19982;&#35266;&#27979;&#29366;&#24577;&#30340;&#32447;&#24615;&#33258;&#22238;&#24402;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ARHMM&#30340;&#20004;&#20010;&#25512;&#24191;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31515;&#21345;&#23572;&#31354;&#38388;&#20013;&#26356;&#19968;&#33324;&#30340;&#33258;&#22238;&#24402;&#21160;&#21147;&#23398;&#65292;&#23427;&#34987;&#25551;&#36848;&#20026;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#20301;&#22235;&#20803;&#25968;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#20197;&#27491;&#30830;&#25551;&#36848;&#26041;&#21521;&#12290;&#36825;&#20123;&#25193;&#23637;&#20801;&#35768;&#25551;&#36848;&#35266;&#23519;&#29366;&#24577;&#30340;&#26356;&#22797;&#26434;&#21160;&#21147;&#23398;&#12290;&#34429;&#28982;&#27492;&#25193;&#23637;&#26159;&#38024;&#23545;ARHMM&#25552;&#20986;&#30340;&#65292;&#20294;&#26131;&#20110;&#25193;&#23637;&#21040;&#20854;&#20182;&#20855;&#26377;&#35266;&#27979;&#31354;&#38388;&#20013;&#33258;&#22238;&#24402;&#21160;&#21147;&#23398;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20363;&#22914;&#33258;&#22238;&#24402;&#38544;&#21322;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent variable models are widely used to perform unsupervised segmentation of time series in different context such as robotics, speech recognition, and economics. One of the most widely used latent variable model is the Auto-Regressive Hidden Markov Model (ARHMM), which combines a latent mode governed by a Markov chain dynamics with a linear Auto-Regressive dynamics of the observed state.  In this work, we propose two generalizations of the ARHMM. First, we propose a more general AR dynamics in Cartesian space, described as a linear combination of non-linear basis functions. Second, we propose a linear dynamics in unit quaternion space, in order to properly describe orientations. These extensions allow to describe more complex dynamics of the observed state.  Although this extension is proposed for the ARHMM, it can be easily extended to other latent variable models with AR dynamics in the observed space, such as Auto-Regressive Hidden semi-Markov Models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.11552</link><description>&lt;p&gt;
&#20943;&#23569;&#12289;&#37325;&#22797;&#21033;&#29992;&#12289;&#22238;&#25910;&#65306;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25193;&#25955;&#27169;&#22411;&#38382;&#19990;&#20197;&#26469;&#65292;&#23427;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#32463;&#36805;&#36895;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#23398;&#20064;&#19968;&#31995;&#21015;&#26102;&#21464;&#30340;&#23545;&#25968;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#36825;&#31181;&#35299;&#37322;&#24050;&#32463;&#28608;&#21457;&#20102;&#22522;&#20110;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#24605;&#24819;&#25104;&#20026;&#21518;&#32493;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#36825;&#20123;&#24819;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#25968;-based&#35299;&#37322;&#65292;&#25506;&#32034;&#20102;&#29992;&#20110;&#28041;&#21450;&#32452;&#21512;&#29983;&#25104;&#21644;&#25351;&#23548;&#30340;&#26465;&#20214;&#12289;&#20462;&#25913;&#21644;&#37325;&#22797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#31867;&#22411;&#30340;&#32452;&#21512;&#20351;&#29992;&#24403;&#21069;&#25216;&#26415;&#22833;&#36133;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37319;&#26679;&#32773;(&#32780;&#19981;&#26159;&#27169;&#22411;)&#23545;&#27492;&#22833;&#36133;&#36127;&#26377;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#37319;&#26679;&#22120;&#65292;&#21463;MCMC&#30340;&#21551;&#21457;&#65292;&#20351;&#32452;&#21512;&#29983;&#25104;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20272;&#35745;&#26679;&#26412;&#32423;&#27169;&#24577;&#24046;&#24322;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#20998;&#25104;&#19981;&#21516;&#30340;&#23376;&#38598;&#24182;&#25506;&#32034;&#20102;&#27169;&#24577;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#23384;&#22312;&#19981;&#24179;&#34913;&#31639;&#27861;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#29305;&#23450;&#23376;&#38598;&#19978;&#30340;&#34920;&#29616;&#19968;&#30452;&#27604;&#21333;&#27169;&#24577;&#30340;&#24046;&#65292;&#19982;&#27169;&#24577;&#20559;&#24046;&#30456;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2302.10912</link><description>&lt;p&gt;
&#24179;&#34913;&#30340;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#38598;&#29992;&#20110;&#19981;&#24179;&#34913;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Balanced Audiovisual Dataset for Imbalance Analysis. (arXiv:2302.10912v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20272;&#35745;&#26679;&#26412;&#32423;&#27169;&#24577;&#24046;&#24322;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#20998;&#25104;&#19981;&#21516;&#30340;&#23376;&#38598;&#24182;&#25506;&#32034;&#20102;&#27169;&#24577;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#23384;&#22312;&#19981;&#24179;&#34913;&#31639;&#27861;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#29305;&#23450;&#23376;&#38598;&#19978;&#30340;&#34920;&#29616;&#19968;&#30452;&#27604;&#21333;&#27169;&#24577;&#30340;&#24046;&#65292;&#19982;&#27169;&#24577;&#20559;&#24046;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24191;&#27867;&#23384;&#22312;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#39046;&#22495;&#20063;&#23384;&#22312;&#30001;&#20110;&#26679;&#26412;&#27169;&#24577;&#38388;&#22266;&#26377;&#24046;&#24322;&#23548;&#33268;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36817;&#26399;&#30340;&#24037;&#20316;&#23581;&#35797;&#20174;&#31639;&#27861;&#35282;&#24230;&#35299;&#20915;&#27169;&#24577;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#27809;&#26377;&#23436;&#20840;&#20998;&#26512;&#25968;&#25454;&#38598;&#20013;&#27169;&#24577;&#20559;&#24046;&#25152;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#25910;&#38598;&#30340;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#24577;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#24448;&#24448;&#27604;&#20854;&#20182;&#27169;&#24577;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#20840;&#38754;&#25506;&#32034;&#27169;&#24577;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20272;&#35745;&#26679;&#26412;&#32423;&#27169;&#24577;&#24046;&#24322;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#20998;&#25104;&#19981;&#21516;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#24778;&#22855;&#22320;&#21457;&#29616;&#65306;&#23384;&#22312;&#19981;&#24179;&#34913;&#31639;&#27861;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#29305;&#23450;&#23376;&#38598;&#19978;&#30340;&#34920;&#29616;&#19968;&#30452;&#27604;&#21333;&#27169;&#24577;&#30340;&#24046;&#65292;&#19982;&#27169;&#24577;&#20559;&#24046;&#30456;&#19968;&#33268;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#27169;&#24577;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#20998;&#26512;&#29616;&#26377;&#19981;&#24179;&#34913;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
The imbalance problem is widespread in the field of machine learning, which also exists in multimodal learning areas caused by the intrinsic discrepancy between modalities of samples. Recent works have attempted to solve the modality imbalance problem from algorithm perspective, however, they do not fully analyze the influence of modality bias in datasets. Concretely, existing multimodal datasets are usually collected under specific tasks, where one modality tends to perform better than other ones in most conditions. In this work, to comprehensively explore the influence of modality bias, we first split existing datasets into different subsets by estimating sample-wise modality discrepancy. We surprisingly find that: the multimodal models with existing imbalance algorithms consistently perform worse than the unimodal one on specific subsets, in accordance with the modality bias. To further explore the influence of modality bias and analyze the effectiveness of existing imbalance algori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38480;&#21160;&#20316;&#19978;&#19979;&#25991;&#33218;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20351;&#24179;&#28369;&#36951;&#25022;&#22312;&#29983;&#20135;&#22330;&#26223;&#20013;&#26356;&#26131;&#20110;&#37319;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.08551</link><description>&lt;p&gt;
&#21487;&#22797;&#29992;&#25968;&#25454;&#38598;&#30340;&#26080;&#38480;&#21160;&#20316;&#19978;&#19979;&#25991;&#33218;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Infinite Action Contextual Bandits with Reusable Data Exhaust. (arXiv:2302.08551v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38480;&#21160;&#20316;&#19978;&#19979;&#25991;&#33218;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20351;&#24179;&#28369;&#36951;&#25022;&#22312;&#29983;&#20135;&#22330;&#26223;&#20013;&#26356;&#26131;&#20110;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26080;&#38480;&#21160;&#20316;&#19978;&#19979;&#25991;&#33218;&#65292;&#24179;&#28369;&#36951;&#25022;&#21644;&#22238;&#24402;&#32467;&#26524;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#30340;&#22312;&#32447;&#24615;&#33021;&#65292;&#35745;&#31639;&#25104;&#26412;&#29420;&#31435;&#20110;&#21160;&#20316;&#38598;&#21512;&#65306;&#28982;&#32780;&#65292;&#32467;&#26524;&#30340;&#25968;&#25454;&#21097;&#20313;&#37327;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#12290;&#36825;&#25387;&#36133;&#20102;&#19979;&#28216;&#30340;&#25968;&#25454;&#31185;&#23398;&#27969;&#31243;&#65292;&#22914;&#31163;&#32447;&#27169;&#22411;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#20855;&#26377;&#30456;&#24403;&#30340;&#24179;&#28369;&#36951;&#25022;&#20445;&#35777;&#65292;&#20294;&#29983;&#25104;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65306;&#20316;&#20026;&#20132;&#25442;&#65292;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#65292;&#20294;&#20165;&#33267;&#20110;&#24179;&#28369;&#24230;&#65288;&#21363;&#20173;&#29420;&#31435;&#20110;&#21160;&#20316;&#38598;&#21512;&#65289;&#12290;&#36825;&#28040;&#38500;&#20102;&#37319;&#29992;&#24179;&#28369;&#36951;&#25022;&#22312;&#29983;&#20135;&#22330;&#26223;&#20013;&#30340;&#20851;&#38190;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For infinite action contextual bandits, smoothed regret and reduction to regression results in state-of-the-art online performance with computational cost independent of the action set: unfortunately, the resulting data exhaust does not have well-defined importance-weights. This frustrates the execution of downstream data science processes such as offline model selection. In this paper we describe an online algorithm with an equivalent smoothed regret guarantee, but which generates well-defined importance weights: in exchange, the online computational cost increases, but only to order smoothness (i.e., still independent of the action set). This removes a key obstacle to adoption of smoothed regret in production scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#27573;&#20223;&#23556;&#20195;&#29702;&#26500;&#24314;&#30340;&#20840;&#23616;&#21644;&#22522;&#20110;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#35299;&#20915;&#32447;&#24615;&#32422;&#26463;&#30340;&#28151;&#21512;&#21464;&#37327;&#38382;&#39064;&#65292;&#31639;&#27861;&#36890;&#36807;&#20004;&#31181;&#25506;&#32034;&#20989;&#25968;&#21487;&#26377;&#25928;&#25628;&#32034;&#21487;&#34892;&#22495;</title><link>http://arxiv.org/abs/2302.04686</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27573;&#20223;&#23556;&#20195;&#29702;&#23454;&#29616;&#28151;&#21512;&#21464;&#37327;&#30340;&#20840;&#23616;&#21644;&#20248;&#20808;&#32423;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Global and Preference-based Optimization with Mixed Variables using Piecewise Affine Surrogates. (arXiv:2302.04686v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#27573;&#20223;&#23556;&#20195;&#29702;&#26500;&#24314;&#30340;&#20840;&#23616;&#21644;&#22522;&#20110;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#35299;&#20915;&#32447;&#24615;&#32422;&#26463;&#30340;&#28151;&#21512;&#21464;&#37327;&#38382;&#39064;&#65292;&#31639;&#27861;&#36890;&#36807;&#20004;&#31181;&#25506;&#32034;&#20989;&#25968;&#21487;&#26377;&#25928;&#25628;&#32034;&#21487;&#34892;&#22495;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#22797;&#26434;&#38480;&#21046;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#28041;&#21450;&#28151;&#21512;&#21464;&#37327;&#65288;&#21363;&#25968;&#20540;&#21644;&#20998;&#31867;&#24615;&#30340;&#21464;&#37327;&#65289;&#30340;&#20248;&#21270;&#38382;&#39064;&#21487;&#33021;&#38590;&#20197;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#26159;&#22797;&#26434;&#27169;&#25311;&#25110;&#23454;&#39564;&#30340;&#32467;&#26524;&#26102;&#65292;&#35780;&#20272;&#20195;&#20215;&#21487;&#33021;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#65292;&#22522;&#20110;&#23545;&#21487;&#34892;&#26679;&#26412;&#19978;&#30446;&#26631;&#20989;&#25968;&#30340;&#20998;&#27573;&#20223;&#23556;&#20195;&#29702;&#26500;&#24314;&#26469;&#35299;&#20915;&#32447;&#24615;&#32422;&#26463;&#30340;&#28151;&#21512;&#21464;&#37327;&#38382;&#39064;&#65292;&#21487;&#35299;&#20915;&#20013;&#21040;&#22823;&#35268;&#27169;&#38382;&#39064;&#65288;&#32534;&#30721;&#21518;&#32422;100&#20010;&#21464;&#37327;&#21644;20&#20010;&#32422;&#26463;&#65289;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#25506;&#32034;&#20989;&#25968;&#26469;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#26377;&#25928;&#22320;&#25628;&#32034;&#21487;&#34892;&#22495;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#31639;&#27861;&#29256;&#26412;&#65292;&#24403;&#21482;&#33021;&#33719;&#24471;&#26679;&#26412;&#38388;&#30340;&#25104;&#23545;&#27604;&#36739;&#32780;&#26410;&#37327;&#21270;&#24213;&#23618;&#35201;&#26368;&#23567;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#21487;&#20351;&#29992;&#35813;&#31639;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization problems involving mixed variables, i.e., variables of numerical and categorical nature, can be challenging to solve, especially in the presence of complex constraints. Moreover, when the objective function is the result of a complicated simulation or experiment, it may be expensive to evaluate. This paper proposes a novel surrogate-based global optimization algorithm to solve linearly constrained mixed-variable problems up to medium-large size (around 100 variables after encoding and 20 constraints) based on constructing a piecewise affine surrogate of the objective function over feasible samples. We introduce two types of exploration functions to efficiently search the feasible domain via mixed-integer linear programming solvers. We also provide a preference-based version of the algorithm, which can be used when only pairwise comparisons between samples can be acquired while the underlying objective function to minimize remains unquantified. The two algorithms are tested
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26102;&#38388;&#27493;&#39588;&#24863;&#30693;&#26657;&#20934;&#21644;&#25286;&#20998;&#24555;&#25463;&#36335;&#26469;&#21152;&#36895;&#22270;&#20687;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#21518;&#35757;&#32451;&#37327;&#21270;&#25152;&#38754;&#20020;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04304</link><description>&lt;p&gt;
Q-Diffusion: &#37327;&#21270;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Q-Diffusion: Quantizing Diffusion Models. (arXiv:2302.04304v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26102;&#38388;&#27493;&#39588;&#24863;&#30693;&#26657;&#20934;&#21644;&#25286;&#20998;&#24555;&#25463;&#36335;&#26469;&#21152;&#36895;&#22270;&#20687;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#21518;&#35757;&#32451;&#37327;&#21270;&#25152;&#38754;&#20020;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36845;&#20195;&#22122;&#22768;&#20272;&#35745;&#65292;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22122;&#22768;&#20272;&#35745;&#27169;&#22411;&#30340;&#24930;&#25512;&#26029;&#12289;&#39640;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#24378;&#24230;&#22952;&#30861;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#25928;&#37319;&#29992;&#12290;&#34429;&#28982;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#34987;&#35748;&#20026;&#26159;&#20854;&#20182;&#20219;&#21153;&#30340;&#39318;&#36873;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#23427;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PTQ&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#29420;&#29305;&#22810;&#26102;&#38388;&#27493;&#39588;&#31649;&#36947;&#21644;&#27169;&#22411;&#26550;&#26500;&#65292;&#21387;&#32553;&#22122;&#22768;&#20272;&#35745;&#32593;&#32476;&#20197;&#21152;&#36895;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#35748;&#20026;&#25193;&#25955;&#27169;&#22411;&#37327;&#21270;&#30340;&#20851;&#38190;&#38590;&#28857;&#26159;&#22122;&#22768;&#20272;&#35745;&#32593;&#32476;&#22312;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#30340;&#21464;&#21270;&#36755;&#20986;&#20998;&#24067;&#21644;&#22122;&#22768;&#20272;&#35745;&#32593;&#32476;&#20013;&#24555;&#25463;&#23618;&#30340;&#21452;&#23792;&#28608;&#27963;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#27493;&#38271;&#24863;&#30693;&#26657;&#20934;&#21644;&#25286;&#20998;&#24555;&#25463;&#36335;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved great success in image synthesis through iterative noise estimation using deep neural networks. However, the slow inference, high memory consumption, and computation intensity of the noise estimation model hinder the efficient adoption of diffusion models. Although post-training quantization (PTQ) is considered a go-to compression method for other tasks, it does not work out-of-the-box on diffusion models. We propose a novel PTQ method specifically tailored towards the unique multi-timestep pipeline and model architecture of the diffusion models, which compresses the noise estimation network to accelerate the generation process. We identify the key difficulty of diffusion model quantization as the changing output distributions of noise estimation networks over multiple time steps and the bimodal activation distribution of the shortcut layers within the noise estimation network. We tackle these challenges with timestep-aware calibration and split shortcut 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#29289;&#29702;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#26174;&#24494;&#38236;&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#21306;&#22495;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2302.04216</link><description>&lt;p&gt;
&#23558;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#19982;&#29289;&#29702;&#20559;&#24046;&#30456;&#32467;&#21512;&#20197;&#25913;&#21892;&#26174;&#24494;&#38236;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Combining Variational Autoencoders and Physical Bias for Improved Microscopy Data Analysis. (arXiv:2302.04216v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#29289;&#29702;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#26174;&#24494;&#38236;&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#21306;&#22495;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#26174;&#24494;&#38236;&#21644;&#25195;&#25551;&#25506;&#38024;&#26174;&#24494;&#38236;&#20135;&#29983;&#22823;&#37327;&#30340;&#22270;&#20687;&#25110;&#39640;&#20809;&#35889;&#25968;&#25454;&#65292;&#20363;&#22914;EELS&#25110;4D STEM&#65292;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#26448;&#26009;&#30340;&#21508;&#31181;&#32467;&#26500;&#12289;&#29289;&#29702;&#21644;&#21270;&#23398;&#24615;&#36136;&#20449;&#24687;&#12290;&#20026;&#20102;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#35782;&#21035;&#20986;&#25968;&#25454;&#20013;&#29289;&#29702;&#19978;&#20998;&#31163;&#30340;&#21306;&#22495;&#65292;&#22914;&#30456;&#12289;&#38081;&#30005;&#21464;&#20307;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;&#20026;&#20102;&#25512;&#23548;&#26131;&#20110;&#35299;&#37322;&#30340;&#29305;&#24449;&#20998;&#26512;&#65292;&#20197;&#21407;&#21017;&#24615;&#21644;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#19982;&#26126;&#30830;&#23450;&#20041;&#30340;&#36793;&#30028;&#30456;&#32467;&#21512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#22686;&#24378;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#33021;&#21147;&#19982;&#29289;&#29702;&#39537;&#21160;&#30340;&#25439;&#22833;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#23547;&#27714;&#22312;&#23545;&#24212;&#20110;&#28508;&#22312;&#34920;&#31034;&#30340;&#22270;&#20687;&#20013;&#26368;&#23567;&#21270;&#19981;&#36830;&#32493;&#24615;&#30340;&#24635;&#38271;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#26448;&#26009;&#65292;&#21253;&#25324;NiO-LSMO&#12289;BiFeO3&#21644;&#30707;&#22696;&#28911;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26174;&#24494;&#38236;&#25968;&#25454;&#20013;&#35782;&#21035;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#21306;&#22495;&#21644;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electron and scanning probe microscopy produce vast amounts of data in the form of images or hyperspectral data, such as EELS or 4D STEM, that contain information on a wide range of structural, physical, and chemical properties of materials. To extract valuable insights from these data, it is crucial to identify physically separate regions in the data, such as phases, ferroic variants, and boundaries between them. In order to derive an easily interpretable feature analysis, combining with well-defined boundaries in a principled and unsupervised manner, here we present a physics augmented machine learning method which combines the capability of Variational Autoencoders to disentangle factors of variability within the data and the physics driven loss function that seeks to minimize the total length of the discontinuities in images corresponding to latent representations. Our method is applied to various materials, including NiO-LSMO, BiFeO3, and graphene. The results demonstrate the effe
&lt;/p&gt;</description></item><item><title>DynGFN&#26159;&#19968;&#31181;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04178</link><description>&lt;p&gt;
DynGFN: &#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#36125;&#21494;&#26031;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;GFlowNets&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04178
&lt;/p&gt;
&lt;p&gt;
DynGFN&#26159;&#19968;&#31181;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#25512;&#26029;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65288;GRN&#65289;&#65292;&#35813;&#32593;&#32476;&#25551;&#36848;&#20102;&#25511;&#21046;&#22522;&#22240;&#34920;&#36798;&#21644;&#32454;&#32990;&#21151;&#33021;&#30340;&#22522;&#22240;&#21450;&#20854;&#20135;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;DynGFN&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#29983;&#25104;&#27969;&#32593;&#32476;&#65292;&#20351;&#29992;RNA&#36895;&#24230;&#25968;&#25454;&#25191;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#24182;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the grand challenges of cell biology is inferring the gene regulatory network (GRN) which describes interactions between genes and their products that control gene expression and cellular function. We can treat this as a causal discovery problem but with two non-standard challenges: (1) regulatory networks are inherently cyclic so we should not model a GRN as a directed acyclic graph (DAG), and (2) observations have significant measurement noise, so for typical sample sizes there will always be a large equivalence class of graphs that are likely given the data, and we want methods that capture this uncertainty. Existing methods either focus on challenge (1), identifying cyclic structure from dynamics, or on challenge (2) learning complex Bayesian posteriors over DAGs, but not both. In this paper we leverage the fact that it is possible to estimate the "velocity" of gene expression with RNA velocity techniques to develop an approach that addresses both challenges. Because we have
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#38024;&#23545;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36127;&#36131;&#20219;&#25968;&#25454;&#31649;&#29702;&#24314;&#35758;&#65292;&#37319;&#29992;&#39044;&#38450;&#24615;&#21453;&#24605;&#30340;&#35266;&#28857;&#65292;&#36981;&#24490;&#21407;&#21017;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#65292;&#35299;&#20915;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.03629</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#21017;&#20027;&#20041;&#30340;&#36127;&#36131;&#20219;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Principlism Guided Responsible Data Curation. (arXiv:2302.03629v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03629
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#38024;&#23545;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36127;&#36131;&#20219;&#25968;&#25454;&#31649;&#29702;&#24314;&#35758;&#65292;&#37319;&#29992;&#39044;&#38450;&#24615;&#21453;&#24605;&#30340;&#35266;&#28857;&#65292;&#36981;&#24490;&#21407;&#21017;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#65292;&#35299;&#20915;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#25972;&#29702;&#23454;&#36341;&#32463;&#24120;&#24573;&#30053;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#25764;&#22238;&#21644;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#38750;&#21516;&#24847;&#32593;&#32476;&#29228;&#21462;&#26500;&#24314;&#30340;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#32570;&#20047;&#20840;&#38754;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#35780;&#20272;&#25152;&#24517;&#38656;&#30340;&#20803;&#25968;&#25454;&#12290;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#27861;&#21518;&#26399;&#35299;&#20915;&#38382;&#39064;&#65292;&#32570;&#20047;&#35828;&#26381;&#21147;&#30340;&#37319;&#29992;&#29702;&#30001;&#25110;&#26410;&#33021;&#25552;&#20379;&#36866;&#24403;&#24212;&#29992;&#30340;&#21512;&#36866;&#32972;&#26223;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#38024;&#23545;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#20027;&#21160;&#39046;&#22495;&#29305;&#23450;&#24314;&#35758;&#65292;&#35299;&#20915;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#21453;&#24605;&#30340;&#35266;&#28857;&#65292;&#24182;&#20511;&#37492;&#20102;&#29616;&#26377;&#30340;&#23454;&#36341;&#21644;&#25351;&#21335;&#65292;&#36981;&#24490;&#21407;&#21017;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. Further, HCCV datasets constructed through nonconsensual web scraping lack the necessary metadata for comprehensive fairness and robustness evaluations. Current remedies address issues post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations for curating HCCV datasets, addressing privacy and bias. We adopt an ante hoc reflective perspective and draw from current practices and guidelines, guided by the ethical framework of principlism.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;RL&#31639;&#27861;&#26469;&#35299;&#20915;&#23376;&#20219;&#21153;&#26159;&#21542;&#33021;&#25191;&#34892;&#20219;&#20309;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22810;&#20219;&#21153;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.02984</link><description>&lt;p&gt;
&#32452;&#21512;&#27867;&#21270;&#30340;&#40065;&#26834;&#23376;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Subtask Learning for Compositional Generalization. (arXiv:2302.02984v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;RL&#31639;&#27861;&#26469;&#35299;&#20915;&#23376;&#20219;&#21153;&#26159;&#21542;&#33021;&#25191;&#34892;&#20219;&#20309;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22810;&#20219;&#21153;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#24378;&#21270;&#23398;&#20064;&#26159;&#22521;&#20859;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#31574;&#30053;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#39640;&#23618;&#20219;&#21153;&#34987;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#65292;&#24182;&#35757;&#32451;&#21333;&#29420;&#30340;&#31574;&#30053;&#26469;&#25191;&#34892;&#27599;&#20010;&#23376;&#20219;&#21153;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#38382;&#39064;&#26159;&#20197;&#19968;&#31181;&#33021;&#22815;&#25191;&#34892;&#20219;&#20309;&#20219;&#21153;&#30340;&#26041;&#24335;&#35757;&#32451;&#23376;&#20219;&#21153;&#31574;&#30053;&#65307;&#36825;&#37324;&#23450;&#20041;&#30340;&#20219;&#21153;&#26159;&#30001;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#12290;&#25105;&#20204;&#26088;&#22312;&#26368;&#22823;&#21270;&#25152;&#26377;&#20219;&#21153;&#30340;&#26368;&#22351;&#24773;&#20917;&#34920;&#29616;&#65292;&#32780;&#19981;&#26159;&#24179;&#22343;&#24773;&#20917;&#34920;&#29616;&#12290;&#25105;&#20204;&#25226;&#35813;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#20010;&#20108;&#20154;&#38646;&#21644;&#21338;&#24328;&#65292;&#20854;&#20013;&#23545;&#25163;&#25361;&#36873;&#23376;&#20219;&#21153;&#24207;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;RL&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#19968;&#20010;&#26159;&#29616;&#26377;&#22810;&#26234;&#33021;&#20307;RL&#31639;&#27861;&#30340;&#25913;&#32534;&#65292;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#24773;&#20917;&#65292;&#21478;&#19968;&#20010;&#26159;&#24322;&#27493;&#29256;&#26412;&#65292;&#33021;&#22815;&#24182;&#34892;&#35757;&#32451;&#23376;&#20219;&#21153;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#22810;&#20219;&#21153;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional reinforcement learning is a promising approach for training policies to perform complex long-horizon tasks. Typically, a high-level task is decomposed into a sequence of subtasks and a separate policy is trained to perform each subtask. In this paper, we focus on the problem of training subtask policies in a way that they can be used to perform any task; here, a task is given by a sequence of subtasks. We aim to maximize the worst-case performance over all tasks as opposed to the average-case performance. We formulate the problem as a two agent zero-sum game in which the adversary picks the sequence of subtasks. We propose two RL algorithms to solve this game: one is an adaptation of existing multi-agent RL algorithms to our setting and the other is an asynchronous version which enables parallel training of subtask policies. We evaluate our approach on two multi-task environments with continuous states and actions and demonstrate that our algorithms outperform state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30446;&#26631;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26500;&#36896;&#20986;&#26377;&#25928;&#30340;&#20195;&#29702;&#20989;&#25968;&#26469;&#20195;&#26367;&#35745;&#31639;&#26114;&#36149;&#30340;&#26799;&#24230;&#65292;&#23454;&#29616;&#22312;&#30446;&#26631;&#31354;&#38388;&#30340;&#26368;&#20248;&#21270;&#65292;&#25674;&#38144;&#26799;&#24230;&#35745;&#31639;&#30340;&#25104;&#26412;&#65292;&#36827;&#32780;&#25552;&#20986;&#20102;SSO&#31639;&#27861;&#36827;&#34892;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#20197;&#36798;&#21040;&#25439;&#22833;&#30340;&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2302.02607</link><description>&lt;p&gt;
&#22522;&#20110;&#30446;&#26631;&#30340;&#38543;&#26426;&#20248;&#21270;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Target-based Surrogates for Stochastic Optimization. (arXiv:2302.02607v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30446;&#26631;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26500;&#36896;&#20986;&#26377;&#25928;&#30340;&#20195;&#29702;&#20989;&#25968;&#26469;&#20195;&#26367;&#35745;&#31639;&#26114;&#36149;&#30340;&#26799;&#24230;&#65292;&#23454;&#29616;&#22312;&#30446;&#26631;&#31354;&#38388;&#30340;&#26368;&#20248;&#21270;&#65292;&#25674;&#38144;&#26799;&#24230;&#35745;&#31639;&#30340;&#25104;&#26412;&#65292;&#36827;&#32780;&#25552;&#20986;&#20102;SSO&#31639;&#27861;&#36827;&#34892;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#20197;&#36798;&#21040;&#25439;&#22833;&#30340;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#26368;&#23567;&#21270;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#35745;&#31639;&#65288;&#21487;&#33021;&#26159;&#38543;&#26426;&#30340;&#65289;&#26799;&#24230;&#26159;&#26114;&#36149;&#30340;&#12290;&#36825;&#31181;&#20989;&#25968;&#22312;&#24378;&#21270;&#23398;&#20064;&#12289;&#27169;&#20223;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#37117;&#24456;&#26222;&#36941;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#20248;&#21270;&#26694;&#26550;&#20351;&#29992;&#65288;&#26114;&#36149;&#30340;&#65289;&#26799;&#24230;&#35745;&#31639;&#26469;&#22312;\emph{&#30446;&#26631;&#31354;&#38388;}&#65288;&#20363;&#22914;&#29992;&#20110;&#20998;&#31867;&#30340;&#32447;&#24615;&#27169;&#22411;&#36755;&#20986;&#30340;logits&#65289;&#20013;&#26500;&#36896;&#20195;&#29702;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#26368;&#23567;&#21270;&#12290;&#36825;&#20801;&#35768;&#23545;&#27169;&#22411;&#36827;&#34892;&#22810;&#20010;&#21442;&#25968;&#26356;&#26032;&#65292;&#25674;&#38144;&#26799;&#24230;&#35745;&#31639;&#30340;&#25104;&#26412;&#12290;&#22312;&#23436;&#20840;&#25209;&#22788;&#29702;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#20195;&#29702;&#26159;&#25439;&#22833;&#30340;&#20840;&#23616;&#19978;&#30028;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65288;&#23616;&#37096;&#22320;&#65289;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#26368;&#32456;&#30340;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;&#30830;&#20445;&#25910;&#25947;&#21040;&#25439;&#22833;&#30340;&#31283;&#23450;&#28857;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#38543;&#26426;&#35774;&#32622;&#20013;&#23454;&#20363;&#21270;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;SSO&#31639;&#27861;&#65292;&#21487;&#20197;&#35270;&#20026;&#30446;&#26631;&#31354;&#38388;&#20013;&#30340;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider minimizing functions for which it is expensive to compute the (possibly stochastic) gradient. Such functions are prevalent in reinforcement learning, imitation learning and adversarial training. Our target optimization framework uses the (expensive) gradient computation to construct surrogate functions in a \emph{target space} (e.g. the logits output by a linear model for classification) that can be minimized efficiently. This allows for multiple parameter updates to the model, amortizing the cost of gradient computation. In the full-batch setting, we prove that our surrogate is a global upper-bound on the loss, and can be (locally) minimized using a black-box optimization algorithm. We prove that the resulting majorization-minimization algorithm ensures convergence to a stationary point of the loss. Next, we instantiate our framework in the stochastic setting and propose the $SSO$ algorithm, which can be viewed as projected stochastic gradient descent in the target space. 
&lt;/p&gt;</description></item><item><title>BLiE&#26159;&#19968;&#31181;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#21482;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;BLiE&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#25628;&#32034;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#35843;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.01539</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;Lipschitz&#20048;&#35266;&#31574;&#30053;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Lipschitz Bandits Approach for Continuous Hyperparameter Optimization. (arXiv:2302.01539v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01539
&lt;/p&gt;
&lt;p&gt;
BLiE&#26159;&#19968;&#31181;&#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#21482;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;BLiE&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#25628;&#32034;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#26159;&#26368;&#20851;&#38190;&#30340;&#38382;&#39064;&#20043;&#19968;&#65292;&#22240;&#20026;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#23545;&#26368;&#32456;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;HPO&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#35201;&#20040;&#38656;&#35201;&#24378;&#30340;&#20551;&#35774;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BLiE&#8212;&#8212;&#19968;&#31181;&#22522;&#20110;Lipschitz&#20048;&#35266;&#31574;&#30053;&#30340;HPO&#31639;&#27861;&#65292;&#23427;&#21482;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#12290;BLiE&#21033;&#29992;&#30446;&#26631;&#20989;&#25968;&#30340;&#26223;&#35266;&#20197;&#33258;&#36866;&#24212;&#22320;&#25628;&#32034;&#36229;&#21442;&#25968;&#31354;&#38388;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;$(i)$ BLiE&#21457;&#29616;&#20855;&#26377;$O(\frac{1}{\epsilon})^{d_z+\beta}$&#20010;&#24635;&#39044;&#31639;&#30340;$\epsilon$&#26368;&#20248;&#36229;&#21442;&#25968;&#65292;&#20854;&#20013;$d_z$&#21644;$\beta$&#26159;&#38382;&#39064;&#20869;&#22312;&#30340;&#65307;$(ii)$ BLiE&#20855;&#26377;&#39640;&#24230;&#21487;&#24182;&#34892;&#24615;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;BLiE&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;HPO&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#24212;&#29992;BLiE&#25628;&#32034;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#35843;&#24230;&#12290;&#19982;&#40664;&#35748;&#35843;&#24230;&#30456;&#27604;&#36739;&#65292;BLiE&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most critical problems in machine learning is HyperParameter Optimization (HPO), since choice of hyperparameters has a significant impact on final model performance. Although there are many HPO algorithms, they either have no theoretical guarantees or require strong assumptions. To this end, we introduce BLiE -- a Lipschitz-bandit-based algorithm for HPO that only assumes Lipschitz continuity of the objective function. BLiE exploits the landscape of the objective function to adaptively search over the hyperparameter space. Theoretically, we show that $(i)$ BLiE finds an $\epsilon$-optimal hyperparameter with $O \left( \frac{1}{\epsilon} \right)^{d_z + \beta}$ total budgets, where $d_z$ and $\beta$ are problem intrinsic; $(ii)$ BLiE is highly parallelizable. Empirically, we demonstrate that BLiE outperforms the state-of-the-art HPO algorithms on benchmark tasks. We also apply BLiE to search for noise schedule of diffusion models. Comparison with the default schedule shows tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23383;&#33829;&#38144;&#20869;&#23481;&#35774;&#35745;&#35780;&#20998;&#21644;&#25552;&#21462;&#35265;&#35299;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20026;&#33829;&#38144;&#20154;&#21592;&#25552;&#20379;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#30340;&#35265;&#35299;&#21644;&#35774;&#35745;&#24314;&#35758;&#65292;&#20197;&#25913;&#21892;&#20854;&#21019;&#24847;&#36807;&#31243;&#24182;&#26174;&#30528;&#25552;&#39640;&#23458;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.01416</link><description>&lt;p&gt;
&#25968;&#23383;&#33829;&#38144;&#20869;&#23481;&#35774;&#35745;&#30340;&#31070;&#32463;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Neural Insights for Digital Marketing Content Design. (arXiv:2302.01416v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23383;&#33829;&#38144;&#20869;&#23481;&#35774;&#35745;&#35780;&#20998;&#21644;&#25552;&#21462;&#35265;&#35299;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20026;&#33829;&#38144;&#20154;&#21592;&#25552;&#20379;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#30340;&#35265;&#35299;&#21644;&#35774;&#35745;&#24314;&#35758;&#65292;&#20197;&#25913;&#21892;&#20854;&#21019;&#24847;&#36807;&#31243;&#24182;&#26174;&#30528;&#25552;&#39640;&#23458;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#33829;&#38144;&#20013;&#65292;&#23581;&#35797;&#26032;&#30340;&#32593;&#31449;&#20869;&#23481;&#26159;&#25552;&#39640;&#23458;&#25143;&#21442;&#19982;&#24230;&#30340;&#20851;&#38190;&#26464;&#26438;&#20043;&#19968;&#12290;&#20294;&#26159;&#65292;&#21019;&#24314;&#25104;&#21151;&#30340;&#33829;&#38144;&#20869;&#23481;&#26159;&#19968;&#39033;&#25163;&#21160;&#19988;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#20026;&#33829;&#38144;&#20154;&#21592;&#25552;&#20379;&#22522;&#20110;AI&#39537;&#21160;&#30340;&#21487;&#34892;&#24615;&#35265;&#35299;&#65292;&#20197;&#25913;&#36827;&#20854;&#21019;&#24847;&#36807;&#31243;&#65292;&#20174;&#32780;&#23558;&#20869;&#23481;&#21019;&#24314;&#21644;&#22312;&#32447;&#23454;&#39564;&#20043;&#38388;&#30340;&#24490;&#29615;&#38381;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23545;&#33829;&#38144;&#20869;&#23481;&#35774;&#35745;&#36827;&#34892;&#35780;&#20998;&#21644;&#25552;&#21462;&#35265;&#35299;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#33829;&#38144;&#20869;&#23481;&#30340;&#21560;&#24341;&#21147;&#65292;&#21518;&#22788;&#29702;&#24402;&#22240;&#26041;&#27861;&#20026;&#33829;&#38144;&#20154;&#21592;&#29983;&#25104;&#26377;&#38024;&#23545;&#24615;&#30340;&#35265;&#35299;&#65292;&#20197;&#25913;&#21892;&#29305;&#23450;&#33829;&#38144;&#20301;&#32622;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#35265;&#35299;&#19981;&#20165;&#25351;&#20986;&#20102;&#24403;&#21069;&#32473;&#23450;&#20869;&#23481;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#36824;&#26681;&#25454;&#21382;&#21490;&#25968;&#25454;&#25552;&#20379;&#20102;&#35774;&#35745;&#24314;&#35758;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35780;&#20998;&#27169;&#22411;&#21644;&#35265;&#35299;&#22312;&#25968;&#37327;&#21644;&#36136;&#37327;&#19978;&#22343;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#23458;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In digital marketing, experimenting with new website content is one of the key levers to improve customer engagement. However, creating successful marketing content is a manual and time-consuming process that lacks clear guiding principles. This paper seeks to close the loop between content creation and online experimentation by offering marketers AI-driven actionable insights based on historical data to improve their creative process. We present a neural-network-based system that scores and extracts insights from a marketing content design, namely, a multimodal neural network predicts the attractiveness of marketing contents, and a post-hoc attribution method generates actionable insights for marketers to improve their content in specific marketing locations. Our insights not only point out the advantages and drawbacks of a given current content, but also provide design recommendations based on historical data. We show that our scoring model and insights work well both quantitatively 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.00422</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Robust online active learning. (arXiv:2302.00422v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#33719;&#24471;&#26631;&#35760;&#30340;&#35266;&#27979;&#25968;&#25454;&#24182;&#19981;&#31616;&#21333;&#65292;&#36890;&#24120;&#38656;&#35201;&#20154;&#24037;&#19987;&#23478;&#24178;&#39044;&#25110;&#20351;&#29992;&#26114;&#36149;&#30340;&#27979;&#35797;&#35774;&#22791;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#25311;&#21512;&#27169;&#22411;&#26102;&#26368;&#20449;&#24687;&#25968;&#25454;&#28857;&#30340;&#24314;&#35758;&#12290;&#20943;&#23569;&#27169;&#22411;&#24320;&#21457;&#25152;&#38656;&#30340;&#35266;&#27979;&#25968;&#25454;&#25968;&#37327;&#21487;&#20197;&#20943;&#36731;&#35757;&#32451;&#25152;&#38656;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#26631;&#35760;&#30456;&#20851;&#30340;&#25805;&#20316;&#25903;&#20986;&#12290;&#29305;&#21035;&#26159;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#22312;&#38656;&#35201;&#22312;&#26497;&#30701;&#26102;&#38388;&#20869;&#20915;&#23450;&#26159;&#21542;&#33719;&#21462;&#25968;&#25454;&#28857;&#26631;&#35760;&#30340;&#39640;&#23481;&#37327;&#29983;&#20135;&#36807;&#31243;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#33268;&#21147;&#20110;&#24320;&#21457;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20294;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#36825;&#20123;&#26041;&#27861;&#30340;&#34892;&#20026;&#20173;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#32447;&#20027;&#21160;&#32447;&#24615;&#22238;&#24402;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#35777;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many industrial applications, obtaining labeled observations is not straightforward as it often requires the intervention of human experts or the use of expensive testing equipment. In these circumstances, active learning can be highly beneficial in suggesting the most informative data points to be used when fitting a model. Reducing the number of observations needed for model development alleviates both the computational burden required for training and the operational expenses related to labeling. Online active learning, in particular, is useful in high-volume production processes where the decision about the acquisition of the label for a data point needs to be taken within an extremely short time frame. However, despite the recent efforts to develop online active learning strategies, the behavior of these methods in the presence of outliers has not been thoroughly examined. In this work, we investigate the performance of online active linear regression in contaminated data strea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#20027;&#35201;&#25506;&#35752;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#21644;&#26368;&#20808;&#36827;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.00058</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65306;&#32508;&#36848;(arXiv&#65306;2302.00058v2 [cs.LG]&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Graph-based Time-Series Anomaly Detection: A Survey. (arXiv:2302.00058v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#20027;&#35201;&#25506;&#35752;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#21644;&#26368;&#20808;&#36827;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#31995;&#32479;&#25345;&#32493;&#25910;&#38598;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22914;&#30005;&#23376;&#21830;&#21153;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#36710;&#36742;&#32500;&#25252;&#21644;&#21307;&#30103;&#30417;&#27979;&#31561;&#39046;&#22495;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20294;&#30001;&#20110;&#38656;&#35201;&#21516;&#26102;&#32771;&#34385;&#21464;&#37327;&#20869;&#37096;&#21644;&#21464;&#37327;&#38388;&#30340;&#20381;&#36182;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#38590;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#32780;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;(G-TSAD)&#12290;&#39318;&#20808;&#25506;&#35752;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#21518;&#22312;&#26102;&#38388;&#24207;&#21015;&#32972;&#26223;&#19979;&#22238;&#39038;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#25216;&#26415;&#22914;&#20309;&#24212;&#29992;&#20110;&#23454;&#38469;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent advances in technology, a wide range of systems continue to collect a large amount of data over time and thus generate time series. Time-Series Anomaly Detection (TSAD) is an important task in various time-series applications such as e-commerce, cybersecurity, vehicle maintenance, and healthcare monitoring. However, this task is very challenging as it requires considering both the intra-variable dependency and the inter-variable dependency, where a variable can be defined as an observation in time series data. Recent graph-based approaches have made impressive progress in tackling the challenges of this field. In this survey, we conduct a comprehensive and up-to-date review of Graph-based TSAD (G-TSAD). First, we explore the significant potential of graph representation learning for time-series data. Then, we review state-of-the-art graph anomaly detection techniques in the context of time series and discuss their strengths and drawbacks. Finally, we discuss the technic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36882;&#24402;&#20248;&#21270;&#31561;&#20215;&#24615;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20844;&#24335;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#19988;&#25512;&#23548;&#20102;&#31639;&#27861;&#30340;&#36951;&#25022;&#19978;&#30028;&#21644;&#26497;&#23567;-&#26368;&#22823;&#19979;&#30028;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#23454;&#29616;&#30340;&#36951;&#25022;&#29575;&#23545;&#20110;&#24773;&#27468;&#25968;&#21644;&#21160;&#20316;&#25968;&#20855;&#26377;&#26368;&#20248;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12601</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#20248;&#21270;&#31561;&#20215;&#24615;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#36951;&#25022;&#36793;&#30028;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Regret Bounds for Markov Decision Processes with Recursive Optimized Certainty Equivalents. (arXiv:2301.12601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36882;&#24402;&#20248;&#21270;&#31561;&#20215;&#24615;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20844;&#24335;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#19988;&#25512;&#23548;&#20102;&#31639;&#27861;&#30340;&#36951;&#25022;&#19978;&#30028;&#21644;&#26497;&#23567;-&#26368;&#22823;&#19979;&#30028;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#23454;&#29616;&#30340;&#36951;&#25022;&#29575;&#23545;&#20110;&#24773;&#27468;&#25968;&#21644;&#21160;&#20316;&#25968;&#20855;&#26377;&#26368;&#20248;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#31561;&#20215;&#24615;&#65288;OCE&#65289;&#26159;&#19968;&#31867;&#39118;&#38505;&#27979;&#37327;&#65292;&#28085;&#30422;&#20102;&#37325;&#35201;&#30340;&#23454;&#20363;&#65292;&#22914;&#29109;&#39118;&#38505;&#65292;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#21644;&#22343;&#20540;&#26041;&#24046;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#34920;&#26684;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#36882;&#24402;OCE&#30340;&#24773;&#33410;&#21270;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20844;&#24335;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20540;&#36845;&#20195;&#21644;&#19978;&#32622;&#20449;&#30028;&#30340;&#26377;&#25928;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#26497;&#23567;-&#26368;&#22823;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#30340;&#36951;&#25022;&#29575;&#23545;&#20110;&#24773;&#27468;&#25968;&#21644;&#21160;&#20316;&#25968;&#20855;&#26377;&#26368;&#20248;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimized certainty equivalent (OCE) is a family of risk measures that cover important examples such as entropic risk, conditional value-at-risk and mean-variance models. In this paper, we propose a new episodic risk-sensitive reinforcement learning formulation based on tabular Markov decision processes with recursive OCEs. We design an efficient learning algorithm for this problem based on value iteration and upper confidence bound. We derive an upper bound on the regret of the proposed algorithm, and also establish a minimax lower bound. Our bounds show that the regret rate achieved by our proposed algorithm has optimal dependence on the number of episodes and the number of actions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22240;&#26524;&#22270;&#26410;&#30693;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#20351;&#29992;&#21407;&#23376;&#24178;&#39044;&#25214;&#21040;&#22870;&#21169;&#33410;&#28857;&#29238;&#33410;&#28857;&#30340;&#26377;&#25928;&#31639;&#27861;&#65292;&#24182;&#25193;&#23637;&#33267;&#22870;&#21169;&#33410;&#28857;&#20855;&#26377;&#22810;&#20010;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#12290;&#21516;&#26102;&#65292;&#36824;&#24471;&#20986;&#20102;&#31639;&#27861;&#25191;&#34892;&#26399;&#26395;&#24178;&#39044;&#27425;&#25968;&#30340;&#31934;&#30830;&#26041;&#31243;&#65292;&#24182;&#35777;&#26126;&#22312;&#29305;&#23450;&#22270;&#24418;&#26465;&#20214;&#19979;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#24555;&#36895;&#25191;&#34892;&#23545;&#25968;&#12290;</title><link>http://arxiv.org/abs/2301.11401</link><description>&lt;p&gt;
&#26080;&#38656;&#22270;&#24418;&#23398;&#20064;&#30340;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Causal Bandits without Graph Learning. (arXiv:2301.11401v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22240;&#26524;&#22270;&#26410;&#30693;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#20351;&#29992;&#21407;&#23376;&#24178;&#39044;&#25214;&#21040;&#22870;&#21169;&#33410;&#28857;&#29238;&#33410;&#28857;&#30340;&#26377;&#25928;&#31639;&#27861;&#65292;&#24182;&#25193;&#23637;&#33267;&#22870;&#21169;&#33410;&#28857;&#20855;&#26377;&#22810;&#20010;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#12290;&#21516;&#26102;&#65292;&#36824;&#24471;&#20986;&#20102;&#31639;&#27861;&#25191;&#34892;&#26399;&#26395;&#24178;&#39044;&#27425;&#25968;&#30340;&#31934;&#30830;&#26041;&#31243;&#65292;&#24182;&#35777;&#26126;&#22312;&#29305;&#23450;&#22270;&#24418;&#26465;&#20214;&#19979;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#24555;&#36895;&#25191;&#34892;&#23545;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24403;&#22240;&#26524;&#22270;&#19981;&#30693;&#36947;&#30340;&#24773;&#20917;&#19979;&#30340;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#21407;&#23376;&#24178;&#39044;&#25214;&#21040;&#22870;&#21169;&#33410;&#28857;&#30340;&#29238;&#33410;&#28857;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#31639;&#27861;&#25191;&#34892;&#30340;&#26399;&#26395;&#24178;&#39044;&#27425;&#25968;&#30340;&#31934;&#30830;&#26041;&#31243;&#65292;&#24182;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#22270;&#24418;&#26465;&#20214;&#19979;&#65292;&#23427;&#21487;&#20197;&#24555;&#36895;&#25191;&#34892;&#23545;&#25968;&#65292;&#25110;&#32773;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#65292;&#25191;&#34892;&#26356;&#24930;&#20294;&#20173;&#28982;&#26159;&#21464;&#37327;&#25968;&#30340;&#20122;&#32447;&#24615;&#32423;&#21035;&#12290;&#25105;&#20204;&#27491;&#24335;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#23427;&#31526;&#21512;&#25105;&#20204;&#20026;&#25191;&#34892;&#21407;&#23376;&#24178;&#39044;&#30340;&#20219;&#20309;&#31639;&#27861;&#24314;&#31435;&#30340;&#36890;&#29992;&#19979;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#25193;&#23637;&#21040;&#22870;&#21169;&#33410;&#28857;&#20855;&#26377;&#22810;&#20010;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#12290;&#20351;&#29992;&#36825;&#20010;&#31639;&#27861;&#21644;&#26469;&#33258;&#36172;&#21338;&#25991;&#29486;&#30340;&#26631;&#20934;&#31639;&#27861;&#21487;&#20197;&#23548;&#33268;&#25913;&#36827;&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the causal bandit problem when the causal graph is unknown and develop an efficient algorithm for finding the parent node of the reward node using atomic interventions. We derive the exact equation for the expected number of interventions performed by the algorithm and show that under certain graphical conditions it could perform either logarithmically fast or, under more general assumptions, slower but still sublinearly in the number of variables. We formally show that our algorithm is optimal as it meets the universal lower bound we establish for any algorithm that performs atomic interventions. Finally, we extend our algorithm to the case when the reward node has multiple parents. Using this algorithm together with a standard algorithm from bandit literature leads to improved regret bounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#24688;&#24403;&#22320;&#34920;&#24449;&#20102;&#34987;&#35757;&#32451;&#23545;&#25968;&#25454;&#25311;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#21151;&#33021;&#23646;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#37322;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20123;&#20248;&#31168;&#24615;&#33021;&#12289;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20013;&#36339;&#36291;&#36830;&#25509;&#21644;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#20351;&#29992;&#12289;&#31232;&#30095;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#20197;&#21450;&#20026;&#20160;&#20040;&#31070;&#32463;&#32593;&#32476;&#21487;&#20316;&#20026;&#20248;&#31168;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;</title><link>http://arxiv.org/abs/2301.09554</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#31232;&#30095;&#27491;&#21017;&#21270;&#65306;&#20449;&#21495;&#22788;&#29702;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Meets Sparse Regularization: A Signal Processing Perspective. (arXiv:2301.09554v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#24688;&#24403;&#22320;&#34920;&#24449;&#20102;&#34987;&#35757;&#32451;&#23545;&#25968;&#25454;&#25311;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#21151;&#33021;&#23646;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#37322;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20123;&#20248;&#31168;&#24615;&#33021;&#12289;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20013;&#36339;&#36291;&#36830;&#25509;&#21644;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#20351;&#29992;&#12289;&#31232;&#30095;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#20197;&#21450;&#20026;&#20160;&#20040;&#31070;&#32463;&#32593;&#32476;&#21487;&#20316;&#20026;&#20248;&#31168;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37117;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#20005;&#26684;&#30340;&#25968;&#23398;&#29702;&#35770;&#65292;&#33021;&#22815;&#20805;&#20998;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24778;&#20154;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30456;&#23545;&#36739;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#30340;&#24320;&#31471;&#12290;&#36825;&#20010;&#26694;&#26550;&#24688;&#24403;&#22320;&#34920;&#24449;&#20102;&#34987;&#35757;&#32451;&#23545;&#25968;&#25454;&#25311;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#21151;&#33021;&#23646;&#24615;&#12290;&#25903;&#25345;&#36825;&#20010;&#26694;&#26550;&#30340;&#20851;&#38190;&#25968;&#23398;&#24037;&#20855;&#21253;&#25324;&#21464;&#25442;&#22495;&#31232;&#30095;&#27491;&#21017;&#21270;&#12289;&#35745;&#31639;&#26426;&#26029;&#23618;&#25668;&#24433;&#30340;Radon&#21464;&#25442;&#21644;&#36924;&#36817;&#29702;&#35770;&#65292;&#36825;&#20123;&#25216;&#26415;&#37117;&#28145;&#28145;&#26681;&#26893;&#20110;&#20449;&#21495;&#22788;&#29702;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#37322;&#20102;&#21152;&#26435;&#34928;&#20943;&#27491;&#21017;&#21270;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20316;&#29992;&#65292;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20013;&#36339;&#36291;&#36830;&#25509;&#21644;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#20351;&#29992;&#65292;&#31232;&#30095;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#31070;&#32463;&#32593;&#32476;&#21487;&#20316;&#20026;&#20248;&#31168;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been wildly successful in practice and most state-of-the-art machine learning methods are based on neural networks. Lacking, however, is a rigorous mathematical theory that adequately explains the amazing performance of deep neural networks. In this article, we present a relatively new mathematical framework that provides the beginning of a deeper understanding of deep learning. This framework precisely characterizes the functional properties of neural networks that are trained to fit to data. The key mathematical tools which support this framework include transform-domain sparse regularization, the Radon transform of computed tomography, and approximation theory, which are all techniques deeply rooted in signal processing. This framework explains the effect of weight decay regularization in neural network training, the use of skip connections and low-rank weight matrices in network architectures, the role of sparsity in neural networks, and explains why neural networ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#24490;&#29615;&#39044;&#27979;&#23398;&#20064;&#31574;&#30053;&#30340;&#32806;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;CPINN&#65289;&#65292;&#29992;&#20110;&#36719;&#20256;&#24863;&#24314;&#27169;&#20013;&#30340;&#21547;&#38750;&#27979;&#37327;&#28304;&#39033;&#30340;PDEs&#27714;&#35299;&#65292;&#35777;&#26126;&#20102;CPINN&#20855;&#26377;&#28385;&#36275;PDEs&#35299;&#30340;&#36817;&#20284;&#23481;&#37327;&#65292;&#25552;&#39640;&#20102;&#36719;&#20256;&#24863;&#24314;&#27169;&#22312;&#26102;&#31354;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.08618</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#39044;&#27979;&#32806;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#36719;&#20256;&#24863;&#24314;&#27169;&#20013;&#27714;&#35299;&#21547;&#38750;&#27979;&#37327;&#28304;&#39033;&#30340;PDEs
&lt;/p&gt;
&lt;p&gt;
Solving PDEs with Unmeasurable Source Terms Using Coupled Physics-Informed Neural Network with Recurrent Prediction in Soft Sensor Modeling. (arXiv:2301.08618v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#24490;&#29615;&#39044;&#27979;&#23398;&#20064;&#31574;&#30053;&#30340;&#32806;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;CPINN&#65289;&#65292;&#29992;&#20110;&#36719;&#20256;&#24863;&#24314;&#27169;&#20013;&#30340;&#21547;&#38750;&#27979;&#37327;&#28304;&#39033;&#30340;PDEs&#27714;&#35299;&#65292;&#35777;&#26126;&#20102;CPINN&#20855;&#26377;&#28385;&#36275;PDEs&#35299;&#30340;&#36817;&#20284;&#23481;&#37327;&#65292;&#25552;&#39640;&#20102;&#36719;&#20256;&#24863;&#24314;&#27169;&#22312;&#26102;&#31354;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21547;&#38750;&#27979;&#37327;&#28304;&#39033;&#30340;&#38750;&#40784;&#27425;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#22312;&#36719;&#20256;&#24863;&#24314;&#27169;&#20013;&#38590;&#20197;&#24456;&#22909;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#24490;&#29615;&#39044;&#27979;&#65288;RP&#65289;&#23398;&#20064;&#31574;&#30053;&#30340;&#32806;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;CPINN&#65289;&#26469;&#36827;&#34892;&#36719;&#20256;&#24863;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#21253;&#21547;NetU&#21644;NetG&#30340;CPINN&#65292;&#20854;&#20013;NetU&#29992;&#20110;&#36817;&#20284;&#30740;&#31350;PDEs&#30340;&#35299;&#65292;NetG&#29992;&#20110;&#27491;&#21017;&#21270;NetU&#30340;&#35757;&#32451;&#65292;&#20004;&#20010;&#32593;&#32476;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;&#25968;&#25454;-&#29289;&#29702;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;&#38543;&#21518;&#65292;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;CPINN&#20855;&#26377;&#28385;&#36275;PDEs&#35299;&#30340;&#36817;&#20284;&#23481;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#35757;&#32451;&#31574;&#30053;&#26469;&#20248;&#21270;&#21644;&#32806;&#21512;&#20004;&#20010;&#32593;&#32476;&#20197;&#23454;&#29616;CPINN&#30340;&#21442;&#25968;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#22312;NetU&#20013;&#24341;&#20837;&#24490;&#29615;&#26426;&#21046;&#26469;&#36827;&#34892;&#26410;&#26469;&#29366;&#24577;&#30340;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;NetU-RP&#65292;&#20197;&#25552;&#39640;&#36719;&#20256;&#24863;&#24314;&#27169;&#22312;&#26102;&#31354;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#27169;&#25311;&#25391;&#21160;&#20301;&#31227;&#31995;&#32479;&#30340;&#20363;&#23376;&#65292;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;CPINN-RP&#26041;&#27861;&#22312;&#27714;&#35299;&#21547;&#38750;&#27979;&#37327;&#28304;&#39033;&#30340;PDEs&#21644;&#36719;&#20256;&#24863;&#24314;&#27169;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonhomogeneous partial differential equations (PDEs) are an applicable model in soft sensor modeling for describing spatiotemporal industrial systems with unmeasurable source terms, which cannot be well solved by existing physics-informed neural networks (PINNs). To this end, a coupled PINN (CPINN) with a recurrent prediction (RP) learning strategy (CPINN-RP) is proposed for soft sensor modeling in spatiotemporal industrial processes, such as vibration displacement. First, CPINN containing NetU and NetG is proposed. NetU is used to approximate the solutions to PDEs under study and NetG is used to regularize the training of NetU. The two networks are integrated into a data-physics-hybrid loss function. Then, we theoretically prove that the proposed CPINN has a satisfying approximation capacity to the PDEs solutions. Besides the theoretical aspects, we propose a hierarchical training strategy to optimize and couple the two networks to achieve the parameters of CPINN. Secondly, NetU-RP is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#25216;&#26415;&#25552;&#39640;&#22270;&#20687;&#20445;&#30495;&#24230;&#30340;&#20154;&#33080;&#21464;&#24418;&#25915;&#20987;&#65292;&#36890;&#36807;&#23558;&#20004;&#31181;&#29305;&#24449;&#32467;&#21512;&#25552;&#39640;&#20102;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#21644;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#31995;&#32479;&#30340;&#26131;&#21463;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.04218</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#25216;&#26415;&#36827;&#34892;&#39640;&#20445;&#30495;&#24230;&#20154;&#33080;&#21464;&#24418;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Leveraging Diffusion For Strong and High Quality Face Morphing Attacks. (arXiv:2301.04218v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#25216;&#26415;&#25552;&#39640;&#22270;&#20687;&#20445;&#30495;&#24230;&#30340;&#20154;&#33080;&#21464;&#24418;&#25915;&#20987;&#65292;&#36890;&#36807;&#23558;&#20004;&#31181;&#29305;&#24449;&#32467;&#21512;&#25552;&#39640;&#20102;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#21644;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#31995;&#32479;&#30340;&#26131;&#21463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#21464;&#24418;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21576;&#29616;&#30001;&#20004;&#20010;&#19981;&#21516;&#36523;&#20221;&#30340;&#29983;&#29289;&#29305;&#24449;&#32452;&#25104;&#30340;&#21464;&#24418;&#22270;&#20687;&#26469;&#27450;&#39575;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#65292;&#20197;&#26399;&#26395;&#35302;&#21457;&#19968;&#20010;&#38169;&#35823;&#30340;&#25509;&#21463;&#65292;&#20174;&#32780;&#23545;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#26550;&#26500;&#26469;&#25913;&#36827;&#22270;&#20687;&#35270;&#35273;&#20445;&#30495;&#24230;&#30340;&#21464;&#24418;&#25915;&#20987;&#65292;&#24182;&#25552;&#39640;&#21464;&#24418;&#25915;&#20987;&#34920;&#31034;&#20004;&#31181;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;Frechet Inception Distance (FID)&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#30340;&#35270;&#35273;&#20445;&#30495;&#24230;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#27979;&#37327;FR&#31995;&#32479;&#23545;&#25152;&#25552;&#20986;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#12290;&#36824;&#27979;&#35797;&#20102;&#19968;&#31181;&#21464;&#24418;&#25915;&#20987;&#26816;&#27979;&#22120;&#26469;&#26816;&#27979;&#25152;&#25552;&#20986;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face morphing attacks seek to deceive a Face Recognition (FR) system by presenting a morphed image consisting of the biometric qualities from two different identities with the aim of triggering a false acceptance with one of the two identities, thereby presenting a significant threat to biometric systems. The success of a morphing attack is dependent on the ability of the morphed image to represent the biometric characteristics of both identities that were used to create the image. We present a novel morphing attack that uses a Diffusion-based architecture to improve the visual fidelity of the image and the ability of the morphing attack to represent characteristics from both identities. We demonstrate the effectiveness of the proposed attack by evaluating its visual fidelity via the Frechet Inception Distance (FID). Also, extensive experiments are conducted to measure the vulnerability of FR systems to the proposed attack. The ability of a morphing attack detector to detect the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MSCDA&#26694;&#26550;&#26469;&#35299;&#20915;&#20083;&#33146;MRI&#20998;&#21106;&#20013;&#39046;&#22495;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#20854;&#20351;&#29992;&#20102;&#22810;&#32423;&#35821;&#20041;&#24341;&#23548;&#23545;&#27604;&#21644;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#23545;&#40784;&#29305;&#24449;&#34920;&#31034;&#12290;&#36890;&#36807;&#20687;&#32032;&#23545;&#27604;&#12289;&#36136;&#24515;&#23545;&#27604;&#20197;&#21450;&#31867;&#21035;&#36328;&#22495;&#37319;&#26679;&#31574;&#30053;&#30340;&#24212;&#29992;&#65292;MSCDA&#22312;&#23567;&#25968;&#25454;&#38598;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.02554</link><description>&lt;p&gt;
MSCDA: &#22810;&#32423;&#35821;&#20041;&#24341;&#23548;&#23545;&#27604;&#25552;&#39640;&#20102;&#23567;&#25968;&#25454;&#38598;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#30340;&#20083;&#33146;MRI&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MSCDA: Multi-level Semantic-guided Contrast Improves Unsupervised Domain Adaptation for Breast MRI Segmentation in Small Datasets. (arXiv:2301.02554v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MSCDA&#26694;&#26550;&#26469;&#35299;&#20915;&#20083;&#33146;MRI&#20998;&#21106;&#20013;&#39046;&#22495;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#20854;&#20351;&#29992;&#20102;&#22810;&#32423;&#35821;&#20041;&#24341;&#23548;&#23545;&#27604;&#21644;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#23545;&#40784;&#29305;&#24449;&#34920;&#31034;&#12290;&#36890;&#36807;&#20687;&#32032;&#23545;&#27604;&#12289;&#36136;&#24515;&#23545;&#27604;&#20197;&#21450;&#31867;&#21035;&#36328;&#22495;&#37319;&#26679;&#31574;&#30053;&#30340;&#24212;&#29992;&#65292;MSCDA&#22312;&#23567;&#25968;&#25454;&#38598;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20083;&#33146;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#24212;&#29992;&#20110;&#32452;&#32455;&#20998;&#21106;&#26041;&#38754;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#30001;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#35774;&#22791;&#12289;&#37319;&#38598;&#21327;&#35758;&#20197;&#21450;&#29983;&#29289;&#23398;&#19978;&#30340;&#24322;&#36136;&#24615;&#25152;&#23548;&#33268;&#30340;&#39046;&#22495;&#20559;&#31227;&#20173;&#28982;&#26159;&#23454;&#29616;&#20020;&#24202;&#24212;&#29992;&#30340;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32423;&#35821;&#20041;&#24341;&#23548;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;&#65288;MSCDA&#65289;&#26694;&#26550;&#26469;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#33258;&#35757;&#32451;&#19982;&#23545;&#27604;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#35843;&#25972;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#20687;&#32032;&#21040;&#20687;&#32032;&#12289;&#20687;&#32032;&#21040;&#36136;&#24515;&#21644;&#36136;&#24515;&#21040;&#36136;&#24515;&#23545;&#27604;&#34701;&#20837;&#23545;&#27604;&#25439;&#22833;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#19981;&#21516;&#23618;&#27425;&#22270;&#20687;&#32972;&#21518;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#31867;&#21035;&#36328;&#22495;&#37319;&#26679;&#31574;&#30053;&#20174;&#30446;&#26631;&#22270;&#20687;&#20013;&#37319;&#26679;&#38170;&#28857;&#65292;&#24182;&#26500;&#24314;&#28151;&#21512;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#20083;&#33146;MRI&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21482;&#26377;&#23569;&#37327;&#26377;&#26631;&#31614;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) applied to breast tissue segmentation in magnetic resonance imaging (MRI) has received increased attention in the last decade, however, the domain shift which arises from different vendors, acquisition protocols, and biological heterogeneity, remains an important but challenging obstacle on the path towards clinical implementation. In this paper, we propose a novel Multi-level Semantic-guided Contrastive Domain Adaptation (MSCDA) framework to address this issue in an unsupervised manner. Our approach incorporates self-training with contrastive learning to align feature representations between domains. In particular, we extend the contrastive loss by incorporating pixel-to-pixel, pixel-to-centroid, and centroid-to-centroid contrasts to better exploit the underlying semantic information of the image at different levels. To resolve the data imbalance problem, we utilize a category-wise cross-domain sampling strategy to sample anchors from target images and build a hybri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#20351;&#29992;&#25674;&#38144;&#20248;&#21270;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.00557</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#29992;&#20110;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Learning to Maximize Mutual Information for Dynamic Feature Selection. (arXiv:2301.00557v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#20351;&#29992;&#25674;&#38144;&#20248;&#21270;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26377;&#21161;&#20110;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#65292;&#20294;&#26631;&#20934;&#26041;&#27861;&#26159;&#20351;&#29992;&#38745;&#24577;&#29305;&#24449;&#23376;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#65288;DFS&#65289;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#26681;&#25454;&#29616;&#26377;&#20449;&#24687;&#39034;&#24207;&#26597;&#35810;&#29305;&#24449;&#12290;DFS&#36890;&#24120;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#65292;&#20294;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#26465;&#20214;&#20114;&#20449;&#24687;&#36138;&#23146;&#22320;&#36873;&#25321;&#29305;&#24449;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#21313;&#20998;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#38656;&#35201;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#23436;&#20840;&#35775;&#38382;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25674;&#38144;&#20248;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#21040;&#26368;&#20248;&#29366;&#24577;&#21518;&#21487;&#20197;&#24674;&#22797;&#36138;&#24515;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#20248;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#20174;&#32780;&#39564;&#35777;&#20854;&#20316;&#20026;&#36825;&#20010;&#38382;&#39064;&#30340;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature selection helps reduce data acquisition costs in ML, but the standard approach is to train models with static feature subsets. Here, we consider the dynamic feature selection (DFS) problem where a model sequentially queries features based on the presently available information. DFS is often addressed with reinforcement learning, but we explore a simpler approach of greedily selecting features based on their conditional mutual information. This method is theoretically appealing but requires oracle access to the data distribution, so we develop a learning approach based on amortized optimization. The proposed method is shown to recover the greedy policy when trained to optimality, and it outperforms numerous existing feature selection methods in our experiments, thus validating it as a simple but powerful approach for this problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23384;&#22312;&#26816;&#27979;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#26102;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10802</link><description>&lt;p&gt;
BTS&#65306;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#23460;&#20869;&#20004;&#25151;&#38388;&#23384;&#22312;&#26816;&#27979;&#20013;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
BTS: Bifold Teacher-Student in Semi-Supervised Learning for Indoor Two-Room Presence Detection Under Time-Varying CSI. (arXiv:2212.10802v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23384;&#22312;&#26816;&#27979;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#26102;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#23460;&#20869;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;CSI&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#22914;&#29289;&#20307;&#31227;&#21160;&#12289;&#22823;&#27668;&#22240;&#32032;&#21644;&#26426;&#22120;&#37325;&#21551;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#32791;&#26102;&#30340;&#26631;&#27880;&#26469;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#35774;&#35745;&#19968;&#20010;&#36830;&#32493;&#30417;&#25511;&#30340;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24605;&#20102;&#19968;&#31181;&#21452;&#25240;&#21472;&#24072;&#29983;&#65288;BTS&#65289;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#23384;&#22312;&#20110;&#31995;&#32479;&#20013;&#30340;&#23384;&#22312;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#21407;&#22987;&#23545;&#20598;&#24072;&#29983;&#32593;&#32476;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;CSI&#20013;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#22686;&#24378;&#30340;&#24809;&#32602;&#25439;&#22833;&#20989;&#25968;&#21033;&#29992;&#29109;&#21644;&#36317;&#31163;&#27979;&#37327;&#26469;&#21306;&#20998;&#28145;&#23618;&#29305;&#24449;&#65292;&#38477;&#20302;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, indoor human presence detection based on supervised learning (SL) and channel state information (CSI) has attracted much attention. However, the existing studies that rely on spatial information of CSI are susceptible to environmental changes, such as object movement, atmospheric factors, and machine rebooting, which degrade prediction accuracy. Moreover, SL-based methods require time-consuming labeling for retraining models. Therefore, it is imperative to design a continuously monitored model life-cycle using a semi-supervised learning (SSL) based scheme. In this paper, we conceive a bifold teacher-student (BTS) learning approach for presence detection systems that combines SSL by utilizing partially labeled and unlabeled datasets. The proposed primal-dual teacher-student network intelligently learns spatial and temporal features from labeled and unlabeled CSI. Additionally, the enhanced penalized loss function leverages entropy and distance measures to distinguish dr
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#20250;&#20248;&#20808;&#32771;&#34385;&#31616;&#21333;&#37096;&#20998;&#32780;&#38750;&#22797;&#26434;&#37096;&#20998;&#65292;&#23548;&#33268;&#24615;&#33021;&#24046;&#24322;&#25193;&#22823;&#65292;&#21363;&#20351;&#25968;&#25454;&#38598;&#24179;&#34913;&#19988;&#27809;&#26377;&#32676;&#20307;/&#26631;&#31614;&#20851;&#32852;&#12290;&#22256;&#38590;&#24046;&#24322;&#26159;&#19968;&#20010;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#37327;&#65292;&#24182;&#22312;&#24120;&#29992;&#27169;&#22411;&#20013;&#36827;&#19968;&#27493;&#25918;&#22823;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#20960;&#31181;&#26041;&#27861;&#24471;&#21040;&#32531;&#35299;&#12290;</title><link>http://arxiv.org/abs/2212.06641</link><description>&lt;p&gt;
&#31616;&#21333;&#24615;&#20559;&#35265;&#23548;&#33268;&#34920;&#29616;&#24046;&#24322;&#25193;&#22823;
&lt;/p&gt;
&lt;p&gt;
Simplicity Bias Leads to Amplified Performance Disparities. (arXiv:2212.06641v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06641
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#20250;&#20248;&#20808;&#32771;&#34385;&#31616;&#21333;&#37096;&#20998;&#32780;&#38750;&#22797;&#26434;&#37096;&#20998;&#65292;&#23548;&#33268;&#24615;&#33021;&#24046;&#24322;&#25193;&#22823;&#65292;&#21363;&#20351;&#25968;&#25454;&#38598;&#24179;&#34913;&#19988;&#27809;&#26377;&#32676;&#20307;/&#26631;&#31614;&#20851;&#32852;&#12290;&#22256;&#38590;&#24046;&#24322;&#26159;&#19968;&#20010;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#37327;&#65292;&#24182;&#22312;&#24120;&#29992;&#27169;&#22411;&#20013;&#36827;&#19968;&#27493;&#25918;&#22823;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#20960;&#31181;&#26041;&#27861;&#24471;&#21040;&#32531;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#30340;&#27169;&#22411;&#23558;&#20250;&#25226;&#25968;&#25454;&#38598;&#20013;&#21738;&#20123;&#37096;&#20998;&#35270;&#20026;&#22256;&#38590;&#65311;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;SGD&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#20559;&#21521;&#31616;&#21333;&#30340;&#20559;&#35265;&#65292;&#23548;&#33268;&#23427;&#20204;&#20248;&#20808;&#23398;&#20064;&#22810;&#25968;&#31867;&#65292;&#25110;&#20381;&#36182;&#26377;&#23475;&#30340;&#34920;&#38754;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#20559;&#22909;"&#31616;&#21333;"&#30340;&#24773;&#20917;&#36828;&#19981;&#27490;&#20110;&#27492;&#65306;&#19968;&#20010;&#27169;&#22411;&#21487;&#33021;&#20250;&#20248;&#20808;&#32771;&#34385;&#20219;&#20309;&#23427;&#21457;&#29616;&#31616;&#21333;&#30340;&#31867;&#21035;&#25110;&#25968;&#25454;&#38598;&#20013;&#30340;&#20219;&#20309;&#32452;-&#20197;&#29306;&#29298;&#23427;&#21457;&#29616;&#22797;&#26434;&#30340;&#37096;&#20998;&#30340;&#24615;&#33021;&#24046;&#24322;&#27979;&#37327;&#20026;&#20195;&#20215;&#12290;&#24403;&#19981;&#21516;&#22797;&#26434;&#24230;&#27700;&#24179;&#30340;&#23376;&#38598;&#19982;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#30456;&#19968;&#33268;&#26102;&#65292;&#25105;&#20204;&#23558;&#27492;&#31216;&#20026;&#22256;&#38590;&#24046;&#24322;&#65292;&#21363;&#20351;&#25968;&#25454;&#38598;&#26159;&#24179;&#34913;&#30340;&#65292;&#20063;&#32570;&#20047;&#32676;&#20307;/&#26631;&#31614;&#20851;&#32852;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22256;&#38590;&#24046;&#24322;&#26159;&#19968;&#20010;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#37327;&#65292;&#24182;&#22312;&#36890;&#24120;&#30340;&#24179;&#22343;&#34920;&#29616;&#24471;&#20998;&#25152;&#36873;&#25321;&#30340;&#24120;&#29992;&#27169;&#22411;&#20013;&#36827;&#19968;&#27493;&#25918;&#22823;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#19968;&#20010;&#25193;&#22686;&#22240;&#23376;&#65292;&#20197;&#20415;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#24046;&#24322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24314;&#35758;&#65292;&#21487;&#20197;&#20943;&#23569;&#22256;&#38590;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Which parts of a dataset will a given model find difficult? Recent work has shown that SGD-trained models have a bias towards simplicity, leading them to prioritize learning a majority class, or to rely upon harmful spurious correlations. Here, we show that the preference for "easy" runs far deeper: A model may prioritize any class or group of the dataset that it finds simple-at the expense of what it finds complex-as measured by performance difference on the test set. When subsets with different levels of complexity align with demographic groups, we term this difficulty disparity, a phenomenon that occurs even with balanced datasets that lack group/label associations. We show how difficulty disparity is a model-dependent quantity, and is further amplified in commonly-used models as selected by typical average performance scores. We quantify an amplification factor across a range of settings in order to compare disparity of different models on a fixed dataset. Finally, we present two r
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DP^2&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24046;&#20998;&#38544;&#31169;&#33258;&#36866;&#24212;&#20248;&#21270;&#20013;&#30340;&#38544;&#31169;&#22122;&#22768;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24310;&#36831;&#20294;&#22122;&#22768;&#36739;&#23567;&#30340;&#39044;&#26465;&#20214;&#22120;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#24615;&#30340;&#22909;&#22788;&#65292;&#24182;&#22312;&#20984;&#21644;&#38750;&#20984;&#38382;&#39064;&#19978;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#23558;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;&#22810;&#36798;4&#20493;&#12290;</title><link>http://arxiv.org/abs/2212.00309</link><description>&lt;p&gt;
&#24310;&#36831;&#39044;&#26465;&#20214;&#22120;&#30340;&#24046;&#20998;&#38544;&#31169;&#33258;&#36866;&#24212;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Adaptive Optimization with Delayed Preconditioners. (arXiv:2212.00309v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DP^2&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24046;&#20998;&#38544;&#31169;&#33258;&#36866;&#24212;&#20248;&#21270;&#20013;&#30340;&#38544;&#31169;&#22122;&#22768;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24310;&#36831;&#20294;&#22122;&#22768;&#36739;&#23567;&#30340;&#39044;&#26465;&#20214;&#22120;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#24615;&#30340;&#22909;&#22788;&#65292;&#24182;&#22312;&#20984;&#21644;&#38750;&#20984;&#38382;&#39064;&#19978;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#23558;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;&#22810;&#36798;4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#30340;&#22122;&#38899;&#21487;&#33021;&#20250;&#25269;&#28040;&#22312;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#30340;&#22909;&#22788;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#20844;&#20849;&#25968;&#25454;&#65289;&#26469;&#22686;&#24378;&#33258;&#36866;&#24212;&#20248;&#21270;&#30340;&#25928;&#26524;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#27809;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#21644;&#26377;&#25928;&#36866;&#24212;&#31169;&#26377;&#33258;&#36866;&#24212;&#20248;&#21270;&#30340;&#26799;&#24230;&#20960;&#20309;&#25216;&#26415;&#12290;&#21463;&#21040;&#33258;&#36866;&#24212;&#26041;&#27861;&#21487;&#20197;&#23481;&#24525;&#38472;&#26087;&#39044;&#26465;&#20214;&#22120;&#30340;&#35266;&#23519;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24310;&#36831;&#39044;&#26465;&#20214;&#24046;&#20998;&#38544;&#31169;&#33258;&#36866;&#24212;&#35757;&#32451;&#65288;DP ^ 2&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20855;&#26377;&#24310;&#36831;&#20294;&#22122;&#38899;&#36739;&#23567;&#30340;&#39044;&#26465;&#20214;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#33258;&#36866;&#24212;&#24615;&#30340;&#22909;&#22788;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#38024;&#23545;&#20984;&#21644;&#38750;&#20984;&#38382;&#39064;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#20998;&#26512;&#20102;&#24310;&#36831;&#21644;&#38544;&#31169;&#22122;&#22768;&#20943;&#23569;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#25506;&#32034;&#20102; DP ^ 2&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#23558;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;&#22810;&#36798;4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy noise may negate the benefits of using adaptive optimizers in differentially private model training. Prior works typically address this issue by using auxiliary information (e.g., public data) to boost the effectiveness of adaptive optimization. In this work, we explore techniques to estimate and efficiently adapt to gradient geometry in private adaptive optimization without auxiliary data. Motivated by the observation that adaptive methods can tolerate stale preconditioners, we propose differentially private adaptive training with delayed preconditioners (DP^2), a simple method that constructs delayed but less noisy preconditioners to better realize the benefits of adaptivity. Theoretically, we provide convergence guarantees for our method for both convex and non-convex problems, and analyze trade-offs between delay and privacy noise reduction. Empirically, we explore DP^2 across several real-world datasets, demonstrating that it can improve convergence speed by as much as 4x 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRONOS&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#22522;&#20110;Wi-Fi CSI&#23454;&#29616;&#26080;&#20154;&#35774;&#22791;NLoS&#20154;&#20307;&#26816;&#27979;&#65292;&#21487;&#20197;&#21306;&#20998;&#25151;&#38388;&#20013;&#30340;&#31227;&#21160;&#20154;&#21592;&#21644;&#31354;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;NLoS&#26465;&#20214;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#25151;&#38388;&#20013;&#30340;&#20154;&#29289;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2211.10354</link><description>&lt;p&gt;
CRONOS&#65306;&#22522;&#20110;Wi-Fi CSI&#30340;&#26080;&#20154;&#35774;&#22791;NLoS&#20154;&#20307;&#26816;&#27979;&#30340;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CRONOS: Colorization and Contrastive Learning for Device-Free NLoS Human Presence Detection using Wi-Fi CSI. (arXiv:2211.10354v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRONOS&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#22522;&#20110;Wi-Fi CSI&#23454;&#29616;&#26080;&#20154;&#35774;&#22791;NLoS&#20154;&#20307;&#26816;&#27979;&#65292;&#21487;&#20197;&#21306;&#20998;&#25151;&#38388;&#20013;&#30340;&#31227;&#21160;&#20154;&#21592;&#21644;&#31354;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;NLoS&#26465;&#20214;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#25151;&#38388;&#20013;&#30340;&#20154;&#29289;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#20840;&#38754;&#26234;&#33021;&#21270;&#26381;&#21153;&#21644;&#24212;&#29992;&#30340;&#38656;&#27714;&#36805;&#36895;&#22686;&#38271;&#12290;&#36890;&#36807;&#20256;&#24863;&#22120;&#25110;&#25668;&#20687;&#22836;&#36827;&#34892;&#26080;&#20154;&#26816;&#27979;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#20197;&#21450;&#23545;&#38745;&#27490;&#20154;&#21592;&#30340;&#38169;&#35823;&#26816;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#21830;&#29992;Wi-Fi&#35774;&#22791;&#25429;&#33719;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;(CSI)&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20449;&#21495;&#29305;&#24449;&#65292;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#38750;&#30452;&#35270;(NLoS)&#21644;&#38745;&#24577;&#22330;&#26223;&#19979;&#23384;&#22312;&#20998;&#31867;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#24403;&#19968;&#20010;&#20154;&#38745;&#27490;&#31449;&#22312;&#25151;&#38388;&#35282;&#33853;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRONOS(&#22522;&#20110;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;NLoS&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;)&#30340;&#31995;&#32479;&#65292;&#23427;&#29983;&#25104;&#21160;&#24577;&#30340;&#22797;&#21457;&#22270;(RPs)&#21644;&#39068;&#33394;&#32534;&#30721;&#30340;CSI&#27604;&#29575;&#20197;&#21306;&#20998;&#25151;&#38388;&#20013;&#30340;&#31227;&#21160;&#20154;&#21592;&#21644;&#31354;&#32622;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#26816;&#32034;&#23454;&#36136;&#24615;&#30340;&#34920;&#24449;&#65292;&#20854;&#20013;&#21672;&#35810;&#25439;&#22833;&#34987;&#21046;&#23450;&#20026;&#21306;&#20998;&#21516;&#31867;&#21644;&#24322;&#31867;&#30340;&#23884;&#20837;&#28857;&#20043;&#38388;&#36317;&#31163;&#24230;&#37327;&#30340;&#25439;&#22833;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;NLoS&#26465;&#20214;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#25151;&#38388;&#20013;&#30340;&#20154;&#29289;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the demand for pervasive smart services and applications has increased rapidly. Device-free human detection through sensors or cameras has been widely adopted, but it comes with privacy issues as well as misdetection for motionless people. To address these drawbacks, channel state information (CSI) captured from commercialized Wi-Fi devices provides rich signal features for accurate detection. However, existing systems suffer from inaccurate classification under a non-line-of-sight (NLoS) and stationary scenario, such as when a person is standing still in a room corner. In this work, we propose a system called CRONOS (Colorization and Contrastive Learning Enhanced NLoS Human Presence Detection), which generates dynamic recurrence plots (RPs) and color-coded CSI ratios to distinguish mobile people from vacancy in a room, respectively. We also incorporate supervised contrastive learning to retrieve substantial representations, where consultation loss is formulated to dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#65292;&#23427;&#23558;&#36755;&#20837;&#20316;&#20026;&#21021;&#22987;&#23618;&#65292;&#36755;&#20986;&#29289;&#29702;&#29305;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#30828;&#37327;&#23376;&#31995;&#32479;&#65292;&#32780;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#30830;&#23450;&#25299;&#25169;&#30456;&#21644;&#32039;&#20945;&#30005;&#33655;&#24207;&#65292;&#20854;&#37327;&#23376;&#29305;&#24615;&#24102;&#26469;&#22810;&#31181;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.05793</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#39640;&#25928;&#20248;&#21270;&#21644;&#37327;&#23376;&#36866;&#29992;&#24615;&#30340;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A fermion neural network with efficient optimization and quantum applicability. (arXiv:2211.05793v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#65292;&#23427;&#23558;&#36755;&#20837;&#20316;&#20026;&#21021;&#22987;&#23618;&#65292;&#36755;&#20986;&#29289;&#29702;&#29305;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#30828;&#37327;&#23376;&#31995;&#32479;&#65292;&#32780;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#30830;&#23450;&#25299;&#25169;&#30456;&#21644;&#32039;&#20945;&#30005;&#33655;&#24207;&#65292;&#20854;&#37327;&#23376;&#29305;&#24615;&#24102;&#26469;&#22810;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24050;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#65292;&#20854;&#29289;&#29702;&#29305;&#24615;&#65288;&#20363;&#22914;&#23616;&#37096;&#24577;&#23494;&#24230;&#25110;&#26465;&#20214;&#30005;&#23548;&#65289;&#22312;&#36755;&#20837;&#20316;&#20026;&#21021;&#22987;&#23618;&#21518;&#20316;&#20026;&#36755;&#20986;&#12290;&#19982;&#21453;&#21521;&#20256;&#25773;&#31867;&#20284;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;FNN&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;FNN&#20063;&#30452;&#25509;&#24212;&#29992;&#20110;&#37327;&#23376;&#31995;&#32479;&#65292;&#21253;&#25324;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#30828;&#31995;&#32479;&#65292;&#24182;&#22312;&#26080;&#39044;&#22788;&#29702;&#25110;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21407;&#20301;&#20998;&#26512;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20043;&#21518;&#65292;FNN&#31934;&#30830;&#22320;&#30830;&#23450;&#25299;&#25169;&#30456;&#21644;&#32039;&#20945;&#30005;&#33655;&#24207;&#12290;&#23427;&#20204;&#30340;&#37327;&#23376;&#29305;&#24615;&#20063;&#24102;&#26469;&#20102;&#21508;&#31181;&#20248;&#21183;&#65306;&#37327;&#23376;&#30456;&#20851;&#24615;&#20351;&#32593;&#32476;&#36830;&#25509;&#26356;&#21152;&#36890;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#28040;&#22833;&#30340;&#26799;&#24230;&#38382;&#39064;&#65292;&#37327;&#23376;&#32416;&#32544;&#21017;&#20026;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical artificial neural networks have witnessed widespread successes in machine-learning applications. Here, we propose fermion neural networks (FNNs) whose physical properties, such as local density of states or conditional conductance, serve as outputs, once the inputs are incorporated as an initial layer. Comparable to back-propagation, we establish an efficient optimization, which entitles FNNs to competitive performance on challenging machine-learning benchmarks. FNNs also directly apply to quantum systems, including hard ones with interactions, and offer in-situ analysis without preprocessing or presumption. Following machine learning, FNNs precisely determine topological phases and emergent charge orders. Their quantum nature also brings various advantages: quantum correlation entitles more general network connectivity and insight into the vanishing gradient problem, quantum entanglement opens up novel avenues for interpretable machine learning, etc.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#21487;&#35299;&#37322;&#30340;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861; TiVaCPD&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26102;&#38388;&#21464;&#21270;&#22270;&#24418;&#22871;&#32034;&#21644;&#32858;&#21512;&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#27979;&#35797;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#33719;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21508;&#31181;&#22797;&#26434;&#21464;&#21270;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.03991</link><description>&lt;p&gt;
&#21160;&#24577;&#21487;&#35299;&#37322;&#30340;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Interpretable Change Point Detection. (arXiv:2211.03991v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03991
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#21487;&#35299;&#37322;&#30340;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861; TiVaCPD&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26102;&#38388;&#21464;&#21270;&#22270;&#24418;&#22871;&#32034;&#21644;&#32858;&#21512;&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#27979;&#35797;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#33719;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21508;&#31181;&#22797;&#26434;&#21464;&#21270;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#30830;&#23450;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21464;&#28857;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#26377;&#21033;&#20110;&#25351;&#23548;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#24182;&#21450;&#26102;&#24212;&#23545;&#28508;&#22312;&#30340;&#39118;&#38505;&#25110;&#26426;&#20250;&#12290;&#29616;&#26377;&#30340;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#22312;&#36861;&#36394;&#22810;&#32500;&#29305;&#24449;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#30340;&#21464;&#21270;&#26041;&#38754;&#20855;&#26377;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#19981;&#33021;&#26377;&#25928;&#22320;&#22312;&#21516;&#19968;&#26102;&#38388;&#24207;&#21015;&#20013;&#36827;&#34892;&#27010;&#25324;&#65292;&#22240;&#20026;&#19981;&#21516;&#31867;&#22411;&#30340;&#21464;&#28857;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#38543;&#30528;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#25429;&#33719;&#21508;&#31181;&#22797;&#26434;&#21464;&#28857;&#30340;&#31867;&#22411;&#24050;&#32463;&#25104;&#20026;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TiVaCPD&#65292;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#26102;&#38388;&#21464;&#21270;&#22270;&#24418;&#22871;&#32034;&#65288;TVGL&#65289;&#26469;&#35782;&#21035;&#22810;&#32500;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#27169;&#24335;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#65292;&#24182;&#23558;&#20854;&#19982;&#32858;&#21512;&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#27979;&#35797;&#30456;&#32467;&#21512;&#65292;&#20197;&#35782;&#21035;&#28508;&#22312;&#21464;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying change points (CPs) in a time series is crucial to guide better decision making across various fields like finance and healthcare and facilitating timely responses to potential risks or opportunities. Existing Change Point Detection (CPD) methods have a limitation in tracking changes in the joint distribution of multidimensional features. In addition, they fail to generalize effectively within the same time series as different types of CPs may require different detection methods. As the volume of multidimensional time series continues to grow, capturing various types of complex CPs such as changes in the correlation structure of the time-series features has become essential. To overcome the limitations of existing methods, we propose TiVaCPD, an approach that uses a Time-Varying Graphical Lasso (TVGL) to identify changes in correlation patterns between multidimensional features over time, and combines that with an aggregate Kernel Maximum Mean Discrepancy (MMD) test to iden
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21463;&#20154;&#31867;&#21327;&#20316;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#27169;&#22411;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#33258;&#20027;&#26816;&#27979;&#36866;&#21512;&#30340;&#21512;&#20316;&#32773;&#65292;&#24182;&#21442;&#32771;&#21512;&#20316;&#32773;&#30340;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#21327;&#20316;&#22270;&#23454;&#29616;&#25104;&#23545;&#21327;&#20316;&#20851;&#31995;&#25351;&#31034;&#65292;&#36890;&#36807;&#23637;&#24320;&#22270;&#23398;&#20064;&#32593;&#32476;&#20197;&#26356;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#24773;&#20917;&#22320;&#23398;&#20064;&#28508;&#22312;&#21512;&#20316;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2210.17101</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#23637;&#24320;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unrolled Graph Learning for Multi-Agent Collaboration. (arXiv:2210.17101v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17101
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21463;&#20154;&#31867;&#21327;&#20316;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#27169;&#22411;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#33258;&#20027;&#26816;&#27979;&#36866;&#21512;&#30340;&#21512;&#20316;&#32773;&#65292;&#24182;&#21442;&#32771;&#21512;&#20316;&#32773;&#30340;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#21327;&#20316;&#22270;&#23454;&#29616;&#25104;&#23545;&#21327;&#20316;&#20851;&#31995;&#25351;&#31034;&#65292;&#36890;&#36807;&#23637;&#24320;&#22270;&#23398;&#20064;&#32593;&#32476;&#20197;&#26356;&#28789;&#27963;&#36866;&#24212;&#21508;&#31181;&#24773;&#20917;&#22320;&#23398;&#20064;&#28508;&#22312;&#21512;&#20316;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#20132;&#25442;&#21463;&#38480;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#32771;&#34385;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#22266;&#23450;&#24378;&#21046;&#24615;&#30340;&#21327;&#20316;&#20851;&#31995;&#19979;&#30340;&#25968;&#25454;&#34701;&#21512;&#65292;&#36825;&#19981;&#22914;&#20154;&#31867;&#21327;&#20316;&#37027;&#26679;&#28789;&#27963;&#21644;&#33258;&#27835;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#32570;&#21475;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#21327;&#20316;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#33258;&#20027;&#26816;&#27979;&#36866;&#21512;&#30340;&#21512;&#20316;&#32773;&#65292;&#24182;&#21442;&#32771;&#21512;&#20316;&#32773;&#30340;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#23454;&#29616;&#36825;&#31181;&#36866;&#24212;&#24615;&#21327;&#20316;&#65292;&#25105;&#20204;&#20351;&#29992;&#21327;&#20316;&#22270;&#26469;&#25351;&#31034;&#25104;&#23545;&#21327;&#20316;&#20851;&#31995;&#12290;&#21327;&#20316;&#22270;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#19981;&#21516;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#27169;&#22411;&#30456;&#20284;&#24230;&#30340;&#22270;&#23398;&#20064;&#25216;&#26415;&#26469;&#33719;&#24471;&#12290;&#30001;&#20110;&#27169;&#22411;&#30456;&#20284;&#24615;&#19981;&#33021;&#36890;&#36807;&#22266;&#23450;&#30340;&#22270;&#24418;&#20248;&#21270;&#26469;&#25551;&#36848;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23637;&#24320;&#22270;&#23398;&#20064;&#32593;&#32476;&#65292;&#20197;&#26356;&#36866;&#24212;&#21508;&#31181;&#24773;&#20917;&#22320;&#23398;&#20064;&#28508;&#22312;&#21512;&#20316;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent learning has gained increasing attention to tackle distributed machine learning scenarios under constrictions of data exchanging. However, existing multi-agent learning models usually consider data fusion under fixed and compulsory collaborative relations among agents, which is not as flexible and autonomous as human collaboration. To fill this gap, we propose a distributed multi-agent learning model inspired by human collaboration, in which the agents can autonomously detect suitable collaborators and refer to collaborators' model for better performance. To implement such adaptive collaboration, we use a collaboration graph to indicate the pairwise collaborative relation. The collaboration graph can be obtained by graph learning techniques based on model similarity between different agents. Since model similarity can not be formulated by a fixed graphical optimization, we design a graph learning network by unrolling, which can learn underlying similar features among potent
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#26694;&#26550;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#21508;&#31181;&#22240;&#32032;&#23545;&#36755;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;GPT-3 Davinci&#27169;&#22411;&#65288;175B&#65289;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2210.12023</link><description>&lt;p&gt;
&#19968;&#31181;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#40065;&#26834;&#24615;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models. (arXiv:2210.12023v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12023
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#26694;&#26550;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#21508;&#31181;&#22240;&#32032;&#23545;&#36755;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;GPT-3 Davinci&#27169;&#22411;&#65288;175B&#65289;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22256;&#38590;&#25968;&#23398;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#21516;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20063;&#22791;&#21463;&#36136;&#30097;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#20381;&#36182;&#20110;&#38382;&#39064;&#25551;&#36848;&#20013;&#30340;&#27973;&#23618;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#24314;&#31435;&#22312;&#34892;&#20026;&#27979;&#35797;&#30340;&#24605;&#24819;&#22522;&#30784;&#19978;&#65292;&#23427;&#33021;&#22815;&#30830;&#23450;&#36755;&#20837;&#20013;&#21508;&#31181;&#22240;&#32032;&#65292;&#20363;&#22914;&#38382;&#39064;&#25991;&#26412;&#30340;&#34920;&#38754;&#24418;&#24335;&#12289;&#25805;&#20316;&#25968;&#21644;&#25968;&#23398;&#36816;&#31639;&#31526;&#23545;&#36755;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#36890;&#36807;&#22312;&#30452;&#35266;&#25512;&#29702;&#36807;&#31243;&#20013;&#25551;&#36848;&#22240;&#26524;&#22270;&#65292;&#23558;&#34892;&#20026;&#20998;&#26512;&#26681;&#25454;&#40065;&#26834;&#24615;&#21644;&#23545;&#36755;&#20837;&#31354;&#38388;&#30340;&#30452;&#25509;&#24178;&#39044;&#25935;&#24863;&#24615;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#20110;&#25968;&#23398;&#39064;&#30340;&#27979;&#35797;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#40065;&#26834;&#24615;&#20284;&#20046;&#24182;&#19981;&#20250;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#19981;&#26029;&#25913;&#21892;&#65292;&#20294;&#19982;&#36739;&#23567;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;GPT-3 Davinci&#27169;&#22411;&#65288;175B&#65289;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#21892;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#32780;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution. By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramati
&lt;/p&gt;</description></item><item><title>AutoMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#35745;&#31639;&#32422;&#26463;&#19979;&#35774;&#35745;&#24322;&#26500;MoE&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.07535</link><description>&lt;p&gt;
AutoMoE&#65306;&#33258;&#36866;&#24212;&#35745;&#31639;&#30340;&#24322;&#26500;&#19987;&#23478;&#28151;&#21512;&#20307;&#65292;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#23454;&#29616;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation. (arXiv:2210.07535v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07535
&lt;/p&gt;
&lt;p&gt;
AutoMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#35745;&#31639;&#32422;&#26463;&#19979;&#35774;&#35745;&#24322;&#26500;MoE&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20219;&#21153;&#20013;&#65292;&#19987;&#23478;&#28151;&#21512;&#20307;&#65288;MoE&#65289;&#27169;&#22411;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;MoE&#30340;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#32771;&#34385;&#21516;&#36136;&#35774;&#35745;&#65292;&#20854;&#20013;&#30456;&#21516;&#25968;&#37327;&#30340;&#30456;&#21516;&#22823;&#23567;&#30340;&#19987;&#23478;&#22343;&#21248;&#22320;&#25918;&#32622;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;MoE&#24037;&#20316;&#27809;&#26377;&#32771;&#34385;&#35745;&#31639;&#32422;&#26463;&#65288;&#20363;&#22914;FLOPs&#12289;&#24310;&#36831;&#65289;&#26469;&#25351;&#23548;&#20854;&#35774;&#35745;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AutoMoE--&#19968;&#20010;&#22312;&#35745;&#31639;&#32422;&#26463;&#19979;&#35774;&#35745;&#24322;&#26500;MoE&#30340;&#26694;&#26550;&#12290;AutoMoE&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26469;&#33719;&#21462;&#39640;&#25928;&#30340;&#31232;&#30095;MoE&#23376;&#21464;&#21387;&#22120;&#65292;&#20855;&#26377;4&#20493;&#25512;&#29702;&#36895;&#24230;&#20248;&#21270;&#65288;CPU&#65289;&#21644;FLOPs&#20943;&#23569;&#65292;&#30456;&#23545;&#20110;&#25163;&#21160;&#35774;&#35745;&#30340;Transformer&#65292;&#22312;NMT&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;BLEU&#20998;&#25968;&#30340;&#24179;&#31283;&#24615;&#12290;&#37319;&#29992;&#23494;&#38598;&#21644;&#31232;&#30095;&#28608;&#27963;&#30340;Transformer&#27169;&#22359;&#30340;&#24322;&#26500;&#25628;&#32034;&#31354;&#38388;&#65288;&#20363;&#22914;&#26377;&#22810;&#23569;&#19987;&#23478;&#65311;&#22312;&#21738;&#37324;&#25918;&#32622;&#23427;&#20204;&#65311;&#23427;&#20204;&#30340;&#22823;&#23567;&#24212;&#35813;&#26159;&#22810;&#23569;&#65311;&#65289;&#20801;&#35768;&#26356;&#22909;&#22320;&#25506;&#32034;&#27169;&#22411;&#35774;&#35745;&#31354;&#38388;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows
&lt;/p&gt;</description></item><item><title>FARE&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#23454;&#38469;&#20844;&#24179;&#24615;&#35777;&#20070;&#30340;FRL&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#23454;&#29616;&#23454;&#38469;&#20445;&#35777;&#24182;&#20445;&#25345;&#20934;&#30830;&#24230;-&#20844;&#24179;&#24230;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2210.07213</link><description>&lt;p&gt;
FARE: &#20855;&#26377;&#23454;&#38469;&#35777;&#20070;&#30340;&#21487;&#35777;&#26126;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FARE: Provably Fair Representation Learning with Practical Certificates. (arXiv:2210.07213v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07213
&lt;/p&gt;
&lt;p&gt;
FARE&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#23454;&#38469;&#20844;&#24179;&#24615;&#35777;&#20070;&#30340;FRL&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#23454;&#29616;&#23454;&#38469;&#20445;&#35777;&#24182;&#20445;&#25345;&#20934;&#30830;&#24230;-&#20844;&#24179;&#24230;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#34920;&#31034;&#23398;&#20064;&#65288;FRL&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#26469;&#20135;&#29983;&#20844;&#24179;&#20998;&#31867;&#22120;&#12290;&#26368;&#36817;&#30340;&#30417;&#31649;&#25351;&#20196;&#24378;&#35843;&#38656;&#35201;&#25552;&#20379;&#23454;&#38469;&#35777;&#20070;&#30340;FRL&#26041;&#27861;&#65292;&#21363;&#22312;&#20219;&#20309;&#39044;&#22788;&#29702;&#25968;&#25454;&#35757;&#32451;&#30340;&#19979;&#28216;&#20998;&#31867;&#22120;&#30340;&#19981;&#20844;&#24179;&#24615;&#19978;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#19978;&#38480;&#65292;&#20174;&#32780;&#30452;&#25509;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#25552;&#20379;&#20445;&#35777;&#12290;&#21019;&#24314;&#36825;&#26679;&#30340;FRL&#26041;&#27861;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#30446;&#21069;&#23578;&#26410;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;FARE&#65288;&#20855;&#26377;&#21463;&#38480;&#32534;&#30721;&#22120;&#30340;&#20844;&#24179;&#24615;&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#23454;&#38469;&#20844;&#24179;&#24615;&#35777;&#20070;&#30340;FRL&#26041;&#27861;&#12290;FARE&#22522;&#20110;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#35265;&#65292;&#21363;&#38480;&#21046;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#21487;&#20197;&#25512;&#23548;&#20986;&#23454;&#38469;&#30340;&#20445;&#35777;&#65292;&#21516;&#26102;&#20173;&#28982;&#20801;&#35768;&#36866;&#24403;&#30340;&#20934;&#30830;&#24230;-&#20844;&#24179;&#24230;&#26435;&#34913;&#65292;&#27604;&#22914;&#25105;&#20204;&#22522;&#20110;&#20844;&#24179;&#26641;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20135;&#29983;&#23454;&#38469;&#30340;&#35777;&#20070;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#24212;&#29992;&#20102;&#19968;&#31181;&#32479;&#35745;&#36807;&#31243;&#65292;&#35745;&#31639;&#26377;&#38480;&#26679;&#26412;&#30340;h&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fair representation learning (FRL) is a popular class of methods aiming to produce fair classifiers via data preprocessing. Recent regulatory directives stress the need for FRL methods that provide practical certificates, i.e., provable upper bounds on the unfairness of any downstream classifier trained on preprocessed data, which directly provides assurance in a practical scenario. Creating such FRL methods is an important challenge that remains unsolved. In this work, we address that challenge and introduce FARE (Fairness with Restricted Encoders), the first FRL method with practical fairness certificates. FARE is based on our key insight that restricting the representation space of the encoder enables the derivation of practical guarantees, while still permitting favorable accuracy-fairness tradeoffs for suitable instantiations, such as one we propose based on fair trees. To produce a practical certificate, we develop and apply a statistical procedure that computes a finite sample h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#38543;&#26426;&#24615;&#21487;&#20197;&#33258;&#28982;&#22320;&#36991;&#20813;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#20013;&#30340;&#20005;&#26684;&#38797;&#28857;&#38382;&#39064;&#65292;&#36825;&#19968;&#35748;&#35782;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#36817;&#26399;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2210.06723</link><description>&lt;p&gt;
&#38543;&#26426;&#22122;&#22768;&#23545;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic noise can be helpful for variational quantum algorithms. (arXiv:2210.06723v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#38543;&#26426;&#24615;&#21487;&#20197;&#33258;&#28982;&#22320;&#36991;&#20813;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#20013;&#30340;&#20005;&#26684;&#38797;&#28857;&#38382;&#39064;&#65292;&#36825;&#19968;&#35748;&#35782;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#36817;&#26399;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38797;&#28857;&#26159;&#23545;&#20110;&#19968;&#38454;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#24565;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36991;&#20813;&#38797;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#24615;&#26469;&#33258;&#28982;&#22320;&#36991;&#20813;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#20013;&#30340;&#38797;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#22312;&#25968;&#20540;&#27169;&#25311;&#21644;&#37327;&#23376;&#30828;&#20214;&#19978;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21464;&#20998;&#31639;&#27861;&#30340;&#33258;&#28982;&#38543;&#26426;&#24615;&#21487;&#20197;&#26377;&#21161;&#20110;&#36991;&#20813;&#20005;&#26684;&#30340;&#38797;&#28857;&#65292;&#21363;&#33267;&#23569;&#20855;&#26377;&#19968;&#20010;&#36127;Hessian&#29305;&#24449;&#20540;&#30340;&#38797;&#28857;&#12290;&#36825;&#20010;&#35265;&#35299;&#34920;&#26126;&#19968;&#23450;&#31243;&#24230;&#30340;&#38543;&#26426;&#22122;&#22768;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#36817;&#26399;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saddle points constitute a crucial challenge for first-order gradient descent algorithms. In notions of classical machine learning, they are avoided for example by means of stochastic gradient descent methods. In this work, we provide evidence that the saddle points problem can be naturally avoided in variational quantum algorithms by exploiting the presence of stochasticity. We prove convergence guarantees and present practical examples in numerical simulations and on quantum hardware. We argue that the natural stochasticity of variational algorithms can be beneficial for avoiding strict saddle points, i.e., those saddle points with at least one negative Hessian eigenvalue. This insight that some levels of shot noise could help is expected to add a new perspective to notions of near-term variational quantum algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#23545;&#38750;&#26377;&#30446;&#26631;&#25968;&#25454;&#27745;&#26579;&#30340;&#40065;&#26834;&#24615;</title><link>http://arxiv.org/abs/2209.14013</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#23545;&#38750;&#26377;&#30446;&#26631;&#25968;&#25454;&#27745;&#26579;&#30340;&#40065;&#26834;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Random Forest Against Untargeted Data Poisoning: An Ensemble-Based Approach. (arXiv:2209.14013v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#23545;&#38750;&#26377;&#30446;&#26631;&#25968;&#25454;&#27745;&#26579;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27491;&#22312;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#20174;&#37329;&#34701;&#21040;&#21307;&#23398;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27491;&#22312;&#25552;&#39640;&#20915;&#31574;&#36807;&#31243;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#22411;&#21644;&#30456;&#24212;&#39044;&#27979;&#30340;&#23433;&#20840;&#24615;&#26041;&#38754;&#65292;&#30456;&#23545;&#39044;&#27979;&#36136;&#37327;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#30340;&#24773;&#20917;&#24182;&#27809;&#26377;&#30456;&#24212;&#30340;&#24471;&#21040;&#20445;&#35777;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#23545;&#27745;&#26579;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;
&lt;/p&gt;
&lt;p&gt;
Machine learning is becoming ubiquitous. From finance to medicine, machine learning models are boosting decision-making processes and even outperforming humans in some tasks. This huge progress in terms of prediction quality does not however find a counterpart in the security of such models and corresponding predictions, where perturbations of fractions of the training set (poisoning) can seriously undermine the model accuracy. Research on poisoning attacks and defenses received increasing attention in the last decade, leading to several promising solutions aiming to increase the robustness of machine learning. Among them, ensemble-based defenses, where different models are trained on portions of the training set and their predictions are then aggregated, provide strong theoretical guarantees at the price of a linear overhead. Surprisingly, ensemble-based defenses, which do not pose any restrictions on the base model, have not been applied to increase the robustness of random forest mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38024;&#23545;COVID-19&#24739;&#32773;&#30340;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#65306;Outcome-specific length-of-stay prediction &#21644; Early mortality prediction&#65292;&#26088;&#22312;&#22635;&#34917;&#20020;&#24202;&#24212;&#29992;&#21644;&#20256;&#32479;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#27604;&#36739;&#21508;&#31181;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2209.07805</link><description>&lt;p&gt;
&#19968;&#39033;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#36827;&#34892; COVID-19 &#39044;&#27979;&#24314;&#27169;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Benchmark for COVID-19 Predictive Modeling Using Electronic Health Records in Intensive Care. (arXiv:2209.07805v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38024;&#23545;COVID-19&#24739;&#32773;&#30340;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#65306;Outcome-specific length-of-stay prediction &#21644; Early mortality prediction&#65292;&#26088;&#22312;&#22635;&#34917;&#20020;&#24202;&#24212;&#29992;&#21644;&#20256;&#32479;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#27604;&#36739;&#21508;&#31181;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19 &#30123;&#24773;&#32473;&#20840;&#29699;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#24102;&#26469;&#20102;&#27785;&#37325;&#30340;&#36127;&#25285;&#65292;&#36896;&#25104;&#24040;&#22823;&#30340;&#31038;&#20250;&#30772;&#22351;&#21644;&#32463;&#27982;&#25439;&#22833;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#65292;&#20363;&#22914; COVID-19 &#24739;&#32773;&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#30340;&#27515;&#20129;&#39118;&#38505;&#39044;&#27979;&#12290;&#23613;&#31649;&#22312;&#26576;&#20123;&#20020;&#24202;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21021;&#27493;&#30340;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#21487;&#20844;&#24179;&#27604;&#36739;&#21508;&#31181;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#65292;&#20197;&#20415;&#20026;&#23454;&#38469;&#20020;&#24202;&#20351;&#29992;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#39044;&#27979;&#20219;&#21153;&#30340;&#21046;&#23450;&#19982;&#37325;&#30151;&#30417;&#25252;&#23460;&#30340;&#23454;&#38469;&#20020;&#24202;&#23454;&#36341;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#65306;&#38024;&#23545; COVID-19 &#37325;&#30151;&#30417;&#25252;&#23460;&#24739;&#32773;&#30340;&#29305;&#23450;&#32467;&#26524;&#39044;&#27979;&#21644;&#26089;&#26399;&#27515;&#20129;&#39044;&#27979;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#26159;&#26681;&#25454;&#22825;&#30495;&#30340;&#20303;&#38498;&#26102;&#38388;&#21644;&#27515;&#20129;&#29575;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#30340;&#65292;&#20197;&#36866;&#24212; COVID-19 &#30340;&#20020;&#24202;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has posed a heavy burden to the healthcare system worldwide and caused huge social disruption and economic loss. Many deep learning models have been proposed to conduct clinical predictive tasks such as mortality prediction for COVID-19 patients in intensive care units using Electronic Health Record (EHR) data. Despite their initial success in certain clinical applications, there is currently a lack of benchmarking results to achieve a fair comparison so that we can select the optimal model for clinical use. Furthermore, there is a discrepancy between the formulation of traditional prediction tasks and real-world clinical practice in intensive care. To fill these gaps, we propose two clinical prediction tasks, Outcome-specific length-of-stay prediction and Early mortality prediction for COVID-19 patients in intensive care units. The two tasks are adapted from the naive length-of-stay and mortality prediction tasks to accommodate the clinical practice for COVID-19 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38598;&#21512;&#32534;&#30721;&#26041;&#27861;UMBC&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#38750;MBC&#32452;&#20214;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20173;&#28385;&#36275;MBC&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;MBC&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#20026;&#20219;&#20309;&#38598;&#21512;&#22823;&#23567;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#37117;&#20855;&#26377;&#24658;&#23450;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#32473;&#20986;&#23436;&#25972;&#38598;&#21512;&#26799;&#24230;&#30340;&#26080;&#20559;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2208.12401</link><description>&lt;p&gt;
&#20855;&#26377;&#36890;&#29992;&#36855;&#20320;&#25209;&#37327;&#19968;&#33268;&#24615;&#21644;&#26080;&#20559;&#23436;&#20840;&#38598;&#21512;&#26799;&#24230;&#36817;&#20284;&#30340;&#21487;&#25193;&#23637;&#38598;&#21512;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scalable Set Encoding with Universal Mini-Batch Consistency and Unbiased Full Set Gradient Approximation. (arXiv:2208.12401v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38598;&#21512;&#32534;&#30721;&#26041;&#27861;UMBC&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#38750;MBC&#32452;&#20214;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20173;&#28385;&#36275;MBC&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;MBC&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#20026;&#20219;&#20309;&#38598;&#21512;&#22823;&#23567;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#37117;&#20855;&#26377;&#24658;&#23450;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#32473;&#20986;&#23436;&#25972;&#38598;&#21512;&#26799;&#24230;&#30340;&#26080;&#20559;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#20851;&#20110;&#38598;&#21512;&#20989;&#25968;&#30340;&#23567;&#25209;&#37327;&#19968;&#33268;&#24615;(MBC)&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#20445;&#35777;&#23558;&#19968;&#20010;&#20998;&#21106;&#30340;&#38598;&#21512;&#30340;&#37096;&#20998;&#39034;&#24207;&#22788;&#29702;&#21644;&#32858;&#21512;&#65292;&#32780;&#20445;&#35777;&#25152;&#26377;&#20998;&#21106;&#30340;&#36755;&#20986;&#30456;&#21516;&#30340;&#38656;&#27714;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MBC&#26550;&#26500;&#30340;&#38480;&#21046;&#23548;&#33268;&#20102;&#20855;&#26377;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#35299;&#20915;&#22312;&#38656;&#35201;&#23436;&#25972;&#38598;&#21512;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#22788;&#29702;&#35757;&#32451;&#20013;&#30340;&#22823;&#22411;&#38598;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#20219;&#24847;&#38750;-MBC&#32452;&#20214;&#30456;&#32467;&#21512;&#30340;&#36890;&#29992;MBC (UMBC) &#31867;&#38598;&#21512;&#20989;&#25968;&#65292;&#21516;&#26102;&#20173;&#28385;&#36275;MBC&#65292;&#20351;&#24471;MBC&#35774;&#32622;&#20013;&#21487;&#20197;&#20351;&#29992;&#26356;&#24191;&#27867;&#30340;&#21151;&#33021;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;MBC&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#20026;&#20219;&#20309;&#38598;&#21512;&#22823;&#23567;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#37117;&#20855;&#26377;&#24658;&#23450;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#32473;&#20986;&#23436;&#25972;&#38598;&#21512;&#26799;&#24230;&#30340;&#26080;&#20559;&#36817;&#20284;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#22270;&#20687;&#23436;&#25104;&#12289;&#25991;&#26412;&#20998;&#31867;&#12289;&#26080;&#30417;&#30563;&#32858;&#31867;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on mini-batch consistency (MBC) for set functions has brought attention to the need for sequentially processing and aggregating chunks of a partitioned set while guaranteeing the same output for all partitions. However, existing constraints on MBC architectures lead to models with limited expressive power. Additionally, prior work has not addressed how to deal with large sets during training when the full set gradient is required. To address these issues, we propose a Universally MBC (UMBC) class of set functions which can be used in conjunction with arbitrary non-MBC components while still satisfying MBC, enabling a wider range of function classes to be used in MBC settings. Furthermore, we propose an efficient MBC training algorithm which gives an unbiased approximation of the full set gradient and has a constant memory overhead for any set size for both train- and test-time. We conduct extensive experiments including image completion, text classification, unsupervised cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedHSSL&#30340;&#32852;&#37030;&#28151;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#26041;&#35270;&#35282;&#21644;&#26412;&#22320;&#35270;&#35282;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#37096;&#20998;&#27169;&#22411;&#32858;&#21512;&#36827;&#19968;&#27493;&#25552;&#39640;&#32852;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.08934</link><description>&lt;p&gt;
&#19968;&#31181;&#28151;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Self-Supervised Learning Framework for Vertical Federated Learning. (arXiv:2208.08934v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedHSSL&#30340;&#32852;&#37030;&#28151;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#26041;&#35270;&#35282;&#21644;&#26412;&#22320;&#35270;&#35282;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#37096;&#20998;&#27169;&#22411;&#32858;&#21512;&#36827;&#19968;&#27493;&#25552;&#39640;&#32852;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;VFL&#26041;&#27861;&#21487;&#33021;&#20250;&#36973;&#36935;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#21033;&#29992;&#32463;&#36807;&#23545;&#40784;&#21644;&#26631;&#35760;&#30340;&#26679;&#26412;&#65288;&#23646;&#20110;&#19981;&#21516;&#26041;&#65289;&#65292;&#32463;&#24120;&#24573;&#30053;&#20102;&#22823;&#22810;&#25968;&#26410;&#23545;&#40784;&#21644;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;&#36825;&#20010;&#25968;&#25454;&#19981;&#36275;&#38459;&#30861;&#20102;&#32852;&#37030;&#19968;&#26041;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedHSSL&#30340;&#32852;&#37030;&#28151;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26041;&#38388;&#35270;&#35282;&#65288;&#21363;&#20998;&#25955;&#30340;&#29305;&#24449;&#65289;&#21644;&#21508;&#26041;&#20869;&#26410;&#23545;&#40784;&#26679;&#26412;&#30340;&#26412;&#22320;&#35270;&#35282;&#65288;&#21363;&#25968;&#25454;&#22686;&#24378;&#65289;&#26469;&#25552;&#39640;VFL&#32852;&#21512;&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;FedHSSL&#36890;&#36807;&#37096;&#20998;&#27169;&#22411;&#32858;&#21512;&#36827;&#19968;&#27493;&#21033;&#29992;&#21508;&#26041;&#20849;&#21516;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#32852;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL), a variant of Federated Learning (FL), has recently drawn increasing attention as the VFL matches the enterprises' demands of leveraging more valuable features to achieve better model performance. However, conventional VFL methods may run into data deficiency as they exploit only aligned and labeled samples (belonging to different parties), leaving often the majority of unaligned and unlabeled samples unused. The data deficiency hampers the effort of the federation.  In this work, we propose a Federated Hybrid Self-Supervised Learning framework, named FedHSSL, that utilizes cross-party views (i.e., dispersed features) of samples aligned among parties and local views (i.e., augmentation) of unaligned samples within each party to improve the representation learning capability of the VFL joint model. FedHSSL further exploits invariant features across parties to boost the performance of the joint model through partial model aggregation. FedHSSL, as a frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#29983;&#29702;&#25968;&#25454;&#23545;&#27493;&#24577;&#29615;&#22659;&#19979;&#30340;&#21387;&#21147;&#36827;&#34892;&#20102;&#26816;&#27979;&#65292;&#20026;&#20010;&#20307;&#30417;&#25511;&#24515;&#29702;&#20581;&#24247;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.04705</link><description>&lt;p&gt;
&#36890;&#36807;&#27493;&#24577;&#24515;&#30005;&#22270;&#21644;&#30382;&#32932;&#30005;&#21453;&#24212;&#25968;&#25454;&#20998;&#31867;&#21387;&#21147;
&lt;/p&gt;
&lt;p&gt;
Classification of Stress via Ambulatory ECG and GSR Data. (arXiv:2208.04705v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#29983;&#29702;&#25968;&#25454;&#23545;&#27493;&#24577;&#29615;&#22659;&#19979;&#30340;&#21387;&#21147;&#36827;&#34892;&#20102;&#26816;&#27979;&#65292;&#20026;&#20010;&#20307;&#30417;&#25511;&#24515;&#29702;&#20581;&#24247;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#26816;&#27979;&#21387;&#21147;&#24182;&#20351;&#20010;&#20154;&#30417;&#25511;&#20854;&#24515;&#29702;&#20581;&#24247;&#21644;&#31119;&#31049;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21487;&#31359;&#25140;&#25216;&#26415;&#30340;&#36827;&#27493;&#29616;&#22312;&#20351;&#36830;&#32493;&#30340;&#29983;&#29702;&#25968;&#25454;&#25910;&#38598;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#24515;&#29702;&#29983;&#29702;&#20998;&#26512;&#25552;&#20379;&#26377;&#20851;&#24515;&#29702;&#20581;&#24247;&#21644;&#34892;&#20026;&#29366;&#24577;&#30340;&#27934;&#23519;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#25968;&#37327;&#24040;&#22823;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#20998;&#26512;&#26469;&#21450;&#26102;&#25552;&#20379;&#32467;&#26524;&#12290;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#22312;&#21463;&#25511;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#20026;&#20581;&#24247;&#24212;&#29992;&#25552;&#20379;&#29983;&#29702;&#25968;&#25454;&#30340;&#33258;&#21160;&#20998;&#31867;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#26080;&#27861;&#25511;&#21046;&#30340;&#27493;&#24577;&#29615;&#22659;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#24314;&#27169;&#26469;&#20811;&#26381;&#12290;&#26412;&#25991;&#20174;&#32463;&#39564;&#35282;&#24230;&#35780;&#20272;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#21387;&#21147;&#30340;&#20960;&#31181;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#33258;&#25105;&#25253;&#21578;&#21387;&#21147;&#27880;&#37322;&#35760;&#24405;&#30340;&#29983;&#29702;&#25968;&#25454;&#22312;&#27493;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35760;&#24405;&#12290;&#22312;SMILE&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#35757;&#32451;&#37096;&#20998;&#20013;&#65292;&#33021;&#22815;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In healthcare, detecting stress and enabling individuals to monitor their mental health and wellbeing is challenging. Advancements in wearable technology now enable continuous physiological data collection. This data can provide insights into mental health and behavioural states through psychophysiological analysis. However, automated analysis is required to provide timely results due to the quantity of data collected. Machine learning has shown efficacy in providing an automated classification of physiological data for health applications in controlled laboratory environments. Ambulatory uncontrolled environments, however, provide additional challenges requiring further modelling to overcome. This work empirically assesses several approaches utilising machine learning classifiers to detect stress using physiological data recorded in an ambulatory setting with self-reported stress annotations. A subset of the training portion SMILE dataset enables the evaluation of approaches before su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Retain-Resample-Release&#37319;&#26679;&#65288;R3&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#36731;&#20256;&#25773;&#22833;&#36133;&#65292;&#20351;&#24471;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#33021;&#22815;&#25104;&#21151;&#22320;&#20174;&#36793;&#30028;&#28857;&#20256;&#25773;&#35299;&#20915;&#26041;&#26696;&#21040;&#20869;&#37096;&#28857;&#12290;</title><link>http://arxiv.org/abs/2207.02338</link><description>&lt;p&gt;
&#21033;&#29992;Retain-Resample-Release (R3)&#37319;&#26679;&#26041;&#27861;&#20943;&#36731;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20256;&#25773;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Mitigating Propagation Failures in Physics-informed Neural Networks using Retain-Resample-Release (R3) Sampling. (arXiv:2207.02338v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Retain-Resample-Release&#37319;&#26679;&#65288;R3&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#36731;&#20256;&#25773;&#22833;&#36133;&#65292;&#20351;&#24471;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#33021;&#22815;&#25104;&#21151;&#22320;&#20174;&#36793;&#30028;&#28857;&#20256;&#25773;&#35299;&#20915;&#26041;&#26696;&#21040;&#20869;&#37096;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#22312;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#28041;&#21450;&#22797;&#26434;PDE&#30340;&#38382;&#39064;&#20013;&#65292;PINN&#26377;&#26102;&#20250;&#26080;&#27861;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#35299;&#12290;&#36825;&#21453;&#26144;&#22312;&#36817;&#26399;&#20851;&#20110;&#8220;&#22833;&#36133;&#27169;&#24335;&#8221;&#29305;&#24449;&#30340;&#20960;&#39033;&#30740;&#31350;&#20013;&#65292;&#23613;&#31649;&#32570;&#20047;&#20851;&#20110;PINN&#22833;&#36133;&#27169;&#24335;&#21644;&#37319;&#26679;&#31574;&#30053;&#20043;&#38388;&#36830;&#25509;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PINN&#22833;&#36133;&#27169;&#24335;&#30340;&#35270;&#35282;&#65292;&#21363;&#20551;&#35774;&#35757;&#32451;PINN&#22522;&#20110;&#20174;&#21021;&#22987;&#21644;/&#25110;&#36793;&#30028;&#26465;&#20214;&#28857;&#21040;&#20869;&#37096;&#28857;&#30340;&#35299;&#8220;&#20256;&#25773;&#8221;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#23384;&#22312;&#20256;&#25773;&#22833;&#36133;&#65292;&#21017;&#37319;&#29992;&#31967;&#31957;&#37319;&#26679;&#31574;&#30053;&#30340;PINN&#21487;&#33021;&#20250;&#21345;&#22312;&#24179;&#20961;&#35299;&#19978;&#65292;&#34920;&#29616;&#20026;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;PDE&#27531;&#24046;&#22330;&#12290;&#20026;&#20102;&#20943;&#36731;&#20256;&#25773;&#22833;&#36133;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Retain-Resample-Release&#37319;&#26679;&#65288;R3&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#39640;PD&#21306;&#22495;&#36880;&#27493;&#32047;&#31215;&#37325;&#21512;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of physics-informed neural networks (PINNs) in approximating partial differential equations (PDEs), PINNs can sometimes fail to converge to the correct solution in problems involving complicated PDEs. This is reflected in several recent studies on characterizing the "failure modes" of PINNs, although a thorough understanding of the connection between PINN failure modes and sampling strategies is missing. In this paper, we provide a novel perspective of failure modes of PINNs by hypothesizing that training PINNs relies on successful "propagation" of solution from initial and/or boundary condition points to interior points. We show that PINNs with poor sampling strategies can get stuck at trivial solutions if there are propagation failures, characterized by highly imbalanced PDE residual fields. To mitigate propagation failures, we propose a novel Retain-Resample-Release sampling (R3) algorithm that can incrementally accumulate collocation points in regions of high PD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2207.01955</link><description>&lt;p&gt;
Ask-AC: &#19968;&#31181;&#24490;&#29615;&#20013;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework. (arXiv:2207.01955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#21462;&#24471;&#20102;&#24456;&#22810;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#26696;&#20173;&#28982;&#20381;&#36182;&#20110;&#26469;&#33258;&#39038;&#38382;&#19987;&#23478;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#65292;&#24418;&#24335;&#21253;&#25324;&#25345;&#32493;&#30417;&#25511;&#25110;&#39044;&#23450;&#20041;&#35268;&#21017;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#19968;&#31181;&#40635;&#28902;&#32780;&#26114;&#36149;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#31216;&#20026;Ask-AC&#65292;&#23427;&#29992;&#19968;&#20010;&#21452;&#21521;&#30340;&#23398;&#20064;&#32773;&#20027;&#21160;&#26426;&#21046;&#26367;&#25442;&#20102;&#21333;&#21521;&#30340;&#39038;&#38382;&#25351;&#23548;&#26426;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23398;&#20064;&#32773;&#21644;&#39038;&#38382;&#20043;&#38388;&#30340;&#23450;&#21046;&#21270;&#21644;&#26377;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;Ask-AC &#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#20114;&#34917;&#30340;&#32452;&#20214;&#65292;&#20998;&#21035;&#26159;&#21160;&#20316;&#35831;&#27714;&#32773;&#21644;&#33258;&#36866;&#24212;&#29366;&#24577;&#36873;&#25321;&#22120;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#32435;&#20837;&#21508;&#31181;&#31163;&#25955;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#20013;&#12290;&#21069;&#32773;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#19981;&#30830;&#23450;&#29366;&#24577;&#19979;&#30340;&#39038;&#38382;&#24178;&#39044;&#65292;&#21518;&#32773;&#21017;&#21487;&#20197;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising results achieved, state-of-the-art interactive reinforcement learning schemes rely on passively receiving supervision signals from advisor experts, in the form of either continuous monitoring or pre-defined rules, which inevitably result in a cumbersome and expensive learning process. In this paper, we introduce a novel initiative advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the unilateral advisor-guidance mechanism with a bidirectional learner-initiative one, and thereby enables a customized and efficacious message exchange between learner and advisor. At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures. The former component allows the agent to initiatively seek advisor intervention in the presence of uncertain states, while the latter identifies the unstable states potentially missed by the for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#24050;&#26377;&#36845;&#20195;&#31639;&#27861;&#36816;&#34892;&#36895;&#24230;&#24555;100&#20493;&#30340;&#22522;&#20110;&#25104;&#23545;&#27604;&#36739;&#30340;&#25490;&#21517;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.00076</link><description>&lt;p&gt;
&#22522;&#20110;&#25104;&#23545;&#27604;&#36739;&#30340;&#25490;&#21517;&#39640;&#25928;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Efficient computation of rankings from pairwise comparisons. (arXiv:2207.00076v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#24050;&#26377;&#36845;&#20195;&#31639;&#27861;&#36816;&#34892;&#36895;&#24230;&#24555;100&#20493;&#30340;&#22522;&#20110;&#25104;&#23545;&#27604;&#36739;&#30340;&#25490;&#21517;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Bradley-Terry&#27169;&#22411;&#36890;&#36807;&#20010;&#20307;&#12289;&#22242;&#38431;&#25110;&#23545;&#35937;&#20043;&#38388;&#30340;&#25104;&#23545;&#27604;&#36739;&#26469;&#30830;&#23450;&#25490;&#21517;&#12290;&#35813;&#27169;&#22411;&#20013;&#25490;&#21517;&#30340;&#20272;&#35745;&#36890;&#24120;&#20351;&#29992;Zermelo&#36817;&#19968;&#30334;&#24180;&#21069;&#39318;&#27425;&#24341;&#20837;&#30340;&#31616;&#21333;&#36845;&#20195;&#31639;&#27861;&#36827;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#19988;&#21516;&#26679;&#31616;&#21333;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#21487;&#35777;&#26126;&#23427;&#36820;&#22238;&#30340;&#32467;&#26524;&#26159;&#30456;&#21516;&#30340;&#65292;&#20294;&#36895;&#24230;&#26356;&#24555;&#12290;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#36895;&#24230;&#21487;&#33021;&#24555;&#20102;&#19968;&#30334;&#20493;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#31034;&#20363;&#25968;&#25454;&#38598;&#26469;&#28436;&#31034;&#35813;&#31639;&#27861;&#65292;&#24182;&#23548;&#20986;&#26377;&#20851;&#20854;&#25910;&#25947;&#24615;&#30340;&#35768;&#22810;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the ranking of individuals, teams, or objects, based on pairwise comparisons between them, using the Bradley-Terry model. Estimates of rankings within this model are commonly made using a simple iterative algorithm first introduced by Zermelo almost a century ago. Here we describe an alternative and similarly simple iteration that provably returns identical results but does so much faster -- over a hundred times faster in some cases. We demonstrate this algorithm with applications to a range of example data sets and derive a number of results regarding its convergence.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20998;&#23376;&#22270;&#23884;&#20837;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;MOLGRAPHEVAL&#25581;&#31034;&#20102;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;GSSL&#26041;&#27861;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2206.08005</link><description>&lt;p&gt;
&#35780;&#20272;&#20998;&#23376;&#22270;&#23884;&#20837;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evaluating Self-Supervised Learning for Molecular Graph Embeddings. (arXiv:2206.08005v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08005
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20998;&#23376;&#22270;&#23884;&#20837;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;MOLGRAPHEVAL&#25581;&#31034;&#20102;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;GSSL&#26041;&#27861;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;GSSL&#65289;&#20026;&#33719;&#21462;&#23884;&#20837;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#36884;&#24452;&#65292;&#26080;&#38656;&#19987;&#23478;&#26631;&#27880;&#65292;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20998;&#23376;&#22270;&#20855;&#26377;&#28145;&#21051;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#28508;&#22312;&#20998;&#23376;&#30340;&#25968;&#37327;&#24778;&#20154;&#65292;&#24182;&#19988;&#33719;&#21462;&#26631;&#31614;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;&#28982;&#32780;&#65292;GSSL&#26041;&#27861;&#19981;&#26159;&#20026;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#26159;&#20026;&#20102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#21487;&#36716;&#31227;&#24615;&#12290;&#36825;&#31181;&#24191;&#27867;&#36866;&#29992;&#24615;&#20351;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#22797;&#26434;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20998;&#23376;&#22270;&#34920;&#31034;&#35780;&#20272;&#8221;&#65288;MOLGRAPHEVAL&#65289;&#65292;&#29983;&#25104;&#20855;&#26377;&#21487;&#35299;&#37322;&#21644;&#22810;&#26679;&#21270;&#23646;&#24615;&#30340;&#20998;&#23376;&#22270;&#23884;&#20837;&#30340;&#35814;&#32454;&#21078;&#26512;&#12290;MOLGRAPHEVAL&#25552;&#20379;&#20102;&#19968;&#32452;&#25506;&#27979;&#20219;&#21153;&#65292;&#20998;&#20026;&#19977;&#31867;&#65306;&#65288;i&#65289;&#36890;&#29992;&#22270;&#24418;&#65292;&#65288;ii&#65289;&#20998;&#23376;&#20122;&#32467;&#26500;&#21644;&#65288;iii&#65289;&#23884;&#20837;&#31354;&#38388;&#23646;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;MOLGRAPHEVAL&#26469;&#22522;&#20934;&#21270;&#29616;&#26377;&#30340;GSSL&#26041;&#27861;&#65292;&#23545;&#27604;&#24403;&#21069;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#20197;&#21450;&#25105;&#20204;&#30340;&#20219;&#21153;&#22871;&#20214;&#65292;&#25105;&#20204;&#21457;&#29616;GSSL&#26041;&#27861;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Self-Supervised Learning (GSSL) provides a robust pathway for acquiring embeddings without expert labelling, a capability that carries profound implications for molecular graphs due to the staggering number of potential molecules and the high cost of obtaining labels. However, GSSL methods are designed not for optimisation within a specific domain but rather for transferability across a variety of downstream tasks. This broad applicability complicates their evaluation. Addressing this challenge, we present "Molecular Graph Representation Evaluation" (MOLGRAPHEVAL), generating detailed profiles of molecular graph embeddings with interpretable and diversified attributes. MOLGRAPHEVAL offers a suite of probing tasks grouped into three categories: (i) generic graph, (ii) molecular substructure, and (iii) embedding space properties. By leveraging MOLGRAPHEVAL to benchmark existing GSSL methods against both current downstream datasets and our suite of tasks, we uncover significant inco
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20108;&#27425;&#27169;&#22411;&#21487;&#20197;&#23637;&#31034;&#20986;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#23398;&#20064;&#29575;&#24773;&#20917;&#19979;&#30340;&#8220;&#24377;&#24339;&#38454;&#27573;&#8221;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#29305;&#24615;&#19978;&#19982;&#31070;&#32463;&#32593;&#32476;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#26159;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2205.11787</link><description>&lt;p&gt;
&#29992;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#30340;&#20108;&#27425;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Quadratic models for understanding neural network dynamics. (arXiv:2205.11787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11787
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20108;&#27425;&#27169;&#22411;&#21487;&#20197;&#23637;&#31034;&#20986;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#23398;&#20064;&#29575;&#24773;&#20917;&#19979;&#30340;&#8220;&#24377;&#24339;&#38454;&#27573;&#8221;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#29305;&#24615;&#19978;&#19982;&#31070;&#32463;&#32593;&#32476;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#26159;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#22686;&#21152;&#26102;&#65292;&#21487;&#20197;&#29992;&#32447;&#24615;&#27169;&#22411;&#26469;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#26576;&#20123;&#29305;&#24615;&#19981;&#33021;&#34987;&#32447;&#24615;&#27169;&#22411;&#25429;&#25417;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#20108;&#27425;&#27169;&#22411;&#21487;&#20197;&#23637;&#31034;&#8220;&#24377;&#24339;&#38454;&#27573;&#8221;[Lewkowycz&#31561;&#20154;&#65292;2020]&#65292;&#24403;&#20351;&#29992;&#22823;&#23398;&#20064;&#29575;&#35757;&#32451;&#27492;&#31867;&#27169;&#22411;&#26102;&#20250;&#20986;&#29616;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#31070;&#32463;&#20108;&#27425;&#27169;&#22411;&#30340;&#34892;&#20026;&#19982;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#29305;&#24615;&#19978;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#23588;&#20854;&#26159;&#22312;&#24377;&#24339;&#38454;&#27573;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#20108;&#27425;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
While neural networks can be approximated by linear models as their width increases, certain properties of wide neural networks cannot be captured by linear models. In this work we show that recently proposed Neural Quadratic Models can exhibit the "catapult phase" [Lewkowycz et al. 2020] that arises when training such models with large learning rates. We then empirically show that the behaviour of neural quadratic models parallels that of neural networks in generalization, especially in the catapult phase regime. Our analysis further demonstrates that quadratic models can be an effective tool for analysis of neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#26126;&#20855;&#26377;&#20219;&#24847;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#23485;&#24230;&#26080;&#38480;&#22686;&#22823;&#30340;&#24773;&#20917;&#19979;&#26377;&#32447;&#24615;&#36716;&#21270;&#30340;&#36235;&#21183;&#12290;&#32467;&#26524;&#25581;&#31034;&#20102;&#36716;&#21270;&#20026;&#32447;&#24615;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#24182;&#25512;&#24191;&#20102;&#19968;&#31995;&#21015;&#20851;&#20110;&#26631;&#20934;&#26550;&#26500;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#32447;&#24615;&#36716;&#21270;&#25110;&#24658;&#23450;&#24615;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2205.11786</link><description>&lt;p&gt;
&#20855;&#26377;&#26377;&#21521;&#26080;&#29615;&#22270;&#26550;&#26500;&#30340;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Transition to Linearity of General Neural Networks with Directed Acyclic Graph Architecture. (arXiv:2205.11786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#26126;&#20855;&#26377;&#20219;&#24847;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#23485;&#24230;&#26080;&#38480;&#22686;&#22823;&#30340;&#24773;&#20917;&#19979;&#26377;&#32447;&#24615;&#36716;&#21270;&#30340;&#36235;&#21183;&#12290;&#32467;&#26524;&#25581;&#31034;&#20102;&#36716;&#21270;&#20026;&#32447;&#24615;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#24182;&#25512;&#24191;&#20102;&#19968;&#31995;&#21015;&#20851;&#20110;&#26631;&#20934;&#26550;&#26500;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#32447;&#24615;&#36716;&#21270;&#25110;&#24658;&#23450;&#24615;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#65292;&#38543;&#30528;&#20854;&#8220;&#23485;&#24230;&#8221;&#25509;&#36817;&#26080;&#31351;&#22823;&#65292;&#19982;&#20219;&#24847;&#26377;&#21521;&#26080;&#29615;&#22270;&#30456;&#20851;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20250;&#21457;&#29983;&#32447;&#24615;&#36716;&#25442;&#12290;&#36825;&#20123;&#26222;&#36890;&#32593;&#32476;&#30340;&#23485;&#24230;&#30001;&#20854;&#31070;&#32463;&#20803;&#30340;&#26368;&#23567;&#20837;&#24230;&#65288;&#38500;&#20102;&#36755;&#20837;&#21644;&#31532;&#19968;&#23618;&#20043;&#22806;&#65289;&#26469;&#21051;&#30011;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#30830;&#23450;&#20102;&#36716;&#25442;&#21040;&#32447;&#24615;&#25152;&#22522;&#20110;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#24182;&#27010;&#25324;&#20102;&#19968;&#20123;&#26088;&#22312;&#34920;&#24449;&#26631;&#20934;&#26550;&#26500;&#19979;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#32447;&#24615;&#36716;&#25442;&#25110;&#24658;&#23450;&#24615;&#30340;&#26368;&#36817;&#30740;&#31350;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we show that feedforward neural networks corresponding to arbitrary directed acyclic graphs undergo transition to linearity as their "width" approaches infinity. The width of these general networks is characterized by the minimum in-degree of their neurons, except for the input and first layers. Our results identify the mathematical structure underlying transition to linearity and generalize a number of recent works aimed at characterizing transition to linearity or constancy of the Neural Tangent Kernel for standard architectures.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#30340;&#26080;&#20559;&#40065;&#26834;&#22270;&#20449;&#24687;&#29942;&#39048;(RGIB)&#26041;&#27861;&#65292;&#23581;&#35797;&#36890;&#36807;&#20445;&#30041;&#33391;&#24615;&#22270;&#20013;&#30340;&#21407;&#22987;&#20449;&#24687;&#24182;&#28040;&#38500;&#23545;&#25239;&#24615;&#22270;&#20013;&#30340;&#23545;&#25239;&#24615;&#20449;&#24687;&#65292;&#26469;&#23398;&#20064;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#33410;&#28857;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2201.08557</link><description>&lt;p&gt;
&#22686;&#24378;&#38750;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#65306;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Toward Enhanced Robustness in Unsupervised Graph Representation Learning: A Graph Information Bottleneck Perspective. (arXiv:2201.08557v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08557
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#30340;&#26080;&#20559;&#40065;&#26834;&#22270;&#20449;&#24687;&#29942;&#39048;(RGIB)&#26041;&#27861;&#65292;&#23581;&#35797;&#36890;&#36807;&#20445;&#30041;&#33391;&#24615;&#22270;&#20013;&#30340;&#21407;&#22987;&#20449;&#24687;&#24182;&#28040;&#38500;&#23545;&#25239;&#24615;&#22270;&#20013;&#30340;&#23545;&#25239;&#24615;&#20449;&#24687;&#65292;&#26469;&#23398;&#20064;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#40065;&#26834;&#22270;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;&#26631;&#31614;&#20449;&#24687;&#24230;&#37327;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#27809;&#26377;&#26631;&#31614;&#20449;&#24687;&#26102;&#38590;&#20197;&#23454;&#29616;&#12290;&#19968;&#31181;&#30452;&#25509;&#30340;&#26041;&#21521;&#26159;&#21033;&#29992;&#20856;&#22411;&#30340;&#38750;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;(UGRL)&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;Infomax&#25216;&#26415;&#26469;&#23398;&#20064;&#40065;&#26834;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;Infomax&#25216;&#26415;&#20174;&#20856;&#22411;&#30340;UGRL&#31227;&#26893;&#21040;&#40065;&#26834;&#30340;UGRL&#21487;&#33021;&#28041;&#21450;&#19968;&#31181;&#26377;&#20559;&#30340;&#20551;&#35774;&#12290;&#37492;&#20110;Infomax&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#20559;&#40065;&#26834;UGRL&#26041;&#27861;&#8212;&#8212;Robust Graph Information Bottleneck (RGIB)&#65292;&#23427;&#22522;&#20110;Information Bottleneck (IB)&#21407;&#29702;&#12290;&#25105;&#20204;&#30340;RGIB&#35797;&#22270;&#36890;&#36807;&#20445;&#30041;&#33391;&#24615;&#22270;&#20013;&#30340;&#21407;&#22987;&#20449;&#24687;&#24182;&#28040;&#38500;&#23545;&#25239;&#24615;&#22270;&#20013;&#30340;&#23545;&#25239;&#24615;&#20449;&#24687;&#65292;&#26469;&#23398;&#20064;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#33410;&#28857;&#34920;&#31034;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#26377;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#22914;&#20309;&#34913;&#37327;&#22270;&#20013;&#30340;&#21407;&#22987;&#20449;&#24687;&#21644;&#23545;&#25239;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#20197;&#21450;2&#65289;&#22914;&#20309;&#22312;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#21644;&#28040;&#38500;&#23545;&#25239;&#20449;&#24687;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#34920;&#26126;RGIB&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22270;&#40065;&#26834;&#24615;&#26041;&#27861;&#65292;&#21253;&#25324;&#38024;&#23545;&#24615;&#21644;&#38750;&#38024;&#23545;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed that GNNs are vulnerable to adversarial attacks. Most existing robust graph learning methods measure model robustness based on label information, rendering them infeasible when label information is not available. A straightforward direction is to employ the widely used Infomax technique from typical Unsupervised Graph Representation Learning (UGRL) to learn robust unsupervised representations. Nonetheless, directly transplanting the Infomax technique from typical UGRL to robust UGRL may involve a biased assumption. In light of the limitation of Infomax, we propose a novel unbiased robust UGRL method called Robust Graph Information Bottleneck (RGIB), which is grounded in the Information Bottleneck (IB) principle. Our RGIB attempts to learn robust node representations against adversarial perturbations by preserving the original information in the benign graph while eliminating the adversarial information in the adversarial graph. There are mainly two challeng
&lt;/p&gt;</description></item><item><title>&#22312;QMIX&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;LSF-SAC&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#28508;&#22312;&#20449;&#24687;&#20849;&#20139;&#26426;&#21046;&#65292;&#21487;&#26174;&#33879;&#25193;&#23637;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#23436;&#20840;&#20998;&#25955;&#25191;&#34892;&#20013;&#20445;&#25345;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.01247</link><description>&lt;p&gt;
&#24102;&#26377;&#28508;&#22312;&#29366;&#24577;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Value Functions Factorization with Latent State Information Sharing in Decentralized Multi-Agent Policy Gradients. (arXiv:2201.01247v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01247
&lt;/p&gt;
&lt;p&gt;
&#22312;QMIX&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;LSF-SAC&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#28508;&#22312;&#20449;&#24687;&#20849;&#20139;&#26426;&#21046;&#65292;&#21487;&#26174;&#33879;&#25193;&#23637;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#23436;&#20840;&#20998;&#25955;&#25191;&#34892;&#20013;&#20445;&#25345;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38598;&#20013;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#30340;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#26041;&#27861;&#26377;&#26395;&#35299;&#20915;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;QMIX&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#22312;StarCraft II&#24494;&#35266;&#31649;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;QMIX&#20013;&#30340;&#21333;&#20010;&#26234;&#33021;&#20307;&#20272;&#35745;&#30340;&#21333;&#35843;&#28151;&#21512;&#34987;&#35748;&#20026;&#38480;&#21046;&#20102;&#23427;&#33021;&#34920;&#31034;&#30340;&#32852;&#21512;&#21160;&#20316;Q&#20540;&#30340;&#33539;&#22260;&#65292;&#21516;&#26102;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#19981;&#36275;&#20197;&#36827;&#34892;&#21333;&#20010;&#26234;&#33021;&#20307;&#20540;&#20989;&#25968;&#20272;&#35745;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#32467;&#26524;&#27425;&#20248;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LSF-SAC&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#20449;&#24687;&#20849;&#20139;&#26426;&#21046;&#20316;&#20026;&#39069;&#22806;&#30340;&#29366;&#24577;&#20449;&#24687;&#65292;&#20197;&#36741;&#21161;&#20010;&#20307;&#26234;&#33021;&#20307;&#22312;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#28508;&#22312;&#30340;&#20010;&#20307;&#29366;&#24577;&#20449;&#24687;&#20849;&#20139;&#21487;&#20197;&#26174;&#33879;&#25193;&#23637;&#20540;&#20989;&#25968;&#20998;&#35299;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;LSF-SAC&#20013;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#36719;&#38480;&#21046;&#23454;&#29616;&#23436;&#20840;&#20998;&#25955;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value function factorization via centralized training and decentralized execution is promising for solving cooperative multi-agent reinforcement tasks. One of the approaches in this area, QMIX, has become state-of-the-art and achieved the best performance on the StarCraft II micromanagement benchmark. However, the monotonic-mixing of per agent estimates in QMIX is known to restrict the joint action Q-values it can represent, as well as the insufficient global state information for single agent value function estimation, often resulting in suboptimality. To this end, we present LSF-SAC, a novel framework that features a variational inference-based information-sharing mechanism as extra state information to assist individual agents in the value function factorization. We demonstrate that such latent individual state information sharing can significantly expand the power of value function factorization, while fully decentralized execution can still be maintained in LSF-SAC through a soft-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#21450;Bandits&#26041;&#27861;&#30340;&#20581;&#22766;&#32479;&#35745;&#23398;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#22810;&#20219;&#21153;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#20849;&#20139;&#20840;&#23616;&#21442;&#25968;&#21644;&#31232;&#30095;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2112.14233</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;Bandits&#36890;&#36807;&#20581;&#22766;&#32479;&#35745;&#23398;
&lt;/p&gt;
&lt;p&gt;
Multitask Learning and Bandits via Robust Statistics. (arXiv:2112.14233v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#21450;Bandits&#26041;&#27861;&#30340;&#20581;&#22766;&#32479;&#35745;&#23398;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#22810;&#20219;&#21153;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#20849;&#20139;&#20840;&#23616;&#21442;&#25968;&#21644;&#31232;&#30095;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#32463;&#24120;&#21516;&#26102;&#38754;&#23545;&#35768;&#22810;&#30456;&#20851;&#20294;&#24322;&#36136;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#20010;&#23398;&#20064;&#23454;&#20363;&#20013;&#30340;&#26410;&#30693;&#21442;&#25968;&#21487;&#20197;&#20998;&#35299;&#20026;&#20849;&#20139;&#20840;&#23616;&#21442;&#25968;&#21152;&#19978;&#31232;&#30095;&#30340;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#22810;&#20219;&#21153;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65292;&#20351;&#29992;&#20581;&#22766;&#32479;&#35745;&#23398;&#65288;&#22312;&#30456;&#20284;&#23454;&#20363;&#19978;&#23398;&#20064;&#65289;&#21644;LASSO&#22238;&#24402;&#65288;&#21435;&#20559;&#24046;&#32467;&#26524;&#65289;&#30340;&#29420;&#29305;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-makers often simultaneously face many related but heterogeneous learning problems. For instance, a large retailer may wish to learn product demand at different stores to solve pricing or inventory problems, making it desirable to learn jointly for stores serving similar customers; alternatively, a hospital network may wish to learn patient risk at different providers to allocate personalized interventions, making it desirable to learn jointly for hospitals serving similar patient populations. Motivated by real datasets, we study a natural setting where the unknown parameter in each learning instance can be decomposed into a shared global parameter plus a sparse instance-specific term. We propose a novel two-stage multitask learning estimator that exploits this structure in a sample-efficient way, using a unique combination of robust statistics (to learn across similar instances) and LASSO regression (to debias the results). Our estimator yields improved sample complexity bound
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#26410;&#30693;&#22343;&#20540;&#21644;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;$k\geq2$&#30340;&#39640;&#26031;&#32452;&#20998;&#28151;&#21512;&#29289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#38752;&#21306;&#20998;&#35813;&#28151;&#21512;&#29289;&#19982;&#32431;&#39640;&#26031;&#20998;&#24067;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#20165;&#38656;&#35201;&#22312;&#22810;&#39033;&#24335;&#19979;&#30028;&#28151;&#21512;&#37325;&#37327;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2112.05445</link><description>&lt;p&gt;
&#36229;&#36234;&#24179;&#34892;&#24179;&#38754;&#65306;&#38750;&#29699;&#24418;&#39640;&#26031;&#28151;&#21512;&#30340;&#20934;&#22810;&#39033;&#24335;&#26102;&#38388;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Beyond Parallel Pancakes: Quasi-Polynomial Time Guarantees for Non-Spherical Gaussian Mixtures. (arXiv:2112.05445v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.05445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#26410;&#30693;&#22343;&#20540;&#21644;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;$k\geq2$&#30340;&#39640;&#26031;&#32452;&#20998;&#28151;&#21512;&#29289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#38752;&#21306;&#20998;&#35813;&#28151;&#21512;&#29289;&#19982;&#32431;&#39640;&#26031;&#20998;&#24067;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#20165;&#38656;&#35201;&#22312;&#22810;&#39033;&#24335;&#19979;&#30028;&#28151;&#21512;&#37325;&#37327;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#26410;&#30693;&#21017;&#21644;&#26410;&#30693;&#26041;&#24046;&#65288;&#23545;&#20110;&#25152;&#26377;&#32452;&#20998;&#37117;&#30456;&#21516;&#65289;&#30340;$k\geq2$&#20010;&#39640;&#26031;&#32452;&#20998;&#28151;&#21512;&#29289;&#65292;&#36825;&#20123;&#32452;&#20998;&#26159;&#33391;&#22909;&#20998;&#31163;&#30340;&#65292;&#21363;&#19981;&#21516;&#32452;&#20998;&#30340;&#32479;&#35745;&#37325;&#21472;&#26368;&#22810;&#20026;$k^{-C}$&#65292;&#20854;&#20013;$C\geq1$&#20026;&#36275;&#22815;&#22823;&#30340;&#24120;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21482;&#26377;&#22312;&#20801;&#35768;&#28151;&#21512;&#26435;&#20026;&#25351;&#25968;&#32423;&#23567;&#25968;&#26102;&#25165;&#20250;&#20986;&#29616;&#36825;&#31181;&#38590;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#22810;&#39033;&#24335;&#19979;&#30028;&#28151;&#21512;&#37325;&#37327;&#65292;&#20934;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#20197;&#23454;&#29616;&#38750;&#24179;&#20961;&#31639;&#27861;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24179;&#26041;&#21644;&#26041;&#27861;&#30340;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20934;&#22810;&#39033;&#24335;&#65292;&#24182;&#19988;&#21487;&#38752;&#22320;&#21306;&#20998;&#19968;&#20010;&#30001;$k\geq2$&#20010;&#33391;&#22909;&#20998;&#31163;&#30340;&#39640;&#26031;&#32452;&#20998;&#28151;&#21512;&#29289;&#21644;&#19968;&#20010;&#32431;&#39640;&#26031;&#20998;&#24067;&#12290;&#20316;&#20026;&#35777;&#20070;&#65292;&#35813;&#31639;&#27861;&#36755;&#20986;&#27492;&#21306;&#20998;&#30340;&#19968;&#20010;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider mixtures of $k\geq 2$ Gaussian components with unknown means and unknown covariance (identical for all components) that are well-separated, i.e., distinct components have statistical overlap at most $k^{-C}$ for a large enough constant $C\ge 1$. Previous statistical-query [DKS17] and lattice-based [BRST21, GVV22] lower bounds give formal evidence that even distinguishing such mixtures from (pure) Gaussians may be exponentially hard (in $k$).  We show that this kind of hardness can only appear if mixing weights are allowed to be exponentially small, and that for polynomially lower bounded mixing weights non-trivial algorithmic guarantees are possible in quasi-polynomial time. Concretely, we develop an algorithm based on the sum-of-squares method with running time quasi-polynomial in the minimum mixing weight. The algorithm can reliably distinguish between a mixture of $k\ge 2$ well-separated Gaussian components and a (pure) Gaussian distribution. As a certificate, the algori
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#23618;&#19978;&#25345;&#32493;&#20248;&#21270;&#25200;&#21160;&#24182;&#25918;&#22823;&#36825;&#20123;&#25200;&#21160;&#65292;&#20351;&#29992;&#36974;&#32617;&#35821;&#35328;&#27169;&#22411;&#22836;&#23545;&#26368;&#32456;&#25200;&#21160;&#30340;&#28508;&#22312;&#20195;&#34920;&#36827;&#34892;&#35299;&#30721;&#65292;&#20197;&#33719;&#21462;&#21487;&#33021;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#24182;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21046;&#36896;&#36817;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#36890;&#29992;&#21644;&#23450;&#21521;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2110.15317</link><description>&lt;p&gt;
&#26725;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38388;&#30340;&#40511;&#27807;&#65281;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework. (arXiv:2110.15317v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#23618;&#19978;&#25345;&#32493;&#20248;&#21270;&#25200;&#21160;&#24182;&#25918;&#22823;&#36825;&#20123;&#25200;&#21160;&#65292;&#20351;&#29992;&#36974;&#32617;&#35821;&#35328;&#27169;&#22411;&#22836;&#23545;&#26368;&#32456;&#25200;&#21160;&#30340;&#28508;&#22312;&#20195;&#34920;&#36827;&#34892;&#35299;&#30721;&#65292;&#20197;&#33719;&#21462;&#21487;&#33021;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#24182;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21046;&#36896;&#36817;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#36890;&#29992;&#21644;&#23450;&#21521;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#23567;&#25200;&#21160;&#30340;&#23545;&#25239;&#26679;&#26412;&#19978;&#20173;&#34920;&#29616;&#19981;&#20339;&#12290;&#20248;&#21270;&#31867;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#34429;&#24050;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#25991;&#26412;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#30452;&#25509;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#30340;&#35270;&#35273;&#22495;&#20248;&#21270;&#31867;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#25193;&#23637;&#21040;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#30340;&#21046;&#36896;&#19978;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#23884;&#20837;&#23618;&#19978;&#25345;&#32493;&#20248;&#21270;&#25200;&#21160;&#65292;&#24182;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#25918;&#22823;&#36825;&#20123;&#25200;&#21160;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#36974;&#32617;&#35821;&#35328;&#27169;&#22411;&#22836;&#23545;&#26368;&#32456;&#25200;&#21160;&#30340;&#28508;&#22312;&#20195;&#34920;&#36827;&#34892;&#35299;&#30721;&#65292;&#20197;&#33719;&#21462;&#21487;&#33021;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;T-PGD&#65289;&#30340;&#25915;&#20987;&#31639;&#27861;&#26469;&#23454;&#20363;&#21270;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#25991;&#26412;&#39046;&#22495;&#20013;&#24120;&#35265;&#30340;&#20351;&#29992;&#20195;&#29702;&#26799;&#24230;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20063;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#21046;&#36896;&#36817;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#36890;&#29992;&#21644;&#23450;&#21521;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent success on various tasks, deep learning techniques still perform poorly on adversarial examples with small perturbations. While optimization-based methods for adversarial attacks are well-explored in the field of computer vision, it is impractical to directly apply them in natural language processing due to the discrete nature of the text. To address the problem, we propose a unified framework to extend the existing optimization-based adversarial attack methods in the vision domain to craft textual adversarial samples. In this framework, continuously optimized perturbations are added to the embedding layer and amplified in the forward propagation process. Then the final perturbed latent representations are decoded with a masked language model head to obtain potential adversarial samples. In this paper, we instantiate our framework with an attack algorithm named Textual Projected Gradient Descent (T-PGD). We find our algorithm effective even using proxy gradient informati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#30340;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#20840;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2110.02398</link><description>&lt;p&gt;
&#36817;&#20284;&#29275;&#39039;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximate Newton policy gradient algorithms. (arXiv:2110.02398v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.02398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#30340;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#20840;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#12290;&#24120;&#24120;&#20351;&#29992;&#21508;&#31181;&#29109;&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#40723;&#21169;&#25506;&#32034;&#21644;&#25552;&#39640;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29109;&#27491;&#21017;&#21270;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#12290;&#22312;Shannon&#29109;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#22797;&#21046;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#23545;&#20110;&#20854;&#20182;&#29109;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#20102;&#20840;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#29275;&#39039;&#31867;&#22411;&#30340;&#20108;&#27425;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#30456;&#24212;&#30340;&#26799;&#24230;&#27969;&#20840;&#23616;&#25910;&#25947;&#20110;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#24037;&#19994;&#35268;&#27169;&#30340;&#31034;&#20363;&#26469;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#36890;&#24120;&#22312;&#21333;&#20301;&#25968;&#36845;&#20195;&#20013;&#25910;&#25947;&#65292;&#24182;&#19988;&#24448;&#24448;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy gradient algorithms have been widely applied to Markov decision processes and reinforcement learning problems in recent years. Regularization with various entropy functions is often used to encourage exploration and improve stability. This paper proposes an approximate Newton method for the policy gradient algorithm with entropy regularization. In the case of Shannon entropy, the resulting algorithm reproduces the natural policy gradient algorithm. For other entropy functions, this method results in brand-new policy gradient algorithms. We prove that all these algorithms enjoy Newton-type quadratic convergence and that the corresponding gradient flow converges globally to the optimal solution. We use synthetic and industrial-scale examples to demonstrate that the proposed approximate Newton method typically converges in single-digit iterations, often orders of magnitude faster than other state-of-the-art algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19979;&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#31995;&#32479;&#26368;&#20248;&#25805;&#20316;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#22312;&#32447;&#31574;&#30053;&#23398;&#20064;&#26368;&#20248;&#25805;&#20316;&#65292;&#24179;&#34913;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#39134;&#34892;&#25511;&#21046;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#19981;&#31934;&#30830;&#30340;&#31995;&#32479;&#21160;&#24577;&#19979;&#30340;&#23574;&#38160;&#31283;&#23450;&#24615;&#32467;&#26524;&#65292;&#20197;&#21450;&#27425;&#20248;&#24773;&#20917;&#19979;&#24494;&#23567;&#21518;&#24724;&#30340;&#35814;&#32454;&#35828;&#26126;&#12290;</title><link>http://arxiv.org/abs/2109.07630</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#31995;&#32479;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Policies in Continuous-Time Linear Systems. (arXiv:2109.07630v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.07630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19979;&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#31995;&#32479;&#26368;&#20248;&#25805;&#20316;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#22312;&#32447;&#31574;&#30053;&#23398;&#20064;&#26368;&#20248;&#25805;&#20316;&#65292;&#24179;&#34913;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#39134;&#34892;&#25511;&#21046;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#19981;&#31934;&#30830;&#30340;&#31995;&#32479;&#21160;&#24577;&#19979;&#30340;&#23574;&#38160;&#31283;&#23450;&#24615;&#32467;&#26524;&#65292;&#20197;&#21450;&#27425;&#20248;&#24773;&#20917;&#19979;&#24494;&#23567;&#21518;&#24724;&#30340;&#35814;&#32454;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#26159;&#32463;&#20856;&#27169;&#22411;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#20851;&#20110;&#24050;&#30693;&#31995;&#32479;&#26368;&#20248;&#25511;&#21046;&#30340;&#25991;&#29486;&#65292;&#20294;&#22312;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19979;&#65292;&#35813;&#38382;&#39064;&#22312;&#25216;&#26415;&#19978;&#38750;&#24120;&#22256;&#38590;&#19988;&#20960;&#20046;&#27809;&#26377;&#32467;&#26524;&#12290;&#26412;&#25991;&#30528;&#25163;&#30740;&#31350;&#27492;&#38382;&#39064;&#65292;&#24182;&#26088;&#22312;&#23398;&#20064;&#65288;&#24182;&#21516;&#26102;&#37096;&#32626;&#65289;&#26368;&#23567;&#21270;&#20108;&#27425;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#25805;&#20316;&#12290;&#20107;&#23454;&#19978;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#35299;&#20915;&#20102;&#24179;&#34913;&#36830;&#32493;&#31995;&#32479;&#30340;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#22312;&#32447;&#31574;&#30053;&#65292;&#36890;&#36807;&#35880;&#24910;&#22320;&#38543;&#26426;&#21442;&#25968;&#20272;&#35745;&#24555;&#36895;&#23398;&#20064;&#26368;&#20248;&#25805;&#20316;&#65292;&#24182;&#24314;&#31435;&#20854;&#24615;&#33021;&#20445;&#35777;&#65306;&#21518;&#24724;&#30028;&#38543;&#26102;&#38388;&#30340;&#24179;&#26041;&#26681;&#20056;&#20197;&#21442;&#25968;&#25968;&#37327;&#22686;&#38271;&#12290;&#23545;&#39134;&#34892;&#25511;&#21046;&#20219;&#21153;&#30340;&#31574;&#30053;&#23454;&#26045;&#35777;&#26126;&#20102;&#20854;&#21151;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#19981;&#31934;&#30830;&#30340;&#31995;&#32479;&#21160;&#24577;&#65292;&#20855;&#26377;&#23574;&#38160;&#30340;&#31283;&#23450;&#24615;&#32467;&#26524;&#65292;&#24182;&#20005;&#26684;&#35828;&#26126;&#20102;&#27425;&#20248;&#24773;&#20917;&#19979;&#30340;&#24494;&#23567;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear dynamical systems that obey stochastic differential equations are canonical models. While optimal control of known systems has a rich literature, the problem is technically hard under model uncertainty and there are hardly any results. We initiate study of this problem and aim to learn (and simultaneously deploy) optimal actions for minimizing a quadratic cost function. Indeed, this work is the first that comprehensively addresses the crucial challenge of balancing exploration versus exploitation in continuous-time systems. We present online policies that learn optimal actions fast by carefully randomizing the parameter estimates, and establish their performance guarantees: a regret bound that grows with square-root of time multiplied by the number of parameters. Implementation of the policy for a flight-control task demonstrates its efficacy. Further, we prove sharp stability results for inexact system dynamics and tightly specify the infinitesimal regret caused by sub-optimal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#36793;&#30028;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20851;&#20110;Mahalanobis&#33539;&#25968;&#30340;&#26799;&#24230;Lipschitz&#26465;&#20214;&#12290;&#36825;&#20026;&#22797;&#21512;&#20984;&#20248;&#21270;&#38382;&#39064;&#21644;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2102.13244</link><description>&lt;p&gt;
&#24490;&#29615;&#22352;&#26631;&#21452;&#24179;&#22343;&#19982;&#22806;&#25512;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cyclic Coordinate Dual Averaging with Extrapolation. (arXiv:2102.13244v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.13244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#36793;&#30028;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20851;&#20110;Mahalanobis&#33539;&#25968;&#30340;&#26799;&#24230;Lipschitz&#26465;&#20214;&#12290;&#36825;&#20026;&#22797;&#21512;&#20984;&#20248;&#21270;&#38382;&#39064;&#21644;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#22359;&#22352;&#26631;&#26041;&#27861;&#26159;&#19968;&#31867;&#22522;&#26412;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#36341;&#65292;&#24182;&#20316;&#20026;&#32479;&#35745;&#23398;&#20064;&#26631;&#20934;&#36719;&#20214;&#21253;&#30340;&#19968;&#37096;&#20998;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#36890;&#24120;&#19981;&#26159;&#24456;&#28165;&#26970;&#65292;&#36804;&#20170;&#20026;&#27490;&#23427;&#20204;&#30340;&#33391;&#22909;&#23454;&#36341;&#34920;&#29616;&#36824;&#27809;&#26377;&#34987;&#29616;&#26377;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25152;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21333;&#35843;&#31639;&#23376;&#30340;&#24191;&#27867;&#31867;&#21035;&#21464;&#20998;&#19981;&#31561;&#24335;&#65288;VI&#65289;&#38382;&#39064;&#30340;&#26032;&#30340;&#22359;&#22352;&#26631;&#26041;&#27861;&#12290;&#35813;&#31867;&#21035;&#21253;&#25324;&#22797;&#21512;&#20984;&#20248;&#21270;&#38382;&#39064;&#21644;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#20316;&#20026;&#29305;&#20363;&#65292;&#24182;&#19988;&#23578;&#26410;&#34987;&#29616;&#26377;&#24037;&#20316;&#25152;&#35299;&#20915;&#12290;&#25152;&#24471;&#21040;&#30340;&#25910;&#25947;&#30028;&#19982;&#20840;&#26799;&#24230;&#26041;&#27861;&#30340;&#26368;&#20248;&#25910;&#25947;&#30028;&#30456;&#21305;&#37197;&#65292;&#20294;&#26159;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Mahalanobis&#33539;&#25968;&#30340;&#26032;&#22411;&#26799;&#24230;Lipschitz&#26465;&#20214;&#12290; &#23545;&#20110;$m$&#22352;&#26631;&#22359;&#65292;&#25105;&#20204;&#30028;&#38480;&#20013;&#25152;&#38656;&#30340;&#26799;&#24230; Lipschitz&#24120;&#25968;&#27704;&#36828;&#19981;&#20250;&#22823;&#20110;&#19968;&#20010;$\sqrt{m}$&#30340;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyclic block coordinate methods are a fundamental class of optimization methods widely used in practice and implemented as part of standard software packages for statistical learning. Nevertheless, their convergence is generally not well understood and so far their good practical performance has not been explained by existing convergence analyses. In this work, we introduce a new block coordinate method that applies to the general class of variational inequality (VI) problems with monotone operators. This class includes composite convex optimization problems and convex-concave min-max optimization problems as special cases and has not been addressed by the existing work. The resulting convergence bounds match the optimal convergence bounds of full gradient methods, but are provided in terms of a novel gradient Lipschitz condition w.r.t.~a Mahalanobis norm. For $m$ coordinate blocks, the resulting gradient Lipschitz constant in our bounds is never larger than a factor $\sqrt{m}$ compare
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;QMIX&#31639;&#27861;&#30340;&#21464;&#20307;&#21644;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340;&#23454;&#29616;&#25216;&#24039;&#65292;&#21457;&#29616;&#26631;&#20934;&#21270;&#20248;&#21270;&#23545;SMAC&#29615;&#22659;&#30340;&#34920;&#29616;&#26377;&#26174;&#33879;&#24433;&#21709;&#65307;&#21333;&#35843;&#24615;&#32422;&#26463;&#33021;&#25552;&#39640;SMAC&#21644;DEPP&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2102.03479</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#29616;&#25216;&#24039;&#21644;&#21333;&#35843;&#24615;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v19 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.03479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;QMIX&#31639;&#27861;&#30340;&#21464;&#20307;&#21644;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340;&#23454;&#29616;&#25216;&#24039;&#65292;&#21457;&#29616;&#26631;&#20934;&#21270;&#20248;&#21270;&#23545;SMAC&#29615;&#22659;&#30340;&#34920;&#29616;&#26377;&#26174;&#33879;&#24433;&#21709;&#65307;&#21333;&#35843;&#24615;&#32422;&#26463;&#33021;&#25552;&#39640;SMAC&#21644;DEPP&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#22914;&#26426;&#22120;&#20154;&#38598;&#32676;&#25511;&#21046;&#21644;&#33258;&#20027;&#36710;&#36742;&#21327;&#35843;&#65292;&#37117;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20219;&#21153;&#12290;QMIX&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;MARL&#31639;&#27861;&#65292;&#24050;&#34987;&#29992;&#20316;&#22522;&#20934;&#29615;&#22659;&#65292;&#20363;&#22914;&#26143;&#38469;&#20105;&#38712;&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#36187;&#65288;SMAC&#65289;&#21644;&#21319;&#32423;&#29256;&#30340;Predator-Prey&#65288;DEPP&#65289;&#12290;&#26368;&#36817;&#65292;QMIX&#30340;&#21464;&#20307;&#26088;&#22312;&#25918;&#26494;QMIX&#30340;&#21333;&#35843;&#24615;&#32422;&#26463;&#65292;&#20174;&#32780;&#25552;&#39640;SMAC&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#21464;&#20307;&#30340;&#20195;&#30721;&#32423;&#20248;&#21270;&#21644;&#21333;&#35843;&#24615;&#32422;&#26463;&#12290;(1)&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#21464;&#20307;&#30340;&#25913;&#36827;&#21463;&#21040;&#21508;&#31181;&#20195;&#30721;&#32423;&#20248;&#21270;&#30340;&#26174;&#33879;&#24433;&#21709;&#65307;(2)&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#26631;&#20934;&#21270;&#20248;&#21270;&#30340;QMIX&#22312;SMAC&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65307;(3)&#38500;&#20102;&#36825;&#20123;&#31639;&#27861;&#20013;&#30340;&#24120;&#35782;&#65292;&#21333;&#35843;&#24615;&#32422;&#26463;&#36824;&#21487;&#20197;&#25552;&#39640;SMAC&#21644;DEPP&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20026;&#20160;&#20040;&#21333;&#35843;&#24615;&#32422;&#26463;&#22312;&#32431;&#21512;&#20316;MARL&#20013;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many complex multi-agent systems such as robot swarms control and autonomous vehicle coordination can be modeled as Multi-Agent Reinforcement Learning (MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a baseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge (SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX target relaxing the monotonicity constraint of QMIX, allowing for performance improvement in SMAC. In this paper, we investigate the code-level optimizations of these variants and the monotonicity constraint. (1) We find that such improvements of the variants are significantly affected by various code-level optimizations. (2) The experiment results show that QMIX with normalized optimizations outperforms other works in SMAC; (3) beyond the common wisdom from these works, the monotonicity constraint can improve sample efficiency in SMAC and DEPP. We also discuss why monotonicity constraints work well in purely coopera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ABSGD&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#25110;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#26679;&#26412;&#20998;&#37197;&#19968;&#20010;&#20010;&#21035;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#35813;&#26435;&#37325;&#31995;&#32479;&#22320;&#19982;&#25968;&#25454;&#30340;&#25439;&#22833;&#20540;&#25104;&#27604;&#20363;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#21040;&#27599;&#20010;&#31867;&#20013;&#20010;&#21035;&#31034;&#20363;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#21152;&#26435;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#24182;&#22312;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO) &#26694;&#26550;&#20013;&#35299;&#37322;&#20026;&#27491;&#21017;&#21270;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2012.06951</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#20559;&#21521;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Attentional-Biased Stochastic Gradient Descent. (arXiv:2012.06951v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.06951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ABSGD&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#25110;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#26679;&#26412;&#20998;&#37197;&#19968;&#20010;&#20010;&#21035;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#35813;&#26435;&#37325;&#31995;&#32479;&#22320;&#19982;&#25968;&#25454;&#30340;&#25439;&#22833;&#20540;&#25104;&#27604;&#20363;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#21040;&#27599;&#20010;&#31867;&#20013;&#20010;&#21035;&#31034;&#20363;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#21152;&#26435;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#24182;&#22312;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO) &#26694;&#26550;&#20013;&#35299;&#37322;&#20026;&#27491;&#21017;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ABSGD&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#25110;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23545;&#21160;&#37327;SGD&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#22312;&#23567;&#25209;&#37327;&#20013;&#20026;&#27599;&#20010;&#26679;&#26412;&#20998;&#37197;&#19968;&#20010;&#20010;&#21035;&#37325;&#35201;&#24615;&#26435;&#37325;&#12290;&#26679;&#26412;&#25968;&#25454;&#30340;&#20010;&#20307;&#32423;&#21035;&#26435;&#37325;&#31995;&#32479;&#22320;&#19982;&#25968;&#25454;&#30340;&#32553;&#25918;&#25439;&#22833;&#20540;&#30340;&#25351;&#25968;&#25104;&#27604;&#20363;&#65292;&#20854;&#20013;&#32553;&#25918;&#22240;&#23376;&#22312;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO) &#26694;&#26550;&#20013;&#34987;&#35299;&#37322;&#20026;&#27491;&#21017;&#21270;&#21442;&#25968;&#12290;&#26681;&#25454;&#32553;&#25918;&#22240;&#23376;&#26159;&#27491;&#36824;&#26159;&#36127;&#65292;ABSGD&#20445;&#35777;&#25910;&#25947;&#21040;&#20449;&#24687;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20540;&#26368;&#22823;&#20540;&#25110;&#26368;&#23567;&#20540;&#26368;&#23567;&#20540; DRO &#38382;&#39064;&#30340;&#38745;&#24577;&#28857;&#12290;&#19982;&#29616;&#26377;&#30340;&#31867;&#32423;&#21152;&#26435;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#27599;&#20010;&#31867;&#20013;&#20010;&#21035;&#31034;&#20363;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#12290;&#19982;&#38656;&#35201;&#19977;&#27425;&#21453;&#21521;&#20256;&#25773;&#30340;&#20803;&#23398;&#20064;&#20351;&#29992;&#30340;&#29616;&#26377;&#20010;&#20307;&#32423;&#21152;&#26435;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a simple yet effective provable method (named ABSGD) for addressing the data imbalance or label noise problem in deep learning. Our method is a simple modification to momentum SGD where we assign an individual importance weight to each sample in the mini-batch. The individual-level weight of sampled data is systematically proportional to the exponential of a scaled loss value of the data, where the scaling factor is interpreted as the regularization parameter in the framework of distributionally robust optimization (DRO). Depending on whether the scaling factor is positive or negative, ABSGD is guaranteed to converge to a stationary point of an information-regularized min-max or min-min DRO problem, respectively. Compared with existing class-level weighting schemes, our method can capture the diversity between individual examples within each class. Compared with existing individual-level weighting methods using meta-learning that require three backward propaga
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#21305;&#37197;&#31639;&#27861;MALTS&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#12289;&#22522;&#20110;&#37325;&#35201;&#21327;&#21464;&#37327;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20960;&#20046;&#31934;&#30830;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/1811.07415</link><description>&lt;p&gt;
MALTS: &#23398;&#20064;&#25289;&#20280;&#21518;&#21305;&#37197;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
MALTS: Matching After Learning to Stretch. (arXiv:1811.07415v9 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1811.07415
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#21305;&#37197;&#31639;&#27861;MALTS&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#12289;&#22522;&#20110;&#37325;&#35201;&#21327;&#21464;&#37327;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20960;&#20046;&#31934;&#30830;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20960;&#20046;&#31934;&#30830;&#21305;&#37197;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;&#20197;&#24448;&#30340;&#21305;&#37197;&#24037;&#20316;&#22823;&#22810;&#20351;&#29992;&#29305;&#23450;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#20302;&#36136;&#37327;&#30340;&#21305;&#37197;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#19981;&#30456;&#20851;&#21327;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21305;&#37197;&#36317;&#31163;&#24230;&#37327;&#65292;&#23427;&#23548;&#33268;&#26356;&#39640;&#36136;&#37327;&#30340;&#21305;&#37197;&#12290;&#23398;&#20064;&#21040;&#30340;&#36317;&#31163;&#24230;&#37327;&#26681;&#25454;&#27599;&#20010;&#21327;&#21464;&#37327;&#23545;&#20110;&#32467;&#26524;&#39044;&#27979;&#30340;&#36129;&#29486;&#26469;&#25289;&#20280;&#21327;&#21464;&#37327;&#31354;&#38388;&#12290;&#36825;&#31181;&#25289;&#20280;&#24847;&#21619;&#30528;&#22312;&#37325;&#35201;&#21327;&#21464;&#37327;&#19978;&#19981;&#21305;&#37197;&#20250;&#20135;&#29983;&#27604;&#22312;&#19981;&#30456;&#20851;&#21327;&#21464;&#37327;&#19978;&#19981;&#21305;&#37197;&#26356;&#22823;&#30340;&#24809;&#32602;&#12290;&#25105;&#20204;&#23398;&#20064;&#28789;&#27963;&#30340;&#36317;&#31163;&#24230;&#37327;&#30340;&#33021;&#21147;&#20351;&#24471;&#21305;&#37197;&#26159;&#21487;&#35299;&#37322;&#30340;&#24182;&#19988;&#26377;&#21161;&#20110;&#20272;&#35745;&#26377;&#26465;&#20214;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a flexible framework that produces high-quality almost-exact matches for causal inference. Most prior work in matching uses ad-hoc distance metrics, often leading to poor quality matches, particularly when there are irrelevant covariates. In this work, we learn an interpretable distance metric for matching, which leads to substantially higher quality matches. The learned distance metric stretches the covariate space according to each covariate's contribution to outcome prediction: this stretching means that mismatches on important covariates carry a larger penalty than mismatches on irrelevant covariates. Our ability to learn flexible distance metrics leads to matches that are interpretable and useful for the estimation of conditional average treatment effects.
&lt;/p&gt;</description></item></channel></rss>