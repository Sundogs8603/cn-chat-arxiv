<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16900</link><description>&lt;p&gt;
InceptionNeXt&#65306;&#24403;Inception&#36935;&#21040;ConvNeXt
&lt;/p&gt;
&lt;p&gt;
InceptionNeXt: When Inception Meets ConvNeXt. (arXiv:2303.16900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;ViTs&#38271;&#31243;&#24314;&#27169;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#36817;&#26399;&#24191;&#27867;&#30740;&#31350;&#21644;&#37319;&#29992;&#20102;&#22823;&#20869;&#26680;&#21367;&#31215;&#26469;&#25193;&#22823;&#24863;&#21463;&#37326;&#21644;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20363;&#22914;ConvNeXt&#37319;&#29992;&#20102;7x7&#28145;&#24230;&#21367;&#31215;&#12290;&#34429;&#28982;&#36825;&#31181;&#28145;&#24230;&#25805;&#20316;&#20165;&#28040;&#32791;&#23569;&#37327;FLOPs&#65292;&#20294;&#30001;&#20110;&#39640;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#65292;&#36825;&#22312;&#21151;&#33021;&#24378;&#22823;&#30340;&#35745;&#31639;&#35774;&#22791;&#19978;&#22823;&#22823;&#25439;&#23475;&#20102;&#27169;&#22411;&#25928;&#29575;&#12290;&#23613;&#31649;&#32553;&#23567;ConvNeXt&#30340;&#20869;&#26680;&#22823;&#23567;&#33021;&#25552;&#39640;&#36895;&#24230;&#65292;&#20294;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#30528;&#19979;&#38477;&#12290;&#22914;&#20309;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21463;Inceptions&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22823;&#20869;&#26680;&#28145;&#24230;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#65292;&#21363;&#23567;&#26041;&#20869;&#26680;&#12289;&#20004;&#20010;&#27491;&#20132;&#24102;&#20869;&#26680;&#21644;&#19968;&#20010;&#20114;&#34917;&#20869;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the long-range modeling ability of ViTs, large-kernel convolutions are widely studied and adopted recently to enlarge the receptive field and improve model performance, like the remarkable work ConvNeXt which employs 7x7 depthwise convolution. Although such depthwise operator only consumes a few FLOPs, it largely harms the model efficiency on powerful computing devices due to the high memory access costs. For example, ConvNeXt-T has similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained on A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt can improve speed, it results in significant performance degradation. It is still unclear how to speed up large-kernel-based CNN models while preserving their performance. To tackle this issue, inspired by Inceptions, we propose to decompose large-kernel depthwise convolution into four parallel branches along channel dimension, i.e. small square kernel, two orthogonal band kernels, and an ide
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#39537;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;silent&#35270;&#39057;&#21098;&#36753;&#21512;&#25104;&#39640;&#20445;&#30495;&#30340;&#20914;&#20987;&#22768;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#20914;&#20987;&#22768;&#21512;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.16897</link><description>&lt;p&gt;
&#29289;&#29702;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#21512;&#25104;&#20914;&#20987;&#22768;
&lt;/p&gt;
&lt;p&gt;
Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos. (arXiv:2303.16897v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#39537;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;silent&#35270;&#39057;&#21098;&#36753;&#21512;&#25104;&#39640;&#20445;&#30495;&#30340;&#20914;&#20987;&#22768;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#20914;&#20987;&#22768;&#21512;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#21457;&#20986;&#30340;&#22768;&#38899;&#36827;&#34892;&#24314;&#27169;&#23545;&#20110;&#23454;&#38469;&#19990;&#30028;&#21644;&#34394;&#25311;&#19990;&#30028;&#20013;&#30340;&#27785;&#28024;&#24335;&#24863;&#23448;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20914;&#20987;&#22768;&#21512;&#25104;&#26041;&#27861;&#20351;&#29992;&#29289;&#29702;&#27169;&#25311;&#26469;&#33719;&#24471;&#19968;&#32452;&#33021;&#22815;&#34920;&#31034;&#21644;&#21512;&#25104;&#22768;&#38899;&#30340;&#29289;&#29702;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#29289;&#20307;&#30340;&#32454;&#33410;&#21644;&#20914;&#20987;&#20301;&#32622;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#24456;&#23569;&#21487;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#24212;&#29992;&#20110;&#20174;&#26222;&#36890;&#35270;&#39057;&#20013;&#21512;&#25104;&#20914;&#20987;&#22768;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#35270;&#39057;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21482;&#33021;&#25429;&#25417;&#21040;&#35270;&#35273;&#20869;&#23481;&#21644;&#20914;&#20987;&#22768;&#20043;&#38388;&#30340;&#24369;&#23545;&#24212;&#20851;&#31995;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#29289;&#29702;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#38745;&#24577;&#35270;&#39057;&#21098;&#36753;&#21512;&#25104;&#39640;&#20445;&#30495;&#30340;&#20914;&#20987;&#22768;&#12290;&#38500;&#20102;&#35270;&#39057;&#20869;&#23481;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#39069;&#22806;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#20914;&#20987;&#22768;&#21512;&#25104;&#36807;&#31243;&#65292;&#36825;&#20123;&#20808;&#39564;&#21253;&#25324;&#26082;&#21487;&#25511;&#21046;&#29289;&#29702;&#21442;&#25968;&#65292;&#21516;&#26102;&#20063;&#33021;&#20445;&#35777;&#38899;&#25928;&#36136;&#37327;&#30340;&#22122;&#22768;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling sounds emitted from physical object interactions is critical for immersive perceptual experiences in real and virtual worlds. Traditional methods of impact sound synthesis use physics simulation to obtain a set of physics parameters that could represent and synthesize the sound. However, they require fine details of both the object geometries and impact locations, which are rarely available in the real world and can not be applied to synthesize impact sounds from common videos. On the other hand, existing video-driven deep learning-based approaches could only capture the weak correspondence between visual content and impact sounds since they lack of physics knowledge. In this work, we propose a physics-driven diffusion model that can synthesize high-fidelity impact sound for a silent video clip. In addition to the video content, we propose to use additional physics priors to guide the impact sound synthesis procedure. The physics priors include both physics parameters that are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;iNaturalist 2021&#19982;ImageNet&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20855;&#26377;&#24378;&#26377;&#21147;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#26631;&#31614;&#21151;&#33021;&#19982;&#30446;&#26631;&#20219;&#21153;&#23545;&#40784;&#65292;&#20197;&#21450;&#36873;&#25321;&#36866;&#24403;&#30340;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#26102;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16887</link><description>&lt;p&gt;
&#25506;&#31350;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Effect of Pretraining Label Granularity. (arXiv:2303.16887v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;iNaturalist 2021&#19982;ImageNet&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20855;&#26377;&#24378;&#26377;&#21147;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#26631;&#31614;&#21151;&#33021;&#19982;&#30446;&#26631;&#20219;&#21153;&#23545;&#40784;&#65292;&#20197;&#21450;&#36873;&#25321;&#36866;&#24403;&#30340;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#26102;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#22914;&#20309;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#20851;&#27880;&#8220;&#32454;&#21040;&#31895;&#8221;&#30340;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#26631;&#31614;&#27604;&#30446;&#26631;&#38382;&#39064;&#26356;&#32454;&#31890;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;iNaturalist 2021&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#24182;&#35266;&#23519;&#21040;&#30456;&#23545;&#20110;&#22522;&#32447;&#38169;&#35823;&#29575;&#26377;8.76&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#25105;&#20204;&#21457;&#29616;&#20197;&#19979;&#26465;&#20214;&#23545;&#20110;&#25913;&#36827;&#38750;&#24120;&#20851;&#38190;&#65306;1&#65289;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20855;&#26377;&#24378;&#22823;&#19988;&#26377;&#24847;&#20041;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;2&#65289;&#20854;&#26631;&#31614;&#21151;&#33021;&#19982;&#30446;&#26631;&#20219;&#21153;&#30340;&#21151;&#33021;&#24378;&#28872;&#23545;&#40784;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;3&#65289;&#36873;&#25321;&#20102;&#36866;&#24403;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#12290;&#25105;&#20204;&#22312;ImageNet&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#26631;&#31614;&#31890;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;ImageNet21k&#19978;&#30340;&#21494;&#26631;&#31614;&#39044;&#35757;&#32451;&#20135;&#29983;&#20102;&#27604;&#20854;&#20182;&#21512;&#20316;&#26631;&#31614;&#26356;&#22909;&#30340;ImageNet1k&#36801;&#31227;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study how pretraining label granularity affects the generalization of deep neural networks in image classification tasks. We focus on the "fine-to-coarse" transfer learning setting where the pretraining label is more fine-grained than that of the target problem. We experiment with this method using the label hierarchy of iNaturalist 2021, and observe a 8.76% relative improvement of the error rate over the baseline. We find the following conditions are key for the improvement: 1) the pretraining dataset has a strong and meaningful label hierarchy, 2) its label function strongly aligns with that of the target task, and most importantly, 3) an appropriate level of pretraining label granularity is chosen. The importance of pretraining label granularity is further corroborated by our transfer learning experiments on ImageNet. Most notably, we show that pretraining at the leaf labels of ImageNet21k produces better transfer results on ImageNet1k than pretraining at other coa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#38544;&#34255;&#27969;&#24418;Hopfield&#27169;&#22411;&#30340;&#24191;&#20041;Hopfield&#27169;&#22411;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#30528;&#19968;&#31181;&#23398;&#20064;&#30456;&#21464;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.16880</link><description>&lt;p&gt;
&#38544;&#34255;&#27969;&#24418;Hopfield&#27169;&#22411;&#21450;&#20854;&#23398;&#20064;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
The Hidden-Manifold Hopfield Model and a learning phase transition. (arXiv:2303.16880v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16880
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#38544;&#34255;&#27969;&#24418;Hopfield&#27169;&#22411;&#30340;&#24191;&#20041;Hopfield&#27169;&#22411;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#30528;&#19968;&#31181;&#23398;&#20064;&#30456;&#21464;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hopfield&#27169;&#22411;&#22312;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#26377;&#30528;&#24736;&#20037;&#30340;&#20256;&#32479;&#65292;&#26159;&#23569;&#25968;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#31070;&#32463;&#32593;&#32476;&#20043;&#19968;&#12290;&#36890;&#36807;&#23558;Hopfield&#27169;&#22411;&#30340;&#29702;&#35770;&#25299;&#23637;&#21040;&#30456;&#20851;&#25968;&#25454;&#19978;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#25551;&#36848;&#23427;&#20204;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;Hopfield&#27169;&#22411;&#65292;&#31216;&#20026;&#38544;&#34255;&#27969;&#24418;Hopfield&#27169;&#22411;&#65306;&#25105;&#20204;&#20351;&#29992;&#31216;&#20026;&#22240;&#23376;&#30340;$D=\alpha_D N$&#20010;&#38543;&#26426;&#21521;&#37327;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#20351;&#29992;&#26469;&#33258;$P=\alpha N$&#20010;&#31034;&#20363;&#30340;Hebb&#35268;&#21017;&#29983;&#25104;&#32806;&#21512;&#65292;&#20854;&#20013;$N$&#26159;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#12290;&#20351;&#29992;&#37325;&#22797;&#26041;&#27861;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#35813;&#27169;&#22411;&#30340;&#30456;&#22270;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#30456;&#21464;&#65292;&#20854;&#20013;&#22312;&#31034;&#20363;&#20013;&#38544;&#34255;&#30340;&#22240;&#23376;&#25104;&#20026;&#21160;&#24577;&#23398;&#30340;&#21560;&#24341;&#23376;&#65307;&#36825;&#31181;&#30456;&#23384;&#22312;&#20110;&#20851;&#38190;&#30340;$\alpha$&#20540;&#20197;&#19978;&#21644;$\alpha_D$&#20851;&#38190;&#30340;&#20540;&#20197;&#19979;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#34892;&#20026;&#31216;&#20026;&#23398;&#20064;&#30456;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Hopfield model has a long-standing tradition in statistical physics, being one of the few neural networks for which a theory is available. Extending the theory of Hopfield models for correlated data could help understand the success of deep neural networks, for instance describing how they extract features from data. Motivated by this, we propose and investigate a generalized Hopfield model that we name Hidden-Manifold Hopfield Model: we generate the couplings from $P=\alpha N$ examples with the Hebb rule using a non-linear transformation of $D=\alpha_D N$ random vectors that we call factors, with $N$ the number of neurons. Using the replica method, we obtain a phase diagram for the model that shows a phase transition where the factors hidden in the examples become attractors of the dynamics; this phase exists above a critical value of $\alpha$ and below a critical value of $\alpha_D$. We call this behaviour learning transition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#20195;&#29702;&#24314;&#27169;&#22312;&#39640;&#32500;&#29289;&#29702;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16869</link><description>&lt;p&gt;
&#27010;&#29575;&#24314;&#27169;&#19982;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#22312;&#39640;&#32500;&#24212;&#21147;&#22330;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of probabilistic modeling and automated machine learning framework for high-dimensional stress field. (arXiv:2303.16869v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#20195;&#29702;&#24314;&#27169;&#22312;&#39640;&#32500;&#29289;&#29702;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26041;&#27861;&#37319;&#29992;&#39640;&#24230;&#22797;&#26434;&#30340;&#25968;&#23398;&#20844;&#24335;&#65292;&#20351;&#24471;&#24314;&#27169;&#22797;&#26434;&#30340;&#29289;&#29702;&#29616;&#35937;&#12289;&#39044;&#27979;&#20851;&#38190;&#24615;&#33021;&#21644;&#35774;&#35745;&#20248;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35745;&#31639;&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#24230;&#20351;&#24471;&#26597;&#35810;&#25104;&#26412;&#26497;&#39640;&#65292;&#36890;&#24120;&#20250;&#37319;&#29992;&#31616;&#21270;&#27169;&#22411;&#20197;&#25442;&#21462;&#39044;&#27979;&#31934;&#24230;&#21644;&#31934;&#30830;&#24230;&#30340;&#20195;&#20215;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#22312;&#20223;&#30495;&#26114;&#36149;&#30340;&#35745;&#31639;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#29942;&#39048;&#26159;&#26080;&#27861;&#22788;&#29702;&#39640;&#32500;&#24230;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#37327;&#65292;&#38656;&#35201;&#30456;&#23545;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#27492;&#31867;&#38382;&#39064;&#65292;&#24120;&#29992;&#30340;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#20250;&#35201;&#27714;&#22823;&#37327;&#30340;&#35745;&#31639;&#35780;&#20272;&#65292;&#20351;&#24471;&#20854;&#20182;&#25968;&#20540;&#22788;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern computational methods, involving highly sophisticated mathematical formulations, enable several tasks like modeling complex physical phenomenon, predicting key properties and design optimization. The higher fidelity in these computer models makes it computationally intensive to query them hundreds of times for optimization and one usually relies on a simplified model albeit at the cost of losing predictive accuracy and precision. Towards this, data-driven surrogate modeling methods have shown a lot of promise in emulating the behavior of the expensive computer models. However, a major bottleneck in such methods is the inability to deal with high input dimensionality and the need for relatively large datasets. With such problems, the input and output quantity of interest are tensors of high dimensionality. Commonly used surrogate modeling methods for such problems, suffer from requirements like high number of computational evaluations that precludes one from performing other nume
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ALUM&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#25506;&#32034;&#24320;&#37319;&#30340;&#23545;&#25239;&#19977;&#20803;&#32452;&#26469;&#20419;&#36827;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#21644;&#38750;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20197;&#24357;&#34917;&#28508;&#22312;&#25945;&#32451;&#23618;&#30340;&#19981;&#36275;&#12290;&#35813;&#26041;&#27861;&#26159;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25972;&#21512;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#28145;&#24230;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16866</link><description>&lt;p&gt;
ALUM: &#20174;&#28508;&#22312;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34917;&#20607;&#23545;&#25239;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
ALUM: Adversarial Data Uncertainty Modeling from Latent Model Uncertainty Compensation. (arXiv:2303.16866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16866
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ALUM&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#25506;&#32034;&#24320;&#37319;&#30340;&#23545;&#25239;&#19977;&#20803;&#32452;&#26469;&#20419;&#36827;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#21644;&#38750;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20197;&#24357;&#34917;&#28508;&#22312;&#25945;&#32451;&#23618;&#30340;&#19981;&#36275;&#12290;&#35813;&#26041;&#27861;&#26159;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25972;&#21512;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#28145;&#24230;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#19981;&#30830;&#23450;&#24230;&#30340;&#39044;&#27979;&#23545;&#20110;&#24314;&#31435;&#21487;&#20449;&#30340;AI&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#25968;&#25454;&#22122;&#22768;&#24341;&#36215;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#28145;&#24230;&#27169;&#22411;&#20986;&#29616;&#19981;&#30830;&#23450;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24341;&#36215;&#37325;&#35201;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#25506;&#32034;&#21644;&#22788;&#29702;&#30001;&#20110;&#22266;&#26377;&#25968;&#25454;&#22122;&#22768;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ALUM&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#19981;&#26159;&#20165;&#22312;&#28145;&#24230;&#27169;&#22411;&#30340;&#26368;&#32456;&#23618;&#20013;&#26681;&#25454;&#38543;&#26426;&#36873;&#25321;&#30340;&#35757;&#32451;&#25968;&#25454;&#24314;&#27169;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#26159;&#25552;&#20986;&#25506;&#32034;&#24320;&#37319;&#30340;&#23545;&#25239;&#19977;&#20803;&#32452;&#20197;&#20419;&#36827;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#21644;&#38750;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#34917;&#20607;&#35757;&#32451;&#19981;&#36275;&#30340;&#28508;&#20239;&#27169;&#22411;&#23618;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#22122;&#22768;&#25968;&#25454;&#24341;&#36215;&#30340;&#20851;&#38190;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#34987;&#36731;&#26494;&#22320;&#37327;&#21270;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;ALUM&#26159;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#29992;&#24456;&#23569;&#30340;&#39069;&#22806;&#35745;&#31639;&#23454;&#29616;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#28145;&#24230;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is critical that the models pay attention not only to accuracy but also to the certainty of prediction. Uncertain predictions of deep models caused by noisy data raise significant concerns in trustworthy AI areas. To explore and handle uncertainty due to intrinsic data noise, we propose a novel method called ALUM to simultaneously handle the model uncertainty and data uncertainty in a unified scheme. Rather than solely modeling data uncertainty in the ultimate layer of a deep model based on randomly selected training data, we propose to explore mined adversarial triplets to facilitate data uncertainty modeling and non-parametric uncertainty estimations to compensate for the insufficiently trained latent model layers. Thus, the critical data uncertainty and model uncertainty caused by noisy data can be readily quantified for improving model robustness. Our proposed ALUM is model-agnostic which can be easily implemented into any existing deep model with little extra computation overhe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#23616;&#37096;&#32467;&#26500;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#23398;&#20064;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#20445;&#25345;&#36755;&#20837;&#31354;&#38388;&#30340;&#23616;&#37096;&#32467;&#26500;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16861</link><description>&lt;p&gt;
&#36229;&#36234;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65306;&#23616;&#37096;&#32467;&#26500;&#20445;&#25345;&#27491;&#21017;&#21270;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Empirical Risk Minimization: Local Structure Preserving Regularization for Improving Adversarial Robustness. (arXiv:2303.16861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16861
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#23616;&#37096;&#32467;&#26500;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#23398;&#20064;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#20445;&#25345;&#36755;&#20837;&#31354;&#38388;&#30340;&#23616;&#37096;&#32467;&#26500;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#34987;&#23545;&#20154;&#31867;&#19981;&#21487;&#24863;&#30693;&#30340;&#25200;&#21160;&#25152;&#27450;&#39575;&#12290;&#35768;&#22810;&#38450;&#24481;&#25514;&#26045;&#24050;&#34987;&#25552;&#20986;&#20197;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#26368;&#20026;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#29420;&#31435;&#22320;&#22788;&#29702;&#35757;&#32451;&#26679;&#26412;&#24182;&#35201;&#27714;&#22823;&#37327;&#30340;&#26679;&#26412;&#26469;&#35757;&#32451;&#40065;&#26834;&#24615;&#32593;&#32476;&#65292;&#21516;&#26102;&#24573;&#30053;&#36825;&#20123;&#26679;&#26412;&#20043;&#38388;&#30340;&#28508;&#22312;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23616;&#37096;&#32467;&#26500;&#20445;&#25345;&#65288;LSP&#65289;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#23398;&#20064;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#20445;&#25345;&#36755;&#20837;&#31354;&#38388;&#30340;&#23616;&#37096;&#32467;&#26500;&#12290;&#36825;&#26679;&#65292;&#21487;&#32531;&#35299;&#24178;&#20928;&#26679;&#26412;&#38468;&#36817;&#23384;&#22312;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#22312;&#20960;&#20010;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#35770;&#26159;&#21542;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#22343;&#30456;&#23545;&#20110;&#22522;&#32447;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is broadly known that deep neural networks are susceptible to being fooled by adversarial examples with perturbations imperceptible by humans. Various defenses have been proposed to improve adversarial robustness, among which adversarial training methods are most effective. However, most of these methods treat the training samples independently and demand a tremendous amount of samples to train a robust network, while ignoring the latent structural information among these samples. In this work, we propose a novel Local Structure Preserving (LSP) regularization, which aims to preserve the local structure of the input space in the learned embedding space. In this manner, the attacking effect of adversarial samples lying in the vicinity of clean samples can be alleviated. We show strong empirical evidence that with or without adversarial training, our method consistently improves the performance of adversarial robustness on several image classification datasets compared to the baseline
&lt;/p&gt;</description></item><item><title>Phy-DRL&#26159;&#19968;&#31181;&#29289;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#31867;&#26446;&#38597;&#26222;&#35834;&#22827;&#22870;&#36175;&#21644;&#27531;&#24046;&#25511;&#21046;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#20855;&#22791;&#21487;&#35777;&#26126;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#20445;&#38556;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#33021;&#22815;&#25552;&#21319;&#35757;&#32451;&#36895;&#24230;&#21644;&#22870;&#21169;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16860</link><description>&lt;p&gt;
&#29289;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20445;&#38556;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Physical Deep Reinforcement Learning Towards Safety Guarantee. (arXiv:2303.16860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16860
&lt;/p&gt;
&lt;p&gt;
Phy-DRL&#26159;&#19968;&#31181;&#29289;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#31867;&#26446;&#38597;&#26222;&#35834;&#22827;&#22870;&#36175;&#21644;&#27531;&#24046;&#25511;&#21046;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#20855;&#22791;&#21487;&#35777;&#26126;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#20445;&#38556;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#33021;&#22815;&#25552;&#21319;&#35757;&#32451;&#36895;&#24230;&#21644;&#22870;&#21169;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#21644;/&#25110;&#34892;&#21160;&#31354;&#38388;&#30340;&#33258;&#20027;&#31995;&#32479;&#30340;&#22797;&#26434;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#20173;&#28982;&#26159;&#38459;&#30861;DRL&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#31995;&#32479;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Phy-DRL&#65306;&#19968;&#31181;&#29289;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#28085;&#30422;&#20004;&#20010;&#26032;&#39062;&#30340;&#26550;&#26500;&#35774;&#35745;&#65306;i&#65289;&#31867;&#26446;&#38597;&#26222;&#35834;&#22827;&#22870;&#36175;&#65292;ii&#65289;&#27531;&#24046;&#25511;&#21046;&#65288;&#21363;&#29289;&#29702;&#27169;&#22411;&#25511;&#21046;&#21644;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#30340;&#38598;&#25104;&#65289;&#12290;&#21516;&#26102;&#23558;&#29289;&#29702;&#22870;&#21169;&#21644;&#27531;&#24046;&#25511;&#21046;&#38598;&#25104;&#20110;Phy-DRL&#20013;&#65292;&#20351;&#20854;&#20855;&#22791;&#21487;&#35777;&#26126;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#20445;&#38556;&#12290;&#36890;&#36807;&#23545;&#20498;&#31435;&#25670;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Phy-DRL&#30340;&#20445;&#38556;&#23433;&#20840;&#21644;&#31283;&#23450;&#20197;&#21450;&#22686;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#26126;&#26174;&#30340;&#35757;&#32451;&#21152;&#36895;&#21644;&#22686;&#21152;&#22870;&#21169;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has achieved tremendous success in many complex decision-making tasks of autonomous systems with high-dimensional state and/or action spaces. However, the safety and stability still remain major concerns that hinder the applications of DRL to safety-critical autonomous systems. To address the concerns, we proposed the Phy-DRL: a physical deep reinforcement learning framework. The Phy-DRL is novel in two architectural designs: i) Lyapunov-like reward, and ii) residual control (i.e., integration of physics-model-based control and data-driven control). The concurrent physical reward and residual control empower the Phy-DRL the (mathematically) provable safety and stability guarantees. Through experiments on the inverted pendulum, we show that the Phy-DRL features guaranteed safety and stability and enhanced robustness, while offering remarkably accelerated training and enlarged reward.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; Iterative Markovian Fitting&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230; Schr\"odinger&#26725;&#65288;SBs&#65289;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#29616;&#20986;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.16852</link><description>&lt;p&gt;
&#25193;&#25955;Schr\"odinger&#26725;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Diffusion Schr\"odinger Bridge Matching. (arXiv:2303.16852v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; Iterative Markovian Fitting&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230; Schr\"odinger&#26725;&#65288;SBs&#65289;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#29616;&#20986;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#36816;&#36755;&#38382;&#39064;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#30528;&#35768;&#22810;&#24212;&#29992;&#65292;&#20363;&#22914;&#26032;&#22411;&#30340;&#36136;&#37327;&#20256;&#36755;&#26041;&#27861;&#65292;&#22914;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#21644;&#27969;&#21305;&#37197;&#27169;&#22411;&#65288;FMMs&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#25110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#23454;&#29616;&#36825;&#26679;&#30340;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#36817;&#20284;&#30830;&#23450;&#24615;&#21160;&#24577;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#26144;&#23556;&#26159;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#24615;&#36136;&#65292;&#20294; DDMs &#21644; FMMs &#24182;&#19981;&#33021;&#20445;&#35777;&#25552;&#20379;&#25509;&#36817; OT &#26144;&#23556;&#30340;&#20256;&#36755;&#12290;&#30456;&#21453;&#65292;Schr\"odinger&#26725;&#65288;SBs&#65289;&#35745;&#31639;&#38543;&#26426;&#21160;&#24577;&#26144;&#23556;&#65292;&#21487;&#20197;&#24674;&#22797;&#27491;&#21017;&#29109;&#29256;&#26412;&#30340; OT&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#25968;&#20540;&#26041;&#27861;&#36817;&#20284; SBs &#30340;&#32500;&#24230;&#32553;&#25918;&#24046;&#25110;&#22312;&#36845;&#20195;&#20013;&#31215;&#32047;&#35823;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36845;&#20195;&#39532;&#23572;&#31185;&#22827;&#25311;&#21512;&#65292;&#19968;&#31181;&#35299;&#20915;&#39640;&#32500;&#24230; SB &#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26041;&#27861;&#35774;&#35745;&#20026;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#23558;&#32622;&#20449;&#20256;&#25773;&#25193;&#23637;&#21040; KL &#25955;&#24230;&#65292;&#21033;&#29992;&#26465;&#20214;&#29420;&#31435;&#24615;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#30830;&#20445;&#19968;&#33268;&#24615;&#21644;&#25910;&#25947;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#29616;&#26377;&#25104;&#26524;&#26041;&#27861;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving transport problems, i.e. finding a map transporting one given distribution to another, has numerous applications in machine learning. Novel mass transport methods motivated by generative modeling have recently been proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models (FMMs) implement such a transport through a Stochastic Differential Equation (SDE) or an Ordinary Differential Equation (ODE). However, while it is desirable in many applications to approximate the deterministic dynamic Optimal Transport (OT) map which admits attractive properties, DDMs and FMMs are not guaranteed to provide transports close to the OT map. In contrast, Schr\"odinger bridges (SBs) compute stochastic dynamic mappings which recover entropy-regularized versions of OT. Unfortunately, existing numerical methods approximating SBs either scale poorly with dimension or accumulate errors across iterations. In this work, we introduce Iterative Markovian Fitting, a new methodology for solv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#25237;&#24433;&#20984;&#32858;&#31867;&#27169;&#22411;&#65292;&#20855;&#26377;&#31751;&#24674;&#22797;&#20445;&#35777;&#21644;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#32858;&#31867;&#31934;&#24230;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16841</link><description>&lt;p&gt;
&#38543;&#26426;&#25237;&#24433;&#30340;&#20984;&#32858;&#31867;&#27169;&#22411;&#65306;&#21160;&#26426;&#65292;&#23454;&#29616;&#21644;&#31751;&#24674;&#22797;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Randomly Projected Convex Clustering Model: Motivation, Realization, and Cluster Recovery Guarantees. (arXiv:2303.16841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#25237;&#24433;&#20984;&#32858;&#31867;&#27169;&#22411;&#65292;&#20855;&#26377;&#31751;&#24674;&#22797;&#20445;&#35777;&#21644;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#32858;&#31867;&#31934;&#24230;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#25237;&#24433;&#20984;&#32858;&#31867;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;$\mathbb{R}^d$&#20013;&#30340;$n$&#20010;&#39640;&#32500;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#20551;&#35774;&#36825;&#20123;&#28857;&#23384;&#22312;$K$&#20010;&#38544;&#34255;&#30340;&#31751;&#12290;&#19982;&#29992;&#20110;&#32858;&#31867;&#21407;&#22987;&#25968;&#25454;&#30340;&#20984;&#32858;&#31867;&#27169;&#22411;&#30456;&#27604;($d$&#32500;)&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#22914;&#26524;&#23384;&#22312;&#20984;&#32858;&#31867;&#27169;&#22411;&#30340;&#23436;&#32654;&#25286;&#20998;&#65292;&#37027;&#20040;&#36890;&#36807;&#23884;&#20837;&#32500;&#24230;$m=O(\epsilon^{-2}\log(n))$&#30340;&#38543;&#26426;&#25237;&#24433;&#20984;&#32858;&#31867;&#27169;&#22411;&#21487;&#20197;&#20445;&#30041;&#25286;&#20998;&#32467;&#26524;&#65292;&#20854;&#20013;$0 &lt; \epsilon &lt; 1$&#26159;&#19968;&#20123;&#32473;&#23450;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#25913;&#36827;&#20026;$O(\epsilon^{-2}\log(K))$&#65292;&#19981;&#21463;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#36824;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#23383;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#38543;&#26426;&#25237;&#24433;&#20984;&#32858;&#31867;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21331;&#36234;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#21576;&#29616;&#30340;&#25968;&#23383;&#23454;&#39564;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#38543;&#26426;&#25237;&#24433;&#20984;&#32858;&#31867;&#27169;&#22411;&#22312;&#32858;&#31867;&#31934;&#24230;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a randomly projected convex clustering model for clustering a collection of $n$ high dimensional data points in $\mathbb{R}^d$ with $K$ hidden clusters. Compared to the convex clustering model for clustering original data with dimension $d$, we prove that, under some mild conditions, the perfect recovery of the cluster membership assignments of the convex clustering model, if exists, can be preserved by the randomly projected convex clustering model with embedding dimension $m = O(\epsilon^{-2}\log(n))$, where $0 &lt; \epsilon &lt; 1$ is some given parameter. We further prove that the embedding dimension can be improved to be $O(\epsilon^{-2}\log(K))$, which is independent of the number of data points. Extensive numerical experiment results will be presented in this paper to demonstrate the robustness and superior performance of the randomly projected convex clustering model. The numerical results presented in this paper also demonstrate that the randomly projected 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaMMUT&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#65292;&#24182;&#22312;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16839</link><description>&lt;p&gt;
MaMMUT: &#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#32852;&#21512;&#23398;&#20064;&#30340;&#31616;&#21333;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaMMUT&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#65292;&#24182;&#22312;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24050;&#20174;&#32534;&#30721;-&#35299;&#30721;&#36716;&#21521;&#20165;&#35299;&#30721;&#30340;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#26222;&#36941;&#35748;&#20026;&#65292;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#29983;&#25104;&#20219;&#21153;&#21644;&#23545;&#27604;&#20219;&#21153;&#65292;&#24448;&#24448;&#20114;&#30456;&#20914;&#31361;&#65292;&#38590;&#20197;&#22312;&#19968;&#20010;&#26550;&#26500;&#20013;&#23481;&#32435;&#65292;&#24182;&#36827;&#19968;&#27493;&#38656;&#35201;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22797;&#26434;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#20165;&#35299;&#30721;&#27169;&#22411;&#65292;&#36825;&#22312;&#32852;&#21512;&#23398;&#20064;&#36825;&#20123;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22411;MaMMUT&#23454;&#29616;&#30340;&#12290;&#23427;&#30001;&#21333;&#19968;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#25991;&#26412;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#25991;&#26412;&#35299;&#30721;&#22120;&#19978;&#30340;&#26032;&#30340;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#19981;&#21516;&#30446;&#26631;&#20219;&#21153;&#30340;&#32852;&#21512;&#35757;&#32451;&#26159;&#31616;&#21333;&#30340;&#65292;&#26377;&#25928;&#30340;&#65292;&#24182;&#26368;&#22823;&#21270;&#20102;&#27169;&#22411;&#30340;&#26435;&#37325;&#20849;&#20139;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#26550;&#26500;&#20351;&#24471;&#23545;&#24320;&#25918;&#35789;&#27719;&#23545;&#35937;&#26816;&#27979;&#30340;&#31616;&#21333;&#25193;&#23637;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of language models have moved from encoder-decoder to decoder-only designs. In addition, the common knowledge has it that the two most popular multimodal tasks, the generative and contrastive tasks, tend to conflict with one another, are hard to accommodate in one architecture, and further need complex adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint training of these diverse-objective tasks is simple, effective, and maximizes the weight-sharing of the model. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#21160;&#25366;&#25496; Empirical AI Research &#39046;&#22495;&#25490;&#34892;&#27036;&#36825;&#19968;&#20219;&#21153;&#31867;&#21035;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#29616;&#35937;&#12290;&#23454;&#39564;&#27979;&#35797;&#20102;&#20808;&#21069;&#25253;&#21578;&#30340;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#22312;&#20854;&#26410;&#35265;&#36807;&#30340;&#25490;&#34892;&#27036;&#26631;&#31614;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#25110; entailment &#33021;&#21147;&#12290;&#26412;&#25991;&#21019;&#24314;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.16835</link><description>&lt;p&gt;
&#38646;&#26679;&#26412; Entrailment &#29992;&#20110; Empirical AI Research &#39046;&#22495;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Entailment of Leaderboards for Empirical AI Research. (arXiv:2303.16835v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#21160;&#25366;&#25496; Empirical AI Research &#39046;&#22495;&#25490;&#34892;&#27036;&#36825;&#19968;&#20219;&#21153;&#31867;&#21035;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#29616;&#35937;&#12290;&#23454;&#39564;&#27979;&#35797;&#20102;&#20808;&#21069;&#25253;&#21578;&#30340;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#22312;&#20854;&#26410;&#35265;&#36807;&#30340;&#25490;&#34892;&#27036;&#26631;&#31614;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#25110; entailment &#33021;&#21147;&#12290;&#26412;&#25991;&#21019;&#24314;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#25991;&#26412;&#34164;&#21547;&#65288;RTE&#65289;&#20219;&#21153;&#31867;&#21035;&#20013;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#29616;&#35937;&#30340;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#21363;&#33258;&#21160;&#25366;&#25496; Empirical AI Research &#39046;&#22495;&#25490;&#34892;&#27036;&#12290;&#35813;&#39046;&#22495;&#30340;&#25490;&#34892;&#27036;&#25552;&#21462;&#20808;&#21069;&#25253;&#21578;&#30340;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#65292;&#22312;&#38750;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#25253;&#21578;&#20102;&#39640;&#20110;90%&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26680;&#24515;&#30340;&#30740;&#31350;&#38382;&#39064;&#20173;&#26410;&#34987;&#26816;&#39564;&#65306;&#36825;&#20123;&#27169;&#22411;&#30495;&#30340;&#23398;&#20064;&#20102; entailment &#21527;&#65311;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#30340;&#23454;&#39564;&#20013;&#65292;&#27979;&#35797;&#20102;&#20004;&#20010;&#20808;&#21069;&#25253;&#21578;&#30340;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#65292;&#22312;&#20854;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#25490;&#34892;&#27036;&#26631;&#31614;&#19978;&#65292;&#27979;&#35797;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#25110; entailment &#33021;&#21147;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#22914;&#26524;&#27169;&#22411;&#23398;&#20064;&#20102; entailment&#65292;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#20063;&#21487;&#33021;&#26159;&#20013;&#31561;&#30340;&#65292;&#25110;&#32773;&#20855;&#20307;&#26469;&#35828;&#65292;&#22909;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#36828;&#31243;&#26631;&#27880;&#21019;&#24314;&#20102;&#38646;&#26679;&#26412;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a large-scale empirical investigation of the zero-shot learning phenomena in a specific recognizing textual entailment (RTE) task category, i.e. the automated mining of leaderboards for Empirical AI Research. The prior reported state-of-the-art models for leaderboards extraction formulated as an RTE task, in a non-zero-shot setting, are promising with above 90% reported performances. However, a central research question remains unexamined: did the models actually learn entailment? Thus, for the experiments in this paper, two prior reported state-of-the-art models are tested out-of-the-box for their ability to generalize or their capacity for entailment, given leaderboard labels that were unseen during training. We hypothesize that if the models learned entailment, their zero-shot performances can be expected to be moderately high as well--perhaps, concretely, better than chance. As a result of this work, a zero-shot labeled dataset is created via distant labeling formulating
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#20854;&#20182;&#36710;&#36742;&#20132;&#20114;&#21512;&#27969;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#39044;&#27979;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#24230;&#25361;&#25112;&#30340;&#20915;&#31574;&#38382;&#39064;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#37319;&#29992;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#25191;&#34892;&#39640;&#32423;&#39550;&#39542;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.16821</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#39044;&#27979;&#30340;&#20132;&#20114;&#21512;&#27969;&#24773;&#20917;&#19979;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Decision Making for Autonomous Driving in Interactive Merge Scenarios via Learning-based Prediction. (arXiv:2303.16821v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#20854;&#20182;&#36710;&#36742;&#20132;&#20114;&#21512;&#27969;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#39044;&#27979;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#24230;&#25361;&#25112;&#30340;&#20915;&#31574;&#38382;&#39064;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#37319;&#29992;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#25191;&#34892;&#39640;&#32423;&#39550;&#39542;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19982;&#20154;&#31867;&#39550;&#39542;&#21592;&#20849;&#20139;&#36947;&#36335;&#30340;&#33258;&#21160;&#20195;&#29702;&#26041;&#38754;&#65292;&#24517;&#39035;&#32771;&#34385;&#20132;&#36890;&#21442;&#19982;&#32773;&#20043;&#38388;&#24494;&#22937;&#30340;&#20114;&#21160;&#12290;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#22240;&#20026;&#20154;&#31867;&#34892;&#20026;&#21463;&#21040;&#38590;&#20197;&#24314;&#27169;&#30340;&#22810;&#31181;&#22240;&#32032;&#65288;&#20363;&#22914;&#20154;&#31867;&#24847;&#22270;&#21644;&#24773;&#32490;&#65289;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#39550;&#39542;&#30340;&#20915;&#31574;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#22797;&#26434;&#30340;&#21512;&#27969;&#20132;&#36890;&#20219;&#21153;&#65292;&#20854;&#20013;&#19981;&#30830;&#23450;&#24615;&#26469;&#33258;&#20854;&#20182;&#39550;&#39542;&#21592;&#30340;&#34892;&#20026;&#21644;&#19981;&#23436;&#32654;&#30340;&#20256;&#24863;&#22120;&#27979;&#37327;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#65292;&#24182;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#22312;&#32447;&#27714;&#35299;&#12290; POMDP&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25191;&#34892;&#39640;&#32423;&#39550;&#39542;&#25805;&#20316;&#30340;&#31574;&#30053;&#65292;&#20363;&#22914;&#35753;&#36947;&#32473;&#36924;&#36817;&#30340;&#36710;&#36742;&#65292;&#19982;&#21069;&#38754;&#30340;&#36710;&#36742;&#20445;&#25345;&#23433;&#20840;&#36317;&#31163;&#25110;&#21512;&#24182;&#21040;&#20132;&#36890;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#20132;&#36890;&#29366;&#24577;&#65292;&#21516;&#26102;&#26126;&#30830;&#32771;&#34385;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents that drive on roads shared with human drivers must reason about the nuanced interactions among traffic participants. This poses a highly challenging decision making problem since human behavior is influenced by a multitude of factors (e.g., human intentions and emotions) that are hard to model. This paper presents a decision making approach for autonomous driving, focusing on the complex task of merging into moving traffic where uncertainty emanates from the behavior of other drivers and imperfect sensor measurements. We frame the problem as a partially observable Markov decision process (POMDP) and solve it online with Monte Carlo tree search. The solution to the POMDP is a policy that performs high-level driving maneuvers, such as giving way to an approaching car, keeping a safe distance from the vehicle in front or merging into traffic. Our method leverages a model learned from data to predict the future states of traffic while explicitly accounting for interaction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#23548;&#20986;&#24212;&#29992;&#20110;&#26377;&#38480;&#25968;&#25454;&#28857;&#23398;&#20064;&#27169;&#22411;&#30340;PAC-Bayesian&#35823;&#24046;&#30028;&#38480;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#24191;&#27867;&#30340;&#23398;&#20064;/&#31995;&#32479;&#36776;&#35782;&#31639;&#27861;&#25552;&#20379;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.16816</link><description>&lt;p&gt;
&#20174;&#32463;&#39564;&#25439;&#22833;&#20013;&#23398;&#20064;LTI-ss&#31995;&#32479;&#30340;PAC-Bayesian&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian bounds for learning LTI-ss systems with input from empirical loss. (arXiv:2303.16816v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#23548;&#20986;&#24212;&#29992;&#20110;&#26377;&#38480;&#25968;&#25454;&#28857;&#23398;&#20064;&#27169;&#22411;&#30340;PAC-Bayesian&#35823;&#24046;&#30028;&#38480;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#24191;&#27867;&#30340;&#23398;&#20064;/&#31995;&#32479;&#36776;&#35782;&#31639;&#27861;&#25552;&#20379;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24102;&#36755;&#20837;&#30340;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65288;LTI&#65289;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#65292;&#25512;&#23548;&#20986;&#19968;&#31181;&#27010;&#29575;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;-Bayesian&#35823;&#24046;&#30028;&#38480;&#12290;&#35813;&#30028;&#38480;&#24191;&#27867;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#29992;&#20110;&#34920;&#24449;&#20174;&#26377;&#38480;&#25968;&#25454;&#28857;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#26412;&#25991;&#23548;&#20986;&#30340;&#30028;&#38480;&#23558;&#26410;&#26469;&#30340;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#19982;&#27169;&#22411;&#22312;&#23398;&#20064;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#39044;&#27979;&#35823;&#24046;&#32852;&#31995;&#36215;&#26469;&#12290;&#22240;&#27492;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#24191;&#27867;&#30340;&#23398;&#20064;/&#31995;&#32479;&#36776;&#35782;&#31639;&#27861;&#25552;&#20379;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;LTI&#31995;&#32479;&#26159;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#19968;&#20010;&#23376;&#31867;&#65292;&#22240;&#27492;&#36825;&#20123;&#35823;&#24046;&#30028;&#38480;&#21487;&#33021;&#26159;PAC-Bayesian&#30028;&#38480;&#36866;&#29992;&#20110;RNN&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we derive a Probably Approxilmately Correct(PAC)-Bayesian error bound for linear time-invariant (LTI) stochastic dynamical systems with inputs. Such bounds are widespread in machine learning, and they are useful for characterizing the predictive power of models learned from finitely many data points. In particular, with the bound derived in this paper relates future average prediction errors with the prediction error generated by the model on the data used for learning. In turn, this allows us to provide finite-sample error bounds for a wide class of learning/system identification algorithms. Furthermore, as LTI systems are a sub-class of recurrent neural networks (RNNs), these error bounds could be a first step towards PAC-Bayesian bounds for RNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;$C^k$&#65288;&#22312;&#23454;&#21464;&#37327;&#24847;&#20041;&#19979;&#65289;&#30340;&#20989;&#25968;&#65292;&#20351;&#29992;&#20855;&#26377;&#21333;&#23618;&#38544;&#34255;&#23618;&#21644;$m$&#20010;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20197;&#38169;&#35823;&#29575;$m^{-k/(2n)}$&#23558;&#20854;&#36924;&#36817;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#36873;&#21462;&#26435;&#20540;$\sigma_j,b_j\in\mathbb{C}$&#21644;$\rho_j\in\mathbb{C}^n$&#23545;$f$&#36830;&#32493;&#65292;&#37027;&#20040;&#33719;&#24471;&#30340;&#36924;&#36817;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.16813</link><description>&lt;p&gt;
&#27973;&#23618;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#23545;$C^k$-&#20989;&#25968;&#30340;&#26368;&#20248;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Optimal approximation of $C^k$-functions using shallow complex-valued neural networks. (arXiv:2303.16813v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;$C^k$&#65288;&#22312;&#23454;&#21464;&#37327;&#24847;&#20041;&#19979;&#65289;&#30340;&#20989;&#25968;&#65292;&#20351;&#29992;&#20855;&#26377;&#21333;&#23618;&#38544;&#34255;&#23618;&#21644;$m$&#20010;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20197;&#38169;&#35823;&#29575;$m^{-k/(2n)}$&#23558;&#20854;&#36924;&#36817;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#36873;&#21462;&#26435;&#20540;$\sigma_j,b_j\in\mathbb{C}$&#21644;$\rho_j\in\mathbb{C}^n$&#23545;$f$&#36830;&#32493;&#65292;&#37027;&#20040;&#33719;&#24471;&#30340;&#36924;&#36817;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#27973;&#23618;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#23545;&#22797;&#31435;&#26041;&#20307;&#19978;$C^k$&#65288;&#22312;&#23454;&#21464;&#37327;&#24847;&#20041;&#19979;&#65289;&#30340;&#20989;&#25968;&#36827;&#34892;&#36924;&#36817;&#30340;&#37327;&#21270;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#21333;&#23618;&#38544;&#34255;&#23618;&#21644;$m$&#20010;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21363;&#24418;&#22914;$z \mapsto \sum_{j=1}^m \sigma_j \cdot \phi\big(\rho_j^T z + b_j\big)$&#30340;&#32593;&#32476;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;&#36825;&#31181;&#24418;&#24335;&#30340;&#20989;&#25968;&#36924;&#36817;$C^k \left(\Omega_n;\mathbb{C}\right)$&#20013;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#24403;$m\to\infty$&#26102;&#35823;&#24046;&#20026;$m^{-k/(2n)}$.&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#36873;&#21462;&#26435;&#20540;$\sigma_j,b_j\in\mathbb{C}$&#21644;$\rho_j\in\mathbb{C}^n$&#23545;$f$&#36830;&#32493;&#24182;&#19988;&#22312;&#36825;&#31181;&#36830;&#32493;&#24615;&#20551;&#35774;&#19979;&#33719;&#24471;&#30340;&#36924;&#36817;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a quantitative result for the approximation of functions of regularity $C^k$ (in the sense of real variables) defined on the complex cube $\Omega_n := [-1,1]^n +i[-1,1]^n\subseteq \mathbb{C}^n$ using shallow complex-valued neural networks. Precisely, we consider neural networks with a single hidden layer and $m$ neurons, i.e., networks of the form $z \mapsto \sum_{j=1}^m \sigma_j \cdot \phi\big(\rho_j^T z + b_j\big)$ and show that one can approximate every function in $C^k \left( \Omega_n; \mathbb{C}\right)$ using a function of that form with error of the order $m^{-k/(2n)}$ as $m \to \infty$, provided that the activation function $\phi: \mathbb{C} \to \mathbb{C}$ is smooth but not polyharmonic on some non-empty open set. Furthermore, we show that the selection of the weights $\sigma_j, b_j \in \mathbb{C}$ and $\rho_j \in \mathbb{C}^n$ is continuous with respect to $f$ and prove that the derived rate of approximation is optimal under this continuity assumption. We also discuss
&lt;/p&gt;</description></item><item><title>&#24314;&#35758;&#23558;&#25512;&#26029;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#25972;&#21512;&#21040;&#27491;&#21017;&#21270;&#24378;&#24230;&#30340;&#20132;&#21449;&#39564;&#35777;&#20013;&#65292;&#20197;&#25913;&#21892;&#39640;&#26031;&#22270;&#27169;&#22411;&#22312;&#35266;&#23519;&#21547;&#22122;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16796</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22359;&#21270;&#27491;&#21017;&#21270;&#21487;&#25913;&#21892;&#35266;&#23519;&#21547;&#22122;&#25968;&#25454;&#30340;&#39640;&#26031;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Module-based regularization improves Gaussian graphical models when observing noisy data. (arXiv:2303.16796v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16796
&lt;/p&gt;
&lt;p&gt;
&#24314;&#35758;&#23558;&#25512;&#26029;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#25972;&#21512;&#21040;&#27491;&#21017;&#21270;&#24378;&#24230;&#30340;&#20132;&#21449;&#39564;&#35777;&#20013;&#65292;&#20197;&#25913;&#21892;&#39640;&#26031;&#22270;&#27169;&#22411;&#22312;&#35266;&#23519;&#21547;&#22122;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#39640;&#26031;&#22270;&#27169;&#22411;&#34920;&#31034;&#22810;&#21464;&#37327;&#30456;&#20851;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#27491;&#21017;&#21270;&#26469;&#31232;&#30095;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#27491;&#21017;&#21270;&#24378;&#24230;&#30340;&#20132;&#21449;&#39564;&#35777;&#20013;&#65292;&#23558;&#25512;&#26029;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#25972;&#21512;&#36215;&#26469;&#20197;&#24179;&#34913;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20351;&#29992;&#39640;&#26031;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#30340;&#26631;&#20934;&#26041;&#27861;&#65288;&#22270;&#24418;&#22871;&#32034;&#27861;&#65289;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#24674;&#22797;&#21644;&#25512;&#26029;&#21547;&#22122;&#22768;&#25968;&#25454;&#20013;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers often represent relations in multi-variate correlational data using Gaussian graphical models, which require regularization to sparsify the models. Acknowledging that they often study the modular structure of the inferred network, we suggest integrating it in the cross-validation of the regularization strength to balance under- and overfitting. Using synthetic and real data, we show that this approach allows us to better recover and infer modular structure in noisy data compared with the graphical lasso, a standard approach using the Gaussian log-likelihood when cross-validating the regularization strength.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GRAF&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#32593;&#32476;&#34701;&#21512;&#22312;&#22810;&#20010;&#32593;&#32476;&#19978;&#20351;&#29992;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;attention-based neighborhood aggregation&#65292;GRAF&#33021;&#22815;&#23398;&#20064;&#27599;&#20010;&#33410;&#28857;&#21644;&#20851;&#32852;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#22312;&#32593;&#32476;&#34701;&#21512;&#20013;&#36827;&#34892;&#36793;&#32536;&#21152;&#26435;&#12290;</title><link>http://arxiv.org/abs/2303.16781</link><description>&lt;p&gt;
GRAF&#65306;&#22270;&#24418;&#27880;&#24847;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GRAF: Graph Attention-aware Fusion Networks. (arXiv:2303.16781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GRAF&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#32593;&#32476;&#34701;&#21512;&#22312;&#22810;&#20010;&#32593;&#32476;&#19978;&#20351;&#29992;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;attention-based neighborhood aggregation&#65292;GRAF&#33021;&#22815;&#23398;&#20064;&#27599;&#20010;&#33410;&#28857;&#21644;&#20851;&#32852;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#22312;&#32593;&#32476;&#34701;&#21512;&#20013;&#36827;&#34892;&#36793;&#32536;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#21253;&#21547;&#22810;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#20316;&#20026;&#19968;&#31181;&#21033;&#29992;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#29305;&#24449;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24050;&#32463;&#35777;&#26126;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#27969;&#34892;&#30340;&#22522;&#20110;GNN&#30340;&#26550;&#26500;&#21482;&#33021;&#22788;&#29702;&#19968;&#20010;&#21516;&#26500;&#32593;&#32476;&#12290;&#20351;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#22810;&#20010;&#32593;&#32476;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#32593;&#32476;&#30340;&#24322;&#26500;&#24615;&#21644;&#29616;&#26377;&#20851;&#32852;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GRAF&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#21033;&#29992;GNN&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#32593;&#32476;&#19978;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#32593;&#32476;&#34701;&#21512;&#12290;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#37051;&#22495;&#32858;&#21512;&#65292;GRAF&#23398;&#20064;&#27599;&#20010;&#33410;&#28857;&#30340;&#37051;&#23621;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#65288;&#31216;&#20026;&#33410;&#28857;&#32423;&#27880;&#24847;&#21147;&#65289;&#20197;&#21450;&#20998;&#23618;&#26041;&#24335;&#19979;&#30340;&#20851;&#32852;&#30340;&#37325;&#35201;&#24615;&#65288;&#31216;&#20026;&#20851;&#32852;&#32423;&#27880;&#24847;&#21147;&#65289;&#12290;&#28982;&#21518;&#65292;GRAF&#22788;&#29702;&#19968;&#20010;&#32593;&#32476;&#34701;&#21512;&#27493;&#39588;&#65292;&#26681;&#25454;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#21644;&#20851;&#32852;&#32423;&#27880;&#24847;&#21147;&#21152;&#26435;&#27599;&#26465;&#36793;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large number of real-world networks include multiple types of nodes and edges. Graph Neural Network (GNN) emerged as a deep learning framework to utilize node features on graph-structured data showing superior performance. However, popular GNN-based architectures operate on one homogeneous network. Enabling them to work on multiple networks brings additional challenges due to the heterogeneity of the networks and the multiplicity of the existing associations. In this study, we present a computational approach named GRAF utilizing GNN-based approaches on multiple networks with the help of attention mechanisms and network fusion. Using attention-based neighborhood aggregation, GRAF learns the importance of each neighbor per node (called node-level attention) followed by the importance of association (called association-level attention) in a hierarchical way. Then, GRAF processes a network fusion step weighing each edge according to learned node- and association-level attention, which r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25506;&#32034;&#23186;&#20307;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;&#36866;&#29992;&#20110;&#22312;&#32447;&#26032;&#38395;&#12289;&#30005;&#35270;&#24191;&#25773;&#25110;&#24191;&#25773;&#33410;&#30446;&#20869;&#23481;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#20844;&#20247;&#33286;&#35770;&#24182;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.16779</link><description>&lt;p&gt;
&#22522;&#20110;&#23186;&#20307;&#20559;&#22909;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#20844;&#20247;&#33286;&#35770;
&lt;/p&gt;
&lt;p&gt;
Language Models Trained on Media Diets Can Predict Public Opinion. (arXiv:2303.16779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25506;&#32034;&#23186;&#20307;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;&#36866;&#29992;&#20110;&#22312;&#32447;&#26032;&#38395;&#12289;&#30005;&#35270;&#24191;&#25773;&#25110;&#24191;&#25773;&#33410;&#30446;&#20869;&#23481;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#20844;&#20247;&#33286;&#35770;&#24182;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20247;&#33286;&#35770;&#21453;&#26144;&#21644;&#22609;&#36896;&#31038;&#20250;&#34892;&#20026;&#65292;&#20294;&#20256;&#32479;&#30340;&#22522;&#20110;&#35843;&#26597;&#30340;&#24037;&#20855;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25506;&#32034;&#23186;&#20307;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;&#36866;&#29992;&#20110;&#22312;&#32447;&#26032;&#38395;&#12289;&#30005;&#35270;&#24191;&#25773;&#25110;&#24191;&#25773;&#33410;&#30446;&#20869;&#23481;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#27169;&#25311;&#24050;&#28040;&#36153;&#19968;&#32452;&#23186;&#20307;&#30340;&#20122;&#32676;&#20307;&#30340;&#24847;&#35265;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#32654;&#22269;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#20013;&#38024;&#23545;COVID-19&#21644;&#28040;&#36153;&#32773;&#20449;&#24515;&#30340;&#35266;&#28857;&#34920;&#36798;&#20316;&#20026;&#22522;&#26412;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#65288;1&#65289;&#21487;&#20197;&#39044;&#27979;&#35843;&#26597;&#21709;&#24212;&#20998;&#24067;&#20013;&#30340;&#20154;&#31867;&#21028;&#26029;&#65292;&#24182;&#19988;&#23545;&#25514;&#36766;&#21644;&#23186;&#20307;&#26333;&#20809;&#28192;&#36947;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#65288;2&#65289;&#22312;&#24314;&#27169;&#26356;&#23494;&#20999;&#20851;&#27880;&#23186;&#20307;&#30340;&#20154;&#26041;&#38754;&#26356;&#20934;&#30830;&#65292;&#65288;3&#65289;&#31526;&#21512;&#20851;&#20110;&#21738;&#20123;&#31867;&#22411;&#30340;&#35266;&#28857;&#21463;&#23186;&#20307;&#28040;&#36153;&#24433;&#21709;&#30340;&#25991;&#29486;&#12290;&#25506;&#27979;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26032;&#26041;&#27861;&#26469;&#30740;&#31350;&#23186;&#20307;&#25928;&#24212;&#65292;&#20855;&#26377;&#22312;&#34917;&#20805;&#35843;&#26597;&#21644;&#39044;&#27979;&#20844;&#20247;&#33286;&#35770;&#26041;&#38754;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20063;&#28041;&#21450;&#35299;&#20915;&#19982;&#23186;&#20307;&#20559;&#35265;&#26377;&#20851;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public opinion reflects and shapes societal behavior, but the traditional survey-based tools to measure it are limited. We introduce a novel approach to probe media diet models -- language models adapted to online news, TV broadcast, or radio show content -- that can emulate the opinions of subpopulations that have consumed a set of media. To validate this method, we use as ground truth the opinions expressed in U.S. nationally representative surveys on COVID-19 and consumer confidence. Our studies indicate that this approach is (1) predictive of human judgements found in survey response distributions and robust to phrasing and channels of media exposure, (2) more accurate at modeling people who follow media more closely, and (3) aligned with literature on which types of opinions are affected by media consumption. Probing language models provides a powerful new method for investigating media effects, has practical applications in supplementing polls and forecasting public opinion, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21576;&#29616;&#20102;&#19968;&#20010;&#20004;&#30334;&#19975;&#20221;&#28921;&#39274;&#39135;&#35889;&#26032;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23558;30&#19975;&#20221;&#39135;&#35889;&#25353;&#29031;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36827;&#34892;&#20998;&#31867;&#21040;9&#20010;&#31867;&#21035;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#23545;&#20854;&#20313;&#30340;1900K&#20221;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.16778</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#20004; &#30334;&#19975;&#20221;&#26631;&#35760;&#32654;&#39135;&#39135;&#35889;&#25968;&#25454;&#38598; - 3A2M
&lt;/p&gt;
&lt;p&gt;
Assorted, Archetypal and Annotated Two Million (3A2M) Cooking Recipes Dataset based on Active Learning. (arXiv:2303.16778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21576;&#29616;&#20102;&#19968;&#20010;&#20004;&#30334;&#19975;&#20221;&#28921;&#39274;&#39135;&#35889;&#26032;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23558;30&#19975;&#20221;&#39135;&#35889;&#25353;&#29031;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36827;&#34892;&#20998;&#31867;&#21040;9&#20010;&#31867;&#21035;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#23545;&#20854;&#20313;&#30340;1900K&#20221;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28921;&#39274;&#39135;&#35889;&#21487;&#20197;&#20132;&#25442;&#28921;&#39274;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#39135;&#21697;&#30340;&#21046;&#20316;&#35828;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#35813;&#39046;&#22495;&#20869;&#30001;&#20110;&#32570;&#20047;&#36275;&#22815;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23558;&#22312;&#32447;&#25214;&#21040;&#30340;&#21407;&#22987;&#39135;&#35889;&#20998;&#31867;&#21040;&#21512;&#36866;&#30340;&#39135;&#21697;&#31867;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#23558;&#39135;&#35889;&#20998;&#31867;&#21487;&#33021;&#26159;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21576;&#29616;&#20102;&#19968;&#20010;&#20004;&#30334;&#19975;&#20221;&#28921;&#39274;&#39135;&#35889;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#23558;&#20854;&#26631;&#35760;&#22312;&#21508;&#33258;&#30340;&#31867;&#21035;&#20013;&#12290;&#20026;&#20102;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;RecipeNLG&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#39135;&#35889;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#21487;&#20449;&#24230;&#24471;&#20998;&#39640;&#20110;86.667&#65285;&#30340;&#20154;&#31867;&#19987;&#23478;&#25353;&#29031;&#20854;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#23558;30&#19975;&#20221;&#39135;&#35889;&#20998;&#31867;&#21040;&#20061;&#20010;&#31867;&#21035;&#20043;&#19968;&#65306;&#28888;&#28953;&#12289;&#39278;&#26009;&#12289;&#33636;&#33756;&#12289;&#34092;&#33756;&#12289;&#24555;&#39184;&#12289;&#40614;&#29255;&#12289;&#39184;&#28857;&#12289;&#37197;&#33756;&#21644;&#34701;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Query-by-Committee&#21644;Human&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#21097;&#20313;&#30340;1900K&#20221;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooking recipes allow individuals to exchange culinary ideas and provide food preparation instructions. Due to a lack of adequate labeled data, categorizing raw recipes found online to the appropriate food genres is a challenging task in this domain. Utilizing the knowledge of domain experts to categorize recipes could be a solution. In this study, we present a novel dataset of two million culinary recipes labeled in respective categories leveraging the knowledge of food experts and an active learning technique. To construct the dataset, we collect the recipes from the RecipeNLG dataset. Then, we employ three human experts whose trustworthiness score is higher than 86.667% to categorize 300K recipe by their Named Entity Recognition (NER) and assign it to one of the nine categories: bakery, drinks, non-veg, vegetables, fast food, cereals, meals, sides and fusion. Finally, we categorize the remaining 1900K recipes using Active Learning method with a blend of Query-by-Committee and Human 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#24773;&#24863;&#35821;&#35328;&#26816;&#27979;COVID-19&#38169;&#35823;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#21333;&#19968;&#30340;&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#26159;&#20302;&#36136;&#37327;&#26631;&#31614;&#21644;&#19981;&#21305;&#37197;&#30340;&#26631;&#31614;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.16777</link><description>&lt;p&gt;
&#19981;&#20919;&#38745;&#65292;&#19981;&#20919;&#38745;&#65292;&#20063;&#19981;&#38215;&#23450;&#65306;&#20351;&#29992;&#24773;&#24863;&#35821;&#35328;&#26816;&#27979;COVID-19&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Not cool, calm or collected: Using emotional language to detect COVID-19 misinformation. (arXiv:2303.16777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#24773;&#24863;&#35821;&#35328;&#26816;&#27979;COVID-19&#38169;&#35823;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#21333;&#19968;&#30340;&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#26159;&#20302;&#36136;&#37327;&#26631;&#31614;&#21644;&#19981;&#21305;&#37197;&#30340;&#26631;&#31614;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22914;Twitter&#19978;&#30340;COVID-19&#38169;&#35823;&#20449;&#24687;&#23545;&#26377;&#25928;&#30340;&#30123;&#24773;&#31649;&#29702;&#26500;&#25104;&#23041;&#32961;&#12290;&#20808;&#21069;&#22312;&#25512;&#29305;&#19978;&#30340;COVID-19&#38169;&#35823;&#20449;&#24687;&#30340;&#24037;&#20316;&#21542;&#35748;&#20102;&#25512;&#29305;&#19978;&#26222;&#36941;&#23384;&#22312;&#30340;&#35832;&#22914;&#24102;&#30005;&#24773;&#24863;&#30340;&#35821;&#20041;&#29305;&#24449;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;COVID-19&#38169;&#35823;&#20449;&#24687;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#25512;&#29305;&#24773;&#24863;&#32534;&#30721;&#22120;&#21644;COVID-19&#38169;&#35823;&#20449;&#24687;&#32534;&#30721;&#22120;&#26469;&#39044;&#27979;&#25512;&#25991;&#26159;&#21542;&#21253;&#21547;COVID-19&#38169;&#35823;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#24773;&#24863;&#32534;&#30721;&#22120;&#22312;&#19968;&#32452;&#26032;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#25105;&#20204;&#30340;COVID-19&#38169;&#35823;&#20449;&#24687;&#32534;&#30721;&#22120;&#22312;COVID-HeRA&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#24773;&#24863;&#21644;&#38169;&#35823;&#20449;&#24687;&#32534;&#30721;&#22120;&#30340;&#32452;&#21512;&#27604;&#21333;&#29420;&#30340;&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#22120;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#20302;&#36136;&#37327;&#26631;&#31614;&#21644;&#19981;&#21305;&#37197;&#30340;&#26631;&#31614;&#20998;&#24067;&#26159;&#25105;&#20204;&#30740;&#31350;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
COVID-19 misinformation on social media platforms such as twitter is a threat to effective pandemic management. Prior works on tweet COVID-19 misinformation negates the role of semantic features common to twitter such as charged emotions. Thus, we present a novel COVID-19 misinformation model, which uses both a tweet emotion encoder and COVID-19 misinformation encoder to predict whether a tweet contains COVID-19 misinformation. Our emotion encoder was fine-tuned on a novel annotated dataset and our COVID-19 misinformation encoder was fine-tuned on a subset of the COVID-HeRA dataset. Experimental results show superior results using the combination of emotion and misinformation encoders as opposed to a misinformation classifier alone. Furthermore, extensive result analysis was conducted, highlighting low quality labels and mismatched label distributions as key limitations to our study.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20050;&#20051;&#29699;&#21333;&#25171;&#27604;&#36187;&#32467;&#26524;&#65292;&#20351;&#29992;&#29699;&#21592;&#21644;&#27604;&#36187;&#32479;&#35745;&#25968;&#25454;&#20316;&#20026;&#29305;&#24449;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;5&#25240;&#20132;&#21449;&#39564;&#35777;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#32858;&#21512;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#26368;&#26032;&#30340;&#31867;&#20284;&#30740;&#31350;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.16776</link><description>&lt;p&gt;
&#20050;&#20051;&#29699;&#27604;&#36187;&#39044;&#27979;&#30340;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised Learning for Table Tennis Match Prediction. (arXiv:2303.16776v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20050;&#20051;&#29699;&#21333;&#25171;&#27604;&#36187;&#32467;&#26524;&#65292;&#20351;&#29992;&#29699;&#21592;&#21644;&#27604;&#36187;&#32479;&#35745;&#25968;&#25454;&#20316;&#20026;&#29305;&#24449;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;5&#25240;&#20132;&#21449;&#39564;&#35777;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#32858;&#21512;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#26368;&#26032;&#30340;&#31867;&#20284;&#30740;&#31350;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#27169;&#22411;&#22312;&#21508;&#31181;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#36816;&#21160;&#20998;&#26512;&#26159;&#20854;&#20013;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#20027;&#27969;&#36816;&#21160;&#30340;&#33258;&#21160;&#35009;&#21028;&#21644;&#20260;&#23475;&#39044;&#38450;&#19978;&#12290;&#23545;&#20110;&#20854;&#20182;&#36816;&#21160;&#65292;&#20363;&#22914;&#20050;&#20051;&#29699;&#65292;&#30740;&#31350;&#25165;&#21018;&#21018;&#24320;&#22987;&#21463;&#21040;&#26356;&#22810;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#20050;&#20051;&#29699;&#21333;&#25171;&#27604;&#36187;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#29699;&#21592;&#21644;&#27604;&#36187;&#32479;&#35745;&#25968;&#25454;&#20316;&#20026;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#35780;&#20272;&#23427;&#20204;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#22312;&#27169;&#22411;&#26041;&#38754;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35768;&#22810;&#27969;&#34892;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;5&#25240;&#20132;&#21449;&#39564;&#35777;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#28040;&#34701;&#30740;&#31350;&#20013;&#35843;&#26597;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#32858;&#21512;&#31574;&#30053;&#65292;&#20197;&#35777;&#26126;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#19981;&#21516;&#27169;&#22411;&#30340;&#34920;&#29616;&#30456;&#24403;&#65292;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#65288;61-70%&#65289;&#19982;&#31867;&#20284;&#36816;&#21160;&#20013;&#30340;&#26368;&#26032;&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#20363;&#22914;...
&lt;/p&gt;
&lt;p&gt;
Machine learning, classification and prediction models have applications across a range of fields. Sport analytics is an increasingly popular application, but most existing work is focused on automated refereeing in mainstream sports and injury prevention. Research on other sports, such as table tennis, has only recently started gaining more traction. This paper proposes the use of machine learning to predict the outcome of table tennis single matches. We use player and match statistics as features and evaluate their relative importance in an ablation study. In terms of models, a number of popular models were explored. We found that 5-fold cross-validation and hyperparameter tuning was crucial to improve model performance. We investigated different feature aggregation strategies in our ablation study to demonstrate the robustness of the models. Different models performed comparably, with the accuracy of the results (61-70%) matching state-of-the-art models in comparable sports, such as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#38750;&#20020;&#24202;&#21644;&#20813;&#36153;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#32467;&#21512;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#25991;&#26412;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#23637;&#31034;&#20851;&#20110;&#30284;&#30151;&#30149;&#31243;&#20449;&#24687;&#30340;&#35745;&#31639;&#26377;&#25928;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.16766</link><description>&lt;p&gt;
&#29992;&#38750;&#20020;&#24202;&#25991;&#26412;&#20449;&#24687;&#26816;&#32034;&#23454;&#29616;&#32959;&#30244;&#30456;&#20851;&#35770;&#22363;&#24086;&#23376;&#30340;&#35745;&#31639;&#26377;&#25928;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Computationally Efficient Labeling of Cancer Related Forum Posts by Non-Clinical Text Information Retrieval. (arXiv:2303.16766v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#38750;&#20020;&#24202;&#21644;&#20813;&#36153;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#32467;&#21512;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#25991;&#26412;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#23637;&#31034;&#20851;&#20110;&#30284;&#30151;&#30149;&#31243;&#20449;&#24687;&#30340;&#35745;&#31639;&#26377;&#25928;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#19978;&#23384;&#22312;&#30528;&#22823;&#37327;&#20851;&#20110;&#30284;&#30151;&#30340;&#20449;&#24687;&#65292;&#20294;&#20998;&#31867;&#21644;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#24456;&#22256;&#38590;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#22788;&#29702;&#30740;&#31350;&#37117;&#28041;&#21450;&#27491;&#24335;&#30340;&#20020;&#24202;&#25968;&#25454;&#65292;&#20294;&#38750;&#20020;&#24202;&#25968;&#25454;&#20013;&#20063;&#26377;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#23558;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#25991;&#26412;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#26041;&#27861;&#32467;&#21512;&#25104;&#19968;&#20010;&#36830;&#36143;&#12289;&#35745;&#31639;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#22522;&#20110;&#38750;&#20020;&#24202;&#21644;&#20813;&#36153;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#28548;&#28165;&#30284;&#30151;&#24739;&#32773;&#30340;&#30149;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;&#21407;&#22411;&#65292;&#21487;&#20197;&#20174;&#38750;&#20020;&#24202;&#35770;&#22363;&#24086;&#23376;&#20013;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#23637;&#31034;&#20851;&#20110;&#30284;&#30151;&#30149;&#31243;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#32858;&#31867;&#31639;&#27861;&#65288;MR-DBSCAN&#12289;DBSCAN&#21644;HDBSCAN&#65289;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#35843;&#25972;&#21518;&#30340;&#20848;&#24503;&#25351;&#25968;&#21644;&#24635;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20316;&#20026;&#26816;&#32034;&#30340;&#24086;&#23376;&#25968;&#37327;&#21644;&#37051;&#22495;&#21322;&#24452;&#20989;&#25968;&#12290;&#32858;&#31867;&#32467;&#26524;&#26174;&#31034;&#65292;&#37051;&#22495;&#21322;&#24452;&#23545;&#32858;&#31867;&#32467;&#26524;&#26377;&#26368;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
An abundance of information about cancer exists online, but categorizing and extracting useful information from it is difficult. Almost all research within healthcare data processing is concerned with formal clinical data, but there is valuable information in non-clinical data too. The present study combines methods within distributed computing, text retrieval, clustering, and classification into a coherent and computationally efficient system, that can clarify cancer patient trajectories based on non-clinical and freely available information. We produce a fully-functional prototype that can retrieve, cluster and present information about cancer trajectories from non-clinical forum posts. We evaluate three clustering algorithms (MR-DBSCAN, DBSCAN, and HDBSCAN) and compare them in terms of Adjusted Rand Index and total run time as a function of the number of posts retrieved and the neighborhood radius. Clustering results show that neighborhood radius has the most significant impact on c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34433;&#32676;&#20248;&#21270;&#30340;&#39640;&#25928;&#35789;&#24615;&#26631;&#27880;&#26041;&#27861;ACO-tagger&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;96.867&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16760</link><description>&lt;p&gt;
&#20351;&#29992;&#34433;&#32676;&#20248;&#21270;&#30340;&#26032;&#22411;&#35789;&#24615;&#26631;&#27880;&#26041;&#27861;ACO-tagger
&lt;/p&gt;
&lt;p&gt;
ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony Optimization. (arXiv:2303.16760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34433;&#32676;&#20248;&#21270;&#30340;&#39640;&#25928;&#35789;&#24615;&#26631;&#27880;&#26041;&#27861;ACO-tagger&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;96.867&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32676;&#26234;&#33021;&#31639;&#27861;&#22240;&#20854;&#35299;&#20915;&#22797;&#26434;&#21644;&#38750;&#30830;&#23450;&#24615;&#38382;&#39064;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#31639;&#27861;&#21463;&#33258;&#28982;&#29983;&#29289;&#30340;&#38598;&#20307;&#34892;&#20026;&#21551;&#21457;&#65292;&#27169;&#25311;&#36825;&#31181;&#34892;&#20026;&#20197;&#24320;&#21457;&#29992;&#20110;&#35745;&#31639;&#20219;&#21153;&#30340;&#26234;&#33021; agent&#12290;&#20854;&#20013;&#19968;&#31181;&#31639;&#27861;&#26159;&#21463;&#21040;&#34434;&#34433;&#35269;&#39135;&#34892;&#20026;&#21450;&#20854;&#20449;&#24687;&#32032;&#37322;&#25918;&#26426;&#21046;&#21551;&#21457;&#30340;&#34433;&#32676;&#20248;&#21270;&#65288;ACO&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#25955;&#21644;&#32452;&#21512;&#24615;&#30340;&#22256;&#38590;&#38382;&#39064;&#12290;&#35789;&#24615;&#26631;&#27880;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#21477;&#23376;&#20013;&#30340;&#27599;&#20010;&#21333;&#35789;&#20998;&#37197;&#19968;&#20010;&#35789;&#24615;&#35282;&#33394;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ACO&#30340;&#39640;&#24615;&#33021;&#35789;&#24615;&#26631;&#27880;&#26041;&#27861;ACO-tagger&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36798;96.867&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24555;&#36895;&#39640;&#25928;&#65292;&#26159;&#23454;&#38469;&#24212;&#29992;&#30340;&#21487;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarm Intelligence algorithms have gained significant attention in recent years as a means of solving complex and non-deterministic problems. These algorithms are inspired by the collective behavior of natural creatures, and they simulate this behavior to develop intelligent agents for computational tasks. One such algorithm is Ant Colony Optimization (ACO), which is inspired by the foraging behavior of ants and their pheromone laying mechanism. ACO is used for solving difficult problems that are discrete and combinatorial in nature. Part-of-Speech (POS) tagging is a fundamental task in natural language processing that aims to assign a part-of-speech role to each word in a sentence. In this research paper, proposed a high-performance POS-tagging method based on ACO called ACO-tagger. This method achieved a high accuracy rate of 96.867%, outperforming several state-of-the-art methods. The proposed method is fast and efficient, making it a viable option for practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20844;&#20247;&#20154;&#29289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20849;&#20139;&#30340;&#20449;&#24687;&#23545; COVID-19 &#30123;&#24773;&#20013;&#30340;&#20844;&#20247;&#24773;&#24863;&#21644;&#22823;&#20247;&#24847;&#35265;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#26512;&#25512;&#25991;&#65292;&#21457;&#29616;&#20844;&#20247;&#20154;&#29289;&#30340;&#20449;&#24687;&#23545;&#20844;&#20247;&#24773;&#24863;&#21644;&#22823;&#20247;&#24847;&#35265;&#20855;&#26377;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.16759</link><description>&lt;p&gt;
&#25506;&#31350;&#21517;&#20154;&#23545;&#20844;&#20247;&#24577;&#24230;&#24433;&#21709;&#30340;&#30740;&#31350;&#65306;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#24773;&#24863;&#20998;&#26512;&#30340; COVID-19 &#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring celebrity influence on public attitude towards the COVID-19 pandemic: social media shared sentiment analysis. (arXiv:2303.16759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20844;&#20247;&#20154;&#29289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20849;&#20139;&#30340;&#20449;&#24687;&#23545; COVID-19 &#30123;&#24773;&#20013;&#30340;&#20844;&#20247;&#24773;&#24863;&#21644;&#22823;&#20247;&#24847;&#35265;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#26512;&#25512;&#25991;&#65292;&#21457;&#29616;&#20844;&#20247;&#20154;&#29289;&#30340;&#20449;&#24687;&#23545;&#20844;&#20247;&#24773;&#24863;&#21644;&#22823;&#20247;&#24847;&#35265;&#20855;&#26377;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19 &#30123;&#24773;&#20026;&#20581;&#24247;&#27807;&#36890;&#24102;&#26469;&#20102;&#26032;&#26426;&#36935;&#65292;&#22686;&#21152;&#20102;&#20844;&#20247;&#20351;&#29992;&#22312;&#32447;&#28192;&#36947;&#33719;&#21462;&#19982;&#20581;&#24247;&#30456;&#20851;&#24773;&#32490;&#30340;&#26426;&#20250;&#12290;&#20154;&#20204;&#24050;&#32463;&#36716;&#21521;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#20998;&#20139;&#19982; COVID-19 &#30123;&#24773;&#24433;&#21709;&#30456;&#20851;&#30340;&#24773;&#24863;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20844;&#20247;&#20154;&#29289;&#65288;&#21363;&#36816;&#21160;&#21592;&#12289;&#25919;&#27835;&#23478;&#12289;&#26032;&#38395;&#24037;&#20316;&#32773;&#65289;&#20849;&#20139;&#30340;&#31038;&#20132;&#20449;&#24687;&#22312;&#20915;&#23450;&#25972;&#20307;&#20844;&#20849;&#35805;&#35821;&#26041;&#21521;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20174; 2020 &#24180; 1 &#26376; 1 &#26085;&#21040; 2022 &#24180; 3 &#26376; 1 &#26085;&#25910;&#38598;&#20102;&#32422; 1300 &#19975;&#26465;&#25512;&#29305;&#12290;&#20351;&#29992;&#19968;&#20010;&#32463;&#36807;&#35843;&#20248;&#30340; DistilRoBERTa &#27169;&#22411;&#35745;&#31639;&#20102;&#27599;&#26465;&#25512;&#25991;&#30340;&#24773;&#32490;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#27604;&#36739;&#19982;&#20844;&#20247;&#20154;&#29289;&#25552;&#21450;&#21516;&#26102;&#20986;&#29616;&#30340; COVID-19 &#30123;&#33495;&#30456;&#20851;&#25512;&#29305;&#21457;&#24067;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312; COVID-19 &#30123;&#24773;&#30340;&#21069;&#20004;&#24180;&#37324;&#65292;&#19982;&#20844;&#20247;&#20154;&#29289;&#20849;&#20139;&#30340;&#20449;&#24687;&#21516;&#26102;&#20986;&#29616;&#30340;&#24773;&#24863;&#20869;&#23481;&#20855;&#26377;&#19968;&#33268;&#30340;&#27169;&#24335;&#65292;&#24433;&#21709;&#20102;&#20844;&#20247;&#33286;&#35770;&#21644;&#22823;&#20247;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has introduced new opportunities for health communication, including an increase in the public use of online outlets for health-related emotions. People have turned to social media networks to share sentiments related to the impacts of the COVID-19 pandemic. In this paper we examine the role of social messaging shared by Persons in the Public Eye (i.e. athletes, politicians, news personnel) in determining overall public discourse direction. We harvested approximately 13 million tweets ranging from 1 January 2020 to 1 March 2022. The sentiment was calculated for each tweet using a fine-tuned DistilRoBERTa model, which was used to compare COVID-19 vaccine-related Twitter posts (tweets) that co-occurred with mentions of People in the Public Eye. Our findings suggest the presence of consistent patterns of emotional content co-occurring with messaging shared by Persons in the Public Eye for the first two years of the COVID-19 pandemic influenced public opinion and larg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.16755</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#35268;&#27169;&#21270;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#29983;&#25104;&#19981;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#23475;&#30340;&#25991;&#26412;&#25110;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31616;&#21333;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#65288;&#21363;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#20043;&#38388;&#30340;&#27604;&#36739;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#27604;&#36739;&#21453;&#39304;&#21482;&#33021;&#20256;&#36798;&#26377;&#38480;&#30340;&#20851;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65288;ILF&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#12290;ILF&#30001;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#32452;&#25104;&#65306;&#31532;&#19968;&#27493;&#65292;&#26681;&#25454;&#36755;&#20837;&#65292;&#21021;&#22987;LM&#36755;&#20986;&#21644;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#33410;&#20197;&#29983;&#25104;&#25913;&#36827;&#12290;&#31532;&#20108;&#27493;&#65292;&#36873;&#25321;&#26368;&#22810;&#21453;&#39304;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#27493;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#30340;&#25913;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ILF&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#31867;&#20284;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;ILF&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;COVID-19&#12289;&#32954;&#28814;&#21644;&#32467;&#26680;&#30149;&#30340;&#24773;&#20917;&#12290;&#25991;&#31456;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#31867;&#25216;&#26415;&#24182;&#35752;&#35770;&#20102;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#25991;&#31456;&#37325;&#28857;&#32771;&#23519;&#20102;&#36817;&#26399;&#20351;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;COVID-19&#12289;&#32954;&#28814;&#21644;&#32467;&#26680;&#30149;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;CX&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#22312;&#32954;&#37096;&#30142;&#30149;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16754</link><description>&lt;p&gt;
COVID-19&#12289;&#32954;&#28814;&#21644;&#32467;&#26680;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#8212;&#8212;&#22522;&#20110;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#26816;&#27979;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep transfer learning for detecting Covid-19, Pneumonia and Tuberculosis using CXR images -- A Review. (arXiv:2303.16754v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;COVID-19&#12289;&#32954;&#28814;&#21644;&#32467;&#26680;&#30149;&#30340;&#24773;&#20917;&#12290;&#25991;&#31456;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#31867;&#25216;&#26415;&#24182;&#35752;&#35770;&#20102;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#25991;&#31456;&#37325;&#28857;&#32771;&#23519;&#20102;&#36817;&#26399;&#20351;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;COVID-19&#12289;&#32954;&#28814;&#21644;&#32467;&#26680;&#30149;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;CX&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#22312;&#32954;&#37096;&#30142;&#30149;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#20809;&#20173;&#28982;&#26159;&#29992;&#20110;&#35786;&#26029;&#32954;&#37096;&#30142;&#30149;&#30340;&#26368;&#24120;&#35265;&#25104;&#20687;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#23569;&#25968;&#19987;&#23478;&#65288;&#25918;&#23556;&#31185;&#21307;&#24072;&#21644;&#32954;&#31185;&#21307;&#24072;&#65289;&#30340;&#35299;&#35835;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#26469;&#26816;&#27979;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;COVID-19&#12289;&#32954;&#28814;&#21644;&#32467;&#26680;&#30149;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;CX&#22270;&#20687;&#20998;&#31867;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#26412;&#25991;&#20840;&#38754;&#32771;&#23519;&#20102;&#26368;&#36817;&#20351;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;COVID-19&#12289;&#32954;&#28814;&#21644;&#32467;&#26680;&#30149;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#31361;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#26412;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;CX&#22270;&#20687;&#20998;&#31867;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#21450;&#36825;&#20123;&#25216;&#26415;&#22312;&#35786;&#26029;&#21644;&#27835;&#30103;&#32954;&#37096;&#30142;&#30149;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chest X-rays remains to be the most common imaging modality used to diagnose lung diseases. However, they necessitate the interpretation of experts (radiologists and pulmonologists), who are few. This review paper investigates the use of deep transfer learning techniques to detect COVID-19, pneumonia, and tuberculosis in chest X-ray (CXR) images. It provides an overview of current state-of-the-art CXR image classification techniques and discusses the challenges and opportunities in applying transfer learning to this domain. The paper provides a thorough examination of recent research studies that used deep transfer learning algorithms for COVID-19, pneumonia, and tuberculosis detection, highlighting the advantages and disadvantages of these approaches. Finally, the review paper discusses future research directions in the field of deep transfer learning for CXR image classification, as well as the potential for these techniques to aid in the diagnosis and treatment of lung diseases.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23457;&#31295;&#20154;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#31639;&#27861;&#38590;&#20197;&#36827;&#34892;&#21407;&#21017;&#27604;&#36739;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#27492;&#25968;&#25454;&#38598;&#30340;&#31639;&#27861;&#27604;&#36739;&#32467;&#26524;&#65292;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#22312;&#36873;&#25321;&#31639;&#27861;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.16750</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23457;&#31295;&#20154;&#20998;&#37197;&#38382;&#39064;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Gold Standard Dataset for the Reviewer Assignment Problem. (arXiv:2303.16750v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23457;&#31295;&#20154;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#31639;&#27861;&#38590;&#20197;&#36827;&#34892;&#21407;&#21017;&#27604;&#36739;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#27492;&#25968;&#25454;&#38598;&#30340;&#31639;&#27861;&#27604;&#36739;&#32467;&#26524;&#65292;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#22312;&#36873;&#25321;&#31639;&#27861;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#21516;&#34892;&#35780;&#23457;&#26399;&#21002;&#25110;&#20250;&#35758;&#27491;&#22312;&#20351;&#29992;&#25110;&#35797;&#22270;&#20351;&#29992;&#31639;&#27861;&#23558;&#25237;&#31295;&#20998;&#37197;&#32473;&#23457;&#31295;&#20154;&#12290;&#36825;&#20123;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#8220;&#30456;&#20284;&#24230;&#20998;&#25968;&#8221;&#65292;&#21363;&#23545;&#23457;&#31295;&#20154;&#22312;&#23457;&#26597;&#35770;&#25991;&#20013;&#30340;&#19987;&#19994;&#27700;&#24179;&#30340;&#25968;&#20540;&#20272;&#35745;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#26469;&#35745;&#31639;&#36825;&#20123;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#23578;&#26410;&#32463;&#36807;&#26377;&#21407;&#21017;&#30340;&#27604;&#36739;&#65292;&#36825;&#20351;&#24471;&#21033;&#30410;&#30456;&#20851;&#32773;&#38590;&#20197;&#20197;&#22522;&#20110;&#35777;&#25454;&#30340;&#26041;&#24335;&#36873;&#25321;&#31639;&#27861;&#12290;&#27604;&#36739;&#29616;&#26377;&#31639;&#27861;&#21644;&#24320;&#21457;&#26356;&#22909;&#31639;&#27861;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#23558;&#29992;&#20110;&#36827;&#34892;&#21487;&#37325;&#22797;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#26032;&#30340;&#30456;&#20284;&#24230;&#24471;&#20998;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#21457;&#24067;&#32473;&#30740;&#31350;&#31038;&#21306;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30001;58&#20301;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#30340;477&#20010;&#33258;&#25105;&#25253;&#21578;&#30340;&#19987;&#19994;&#27700;&#24179;&#20998;&#25968;&#32452;&#25104;&#65292;&#29992;&#20110;&#35780;&#20272;&#20182;&#20204;&#20808;&#21069;&#38405;&#35835;&#30340;&#35770;&#25991;&#30340;&#23457;&#26597;&#32463;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#27604;&#36739;&#21508;&#31181;&#31639;&#27861;&#65292;&#24182;&#23545;&#26631;&#20934;&#25968;&#25454;&#38598;&#30340;&#35774;&#35745;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the "similarity score"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously.  We use this data to compare s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;ILF&#65292;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16749</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;ILF&#65292;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#28508;&#21147;&#26159;&#26368;&#36817;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Language Feedback&#65288;ILF&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;ILF&#22312;&#35757;&#32451;&#26399;&#38388;&#20165;&#38656;&#35201;&#23569;&#37327;&#30340;&#20154;&#24037;&#32534;&#20889;&#21453;&#39304;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#26102;&#19981;&#38656;&#35201;&#30456;&#21516;&#30340;&#21453;&#39304;&#65292;&#22240;&#27492;&#20351;&#29992;&#36215;&#26469;&#26082;&#26041;&#20415;&#21448;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;ILF&#21487;&#20197;&#34987;&#35270;&#20026;&#26368;&#23567;&#21270;&#19982;&#22522;&#20934;&#20998;&#24067;&#30340;KL&#25955;&#24230;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#24182;&#22312;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;ILF&#22312;Mostly Basic Python Problems(MBPP)&#22522;&#20934;&#27979;&#35797;&#19978;&#23558;Codegen-Mono 6.1B&#27169;&#22411;&#30340;pass @ 1&#35206;&#30422;&#29575;&#30456;&#23545;&#25552;&#39640;&#20102;38%&#65288;&#32477;&#23545;&#25552;&#39640;&#20102;10%&#65289;&#65292;&#32988;&#36807;&#20102;&#22312;MBPP&#19978;&#24494;&#35843;&#21644;&#22312;&#20154;&#31867;&#20462;&#22797;&#30340;&#31243;&#24207;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21322;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#21333;&#20391;&#27491;&#20132;&#32422;&#26463;&#26469;&#32771;&#34385;&#35270;&#22270;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#24352;&#37327;Schatten p-&#33539;&#25968;&#27491;&#21017;&#21270;&#26469;&#34920;&#24449;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#31751;&#32467;&#26500;&#21644;&#21033;&#29992;&#35270;&#22270;&#38388;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.16748</link><description>&lt;p&gt;
&#22522;&#20110;&#21322;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-View Clustering via Semi-non-negative Tensor Factorization. (arXiv:2303.16748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16748
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21322;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#21333;&#20391;&#27491;&#20132;&#32422;&#26463;&#26469;&#32771;&#34385;&#35270;&#22270;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#24352;&#37327;Schatten p-&#33539;&#25968;&#27491;&#21017;&#21270;&#26469;&#34920;&#24449;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#31751;&#32467;&#26500;&#21644;&#21033;&#29992;&#35270;&#22270;&#38388;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#21450;&#20854;&#21464;&#20307;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#35270;&#22270;&#32858;&#31867;(MVC)&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;MVC&#26041;&#27861;&#20998;&#21035;&#23545;&#27599;&#20010;&#35270;&#22270;&#25968;&#25454;&#36827;&#34892;&#30697;&#38453;&#20998;&#35299;&#65292;&#24573;&#30053;&#20102;&#35270;&#22270;&#38388;&#30340;&#24433;&#21709;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#21033;&#29992;&#35270;&#22270;&#20869;&#37096;&#31354;&#38388;&#32467;&#26500;&#21644;&#35270; &#22270;&#38388;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21322;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;(Semi-NTF)&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;Semi-NTF&#30340;&#26032;&#22411;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#20854;&#20013;&#32422;&#26463;&#26465;&#20214;&#20026;&#21333;&#20391;&#27491;&#20132;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30452;&#25509;&#22312;&#31532;3&#38454;&#24352;&#37327;&#19978;&#25191;&#34892;Semi-NTF&#65292;&#35813;&#24352;&#37327;&#30001;&#35270;&#22270;&#30340;&#38170;&#22270;&#32452;&#25104;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#27169;&#22411;&#30452;&#25509;&#32771;&#34385;&#35270;&#22270;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#24352;&#37327;Schatten p-&#33539;&#25968;&#27491;&#21017;&#21270;&#20316;&#20026;&#23545;&#31532;3&#38454;&#24352;&#37327;&#30340;&#31209;&#36817;&#20284;&#65292;&#26469;&#34920;&#24449;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#31751;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#35270;&#22270;&#38388;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering (MVC) based on non-negative matrix factorization (NMF) and its variants have received a huge amount of attention in recent years due to their advantages in clustering interpretability. However, existing NMF-based multi-view clustering methods perform NMF on each view data respectively and ignore the impact of between-view. Thus, they can't well exploit the within-view spatial structure and between-view complementary information. To resolve this issue, we present semi-non-negative tensor factorization (Semi-NTF) and develop a novel multi-view clustering based on Semi-NTF with one-side orthogonal constraint. Our model directly performs Semi-NTF on the 3rd-order tensor which is composed of anchor graphs of views. Thus, our model directly considers the between-view relationship. Moreover, we use the tensor Schatten p-norm regularization as a rank approximation of the 3rd-order tensor which characterizes the cluster structure of multi-view data and exploits the between
&lt;/p&gt;</description></item><item><title>&#31354;&#38388;&#20998;&#23376;&#25104;&#20687;&#26041;&#27861;&#25552;&#20379;&#20102;&#20998;&#23376;&#27169;&#24335;&#24418;&#25104;&#21644;&#32500;&#25252;&#30340;&#26032;&#26426;&#36935;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#20998;&#26512;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24110;&#21161;&#35299;&#24320;&#22797;&#26434;&#29983;&#29289;&#31995;&#32479;&#20013;&#30340;&#20449;&#21495;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16725</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#20013;&#25581;&#31034;&#29983;&#29289;&#23398;&#27934;&#35265;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Uncovering Biological Insights in Spatial Transcriptomics Data. (arXiv:2303.16725v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16725
&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#20998;&#23376;&#25104;&#20687;&#26041;&#27861;&#25552;&#20379;&#20102;&#20998;&#23376;&#27169;&#24335;&#24418;&#25104;&#21644;&#32500;&#25252;&#30340;&#26032;&#26426;&#36935;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#20998;&#26512;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24110;&#21161;&#35299;&#24320;&#22797;&#26434;&#29983;&#29289;&#31995;&#32479;&#20013;&#30340;&#20449;&#21495;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32454;&#32990;&#31995;&#32479;&#30340;&#21457;&#32946;&#21644;&#31283;&#24577;&#37117;&#38656;&#35201;&#23545;&#31354;&#38388;&#20998;&#23376;&#27169;&#24335;&#24418;&#25104;&#21644;&#32500;&#25252;&#36827;&#34892;&#31934;&#32454;&#30340;&#25511;&#21046;&#12290;&#31354;&#38388;&#20998;&#36776;&#29575;&#21644;&#39640;&#36890;&#37327;&#20998;&#23376;&#25104;&#20687;&#26041;&#27861;&#30340;&#36827;&#27493;&#65288;&#22914;&#22810;&#37325;&#20813;&#30123;&#33639;&#20809;&#21644;&#31354;&#38388;&#36716;&#24405;&#32452;&#65288;ST&#65289;&#65289;&#25552;&#20379;&#20102;&#20196;&#20154;&#25391;&#22859;&#30340;&#26032;&#26426;&#36935;&#65292;&#22686;&#24378;&#20102;&#25105;&#20204;&#23545;&#20581;&#24247;&#21644;&#30142;&#30149;&#36807;&#31243;&#20013;&#36825;&#20123;&#36807;&#31243;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#29305;&#21035;&#26159;ST&#20135;&#29983;&#30340;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#24050;&#32463;&#23548;&#33268;&#20102;&#21019;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65288;&#20027;&#35201;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#29616;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#38598;&#25104;&#30340;&#23454;&#39564;&#21644;&#35745;&#31639;&#24037;&#20316;&#27969;&#31243;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#20197;&#20174;&#22797;&#26434;&#30340;&#29983;&#29289;&#31995;&#32479;&#30340;&#22122;&#22768;&#20013;&#35299;&#24320;&#20449;&#21495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20027;&#35201;&#30340;ST&#20998;&#26512;&#30446;&#26631;&#65292;&#36825;&#20123;&#20998;&#26512;&#30446;&#26631;&#20351;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Development and homeostasis in multicellular systems both require exquisite control over spatial molecular pattern formation and maintenance. Advances in spatially-resolved and high-throughput molecular imaging methods such as multiplexed immunofluorescence and spatial transcriptomics (ST) provide exciting new opportunities to augment our fundamental understanding of these processes in health and disease. The large and complex datasets resulting from these techniques, particularly ST, have led to rapid development of innovative machine learning (ML) tools primarily based on deep learning techniques. These ML tools are now increasingly featured in integrated experimental and computational workflows to disentangle signals from noise in complex biological systems. However, it can be difficult to understand and balance the different implicit assumptions and methodologies of a rapidly expanding toolbox of analytical tools in ST. To address this, we summarize major ST analysis goals that ML 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#20013;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#27880; Kullback - Leibler &#25955;&#24230;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39057;&#32321;&#25628;&#32034;&#27491;&#21017;&#21270;&#30340;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.16721</link><description>&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#20877;&#25506;&#65306;Kullback - Leibler &#25955;&#24230;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood method revisited: Gauge symmetry in Kullback -- Leibler divergence and performance-guaranteed regularization. (arXiv:2303.16721v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#20013;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#27880; Kullback - Leibler &#25955;&#24230;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39057;&#32321;&#25628;&#32034;&#27491;&#21017;&#21270;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#26159;&#20272;&#35745;&#25968;&#25454;&#32972;&#21518;&#27010;&#29575;&#30340;&#26368;&#30693;&#21517;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#33719;&#24471;&#19982;&#32463;&#39564;&#20998;&#24067;&#26368;&#25509;&#36817;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#12290;&#28982;&#21518;&#65292;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#36807;&#24230;&#25509;&#36817;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#20294;&#26159;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#30693;&#20043;&#29978;&#23569;&#12290;&#27491;&#21017;&#21270;&#30340;&#24605;&#24819;&#31867;&#20284;&#20110;&#32416;&#38169;&#20195;&#30721;&#65292;&#36890;&#36807;&#23558;&#27425;&#20248;&#35299;&#19982;&#38169;&#35823;&#25509;&#25910;&#21040;&#30340;&#20195;&#30721;&#28151;&#21512;&#65292;&#33719;&#24471;&#26368;&#20248;&#35299;&#30721;&#12290;&#32416;&#38169;&#20195;&#30721;&#20013;&#30340;&#26368;&#20248;&#35299;&#30721;&#26159;&#22522;&#20110;&#35268;&#33539;&#23545;&#31216;&#24615;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#20851;&#27880; Kullback - Leibler &#25955;&#24230;&#20013;&#30340;&#35268;&#33539;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#20013;&#30340;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#27491;&#21017;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26368;&#20248;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#39057;&#32321;&#25628;&#32034;&#27491;&#21017;&#21270;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The maximum likelihood method is the best-known method for estimating the probabilities behind the data. However, the conventional method obtains the probability model closest to the empirical distribution, resulting in overfitting. Then regularization methods prevent the model from being excessively close to the wrong probability, but little is known systematically about their performance. The idea of regularization is similar to error-correcting codes, which obtain optimal decoding by mixing suboptimal solutions with an incorrectly received code. The optimal decoding in error-correcting codes is achieved based on gauge symmetry. We propose a theoretically guaranteed regularization in the maximum likelihood method by focusing on a gauge symmetry in Kullback -- Leibler divergence. In our approach, we obtain the optimal model without the need to search for hyperparameters frequently appearing in regularization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25299;&#25169;&#30340;&#28857;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#25299;&#25169;&#29305;&#24449;&#25551;&#36848;&#28857;&#20113;&#20869;&#30340;&#25968;&#25454;&#28857;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#22270;&#27169;&#22411;&#26041;&#27861;&#26356;&#20855;&#26377;&#20581;&#22766;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.16716</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#30340;&#28857;&#20113;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topological Point Cloud Clustering. (arXiv:2303.16716v1 [math.AT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25299;&#25169;&#30340;&#28857;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#25299;&#25169;&#29305;&#24449;&#25551;&#36848;&#28857;&#20113;&#20869;&#30340;&#25968;&#25454;&#28857;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#22270;&#27169;&#22411;&#26041;&#27861;&#26356;&#20855;&#26377;&#20581;&#22766;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#25299;&#25169;&#28857;&#20113;&#32858;&#31867;&#65288;TPCC&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#28857;&#20113;&#23545;&#20110;&#20840;&#23616;&#25299;&#25169;&#29305;&#24449;&#30340;&#36129;&#29486;&#26469;&#32858;&#31867;&#28857;&#12290;TPCC&#20174;&#35889;&#32858;&#31867;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#32508;&#21512;&#20102;&#26377;&#21033;&#30340;&#29305;&#24449;&#65292;&#22522;&#20110;&#32771;&#34385;&#19982;&#25152;&#32771;&#34385;&#30340;&#28857;&#20113;&#30456;&#20851;&#32852;&#30340;&#19968;&#20010;&#21333;&#24418;&#22797;&#21512;&#20307;&#30340;&#35889;&#29305;&#24615;&#12290;&#30001;&#20110;&#23427;&#22522;&#20110;&#32771;&#34385;&#31232;&#30095;&#29305;&#24449;&#21521;&#37327;&#35745;&#31639;&#65292;TPCC&#21516;&#26679;&#23481;&#26131;&#35299;&#37322;&#21644;&#23454;&#29616;&#65292;&#23601;&#20687;&#35889;&#32858;&#31867;&#19968;&#26679;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19981;&#20165;&#20851;&#27880;&#19982;&#20174;&#28857;&#20113;&#25968;&#25454;&#21019;&#24314;&#30340;&#22270;&#30456;&#20851;&#32852;&#30340;&#21333;&#20010;&#30697;&#38453;&#65292;&#32780;&#26159;&#20851;&#27880;&#19982;&#24688;&#24403;&#26500;&#36896;&#30340;&#21333;&#24418;&#22797;&#21512;&#20307;&#30456;&#20851;&#32852;&#30340;&#25972;&#20010;Hodge-Laplacian&#30340;&#19968;&#25972;&#22871;&#30697;&#38453;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#25551;&#36848;&#28857;&#20113;&#20869;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21463;&#30410;&#20110;&#25299;&#25169;&#25216;&#26415;&#30456;&#23545;&#20110;&#22122;&#22768;&#30340;&#30456;&#23545;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;TPCC&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Topological Point Cloud Clustering (TPCC), a new method to cluster points in an arbitrary point cloud based on their contribution to global topological features. TPCC synthesizes desirable features from spectral clustering and topological data analysis and is based on considering the spectral properties of a simplicial complex associated to the considered point cloud. As it is based on considering sparse eigenvector computations, TPCC is similarly easy to interpret and implement as spectral clustering. However, by focusing not just on a single matrix associated to a graph created from the point cloud data, but on a whole set of Hodge-Laplacians associated to an appropriately constructed simplicial complex, we can leverage a far richer set of topological features to characterize the data points within the point cloud and benefit from the relative robustness of topological techniques against noise. We test the performance of TPCC on both synthetic and real-world data and compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#26032;&#26041;&#27861;TraVaG&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21644;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#26032;&#21464;&#20307;&#65292;&#21516;&#26102;&#25552;&#20379;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#22312;&#20986;&#29616;&#39640;&#27604;&#20363;&#31232;&#26377;&#21464;&#20307;&#26102;&#25552;&#20379;&#24037;&#19994;&#35268;&#27169;&#30340;&#22909;&#22788;&#21644;&#25552;&#39640;&#38544;&#31169;&#20445;&#35777;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.16704</link><description>&lt;p&gt;
&#20351;&#29992;GAN&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#26085;&#24535;&#21464;&#20307;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TraVaG: Differentially Private Trace Variant Generation Using GANs. (arXiv:2303.16704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#26032;&#26041;&#27861;TraVaG&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21644;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#26032;&#21464;&#20307;&#65292;&#21516;&#26102;&#25552;&#20379;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#22312;&#20986;&#29616;&#39640;&#27604;&#20363;&#31232;&#26377;&#21464;&#20307;&#26102;&#25552;&#20379;&#24037;&#19994;&#35268;&#27169;&#30340;&#22909;&#22788;&#21644;&#25552;&#39640;&#38544;&#31169;&#20445;&#35777;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#25366;&#25496;&#22312;&#24037;&#19994;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#65292;&#28982;&#32780;&#20107;&#20214;&#25968;&#25454;&#20013;&#21253;&#25324;&#30340;&#25935;&#24863;&#21644;&#31169;&#20154;&#20449;&#24687;&#30340;&#38544;&#31169;&#38382;&#39064;&#20063;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20026;&#20027;&#35201;&#27969;&#31243;&#25366;&#25496;&#25216;&#26415;&#65292;&#22914;&#27969;&#31243;&#21457;&#29616;&#65292;&#25552;&#20379;&#24046;&#20998;&#38544;&#31169;&#31561;&#38544;&#31169;&#20445;&#35777;&#30340;&#21464;&#20307;&#12290;&#28982;&#32780;&#65292;&#37322;&#25918;&#21464;&#20307;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20173;&#26410;&#28385;&#36275;&#24037;&#19994;&#35268;&#27169;&#20351;&#29992;&#30340;&#25152;&#26377;&#35201;&#27714;&#65292;&#24182;&#19988;&#22312;&#20986;&#29616;&#39640;&#39057;&#31232;&#26377;&#21464;&#20307;&#26102;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#30340;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26032;&#26041;&#27861;TraVaG&#65292;&#29992;&#20110;&#21457;&#24067;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#19981;&#21516;&#21464;&#20307;&#65292;&#24182;&#22312;&#20986;&#29616;&#39640;&#27604;&#20363;&#31232;&#26377;&#21464;&#20307;&#26102;&#25552;&#20379;&#24037;&#19994;&#35268;&#27169;&#30340;&#22909;&#22788;&#21644;&#25552;&#39640;&#38544;&#31169;&#20445;&#35777;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;TraVaG&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21644;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#26032;&#21464;&#20307;&#26469;&#20811;&#26381;&#20197;&#24448;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Process mining is rapidly growing in the industry. Consequently, privacy concerns regarding sensitive and private information included in event data, used by process mining algorithms, are becoming increasingly relevant. State-of-the-art research mainly focuses on providing privacy guarantees, e.g., differential privacy, for trace variants that are used by the main process mining techniques, e.g., process discovery. However, privacy preservation techniques for releasing trace variants still do not fulfill all the requirements of industry-scale usage. Moreover, providing privacy guarantees when there exists a high rate of infrequent trace variants is still a challenge. In this paper, we introduce TraVaG as a new approach for releasing differentially private trace variants based on \text{Generative Adversarial Networks} (GANs) that provides industry-scale benefits and enhances the level of privacy guarantees when there exists a high ratio of infrequent variants. Moreover, TraVaG overcome
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#30340;&#23616;&#37096;&#32447;&#24615;&#21270;&#27010;&#29575;&#36870;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#29305;&#24449;&#21270;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16698</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#32447;&#24615;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#30340;&#23616;&#37096;&#32447;&#24615;&#21270;&#27010;&#29575;&#36870;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Probabilistic inverse optimal control with local linearization for non-linear partially observable systems. (arXiv:2303.16698v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#30340;&#23616;&#37096;&#32447;&#24615;&#21270;&#27010;&#29575;&#36870;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#29305;&#24449;&#21270;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#29305;&#24449;&#21270;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#35201;&#27714;&#24050;&#30693;&#25511;&#21046;&#20449;&#21495;&#65292;&#25110;&#32773;&#20165;&#38480;&#20110;&#23436;&#20840;&#21487;&#35266;&#27979;&#25110;&#32447;&#24615;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#29575;&#36870;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#38543;&#26426;&#31995;&#32479;&#30340;&#20002;&#22833;&#25511;&#21046;&#20449;&#21495;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#35813;&#26041;&#27861;&#32479;&#19968;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#30340;&#24863;&#35273;&#21644;&#25511;&#21046;&#31995;&#32479;&#30340;&#22122;&#22768;&#29305;&#24449;&#30340;&#26174;&#24335;&#27169;&#22411;&#20197;&#21450;&#23616;&#37096;&#32447;&#24615;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#36817;&#20284;&#20284;&#28982;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#27491;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#29256;&#26412;&#30340;&#32463;&#20856;&#25511;&#21046;&#20219;&#21153;&#65292;&#23548;&#33322;&#20219;&#21153;&#21644;&#25163;&#21160;&#36798;&#21040;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#21487;&#29992;&#20110;&#27169;&#20223;&#23398;&#20064;&#21040;&#24863;&#35273;&#36816;&#21160;&#31070;&#32463;&#31185;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse optimal control methods can be used to characterize behavior in sequential decision-making tasks. Most existing work, however, requires the control signals to be known, or is limited to fully-observable or linear systems. This paper introduces a probabilistic approach to inverse optimal control for stochastic non-linear systems with missing control signals and partial observability that unifies existing approaches. By using an explicit model of the noise characteristics of the sensory and control systems of the agent in conjunction with local linearization techniques, we derive an approximate likelihood for the model parameters, which can be computed within a single forward pass. We evaluate our proposed method on stochastic and partially observable version of classic control tasks, a navigation task, and a manual reaching task. The proposed method has broad applicability, ranging from imitation learning to sensorimotor neuroscience.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25104;&#21151;&#35299;&#20915;&#20102;&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#65292;&#39318;&#27425;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#35813;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.16686</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Communication Load Balancing via Efficient Inverse Reinforcement Learning. (arXiv:2303.16686v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25104;&#21151;&#35299;&#20915;&#20102;&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#65292;&#39318;&#27425;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#35813;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;&#26088;&#22312;&#24179;&#34913;&#19981;&#21516;&#21487;&#29992;&#36164;&#28304;&#20043;&#38388;&#30340;&#36127;&#36733;&#65292;&#20174;&#32780;&#25552;&#39640;&#32593;&#32476;&#31995;&#32479;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#26412;&#25991;&#23558;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#25551;&#36848;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26377;&#25928;&#35299;&#20915;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#21033;&#29992;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#38656;&#35201;&#26126;&#30830;&#30340;&#22870;&#21169;&#23450;&#20041;&#12290;&#26500;&#24314;&#36825;&#31181;&#22870;&#21169;&#20989;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#19987;&#23478;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#32780;&#19988;&#23545;&#20110;&#26368;&#20248;&#22870;&#21169;&#20989;&#25968;&#30340;&#24418;&#24335;&#32570;&#20047;&#20849;&#35782;&#12290;&#26412;&#25991;&#37319;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#19968;&#32452;&#28436;&#31034;&#20013;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#22312;&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;&#39046;&#22495;&#25104;&#21151;&#24212;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#19968;&#32452;&#28436;&#31034;&#20013;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#65292;&#28982;&#21518;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#36127;&#36733;&#22343;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication load balancing aims to balance the load between different available resources, and thus improve the quality of service for network systems. After formulating the load balancing (LB) as a Markov decision process problem, reinforcement learning (RL) has recently proven effective in addressing the LB problem. To leverage the benefits of classical RL for load balancing, however, we need an explicit reward definition. Engineering this reward function is challenging, because it involves the need for expert knowledge and there lacks a general consensus on the form of an optimal reward function. In this work, we tackle the communication load balancing problem from an inverse reinforcement learning (IRL) approach. To the best of our knowledge, this is the first time IRL has been successfully applied in the field of communication load balancing. Specifically, first, we infer a reward function from a set of demonstrations, and then learn a reinforcement learning load balancing polic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#37325;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#26368;&#36866;&#21512;&#25191;&#34892;&#30340;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#36890;&#20449;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#65292;&#20854;&#26681;&#25454;&#19981;&#21516;&#27969;&#37327;&#22330;&#26223;&#19979;&#30340;&#31574;&#30053;&#35757;&#32451;&#24211;&#36827;&#34892;&#36873;&#25321;&#65292;&#21487;&#20197;&#32988;&#36807;&#20256;&#32479;&#30340;&#36127;&#36733;&#24179;&#34913;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16685</link><description>&lt;p&gt;
&#26410;&#30693;&#27969;&#37327;&#22330;&#26223;&#19979;&#30340;&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;&#20013;&#30340;&#31574;&#30053;&#37325;&#29992;
&lt;/p&gt;
&lt;p&gt;
Policy Reuse for Communication Load Balancing in Unseen Traffic Scenarios. (arXiv:2303.16685v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16685
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#37325;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#26368;&#36866;&#21512;&#25191;&#34892;&#30340;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#36890;&#20449;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#65292;&#20854;&#26681;&#25454;&#19981;&#21516;&#27969;&#37327;&#22330;&#26223;&#19979;&#30340;&#31574;&#30053;&#35757;&#32451;&#24211;&#36827;&#34892;&#36873;&#25321;&#65292;&#21487;&#20197;&#32988;&#36807;&#20256;&#32479;&#30340;&#36127;&#36733;&#24179;&#34913;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#20449;&#32593;&#32476;&#22797;&#26434;&#24615;&#21644;&#27969;&#37327;&#22686;&#38271;&#30340;&#25345;&#32493;&#22686;&#21152;&#65292;&#36890;&#20449;&#36127;&#36733;&#24179;&#34913;&#35299;&#20915;&#26041;&#26696;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;RL&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#23545;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#36935;&#21040;&#30340;&#22330;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#37325;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#31574;&#30053;&#36873;&#25321;&#22120;&#22522;&#20110;&#24403;&#21069;&#30340;&#27969;&#37327;&#29366;&#20917;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;RL&#31574;&#30053;&#26469;&#25191;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#31574;&#30053;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#22312;&#19981;&#21516;&#27969;&#37327;&#22330;&#26223;&#19979;&#35757;&#32451;&#30340;&#31574;&#30053;&#12290;&#22312;&#37096;&#32626;&#21040;&#26410;&#30693;&#30340;&#27969;&#37327;&#22330;&#26223;&#26102;&#65292;&#25105;&#20204;&#26681;&#25454;&#24403;&#21069;&#22330;&#26223;&#30340;&#21069;&#19968;&#22825;&#30340;&#27969;&#37327;&#19982;&#35757;&#32451;&#26399;&#38388;&#35266;&#23519;&#21040;&#30340;&#27969;&#37327;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#20174;&#31574;&#30053;&#24211;&#20013;&#36873;&#25321;&#31574;&#30053;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#32988;&#36807;&#20256;&#32479;&#30340;&#36127;&#36733;&#24179;&#34913;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous growth in communication network complexity and traffic volume, communication load balancing solutions are receiving increasing attention. Specifically, reinforcement learning (RL)-based methods have shown impressive performance compared with traditional rule-based methods. However, standard RL methods generally require an enormous amount of data to train, and generalize poorly to scenarios that are not encountered during training. We propose a policy reuse framework in which a policy selector chooses the most suitable pre-trained RL policy to execute based on the current traffic condition. Our method hinges on a policy bank composed of policies trained on a diverse set of traffic scenarios. When deploying to an unknown traffic scenario, we select a policy from the policy bank based on the similarity between the previous-day traffic of the current scenario and the traffic observed during training. Experiments demonstrate that this framework can outperform classical a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#31070;&#32463;&#33539;&#24335;&#27169;&#22359;&#25193;&#23637;&#21040;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#23454;&#38469;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20197;&#21450;&#24378;&#21046;&#23454;&#29616;&#31526;&#21495;&#20114;&#26021;&#23646;&#24615;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16674</link><description>&lt;p&gt;
&#23454;&#38469;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic Rule Learning in Real-world Classification Tasks. (arXiv:2303.16674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#31070;&#32463;&#33539;&#24335;&#27169;&#22359;&#25193;&#23637;&#21040;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#23454;&#38469;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20197;&#21450;&#24378;&#21046;&#23454;&#29616;&#31526;&#21495;&#20114;&#26021;&#23646;&#24615;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;&#22240;&#20854;&#27604;&#32431;&#31070;&#32463;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#19988;&#27604;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;&#25193;&#23637;&#24615;&#26356;&#22909;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; pix2rule &#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#33539;&#24335;&#27169;&#22359;&#26469;&#20351;&#29992;&#21069;&#21521;&#23618;&#23398;&#20064;&#31526;&#21495;&#35268;&#21017;&#12290;&#23613;&#31649;&#22312;&#21512;&#25104;&#20108;&#20998;&#31867;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294; pix2rule &#23578;&#26410;&#24212;&#29992;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22914;&#22522;&#20110;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#26631;&#31614;&#21644;&#22810;&#31867;&#20998;&#31867;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31070;&#32463;&#33539;&#24335;&#27169;&#22359;&#25193;&#23637;&#21040;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65306;(i) &#25903;&#25345;&#23454;&#38469;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#30340;&#35268;&#21017;&#23398;&#20064;&#65292;(ii) &#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#24378;&#21046;&#31526;&#21495;&#20114;&#26021;&#23646;&#24615;(&#21363;&#39044;&#27979;&#24688;&#22909;&#19968;&#20010;&#31867;)&#65292;&#20197;&#21450;(iii) &#25506;&#32034;&#20854;&#22312;&#22823;&#36755;&#20837;&#36755;&#20986;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#20110; pix2rule &#30340;&#31070;&#32463; DNF &#27169;&#22359;&#35757;&#32451;&#20102;&#19968;&#20010;&#26222;&#36890;&#30340;&#31070;&#32463; DNF &#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36890;&#36807;&#25913;&#36827;&#29616;&#26377;&#30340;&#31070;&#32463; DNF &#27169;&#22359;&#23454;&#29616;&#22810;&#31867;&#20998;&#31867;&#20013;&#30340;&#20114;&#26021;&#24615;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25193;&#23637;&#30340;&#31070;&#32463; DNF &#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#32431;&#31070;&#32463;&#27169;&#22411;&#21644;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic rule learning has attracted lots of attention as it offers better interpretability than pure neural models and scales better than symbolic rule learning. A recent approach named pix2rule proposes a neural Disjunctive Normal Form (neural DNF) module to learn symbolic rules with feed-forward layers. Although proved to be effective in synthetic binary classification, pix2rule has not been applied to more challenging tasks such as multi-label and multi-class classifications over real-world data. In this paper, we address this limitation by extending the neural DNF module to (i) support rule learning in real-world multi-class and multi-label classification tasks, (ii) enforce the symbolic property of mutual exclusivity (i.e. predicting exactly one class) in multi-class classification, and (iii) explore its scalability over large inputs and outputs. We train a vanilla neural DNF model similar to pix2rule's neural DNF module for multi-label classification, and we propose a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2303.16668</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#25308;&#21344;&#24237;&#23481;&#38169;&#32858;&#21512;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates. (arXiv:2303.16668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#12290;FLANDERS&#23558;&#27599;&#20010;FL&#36718;&#27425;&#20013;&#30001;&#23458;&#25143;&#31471;&#21457;&#36865;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#35270;&#20026;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#35266;&#27979;&#19982;&#30001;&#30697;&#38453;&#33258;&#22238;&#24402;&#39044;&#27979;&#27169;&#22411;&#20272;&#35745;&#30340;&#35266;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#20316;&#20026;&#36825;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#20540;&#12290;&#22312;&#19981;&#21516;FL&#35774;&#32622;&#19979;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FLANDERS&#22312;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#26041;&#38754;&#19982;&#26368;&#24378;&#22823;&#30340;&#22522;&#32447;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#30456;&#27604;&#65292; FLANDERS&#21363;&#20351;&#22312;&#26497;&#20854;&#20005;&#37325;&#30340;&#25915;&#20987;&#22330;&#26223;&#19979;&#20173;&#28982;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose FLANDERS, a novel federated learning (FL) aggregation scheme robust to Byzantine attacks. FLANDERS considers the local model updates sent by clients at each FL round as a matrix-valued time series. Then, it identifies malicious clients as outliers of this time series by comparing actual observations with those estimated by a matrix autoregressive forecasting model. Experiments conducted on several datasets under different FL settings demonstrate that FLANDERS matches the robustness of the most powerful baselines against Byzantine clients. Furthermore, FLANDERS remains highly effective even under extremely severe attack scenarios, as opposed to existing defense strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#20102;&#19968;&#20010;&#22240;&#26524;&#12289;&#26102;&#19981;&#21464;&#19988;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#31995;&#32479;&#30340;&#27969;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26550;&#26500;&#33021;&#22815;&#36817;&#20284;&#34920;&#31034;&#27969;&#20989;&#25968;&#65292;&#22312;&#23454;&#39564;&#20013;&#24212;&#29992;&#20110;Van der Pol&#21644;FitzHugh Nagumo&#25391;&#33633;&#22120;&#30340;&#36712;&#36857;&#37325;&#29616;&#21644;&#26368;&#20248;&#25511;&#21046;&#36755;&#20837;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2303.16656</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27969;&#20989;&#25968;&#21450;&#20854;&#22312;&#38750;&#32447;&#24615;&#25391;&#33633;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning Flow Functions from Data with Applications to Nonlinear Oscillators. (arXiv:2303.16656v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#20102;&#19968;&#20010;&#22240;&#26524;&#12289;&#26102;&#19981;&#21464;&#19988;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#31995;&#32479;&#30340;&#27969;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26550;&#26500;&#33021;&#22815;&#36817;&#20284;&#34920;&#31034;&#27969;&#20989;&#25968;&#65292;&#22312;&#23454;&#39564;&#20013;&#24212;&#29992;&#20110;Van der Pol&#21644;FitzHugh Nagumo&#25391;&#33633;&#22120;&#30340;&#36712;&#36857;&#37325;&#29616;&#21644;&#26368;&#20248;&#25511;&#21046;&#36755;&#20837;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#26550;&#26500;&#26469;&#20174;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#12289;&#26102;&#19981;&#21464;&#19988;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#31995;&#32479;&#30340;&#27969;&#20989;&#25968;&#12290;&#36890;&#36807;&#23558;&#25511;&#21046;&#36755;&#20837;&#30340;&#31867;&#21035;&#38480;&#21046;&#20026;&#20998;&#27573;&#24120;&#25968;&#20989;&#25968;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#27969;&#20989;&#25968;&#31561;&#20215;&#20110;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#31995;&#32479;&#30340;&#29366;&#24577;&#26144;&#23556;&#30340;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#19968;&#20010;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#26500;&#25104;&#30340;RNN&#65292;&#20197;&#23558;&#31995;&#32479;&#29366;&#24577;&#26144;&#23556;&#21040;RNN&#30340;&#38544;&#34255;&#29366;&#24577;&#20197;&#21450;&#23558;&#38544;&#34255;&#29366;&#24577;&#26144;&#23556;&#22238;&#31995;&#32479;&#29366;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#22240;&#26524;&#24615;&#21644;&#26102;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26550;&#26500;&#33021;&#22815;&#36817;&#20284;&#22320;&#34920;&#31034;&#27969;&#20989;&#25968;&#65292;&#19988;&#21487;&#20197;&#22312;&#20219;&#24847;&#26102;&#38388;&#26597;&#35810;&#23398;&#20064;&#21040;&#30340;&#27969;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;Van der Pol&#21644;FitzHugh Nagumo&#25391;&#33633;&#22120;&#30340;&#27169;&#22411;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#22320;&#37325;&#29616;&#36825;&#20004;&#20010;&#31995;&#32479;&#30340;&#36712;&#36857;&#12290;&#23545;&#20110;Van der Pol&#25391;&#33633;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26550;&#26500;&#22914;&#20309;&#29992;&#20110;&#35745;&#31639;&#23558;&#31995;&#32479;&#24341;&#23548;&#21040;&#25152;&#38656;&#36712;&#36857;&#30340;&#26368;&#20248;&#25511;&#21046;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a recurrent neural network (RNN) based architecture to learn the flow function of a causal, time-invariant and continuous-time control system from trajectory data. By restricting the class of control inputs to piecewise constant functions, we show that learning the flow function is equivalent to learning the input-to-state map of a discrete-time dynamical system. This motivates the use of an RNN together with encoder and decoder networks which map the state of the system to the hidden state of the RNN and back. We show that the proposed architecture is able to approximate the flow function by exploiting the system's causality and time-invariance. The output of the learned flow function model can be queried at any time instant. We experimentally validate the proposed method using models of the Van der Pol and FitzHugh Nagumo oscillators. In both cases, the results demonstrate that the architecture is able to closely reproduce the trajectories of these two systems. For the Va
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GUT&#30340;&#26032;&#22411;&#20998;&#23618;&#24335;&#21338;&#24328;&#35770;&#23454;&#29992;&#26641;&#27169;&#22411;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#20013;&#23454;&#29616;&#39640;&#20302;&#23618;&#27425;&#31574;&#30053;&#20998;&#35299;&#65292;&#20351;&#29992;&#20195;&#29702;&#38656;&#27714;&#20316;&#20026;&#26032;&#30340;&#25910;&#30410;&#24230;&#37327;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#21487;&#20197;&#24110;&#21161;MAS&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#33719;&#32988;&#29575;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.16641</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#21338;&#24328;&#35770;&#20915;&#31574;&#30340;&#20998;&#23618;&#24335;&#23454;&#29992;&#26641;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Game-Theoretic Decision-Making for Cooperative Multi-Agent Systems Under the Presence of Adversarial Agents. (arXiv:2303.16641v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GUT&#30340;&#26032;&#22411;&#20998;&#23618;&#24335;&#21338;&#24328;&#35770;&#23454;&#29992;&#26641;&#27169;&#22411;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#20013;&#23454;&#29616;&#39640;&#20302;&#23618;&#27425;&#31574;&#30053;&#20998;&#35299;&#65292;&#20351;&#29992;&#20195;&#29702;&#38656;&#27714;&#20316;&#20026;&#26032;&#30340;&#25910;&#30410;&#24230;&#37327;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#21487;&#20197;&#24110;&#21161;MAS&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#33719;&#32988;&#29575;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21361;&#38505;&#24773;&#22659;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#20013;&#65292;&#22522;&#30784;&#30340;&#20851;&#31995;&#21487;&#20197;&#34987;&#34920;&#31034;&#25104;&#21338;&#24328;&#35770;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#31216;&#20026;&#21338;&#24328;&#35770;&#23454;&#29992;&#26641;&#65288;GUT&#65289;&#65292;&#23427;&#23558;&#39640;&#23618;&#27425;&#31574;&#30053;&#20998;&#35299;&#20026;&#21487;&#25191;&#34892;&#30340;&#20302;&#23618;&#27425;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#21512;&#20316;MAS&#20915;&#31574;&#12290;&#23427;&#32467;&#21512;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#38656;&#27714;&#30340;&#23454;&#26102;&#31574;&#30053;&#28216;&#25103;&#30340;&#26032;&#25910;&#30410;&#24230;&#37327;&#12290;&#25105;&#20204;&#22312;Explore&#28216;&#25103;&#39046;&#22495;&#20013;&#65292;&#20174;&#24179;&#34913;&#25104;&#21151;&#27010;&#29575;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35282;&#24230;&#34913;&#37327;&#20102;MAS&#30340;&#32489;&#25928;&#65292;&#35780;&#20272;&#20102;GUT&#26041;&#27861;&#30456;&#23545;&#20110;&#36138;&#24515;&#22320;&#20381;&#36182;&#20110;&#32452;&#21512;&#21160;&#20316;&#22870;&#21169;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#27169;&#25311;&#30340;&#30830;&#23450;&#32467;&#26524;&#34920;&#26126;GUT&#21487;&#20197;&#32452;&#32455;&#26356;&#22797;&#26434;&#30340;MAS&#21327;&#20316;&#20851;&#31995;&#65292;&#24110;&#21161;&#22242;&#38431;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#33719;&#32988;&#29575;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#27169;&#25311;&#24212;&#29992;GUT&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Underlying relationships among Multi-Agent Systems (MAS) in hazardous scenarios can be represented as Game-theoretic models. This paper proposes a new hierarchical network-based model called Game-theoretic Utility Tree (GUT), which decomposes high-level strategies into executable low-level actions for cooperative MAS decisions. It combines with a new payoff measure based on agent needs for real-time strategy games. We present an Explore game domain, where we measure the performance of MAS achieving tasks from the perspective of balancing the success probability and system costs. We evaluate the GUT approach against state-of-the-art methods that greedily rely on rewards of the composite actions. Conclusive results on extensive numerical simulations indicate that GUT can organize more complex relationships among MAS cooperation, helping the group achieve challenging tasks with lower costs and higher winning rates. Furthermore, we demonstrated the applicability of the GUT using the simula
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26816;&#27979;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#34987;&#38024;&#23545;&#24615;&#25915;&#20987;&#65292;&#22240;&#27492;&#20445;&#25252;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#32467;&#26524;&#23545;&#29616;&#20195;&#30005;&#21147;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2303.16633</link><description>&lt;p&gt;
&#38024;&#23545;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340;&#26377;&#30446;&#26631;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Targeted Adversarial Attacks on Wind Power Forecasts. (arXiv:2303.16633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#27979;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#34987;&#38024;&#23545;&#24615;&#25915;&#20987;&#65292;&#22240;&#27492;&#20445;&#25252;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#32467;&#26524;&#23545;&#29616;&#20195;&#30005;&#21147;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22810;&#31181;&#29992;&#20110;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25110;&#29289;&#29702;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#39118;&#30005;&#22330;&#25110;&#25972;&#20010;&#22320;&#21306;&#30340;&#39118;&#21147;&#21457;&#30005;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24120;&#24120;&#20250;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#23545;&#29616;&#20195;&#30005;&#21147;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#20445;&#25252;&#39044;&#27979;&#32467;&#26524;&#20813;&#21463;&#36825;&#31181;&#23041;&#32961;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#39044;&#27979;&#27169;&#22411;&#23545;&#26377;&#30446;&#26631;&#12289;&#21322;&#26377;&#30446;&#26631;&#21644;&#26080;&#30446;&#26631;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#39118;&#30005;&#22330;&#21151;&#29575;&#21457;&#30005;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#21644;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#24503;&#22269;&#25972;&#20010;&#22320;&#21306;&#30340;&#39118;&#21147;&#21457;&#30005;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24635;&#23545;&#25239;&#40065;&#26834;&#24615;&#35780;&#20998;&#65288;TARS&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#22238;&#24402;&#27169;&#22411;&#31283;&#20581;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, researchers proposed a variety of deep learning models for wind power forecasting. These models predict the wind power generation of wind farms or entire regions more accurately than traditional machine learning algorithms or physical models. However, latest research has shown that deep learning models can often be manipulated by adversarial attacks. Since wind power forecasts are essential for the stability of modern power systems, it is important to protect them from this threat. In this work, we investigate the vulnerability of two different forecasting models to targeted, semitargeted, and untargeted adversarial attacks. We consider a Long Short-Term Memory (LSTM) network for predicting the power generation of a wind farm and a Convolutional Neural Network (CNN) for forecasting the wind power generation throughout Germany. Moreover, we propose the Total Adversarial Robustness Score (TARS), an evaluation metric for quantifying the robustness of regression models to 
&lt;/p&gt;</description></item><item><title>Fairlearn&#26159;&#19968;&#20010;&#26377;&#21161;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20844;&#24179;&#24615;&#30340;&#24320;&#28304;&#39033;&#30446;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#22320;&#35780;&#20272;&#21463;&#24433;&#21709;&#20154;&#32676;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#25552;&#20379;&#22810;&#31181;&#31639;&#27861;&#26469;&#32531;&#35299;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#22312;&#32771;&#34385;&#31995;&#32479;&#30340;&#26356;&#24191;&#27867;&#31038;&#20250;&#32972;&#26223;&#26041;&#38754;&#25552;&#20379;&#20102;&#23398;&#20064;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.16626</link><description>&lt;p&gt;
Fairlearn: &#35780;&#20272;&#21644;&#25552;&#21319;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairlearn: Assessing and Improving Fairness of AI Systems. (arXiv:2303.16626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16626
&lt;/p&gt;
&lt;p&gt;
Fairlearn&#26159;&#19968;&#20010;&#26377;&#21161;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20844;&#24179;&#24615;&#30340;&#24320;&#28304;&#39033;&#30446;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#22320;&#35780;&#20272;&#21463;&#24433;&#21709;&#20154;&#32676;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#25552;&#20379;&#22810;&#31181;&#31639;&#27861;&#26469;&#32531;&#35299;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#22312;&#32771;&#34385;&#31995;&#32479;&#30340;&#26356;&#24191;&#27867;&#31038;&#20250;&#32972;&#26223;&#26041;&#38754;&#25552;&#20379;&#20102;&#23398;&#20064;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Fairlearn&#26159;&#19968;&#20010;&#24320;&#28304;&#39033;&#30446;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#35780;&#20272;&#21644;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#35813;&#30456;&#20851;&#30340;Python&#24211;&#65292;&#20063;&#21517;&#20026;Fairlearn&#65292;&#25903;&#25345;&#36328;&#21463;&#24433;&#21709;&#30340;&#20154;&#21475;&#35780;&#20272;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#21253;&#25324;&#20960;&#31181;&#31639;&#27861;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#22522;&#20110;&#20844;&#24179;&#26159;&#19968;&#20010;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#30340;&#35748;&#35782;&#65292;&#35813;&#39033;&#30446;&#38598;&#25104;&#20102;&#23398;&#20064;&#36164;&#28304;&#65292;&#20197;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#32771;&#34385;&#19968;&#20010;&#31995;&#32479;&#30340;&#26356;&#24191;&#27867;&#31038;&#20250;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairlearn is an open source project to help practitioners assess and improve fairness of artificial intelligence (AI) systems. The associated Python library, also named fairlearn, supports evaluation of a model's output across affected populations and includes several algorithms for mitigating fairness issues. Grounded in the understanding that fairness is a sociotechnical challenge, the project integrates learning resources that aid practitioners in considering a system's broader societal context.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.16618</link><description>&lt;p&gt;
&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#20026;&#23545;&#35805;&#25935;&#24863;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#29305;&#23450;&#29305;&#24449;&#30340;&#20154;&#21592;&#21644;/&#25110;&#29305;&#23450;&#29615;&#22659;&#20013;&#30340;&#35828;&#35805;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#30340;&#35282;&#33394;&#27880;&#37322;&#38590;&#20197;&#24471;&#21040;&#21644;&#25104;&#21151;&#21033;&#29992;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#24182;&#25551;&#36848;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#25163;&#21160;&#27880;&#37322;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27969;&#34892;&#30340; Cornell &#30005;&#24433;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340; 863 &#21517;&#28436;&#35762;&#32773;&#65292;&#21253;&#25324;&#29305;&#24449;&#24341;&#29992;&#21644;&#35282;&#33394;&#25551;&#36848;&#65292;&#20197;&#21450;&#36229;&#36807; 95&#65285; &#30340;&#29305;&#33394;&#30005;&#24433;&#30340;&#19968;&#32452;&#33258;&#21160;&#25552;&#21462;&#30340;&#20845;&#20010;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#27492;&#31867;&#27880;&#37322;&#26469;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22256;&#24785;&#20943;&#23569;&#39640;&#36798; 8.5&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35762;&#32773;&#65292;&#21363;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#22521;&#35757;&#25968;&#25454;&#30340;&#28436;&#35762;&#32773;&#65292;&#20381;&#36182;&#20110;&#35282;&#33394;&#30340;&#20154;&#21475;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#25910;&#38598;&#27492;&#31867;&#20803;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#39033;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#65292;&#20197;&#31361;&#20986;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#21644;&#21452;&#21521;&#35757;&#32451;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#20462;&#25913;&#25991;&#26412;&#23384;&#22312;&#22122;&#22768;&#25110;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.16604</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#21644;&#21452;&#21521;&#35757;&#32451;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bi-directional Training for Composed Image Retrieval via Text Prompt Learning. (arXiv:2303.16604v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#21644;&#21452;&#21521;&#35757;&#32451;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#20462;&#25913;&#25991;&#26412;&#23384;&#22312;&#22122;&#22768;&#25110;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26159;&#26681;&#25454;&#21253;&#21547;&#21442;&#32771;&#22270;&#20687;&#21644;&#25551;&#36848;&#25152;&#38656;&#26356;&#25913;&#30340;&#20462;&#25913;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#29992;&#25143;&#26597;&#35810;&#26469;&#25628;&#32034;&#30446;&#26631;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#30340;&#26041;&#27861;&#23398;&#20064;&#20174;&#65288;&#21442;&#32771;&#22270;&#20687;&#65292;&#20462;&#25913;&#25991;&#26412;&#65289;&#23545;&#21040;&#22270;&#20687;&#23884;&#20837;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#22823;&#22411;&#22270;&#20687;&#35821;&#26009;&#24211;&#36827;&#34892;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#35757;&#32451;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#36825;&#31181;&#21453;&#21521;&#26597;&#35810;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#20307;&#31995;&#32467;&#26500;&#12290;&#20026;&#20102;&#32534;&#30721;&#21452;&#21521;&#26597;&#35810;&#65292;&#25105;&#20204;&#22312;&#20462;&#25913;&#25991;&#26412;&#21069;&#38754;&#28155;&#21152;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20196;&#29260;&#65292;&#25351;&#23450;&#26597;&#35810;&#30340;&#26041;&#21521;&#65292;&#28982;&#21518;&#24494;&#35843;&#25991;&#26412;&#23884;&#20837;&#27169;&#22359;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#27809;&#26377;&#23545;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20854;&#20182;&#26356;&#25913;&#12290;&#22312;&#20004;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21452;&#21521;&#35757;&#32451;&#22312;&#25552;&#39640;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20462;&#25913;&#25991;&#26412;&#23384;&#22312;&#22122;&#22768;&#25110;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as describe by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20110;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#21355;&#26143;&#24191;&#25773;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#25512;&#26029;&#25915;&#20987;&#21644;&#24046;&#20998;&#38544;&#31169;&#31934;&#24230;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16603</link><description>&lt;p&gt;
&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#21355;&#26143;&#24191;&#25773;&#31995;&#32479;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in MIMO Satellite Broadcast System. (arXiv:2303.16603v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20110;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#21355;&#26143;&#24191;&#25773;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#25512;&#26029;&#25915;&#20987;&#21644;&#24046;&#20998;&#38544;&#31169;&#31934;&#24230;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#26080;&#32447;&#36793;&#32536;&#20445;&#25252;&#23458;&#25143;&#31471;&#25968;&#25454;&#23433;&#20840;&#65292;&#20351;&#20043;&#20813;&#21463;&#25915;&#20987;&#29978;&#33267;&#33021;&#22815;&#19981;&#23558;&#25968;&#25454;&#19978;&#20256;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;SMC&#65289;&#65292;&#20294;&#23545;&#20110;&#25512;&#26029;&#25915;&#20987;&#23481;&#26131;&#21463;&#21040;&#23041;&#32961;&#65307;&#35201;&#20040;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#65292;&#20294;&#23545;&#20110;&#30456;&#23545;&#36739;&#23569;&#25968;&#25454;&#30340;&#22823;&#37327;&#23458;&#25143;&#31471;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#27979;&#35797;&#31934;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31995;&#32479;&#30340;&#20869;&#37096;&#24341;&#20837;&#20102;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a type of distributed machine learning at the wireless edge that preserves the privacy of clients' data from adversaries and even the central server. Existing federated learning approaches either use (i) secure multiparty computation (SMC) which is vulnerable to inference or (ii) differential privacy which may decrease the test accuracy given a large number of parties with relatively small amounts of data each. To tackle the problem with the existing methods in the literature, In this paper, we introduce incorporate federated learning in the inner-working of MIMO systems.
&lt;/p&gt;</description></item><item><title>DNN&#35757;&#32451;&#20013;&#38271;&#23614;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#23558;&#32473;&#19981;&#21516;&#36755;&#20986;&#31867;&#21035;&#25552;&#20379;&#19981;&#21516;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#26412;&#25991;&#39318;&#27425;&#25351;&#20986;&#23548;&#33268;&#33410;&#28857;&#25935;&#24863;&#24615;&#21464;&#21270;&#30340;&#33410;&#28857;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.16589</link><description>&lt;p&gt;
&#35770;&#25991;&#28023;&#25253;&#65306;&#35757;&#32451;DNN&#20013;&#20559;&#24046;&#12289;&#33410;&#28857;&#25935;&#24863;&#24615;&#21644;&#38271;&#23614;&#20998;&#24067;&#20043;&#38388;&#30340;&#38142;&#25509; (arXiv:2303.16589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Poster: Link between Bias, Node Sensitivity and Long-Tail Distribution in trained DNNs. (arXiv:2303.16589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16589
&lt;/p&gt;
&lt;p&gt;
DNN&#35757;&#32451;&#20013;&#38271;&#23614;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#23558;&#32473;&#19981;&#21516;&#36755;&#20986;&#31867;&#21035;&#25552;&#20379;&#19981;&#21516;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#26412;&#25991;&#39318;&#27425;&#25351;&#20986;&#23548;&#33268;&#33410;&#28857;&#25935;&#24863;&#24615;&#21464;&#21270;&#30340;&#33410;&#28857;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#23398;&#20064;(&#21644;&#37325;&#26032;&#23398;&#20064;)&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#19968;&#33324;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#20998;&#24067;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;DNNs&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35757;&#32451;&#30340;DNNs&#21487;&#33021;&#23545;&#19981;&#21516;&#30340;&#36755;&#20986;&#31867;&#21035;&#25552;&#20379;&#19981;&#21516;&#31243;&#24230;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#24378;&#35843;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#25972;&#20307;&#20559;&#24046;&#65292;&#20294;&#26412;&#25991;&#39318;&#27425;&#25351;&#20986;&#20102;&#23548;&#33268;&#33410;&#28857;&#23545;&#19981;&#21516;&#36755;&#20986;&#31867;&#21035;&#25935;&#24863;&#24615;&#21464;&#21270;&#30340;&#33410;&#28857;&#20559;&#24046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#24378;&#35843;DNNs&#20013;&#36825;&#31181;&#29420;&#29305;&#25361;&#25112;&#30340;&#24037;&#20316;&#65292;&#35752;&#35770;&#20854;&#21487;&#33021;&#30340;&#21407;&#22240;&#65292;&#24182;&#20026;&#36825;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#24773;&#22659;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#30340;&#23454;&#35777;&#26696;&#20363;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to their remarkable learning (and relearning) capabilities, deep neural networks (DNNs) find use in numerous real-world applications. However, the learning of these data-driven machine learning models is generally as good as the data available to them for training. Hence, training datasets with long-tail distribution pose a challenge for DNNs, since the DNNs trained on them may provide a varying degree of classification performance across different output classes. While the overall bias of such networks is already highlighted in existing works, this work identifies the node bias that leads to a varying sensitivity of the nodes for different output classes. To the best of our knowledge, this is the first work highlighting this unique challenge in DNNs, discussing its probable causes, and providing open challenges for this new research direction. We support our reasoning using an empirical case study of the networks trained on a real-world dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#23545;&#20914;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#32463;&#20856;&#25110;&#37327;&#23376;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#20854;&#20998;&#24067;&#24335;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16585</link><description>&lt;p&gt;
&#37327;&#23376;&#28145;&#24230;&#23545;&#20914;
&lt;/p&gt;
&lt;p&gt;
Quantum Deep Hedging. (arXiv:2303.16585v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#23545;&#20914;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#32463;&#20856;&#25110;&#37327;&#23376;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#20854;&#20998;&#24067;&#24335;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#37329;&#34701;&#31561;&#34892;&#19994;&#26377;&#30528;&#28508;&#22312;&#21464;&#38761;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#35770;&#25991;&#30740;&#31350;&#23545;&#20914;&#38382;&#39064;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#23454;&#29616;&#30495;&#23454;&#24066;&#22330;&#30340;&#26377;&#21147;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#22522;&#20110;&#31574;&#30053;&#25628;&#32034;&#21644;&#20998;&#24067;&#24335;Actor-Critic&#31639;&#27861;&#30340;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#20132;&#21644;&#22797;&#21512;&#23618;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#22788;&#29702;&#31574;&#30053;&#21644;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#20351;&#29992;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#25311;&#65292;&#34920;&#26126;&#37327;&#23376;&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#24471;&#21040;&#30340;&#24615;&#33021;&#27604;&#26631;&#20934;&#26041;&#27861;&#26356;&#22909;&#65292;&#26080;&#35770;&#26159;&#32463;&#20856;&#30340;&#36824;&#26159;&#37327;&#23376;&#30340;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#25152;&#25552;&#20986;&#27169;&#22411;&#23454;&#29616;&#22312;&#19968;&#20010;&#26368;&#22810;&#26377;16&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#31163;&#23376;&#38519;&#38449;&#37327;&#23376;&#22788;&#29702;&#22120;&#19978;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning has the potential for a transformative impact across industry sectors and in particular in finance. In our work we look at the problem of hedging where deep reinforcement learning offers a powerful framework for real markets. We develop quantum reinforcement learning methods based on policy-search and distributional actor-critic algorithms that use quantum neural network architectures with orthogonal and compound layers for the policy and value functions. We prove that the quantum neural networks we use are trainable, and we perform extensive simulations that show that quantum models can reduce the number of trainable parameters while achieving comparable performance and that the distributional approach obtains better performance than other standard approaches, both classical and quantum. We successfully implement the proposed models on a trapped-ion quantum processor, utilizing circuits with up to $16$ qubits, and observe performance that agrees well with nois
&lt;/p&gt;</description></item><item><title>PMAA&#26159;&#19968;&#31181;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#30340;&#39640;&#24615;&#33021;&#21355;&#26143;&#36965;&#24863;&#20113;&#21435;&#38500;&#26550;&#26500;&#12290;&#20854;&#20013;&#65292;&#29420;&#29305;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#23616;&#37096;&#20132;&#20114;&#27169;&#22359;&#33021;&#22815;&#21516;&#26102;&#34920;&#31034;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#29305;&#24449;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2303.16565</link><description>&lt;p&gt;
PMAA&#65306;&#19968;&#31181;&#22522;&#20110;&#28176;&#36827;&#24335;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#21355;&#26143;&#36965;&#24863;&#20113;&#21435;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PMAA: A Progressive Multi-scale Attention Autoencoder Model for High-Performance Cloud Removal from Multi-temporal Satellite Imagery. (arXiv:2303.16565v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16565
&lt;/p&gt;
&lt;p&gt;
PMAA&#26159;&#19968;&#31181;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#30340;&#39640;&#24615;&#33021;&#21355;&#26143;&#36965;&#24863;&#20113;&#21435;&#38500;&#26550;&#26500;&#12290;&#20854;&#20013;&#65292;&#29420;&#29305;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#23616;&#37096;&#20132;&#20114;&#27169;&#22359;&#33021;&#22815;&#21516;&#26102;&#34920;&#31034;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#29305;&#24449;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#36965;&#24863;&#22270;&#20687;&#20998;&#26512;&#22312;&#36828;&#31243;&#24863;&#30693;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#30001;&#20113;&#23618;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#20005;&#37325;&#38459;&#30861;&#20102;&#20854;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28176;&#36827;&#24335;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#8221;&#65288;PMAA&#65289;&#30340;&#39640;&#24615;&#33021;&#20113;&#21435;&#38500;&#26550;&#26500;&#65292;&#21516;&#26102;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#12290;&#20854;&#20027;&#35201;&#21253;&#25324;&#20113;&#26816;&#27979;&#21644;&#20113;&#21435;&#38500;&#27169;&#22359;&#12290;&#20113;&#26816;&#27979;&#36890;&#36807;&#20113;&#25513;&#27169;&#21152;&#24378;&#20113;&#21306;&#22495;&#20197;&#20419;&#36827;&#20113;&#21435;&#38500;&#12290;&#20113;&#21435;&#38500;&#27169;&#22359;&#20027;&#35201;&#21253;&#25324;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;MAM&#65289;&#21644;&#23616;&#37096;&#20132;&#20114;&#27169;&#22359;&#65288;LIM&#65289;&#12290;PMAA&#36890;&#36807;MAM&#24314;&#31435;&#22810;&#23610;&#24230;&#29305;&#24449;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#65292;&#24182;&#36890;&#36807;LIM&#35843;&#33410;&#32454;&#31890;&#24230;&#32454;&#33410;&#30340;&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#21516;&#19968;&#32423;&#21035;&#19978;&#21516;&#26102;&#21576;&#29616;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#29305;&#24449;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;&#20511;&#21161;&#19981;&#21516;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#31034;&#30340;&#24110;&#21161;&#19979;&#65292;PMAA&#22312;&#23450;&#37327;&#21644;&#35270;&#35273;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#20808;&#21069;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Satellite imagery analysis plays a vital role in remote sensing, but the information loss caused by cloud cover seriously hinders its application. This study presents a high-performance cloud removal architecture called Progressive Multi-scale Attention Autoencoder (PMAA), which simultaneously leverages global and local information. It mainly consists of a cloud detection backbone and a cloud removal module. The cloud detection backbone uses cloud masks to reinforce cloudy areas to prompt the cloud removal module. The cloud removal module mainly comprises a novel Multi-scale Attention Module (MAM) and a Local Interaction Module (LIM). PMAA establishes the long-range dependency of multi-scale features using MAM and modulates the reconstruction of the fine-grained details using LIM, allowing for the simultaneous representation of fine- and coarse-grained features at the same level. With the help of diverse and multi-scale feature representation, PMAA outperforms the previous state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340; Minecraft &#24320;&#25918;&#24335;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#22522;&#26412;&#25216;&#33021;&#21644;&#25216;&#33021;&#35268;&#21010;&#30340;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#35299;&#20915;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;24&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#24182;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16563</link><description>&lt;p&gt;
Plan4MC: &#22522;&#20110;&#25216;&#33021;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340; Minecraft &#24320;&#25918;&#24335;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks. (arXiv:2303.16563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340; Minecraft &#24320;&#25918;&#24335;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#22522;&#26412;&#25216;&#33021;&#21644;&#25216;&#33021;&#35268;&#21010;&#30340;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#35299;&#20915;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;24&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#24182;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312; Minecraft &#20013;&#26500;&#24314;&#19968;&#20010;&#22810;&#20219;&#21153;&#26234;&#33021;&#20307;&#12290;&#22312;&#27809;&#26377;&#20154;&#24037;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#36825;&#20010;&#24320;&#25918;&#24335;&#29615;&#22659;&#20013;&#30340;&#38271;&#31243;&#20219;&#21153;&#26159;&#26497;&#20854;&#26679;&#26412;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558; Minecraft &#20219;&#21153;&#30340;&#35299;&#20915;&#20998;&#35299;&#25104;&#23398;&#20064;&#22522;&#26412;&#25216;&#33021;&#21644;&#22522;&#20110;&#25216;&#33021;&#36827;&#34892;&#35268;&#21010;&#20004;&#20010;&#38454;&#27573;&#12290;&#25105;&#20204;&#22312; Minecraft &#20013;&#25552;&#20986;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#32454;&#31890;&#24230;&#22522;&#26412;&#25216;&#33021;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#20869;&#22312;&#22870;&#21169;&#30340; RL &#26041;&#27861;&#26469;&#23454;&#29616;&#25104;&#21151;&#29575;&#39640;&#30340;&#22522;&#26412;&#25216;&#33021;&#23398;&#20064;&#12290;&#22312;&#25216;&#33021;&#35268;&#21010;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21457;&#29616;&#25216;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#39044;&#20808;&#26500;&#24314;&#25216;&#33021;&#22270;&#12290;&#24403;&#26234;&#33021;&#20307;&#35299;&#20915;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#30340;&#25216;&#33021;&#25628;&#32034;&#31639;&#27861;&#22312;&#25216;&#33021;&#22270;&#19978;&#34892;&#36208;&#24182;&#29983;&#25104;&#36866;&#24403;&#30340;&#25216;&#33021;&#35745;&#21010;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102; 24 &#20010;&#19981;&#21516;&#30340; Minecraft &#20219;&#21153;&#65292;&#20854;&#20013;&#35768;&#22810;&#20219;&#21153;&#38656;&#35201;&#36830;&#32493;&#25191;&#34892;&#36229;&#36807; 10 &#20010;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;&#39033;&#30446;&#30340;&#32593;&#22336;&#21644;&#20195;&#30721;&#21487;&#20197;&#22312; https://www.rocwang.me/plan4mc.html &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study building a multi-task agent in Minecraft. Without human demonstrations, solving long-horizon tasks in this open-ended environment with reinforcement learning (RL) is extremely sample inefficient. To tackle the challenge, we decompose solving Minecraft tasks into learning basic skills and planning over the skills. We propose three types of fine-grained basic skills in Minecraft, and use RL with intrinsic rewards to accomplish basic skills with high success rates. For skill planning, we use Large Language Models to find the relationships between skills and build a skill graph in advance. When the agent is solving a task, our skill search algorithm walks on the skill graph and generates the proper skill plans for the agent. In experiments, our method accomplishes 24 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills. Our method outperforms baselines in most tasks by a large margin. The project's website and code can be found at https:
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#20102;&#24102;&#38543;&#26426;&#21442;&#25968;&#30340;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.16548</link><description>&lt;p&gt;
&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#20013;&#24102;&#38543;&#26426;&#21442;&#25968;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Methods for Discrete Time Linear Quadratic Regulator With Random Parameters. (arXiv:2303.16548v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#20102;&#24102;&#38543;&#26426;&#21442;&#25968;&#30340;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#31163;&#25955;&#26102;&#38388;&#30340;&#32447;&#24615;&#31995;&#32479;&#21644;&#20108;&#27425;&#20934;&#21017;&#30340;&#26080;&#38480;&#26102;&#22495;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#21442;&#25968;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#20110;&#26102;&#38388;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#22312;&#36825;&#31181;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24212;&#29992;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26469;&#23547;&#25214;&#26368;&#20248;&#25511;&#21046;&#65292;&#32780;&#19981;&#38656;&#35201;&#20102;&#35299;&#21442;&#25968;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29366;&#24577;&#36807;&#31243;&#30340;&#27425;&#39640;&#26031;&#24615;&#36136;&#65292;&#24182;&#26681;&#25454;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#24369;&#19988;&#26356;&#26131;&#39564;&#35777;&#30340;&#20551;&#35774;&#65292;&#24314;&#31435;&#20102;&#27492;&#26041;&#27861;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies an infinite horizon optimal control problem for discrete-time linear system and quadratic criteria, both with random parameters which are independent and identically distributed with respect to time. In this general setting, we apply the policy gradient method, a reinforcement learning technique, to search for the optimal control without requiring knowledge of statistical information of the parameters. We investigate the sub-Gaussianity of the state process and establish global linear convergence guarantee for this approach based on assumptions that are weaker and easier to verify compared to existing results. Numerical experiments are presented to illustrate our result.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#25324;&#20102;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20013;&#22522;&#20110;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#24773;&#20917;&#19979;&#21807;&#19968;&#24615;&#38382;&#39064;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16535</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20013;&#22522;&#20110;&#38750;&#32447;&#24615;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#30340;&#21407;&#21017;&#20998;&#31163;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Independent Component Analysis for Principled Disentanglement in Unsupervised Deep Learning. (arXiv:2303.16535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#25324;&#20102;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20013;&#22522;&#20110;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#24773;&#20917;&#19979;&#21807;&#19968;&#24615;&#38382;&#39064;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#22914;&#20309;&#25214;&#21040;&#26377;&#29992;&#30340;&#39640;&#32500;&#25968;&#25454;&#34920;&#31034;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#20998;&#31163;&#8221;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#32570;&#20047;&#36866;&#24403;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#22312;&#32447;&#24615;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#19988;&#20855;&#26377;&#22522;&#20110;&#33391;&#23450;&#20041;&#30340;&#27010;&#29575;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#12290; &#28982;&#32780;&#65292;&#23558;ICA&#25193;&#23637;&#21040;&#38750;&#32447;&#24615;&#24773;&#20917;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#21487;&#35782;&#21035;&#24615;&#65292;&#21363;&#34920;&#31034;&#30340;&#21807;&#19968;&#24615;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20351;&#29992;&#26102;&#38388;&#32467;&#26500;&#25110;&#26576;&#20123;&#36741;&#21161;&#20449;&#24687;&#30340;&#38750;&#32447;&#24615;&#25193;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#23454;&#38469;&#19978;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#22240;&#27492;&#24050;&#32463;&#24320;&#21457;&#20986;&#36234;&#26469;&#36234;&#22810;&#30340;&#31639;&#27861;&#12290;&#29305;&#21035;&#26159;&#19968;&#20123;&#33258;&#30417;&#30563;&#31639;&#27861;&#21487;&#20197;&#26174;&#31034;&#20986;&#20272;&#35745;&#38750;&#32447;&#24615;ICA&#65292;&#21363;&#20351;&#26368;&#21021;&#26159;&#20174;&#21551;&#21457;&#24335;&#35282;&#24230;&#25552;&#20986;&#30340;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#38750;&#32447;&#24615;ICA&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central problem in unsupervised deep learning is how to find useful representations of high-dimensional data, sometimes called "disentanglement". Most approaches are heuristic and lack a proper theoretical foundation. In linear representation learning, independent component analysis (ICA) has been successful in many applications areas, and it is principled, i.e. based on a well-defined probabilistic model. However, extension of ICA to the nonlinear case has been problematic due to the lack of identifiability, i.e. uniqueness of the representation. Recently, nonlinear extensions that utilize temporal structure or some auxiliary information have been proposed. Such models are in fact identifiable, and consequently, an increasing number of algorithms have been developed. In particular, some self-supervised algorithms can be shown to estimate nonlinear ICA, even though they have initially been proposed from heuristic perspectives. This paper reviews the state-of-the-art of nonlinear ICA 
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#39044;&#27979;&#26399;&#36135;&#20215;&#26684;&#36235;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#20219;&#21153;&#35774;&#35745;&#21644;&#36830;&#32493;&#35757;&#32451;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.16532</link><description>&lt;p&gt;
&#24322;&#26500;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26399;&#36135;&#37327;&#21270;&#25237;&#36164;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Futures Quantitative Investment with Heterogeneous Continual Graph Neural Network. (arXiv:2303.16532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16532
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39044;&#27979;&#26399;&#36135;&#20215;&#26684;&#36235;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#20219;&#21153;&#35774;&#35745;&#21644;&#36830;&#32493;&#35757;&#32451;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#35745;&#37327;&#27169;&#22411;&#39044;&#27979;&#26399;&#36135;&#20215;&#26684;&#36235;&#21183;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#32771;&#34385;&#21040;&#26399;&#36135;&#21382;&#21490;&#25968;&#25454;&#20197;&#21450;&#19981;&#21516;&#26399;&#36135;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#27492;&#31867;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22235;&#20010;&#24322;&#26500;&#20219;&#21153;&#26469;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#29305;&#24449;&#65306;&#20215;&#26684;&#22238;&#24402;&#12289;&#31227;&#21160;&#24179;&#22343;&#20215;&#26684;&#22238;&#24402;&#12289;&#30701;&#26102;&#38388;&#20869;&#30340;&#20215;&#26684;&#24046;&#22238;&#24402;&#21644;&#21464;&#21270;&#28857;&#26816;&#27979;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#26631;&#31614;&#65292;&#25105;&#20204;&#37319;&#29992;&#36830;&#32493;&#35757;&#32451;&#30340;&#26041;&#24335;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a challenging problem to predict trends of futures prices with traditional econometric models as one needs to consider not only futures' historical data but also correlations among different futures. Spatial-temporal graph neural networks (STGNNs) have great advantages in dealing with such kind of spatial-temporal data. However, we cannot directly apply STGNNs to high-frequency future data because future investors have to consider both the long-term and short-term characteristics when doing decision-making. To capture both the long-term and short-term features, we exploit more label information by designing four heterogeneous tasks: price regression, price moving average regression, price gap regression (within a short interval), and change-point detection, which involve both long-term and short-term scenes. To make full use of these labels, we train our model in a continual manner. Traditional continual GNNs define the gradient of prices as the parameter important to overcome ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;&#29702;&#35770;&#65292;&#24182;&#24635;&#32467;&#20102;&#21033;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65307;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#32473;&#23450;&#30340;&#37319;&#26679;&#31574;&#30053;&#30340;&#36136;&#37327;&#65292;&#30740;&#31350;&#20102;&#37319;&#26679;&#26041;&#26696;&#21644;&#25152;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.16529</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Importance Sampling for Stochastic Gradient Descent in Deep Neural Networks. (arXiv:2303.16529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;&#29702;&#35770;&#65292;&#24182;&#24635;&#32467;&#20102;&#21033;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65307;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#32473;&#23450;&#30340;&#37319;&#26679;&#31574;&#30053;&#30340;&#36136;&#37327;&#65292;&#30740;&#31350;&#20102;&#37319;&#26679;&#26041;&#26696;&#21644;&#25152;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36890;&#36807;&#23545;&#35757;&#32451;&#38598;&#36827;&#34892;&#22343;&#21248;&#37319;&#26679;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#26679;&#26412;&#26469;&#26500;&#24314;&#26080;&#20559;&#26799;&#24230;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#30340;&#26576;&#20010;&#38454;&#27573;&#65292;&#26576;&#20123;&#25968;&#25454;&#27604;&#20854;&#20182;&#25968;&#25454;&#26356;&#26377;&#21161;&#20110;&#32487;&#32493;&#23398;&#20064;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#21033;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#20986;&#27604;&#22343;&#21248;&#37319;&#26679;&#26041;&#26696;&#26356;&#22909;&#30340;&#37319;&#26679;&#31574;&#30053;&#12290;&#22312;&#22238;&#39038;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;&#29702;&#35770;&#21518;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#32473;&#23450;&#30340;&#37319;&#26679;&#31574;&#30053;&#30340;&#36136;&#37327;&#65292;&#24182;&#30740;&#31350;&#20102;&#37319;&#26679;&#26041;&#26696;&#21644;&#25152;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent samples uniformly the training set to build an unbiased gradient estimate with a limited number of samples. However, at a given step of the training process, some data are more helpful than others to continue learning. Importance sampling for training deep neural networks has been widely studied to propose sampling schemes yielding better performance than the uniform sampling scheme. After recalling the theory of importance sampling for deep learning, this paper reviews the challenges inherent to this research area. In particular, we propose a metric allowing the assessment of the quality of a given sampling scheme; and we study the interplay between the sampling scheme and the optimizer used.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#65288;ANN-BP&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29028;&#26609;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;ANN-BP&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#26631;&#31614;&#26367;&#20195;&#26041;&#26696;&#65292;&#23558;&#26609;&#23376;&#31283;&#23450;&#24615;&#25193;&#23637;&#21040;&#22235;&#20010;&#31867;&#21035;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#26609;&#23376;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16524</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#31070;&#32463;&#32593;&#32476;BP&#26550;&#26500;&#30340;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#29028;&#26609;&#31283;&#23450;&#24615;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Ensemble Learning Model on Artificial Neural Network-Backpropagation (ANN-BP) Architecture for Coal Pillar Stability Classification. (arXiv:2303.16524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#65288;ANN-BP&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29028;&#26609;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;ANN-BP&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#26631;&#31614;&#26367;&#20195;&#26041;&#26696;&#65292;&#23558;&#26609;&#23376;&#31283;&#23450;&#24615;&#25193;&#23637;&#21040;&#22235;&#20010;&#31867;&#21035;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#26609;&#23376;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29028;&#26609;&#26159;&#30830;&#20445;&#22320;&#19979;&#30828;&#23721;&#30719;&#23665;&#23433;&#20840;&#30340;&#37325;&#35201;&#32467;&#26500;&#21333;&#20803;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#23545;&#22320;&#19979;&#26609;&#23376;&#30340;&#31283;&#23450;&#24615;&#36827;&#34892;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#19968;&#20010;&#24120;&#29992;&#30340;&#35780;&#20272;&#26609;&#23376;&#31283;&#23450;&#24615;&#30340;&#25351;&#26631;&#26159;&#23433;&#20840;&#31995;&#25968;&#65288;SF&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20351;&#29992;SF&#36827;&#34892;&#26609;&#23376;&#31283;&#23450;&#24615;&#35780;&#20272;&#26102;&#65292;&#24120;&#24120;&#20986;&#29616;&#28165;&#26224;&#30340;&#36793;&#30028;&#19981;&#21487;&#38752;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#65288;ANN-BP&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#22312;&#26609;&#23376;&#31283;&#23450;&#24615;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#26609;&#23376;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#26377;&#19977;&#31181;ANN-BP&#65292;&#20998;&#21035;&#30001;&#20854;&#28608;&#27963;&#20989;&#25968;&#21306;&#20998;&#65306;ANN-BP ReLU&#12289;ANN-BP ELU&#21644;ANN-BP GELU&#12290;&#26412;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#19982;SF&#30340;&#36866;&#24212;&#24615;&#26469;&#32771;&#34385;&#26609;&#23376;&#30340;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#26609;&#23376;&#31283;&#23450;&#24615;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#20855;&#26377;&#36866;&#24403;&#30340;&#23433;&#20840;&#31995;&#25968;&#32780;&#22833;&#36133;&#12289;&#20855;&#26377;&#36866;&#24403;&#30340;&#23433;&#20840;&#31995;&#25968;&#32780;&#23436;&#22909;&#12289;&#19981;&#20855;&#26377;&#36866;&#24403;&#30340;&#23433;&#20840;&#31995;&#25968;&#32780;&#22833;&#36133;&#21644;&#19981;&#20855;&#26377;&#36866;&#24403;&#30340;&#23433;&#20840;&#31995;&#25968;&#32780;&#23436;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pillars are important structural units used to ensure mining safety in underground hard rock mines. Therefore, precise predictions regarding the stability of underground pillars are required. One common index that is often used to assess pillar stability is the Safety Factor (SF). Unfortunately, such crisp boundaries in pillar stability assessment using SF are unreliable. This paper presents a novel application of Artificial Neural Network-Backpropagation (ANN-BP) and Deep Ensemble Learning for pillar stability classification. There are three types of ANN-BP used for the classification of pillar stability distinguished by their activation functions: ANN-BP ReLU, ANN-BP ELU, and ANN-BP GELU. This research also presents a new labeling alternative for pillar stability by considering its suitability with the SF. Thus, pillar stability is expanded into four categories: failed with a suitable safety factor, intact with a suitable safety factor, failed without a suitable safety factor, and in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#30340;&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#24378;&#27491;&#21017;&#21270;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16521</link><description>&lt;p&gt;
&#22312;&#19981;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#21152;&#24378;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#20013;&#30340;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Hard Regularization to Prevent Collapse in Online Deep Clustering without Data Augmentation. (arXiv:2303.16521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#30340;&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#24378;&#27491;&#21017;&#21270;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#26159;&#25351;&#32852;&#21512;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#21644;&#32858;&#31867;&#27169;&#22411;&#65292;&#20197;&#23558;&#27599;&#20010;&#26032;&#25968;&#25454;&#28857;&#25110;&#25209;&#22788;&#29702;&#20998;&#37197;&#21040;&#32858;&#31867;&#26631;&#31614;&#20013;&#12290;&#23613;&#31649;&#27604;&#31163;&#32447;&#26041;&#27861;&#26356;&#24555;&#36895;&#21644;&#26356;&#28789;&#27963;&#65292;&#20294;&#22312;&#32447;&#32858;&#31867;&#24456;&#23481;&#26131;&#36798;&#21040;&#23849;&#28291;&#35299;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#23558;&#25152;&#26377;&#36755;&#20837;&#26144;&#23556;&#21040;&#21516;&#19968;&#28857;&#65292;&#24182;&#23558;&#25152;&#26377;&#36755;&#20837;&#25918;&#20837;&#21333;&#20010;&#32858;&#31867;&#20013;&#12290;&#29616;&#26377;&#25104;&#21151;&#27169;&#22411;&#37319;&#29992;&#20102;&#21508;&#31181;&#25216;&#26415;&#26469;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#25110;&#26088;&#22312;&#20351;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#32858;&#31867;&#30340;&#24179;&#22343;&#36719;&#20998;&#37197;&#30456;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#23545;&#30828;&#20998;&#37197;&#36827;&#34892;&#20102;&#35268;&#21017;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#23548;&#20986;&#19968;&#20010;&#30452;&#35266;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#21487;&#20197;&#30452;&#25509;&#21253;&#21547;&#22312;&#32534;&#30721;&#22120;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;&#22312;&#22235;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#21152;&#31283;&#23450;&#22320;&#36991;&#20813;&#20102;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online deep clustering refers to the joint use of a feature extraction network and a clustering model to assign cluster labels to each new data point or batch as it is processed. While faster and more versatile than offline methods, online clustering can easily reach the collapsed solution where the encoder maps all inputs to the same point and all are put into a single cluster. Successful existing models have employed various techniques to avoid this problem, most of which require data augmentation or which aim to make the average soft assignment across the dataset the same for each cluster. We propose a method that does not require data augmentation, and that, differently from existing methods, regularizes the hard assignments. Using a Bayesian framework, we derive an intuitive optimization objective that can be straightforwardly included in the training of the encoder network. Tested on four image datasets, we show that it consistently avoids collapse more robustly than other method
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#23458;&#25143;&#36129;&#29486;&#35780;&#20272;&#19979;&#21516;&#26102;&#20248;&#21270;&#21327;&#20316;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#20844;&#24179;&#24615;&#65292;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.16520</link><description>&lt;p&gt;
&#23458;&#25143;&#36129;&#29486;&#35780;&#20272;&#19979;&#30340;&#20844;&#24179;&#32852;&#37030;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Fair Federated Medical Image Segmentation via Client Contribution Estimation. (arXiv:2303.16520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#23458;&#25143;&#36129;&#29486;&#35780;&#20272;&#19979;&#21516;&#26102;&#20248;&#21270;&#21327;&#20316;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#20844;&#24179;&#24615;&#65292;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#30830;&#20445;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35805;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#26681;&#25454;&#23458;&#25143;&#30340;&#36129;&#29486;&#65288;&#21327;&#20316;&#20844;&#24179;&#24615;&#65289;&#26469;&#22870;&#21169;&#23458;&#25143;&#65292;&#20197;&#21450;&#22914;&#20309;&#23454;&#29616;&#23458;&#25143;&#20043;&#38388;&#30340;&#24615;&#33021;&#22343;&#34913;&#65288;&#24615;&#33021;&#20844;&#24179;&#24615;&#65289;&#12290;&#23613;&#31649;&#24050;&#32463;&#22312;&#20854;&#20013;&#19968;&#20010;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23558;&#20108;&#32773;&#32771;&#34385;&#22312;&#19968;&#36215;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#21560;&#24341;&#21644;&#28608;&#21169;&#26356;&#22810;&#19981;&#21516;&#31867;&#22411;&#30340;&#23458;&#25143;&#21152;&#20837;&#32852;&#37030;&#23398;&#20064;&#65292;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20248;&#21270;&#20004;&#31181;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26799;&#24230;&#21644;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#23458;&#25143;&#36129;&#29486;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#26799;&#24230;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#30417;&#27979;&#27599;&#20010;&#23458;&#25143;&#30456;&#23545;&#20110;&#20854;&#20182;&#23458;&#25143;&#30340;&#26799;&#24230;&#26041;&#21521;&#24046;&#24322;&#12290;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#27169;&#22411;&#26469;&#27979;&#37327;&#23458;&#25143;&#25968;&#25454;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#22522;&#20110;&#36825;&#31181;&#36129;&#29486;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#36129;&#29486;&#35780;&#20272;&#36827;&#34892;&#30340;&#32852;&#37030;&#35757;&#32451;&#65288;FedCE&#65289;&#65292;&#21363;&#20351;&#29992;&#20272;&#35745;&#30340;&#26041;&#24335;&#20316;&#20026;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to ensure fairness is an important topic in federated learning (FL). Recent studies have investigated how to reward clients based on their contribution (collaboration fairness), and how to achieve uniformity of performance across clients (performance fairness). Despite achieving progress on either one, we argue that it is critical to consider them together, in order to engage and motivate more diverse clients joining FL to derive a high-quality global model. In this work, we propose a novel method to optimize both types of fairness simultaneously. Specifically, we propose to estimate client contribution in gradient and data space. In gradient space, we monitor the gradient direction differences of each client with respect to others. And in data space, we measure the prediction error on client data using an auxiliary model. Based on this contribution estimation, we propose a FL method, federated training via contribution estimation (FedCE), i.e., using estimation as global model agg
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#36845;&#20195;&#30340;Landing&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24378;&#21046;&#25191;&#34892;&#27491;&#20132;&#32422;&#26463;&#30340;&#21516;&#26102;&#39034;&#30021;&#22320;&#21560;&#24341;&#21040;&#27491;&#20132;&#32422;&#26463;&#27969;&#24418;&#19978;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#31181;&#31639;&#27861;&#20197;&#25903;&#25345;&#26031;&#25176;&#33778;&#23572;&#65288;Stiefel&#65289;&#27969;&#24418;&#65292;&#24182;&#25552;&#20379;&#20102;&#38543;&#26426;&#21644;&#26041;&#24046;&#32422;&#20943;&#31639;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30456;&#21516;&#20294;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2303.16510</link><description>&lt;p&gt;
&#22312;&#27491;&#20132;&#32422;&#26463;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#19981;&#21487;&#34892;&#30830;&#23450;&#24615;&#12289;&#38543;&#26426;&#21644;&#26041;&#24046;&#32422;&#20943;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints. (arXiv:2303.16510v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#36845;&#20195;&#30340;Landing&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24378;&#21046;&#25191;&#34892;&#27491;&#20132;&#32422;&#26463;&#30340;&#21516;&#26102;&#39034;&#30021;&#22320;&#21560;&#24341;&#21040;&#27491;&#20132;&#32422;&#26463;&#27969;&#24418;&#19978;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#31181;&#31639;&#27861;&#20197;&#25903;&#25345;&#26031;&#25176;&#33778;&#23572;&#65288;Stiefel&#65289;&#27969;&#24418;&#65292;&#24182;&#25552;&#20379;&#20102;&#38543;&#26426;&#21644;&#26041;&#24046;&#32422;&#20943;&#31639;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30456;&#21516;&#20294;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#20132;&#32422;&#26463;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#37117;&#20250;&#33258;&#28982;&#22320;&#20986;&#29616;&#65292;&#20174;&#20027;&#25104;&#20998;&#20998;&#26512;&#21040;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#20351;&#29992;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#26469;&#27714;&#35299;&#65292;&#35813;&#31639;&#27861;&#22312;&#24378;&#21046;&#25191;&#34892;&#32422;&#26463;&#26102;&#26368;&#32791;&#36153;&#26102;&#38388;&#12290;&#26368;&#36817;&#65292;Ablin&#65286;Peyr\'e&#65288;2022&#65289;&#25552;&#20986;&#20102;Landing&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#24265;&#20215;&#36845;&#20195;&#26041;&#27861;&#65292;&#23427;&#19981;&#24378;&#21046;&#25191;&#34892;&#27491;&#20132;&#32422;&#26463;&#65292;&#20294;&#20250;&#20197;&#24179;&#28369;&#30340;&#26041;&#24335;&#21560;&#24341;&#21040;&#27969;&#24418;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;Landing&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#23454;&#29992;&#21644;&#29702;&#35770;&#21457;&#23637;&#12290;&#39318;&#20808;&#65292;&#35813;&#26041;&#27861;&#34987;&#25193;&#23637;&#21040;&#26031;&#25176;&#33778;&#23572;&#27969;&#24418;&#65292;&#21363;&#30697;&#24418;&#27491;&#20132;&#30697;&#38453;&#30340;&#38598;&#21512;&#12290;&#24403;&#25104;&#26412;&#20989;&#25968;&#26159;&#35768;&#22810;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#26102;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#38543;&#26426;&#21644;&#26041;&#24046;&#32422;&#20943;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#23427;&#20204;&#30340;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#30456;&#21516;&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orthogonality constraints naturally appear in many machine learning problems, from Principal Components Analysis to robust neural network training. They are usually solved using Riemannian optimization algorithms, which minimize the objective function while enforcing the constraint. However, enforcing the orthogonality constraint can be the most time-consuming operation in such algorithms. Recently, Ablin &amp; Peyr\'e (2022) proposed the Landing algorithm, a method with cheap iterations that does not enforce the orthogonality constraint but is attracted towards the manifold in a smooth manner. In this article, we provide new practical and theoretical developments for the landing algorithm. First, the method is extended to the Stiefel manifold, the set of rectangular orthogonal matrices. We also consider stochastic and variance reduction algorithms when the cost function is an average of many functions. We demonstrate that all these methods have the same rate of convergence as their Rieman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#22810;&#30446;&#26631;&#22238;&#24402;&#20013;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#35299;&#37322;&#30340;&#25216;&#26415;&#65292;&#24182;&#22312;&#24191;&#27867;&#23454;&#39564;&#35780;&#20272;&#20013;&#35777;&#26126;&#20854;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16506</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#22312;&#22810;&#30446;&#26631;&#22238;&#24402;&#20013;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Local Interpretability of Random Forests for Multi-Target Regression. (arXiv:2303.16506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#22810;&#30446;&#26631;&#22238;&#24402;&#20013;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#35299;&#37322;&#30340;&#25216;&#26415;&#65292;&#24182;&#22312;&#24191;&#27867;&#23454;&#39564;&#35780;&#20272;&#20013;&#35777;&#26126;&#20854;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#22238;&#24402;&#22312;&#20247;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#34429;&#28982;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#35299;&#37322;&#12290;&#21487;&#35299;&#37322;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#33021;&#30452;&#25509;&#24433;&#21709;&#20154;&#31867;&#31119;&#31049;&#26102;&#26356;&#20026;&#37325;&#35201;&#12290;&#34429;&#28982;&#23384;&#22312;&#38024;&#23545;&#22810;&#30446;&#26631;&#22238;&#24402;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#25216;&#26415;&#65292;&#20294;&#27809;&#26377;&#29305;&#23450;&#20110;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#20026;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#22810;&#30446;&#26631;&#22238;&#24402;&#20013;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#21463;&#26368;&#36817;&#29992;&#20110;&#38543;&#26426;&#26862;&#26519;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#29305;&#23450;&#25216;&#26415;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#35299;&#37322;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-target regression is useful in a plethora of applications. Although random forest models perform well in these tasks, they are often difficult to interpret. Interpretability is crucial in machine learning, especially when it can directly impact human well-being. Although model-agnostic techniques exist for multi-target regression, specific techniques tailored to random forest models are not available. To address this issue, we propose a technique that provides rule-based interpretations for instances made by a random forest model for multi-target regression, influenced by a recent model-specific technique for random forest interpretability. The proposed technique was evaluated through extensive experiments and shown to offer competitive interpretations compared to state-of-the-art techniques.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#25968;&#28608;&#27963;&#20989;&#25968;&#23450;&#20041;&#30340;&#31070;&#32463;&#20989;&#25968;&#26469;&#23454;&#29616;&#36807;&#21442;&#25968;&#21270;&#25351;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.16504</link><description>&lt;p&gt;
&#19968;&#31181;&#36807;&#21442;&#25968;&#21270;&#25351;&#25968;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Over-parameterized Exponential Regression. (arXiv:2303.16504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16504
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#25968;&#28608;&#27963;&#20989;&#25968;&#23450;&#20041;&#30340;&#31070;&#32463;&#20989;&#25968;&#26469;&#23454;&#29616;&#36807;&#21442;&#25968;&#21270;&#25351;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#30740;&#31350;&#24341;&#21457;&#20102;&#35768;&#22810;&#20851;&#27880;&#65292;&#26088;&#22312;&#36890;&#36807;&#36807;&#21442;&#25968;&#21270;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#39046;&#22495;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#25351;&#25968;&#28608;&#27963;&#20989;&#25968;&#30340;&#20852;&#36259;&#65292;&#29305;&#21035;&#26159;&#22312;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#25968;&#23398;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#28608;&#27963;&#20989;&#25968;&#23450;&#20041;&#31070;&#32463;&#20989;&#25968; $F: \mathbb{R}^{d \times m} \times \mathbb{R}^d \rightarrow \mathbb{R}$&#12290;&#32473;&#23450;&#19968;&#32452;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#28857; $\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} \subset \mathbb{R}^d \times \mathbb{R}$&#65292;&#20854;&#20013; $n$ &#34920;&#31034;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#36825;&#37324; $F(W(t),x)$ &#21487;&#20197;&#29992; $F(W(t),x) := \sum_{r=1}^m a_r \exp(\langle w_r, x \rangle)$ &#34920;&#31034;&#65292;&#20854;&#20013; $m$ &#34920;&#31034;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#65292;$w_r(t)$ &#26159;&#26102;&#38388; $t$ &#19978;&#30340;&#26435;&#37325;&#12290;&#22266;&#23450;&#26435;&#37325; $a_r$ &#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#20250;&#25913;&#21464;&#65292;&#36825;&#26159;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, there has been a significant amount of research focused on studying the ReLU activation function, with the aim of achieving neural network convergence through over-parametrization. However, recent developments in the field of Large Language Models (LLMs) have sparked interest in the use of exponential activation functions, specifically in the attention mechanism.  Mathematically, we define the neural function $F: \mathbb{R}^{d \times m} \times \mathbb{R}^d \rightarrow \mathbb{R}$ using an exponential activation function. Given a set of data points with labels $\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} \subset \mathbb{R}^d \times \mathbb{R}$ where $n$ denotes the number of the data. Here $F(W(t),x)$ can be expressed as $F(W(t),x) := \sum_{r=1}^m a_r \exp(\langle w_r, x \rangle)$, where $m$ represents the number of neurons, and $w_r(t)$ are weights at time $t$. It's standard in literature that $a_r$ are the fixed weights and it's never changed during the trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32479;&#19968;&#20998;&#26512;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;SGD&#31867;&#22411;&#26041;&#27861;&#65292;&#22312;&#24378;&#20984;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.16502</link><description>&lt;p&gt;
SGD&#31867;&#22411;&#26041;&#27861;&#30340;&#32479;&#19968;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unified analysis of SGD-type methods. (arXiv:2303.16502v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32479;&#19968;&#20998;&#26512;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;SGD&#31867;&#22411;&#26041;&#27861;&#65292;&#22312;&#24378;&#20984;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#20174;&#65288;Gorbunov et al.&#65292;2020&#65289;&#23545;&#20110;&#24378;&#20984;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#20013;SGD&#31867;&#22411;&#26041;&#27861;&#30340;&#32479;&#19968;&#20998;&#26512;&#26041;&#27861;&#12290;&#35752;&#35770;&#20102;&#19981;&#21516;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#20998;&#26512;&#30340;&#30456;&#20284;&#20043;&#22788;&#20197;&#21450;&#29616;&#26377;&#26694;&#26550;&#30340;&#25193;&#23637;&#12290;&#21516;&#26102;&#25552;&#21040;&#20102;&#20998;&#26512;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This note focuses on a simple approach to the unified analysis of SGD-type methods from (Gorbunov et al., 2020) for strongly convex smooth optimization problems. The similarities in the analyses of different stochastic first-order methods are discussed along with the existing extensions of the framework. The limitations of the analysis and several alternative approaches are mentioned as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29702;&#35770;&#35777;&#26126;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#26159;&#38477;&#20302;Adam&#25110;AdamW&#33719;&#24471;&#36755;&#20986;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#30340;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#26041;&#38024;&#20026;Adam&#25110;AdamW&#20248;&#21270;&#31639;&#27861;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;Lipschitz&#24120;&#25968;&#36739;&#20302;&#19988;&#26368;&#22823;&#20540;&#36739;&#23567;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16464</link><description>&lt;p&gt;
Adam&#21644;AdamW&#20248;&#21270;&#22120;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#25928;&#24212;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers. (arXiv:2303.16464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#35777;&#26126;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#26159;&#38477;&#20302;Adam&#25110;AdamW&#33719;&#24471;&#36755;&#20986;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#30340;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#26041;&#38024;&#20026;Adam&#25110;AdamW&#20248;&#21270;&#31639;&#27861;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;Lipschitz&#24120;&#25968;&#36739;&#20302;&#19988;&#26368;&#22823;&#20540;&#36739;&#23567;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#19982;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#26159;&#38477;&#20302;Adam&#25110;AdamW&#33719;&#24471;&#36755;&#20986;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20316;&#20026;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#26102;&#20248;&#21270;&#31639;&#27861;&#20026;Adam&#25110;AdamW&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#26412;&#25991;&#36873;&#25321;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20154;&#33080;&#24180;&#40836;&#35780;&#20272;&#38382;&#39064;&#26469;&#35780;&#20272;&#29702;&#35770;&#30028;&#38480;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#65292;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20174;&#19981;&#21516;&#20998;&#24067;&#20013;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Lipschitz&#24120;&#25968;&#36739;&#20302;&#19988;&#26368;&#22823;&#20540;&#36739;&#23567;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;Adam&#25110;AdamW&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization performance of deep neural networks with regard to the optimization algorithm is one of the major concerns in machine learning. This performance can be affected by various factors. In this paper, we theoretically prove that the Lipschitz constant of a loss function is an important factor to diminish the generalization error of the output model obtained by Adam or AdamW. The results can be used as a guideline for choosing the loss function when the optimization algorithm is Adam or AdamW. In addition, to evaluate the theoretical bound in a practical setting, we choose the human age estimation problem in computer vision. For assessing the generalization better, the training and test datasets are drawn from different distributions. Our experimental evaluation shows that the loss function with lower Lipschitz constant and maximum value improves the generalization of the model trained by Adam or AdamW.
&lt;/p&gt;</description></item><item><title>GNNBuilder&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;GNN&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20026;&#29992;&#25143;&#20219;&#24847;&#23450;&#20041;&#30340;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21152;&#36895;&#22120;&#65292;&#19988;&#36816;&#34892;&#36895;&#24230;&#27604;&#36719;&#20214;&#22522;&#20934;&#24555;&#22810;&#36798;12.95&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.16459</link><description>&lt;p&gt;
GNNBuilder&#65306;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#29983;&#25104;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization. (arXiv:2303.16459v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16459
&lt;/p&gt;
&lt;p&gt;
GNNBuilder&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;GNN&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20026;&#29992;&#25143;&#20219;&#24847;&#23450;&#20041;&#30340;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21152;&#36895;&#22120;&#65292;&#19988;&#36816;&#34892;&#36895;&#24230;&#27604;&#36719;&#20214;&#22522;&#20934;&#24555;&#22810;&#36798;12.95&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26377;&#24456;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21152;&#36895;&#22120;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#29992;&#25143;&#30340;&#30828;&#20214;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#24120;&#38024;&#23545;&#19968;&#31181;&#29305;&#23450;&#30340;GNN&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#23454;&#38469;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNNBuilder&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;GNN&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#12290;&#23427;&#20855;&#26377;&#22235;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;GNNBuilder&#21487;&#20197;&#33258;&#21160;&#20026;&#29992;&#25143;&#20219;&#24847;&#23450;&#20041;&#30340;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#29983;&#25104;GNN&#21152;&#36895;&#22120;&#65307;&#65288;2&#65289;GNNBuilder&#37319;&#29992;&#26631;&#20934;&#30340;PyTorch&#32534;&#31243;&#25509;&#21475;&#65292;&#20026;&#31639;&#27861;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#38646;&#24320;&#38144;&#65307;&#65288;3&#65289;GNNBuilder&#25903;&#25345;&#31471;&#21040;&#31471;&#30340;&#20195;&#30721;&#29983;&#25104;&#12289;&#27169;&#25311;&#12289;&#21152;&#36895;&#22120;&#20248;&#21270;&#21644;&#30828;&#20214;&#37096;&#32626;&#65292;&#23454;&#29616;&#20102;GNN&#21152;&#36895;&#22120;&#35774;&#35745;&#30340;&#19968;&#38190;&#24335;&#25805;&#20316;&#65307;&#65288;4&#65289;GNNBuilder&#37197;&#22791;&#20102;&#20854;&#25152;&#29983;&#25104;&#30340;&#21152;&#36895;&#22120;&#30340;&#20934;&#30830;&#24615;&#33021;&#27169;&#22411;&#65292;&#20351;&#24471;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65288;DSE&#65289;&#24555;&#36895;&#32780;&#28789;&#27963;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#39318;&#20808;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21152;&#36895;&#22120;&#22312;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;GNN&#21152;&#36895;&#22120;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#65292;&#36816;&#34892;&#36895;&#24230;&#27604;&#36719;&#20214;&#22522;&#20934;&#24555;&#20102;&#22810;&#36798;12.95&#20493;&#12290;&#20854;&#27425;&#65292;&#23545;&#19981;&#35268;&#21017;&#22270;&#22788;&#29702;&#30340;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;GNNBuilder&#30340;&#20986;&#33394;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#26368;&#21518;&#65292;&#22312;&#21333;&#20010;GPU&#30340;&#26381;&#21153;&#22120;&#19978;&#65292;&#25105;&#20204;&#23545;400&#20010;GNN&#27169;&#22411;&#36827;&#34892;&#20102;30&#20998;&#38047;&#30340;DSE&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;GNNBuilder&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are plenty of graph neural network (GNN) accelerators being proposed. However, they highly rely on users' hardware expertise and are usually optimized for one specific GNN model, making them challenging for practical use . Therefore, in this work, we propose GNNBuilder, the first automated, generic, end-to-end GNN accelerator generation framework. It features four advantages: (1) GNNBuilder can automatically generate GNN accelerators for a wide range of GNN models arbitrarily defined by users; (2) GNNBuilder takes standard PyTorch programming interface, introducing zero overhead for algorithm developers; (3) GNNBuilder supports end-to-end code generation, simulation, accelerator optimization, and hardware deployment, realizing a push-button fashion for GNN accelerator design; (4) GNNBuilder is equipped with accurate performance models of its generated accelerator, enabling fast and flexible design space exploration (DSE). In the experiments, first, we show that our accelerator pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;W2PGNN&#65292;&#26088;&#22312;&#22238;&#31572;&#20309;&#26102;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20351;&#29992;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#19979;&#28216;&#25968;&#25454;&#30340;&#22797;&#26434;&#29983;&#25104;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.16458</link><description>&lt;p&gt;
&#20309;&#26102;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65311;&#22522;&#20110;&#25968;&#25454;&#29983;&#25104;&#35270;&#35282;&#30340;&#22238;&#31572;&#65281;
&lt;/p&gt;
&lt;p&gt;
When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!. (arXiv:2303.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;W2PGNN&#65292;&#26088;&#22312;&#22238;&#31572;&#20309;&#26102;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20351;&#29992;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#19979;&#28216;&#25968;&#25454;&#30340;&#22797;&#26434;&#29983;&#25104;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#39044;&#35757;&#32451;&#22312;&#23398;&#26415;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#23581;&#35797;&#65292;&#20294;&#36127;&#38754;&#36801;&#31227;&#26159;&#23558;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26102;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#22270;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#20309;&#26102;&#39044;&#35757;&#32451;&#21644;&#22914;&#20309;&#39044;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#20505;&#26080;&#35770;&#31574;&#30053;&#22914;&#20309;&#20808;&#36827;&#65292;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#20173;&#28982;&#26080;&#27861;&#24102;&#26469;&#26126;&#26174;&#30340;&#22909;&#22788;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;W2PGNN&#26469;&#22238;&#31572;&#20309;&#26102;&#39044;&#35757;&#32451;&#30340;&#20851;&#38190;&#38382;&#39064;&#65288;&#21363;&#25105;&#20204;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#21487;&#20197;&#21033;&#29992;&#22270;&#39044;&#35757;&#32451;&#65289;&#65292;&#28982;&#21518;&#20877;&#36827;&#34892;&#36153;&#21147;&#30340;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#19979;&#28216;&#25968;&#25454;&#30340;&#22797;&#26434;&#29983;&#25104;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, graph pre-training has attracted wide research attention, which aims to learn transferable knowledge from unlabeled graph data so as to improve downstream performance. Despite these recent attempts, the negative transfer is a major issue when applying graph pre-trained models to downstream tasks. Existing works made great efforts on the issue of what to pre-train and how to pre-train by designing a number of graph pre-training and fine-tuning strategies. However, there are indeed cases where no matter how advanced the strategy is, the "pre-train and fine-tune" paradigm still cannot achieve clear benefits. This paper introduces a generic framework W2PGNN to answer the crucial question of when to pre-train (i.e., in what situations could we take advantage of graph pre-training) before performing effortful pre-training or fine-tuning. We start from a new perspective to explore the complex generative mechanisms from the pre-training data to downstream data. In particular, W2PGNN 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#20869;&#37096;&#27979;&#37327;&#20013;&#37325;&#26500;&#26925;&#22278;&#38382;&#39064;&#20013;&#30340;&#23548;&#30005;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#36830;&#32493;&#21644;&#32463;&#39564;&#25439;&#22833;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#25968;&#25454;&#22122;&#22768;&#30340;&#20248;&#24322;&#31283;&#23450;&#24615;&#20197;&#21450;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16454</link><description>&lt;p&gt;
&#28151;&#21512;&#26368;&#23567;&#20108;&#20056;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#37096;&#27979;&#37327;&#19979;&#30340;&#23548;&#30005;&#29575;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Conductivity Imaging from Internal Measurements with Mixed Least-Squares Deep Neural Networks. (arXiv:2303.16454v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#20869;&#37096;&#27979;&#37327;&#20013;&#37325;&#26500;&#26925;&#22278;&#38382;&#39064;&#20013;&#30340;&#23548;&#30005;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#36830;&#32493;&#21644;&#32463;&#39564;&#25439;&#22833;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#25968;&#25454;&#22122;&#22768;&#30340;&#20248;&#24322;&#31283;&#23450;&#24615;&#20197;&#21450;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#19968;&#20010;&#20869;&#37096;&#27979;&#37327;&#20013;&#37325;&#26500;&#26925;&#22278;&#38382;&#39064;&#20013;&#30340;&#23548;&#30005;&#29575;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#25511;&#21046;&#26041;&#31243;&#30340;&#28151;&#21512;&#25913;&#36896;&#65292;&#24182;&#21033;&#29992;&#26631;&#20934;&#30340;&#26368;&#23567;&#20108;&#20056;&#30446;&#26631;&#20989;&#25968;&#21516;&#26102;&#36817;&#20284;&#23548;&#30005;&#29575;&#21644;&#36890;&#37327;&#65292;&#20197;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#35797;&#25506;&#20989;&#25968;&#12290;&#25105;&#20204;&#23545;&#36830;&#32493;&#21644;&#32463;&#39564;&#25439;&#22833;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#21253;&#25324;&#36890;&#36807;&#22122;&#22768;&#27700;&#24179;&#12289;&#21508;&#31181;&#24809;&#32602;&#21442;&#25968;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21442;&#25968;&#65288;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#21442;&#25968;&#36793;&#30028;&#65289;&#26174;&#24335;&#22320;&#20272;&#35745;&#35823;&#24046;&#30340;&#20005;&#26684;&#20272;&#35745;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#35813;&#26041;&#27861;&#30340;&#19981;&#21516;&#29305;&#28857;&#65292;&#20363;&#22914;&#23545;&#20110;&#25968;&#25454;&#22122;&#22768;&#30340;&#20248;&#24322;&#31283;&#23450;&#24615;&#20197;&#21450;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we develop a novel approach using deep neural networks to reconstruct the conductivity distribution in elliptic problems from one internal measurement. The approach is based on a mixed reformulation of the governing equation and utilizes the standard least-squares objective to approximate the conductivity and flux simultaneously, with deep neural networks as ansatz functions. We provide a thorough analysis of the neural network approximations for both continuous and empirical losses, including rigorous error estimates that are explicit in terms of the noise level, various penalty parameters and neural network architectural parameters (depth, width and parameter bound). We also provide extensive numerical experiments in two- and multi-dimensions to illustrate distinct features of the approach, e.g., excellent stability with respect to data noise and capability of solving high-dimensional problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#22635;&#20805;&#20013;&#38388;&#34507;&#30333;&#24207;&#21015;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;SEIFER&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#22635;&#20805;&#20013;&#38388;&#21464;&#25442;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(ProtFIM)&#65292;&#26356;&#36866;&#21512;&#20110;&#34507;&#30333;&#36136;&#24037;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.16452</link><description>&lt;p&gt;
ProtFIM&#65306;&#22522;&#20110;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#22635;&#20805;&#20013;&#38388;&#34507;&#30333;&#24207;&#21015;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
ProtFIM: Fill-in-Middle Protein Sequence Design via Protein Language Models. (arXiv:2303.16452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#22635;&#20805;&#20013;&#38388;&#34507;&#30333;&#24207;&#21015;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;SEIFER&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#22635;&#20805;&#20013;&#38388;&#21464;&#25442;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(ProtFIM)&#65292;&#26356;&#36866;&#21512;&#20110;&#34507;&#30333;&#36136;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#65292;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;(PLMs)&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#24037;&#20855;&#12290;&#20294;&#26159;&#22312;&#23454;&#38469;&#30340;&#34507;&#30333;&#36136;&#24037;&#31243;&#20013;&#65292;&#26377;&#35768;&#22810;&#24773;&#20917;&#26159;&#22312;&#20445;&#25345;&#20854;&#23427;&#27531;&#22522;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#34507;&#30333;&#36136;&#20013;&#38388;&#30340;&#27688;&#22522;&#37240;&#12290;&#30001;&#20110;PLMs&#30340;&#20174;&#24038;&#21040;&#21491;&#30340;&#24615;&#36136;&#65292;&#29616;&#26377;&#30340;PLMs&#36890;&#36807;&#20419;&#20351;&#21069;&#32512;&#27531;&#22522;&#26469;&#20462;&#25913;&#21518;&#32512;&#27531;&#22522;&#65292;&#36825;&#23545;&#20110;&#32771;&#34385;&#25972;&#20010;&#21608;&#22260;&#32972;&#26223;&#30340;&#22635;&#20805;&#20219;&#21153;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20026;&#20102;&#25214;&#21040;&#26356;&#26377;&#25928;&#30340;PLMs&#29992;&#20110;&#34507;&#30333;&#36136;&#24037;&#31243;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;&#20108;&#32423;&#32467;&#26500;&#20013;&#38388;&#22635;&#20805;&#8212;&#8212;SEIFER&#65292;&#23427;&#36817;&#20284;&#20110;&#22635;&#20805;&#24207;&#21015;&#35774;&#35745;&#22330;&#26223;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#27169;&#22411;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#22635;&#20805;&#20013;&#38388;&#21464;&#25442;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(&#31216;&#20026;ProtFIM)&#65292;&#26356;&#36866;&#21512;&#20110;&#34507;&#30333;&#36136;&#24037;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProtFIM&#29983;&#25104;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#20013;&#38388;&#37096;&#20998;&#65292;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#32467;&#26500;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models (pLMs), pre-trained via causal language modeling on protein sequences, have been a promising tool for protein sequence design. In real-world protein engineering, there are many cases where the amino acids in the middle of a protein sequence are optimized while maintaining other residues. Unfortunately, because of the left-to-right nature of pLMs, existing pLMs modify suffix residues by prompting prefix residues, which are insufficient for the infilling task that considers the whole surrounding context. To find the more effective pLMs for protein engineering, we design a new benchmark, Secondary structureE InFilling rEcoveRy, SEIFER, which approximates infilling sequence design scenarios. With the evaluation of existing models on the benchmark, we reveal the weakness of existing language models and show that language models trained via fill-in-middle transformation, called ProtFIM, are more appropriate for protein engineering. Also, we prove that ProtFIM generate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Product Autoencoder&#26469;&#36890;&#36807;&#21487;&#31649;&#29702;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#23454;&#29616;&#30456;&#23545;&#36739;&#22823;&#30340;&#20195;&#30721;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.16424</link><description>&lt;p&gt;
ProductAE&#65306;&#38754;&#21521;&#22823;&#32500;&#25968;&#30340;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#32416;&#38169;&#30721;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
ProductAE: Toward Deep Learning Driven Error-Correction Codes of Large Dimensions. (arXiv:2303.16424v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Product Autoencoder&#26469;&#36890;&#36807;&#21487;&#31649;&#29702;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#23454;&#29616;&#30456;&#23545;&#36739;&#22823;&#30340;&#20195;&#30721;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20960;&#21313;&#24180;&#30340;&#29702;&#35770;&#30740;&#31350;&#24050;&#32463;&#21457;&#26126;&#20102;&#20960;&#20010;&#32416;&#38169;&#30721;&#31867;&#21035;&#65292;&#20294;&#36825;&#20123;&#30721;&#30340;&#35774;&#35745;&#21364;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#20381;&#38752;&#20154;&#31867;&#26234;&#24935;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#35774;&#35745;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24037;&#20855;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#21644;&#21152;&#36895;&#65292;&#20174;&#32780;&#23454;&#29616;&#19982;&#32463;&#20856;&#35774;&#35745;&#30456;&#27604;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#22686;&#30410;&#30340;ML&#39537;&#21160;&#30340;&#32416;&#38169;&#30721;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#25361;&#25112;&#26159;&#65292;&#23545;&#20110;&#22823;&#30721;&#32500;&#24230;&#26469;&#35828;&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#23436;&#20840;&#30340;ML&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#26469;&#35828;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#65292;&#22914;&#26524;&#19981;&#26159;&#19981;&#21487;&#33021;&#30340;&#30340;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Product Autoencoder&#65288;ProductAE&#65289;-&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#65288;&#32534;&#30721;&#22120;&#65292;&#35299;&#30721;&#22120;&#65289;&#23545;&#30340;&#31995;&#21015;-&#26088;&#22312;&#36890;&#36807;&#21487;&#31649;&#29702;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#23454;&#29616;&#30456;&#23545;&#36739;&#22823;&#30340;&#20195;&#30721;&#65288;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65289;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#32463;&#20856;&#20056;&#31215;&#30721;&#30340;&#24605;&#24819;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;ProductAE&#26500;&#24314;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
While decades of theoretical research have led to the invention of several classes of error-correction codes, the design of such codes is an extremely challenging task, mostly driven by human ingenuity. Recent studies demonstrate that such designs can be effectively automated and accelerated via tools from machine learning (ML), thus enabling ML-driven classes of error-correction codes with promising performance gains compared to classical designs. A fundamental challenge, however, is that it is prohibitively complex, if not impossible, to design and train fully ML-driven encoder and decoder pairs for large code dimensions. In this paper, we propose Product Autoencoder (ProductAE) -- a computationally-efficient family of deep learning driven (encoder, decoder) pairs -- aimed at enabling the training of relatively large codes (both encoder and decoder) with a manageable training complexity. We build upon ideas from classical product codes and propose constructing large neural codes usin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#20083;&#33146;X&#20809;&#26816;&#26597;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23384;&#22312;&#38382;&#39064;&#21644;&#24555;&#25463;&#26041;&#24335;&#65292;&#24314;&#35758;&#21033;&#29992;&#37319;&#26679;&#26041;&#24335;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23545;&#30456;&#24212;&#30340;&#35780;&#20272;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;</title><link>http://arxiv.org/abs/2303.16417</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#20083;&#33146;X&#20809;&#26816;&#26597;&#20013;&#30340;&#38382;&#39064;&#21644;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Problems and shortcuts in deep learning for screening mammography. (arXiv:2303.16417v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#20083;&#33146;X&#20809;&#26816;&#26597;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23384;&#22312;&#38382;&#39064;&#21644;&#24555;&#25463;&#26041;&#24335;&#65292;&#24314;&#35758;&#21033;&#29992;&#37319;&#26679;&#26041;&#24335;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23545;&#30456;&#24212;&#30340;&#35780;&#20272;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#26410;&#34987;&#21457;&#29616;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#65288;1&#65289;&#35782;&#21035;&#20986;&#21487;&#33021;&#20250;&#25552;&#39640;&#24615;&#33021;&#30340;&#38382;&#39064;&#20197;&#21450;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#65288;2&#65289;&#25552;&#20986;&#20102;&#35757;&#32451;&#21644;&#20998;&#26512;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;120,112&#20010;&#32654;&#22269;&#20083;&#33146;&#26816;&#26597;&#21644;16,693&#20010;&#33521;&#22269;&#20083;&#33146;&#26816;&#26597;&#36827;&#34892;&#20102;&#30284;&#30151;&#20998;&#31867;&#30340;AI&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20010;&#20165;&#20351;&#29992;&#26080;&#20083;&#25151;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#29305;&#23450;&#27979;&#35797;&#38598;&#19978;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;AUC&#20998;&#25968;&#30340;&#24726;&#35770;&#29616;&#35937;&#20986;&#29616;&#22312;&#20102;&#20004;&#20010;&#22269;&#23478;&#30340;&#27979;&#35797;&#38598;&#19978;&#65292;&#65288;0.838&#21644;0.892&#65289;&#65292;&#20294;&#26159;&#22312;&#32852;&#21512;&#27979;&#35797;&#38598;&#19978;AUC&#20998;&#25968;&#39640;&#36798;0.945&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31561;&#37327;&#37319;&#26679;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#30284;&#30151;&#25968;&#25454;&#21487;&#20197;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work reveals undiscovered challenges in the performance and generalizability of deep learning models. We (1) identify spurious shortcuts and evaluation issues that can inflate performance and (2) propose training and analysis methods to address them.  We trained an AI model to classify cancer on a retrospective dataset of 120,112 US exams (3,467 cancers) acquired from 2008 to 2017 and 16,693 UK exams (5,655 cancers) acquired from 2011 to 2015.  We evaluated on a screening mammography test set of 11,593 US exams (102 cancers; 7,594 women; age 57.1 \pm 11.0) and 1,880 UK exams (590 cancers; 1,745 women; age 63.3 \pm 7.2). A model trained on images of only view markers (no breast) achieved a 0.691 AUC. The original model trained on both datasets achieved a 0.945 AUC on the combined US+UK dataset but paradoxically only 0.838 and 0.892 on the US and UK datasets, respectively. Sampling cancers equally from both datasets during training mitigated this shortcut. A similar AUC paradox (0.9
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#23646;&#24615;&#21644;&#21270;&#23398;&#25968;&#25454;&#39044;&#27979;&#20808;&#36827;&#26448;&#26009;&#30340;&#29305;&#24615;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#20449;&#24687;&#31354;&#38388;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#20248;&#21183;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;114210&#31181;&#32452;&#25104;&#26465;&#20214;&#19979;&#30340;913680&#20010;&#23646;&#24615;&#25968;&#25454;&#28857;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#26448;&#26009;&#21644;&#23610;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16412</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#20808;&#36827;&#26448;&#26009;&#22810;&#31181;&#29305;&#24615;&#30340;&#32508;&#21512;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive and Versatile Multimodal Deep Learning Approach for Predicting Diverse Properties of Advanced Materials. (arXiv:2303.16412v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#23646;&#24615;&#21644;&#21270;&#23398;&#25968;&#25454;&#39044;&#27979;&#20808;&#36827;&#26448;&#26009;&#30340;&#29305;&#24615;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#20449;&#24687;&#31354;&#38388;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#20248;&#21183;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;114210&#31181;&#32452;&#25104;&#26465;&#20214;&#19979;&#30340;913680&#20010;&#23646;&#24615;&#25968;&#25454;&#28857;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#26448;&#26009;&#21644;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#65288;MDL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21512;&#24182;&#29289;&#29702;&#23646;&#24615;&#21644;&#21270;&#23398;&#25968;&#25454;&#26469;&#39044;&#27979;10&#32500;&#19993;&#28911;&#37240;&#32858;&#21512;&#29289;&#22797;&#21512;&#26448;&#26009;&#30340;&#29289;&#29702;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;MDL&#27169;&#22411;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65292;&#20854;&#20013;&#19977;&#20010;&#27169;&#22359;&#26159;&#29992;&#20110;&#26448;&#26009;&#32467;&#26500;&#34920;&#24449;&#30340;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#31532;&#22235;&#20010;&#27169;&#22359;&#29992;&#20110;&#23646;&#24615;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22788;&#29702;&#20102;18&#32500;&#30340;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;10&#20010;&#32452;&#25104;&#36755;&#20837;&#21644;8&#20010;&#23646;&#24615;&#36755;&#20986;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;114210&#31181;&#32452;&#25104;&#26465;&#20214;&#19979;&#30340;913680&#20010;&#23646;&#24615;&#25968;&#25454;&#28857;&#12290;&#23545;&#20110;&#35745;&#31639;&#26448;&#26009;&#31185;&#23398;&#29305;&#21035;&#26159;&#23545;&#20110;&#32467;&#26500;&#26410;&#23450;&#20041;&#30340;&#26448;&#26009;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#26159;&#21069;&#25152;&#26410;&#26377;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#20998;&#26512;&#39640;&#32500;&#20449;&#24687;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#36870;&#21521;&#26448;&#26009;&#35774;&#35745;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181;&#26448;&#26009;&#21644;&#23610;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21069;&#25552;&#26159;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#12290;&#36825;&#39033;&#30740;&#31350;&#25512;&#21160;&#20102;&#26410;&#26469;&#30740;&#31350;&#19981;&#21516;&#26448;&#26009;&#21644;&#24320;&#21457;&#26356;&#22797;&#26434;&#30340;MDL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a multimodal deep learning (MDL) framework for predicting physical properties of a 10-dimensional acrylic polymer composite material by merging physical attributes and chemical data. Our MDL model comprises four modules, including three generative deep learning models for material structure characterization and a fourth model for property prediction. Our approach handles an 18-dimensional complexity, with 10 compositional inputs and 8 property outputs, successfully predicting 913,680 property data points across 114,210 composition conditions. This level of complexity is unprecedented in computational materials science, particularly for materials with undefined structures. We propose a framework to analyze the high-dimensional information space for inverse material design, demonstrating flexibility and adaptability to various materials and scales, provided sufficient data is available. This study advances future research on different materials and the development of more soph
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LMDA-Net&#30340;&#36731;&#37327;&#32423;&#22810;&#32500;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#26088;&#22312;&#25552;&#39640;&#36890;&#29992;EEG&#33041;&#26426;&#25509;&#21475;&#33539;&#20363;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#35813;&#32593;&#32476;&#20351;&#29992;&#20004;&#20010;&#26032;&#22411;&#27880;&#24847;&#21147;&#27169;&#22359;&#8212;&#8212;&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#28145;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#24320;&#39640;&#24433;&#21709;&#21147;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LMDA-Net&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20195;&#34920;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16407</link><description>&lt;p&gt;
LMDA-Net: &#29992;&#20110;&#36890;&#29992;EEG&#33041;&#26426;&#25509;&#21475;&#33539;&#20363;&#30340;&#36731;&#37327;&#32423;&#22810;&#32500;&#27880;&#24847;&#21147;&#32593;&#32476;&#21450;&#20854;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
LMDA-Net:A lightweight multi-dimensional attention network for general EEG-based brain-computer interface paradigms and interpretability. (arXiv:2303.16407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LMDA-Net&#30340;&#36731;&#37327;&#32423;&#22810;&#32500;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#26088;&#22312;&#25552;&#39640;&#36890;&#29992;EEG&#33041;&#26426;&#25509;&#21475;&#33539;&#20363;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#35813;&#32593;&#32476;&#20351;&#29992;&#20004;&#20010;&#26032;&#22411;&#27880;&#24847;&#21147;&#27169;&#22359;&#8212;&#8212;&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#28145;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#24320;&#39640;&#24433;&#21709;&#21147;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LMDA-Net&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20195;&#34920;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;EEG&#30340;&#27963;&#21160;&#21644;&#29366;&#24577;&#35782;&#21035;&#38656;&#35201;&#20351;&#29992;&#31070;&#32463;&#31185;&#23398;&#20808;&#21069;&#30693;&#35782;&#29983;&#25104;&#23450;&#37327;EEG&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;BCI&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20250;&#36935;&#21040;&#35832;&#22914;&#36328;&#25968;&#25454;&#38598;&#30340;&#36890;&#29992;&#24615;&#24046;&#12289;&#39640;&#39044;&#27979;&#27874;&#21160;&#24615;&#21644;&#20302;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#22810;&#32500;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#31216;&#20026;LMDA-Net&#12290;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#19987;&#20026;EEG&#20449;&#21495;&#35774;&#35745;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#27169;&#22359;&#8212;&#8212;&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#28145;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;LMDA-Net&#21487;&#20197;&#26377;&#25928;&#22320;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#32500;&#24230;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#21508;&#31181;BCI&#20219;&#21153;&#19978;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290; LMDA-Net&#22312;&#21253;&#25324;&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#21644;P300-Speller&#33539;&#20363;&#22312;&#20869;&#30340;&#22235;&#20010;&#20844;&#24320;&#39640;&#24433;&#21709;&#21147;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20854;&#20182;&#20195;&#34920;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LMDA-Net&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20195;&#34920;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
EEG-based recognition of activities and states involves the use of prior neuroscience knowledge to generate quantitative EEG features, which may limit BCI performance. Although neural network-based methods can effectively extract features, they often encounter issues such as poor generalization across datasets, high predicting volatility, and low model interpretability. Hence, we propose a novel lightweight multi-dimensional attention network, called LMDA-Net. By incorporating two novel attention modules designed specifically for EEG signals, the channel attention module and the depth attention module, LMDA-Net can effectively integrate features from multiple dimensions, resulting in improved classification performance across various BCI tasks. LMDA-Net was evaluated on four high-impact public datasets, including motor imagery (MI) and P300-Speller paradigms, and was compared with other representative models. The experimental results demonstrate that LMDA-Net outperforms other represen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;HiREST&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#23558;&#20998;&#23618;&#20449;&#24687;&#26816;&#32034;&#21644;&#35270;&#35273;/&#25991;&#26412;&#36880;&#27493;&#24635;&#32467;&#20174;&#25945;&#23398;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#25512;&#24191;&#65292;&#20351;&#24471;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#19979;&#21487;&#20197;&#20849;&#21516;&#25628;&#32034;&#35270;&#39057;&#35821;&#26009;&#24211;&#65292;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2303.16406</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;&#35270;&#39057;&#30636;&#38388;&#26816;&#32034;&#21644;&#20998;&#27493;&#23383;&#24149;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Video-Moment Retrieval and Step-Captioning. (arXiv:2303.16406v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16406
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;HiREST&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#23558;&#20998;&#23618;&#20449;&#24687;&#26816;&#32034;&#21644;&#35270;&#35273;/&#25991;&#26412;&#36880;&#27493;&#24635;&#32467;&#20174;&#25945;&#23398;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#25512;&#24191;&#65292;&#20351;&#24471;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#19979;&#21487;&#20197;&#20849;&#21516;&#25628;&#32034;&#35270;&#39057;&#35821;&#26009;&#24211;&#65292;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20154;&#20204;&#22312;&#23547;&#25214;&#22823;&#22411;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#30340;&#20449;&#24687;&#26041;&#38754;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#29420;&#31435;&#30740;&#31350;&#20102;&#30456;&#20851;&#20219;&#21153;&#65292;&#22914;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#26816;&#32034;&#12289;&#30636;&#38388;&#26816;&#32034;&#12289;&#35270;&#39057;&#25688;&#35201;&#21644;&#35270;&#39057;&#23383;&#24149;&#65292;&#27809;&#26377;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#21487;&#20197;&#20849;&#21516;&#25628;&#32034;&#35270;&#39057;&#35821;&#26009;&#24211;&#65292;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;&#36825;&#26679;&#30340;&#31471;&#21040;&#31471;&#35774;&#32622;&#23558;&#20801;&#35768;&#35768;&#22810;&#26377;&#36259;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20363;&#22914;&#22522;&#20110;&#25991;&#26412;&#30340;&#25628;&#32034;&#65292;&#20174;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#30456;&#20851;&#30340;&#35270;&#39057;&#65292;&#25552;&#21462;&#26368;&#30456;&#20851;&#30340;&#30636;&#38388;&#65292;&#24182;&#23558;&#30636;&#38388;&#20998;&#25104;&#37325;&#35201;&#30340;&#27493;&#39588;&#65292;&#24182;&#21152;&#19978;&#23383;&#24149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiREST(Hierarchical REtrieval and STep-captioning)&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#25945;&#23398;&#35270;&#39057;&#35821;&#26009;&#24211;&#30340;&#20998;&#23618;&#20449;&#24687;&#26816;&#32034;&#21644;&#35270;&#35273;/&#25991;&#26412;&#20998;&#38454;&#27573;&#24635;&#32467;&#12290;HiREST&#30001;&#26469;&#33258;&#25945;&#23398;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;3.4K&#20010;&#25991;&#26412;-&#35270;&#39057;&#23545;&#32452;&#25104;&#65292;&#20854;&#20013;1.1K&#20010;&#35270;&#39057;&#20855;&#26377;&#19982;&#25991;&#26412;&#26597;&#35810;&#30456;&#20851;&#30340;&#30636;&#38388;&#36328;&#24230;&#27880;&#37322;&#21644;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in searching for information from large video corpora. Prior works have studied relevant tasks, such as text-based video retrieval, moment retrieval, video summarization, and video captioning in isolation, without an end-to-end setup that can jointly search from video corpora and generate summaries. Such an end-to-end setup would allow for many interesting applications, e.g., a text-based search that finds a relevant video from a video corpus, extracts the most relevant moment from that video, and segments the moment into important steps with captions. To address this, we present the HiREST (HIerarchical REtrieval and STep-captioning) dataset and propose a new benchmark that covers hierarchical information retrieval and visual/textual stepwise summarization from an instructional video corpus. HiREST consists of 3.4K text-video pairs from an instructional video dataset, where 1.1K videos have annotations of moment spans relevant to text query and breakdown of e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36830;&#25509;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#25506;&#31350;&#20102;&#36947;&#36335;&#29305;&#24449;&#23545;&#20010;&#20154;&#36229;&#36895;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24314;&#31435;&#27169;&#22411;&#39044;&#27979;&#36229;&#36895;&#27604;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#36895;&#24230;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2303.16396</link><description>&lt;p&gt;
&#21033;&#29992;&#36830;&#25509;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#35780;&#20272;&#36229;&#36895;&#39550;&#39542;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Using Connected Vehicle Trajectory Data to Evaluate the Effects of Speeding. (arXiv:2303.16396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36830;&#25509;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#25506;&#31350;&#20102;&#36947;&#36335;&#29305;&#24449;&#23545;&#20010;&#20154;&#36229;&#36895;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24314;&#31435;&#27169;&#22411;&#39044;&#27979;&#36229;&#36895;&#27604;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#36895;&#24230;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#36895;&#39550;&#39542;&#26159;&#20132;&#36890;&#20107;&#25925;&#20013;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#21508;&#20010;&#20132;&#36890;&#37096;&#38376;&#24050;&#25552;&#20986;&#36895;&#24230;&#31649;&#29702;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#20027;&#24178;&#36947;&#19978;&#30340;&#36229;&#36895;&#34892;&#39542;&#12290;&#34429;&#28982;&#24050;&#26377;&#35768;&#22810;&#20851;&#20110;&#20998;&#26512;&#36229;&#36895;&#27604;&#20363;&#30340;&#30740;&#31350;&#65292;&#20294;&#24456;&#23569;&#30740;&#31350;&#32771;&#34385;&#20102;&#36229;&#36895;&#34892;&#20026;&#23545;&#20010;&#20154;&#26053;&#31243;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#30740;&#31350;&#25152;&#21033;&#29992;&#30340;&#25506;&#27979;&#22120;&#36895;&#24230;&#25968;&#25454;&#26377;&#38480;&#65292;&#26080;&#27861;&#20102;&#35299;&#39550;&#39542;&#21592;&#25152;&#37319;&#21462;&#30340;&#36335;&#24452;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#32473;&#23450;&#26053;&#31243;&#20013;&#20010;&#20154;&#32463;&#21382;&#30340;&#21508;&#31181;&#36947;&#36335;&#29305;&#24449;&#23545;&#36229;&#36895;&#27604;&#20363;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#36830;&#25509;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#30830;&#23450;&#39550;&#39542;&#21592;&#25152;&#37319;&#21462;&#30340;&#36335;&#24452;&#65292;&#20197;&#21450;&#19982;&#36710;&#36742;&#30456;&#20851;&#30340;&#21464;&#37327;&#12290;&#20351;&#29992;&#22810;&#20010;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36229;&#36895;&#27604;&#20363;&#30340;&#27700;&#24179;&#12290;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#20026;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#65292;&#20934;&#30830;&#24230;&#20026;0.756&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#36947;&#36335;&#29305;&#24449;&#23545;&#20010;&#20154;&#36229;&#36895;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#20026;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#36895;&#24230;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speeding has been and continues to be a major contributing factor to traffic fatalities. Various transportation agencies have proposed speed management strategies to reduce the amount of speeding on arterials. While there have been various studies done on the analysis of speeding proportions above the speed limit, few studies have considered the effect on the individual's journey. Many studies utilized speed data from detectors, which is limited in that there is no information of the route that the driver took. This study aims to explore the effects of various roadway features an individual experiences for a given journey on speeding proportions. Connected vehicle trajectory data was utilized to identify the path that a driver took, along with the vehicle related variables. The level of speeding proportion is predicted using multiple learning models. The model with the best performance, Extreme Gradient Boosting, achieved an accuracy of 0.756. The proposed model can be used to understa
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;&#40657;&#30418;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#21508;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#37322;&#26041;&#27861;&#24212;&#36816;&#32780;&#29983;&#65292;&#20294;&#26159;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#39044;&#27979;&#27491;&#30830;&#65292;&#20063;&#21487;&#33021;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#23398;&#20064;&#26694;&#26550;&#65306;&#20998;&#24067;&#31283;&#20581;&#35299;&#37322;(DRE)&#65292;&#23427;&#21487;&#20197;&#22312;&#36234;&#30028;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#24378;&#20581;&#30340;&#35299;&#37322;&#20449;&#21495;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16390</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#37322;&#26159;&#21542;&#23545;&#36234;&#30028;&#25968;&#25454;&#20855;&#26377;&#31283;&#20581;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Data-driven Explanations Robust against Out-of-distribution Data?. (arXiv:2303.16390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16390
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#40657;&#30418;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#21508;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#37322;&#26041;&#27861;&#24212;&#36816;&#32780;&#29983;&#65292;&#20294;&#26159;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#39044;&#27979;&#27491;&#30830;&#65292;&#20063;&#21487;&#33021;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#23398;&#20064;&#26694;&#26550;&#65306;&#20998;&#24067;&#31283;&#20581;&#35299;&#37322;(DRE)&#65292;&#23427;&#21487;&#20197;&#22312;&#36234;&#30028;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#24378;&#20581;&#30340;&#35299;&#37322;&#20449;&#21495;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#40657;&#30418;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;&#21508;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#26029;&#21463;&#21040;&#20998;&#24067;&#36716;&#31227;&#30340;&#25361;&#25112;&#12290;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#37322;&#26159;&#21542;&#23545;&#36234;&#30028;&#25968;&#25454;&#20855;&#26377;&#31283;&#20581;&#24615;&#65311;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#39044;&#27979;&#27491;&#30830;&#65292;&#20063;&#21487;&#33021;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;&#22914;&#20309;&#24320;&#21457;&#38024;&#23545;&#36234;&#30028;&#25968;&#25454;&#30340;&#24378;&#20581;&#35299;&#37322;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#26080;&#20851;&#23398;&#20064;&#26694;&#26550;&#65306;&#20998;&#24067;&#24335;&#31283;&#20581;&#35299;&#37322;(DRE)&#12290;&#20851;&#38190;&#24605;&#36335;&#26159;&#21463;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#20805;&#20998;&#21033;&#29992;&#30456;&#20114;&#20998;&#24067;&#30340;&#20449;&#24687;&#65292;&#20026;&#35299;&#37322;&#30340;&#23398;&#20064;&#25552;&#20379;&#30417;&#30563;&#20449;&#21495;&#65292;&#26080;&#38656;&#20154;&#31867;&#27880;&#37322;&#12290;&#24378;&#20581;&#30340;&#35299;&#37322;&#33021;&#21542;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65311;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
As black-box models increasingly power high-stakes applications, a variety of data-driven explanation methods have been introduced. Meanwhile, machine learning models are constantly challenged by distributional shifts. A question naturally arises: Are data-driven explanations robust against out-of-distribution data? Our empirical results show that even though predict correctly, the model might still yield unreliable explanations under distributional shifts. How to develop robust explanations against out-of-distribution data? To address this problem, we propose an end-to-end model-agnostic learning framework Distributionally Robust Explanations (DRE). The key idea is, inspired by self-supervised learning, to fully utilizes the inter-distribution information to provide supervisory signals for the learning of explanations without human annotation. Can robust explanations benefit the model's generalization capability? We conduct extensive experiments on a wide range of tasks and data types
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#36136;&#22810;&#22771;&#25193;&#25955;&#21152;&#26435;MRI&#20272;&#35745;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#20989;&#25968;&#30340;&#21333;&#38454;&#27573;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#25195;&#25551;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16376</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#24322;&#36136;&#22810;&#22771;&#25193;&#25955;&#21152;&#26435;MRI&#20272;&#35745;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#20989;&#25968;&#30340;&#21333;&#38454;&#27573;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Unified Single-stage Learning Model for Estimating Fiber Orientation Distribution Functions on Heterogeneous Multi-shell Diffusion-weighted MRI. (arXiv:2303.16376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#36136;&#22810;&#22771;&#25193;&#25955;&#21152;&#26435;MRI&#20272;&#35745;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#20989;&#25968;&#30340;&#21333;&#38454;&#27573;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#25195;&#25551;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#21152;&#26435;MRI&#36890;&#36807;&#20854;&#22312;q&#31354;&#38388;&#20013;&#30340;&#39057;&#35889;&#27979;&#37327;&#27599;&#20010;&#20307;&#32032;&#20013;&#26412;&#22320;&#25193;&#25955;&#36807;&#31243;&#30340;&#26041;&#21521;&#21644;&#23610;&#24230;&#65292;&#36890;&#24120;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;shell&#20013;&#33719;&#21462;&#12290; &#26368;&#36817;&#22312;&#24494;&#32467;&#26500;&#25104;&#20687;&#21644;&#22810;&#32452;&#32455;&#20998;&#35299;&#26041;&#38754;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20449;&#21495;&#30340;&#24452;&#21521;b&#20540;&#20381;&#36182;&#24615;&#30340;&#20851;&#27880;&#12290; &#22240;&#27492;&#65292;&#22312;&#32452;&#32455;&#20998;&#31867;&#21644;&#24494;&#35266;&#32467;&#26500;&#20272;&#35745;&#26041;&#38754;&#65292;&#38656;&#35201;&#25193;&#23637;&#24452;&#21521;&#21644;&#35282;&#21521;&#22495;&#30340;&#20449;&#21495;&#34920;&#31034;&#12290; &#25552;&#20986;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#27169;&#25311;DW-MRI&#20449;&#21495;&#19982;&#29983;&#29289;&#24494;&#32467;&#26500;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290; &#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22810;&#22771;&#22810;&#32452;&#32455;&#32422;&#26463;&#29699;&#24418;&#21435;&#21367;&#31215;&#65289;&#20855;&#26377;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#25195;&#25551;&#19968;&#33268;&#24615;&#12290; &#28982;&#32780;&#65292;&#30001;&#20110;&#23398;&#20064;&#36807;&#31243;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25193;&#25955;&#36807;&#31243;&#21644;&#32452;&#32455;&#24494;&#32467;&#26500;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#22240;&#27492;&#36890;&#24120;&#38656;&#35201;&#22810;&#38454;&#27573;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-weighted (DW) MRI measures the direction and scale of the local diffusion process in every voxel through its spectrum in q-space, typically acquired in one or more shells. Recent developments in micro-structure imaging and multi-tissue decomposition have sparked renewed attention to the radial b-value dependence of the signal. Applications in tissue classification and micro-architecture estimation, therefore, require a signal representation that extends over the radial as well as angular domain. Multiple approaches have been proposed that can model the non-linear relationship between the DW-MRI signal and biological microstructure. In the past few years, many deep learning-based methods have been developed towards faster inference speed and higher inter-scan consistency compared with traditional model-based methods (e.g., multi-shell multi-tissue constrained spherical deconvolution). However, a multi-stage learning strategy is typically required since the learning process rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;</title><link>http://arxiv.org/abs/2303.16372</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#30340;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#21644;&#24230;&#37327;&#38544;&#31169;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#19979;&#30028;&#65292;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#65292;&#19988;&#25193;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#38544;&#31169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#26102;&#31169;&#26377;&#23398;&#20064;&#31639;&#27861;&#30340;&#35821;&#20041;&#20445;&#35777;&#24378;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23548;&#20986;&#38750;&#28176;&#36827;&#37327;&#32423;&#19979;&#30028;&#26469;&#30740;&#31350;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21644;&#24230;&#37327;&#38544;&#31169;&#65288;mDP&#65289;&#30340;&#23398;&#20064;&#22120;&#23545;&#25239;&#32773;&#37325;&#26500;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#23545;mDP&#30340;&#20998;&#26512;&#35206;&#30422;&#20102;&#39640;&#32500;&#24773;&#20917;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#23545;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;DP-SGD&#21644;Projected Noisy SGD&#36827;&#34892;&#20102;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#25193;&#23637;&#38544;&#31169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#20174;&#38543;&#26426;&#29366;&#24577;&#31354;&#38388;&#31995;&#32479;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;/&#25968;&#25454;&#20013;&#36827;&#34892;&#26497;&#22823;&#20284;&#28982;&#65288;ML&#65289;&#24179;&#28369;&#20272;&#35745;&#65292;&#25152;&#24471;&#21040;&#30340;&#20272;&#35745;&#32467;&#26524;&#27604;&#20256;&#32479;&#26041;&#27861;&#31934;&#24230;&#26356;&#39640;&#12290;&#31639;&#27861;&#22522;&#20110;EM&#26799;&#24230;&#31890;&#23376;&#31639;&#27861;&#65292;&#36882;&#24402;&#22320;&#36827;&#34892;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.16364</link><description>&lt;p&gt;
&#38544;&#34255;&#20449;&#24687;&#19979;&#22522;&#20110;&#26497;&#22823;&#20284;&#28982;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24179;&#28369;&#20272;&#35745;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood smoothing estimation in state-space models: An incomplete-information based approach. (arXiv:2303.16364v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#20174;&#38543;&#26426;&#29366;&#24577;&#31354;&#38388;&#31995;&#32479;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;/&#25968;&#25454;&#20013;&#36827;&#34892;&#26497;&#22823;&#20284;&#28982;&#65288;ML&#65289;&#24179;&#28369;&#20272;&#35745;&#65292;&#25152;&#24471;&#21040;&#30340;&#20272;&#35745;&#32467;&#26524;&#27604;&#20256;&#32479;&#26041;&#27861;&#31934;&#24230;&#26356;&#39640;&#12290;&#31639;&#27861;&#22522;&#20110;EM&#26799;&#24230;&#31890;&#23376;&#31639;&#27861;&#65292;&#36882;&#24402;&#22320;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;Rauch &#65288;1963&#24180;&#65289;&#21644;et al.&#65288;1965&#24180;&#65289;&#30340;&#32463;&#20856;&#20316;&#21697;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38543;&#26426;&#29366;&#24577;&#31354;&#38388;&#31995;&#32479;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;/&#25968;&#25454;&#20013;&#36827;&#34892;&#26497;&#22823;&#20284;&#28982;&#65288;ML&#65289;&#24179;&#28369;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#20171;&#32461;&#20102;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24471;&#20998;&#20989;&#25968;&#21644;&#26465;&#20214;&#35266;&#27979;&#20449;&#24687;&#30697;&#38453;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#30340;&#20998;&#24067;&#24335;&#36523;&#20221;&#12290;&#21033;&#29992;&#36825;&#20123;&#36523;&#20221;&#65292;&#25552;&#20986;&#20102;ML smoother $\widehat{x}_{k\vert n}^s =\argmax_{x_k} \log f(x_k,\widehat{x}_{k+1\vert n}^s, y_{0:n}\vert\theta)$&#65292;$k\leq n-1$&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21644;ML&#29366;&#24577;&#20272;&#35745;$\widehat{x}_k=\argmax_{x_k} \log f(x_k,y_{0:k}\vert\theta)$&#30456;&#27604;&#65292;ML&#24179;&#28369;&#20272;&#35745;&#22120;&#32473;&#20986;&#30340;&#29366;&#24577;$x_k$&#20272;&#35745;&#20855;&#26377;&#26356;&#23569;&#30340;&#26631;&#20934;&#35823;&#24046;&#65292;&#24182;&#19988;&#26356;&#31526;&#21512;&#23545;&#25968;&#20284;&#28982;&#12290;zhi&#12290;&#22522;&#20110;EM&#26799;&#24230;&#31890;&#23376;&#31639;&#27861;&#32473;&#20986;&#36882;&#24402;&#20272;&#35745;&#65292;&#35813;&#31639;&#27861;&#25193;&#23637;&#20102;\cite{Lange}&#30340;ML&#24179;&#28369;&#20272;&#35745;&#24037;&#20316;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#26126;&#30830;&#30340;&#36845;&#20195;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits classical works of Rauch (1963, et al. 1965) and develops a novel method for maximum likelihood (ML) smoothing estimation from incomplete information/data of stochastic state-space systems. Score function and conditional observed information matrices of incomplete data are introduced and their distributional identities are established. Using these identities, the ML smoother $\widehat{x}_{k\vert n}^s =\argmax_{x_k} \log f(x_k,\widehat{x}_{k+1\vert n}^s, y_{0:n}\vert\theta)$, $k\leq n-1$, is presented. The result shows that the ML smoother gives an estimate of state $x_k$ with more adherence of loglikehood having less standard errors than that of the ML state estimator $\widehat{x}_k=\argmax_{x_k} \log f(x_k,y_{0:k}\vert\theta)$, with $\widehat{x}_{n\vert n}^s=\widehat{x}_n$. Recursive estimation is given in terms of an EM-gradient-particle algorithm which extends the work of \cite{Lange} for ML smoothing estimation. The algorithm has an explicit iteration update whi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#21306;&#20998;ChatGPT&#29983;&#25104;&#25991;&#26412;&#21644;&#23398;&#26415;&#31185;&#23398;&#23478;&#25776;&#20889;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;2.5k&#20010;&#26679;&#26412;&#19978;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;99.2%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.16352</link><description>&lt;p&gt;
&#29992;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20197;&#36229;&#36807;99%&#30340;&#20934;&#30830;&#24230;&#21306;&#20998;ChatGPT&#21644;&#23398;&#26415;&#31185;&#23398;&#23478;&#30340;&#20316;&#32773;&#36523;&#20221;
&lt;/p&gt;
&lt;p&gt;
ChatGPT or academic scientist? Distinguishing authorship with over 99% accuracy using off-the-shelf machine learning tools. (arXiv:2303.16352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16352
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#21306;&#20998;ChatGPT&#29983;&#25104;&#25991;&#26412;&#21644;&#23398;&#26415;&#31185;&#23398;&#23478;&#25776;&#20889;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;2.5k&#20010;&#26679;&#26412;&#19978;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;99.2%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20351;&#24191;&#22823;&#32676;&#20247;&#33021;&#22815;&#35775;&#38382;AI&#29983;&#25104;&#30340;&#25991;&#31456;&#65292;&#22312;&#30701;&#30701;&#20960;&#20010;&#26376;&#20869;&#65292;&#36825;&#20010;&#20135;&#21697;&#39072;&#35206;&#20102;&#30693;&#35782;&#32463;&#27982;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#24037;&#20316;&#12289;&#23398;&#20064;&#21644;&#20889;&#20316;&#26041;&#24335;&#30340;&#25991;&#21270;&#21464;&#38761;&#12290;&#29616;&#22312;&#21306;&#20998;&#20154;&#31867;&#20889;&#20316;&#21644;AI&#20889;&#20316;&#30340;&#38656;&#27714;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#21644;&#32039;&#36843;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#31561;&#25945;&#32946;&#21644;&#23398;&#26415;&#20889;&#20316;&#31561;&#39046;&#22495;&#65292;AI&#22312;&#20043;&#21069;&#19981;&#26366;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#23041;&#32961;&#25110;&#32773;&#36129;&#29486;&#32773;&#12290;&#38024;&#23545;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21306;&#20998;ChatGPT&#29983;&#25104;&#25991;&#26412;&#21644;&#65288;&#20154;&#31867;&#65289;&#23398;&#26415;&#31185;&#23398;&#23478;&#25776;&#20889;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#20381;&#38752;&#26222;&#21450;&#19988;&#26131;&#20110;&#33719;&#21462;&#30340;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;&#65292;&#23398;&#26415;&#31185;&#23398;&#23478;&#36825;&#19968;&#29305;&#23450;&#20154;&#32676;&#30340;&#20889;&#20316;&#19982;ChatGPT&#26377;&#20309;&#19981;&#21516;&#65292;&#36825;&#31181;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#23548;&#33268;&#25105;&#20204;&#21457;&#29616;&#20102;&#29992;&#20110;&#21306;&#20998;&#65288;&#36825;&#20123;&#65289;&#20154;&#31867;&#21644;AI&#30340;&#26032;&#29305;&#24449;&#12290;&#20363;&#22914;&#65292;&#31185;&#23398;&#23478;&#32463;&#24120;&#20889;&#38271;&#27573;&#33853;&#65292;&#21916;&#27426;&#20351;&#29992;&#27169;&#31946;&#35821;&#35328;&#65292;&#32463;&#24120;&#20351;&#29992;but&#12289;however&#21644;although&#31561;&#35789;&#27719;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;2.5k&#20010;&#26679;&#26412;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;99.2%&#30340;&#20934;&#30830;&#29575;&#65292;&#21306;&#20998;ChatGPT&#21644;&#23398;&#26415;&#31185;&#23398;&#23478;&#30340;&#20889;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has enabled access to AI-generated writing for the masses, and within just a few months, this product has disrupted the knowledge economy, initiating a culture shift in the way people work, learn, and write. The need to discriminate human writing from AI is now both critical and urgent, particularly in domains like higher education and academic writing, where AI had not been a significant threat or contributor to authorship. Addressing this need, we developed a method for discriminating text generated by ChatGPT from (human) academic scientists, relying on prevalent and accessible supervised classification methods. We focused on how a particular group of humans, academic scientists, write differently than ChatGPT, and this targeted approach led to the discovery of new features for discriminating (these) humans from AI; as examples, scientists write long paragraphs and have a penchant for equivocal language, frequently using words like but, however, and although. With a set of 2
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#38754;&#37096;&#29305;&#24449;&#21457;&#29616;&#20102;&#20154;&#20204;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#29978;&#33267;&#21487;&#20197;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#25512;&#24191;&#12290;&#36825;&#31181;&#39044;&#27979;&#30340;&#31934;&#24230;&#27604;&#20154;&#31867;&#35780;&#20998;&#32773;&#39640;&#65292;&#30456;&#24403;&#20110;&#19968;&#20123;&#24037;&#20316;&#38754;&#35797;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16343</link><description>&lt;p&gt;
&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#21487;&#20197;&#20174;&#38754;&#37096;&#22270;&#20687;&#20013;&#26174;&#31034;&#25919;&#27835;&#21462;&#21521;&#65292;&#21363;&#20351;&#25511;&#21046;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#21644;&#33258;&#25105;&#34920;&#29616;&#12290;(arXiv: 2303.16343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Facial recognition technology can expose political orientation from facial images even when controlling for demographics and self-presentation. (arXiv:2303.16343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#38754;&#37096;&#29305;&#24449;&#21457;&#29616;&#20102;&#20154;&#20204;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#29978;&#33267;&#21487;&#20197;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#25512;&#24191;&#12290;&#36825;&#31181;&#39044;&#27979;&#30340;&#31934;&#24230;&#27604;&#20154;&#31867;&#35780;&#20998;&#32773;&#39640;&#65292;&#30456;&#24403;&#20110;&#19968;&#20123;&#24037;&#20316;&#38754;&#35797;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36816;&#29992;&#38754;&#37096;&#35782;&#21035;&#31639;&#27861;&#65292;&#20174;&#23454;&#39564;&#23460;&#35774;&#32622;&#19979;&#25293;&#25668;&#30340;591&#24352;&#20013;&#24615;&#38754;&#37096;&#22270;&#20687;&#20013;&#25552;&#21462;&#38754;&#37096;&#25551;&#36848;&#31526;&#12290;&#22312;&#25511;&#21046;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#39044;&#27979;&#21442;&#19982;&#32773;&#22312;&#25919;&#27835;&#21462;&#21521;&#37327;&#34920;&#19978;&#30340;&#24471;&#20998;(Cronbach&#30340;&#945;=0.94)&#12290;&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;r = 0.20&#65292;&#36828;&#20248;&#20110;&#20154;&#31867;&#35780;&#20998;&#32773;&#65292;&#19982;&#24037;&#20316;&#38754;&#35797;&#39044;&#27979;&#24037;&#20316;&#25104;&#21151;&#12289;&#37202;&#31934;&#39537;&#21160;&#25915;&#20987;&#24615;&#25110;&#24515;&#29702;&#27835;&#30103;&#25913;&#21892;&#24515;&#29702;&#20581;&#24247;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#20174;&#26631;&#20934;&#21270;&#22270;&#20687;&#34893;&#29983;&#20986;&#30340;&#27169;&#22411;&#22312;3,401&#21517;&#26469;&#33258;&#32654;&#22269;&#12289;&#33521;&#22269;&#21644;&#21152;&#25343;&#22823;&#30340;&#25919;&#27835;&#20154;&#29289;&#30340;&#33258;&#28982;&#22270;&#20687;&#26679;&#26412;&#20013;&#34920;&#29616;&#33391;&#22909;(r = 0.12)&#65292;&#34920;&#26126;&#38754;&#37096;&#22806;&#35980;&#21644;&#25919;&#27835;&#21462;&#21521;&#20043;&#38388;&#30340;&#20851;&#32852;&#21487;&#25512;&#24191;&#21040;&#25105;&#20204;&#20043;&#22806;&#30340;&#20154;&#32676;&#12290;&#38754;&#37096;&#29305;&#24449;&#19982;&#25919;&#27835;&#21462;&#21521;&#30456;&#20851;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#20445;&#23432;&#27966;&#30340;&#19979;&#21322;&#33080;&#37096;&#20998;&#26356;&#22823;&#65292;&#34429;&#28982;&#25919;&#27835;&#21462;&#21521;&#19981;&#33021;&#20934;&#30830;&#22320;&#39044;&#27979;&#20010;&#20307;&#38754;&#37096;&#29305;&#24449;&#30340;&#25152;&#26377;&#21464;&#21270;&#65292;&#20294;&#26159;&#36825;&#31181;&#21457;&#29616;&#36824;&#26159;&#23500;&#26377;&#21551;&#21457;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A facial recognition algorithm was used to extract face descriptors from carefully standardized images of 591 neutral faces taken in the laboratory setting. Face descriptors were entered into a cross-validated linear regression to predict participants' scores on a political orientation scale (Cronbach's alpha=.94) while controlling for age, gender, and ethnicity. The model's performance exceeded r=.20: much better than that of human raters and on par with how well job interviews predict job success, alcohol drives aggressiveness, or psychological therapy improves mental health. Moreover, the model derived from standardized images performed well (r=.12) in a sample of naturalistic images of 3,401 politicians from the U.S., UK, and Canada, suggesting that the associations between facial appearance and political orientation generalize beyond our sample. The analysis of facial features associated with political orientation revealed that conservatives had larger lower faces, although politi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26412;&#22320;&#32531;&#23384;&#26356;&#26032;&#35268;&#21017;&#26469;&#31649;&#29702;&#21160;&#24577;&#25968;&#25454;&#20998;&#24067;&#21644;&#26377;&#38480;&#30340;&#32531;&#23384;&#23481;&#37327;&#65292;&#20197;&#24212;&#23545;&#27969;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#38598;&#19982;&#38271;&#26399;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.16340</link><description>&lt;p&gt;
&#20851;&#20110;&#27969;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#26412;&#22320;&#32531;&#23384;&#26356;&#26032;&#35268;&#21017;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Local Cache Update Rules in Streaming Federated Learning. (arXiv:2303.16340v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26412;&#22320;&#32531;&#23384;&#26356;&#26032;&#35268;&#21017;&#26469;&#31649;&#29702;&#21160;&#24577;&#25968;&#25454;&#20998;&#24067;&#21644;&#26377;&#38480;&#30340;&#32531;&#23384;&#23481;&#37327;&#65292;&#20197;&#24212;&#23545;&#27969;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#38598;&#19982;&#38271;&#26399;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#27969;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26412;&#22320;&#32531;&#23384;&#26356;&#26032;&#35268;&#21017;&#65292;&#20197;&#31649;&#29702;&#21160;&#24577;&#25968;&#25454;&#20998;&#24067;&#21644;&#26377;&#38480;&#30340;&#32531;&#23384;&#23481;&#37327;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#20381;&#36182;&#20110;&#22266;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#22312;SFL&#20013;&#65292;&#25968;&#25454;&#26159;&#27969;&#24335;&#30340;&#65292;&#24182;&#19988;&#20854;&#20998;&#24067;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#65292;&#23548;&#33268;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#38598;&#19982;&#38271;&#26399;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26412;&#22320;&#32531;&#23384;&#26356;&#26032;&#35268;&#21017;&#8212;&#8212;&#20808;&#36827;&#20808;&#20986;&#65288;FIFO&#65289;&#12289;&#38745;&#24577;&#27604;&#20363;&#36873;&#25321;&#24615;&#26367;&#25442;&#65288;SRSR&#65289;&#21644;&#21160;&#24577;&#27604;&#20363;&#36873;&#25321;&#24615;&#26367;&#25442;&#65288;DRSR&#65289;&#8212;&#8212;&#22312;&#32771;&#34385;&#26377;&#38480;&#30340;&#32531;&#23384;&#23481;&#37327;&#30340;&#21516;&#26102;&#26356;&#26032;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#32531;&#23384;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#22522;&#20110;&#38271;&#26399;&#25968;&#25454;&#20998;&#24067;&#21644;&#23458;&#25143;&#31471;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#38388;&#20998;&#24067;&#19981;&#19968;&#33268;&#24230;&#30340;&#25910;&#25947;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#65306;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we address the emerging field of Streaming Federated Learning (SFL) and propose local cache update rules to manage dynamic data distributions and limited cache capacity. Traditional federated learning relies on fixed data sets, whereas in SFL, data is streamed, and its distribution changes over time, leading to discrepancies between the local training dataset and long-term distribution. To mitigate this problem, we propose three local cache update rules - First-In-First-Out (FIFO), Static Ratio Selective Replacement (SRSR), and Dynamic Ratio Selective Replacement (DRSR) - that update the local cache of each client while considering the limited cache capacity. Furthermore, we derive a convergence bound for our proposed SFL algorithm as a function of the distribution discrepancy between the long-term data distribution and the client's local training dataset. We then evaluate our proposed algorithm on two datasets: a network traffic classification dataset and an image class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#23637;&#20102;PCA-Net&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#24471;&#20986;&#20102;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#28508;&#22312;&#38556;&#30861;&#65306;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16317</link><description>&lt;p&gt;
PCA-Net&#65306;&#25805;&#20316;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#19978;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Operator learning with PCA-Net: upper and lower complexity bounds. (arXiv:2303.16317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#23637;&#20102;PCA-Net&#30340;&#36817;&#20284;&#29702;&#35770;&#65292;&#24471;&#20986;&#20102;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#28508;&#22312;&#38556;&#30861;&#65306;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;PCA-Net&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#23427;&#23558;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#36924;&#36817;&#28508;&#22312;&#30340;&#31639;&#23376;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#36817;&#20284;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#25913;&#36827;&#24182;&#26174;&#30528;&#25193;&#23637;&#20102;&#27492;&#26041;&#21521;&#30340;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312;&#23450;&#24615;&#30028;&#38480;&#26041;&#38754;&#65292;&#26412;&#25991;&#24471;&#20986;&#20102;&#26032;&#39062;&#30340;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#65292;&#22312;&#23545;&#28508;&#22312;&#31639;&#23376;&#21644;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#26368;&#23567;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#12290;&#22312;&#23450;&#37327;&#38480;&#21046;&#26041;&#38754;&#65292;&#26412;&#25991;&#35782;&#21035;&#20102;&#20351;&#29992;PCA-Net&#36827;&#34892;&#39640;&#25928;&#25805;&#20316;&#23398;&#20064;&#30340;&#20004;&#20010;&#28508;&#22312;&#38556;&#30861;&#65292;&#36890;&#36807;&#23548;&#20986;&#19979;&#30028;&#36827;&#34892;&#20102;&#20005;&#26684;&#35777;&#26126;&#65292;&#31532;&#19968;&#20010;&#38556;&#30861;&#19982;&#36755;&#20986;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#26377;&#20851;&#65292;&#30001;PCA&#29305;&#24449;&#20540;&#30340;&#32531;&#24930;&#34928;&#20943;&#26469;&#34913;&#37327;&#65307;&#21478;&#19968;&#20010;&#38556;&#30861;&#28041;&#21450;&#26080;&#38480;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#30340;&#31639;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators are gaining attention in computational science and engineering. PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate an underlying operator. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction. In terms of qualitative bounds, this paper derives a novel universal approximation result, under minimal assumptions on the underlying operator and the data-generating distribution. In terms of quantitative bounds, two potential obstacles to efficient operator learning with PCA-Net are identified, and made rigorous through the derivation of lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates the inherent complexity of the space of operators between infinite-dimensional input and output spaces, 
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#36229;&#36807;150&#31687;&#25991;&#31456;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#39044;&#27979;&#29359;&#32618;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#36164;&#26009;&#12290;</title><link>http://arxiv.org/abs/2303.16310</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#29359;&#32618;&#39044;&#27979;&#65306;&#31995;&#32479;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Crime Prediction Using Machine Learning and Deep Learning: A Systematic Review and Future Directions. (arXiv:2303.16310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#36229;&#36807;150&#31687;&#25991;&#31456;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#39044;&#27979;&#29359;&#32618;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#29359;&#32618;&#22312;&#26368;&#36817;&#20960;&#24180;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#30456;&#24403;&#20851;&#27880;&#65292;&#20027;&#35201;&#20851;&#27880;&#35782;&#21035;&#29359;&#32618;&#20107;&#20214;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#36229;&#36807;150&#31687;&#25991;&#31456;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#39044;&#27979;&#29359;&#32618;&#30340;&#24773;&#20917;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#30740;&#31350;&#20154;&#21592;&#29992;&#20110;&#29359;&#32618;&#39044;&#27979;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#24212;&#29992;&#20110;&#39044;&#27979;&#29359;&#32618;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#26480;&#20986;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#29359;&#32618;&#27963;&#21160;&#30456;&#20851;&#19981;&#21516;&#36235;&#21183;&#21644;&#22240;&#32032;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24378;&#35843;&#20102;&#21487;&#33021;&#23384;&#22312;&#30340;&#24046;&#36317;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#20197;&#22686;&#24378;&#29359;&#32618;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#20171;&#32461;&#30340;&#20851;&#20110;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#29359;&#32618;&#30340;&#30740;&#31350;&#32508;&#36848;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting crime using machine learning and deep learning techniques has gained considerable attention from researchers in recent years, focusing on identifying patterns and trends in crime occurrences. This review paper examines over 150 articles to explore the various machine learning and deep learning algorithms applied to predict crime. The study provides access to the datasets used for crime prediction by researchers and analyzes prominent approaches applied in machine learning and deep learning algorithms to predict crime, offering insights into different trends and factors related to criminal activities. Additionally, the paper highlights potential gaps and future directions that can enhance the accuracy of crime prediction. Finally, the comprehensive overview of research discussed in this paper on crime prediction using machine learning and deep learning approaches serves as a valuable reference for researchers in this field. By gaining a deeper understanding of crime predictio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#20110;&#27969;&#25968;&#25454;&#19978;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#65292;&#20026;&#20351;&#29992;&#22266;&#23450;&#22823;&#23567;&#28369;&#21160;&#31383;&#21475;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#24378;&#31283;&#20581;&#24615;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2303.16308</link><description>&lt;p&gt;
&#28369;&#21160;&#31383;&#21475;&#27969;&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provable Robustness for Streaming Models with a Sliding Window. (arXiv:2303.16308v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#27969;&#25968;&#25454;&#19978;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#65292;&#20026;&#20351;&#29992;&#22266;&#23450;&#22823;&#23567;&#28369;&#21160;&#31383;&#21475;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#24378;&#31283;&#20581;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#26377;&#20851;&#21487;&#35777;&#26126;&#30340;&#31283;&#20581;&#24615;&#30340;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#38745;&#24577;&#39044;&#27979;&#38382;&#39064;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#31561;&#65292;&#20854;&#20013;&#20551;&#23450;&#36755;&#20837;&#26679;&#26412;&#26159;&#29420;&#31435;&#30340;&#65292;&#24182;&#19988;&#27169;&#22411;&#24615;&#33021;&#26159;&#22312;&#36755;&#20837;&#20998;&#24067;&#19978;&#30340;&#26399;&#26395;&#12290;&#23545;&#20110;&#21333;&#20010;&#36755;&#20837;&#23454;&#20363;&#65292;&#21487;&#20197;&#24471;&#20986;&#31283;&#20581;&#24615;&#35777;&#26126;&#65292;&#20294;&#26159;&#20551;&#35774;&#35813;&#27169;&#22411;&#23545;&#27599;&#20010;&#23454;&#20363;&#21333;&#29420;&#36827;&#34892;&#35780;&#20272;&#30340;&#31283;&#20581;&#24615;&#35777;&#26126;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#25968;&#25454;&#27969;&#19978;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#36755;&#20837;&#20316;&#20026;&#19968;&#31995;&#21015;&#21487;&#33021;&#30456;&#20851;&#30340;&#39033;&#21576;&#29616;&#12290;&#25105;&#20204;&#20026;&#20351;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#28369;&#21160;&#31383;&#21475;&#30340;&#27169;&#22411;&#25512;&#23548;&#20986;&#20102;&#24378;&#31283;&#20581;&#24615;&#35777;&#26126;&#65292;&#20445;&#35777;&#20102;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#26679;&#26412;&#65292;&#24182;&#20026;&#27969;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#24378;&#31283;&#20581;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The literature on provable robustness in machine learning has primarily focused on static prediction problems, such as image classification, in which input samples are assumed to be independent and model performance is measured as an expectation over the input distribution. Robustness certificates are derived for individual input instances with the assumption that the model is evaluated on each instance separately. However, in many deep learning applications such as online content recommendation and stock market analysis, models use historical data to make predictions. Robustness certificates based on the assumption of independent input samples are not directly applicable in such scenarios. In this work, we focus on the provable robustness of machine learning models in the context of data streams, where inputs are presented as a sequence of potentially correlated items. We derive robustness certificates for models that use a fixed-size sliding window over the input stream. Our guarante
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#23545;&#20840;&#29699;&#20013;&#38271;&#26399;&#22825;&#27668;&#39044;&#25253;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#28508;&#22312;&#22909;&#22788;&#21644;&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#19978;&#21487;&#25552;&#39640;&#39640;&#36798;12%&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#21152;&#24378;&#23545;&#22825;&#27668;&#39044;&#25253;&#31934;&#24230;&#27979;&#37327;&#30340;&#30740;&#31350;&#65292;&#23558;&#26377;&#21161;&#20110;&#26356;&#22909;&#30340;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#36827;&#19968;&#27493;&#20248;&#21270;&#36825;&#20123;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16301</link><description>&lt;p&gt;
&#19968;&#20221;&#26426;&#22120;&#23398;&#20064;&#25253;&#21578;: &#20840;&#29699;&#20013;&#38271;&#26399;&#39044;&#25253;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Outlook: Post-processing of Global Medium-range Forecasts. (arXiv:2303.16301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#23545;&#20840;&#29699;&#20013;&#38271;&#26399;&#22825;&#27668;&#39044;&#25253;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#28508;&#22312;&#22909;&#22788;&#21644;&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#19978;&#21487;&#25552;&#39640;&#39640;&#36798;12%&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#21152;&#24378;&#23545;&#22825;&#27668;&#39044;&#25253;&#31934;&#24230;&#27979;&#37327;&#30340;&#30740;&#31350;&#65292;&#23558;&#26377;&#21161;&#20110;&#26356;&#22909;&#30340;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#36827;&#19968;&#27493;&#20248;&#21270;&#36825;&#20123;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#22788;&#29702;&#25216;&#26415;&#36890;&#24120;&#26159;&#23558;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;(Numerical Weather Prediction, NWP)&#27169;&#22411;&#30340;&#36755;&#20986;&#20316;&#20026;&#36755;&#20837;&#65292;&#24212;&#29992;&#20110;&#32447;&#24615;&#32479;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21253;&#21547;&#38468;&#21152;&#30340;&#35266;&#27979;&#25968;&#25454;&#25110;&#20915;&#23450;&#26356;&#32454;&#33268;&#23610;&#24230;&#30340;&#31995;&#32479;&#24615;&#38169;&#35823;&#65292;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#26412;&#22320;&#21270;&#39044;&#25253;&#12290;&#22312;&#26412;&#27425;&#35797;&#28857;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;(NN)&#30340;&#26041;&#27861;&#26469;&#21518;&#22788;&#29702;&#20840;&#29699;30&#20010;&#22402;&#30452;&#23618;&#27425;&#19978;&#30340;&#22810;&#31181;&#22825;&#27668;&#29305;&#24449;-Temperature, moisture, wind, geopotential height, precipitable water-&#20197;&#21450;&#21040;&#36798;&#26102;&#38388;&#38271;&#36798;7&#22825;&#30340;&#39046;&#20808;&#27700;&#24179;&#30340;&#22909;&#22788;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#28201;&#24230;850hPa&#30340;&#39046;&#22495;&#20013;&#23454;&#29616;&#39640;&#36798;12% (RMSE)&#30340;&#31934;&#24230;&#25552;&#39640;&#65292;&#23545;&#20110;7&#22825;&#30340;&#39044;&#25253;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#38656;&#35201;&#21152;&#24378;&#23545;&#23574;&#38160;&#21644;&#27491;&#30830;&#22825;&#27668;&#39044;&#25253;&#30340;&#23458;&#35266;&#24230;&#37327;&#30340;&#22522;&#30784;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#32447;&#24615;&#32479;&#35745;&#27169;&#22411;&#21040;&#26356;&#22797;&#26434;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#22914;&#20309;&#20351;&#29992;&#26631;&#20934;&#25351;&#26631;&#20363;&#22914;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#25110;&#24322;&#24120;&#30456;&#20851;&#31995;&#25968;(ACC)&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-processing typically takes the outputs of a Numerical Weather Prediction (NWP) model and applies linear statistical techniques to produce improve localized forecasts, by including additional observations, or determining systematic errors at a finer scale. In this pilot study, we investigate the benefits and challenges of using non-linear neural network (NN) based methods to post-process multiple weather features -- temperature, moisture, wind, geopotential height, precipitable water -- at 30 vertical levels, globally and at lead times up to 7 days. We show that we can achieve accuracy improvements of up to 12% (RMSE) in a field such as temperature at 850hPa for a 7 day forecast. However, we recognize the need to strengthen foundational work on objectively measuring a sharp and correct forecast. We discuss the challenges of using standard metrics such as root mean squared error (RMSE) or anomaly correlation coefficient (ACC) as we move from linear statistical models to more complex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.16296</link><description>&lt;p&gt;
Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65306;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Dice&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#30340;&#35768;&#22810;&#33258;&#21160;&#20998;&#21106;&#26041;&#26696;&#20013;&#65292;&#36719;Dice&#25439;&#22833;&#65288;SDL&#65289;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#25581;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#32972;&#21518;&#30340;&#19968;&#20123;&#21407;&#22240;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20854;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23454;&#29616;&#25903;&#25345;&#30452;&#25509;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#23427;&#30340;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22312;&#20351;&#29992;SDL&#21644;&#30740;&#31350;&#21033;&#29992;&#36719;&#26631;&#31614;&#30340;&#21516;&#26102;&#36827;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21327;&#21516;&#20316;&#29992;&#20173;&#28982;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65288;DMLs&#65289;&#65292;&#23427;&#20204;&#65288;i&#65289;&#22312;&#30828;&#26631;&#31614;&#30340;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;SDL&#30456;&#21516;&#65292;&#20294;&#65288;ii&#65289;&#20063;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#30340;QUBIQ&#12289;LiTS&#21644;KiTS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DMLs&#19982;&#36719;&#26631;&#31614;&#65288;&#22914;&#24179;&#22343;&#12289;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#65289;&#30340;&#28508;&#22312;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;DMLs&#19982;&#30828;&#26631;&#31614;&#65288;&#22914;&#22823;&#22810;&#25968;&#25237;&#31080;&#21644;&#38543;&#26426;&#36873;&#25321;&#65289;&#30456;&#27604;&#65292;&#20135;&#29983;&#20102;&#26356;&#20248;&#31168;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The soft Dice loss (SDL) has taken a pivotal role in many automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct use in settings with soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be used in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calib
&lt;/p&gt;</description></item><item><title>XAIR&#26159;&#19968;&#20010;&#35299;&#20915;AR&#20013;XAI&#36755;&#20986;&#35299;&#37322;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#35774;&#35745;&#32773;&#25552;&#20379;&#25351;&#21335;&#21644;&#25903;&#25345;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;XAI&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.16292</link><description>&lt;p&gt;
XAIR&#65306;&#22686;&#24378;&#29616;&#23454;&#20013;&#21487;&#35299;&#37322;AI&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
XAIR: A Framework of Explainable AI in Augmented Reality. (arXiv:2303.16292v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16292
&lt;/p&gt;
&lt;p&gt;
XAIR&#26159;&#19968;&#20010;&#35299;&#20915;AR&#20013;XAI&#36755;&#20986;&#35299;&#37322;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#35774;&#35745;&#32773;&#25552;&#20379;&#25351;&#21335;&#21644;&#25903;&#25345;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;XAI&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24050;&#32463;&#25104;&#20026;AI&#39537;&#21160;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38543;&#30528;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;XAI&#22312;AR&#20013;&#30340;&#20316;&#29992;&#20063;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#26368;&#32456;&#29992;&#25143;&#23558;&#32463;&#24120;&#19982;&#26234;&#33021;&#26381;&#21153;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#26377;&#25928;&#30340;AR XAI&#20307;&#39564;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;XAIR&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#20309;&#26102;&#65292;&#20160;&#20040;&#21644;&#22914;&#20309;&#22312;AR&#20013;&#25552;&#20379;AI&#36755;&#20986;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#26159;&#22522;&#20110;&#23545;XAI&#21644;HCI&#30740;&#31350;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#32508;&#36848;&#12289;&#23545;500&#22810;&#21517;&#26368;&#32456;&#29992;&#25143;&#23545;&#22522;&#20110;AR&#30340;&#35299;&#37322;&#20559;&#22909;&#30340;&#22823;&#35268;&#27169;&#35843;&#26597;&#20197;&#21450;&#23545;12&#20301;&#19987;&#23478;&#25910;&#38598;&#20182;&#20204;&#22312;AR XAI&#35774;&#35745;&#26041;&#38754;&#30340;&#35265;&#35299;&#30340;&#19977;&#20010;&#30740;&#35752;&#20250;&#32780;&#26500;&#24314;&#30340;&#12290;&#36890;&#36807;&#19982;10&#21517;&#35774;&#35745;&#24072;&#21644;&#21478;&#22806;12&#21517;&#26368;&#32456;&#29992;&#25143;&#36827;&#34892;&#30340;&#30740;&#31350;&#39564;&#35777;&#20102;XAIR&#30340;&#25928;&#29992;&#21644;&#26377;&#25928;&#24615;&#12290;XAIR&#21487;&#20197;&#20026;&#35774;&#35745;&#24072;&#25552;&#20379;&#25351;&#21335;&#65292;&#28608;&#21457;&#20182;&#20204;&#21457;&#29616;&#26032;&#30340;&#35774;&#35745;&#26426;&#20250;&#65292;&#24182;&#22312;AR&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;XAI&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) has established itself as an important component of AI-driven interactive systems. With Augmented Reality (AR) becoming more integrated in daily lives, the role of XAI also becomes essential in AR because end-users will frequently interact with intelligent services. However, it is unclear how to design effective XAI experiences for AR. We propose XAIR, a design framework that addresses "when", "what", and "how" to provide explanations of AI output in AR. The framework was based on a multi-disciplinary literature review of XAI and HCI research, a large-scale survey probing 500+ end-users' preferences for AR-based explanations, and three workshops with 12 experts collecting their insights about XAI design in AR. XAIR's utility and effectiveness was verified via a study with 10 designers and another study with 12 end-users. XAIR can provide guidelines for designers, inspiring them to identify new design opportunities and achieve effective XAI designs in AR.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2303.16281</link><description>&lt;p&gt;
&#22823;&#35937;&#30340;&#36879;&#35270;&#38236;&#65306;&#35843;&#26597;&#35895;&#27468;&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#30340;&#35821;&#35328;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16281
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35895;&#27468;&#25628;&#32034;&#8220;&#20174;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#20449;&#24687;&#65292;&#20197;&#20415;&#20320;&#21487;&#20197;&#24418;&#25104;&#33258;&#24049;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#8221;&#30340;&#20219;&#21153;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35895;&#27468;&#21450;&#20854;&#26368;&#31361;&#20986;&#30340;&#25628;&#32034;&#32467;&#26524; - &#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#65292;&#20165;&#21453;&#26144;&#19982;&#8220;&#20315;&#25945;&#8221;&#12289;&#8220;&#33258;&#30001;&#20027;&#20041;&#8221;&#12289;&#8220;&#27542;&#27665;&#21270;&#8221;&#12289;&#8220;&#20234;&#26391;&#8221;&#21644;&#8220;&#32654;&#22269;&#8221;&#31561;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#12290;&#31616;&#21333;&#22320;&#35828;&#65292;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#30456;&#21516;&#25628;&#32034;&#20013;&#65292;&#23427;&#20204;&#20197;&#19981;&#21516;&#31243;&#24230;&#21576;&#29616;&#19981;&#21516;&#30340;&#20449;&#24687;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#65289;&#65292;&#32780;&#19981;&#26159;&#21576;&#29616;&#22797;&#26434;&#20027;&#39064;&#30340;&#20840;&#29699;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#25628;&#32034;&#20351;&#25105;&#20204;&#25104;&#20026;&#35866;&#35821;&#20013;&#30340;&#30450;&#20154;&#65292;&#20165;&#35302;&#25720;&#23567;&#35937;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#19981;&#30693;&#36947;&#20854;&#20182;&#25991;&#21270;&#30340;&#35270;&#35282;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#29992;&#20110;&#25628;&#32034;&#30340;&#35821;&#35328;&#26368;&#32456;&#25104;&#20026;&#20419;&#36827;&#26412;&#26063;&#20013;&#24515;&#20027;&#20041;&#35266;&#28857;&#30340;&#25991;&#21270;&#36807;&#28388;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#20154;&#26681;&#25454;&#33258;&#24049;&#30340;&#25991;&#21270;&#35780;&#20272;&#20854;&#20182;&#20154;&#25110;&#24605;&#24819;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;ChatGPT&#20013;&#28145;&#28145;&#23884;&#20837;&#20102;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrary to Google Search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that Google and its most prominent returned results -- Wikipedia and YouTube, simply reflect the narrow set of cultural stereotypes tied to the search language for complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and "America." Simply stated, they present, to varying degrees, distinct information across the same search in different languages (we call it 'language bias'). Instead of presenting a global picture of a complex topic, our online searches turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. The language we use to search ends up as a cultural filter to promote ethnocentric views, where a person evaluates other people or ideas based on their own culture. We also find that language bias is deeply embedded in ChatGPT. As it is primaril
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#22806;&#25512;&#30340;&#21152;&#36895;&#24490;&#29615;&#22352;&#26631;&#23545;&#20598;&#24179;&#22343;&#27861;(A-CODER)&#29992;&#20110;&#22797;&#21512;&#20984;&#20248;&#21270;&#65292;&#20854;&#21487;&#33719;&#24471;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#23545;&#22359;&#30340;&#25968;&#37327;&#20381;&#36182;&#24615;&#26377;&#25152;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.16279</link><description>&lt;p&gt;
&#24102;&#22806;&#25512;&#30340;&#21152;&#36895;&#24490;&#29615;&#22352;&#26631;&#23545;&#20598;&#24179;&#22343;&#27861;&#22312;&#22797;&#21512;&#20984;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Accelerated Cyclic Coordinate Dual Averaging with Extrapolation for Composite Convex Optimization. (arXiv:2303.16279v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#22806;&#25512;&#30340;&#21152;&#36895;&#24490;&#29615;&#22352;&#26631;&#23545;&#20598;&#24179;&#22343;&#27861;(A-CODER)&#29992;&#20110;&#22797;&#21512;&#20984;&#20248;&#21270;&#65292;&#20854;&#21487;&#33719;&#24471;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#23545;&#22359;&#30340;&#25968;&#37327;&#20381;&#36182;&#24615;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#37096;&#20998;&#19968;&#38454;&#20449;&#24687;&#20197;&#24490;&#29615;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#21487;&#33021;&#26159;&#33719;&#24471;&#21487;&#25193;&#23637;&#19968;&#38454;&#26041;&#27861;&#30340;&#26368;&#33258;&#28982;&#31574;&#30053;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;&#24490;&#29615;&#26041;&#26696;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#24490;&#29615;&#26041;&#26696;&#27604;&#38543;&#26426;&#26041;&#26696;&#19981;&#22826;&#34987;&#29702;&#35299;&#12290;&#25105;&#20204;&#21463;&#21040;&#20102;&#23545;&#24191;&#20041;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#22806;&#25512;&#24490;&#29615;&#26041;&#26696;&#20998;&#26512;&#26368;&#36817;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#22806;&#25512;&#30340;&#21152;&#36895;&#24490;&#29615;&#22352;&#26631;&#23545;&#20598;&#24179;&#22343;&#27861;(A-CODER)&#29992;&#20110;&#22797;&#21512;&#20984;&#20248;&#21270;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#34920;&#31034;&#20026;&#36890;&#36807;&#26799;&#24230;&#39044;&#35328;&#22120;&#35775;&#38382;&#30340;&#24179;&#28369;&#20984;&#20989;&#25968;&#21644;&#36890;&#36807;&#36817;&#31471;&#39044;&#35328;&#22120;&#35775;&#38382;&#30340;&#20984;&#20989;&#25968;&#65288;&#21487;&#33021;&#26159;&#19981;&#24179;&#28369;&#20989;&#25968;&#65289;&#30340;&#24635;&#21644;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; A-CODER &#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#22312;&#22359;&#30340;&#25968;&#37327;&#20381;&#36182;&#24615;&#19978;&#26377;&#20102;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#24179;&#28369;&#37096;&#20998;&#20026;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; variance&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting partial first-order information in a cyclic way is arguably the most natural strategy to obtain scalable first-order methods. However, despite their wide use in practice, cyclic schemes are far less understood from a theoretical perspective than their randomized counterparts. Motivated by a recent success in analyzing an extrapolated cyclic scheme for generalized variational inequalities, we propose an Accelerated Cyclic Coordinate Dual Averaging with Extrapolation (A-CODER) method for composite convex optimization, where the objective function can be expressed as the sum of a smooth convex function accessible via a gradient oracle and a convex, possibly nonsmooth, function accessible via a proximal oracle. We show that A-CODER attains the optimal convergence rate with improved dependence on the number of blocks compared to prior work. Furthermore, for the setting where the smooth component of the objective function is expressible in a finite sum form, we introduce a varianc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; WakeNet &#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#22810;&#20445;&#30495;&#24230;&#36716;&#31227;&#23398;&#20064;&#23558;&#20302;&#20445;&#30495;&#24230;&#39640;&#26031;&#28065;&#27169;&#22411;&#24494;&#35843;&#29983;&#25104;&#20013;&#39640;&#20445;&#30495;&#24230;&#21367;&#26354;&#28065;&#27169;&#22411;&#30340;&#31934;&#30830;&#28065;&#27969;&#32467;&#26524;&#65292;&#20197;&#21152;&#36895;&#22522;&#20110;&#20559;&#33322;&#21644;&#24067;&#23616;&#20248;&#21270;&#30340;&#28065;&#27969;&#25511;&#21046;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16274</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20445;&#30495;&#28145;&#24230;&#36716;&#31227;&#23398;&#20064;&#28065;&#27169;&#22411;&#30340;&#39118;&#30005;&#22330;&#20559;&#33322;&#21644;&#24067;&#23616;&#20248;&#21270;&#21152;&#36895;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerated wind farm yaw and layout optimisation with multi-fidelity deep transfer learning wake models. (arXiv:2303.16274v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; WakeNet &#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#22810;&#20445;&#30495;&#24230;&#36716;&#31227;&#23398;&#20064;&#23558;&#20302;&#20445;&#30495;&#24230;&#39640;&#26031;&#28065;&#27169;&#22411;&#24494;&#35843;&#29983;&#25104;&#20013;&#39640;&#20445;&#30495;&#24230;&#21367;&#26354;&#28065;&#27169;&#22411;&#30340;&#31934;&#30830;&#28065;&#27969;&#32467;&#26524;&#65292;&#20197;&#21152;&#36895;&#22522;&#20110;&#20559;&#33322;&#21644;&#24067;&#23616;&#20248;&#21270;&#30340;&#28065;&#27969;&#25511;&#21046;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#30005;&#22330;&#24314;&#27169;&#26159;&#19968;&#20010;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20247;&#22810;&#22522;&#20110;&#20998;&#26512;&#21450;&#35745;&#31639;&#30340;&#26041;&#27861;&#26469;&#25299;&#23637;&#39118;&#30005;&#22330;&#25928;&#29575;&#36793;&#30028;&#12289;&#26368;&#22823;&#21270;&#21457;&#30005;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550; WakeNet&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#20559;&#33322;&#35282;&#24230;&#12289;&#39118;&#36895;&#21644;&#28237;&#27969;&#24378;&#24230;&#19979;&#65292;&#20197;99.8%&#30340;&#24179;&#22343;&#31934;&#24230;&#29983;&#25104;&#27867;&#21270;&#30340; 2D &#28065;&#27969;&#36895;&#22330;&#65292;&#19982; FLORIS &#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#39118;&#30005;&#22330;&#27169;&#25311;&#36719;&#20214;&#35745;&#31639;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#12290;&#30001;&#20110;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#29983;&#25104;&#25104;&#26412;&#39640;&#26114;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#22810;&#20445;&#30495;&#24230;&#36716;&#31227;&#23398;&#20064;&#30340;&#23454;&#29992;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20302;&#20445;&#30495;&#24230;&#39640;&#26031;&#28065;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#21518;&#24494;&#35843;&#65292;&#20197;&#20415;&#33719;&#24471;&#20013;&#31561;&#20445;&#30495;&#24230;&#21367;&#26354;&#28065;&#27169;&#22411;&#30340;&#31934;&#30830;&#28065;&#27969;&#32467;&#26524;&#12290;&#36890;&#36807;&#20559;&#33322;&#21644;&#24067;&#23616;&#20248;&#21270;&#31561;&#22810;&#31181;&#28065;&#27969;&#25511;&#21046;&#38382;&#39064;&#30340;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102; WakeNet &#30340;&#20581;&#22766;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;WakeNet &#33021;&#22815;&#23454;&#29616;&#27604;&#22522;&#20934;&#22330;&#26223;&#22686;&#21152; 12.8% &#30340;&#21457;&#30005;&#37327;&#65292;&#24182;&#27604; FLORIS &#25552;&#39640; 3.2%&#65292;&#19988;&#35745;&#31639;&#21344;&#29992;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wind farm modelling has been an area of rapidly increasing interest with numerous analytical as well as computational-based approaches developed to extend the margins of wind farm efficiency and maximise power production. In this work, we present the novel ML framework WakeNet, which can reproduce generalised 2D turbine wake velocity fields at hub-height over a wide range of yaw angles, wind speeds and turbulence intensities (TIs), with a mean accuracy of 99.8% compared to the solution calculated using the state-of-the-art wind farm modelling software FLORIS. As the generation of sufficient high-fidelity data for network training purposes can be cost-prohibitive, the utility of multi-fidelity transfer learning has also been investigated. Specifically, a network pre-trained on the low-fidelity Gaussian wake model is fine-tuned in order to obtain accurate wake results for the mid-fidelity Curl wake model. The robustness and overall performance of WakeNet on various wake steering control 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#21483;one-shot VFL&#30340;&#23454;&#29992;VFL&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#36890;&#20449;&#29942;&#39048;&#21644;&#26377;&#38480;&#37325;&#21472;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;few-shot VFL&#21017;&#21487;&#20197;&#22312;&#36827;&#34892;&#19968;&#27425;&#25110;&#20165;&#23569;&#37327;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16270</link><description>&lt;p&gt;
&#22312;&#26377;&#38480;&#37325;&#21472;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples. (arXiv:2303.16270v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#21483;one-shot VFL&#30340;&#23454;&#29992;VFL&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#36890;&#20449;&#29942;&#39048;&#21644;&#26377;&#38480;&#37325;&#21472;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;few-shot VFL&#21017;&#21487;&#20197;&#22312;&#36827;&#34892;&#19968;&#27425;&#25110;&#20165;&#23569;&#37327;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;(VFL)&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;VFL&#22788;&#29702;&#30340;&#26159;&#23458;&#25143;&#31471;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#20294;&#20849;&#20139;&#19968;&#20123;&#37325;&#21472;&#26679;&#26412;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;VFL&#26041;&#27861;&#23384;&#22312;&#36890;&#20449;&#25104;&#26412;&#39640;&#21644;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#30340;&#26377;&#38480;&#37325;&#21472;&#26679;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#21483; \textbf{one-shot VFL} &#30340;&#23454;&#29992;VFL&#26694;&#26550;&#65292;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#36890;&#20449;&#29942;&#39048;&#21644;&#26377;&#38480;&#37325;&#21472;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102; \textbf{few-shot VFL}&#65292;&#22312;&#21482;&#36827;&#34892;&#19968;&#27425;&#25110;&#20165;&#23569;&#37327;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#23458;&#25143;&#31471;&#21482;&#38656;&#35201;&#19982;&#26381;&#21153;&#22120;&#36827;&#34892;&#19968;&#27425;&#25110;&#20165;&#23569;&#37327;&#36890;&#20449;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;VFL&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a popular collaborative learning approach that enables clients to train a global model without sharing their local data. Vertical federated learning (VFL) deals with scenarios in which the data on clients have different feature spaces but share some overlapping samples. Existing VFL approaches suffer from high communication costs and cannot deal efficiently with limited overlapping samples commonly seen in the real world. We propose a practical vertical federated learning (VFL) framework called \textbf{one-shot VFL} that can solve the communication bottleneck and the problem of limited overlapping samples simultaneously based on semi-supervised learning. We also propose \textbf{few-shot VFL} to improve the accuracy further with just one more communication round between the server and the clients. In our proposed framework, the clients only need to communicate with the server once or only a few times. We evaluate the proposed VFL framework on both image and tabular
&lt;/p&gt;</description></item><item><title>TimeBalance&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#19981;&#21464;&#21644;&#26102;&#38388;&#19981;&#21516;&#30340;&#33258;&#30417;&#30563;&#35270;&#39057;&#34920;&#31034;&#26469;&#23398;&#20064;&#20934;&#30830;&#30340;&#34892;&#20026;&#35782;&#21035;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.16268</link><description>&lt;p&gt;
TimeBalance&#65306;&#38754;&#21521;&#21322;&#30417;&#30563;&#34892;&#20026;&#35782;&#21035;&#30340;&#26102;&#38388;&#19981;&#21464;&#21644;&#26102;&#38388;&#19981;&#21516;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition. (arXiv:2303.16268v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16268
&lt;/p&gt;
&lt;p&gt;
TimeBalance&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#19981;&#21464;&#21644;&#26102;&#38388;&#19981;&#21516;&#30340;&#33258;&#30417;&#30563;&#35270;&#39057;&#34920;&#31034;&#26469;&#23398;&#20064;&#20934;&#30830;&#30340;&#34892;&#20026;&#35782;&#21035;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22270;&#20687;&#30456;&#27604;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#23545;&#20110;&#35270;&#39057;&#39046;&#22495;&#26356;&#20855;&#26377;&#20248;&#21183;&#65292;&#22240;&#20026;&#20854;&#27880;&#37322;&#25104;&#26412;&#21644;&#32500;&#24230;&#26356;&#39640;&#12290;&#20026;&#20102;&#23398;&#20064;&#21322;&#30417;&#30563;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#30340;&#38745;&#24577;&#21644;&#36816;&#21160;&#30456;&#20851;&#29305;&#24449;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#30828;&#36755;&#20837;&#24402;&#32435;&#20559;&#24046;&#65292;&#22914;&#20351;&#29992;&#20004;&#31181;&#27169;&#24335;&#65288;RGB&#21644;Optical-flow&#65289;&#25110;&#19981;&#21516;&#25773;&#25918;&#36895;&#29575;&#30340;&#20004;&#20010;&#27969;&#12290;&#25105;&#20204;&#19981;&#20351;&#29992;&#19981;&#21516;&#30340;&#36755;&#20837;&#27969;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#65292;&#30456;&#21453;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#33258;&#30417;&#30563;&#35270;&#39057;&#34920;&#31034;&#65292;&#23588;&#20854;&#26159;&#25105;&#20204;&#20351;&#29992;&#26102;&#38388;&#19981;&#21464;&#21644;&#26102;&#38388;&#19981;&#21516;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#20123;&#34920;&#31034;&#26681;&#25454;&#21160;&#20316;&#30340;&#24615;&#36136;&#30456;&#20114;&#34917;&#20805;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimeBalance&#30340;&#24072;&#29983;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#26102;&#38388;&#19981;&#21464;&#21644;&#26102;&#38388;&#19981;&#21516;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#25552;&#28860;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#20197;&#23398;&#20064;&#20934;&#30830;&#30340;&#34892;&#20026;&#35782;&#21035;&#29305;&#24449;&#12290;&#22312;UCF101&#65292;HMDB51&#21644;Kinetics&#31561;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning can be more beneficial for the video domain compared to images because of its higher annotation cost and dimensionality. Besides, any video understanding task requires reasoning over both spatial and temporal dimensions. In order to learn both the static and motion related features for the semi-supervised action recognition task, existing methods rely on hard input inductive biases like using two-modalities (RGB and Optical-flow) or two-stream of different playback rates. Instead of utilizing unlabeled videos through diverse input streams, we rely on self-supervised video representations, particularly, we utilize temporally-invariant and temporally-distinctive representations. We observe that these representations complement each other depending on the nature of the action. Based on this observation, we propose a student-teacher semi-supervised learning framework, TimeBalance, where we distill the knowledge from a temporally-invariant and a temporally-distincti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270;&#20102;&#19968;&#31181;&#40657;&#30418;&#20132;&#26131;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312; DA &#33021;&#28304;&#24066;&#22330;&#19978;&#30001;&#20013;&#22411;&#29983;&#20135;&#32773;&#33258;&#21160;&#36827;&#34892;&#20132;&#26131;&#12290;</title><link>http://arxiv.org/abs/2303.16266</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33021;&#28304;&#20132;&#26131;&#31574;&#30053;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning for optimization of energy trading strategy. (arXiv:2303.16266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270;&#20102;&#19968;&#31181;&#40657;&#30418;&#20132;&#26131;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312; DA &#33021;&#28304;&#24066;&#22330;&#19978;&#30001;&#20013;&#22411;&#29983;&#20135;&#32773;&#33258;&#21160;&#36827;&#34892;&#20132;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#33021;&#28304;&#26469;&#33258;&#22823;&#37327;&#23567;&#22411;&#29983;&#20135;&#32773;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#36825;&#20123;&#26469;&#28304;&#30340;&#25928;&#29575;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#20063;&#26159;&#38543;&#26426;&#30340;&#65292;&#21152;&#21095;&#20102;&#33021;&#28304;&#24066;&#22330;&#24179;&#34913;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#22269;&#23478;&#65292;&#36825;&#31181;&#24179;&#34913;&#26159;&#22312;&#39044;&#27979;&#26085;&#65288;DA&#65289;&#33021;&#28304;&#24066;&#22330;&#19978;&#23436;&#25104;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#30001;&#20013;&#22411;&#29983;&#20135;&#32773;&#22312;DA&#33021;&#28304;&#24066;&#22330;&#19978;&#30340;&#33258;&#21160;&#21270;&#20132;&#26131;&#12290;&#25105;&#20204;&#23558;&#27492;&#27963;&#21160;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35268;&#33539;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#21487;&#20197;&#20351;&#29992;&#29616;&#23454;&#25968;&#25454;&#20248;&#21270;&#21363;&#29992;&#31574;&#30053;&#12290;&#25105;&#20204;&#21512;&#25104;&#21442;&#25968;&#21270;&#20132;&#26131;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270;&#19968;&#20010;&#40657;&#30418;&#20132;&#26131;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21033;&#29992;&#26469;&#33258;&#29615;&#22659;&#30340;&#21487;&#29992;&#20449;&#24687;&#26469;&#24433;&#21709;&#26410;&#26469;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasing part of energy is produced from renewable sources by a large number of small producers. The efficiency of these sources is volatile and, to some extent, random, exacerbating the energy market balance problem. In many countries, that balancing is performed on day-ahead (DA) energy markets. In this paper, we consider automated trading on a DA energy market by a medium size prosumer. We model this activity as a Markov Decision Process and formalize a framework in which a ready-to-use strategy can be optimized with real-life data. We synthesize parametric trading strategies and optimize them with an evolutionary algorithm. We also use state-of-the-art reinforcement learning algorithms to optimize a black-box trading strategy fed with available information from the environment that can impact future prices.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#26550;&#26500;&#21644;&#23545;&#35805;&#21382;&#21490;&#25688;&#35201;&#23454;&#29616;&#27867;&#21270;&#30340;&#38646;&#26679;&#26412;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#22312;&#26631;&#20934;&#39046;&#22495;&#36716;&#31227;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16252</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#25688;&#35201;&#21644;&#39046;&#22495;&#26550;&#26500;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Generalizable End-to-End Task-Oriented Dialog System using Context Summarization and Domain Schema. (arXiv:2303.16252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#26550;&#26500;&#21644;&#23545;&#35805;&#21382;&#21490;&#25688;&#35201;&#23454;&#29616;&#27867;&#21270;&#30340;&#38646;&#26679;&#26412;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#22312;&#26631;&#20934;&#39046;&#22495;&#36716;&#31227;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#36890;&#36807;&#20419;&#36827;&#30452;&#35266;&#21644;&#34920;&#36798;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#23436;&#25104;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290; &#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23558;&#38382;&#39064;&#21046;&#23450;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#22312;&#30417;&#30563;&#35774;&#32622;&#20013;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#12290; &#36825;&#38656;&#35201;&#20026;&#27599;&#20010;&#26032;&#39046;&#22495;&#25110;&#20219;&#21153;&#25552;&#20379;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#26159;&#26497;&#20854;&#36153;&#21147;&#21644;&#26114;&#36149;&#30340;&#65292;&#20174;&#32780;&#20351;&#20854;&#25104;&#20026;&#25193;&#23637;&#31995;&#32479;&#21040;&#21508;&#31181;&#39046;&#22495;&#30340;&#29942;&#39048;&#12290; &#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;ZS-ToD&#65292;&#23427;&#21033;&#29992;&#39046;&#22495;&#26550;&#26500;&#65292;&#20801;&#35768;&#23545;&#26410;&#30693;&#39046;&#22495;&#36827;&#34892;&#24378;&#22823;&#30340;&#27867;&#21270;&#65292;&#24182;&#21033;&#29992;&#23545;&#35805;&#21382;&#21490;&#30340;&#26377;&#25928;&#25688;&#35201;&#12290; &#25105;&#20204;&#20351;&#29992;GPT-2&#20316;&#20026;&#39592;&#24178;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#27493;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#20013;&#31532;&#19968;&#27493;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#23545;&#35805;&#25968;&#25454;&#30340;&#19968;&#33324;&#32467;&#26500;&#65292;&#31532;&#20108;&#27493;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#32452;&#21512;&#26469;&#20248;&#21270;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#12290; &#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#39046;&#22495;&#36716;&#31227;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialog systems empower users to accomplish their goals by facilitating intuitive and expressive natural language interactions. State-of-the-art approaches in task-oriented dialog systems formulate the problem as a conditional sequence generation task and fine-tune pre-trained causal language models in the supervised setting. This requires labeled training data for each new domain or task, and acquiring such data is prohibitively laborious and expensive, thus making it a bottleneck for scaling systems to a wide range of domains. To overcome this challenge, we introduce a novel Zero-Shot generalizable end-to-end Task-oriented Dialog system, ZS-ToD, that leverages domain schemas to allow for robust generalization to unseen domains and exploits effective summarization of the dialog history. We employ GPT-2 as a backbone model and introduce a two-step training process where the goal of the first step is to learn the general structure of the dialog data and the second step opti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26500;&#36896;&#26041;&#27861;&#8212;&#8212;&#24179;&#28369;&#30340;&#31215;&#20998;&#34920;&#31034;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20445;&#35777;&#36924;&#36817;&#31934;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.16251</link><description>&lt;p&gt;
&#38543;&#26426;&#21021;&#22987;&#21270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22411;&#21442;&#32771;&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Function Approximation with Randomly Initialized Neural Networks for Approximate Model Reference Adaptive Control. (arXiv:2303.16251v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26500;&#36896;&#26041;&#27861;&#8212;&#8212;&#24179;&#28369;&#30340;&#31215;&#20998;&#34920;&#31034;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20445;&#35777;&#36924;&#36817;&#31934;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#29702;&#35770;&#20013;&#30340;&#32463;&#20856;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28608;&#27963;&#20989;&#25968;&#28385;&#36275;&#26576;&#20123;&#28201;&#21644;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#23618;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#36924;&#36817;&#20219;&#24847;&#36830;&#32493;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#29702;&#35770;&#24182;&#27809;&#26377;&#32473;&#20986;&#19968;&#31181;&#26500;&#36896;&#24615;&#26041;&#27861;&#26469;&#29983;&#25104;&#32593;&#32476;&#21442;&#25968;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#19987;&#38376;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#20989;&#25968;&#21644;&#26576;&#20123;&#31867;&#30340;&#35299;&#26512;&#20989;&#25968;&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#28608;&#27963;&#30340;&#32447;&#24615;&#32452;&#21512;&#23454;&#29616;&#39640;&#31934;&#24230;&#36924;&#36817;&#12290;&#36825;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#20102;&#29305;&#27530;&#30340;&#31215;&#20998;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#20381;&#36182;&#20110;&#25152;&#20351;&#29992;&#30340;&#20855;&#20307;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#24179;&#28369;&#30340;&#31215;&#20998;&#34920;&#31034;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#28608;&#27963;&#24418;&#25104;&#30446;&#26631;&#20989;&#25968;&#31215;&#20998;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#32780;&#23545;&#20110;&#36825;&#20123;&#28608;&#27963;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#30452;&#25509;&#30340;&#31215;&#20998;&#34920;&#31034;&#12290;&#26032;&#30340;&#26500;&#36896;&#20351;&#24471;&#21487;&#20197;&#20026;&#38543;&#26426;&#21021;&#22987;&#21270;&#32593;&#32476;&#25552;&#20379;&#36924;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical results in neural network approximation theory show how arbitrary continuous functions can be approximated by networks with a single hidden layer, under mild assumptions on the activation function. However, the classical theory does not give a constructive means to generate the network parameters that achieve a desired accuracy. Recent results have demonstrated that for specialized activation functions, such as ReLUs and some classes of analytic functions, high accuracy can be achieved via linear combinations of randomly initialized activations. These recent works utilize specialized integral representations of target functions that depend on the specific activation functions used. This paper defines mollified integral representations, which provide a means to form integral representations of target functions using activations for which no direct integral representation is currently known. The new construction enables approximation guarantees for randomly initialized networks
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ytopt&#65292;&#19968;&#31181;&#20302;&#24310;&#36831;&#30340;&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#28151;&#21512;MPI / OpenMP&#31185;&#23398;&#24212;&#29992;&#65292;&#23454;&#29616;&#23545;&#24615;&#33021;&#21644;&#33021;&#28304;&#30340;&#33258;&#21160;&#35843;&#20248;&#65292;&#21516;&#26102;&#20248;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#33021;&#28304;&#28040;&#32791;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24212;&#29992;&#26694;&#26550;&#21487;&#23454;&#29616;&#39640;&#36798;34&#65285;&#30340;&#33021;&#28304;&#33410;&#30465;&#65292;&#19988;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16245</link><description>&lt;p&gt;
ytopt&#65306;&#33258;&#21160;&#35843;&#20248;&#22312;&#22823;&#35268;&#27169;&#33021;&#28304;&#25928;&#29575;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ytopt: Autotuning Scientific Applications for Energy Efficiency at Large Scales. (arXiv:2303.16245v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ytopt&#65292;&#19968;&#31181;&#20302;&#24310;&#36831;&#30340;&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#28151;&#21512;MPI / OpenMP&#31185;&#23398;&#24212;&#29992;&#65292;&#23454;&#29616;&#23545;&#24615;&#33021;&#21644;&#33021;&#28304;&#30340;&#33258;&#21160;&#35843;&#20248;&#65292;&#21516;&#26102;&#20248;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#33021;&#28304;&#28040;&#32791;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24212;&#29992;&#26694;&#26550;&#21487;&#23454;&#29616;&#39640;&#36798;34&#65285;&#30340;&#33021;&#28304;&#33410;&#30465;&#65292;&#19988;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#20837;&#24322;&#26500;&#35745;&#31639;&#26102;&#20195;&#21518;&#65292;&#21512;&#29702;&#20248;&#21270;&#33021;&#28304;&#21644;&#24615;&#33021;&#24050;&#32463;&#25104;&#20026;&#25552;&#39640;&#31185;&#23398;&#24212;&#29992;&#36816;&#34892;&#25928;&#29575;&#21644;&#20248;&#21270;&#31649;&#29702;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20302;&#24310;&#36831;&#30340;&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#65292;&#21487;&#23545;&#21508;&#31181;&#22823;&#35268;&#27169;&#28151;&#21512;MPI / OpenMP&#31185;&#23398;&#24212; &#27665;&#36827;&#34892;&#24615;&#33021;&#21644;&#33021;&#28304;&#33258;&#21160;&#35843;&#20248;&#65292;&#24182;&#25506;&#31350;&#38754;&#20020;&#33021;&#28304;&#32422;&#26463;&#24773;&#20917;&#19979;&#24212;&#29992;&#36816;&#34892;&#26102;&#38388;&#21644;&#33021;&#28304;&#32791; &#25955;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#22312;Argonne&#22269;&#23478;&#23454;&#39564;&#23460;&#30340;Theta&#21644;Oak Ridge&#22269;&#23478;&#23454;&#39564;&#23460;&#30340;Summit&#20004;&#20010; &#29983;&#20135;&#31995;&#32479;&#19978;&#33258;&#21160;&#35843;&#20248;&#20102;&#22235;&#20010;ECP&#20195;&#29702;&#24212;&#29992;- XSBench&#65292;AMG&#65292;SWFFT&#21644;SW4lite&#12290;&#26412;&#26041;&#27861;&#20351; &#29992;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#20195;&#29702;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#26377;&#25928;&#22320;&#25628;&#32034;&#22810;&#36798;6&#30334;&#19975;&#20010;&#19981;&#21516;&#37197;&#32622;&#30340;&#21442;&#25968;&#31354; &#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#22823;&#35268;&#27169;&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#20855;&#26377;&#20302;&#24320;&#38144;&#21644;&#33391;&#22909;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;&#36890; &#36807;&#25152;&#25552;&#20986;&#30340;&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#26576;&#20123;ECP&#20195;&#29702;&#24212;&#29992;&#30340;&#33021;&#28304;&#28040;&#32791;&#38477;&#20302;&#39640;&#36798;34&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
As we enter the exascale computing era, efficiently utilizing power and optimizing the performance of scientific applications under power and energy constraints has become critical and challenging. We propose a low-overhead autotuning framework to autotune performance and energy for various hybrid MPI/OpenMP scientific applications at large scales and to explore the tradeoffs between application runtime and power/energy for energy efficient application execution, then use this framework to autotune four ECP proxy applications -XSBench, AMG, SWFFT, and SW4lite. Our approach uses Bayesian optimization with a Random Forest surrogate model to effectively search parameter spaces with up to 6 million different configurations on two large-scale production systems, Theta at Argonne National Laboratory and Summit at Oak Ridge National Laboratory. The experimental results show that our autotuning framework at large scales has low overhead and achieves good scalability. Using the proposed autot
&lt;/p&gt;</description></item><item><title>Tetra-AML &#26159;&#19968;&#22871;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#21487;&#20197;&#33258;&#21160;&#23436;&#25104;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#36824;&#25552;&#20379;&#20102;&#27169;&#22411;&#21387;&#32553;&#21151;&#33021;&#65292;&#36890;&#36807;&#37327;&#21270;&#12289;&#21098;&#26525;&#21644;&#24352;&#37327;&#32593;&#32476;&#26469;&#23454;&#29616;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#35813;&#24037;&#20855;&#31665;&#30340;&#34920;&#29616;&#20248;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21516;&#26102;&#21487;&#20197;&#23558;&#20869;&#23384;&#20351;&#29992;&#21387;&#32553;&#33267;&#21407;&#26469;&#30340; 1/14.5&#65292;&#20165;&#25439;&#22833; 3.2&#65285; &#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16214</link><description>&lt;p&gt;
Tetra-AML: &#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tetra-AML: Automatic Machine Learning via Tensor Networks. (arXiv:2303.16214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16214
&lt;/p&gt;
&lt;p&gt;
Tetra-AML &#26159;&#19968;&#22871;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#21487;&#20197;&#33258;&#21160;&#23436;&#25104;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#36824;&#25552;&#20379;&#20102;&#27169;&#22411;&#21387;&#32553;&#21151;&#33021;&#65292;&#36890;&#36807;&#37327;&#21270;&#12289;&#21098;&#26525;&#21644;&#24352;&#37327;&#32593;&#32476;&#26469;&#23454;&#29616;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#35813;&#24037;&#20855;&#31665;&#30340;&#34920;&#29616;&#20248;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21516;&#26102;&#21487;&#20197;&#23558;&#20869;&#23384;&#20351;&#29992;&#21387;&#32553;&#33267;&#21407;&#26469;&#30340; 1/14.5&#65292;&#20165;&#25439;&#22833; 3.2&#65285; &#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26377;&#28145;&#21051;&#30340;&#24212;&#29992;&#65292;&#20294;&#22312;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#24040;&#22411;&#27169;&#22411;&#26102;&#20195;&#65292;&#38024;&#23545;&#21830;&#19994;&#24212;&#29992;&#36827;&#34892;&#20248;&#21270;&#21644;&#37096;&#32626;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#36130;&#21153;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Tetra-AML &#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#33258;&#23450;&#20041;&#30340;&#40657;&#30418;&#24352;&#37327;&#30697;&#38453;&#20248;&#21270;&#31639;&#27861; TetraOpt &#33258;&#21160;&#23436;&#25104;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#35813;&#24037;&#20855;&#31665;&#36824;&#25552;&#20379;&#27169;&#22411;&#21387;&#32553;&#21151;&#33021;&#65292;&#21253;&#25324;&#37327;&#21270;&#21644;&#21098;&#26525;&#65292;&#24182;&#20351;&#29992;&#24352;&#37327;&#32593;&#32476;&#26469;&#23454;&#29616;&#21387;&#32553;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29992;&#20110;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; CIFAR-10 &#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102; ResNet-18 &#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#30340;&#20869;&#23384;&#21482;&#26377;&#21407;&#26469;&#30340; 1/14.5&#65292;&#21516;&#26102;&#20165;&#25439;&#22833;&#20102; 3.2&#65285; &#30340;&#20934;&#30830;&#24230;&#12290;&#25152;&#21576;&#29616;&#30340;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#19981;&#20165;&#38480;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have revolutionized many aspects of society but in the era of huge models with billions of parameters, optimizing and deploying them for commercial applications can require significant computational and financial resources. To address these challenges, we introduce the Tetra-AML toolbox, which automates neural architecture search and hyperparameter optimization via a custom-developed black-box Tensor train Optimization algorithm, TetraOpt. The toolbox also provides model compression through quantization and pruning, augmented by compression using tensor networks. Here, we analyze a unified benchmark for optimizing neural networks in computer vision tasks and show the superior performance of our approach compared to Bayesian optimization on the CIFAR-10 dataset. We also demonstrate the compression of ResNet-18 neural networks, where we use 14.5 times less memory while losing just 3.2% of accuracy. The presented framework is generic, not limited by computer vision problem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20943;&#23569;&#31354;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20998;&#27835;&#30340;EMO&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23558;&#25972;&#20010;&#32593;&#32476;&#19978;&#22797;&#26434;&#30340;EMO&#21098;&#26525;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#32593;&#32476;&#19978;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#22522;&#20110;&#36328;&#32593;&#32476;&#32422;&#26463;&#30340;&#23376;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16212</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#65306;&#39640;&#25928;&#24555;&#36895;
&lt;/p&gt;
&lt;p&gt;
An EMO Joint Pruning with Multiple Sub-networks: Fast and Effect. (arXiv:2303.16212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20943;&#23569;&#31354;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20998;&#27835;&#30340;EMO&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23558;&#25972;&#20010;&#32593;&#32476;&#19978;&#22797;&#26434;&#30340;EMO&#21098;&#26525;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#32593;&#32476;&#19978;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#22522;&#20110;&#36328;&#32593;&#32476;&#32422;&#26463;&#30340;&#23376;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36827;&#21270;&#22810;&#30446;&#26631;&#65288;EMO&#65289;&#30340;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#21487;&#20197;&#24179;&#34913;&#32593;&#32476;&#30340;&#21098;&#26525;&#29575;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#20110;&#31181;&#32676;&#30340;&#29305;&#24615;&#65292;&#23427;&#32463;&#24120;&#21463;&#21040;&#22797;&#26434;&#30340;&#21098;&#26525;&#20248;&#21270;&#31354;&#38388;&#21644;&#39640;&#24230;&#36164;&#28304;&#28040;&#32791;&#30340;&#21098;&#26525;&#32467;&#26500;&#39564;&#35777;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#65288;EMO-PMS&#65289;&#65292;&#20197;&#20943;&#23569;&#31354;&#38388;&#22797;&#26434;&#24230;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27835;&#30340;EMO&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23558;&#25972;&#20010;&#32593;&#32476;&#19978;&#22797;&#26434;&#30340;EMO&#21098;&#26525;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#32593;&#32476;&#19978;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#19968;&#26041;&#38754;&#65292;&#36825;&#31181;&#20998;&#35299;&#20943;&#23569;&#20102;&#21098;&#26525;&#20248;&#21270;&#31354;&#38388;&#24182;&#38477;&#20302;&#20102;&#20248;&#21270;&#38590;&#24230;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#36739;&#23567;&#30340;&#32593;&#32476;&#32467;&#26500;&#25910;&#25947;&#26356;&#24555;&#65292;&#22240;&#27492;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#36739;&#20302;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#36328;&#32593;&#32476;&#32422;&#26463;&#30340;&#23376;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The network pruning algorithm based on evolutionary multi-objective (EMO) can balance the pruning rate and performance of the network. However, its population-based nature often suffers from the complex pruning optimization space and the highly resource-consuming pruning structure verification process, which limits its application. To this end, this paper proposes an EMO joint pruning with multiple sub-networks (EMO-PMS) to reduce space complexity and resource consumption. First, a divide-and-conquer EMO network pruning framework is proposed, which decomposes the complex EMO pruning task on the whole network into easier sub-tasks on multiple sub-networks. On the one hand, this decomposition reduces the pruning optimization space and decreases the optimization difficulty; on the other hand, the smaller network structure converges faster, so the computational resource consumption of the proposed algorithm is lower. Secondly, a sub-network training method based on cross-network constraint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35782;&#21035;&#21644;&#21033;&#29992;&#25968;&#25454;&#39033;&#19978;&#30340;&#21452;&#23556;&#21464;&#25442;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#29305;&#24449;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21333;&#35789;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.16211</link><description>&lt;p&gt;
&#29992;&#20110;&#21333;&#35789;&#30340;&#32452;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Combinatorial Convolutional Neural Networks for Words. (arXiv:2303.16211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35782;&#21035;&#21644;&#21033;&#29992;&#25968;&#25454;&#39033;&#19978;&#30340;&#21452;&#23556;&#21464;&#25442;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#29305;&#24449;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21333;&#35789;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35782;&#21035;&#21644;&#21033;&#29992;&#25968;&#25454;&#39033;&#19978;&#30340;&#21452;&#23556;&#21464;&#25442;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#29305;&#24449;&#65288;&#25105;&#20204;&#31216;&#20854;&#20026;&#32452;&#21512;&#27169;&#24335;&#65289;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#35782;&#21035;&#20986;&#36825;&#26679;&#30340;&#27169;&#24335;&#21487;&#33021;&#24456;&#37325;&#35201;&#65292;&#24182;&#24314;&#35758;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20805;&#20998;&#25551;&#36848;&#36755;&#20837;&#39033;&#32452;&#21512;&#27169;&#24335;&#30340;&#20449;&#24687;&#65292;&#35753;&#32593;&#32476;&#30830;&#23450;&#39044;&#27979;&#30456;&#20851;&#20869;&#23481;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21333;&#35789;&#20998;&#31867;&#30340;&#32452;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper discusses the limitations of deep learning models in identifying and utilizing features that remain invariant under a bijective transformation on the data entries, which we refer to as combinatorial patterns. We argue that the identification of such patterns may be important for certain applications and suggest providing neural networks with information that fully describes the combinatorial patterns of input entries and allows the network to determine what is relevant for prediction. To demonstrate the feasibility of this approach, we present a combinatorial convolutional neural network for word classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#24212;&#29992;&#28145;&#24230;&#38598;&#21512;&#37327;&#21270;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#26694;&#26550;&#65292;&#20854;&#22312;&#22238;&#24402;&#20934;&#30830;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#38752;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16210</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#21512;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#37327;&#21270;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Multi-output Regression Task. (arXiv:2303.16210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#24212;&#29992;&#28145;&#24230;&#38598;&#21512;&#37327;&#21270;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#26694;&#26550;&#65292;&#20854;&#22312;&#22238;&#24402;&#20934;&#30830;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#38752;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#21512;&#26159;&#36924;&#36817;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#19968;&#31181;&#31616;&#21333;&#30452;&#25509;&#30340;&#26041;&#27861;&#65292;&#24050;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#20998;&#31867;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#25506;&#31350;&#35813;&#26041;&#27861;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#39044;&#27979;&#23548;&#24377;&#32467;&#26500;&#30340;&#31354;&#27668;&#21160;&#21147;&#24615;&#33021;&#12290;&#36890;&#36807;&#20180;&#32454;&#30740;&#31350;&#38598;&#21512;&#20013;&#31070;&#32463;&#32593;&#32476;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#35266;&#23519;&#21040;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26222;&#36941;&#23384;&#22312;&#20302;&#20272;&#30340;&#36235;&#21183;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20107;&#21518;&#26657;&#20934;&#30340;&#28145;&#24230;&#38598;&#21512;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20854;&#25913;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24615;&#33021;&#12290;&#30452;&#35266;&#22320;&#23558;&#20854;&#19982;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#26159;&#24037;&#31243;&#20013;&#26368;&#24120;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22238;&#24402;&#20934;&#30830;&#24615;&#12289;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#38752;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#20063;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensemble is a simple and straightforward approach for approximating Bayesian inference and has been successfully applied to many classification tasks. This study aims to comprehensively investigate this approach in the multi-output regression task to predict the aerodynamic performance of a missile configuration. By scrutinizing the effect of the number of neural networks used in the ensemble, an obvious trend toward underconfidence in estimated uncertainty is observed. In this context, we propose the deep ensemble framework that applies the post-hoc calibration method, and its improved uncertainty quantification performance is demonstrated. It is compared with Gaussian process regression, the most prevalent model for uncertainty quantification in engineering, and is proven to have superior performance in terms of regression accuracy, reliability of estimated uncertainty, and training efficiency. Finally, the impact of the suggested framework on the results of Bayesian optimizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27688;&#22522;&#37240;&#20998;&#23376;&#25351;&#32441;&#37325;&#29992;&#30340;&#34507;&#30333;&#36136;&#25351;&#32441;&#31639;&#27861;AmorProt&#65292;&#20351;&#29992;&#26641;&#24418;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#28096;&#31881;&#26679;&#34507;&#30333;&#20998;&#32452;&#21644;&#31561;&#30005;&#28857;&#22238;&#24402;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24179;&#21488;&#30340;&#36866;&#29992;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.16209</link><description>&lt;p&gt;
AmorProt&#65306;&#22522;&#20110;&#27688;&#22522;&#37240;&#20998;&#23376;&#25351;&#32441;&#37325;&#29992;&#30340;&#34507;&#30333;&#36136;&#25351;&#32441;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
AmorProt: Amino Acid Molecular Fingerprints Repurposing based Protein Fingerprint. (arXiv:2303.16209v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27688;&#22522;&#37240;&#20998;&#23376;&#25351;&#32441;&#37325;&#29992;&#30340;&#34507;&#30333;&#36136;&#25351;&#32441;&#31639;&#27861;AmorProt&#65292;&#20351;&#29992;&#26641;&#24418;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#28096;&#31881;&#26679;&#34507;&#30333;&#20998;&#32452;&#21644;&#31561;&#30005;&#28857;&#22238;&#24402;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#24179;&#21488;&#30340;&#36866;&#29992;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#34507;&#30333;&#36136;&#27835;&#30103;&#22312;&#20960;&#20046;&#25152;&#26377;&#21307;&#30103;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#24050;&#32463;&#26377;&#35768;&#22810;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#34507;&#30333;&#36136;&#30340;&#30456;&#20851;&#24037;&#20316;&#12290;&#20154;&#24037;&#26234;&#33021;&#20351;&#24471;&#22312;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36827;&#34892;&#21547;&#26377;&#22823;&#37327;&#25968;&#25454;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#19982;&#24320;&#21457;&#30340;&#21508;&#31181;&#20998;&#23376;&#25351;&#32441;&#31639;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#34507;&#30333;&#36136;&#25351;&#32441;&#31639;&#27861;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27688;&#22522;&#37240;&#20998;&#23376;&#25351;&#32441;&#37325;&#29992;&#30340;&#34507;&#30333;&#36136;&#25351;&#32441;&#31639;&#27861;&#65288;AmorProt&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#21033;&#29992;&#23545;&#24212;&#20110;20&#31181;&#27688;&#22522;&#37240;&#30340;&#20998;&#23376;&#25351;&#32441;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#34920;&#31034;&#26041;&#27861;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#65288;1&#65289;&#28096;&#31881;&#26679;&#34507;&#30333;&#20998;&#32452;&#21644;&#65288;2&#65289;&#31561;&#30005;&#28857;&#22238;&#24402;&#26469;&#27604;&#36739;&#20102;&#22522;&#20110;&#26641;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#21644;&#20197;&#19979;&#23454;&#39564;&#23637;&#31034;&#20102;&#24320;&#21457;&#24179;&#21488;&#30340;&#36866;&#29992;&#24615;&#21644;&#20248;&#21183;&#65306;&#65288;3&#65289;&#25968;&#25454;&#38598;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
As protein therapeutics play an important role in almost all medical fields, numerous studies have been conducted on proteins using artificial intelligence. Artificial intelligence has enabled data driven predictions without the need for expensive experiments. Nevertheless, unlike the various molecular fingerprint algorithms that have been developed, protein fingerprint algorithms have rarely been studied. In this study, we proposed the amino acid molecular fingerprints repurposing based protein (AmorProt) fingerprint, a protein sequence representation method that effectively uses the molecular fingerprints corresponding to 20 amino acids. Subsequently, the performances of the tree based machine learning and artificial neural network models were compared using (1) amyloid classification and (2) isoelectric point regression. Finally, the applicability and advantages of the developed platform were demonstrated through a case study and the following experiments: (3) comparison of dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#22312;&#22343;&#21248;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#36716;&#25442;&#25104;&#19968;&#20010;&#22312;&#20219;&#24847;&#26410;&#30693;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#32780;&#19988;&#23545;&#20110;&#21333;&#35843;&#20998;&#24067;&#65292;&#21482;&#38656;&#35201;&#29992;$\mathcal{D}$&#20013;&#30340;&#26679;&#26412;&#12290;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#19968;&#20010;&#31639;&#27861;&#23558;$\mathcal{D}$&#36924;&#36817;&#25104;&#30001;&#23376;&#31435;&#26041;&#20307;&#28151;&#21512;&#32780;&#25104;&#30340;&#28151;&#21512;&#22343;&#21248;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.16208</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#24067;&#20998;&#35299;&#25552;&#39640;&#22343;&#21248;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Lifting uniform learners via distributional decomposition. (arXiv:2303.16208v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#22312;&#22343;&#21248;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#36716;&#25442;&#25104;&#19968;&#20010;&#22312;&#20219;&#24847;&#26410;&#30693;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#32780;&#19988;&#23545;&#20110;&#21333;&#35843;&#20998;&#24067;&#65292;&#21482;&#38656;&#35201;&#29992;$\mathcal{D}$&#20013;&#30340;&#26679;&#26412;&#12290;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#19968;&#20010;&#31639;&#27861;&#23558;$\mathcal{D}$&#36924;&#36817;&#25104;&#30001;&#23376;&#31435;&#26041;&#20307;&#28151;&#21512;&#32780;&#25104;&#30340;&#28151;&#21512;&#22343;&#21248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20219;&#20309;&#22312;&#22343;&#21248;&#20998;&#24067;&#19979;&#26377;&#25928;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#36716;&#25442;&#25104;&#19968;&#20010;&#22312;&#20219;&#24847;&#26410;&#30693;&#20998;&#24067;$\mathcal{D}$&#19979;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#36716;&#25442;&#25928;&#29575;&#38543;$\mathcal{D}$&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#32780;&#21464;&#21270;&#65292;&#23545;&#20110;&#22312;$\{\pm 1\}^n$&#19978;&#30340;&#20998;&#24067;&#65292;&#20854;pmf&#30001;&#28145;&#24230;&#20026;$d$&#30340;&#20915;&#31574;&#26641;&#35745;&#31639;&#65292;&#21017;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$\mathrm{poly}(n, (md)^d)$&#65292;&#20854;&#20013;$m$&#26159;&#21407;&#22987;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;&#21333;&#35843;&#20998;&#24067;&#65292;&#25105;&#20204;&#30340;&#36716;&#25442;&#20165;&#20351;&#29992;$\mathcal{D}$&#20013;&#30340;&#26679;&#26412;&#65292;&#32780;&#23545;&#20110;&#19968;&#33324;&#20998;&#24067;&#65292;&#25105;&#20204;&#20351;&#29992;&#23376;&#31435;&#26041;&#20307;&#26465;&#20214;&#26679;&#26412;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#19968;&#20010;&#31639;&#27861;&#65292;&#23427;&#22312;&#32473;&#20986;$\mathcal{D}$&#30340;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26368;&#20248;&#20915;&#31574;&#26641;&#20998;&#35299;$\mathcal{D}$&#65306;&#19968;&#20010;&#36924;&#36817;&#20102;$\mathcal{D}$&#30340;&#28151;&#21512;&#22343;&#21248;&#20998;&#24067;&#30340;&#20998;&#31163;&#23376;&#31435;&#26041;&#20307;&#12290;&#36890;&#36807;&#36825;&#20010;&#20998;&#35299;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#23376;&#31435;&#26041;&#20307;&#19978;&#36816;&#34892;&#22343;&#21248;&#20998;&#24067;&#23398;&#20064;&#22120;&#65292;&#24182;&#23558;&#32467;&#26524;&#21512;&#24182;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how any PAC learning algorithm that works under the uniform distribution can be transformed, in a blackbox fashion, into one that works under an arbitrary and unknown distribution $\mathcal{D}$. The efficiency of our transformation scales with the inherent complexity of $\mathcal{D}$, running in $\mathrm{poly}(n, (md)^d)$ time for distributions over $\{\pm 1\}^n$ whose pmfs are computed by depth-$d$ decision trees, where $m$ is the sample complexity of the original algorithm. For monotone distributions our transformation uses only samples from $\mathcal{D}$, and for general ones it uses subcube conditioning samples.  A key technical ingredient is an algorithm which, given the aforementioned access to $\mathcal{D}$, produces an optimal decision tree decomposition of $\mathcal{D}$: an approximation of $\mathcal{D}$ as a mixture of uniform distributions over disjoint subcubes. With this decomposition in hand, we run the uniform-distribution learner on each subcube and combine the 
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#29992;&#20316;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#20316;&#32773;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16203</link><description>&lt;p&gt;
&#24744;&#30340;&#25193;&#25955;&#27169;&#22411;&#26263;&#20013;&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Your Diffusion Model is Secretly a Zero-Shot Classifier. (arXiv:2303.16203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16203
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#29992;&#20316;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#20316;&#32773;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20026;&#22823;&#37327;&#25552;&#31034;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#29992;&#20363;&#21040;&#30446;&#21069;&#20026;&#27490;&#37117;&#21482;&#20851;&#27880;&#25277;&#26679;&#65292;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#36824;&#21487;&#20197;&#25552;&#20379;&#26377;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#20043;&#22806;&#30340;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;Stable Diffusion&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#25191;&#34892;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20248;&#20110;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25105;&#20204;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#27604;&#31454;&#20105;&#24615;&#30340;&#23545;&#27604;&#26041;&#27861;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21576;&#29616;&#20102;&#23450;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#23427;&#23398;&#20064;&#20102;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. We also find that our diffusion-based approach has stronger multimodal relational reasoning abilities than competing contrastive approaches. Finally, we eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#20445;&#25345;&#22522;&#26412;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#27169;&#25311;&#20013;&#30340;&#36830;&#32493;&#19981;&#21464;&#37327;&#26469;&#35774;&#35745;&#26356;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;PDE&#27714;&#35299;&#22120;&#65292;&#20851;&#38190;&#22312;&#20110;&#36890;&#36807;&#32416;&#38169;&#20445;&#25345;&#19981;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16110</link><description>&lt;p&gt;
&#36890;&#36807;&#32416;&#38169;&#20445;&#25345;&#19981;&#21464;&#37327;&#22312;&#26426;&#22120;&#23398;&#20064;PDE&#27714;&#35299;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Invariant preservation in machine learned PDE solvers via error correction. (arXiv:2303.16110v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#20445;&#25345;&#22522;&#26412;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#27169;&#25311;&#20013;&#30340;&#36830;&#32493;&#19981;&#21464;&#37327;&#26469;&#35774;&#35745;&#26356;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;PDE&#27714;&#35299;&#22120;&#65292;&#20851;&#38190;&#22312;&#20110;&#36890;&#36807;&#32416;&#38169;&#20445;&#25345;&#19981;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#22312;&#21487;&#33021;&#24102;&#26469;&#30340;&#20934;&#30830;&#24615;&#21644;/&#25110;&#36895;&#24230;&#19978;&#30340;&#28508;&#22312;&#25910;&#30410;&#25442;&#21462;&#26631;&#20934;&#25968;&#20540;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;&#20445;&#35777;&#27714;&#35299;&#22120;&#36755;&#20986;&#31934;&#30830;&#35299;&#30340;&#21807;&#19968;&#26041;&#27861;&#26159;&#22312;&#32593;&#26684;&#38388;&#36317;$\Delta x$&#21644;&#26102;&#38388;&#27493;&#38271;$\Delta t$&#36235;&#36817;&#20110;&#38646;&#30340;&#26497;&#38480;&#19979;&#20351;&#29992;&#25910;&#25947;&#26041;&#27861;&#12290;&#23398;&#20250;&#22312;&#22823;$\Delta x$&#21644;/&#25110;$\Delta t$&#19979;&#26356;&#26032;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#27714;&#35299;&#22120;&#27704;&#36828;&#26080;&#27861;&#20445;&#35777;&#23436;&#32654;&#30340;&#20934;&#30830;&#24615;&#12290;&#19968;&#23450;&#31243;&#24230;&#30340;&#35823;&#24046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#22240;&#27492;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#22914;&#20309;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#27714;&#35299;&#22120;&#20197;&#32473;&#20986;&#25105;&#20204;&#24895;&#24847;&#23481;&#24525;&#30340;&#38169;&#35823;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20445;&#25345;&#22522;&#26412;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#27169;&#25311;&#20013;&#30340;&#36830;&#32493;&#19981;&#21464;&#37327;&#26469;&#35774;&#35745;&#26356;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;PDE&#27714;&#35299;&#22120;&#12290;&#36825;&#31867;&#19981;&#21464;&#37327;&#30340;&#31034;&#20363;&#21253;&#25324;&#36136;&#37327;&#23432;&#24658;&#12289;&#33021;&#37327;&#23432;&#24658;&#12289;&#28909;&#21147;&#23398;&#31532;&#20108;&#23450;&#24459;&#21644;&#65288;&#25110;&#65289;&#38750;&#36127;&#23494;&#24230;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#24456;&#31616;&#21333;&#65306;&#20026;&#20102;&#20445;&#25345;&#19981;&#21464;&#37327;&#65292;&#25105;&#20204;&#24378;&#21046;&#27714;&#35299;&#22120;&#30340;&#35823;&#24046;&#19982;&#28385;&#36275;&#30456;&#20851;&#19981;&#21464;&#37327;&#30340;&#31163;&#25955;&#20989;&#25968;&#31354;&#38388;&#27491;&#20132;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#31034;&#20363;PDE&#19978;&#28436;&#31034;&#20102;&#36825;&#19968;&#24605;&#24819;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;Burgers&#26041;&#31243;&#12289;Navier-Stokes&#26041;&#31243;&#21644;&#19981;&#21487;&#21387;&#32553;&#30340;Euler&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learned partial differential equation (PDE) solvers trade the reliability of standard numerical methods for potential gains in accuracy and/or speed. The only way for a solver to guarantee that it outputs the exact solution is to use a convergent method in the limit that the grid spacing $\Delta x$ and timestep $\Delta t$ approach zero. Machine learned solvers, which learn to update the solution at large $\Delta x$ and/or $\Delta t$, can never guarantee perfect accuracy. Some amount of error is inevitable, so the question becomes: how do we constrain machine learned solvers to give us the sorts of errors that we are willing to tolerate? In this paper, we design more reliable machine learned PDE solvers by preserving discrete analogues of the continuous invariants of the underlying PDE. Examples of such invariants include conservation of mass, conservation of energy, the second law of thermodynamics, and/or non-negative density. Our key insight is simple: to preserve invariants,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.15975</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery. (arXiv:2303.15975v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#38271;&#23398;&#20064;&#32773;&#20013;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#36830;&#32493;&#22320;&#21457;&#29616;&#26032;&#27010;&#24565;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26399;&#26395;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#36825;&#31867;&#38382;&#39064;&#22312;&#38750;&#24120;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#37096;&#20998;&#35299;&#20915;&#65292;&#20854;&#20013;&#35201;&#20040;&#20026;&#21457;&#29616;&#26032;&#27010;&#24565;&#25552;&#20379;&#26377;&#26631;&#21495;&#30340;&#25968;&#25454;&#65288;&#20363;&#22914; NCD&#65289;&#65292;&#35201;&#20040;&#23398;&#20064;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#22686;&#37327;&#27493;&#39588;&#20013;&#21457;&#29983;&#65288;&#20363;&#22914;&#31867; iNCD&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026; MSc-iNCD&#65292;&#20854;&#20013;&#23398;&#20064;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#65292;&#24182;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#22312;&#36739;&#38271;&#30340;&#23398;&#20064;&#24773;&#22659;&#19979;&#20855;&#26377;&#24377;&#24615;&#65292;&#32780;&#19988;&#19982;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#32447;&#30340;&#26377;&#25928;&#24615;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#22522;&#20934;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering novel concepts from unlabelled data and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where either access to labelled data is provided for discovering novel concepts (e.g., NCD) or learning occurs for a limited number of incremental steps (e.g., class-iNCD). In this work we challenge the status quo and propose a more challenging and practical learning paradigm called MSc-iNCD, where learning occurs continuously and unsupervisedly, while exploiting the rich priors from large-scale pre-trained models. To this end, we propose simple baselines that are not only resilient under longer learning scenarios, but are surprisingly strong when compared with sophisticated state-of-the-art methods. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines, which significantly raises the bar.
&lt;/p&gt;</description></item><item><title>&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#38656;&#30830;&#20445;&#22312;&#21512;&#29702;&#20351;&#29992;&#30340;&#33539;&#22260;&#20869;&#65292;&#20294;&#36825;&#24182;&#19981;&#20445;&#35777;&#12290;&#22312;&#20351;&#29992;&#26377;&#29256;&#26435;&#20869;&#23481;&#26102;&#38656;&#35201;&#27880;&#24847;&#39118;&#38505;&#24182;&#21487;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.15715</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#19982;&#21512;&#29702;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Foundation Models and Fair Use. (arXiv:2303.15715v1 [cs.CY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15715
&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#38656;&#30830;&#20445;&#22312;&#21512;&#29702;&#20351;&#29992;&#30340;&#33539;&#22260;&#20869;&#65292;&#20294;&#36825;&#24182;&#19981;&#20445;&#35777;&#12290;&#22312;&#20351;&#29992;&#26377;&#29256;&#26435;&#20869;&#23481;&#26102;&#38656;&#35201;&#27880;&#24847;&#39118;&#38505;&#24182;&#21487;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26159;&#22522;&#20110;&#26377;&#29256;&#26435;&#30340;&#26448;&#26009;&#35757;&#32451;&#20986;&#26469;&#30340;&#12290;&#22312;&#25968;&#25454;&#21019;&#24314;&#32773;&#27809;&#26377;&#24471;&#21040;&#36866;&#24403;&#30340;&#24402;&#23646;&#25110;&#36180;&#20607;&#26102;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#24102;&#26469;&#27861;&#24459;&#21644;&#36947;&#24503;&#39118;&#38505;&#12290;&#22312;&#32654;&#22269;&#21644;&#20854;&#20182;&#20960;&#20010;&#22269;&#23478;&#65292;&#29256;&#26435;&#20869;&#23481;&#21487;&#33021;&#34987;&#29992;&#20110;&#26500;&#24314;&#22522;&#30784;&#27169;&#22411;&#32780;&#19981;&#20250;&#20135;&#29983;&#27861;&#24459;&#36131;&#20219;&#65292;&#36825;&#26159;&#22240;&#20026;&#21512;&#29702;&#20351;&#29992;&#21407;&#21017;&#30340;&#23384;&#22312;&#12290;&#20294;&#26159;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#22914;&#26524;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#20135;&#29983;&#20102;&#19982;&#29256;&#26435;&#25968;&#25454;&#30456;&#20284;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#24433;&#21709;&#21040;&#36825;&#20123;&#25968;&#25454;&#24066;&#22330;&#30340;&#24773;&#20917;&#19979;&#65292;&#21512;&#29702;&#20351;&#29992;&#21407;&#21017;&#23558;&#19981;&#20877;&#36866;&#29992;&#20110;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#24378;&#35843;&#21512;&#29702;&#20351;&#29992;&#21407;&#21017;&#24182;&#19981;&#20445;&#35777;&#65292;&#21487;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#25165;&#33021;&#30830;&#20445;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#22312;&#21512;&#29702;&#20351;&#29992;&#30340;&#33539;&#22260;&#20869;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20110;&#26377;&#29256;&#26435;&#20869;&#23481;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#30456;&#20851;&#30340;&#32654;&#22269;&#26696;&#20363;&#27861;&#65292;&#24182;&#31867;&#27604;&#29983;&#25104;&#25991;&#26412;&#12289;&#28304;&#20195;&#30721;&#21644;&#21487;&#35270;&#21270;&#31561;&#21487;&#33021;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing foundation models are trained on copyrighted material. Deploying these models can pose both legal and ethical risks when data creators fail to receive appropriate attribution or compensation. In the United States and several other countries, copyrighted content may be used to build foundation models without incurring liability due to the fair use doctrine. However, there is a caveat: If the model produces output that is similar to copyrighted data, particularly in scenarios that affect the market of that data, fair use may no longer apply to the output of the model. In this work, we emphasize that fair use is not guaranteed, and additional work may be necessary to keep model development and deployment squarely in the realm of fair use. First, we survey the potential risks of developing and deploying foundation models based on copyrighted content. We review relevant U.S. case law, drawing parallels to existing and potential applications for generating text, source code, and vis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22768;&#23398;&#21644;&#35270;&#35273;&#20449;&#24687;&#36716;&#21270;&#20026;&#25991;&#26412;&#25551;&#36848;&#24182;&#19982;&#21475;&#35821;&#25991;&#26412;&#20018;&#32852;&#65292;&#20174;&#32780;&#23558;&#38750;&#35821;&#35328;&#20449;&#24687;&#32435;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#25910;&#38598;&#21644;&#24314;&#27169;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#38750;&#35821;&#35328;&#32447;&#32034;&#19982;&#35821;&#35328;&#26080;&#32541;&#38598;&#25104;&#65292;&#23545;&#20110;&#22810;&#27169;&#24577;&#34892;&#20026;&#29702;&#35299;&#20219;&#21153;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.15430</link><description>&lt;p&gt;
TextMI:&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#21270;&#20026;&#25991;&#26412;&#24418;&#24335;&#65292;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#38750;&#35821;&#35328;&#32447;&#32034;
&lt;/p&gt;
&lt;p&gt;
TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models. (arXiv:2303.15430v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22768;&#23398;&#21644;&#35270;&#35273;&#20449;&#24687;&#36716;&#21270;&#20026;&#25991;&#26412;&#25551;&#36848;&#24182;&#19982;&#21475;&#35821;&#25991;&#26412;&#20018;&#32852;&#65292;&#20174;&#32780;&#23558;&#38750;&#35821;&#35328;&#20449;&#24687;&#32435;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#25910;&#38598;&#21644;&#24314;&#27169;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#38750;&#35821;&#35328;&#32447;&#32034;&#19982;&#35821;&#35328;&#26080;&#32541;&#38598;&#25104;&#65292;&#23545;&#20110;&#22810;&#27169;&#24577;&#34892;&#20026;&#29702;&#35299;&#20219;&#21153;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#21508;&#31181;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21516;&#19968;&#27169;&#22411;&#26080;&#27861;&#24212;&#29992;&#20110;&#22810;&#27169;&#24335;&#34892;&#20026;&#29702;&#35299;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#35270;&#39057;&#24773;&#24863;/&#24189;&#40664;&#26816;&#27979;&#65289;&#65292;&#38500;&#38750;&#21487;&#20197;&#23558;&#38750;&#35821;&#35328;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#22768;&#23398;&#21644;&#35270;&#35273;&#65289;&#19982;&#35821;&#35328;&#38598;&#25104;&#12290;&#32852;&#21512;&#24314;&#27169;&#22810;&#20010;&#27169;&#24577;&#26174;&#30528;&#22686;&#21152;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#24182;&#20351;&#35757;&#32451;&#36807;&#31243;&#21464;&#24471;&#38656;&#25968;&#25454;&#37327;&#22823;&#12290;&#34429;&#28982;&#36890;&#36807;&#32593;&#32476;&#21487;&#20197;&#33719;&#24471;&#22823;&#37327;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#20294;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#35270;&#39057;&#25968;&#25454;&#38598;&#26497;&#20854;&#26114;&#36149;&#65292;&#26080;&#35770;&#26159;&#26102;&#38388;&#19978;&#36824;&#26159;&#37329;&#38065;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#20197;&#25991;&#26412;&#24418;&#24335;&#21576;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#25104;&#21151;&#22320;&#23558;&#38750;&#35821;&#35328;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22768;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36716;&#21270;&#20026;&#23545;&#24212;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#19982;&#21475;&#35821;&#25991;&#26412;&#20018;&#32852;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#22686;&#24378;&#30340;&#36755;&#20837;&#39304;&#36865;&#32473;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#20854;&#24615;&#33021;&#19982;&#30452;&#25509;&#20351;&#29992;&#38750;&#35821;&#35328;&#29305;&#24449;&#30340;&#30456;&#21516;&#27169;&#22411;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;TextMI&#19981;&#20165;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#25910;&#38598;&#21644;&#24314;&#27169;&#36807;&#31243;&#65292;&#32780;&#19988;&#23454;&#29616;&#20102;&#38750;&#35821;&#35328;&#32447;&#32034;&#19982;&#35821;&#35328;&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models have recently achieved ground-breaking performance in a wide variety of language understanding tasks. However, the same model can not be applied to multimodal behavior understanding tasks (e.g., video sentiment/humor detection) unless non-verbal features (e.g., acoustic and visual) can be integrated with language. Jointly modeling multiple modalities significantly increases the model complexity, and makes the training process data-hungry. While an enormous amount of text data is available via the web, collecting large-scale multimodal behavioral video datasets is extremely expensive, both in terms of time and money. In this paper, we investigate whether large language models alone can successfully incorporate non-verbal information when they are presented in textual form. We present a way to convert the acoustic and visual information into corresponding textual descriptions and concatenate them with the spoken text. We feed this augmented input to a pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39044;&#27979;&#19981;&#21516;&#36890;&#36135;&#33192;&#32960;&#29575;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#36229;&#36234;&#20256;&#32479;&#27169;&#22411;&#65292;&#25104;&#20026;&#37329;&#34701;&#20915;&#31574;&#20013;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.15364</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#32960;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Inflation forecasting with attention based transformer neural networks. (arXiv:2303.15364v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39044;&#27979;&#19981;&#21516;&#36890;&#36135;&#33192;&#32960;&#29575;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#36229;&#36234;&#20256;&#32479;&#27169;&#22411;&#65292;&#25104;&#20026;&#37329;&#34701;&#20915;&#31574;&#20013;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#32960;&#26159;&#36164;&#37329;&#37197;&#32622;&#20915;&#31574;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#20854;&#39044;&#27979;&#26159;&#25919;&#24220;&#21644;&#20013;&#22830;&#38134;&#34892;&#30340;&#22522;&#26412;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39044;&#27979;&#21462;&#20915;&#20110;&#20302;&#39057;&#39640;&#27874;&#21160;&#25968;&#25454;&#19988;&#32570;&#20047;&#28165;&#26224;&#30340;&#35299;&#37322;&#21464;&#37327;&#65292;&#22240;&#27492;&#39044;&#27979;&#36890;&#32960;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#65288;&#28145;&#24230;&#65289;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24050;&#26174;&#31034;&#20986;&#24778;&#20154;&#30340;&#32467;&#26524;&#65292;&#36880;&#28176;&#25104;&#20026;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#26631;&#26438;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#39044;&#27979;&#19981;&#21516;&#36890;&#32960;&#29575;&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#19982;&#32463;&#20856;&#26102;&#38388;&#24207;&#21015;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#25913;&#36827;&#21518;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;16&#20010;&#23454;&#39564;&#20013;&#24179;&#22343;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;6&#20010;&#23454;&#39564;&#65292;&#22312;&#30740;&#31350;&#30340;4&#20010;&#36890;&#36135;&#33192;&#32960;&#29575;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#36890;&#32960;&#39044;&#27979;&#27169;&#22411;&#26377;&#36229;&#36234;&#20256;&#32479;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24182;&#21487;&#20197;&#25104;&#20026;&#37329;&#34701;&#20915;&#31574;&#20013;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inflation is a major determinant for allocation decisions and its forecast is a fundamental aim of governments and central banks. However, forecasting inflation is not a trivial task, as its prediction relies on low frequency, highly fluctuating data with unclear explanatory variables. While classical models show some possibility of predicting inflation, reliably beating the random walk benchmark remains difficult. Recently, (deep) neural networks have shown impressive results in a multitude of applications, increasingly setting the new state-of-the-art. This paper investigates the potential of the transformer deep neural network architecture to forecast different inflation rates. The results are compared to a study on classical time series and machine learning models. We show that our adapted transformer, on average, outperforms the baseline in 6 out of 16 experiments, showing best scores in two out of four investigated inflation rates. Our results demonstrate that a transformer based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#37327;&#26377;&#25928;&#22320;&#21435;&#22122;&#26174;&#24494;&#38236;&#22270;&#20687;&#65292;&#20943;&#36731;&#25968;&#25454;&#33719;&#21462;&#36127;&#25285;&#65292;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.15214</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#36890;&#29992;&#26174;&#24494;&#38236;&#22270;&#20687;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Generalizable Denoising of Microscopy Images using Generative Adversarial Networks and Contrastive Learning. (arXiv:2303.15214v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#37327;&#26377;&#25928;&#22320;&#21435;&#22122;&#26174;&#24494;&#38236;&#22270;&#20687;&#65292;&#20943;&#36731;&#25968;&#25454;&#33719;&#21462;&#36127;&#25285;&#65292;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#24494;&#38236;&#22270;&#20687;&#24448;&#24448;&#21463;&#21040;&#39640;&#27700;&#24179;&#30340;&#22122;&#22768;&#24178;&#25200;&#65292;&#36825;&#20250;&#38459;&#30861;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#19982;&#32467;&#26500;&#20445;&#25345;&#30340;&#25439;&#22833;&#39033;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#35757;&#32451;&#65292;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#36827;&#19968;&#27493;&#25552;&#39640;&#21435;&#22122;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#22312;&#19977;&#20010;&#33879;&#21517;&#30340;&#26174;&#24494;&#38236;&#25104;&#20687;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#21435;&#22122;&#36136;&#37327;&#65292;&#20943;&#36731;&#33719;&#21462;&#25104;&#23545;&#25968;&#25454;&#30340;&#36127;&#25285;&#65292;&#24182;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21487;&#36731;&#26494;&#25193;&#23637;&#21040;&#26174;&#24494;&#38236;&#22270;&#20687;&#20197;&#22806;&#30340;&#20854;&#20182;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Microscopy images often suffer from high levels of noise, which can hinder further analysis and interpretation. Content-aware image restoration (CARE) methods have been proposed to address this issue, but they often require large amounts of training data and suffer from over-fitting. To overcome these challenges, we propose a novel framework for few-shot microscopy image denoising. Our approach combines a generative adversarial network (GAN) trained via contrastive learning (CL) with two structure preserving loss terms (Structural Similarity Index and Total Variation loss) to further improve the quality of the denoised images using little data. We demonstrate the effectiveness of our method on three well-known microscopy imaging datasets, and show that we can drastically reduce the amount of training data while retaining the quality of the denoising, thus alleviating the burden of acquiring paired data and enabling few-shot learning. The proposed framework can be easily extended to oth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#35777;&#26126;$\ell_2$-norm&#19979;&#30340;&#26679;&#26412;&#22806;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#32771;&#34385;&#32593;&#32476;&#26550;&#26500;&#21644;&#20855;&#20307;&#32452;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#25913;&#21892;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.14961</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;&#36827;&#34892;&#35748;&#35777;&#21644;&#23545;&#25239;&#24615;&#30340;&#40065;&#26834;&#30340;&#26679;&#26412;&#22806;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection. (arXiv:2303.14961v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#35777;&#26126;$\ell_2$-norm&#19979;&#30340;&#26679;&#26412;&#22806;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#32771;&#34385;&#32593;&#32476;&#26550;&#26500;&#21644;&#20855;&#20307;&#32452;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#25913;&#21892;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#19981;&#26029;&#25193;&#23637;&#65292;&#30830;&#20445;&#20854;&#23433;&#20840;&#24615;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#35782;&#21035;&#32473;&#23450;&#26679;&#26412;&#26159;&#21542;&#26469;&#33258;&#35757;&#32451;&#20998;&#24067;&#65292;&#25110;&#32773;&#26159;&#19968;&#20010;&#8220;&#26679;&#26412;&#22806;&#8221;&#65288;OOD&#65289;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#23545;&#25163;&#21487;&#20197;&#20197;&#19968;&#31181;&#23548;&#33268;&#20998;&#31867;&#22120;&#20570;&#20986;&#33258;&#20449;&#39044;&#27979;&#30340;&#26041;&#24335;&#25805;&#32437;OOD&#26679;&#26412;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36755;&#20837;&#30340;L2&#33539;&#22260;&#20869;&#35777;&#26126;&#22312;&#19981;&#32771;&#34385;&#32593;&#32476;&#26550;&#26500;&#20197;&#21450;&#19981;&#38656;&#35201;&#29305;&#23450;&#32452;&#20214;&#25110;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;OOD&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#25216;&#26415;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#20110;&#20998;&#24067;&#26679;&#26412;&#30340;&#39640;&#27700;&#24179;&#30340;&#35748;&#35777;&#21644;&#23545;&#25239;&#30340;&#32467;&#26524;&#12290;&#22312;CIFAR10/100&#30340;&#25152;&#26377;OOD&#26816;&#27979;&#25351;&#26631;&#30340;&#24179;&#22343;&#20540;&#26174;&#31034;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#25552;&#39640;&#20102;&#32422;13&#65285;/ 5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of machine learning continues to expand, the importance of ensuring its safety cannot be overstated. A key concern in this regard is the ability to identify whether a given sample is from the training distribution, or is an "Out-Of-Distribution" (OOD) sample. In addition, adversaries can manipulate OOD samples in ways that lead a classifier to make a confident prediction. In this study, we present a novel approach for certifying the robustness of OOD detection within a $\ell_2$-norm around the input, regardless of network architecture and without the need for specific components or additional training. Further, we improve current techniques for detecting adversarial attacks on OOD samples, while providing high levels of certified and adversarial robustness on in-distribution samples. The average of all OOD detection metrics on CIFAR10/100 shows an increase of $\sim 13 \% / 5\%$ relative to previous approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#23610;&#24230;&#19981;&#21464;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#19981;&#20351;&#29992;&#31354;&#38388;&#21367;&#31215;&#25110;&#20998;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#23618;&#20869;&#29305;&#24449;&#22270;&#20998;&#35299;&#20026;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#24182;&#36890;&#36807;&#25554;&#20540;&#36825;&#20123;&#29305;&#24449;&#29983;&#25104;&#22270;&#20687;&#65292;&#20351;&#24471;&#27169;&#22411;&#20855;&#26377;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.14157</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#34892;&#21015;&#20132;&#38169;&#20687;&#32032;&#21512;&#25104;&#30340;&#39640;&#25928;&#23610;&#24230;&#19981;&#21464;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis. (arXiv:2303.14157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14157
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#23610;&#24230;&#19981;&#21464;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#19981;&#20351;&#29992;&#31354;&#38388;&#21367;&#31215;&#25110;&#20998;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#23618;&#20869;&#29305;&#24449;&#22270;&#20998;&#35299;&#20026;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#24182;&#36890;&#36807;&#25554;&#20540;&#36825;&#20123;&#29305;&#24449;&#29983;&#25104;&#22270;&#20687;&#65292;&#20351;&#24471;&#27169;&#22411;&#20855;&#26377;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#24847;&#27604;&#20363;&#22270;&#20687;&#21512;&#25104;&#20026;&#21512;&#25104;&#22312;&#20219;&#24847;&#27604;&#20363;&#19979;&#21512;&#25104;&#36924;&#30495;&#22270;&#20687;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#36229;&#20986;&#20102;2K&#20998;&#36776;&#29575;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GAN&#30340;&#35299;&#20915;&#26041;&#26696;&#36807;&#24230;&#20381;&#36182;&#20110;&#21367;&#31215;&#21644;&#20998;&#23618;&#26550;&#26500;&#65292;&#22312;&#32553;&#25918;&#36755;&#20986;&#20998;&#36776;&#29575;&#26102;&#20250;&#24341;&#20837;&#19981;&#19968;&#33268;&#24615;&#21644;&#8220;&#32441;&#29702;&#31896;&#36830;&#8221;&#38382;&#39064;&#12290;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#22522;&#20110;INR&#30340;&#29983;&#25104;&#22120;&#20174;&#35774;&#35745;&#19978;&#26159;&#23610;&#24230;&#31561;&#21464;&#30340;&#65292;&#20294;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#32531;&#24930;&#30340;&#25512;&#29702;&#22952;&#30861;&#20102;&#36825;&#20123;&#32593;&#32476;&#22312;&#22823;&#35268;&#27169;&#25110;&#23454;&#26102;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65306;&#20855;&#26377;&#34892;&#21015;&#20132;&#38169;&#20687;&#32032;&#21512;&#25104;&#30340;&#21015;-&#34892;&#20132;&#38169;&#20687;&#32032;&#21512;&#25104;&#65288;$\textbf{CREPS}$&#65289;&#12290;&#19981;&#20351;&#29992;&#20219;&#20309;&#31354;&#38388;&#21367;&#31215;&#25110;&#20174;&#31895;&#21040;&#32454;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#33410;&#30465;&#20869;&#23384;&#21344;&#29992;&#24182;&#20351;&#31995;&#32479;&#21487;&#25193;&#23637;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#32447;&#34920;&#31034;&#27861;&#65292;&#23558;&#23618;&#20869;&#29305;&#24449;&#22270;&#20998;&#35299;&#20026;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#29420;&#31435;&#8220;&#21402;&#26465;&#8221;&#25554;&#20540;&#36825;&#20123;&#26465;&#24102;&#30340;&#34701;&#21512;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;$\textbf{CREPS}$&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#20445;&#25345;&#26356;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#30340;&#21516;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any-scale image synthesis offers an efficient and scalable solution to synthesize photo-realistic images at any scale, even going beyond 2K resolution. However, existing GAN-based solutions depend excessively on convolutions and a hierarchical architecture, which introduce inconsistency and the $``$texture sticking$"$ issue when scaling the output resolution. From another perspective, INR-based generators are scale-equivariant by design, but their huge memory footprint and slow inference hinder these networks from being adopted in large-scale or real-time systems. In this work, we propose $\textbf{C}$olumn-$\textbf{R}$ow $\textbf{E}$ntangled $\textbf{P}$ixel $\textbf{S}$ynthesis ($\textbf{CREPS}$), a new generative model that is both efficient and scale-equivariant without using any spatial convolutions or coarse-to-fine design. To save memory footprint and make the system scalable, we employ a novel bi-line representation that decomposes layer-wise feature maps into separate $``$thick
&lt;/p&gt;</description></item><item><title>InDI&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#65292;&#20462;&#22797;&#25928;&#26524;&#26356;&#20855;&#26377;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.11435</link><description>&lt;p&gt;
&#30452;&#25509;&#36845;&#20195;&#21453;&#28436;&#65306;&#22270;&#20687;&#20462;&#22797;&#30340;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration. (arXiv:2303.11435v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11435
&lt;/p&gt;
&lt;p&gt;
InDI&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#65292;&#20462;&#22797;&#25928;&#26524;&#26356;&#20855;&#26377;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#36845;&#20195;&#21453;&#28436;&#65288;InDI&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#23427;&#36991;&#20813;&#20102;&#25152;&#35859;&#30340;&#8220;&#22343;&#20540;&#22238;&#24402;&#8221;&#25928;&#24212;&#65292;&#24182;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#12290;&#23427;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#23454;&#29616;&#65292;&#31867;&#20284;&#20110;&#29983;&#25104;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#22270;&#20687;&#20462;&#22797;&#26159;&#19968;&#20010;&#27424;&#23450;&#38382;&#39064;&#65292;&#22810;&#20010;&#39640;&#36136;&#37327;&#22270;&#20687;&#37117;&#21487;&#33021;&#26159;&#32473;&#23450;&#20302;&#36136;&#37327;&#36755;&#20837;&#30340;&#21487;&#34892;&#37325;&#26500;&#12290;&#22240;&#27492;&#65292;&#21333;&#27493;&#22238;&#24402;&#27169;&#22411;&#30340;&#32467;&#26524;&#36890;&#24120;&#26159;&#25152;&#26377;&#21487;&#33021;&#35299;&#37322;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#22240;&#27492;&#32570;&#20047;&#32454;&#33410;&#21644;&#30495;&#23454;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called ``regression to the mean'' effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.  Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. % The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.  While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23558;&#22270;&#20687;&#33267;&#22270;&#20687;&#21644;&#31867;&#21035;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#32452;&#21512;&#65292;&#29992;&#20110;&#26377;&#20803;&#25968;&#25454;&#26631;&#31614;&#30340;&#22270;&#20687;&#37325;&#24314;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31867;&#21035;&#24341;&#23548;&#30340;&#22270;&#20687;&#33267;&#22270;&#20687;&#25193;&#25955;&#21487;&#20197;&#25552;&#21319;&#22270;&#20687;&#37325;&#24314;&#30340;&#20869;&#23481;&#65292;&#20248;&#20110;&#26410;&#24341;&#23548;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08863</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#22270;&#20687;&#25193;&#25955;&#65306;&#36890;&#36807;&#31867;&#21035;&#26631;&#31614;&#20174;&#20142;&#22330;&#22270;&#20687;&#20013;&#32454;&#32990;&#32472;&#30011;
&lt;/p&gt;
&lt;p&gt;
Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels. (arXiv:2303.08863v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23558;&#22270;&#20687;&#33267;&#22270;&#20687;&#21644;&#31867;&#21035;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#32452;&#21512;&#65292;&#29992;&#20110;&#26377;&#20803;&#25968;&#25454;&#26631;&#31614;&#30340;&#22270;&#20687;&#37325;&#24314;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31867;&#21035;&#24341;&#23548;&#30340;&#22270;&#20687;&#33267;&#22270;&#20687;&#25193;&#25955;&#21487;&#20197;&#25552;&#21319;&#22270;&#20687;&#37325;&#24314;&#30340;&#20869;&#23481;&#65292;&#20248;&#20110;&#26410;&#24341;&#23548;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#65292;&#24120;&#24120;&#20986;&#29616;&#24102;&#26377;&#33258;&#30001;&#25110;&#24265;&#20215;&#20803;&#25968;&#25454;&#24418;&#24335;&#30340;&#31867;&#21035;&#26631;&#31614;&#30340;&#22270;&#20687;&#37325;&#24314;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#24341;&#23548;&#25110;&#39118;&#26684;&#36716;&#31227;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#26080;&#27861;&#36716;&#21270;&#20026;&#25552;&#20379;&#31163;&#25955;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#21487;&#23558;&#22270;&#20687;&#33267;&#22270;&#20687;&#21644;&#31867;&#21035;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#32452;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#26174;&#24494;&#38236;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#21644;&#27809;&#26377;&#20803;&#25968;&#25454;&#26631;&#31614;&#12290;&#36890;&#36807;&#25506;&#32034;&#20855;&#26377;&#30456;&#20851;&#26631;&#31614;&#30340;&#22270;&#20687;&#33267;&#22270;&#20687;&#25193;&#25955;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#21035;&#24341;&#23548;&#30340;&#22270;&#20687;&#33267;&#22270;&#20687;&#25193;&#25955;&#21487;&#20197;&#25552;&#39640;&#37325;&#24314;&#22270;&#20687;&#30340;&#26377;&#24847;&#20041;&#20869;&#23481;&#65292;&#24182;&#22312;&#26377;&#29992;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#26410;&#24341;&#23548;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-to-image reconstruction problems with free or inexpensive metadata in the form of class labels appear often in biological and medical image domains. Existing text-guided or style-transfer image-to-image approaches do not translate to datasets where additional information is provided as discrete classes. We introduce and implement a model which combines image-to-image and class-guided denoising diffusion probabilistic models. We train our model on a real-world dataset of microscopy images used for drug discovery, with and without incorporating metadata labels. By exploring the properties of image-to-image diffusion with relevant labels, we show that class-guided image-to-image diffusion can improve the meaningful content of the reconstructed images and outperform the unguided model in useful downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#20013;&#30340;&#37325;&#35201;&#35774;&#35745;&#36873;&#25321;&#65292;&#21253;&#25324;&#35299;&#32806;&#26680;&#24515;&#26550;&#26500;&#21644;&#19981;&#30830;&#23450;&#24615;&#22836;&#26041;&#26696;&#30340;&#35757;&#32451;&#12289;&#26680;&#24515;&#26550;&#26500;&#34920;&#36798;&#33021;&#21147;&#12289;&#36991;&#20813;&#29305;&#24449;&#23849;&#28291;&#30340;&#39069;&#22806;&#26550;&#26500;&#32422;&#26463;&#21644;&#20808;&#39564;&#24433;&#21709;&#31561;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35757;&#32451;&#26041;&#26696;&#21644;&#26680;&#24515;&#26550;&#26500;&#34920;&#36798;&#33021;&#21147;&#23545;&#19981;&#30830;&#23450;&#24615;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36991;&#20813;&#29305;&#24449;&#23849;&#28291;&#30340;&#39069;&#22806;&#26550;&#26500;&#32422;&#26463;&#21487;&#33021;&#20250;&#30772;&#22351;OOD&#27867;&#21270;&#21644;&#26816;&#27979;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;DUMs&#23450;&#20041;&#30340;&#20808;&#39564;&#21017;&#23545;&#26368;&#32456;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.05796</link><description>&lt;p&gt;
&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#20013;&#30340;&#35757;&#32451;&#12289;&#26550;&#26500;&#21644;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Training, Architecture, and Prior for Deterministic Uncertainty Methods. (arXiv:2303.05796v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#20013;&#30340;&#37325;&#35201;&#35774;&#35745;&#36873;&#25321;&#65292;&#21253;&#25324;&#35299;&#32806;&#26680;&#24515;&#26550;&#26500;&#21644;&#19981;&#30830;&#23450;&#24615;&#22836;&#26041;&#26696;&#30340;&#35757;&#32451;&#12289;&#26680;&#24515;&#26550;&#26500;&#34920;&#36798;&#33021;&#21147;&#12289;&#36991;&#20813;&#29305;&#24449;&#23849;&#28291;&#30340;&#39069;&#22806;&#26550;&#26500;&#32422;&#26463;&#21644;&#20808;&#39564;&#24433;&#21709;&#31561;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35757;&#32451;&#26041;&#26696;&#21644;&#26680;&#24515;&#26550;&#26500;&#34920;&#36798;&#33021;&#21147;&#23545;&#19981;&#30830;&#23450;&#24615;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36991;&#20813;&#29305;&#24449;&#23849;&#28291;&#30340;&#39069;&#22806;&#26550;&#26500;&#32422;&#26463;&#21487;&#33021;&#20250;&#30772;&#22351;OOD&#27867;&#21270;&#21644;&#26816;&#27979;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;DUMs&#23450;&#20041;&#30340;&#20808;&#39564;&#21017;&#23545;&#26368;&#32456;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#26500;&#24314;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;DUMs&#30340;&#37325;&#35201;&#35774;&#35745;&#36873;&#25321;&#65306;&#65288;1&#65289;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#26680;&#24515;&#26550;&#26500;&#21644;&#19981;&#30830;&#23450;&#24615;&#22836;&#26041;&#26696;&#35299;&#32806;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#24615;&#33021;&#12290;&#65288;2&#65289;&#25105;&#20204;&#35777;&#26126;&#20102;&#26680;&#24515;&#26550;&#26500;&#34920;&#36798;&#33021;&#21147;&#23545;&#19981;&#30830;&#23450;&#24615;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#36991;&#20813;&#29305;&#24449;&#23849;&#28291;&#30340;&#39069;&#22806;&#26550;&#26500;&#32422;&#26463;&#21487;&#33021;&#30772;&#22351;OOD&#27867;&#21270;&#21644;&#26816;&#27979;&#20043;&#38388;&#30340;&#25240;&#34935;&#12290; &#65288;3&#65289;&#19982;&#20854;&#20182;&#36125;&#21494;&#26031;&#27169;&#22411;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DUMs&#23450;&#20041;&#30340;&#20808;&#39564;&#23545;&#26368;&#32456;&#24615;&#33021;&#27809;&#26377;&#24378;&#28872;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and efficient uncertainty estimation is crucial to build reliable Machine Learning (ML) models capable to provide calibrated uncertainty estimates, generalize and detect Out-Of-Distribution (OOD) datasets. To this end, Deterministic Uncertainty Methods (DUMs) is a promising model family capable to perform uncertainty estimation in a single forward pass. This work investigates important design choices in DUMs: (1) we show that training schemes decoupling the core architecture and the uncertainty head schemes can significantly improve uncertainty performances. (2) we demonstrate that the core architecture expressiveness is crucial for uncertainty performance and that additional architecture constraints to avoid feature collapse can deteriorate the trade-off between OOD generalization and detection. (3) Contrary to other Bayesian models, we show that the prior defined by DUMs do not have a strong effect on the final performances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-ViT&#30693;&#35782;&#33976;&#39311;&#30340;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;PreFallKD&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#26816;&#27979;&#30693;&#35782;&#36716;&#31227;&#21040;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#29983;&#27169;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.03634</link><description>&lt;p&gt;
PreFallKD: &#22522;&#20110;CNN-ViT&#30693;&#35782;&#33976;&#39311;&#30340;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PreFallKD: Pre-Impact Fall Detection via CNN-ViT Knowledge Distillation. (arXiv:2303.03634v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-ViT&#30693;&#35782;&#33976;&#39311;&#30340;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;PreFallKD&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#26816;&#27979;&#30693;&#35782;&#36716;&#31227;&#21040;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#29983;&#27169;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a pre-impact fall detection system called PreFallKD, which uses CNN-ViT knowledge distillation to transfer detection knowledge from a pre-trained teacher model to a lightweight convolutional neural network student model. The system achieves a balance between detection performance and computational complexity.
&lt;/p&gt;
&lt;p&gt;
&#36300;&#20498;&#20107;&#25925;&#26159;&#32769;&#40836;&#21270;&#31038;&#20250;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;&#65292;&#20197;&#25903;&#25345;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#36300;&#20498;&#20445;&#25252;&#31995;&#32479;&#65292;&#20197;&#39044;&#38450;&#20005;&#37325;&#30340;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#21482;&#20351;&#29992;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#21040;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#35774;&#22791;&#21644;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;CNN-ViT&#30693;&#35782;&#33976;&#39311;&#30340;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;&#65292;&#21363;PreFallKD&#65292;&#20197;&#22312;&#26816;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#25152;&#25552;&#20986;&#30340;PreFallKD&#23558;&#26816;&#27979;&#30693;&#35782;&#20174;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#65288;&#35270;&#35273;&#21464;&#25442;&#22120;&#65289;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#65288;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;KFall&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23558;PreFallKD&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fall accidents are critical issues in an aging and aged society. Recently, many researchers developed pre-impact fall detection systems using deep learning to support wearable-based fall protection systems for preventing severe injuries. However, most works only employed simple neural network models instead of complex models considering the usability in resource-constrained mobile devices and strict latency requirements. In this work, we propose a novel pre-impact fall detection via CNN-ViT knowledge distillation, namely PreFallKD, to strike a balance between detection performance and computational complexity. The proposed PreFallKD transfers the detection knowledge from the pre-trained teacher model (vision transformer) to the student model (lightweight convolutional neural networks). Additionally, we apply data augmentation techniques to tackle issues of data imbalance. We conduct the experiment on the KFall public dataset and compare PreFallKD with other state-of-the-art models. The
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#32806;&#21512;&#22810;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32806;&#21512;&#22810;&#21464;&#37327;&#26144;&#23556;&#38382;&#39064;&#30340;&#38590;&#28857;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.02304</link><description>&lt;p&gt;
&#38024;&#23545;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32806;&#21512;&#22810;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Coupled Multiwavelet Neural Operator Learning for Coupled Partial Differential Equations. (arXiv:2303.02304v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#32806;&#21512;&#22810;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32806;&#21512;&#22810;&#21464;&#37327;&#26144;&#23556;&#38382;&#39064;&#30340;&#38590;&#28857;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#25551;&#36848;&#35768;&#22810;&#29289;&#29702;&#36807;&#31243;&#22797;&#26434;&#21160;&#24577;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#31070;&#32463;&#31639;&#23376;&#24050;&#32463;&#23637;&#31034;&#20986;&#36890;&#36807;&#22312;&#20613;&#37324;&#21494;/&#23567;&#27874;&#31354;&#38388;&#30452;&#25509;&#23398;&#20064;&#31215;&#20998;&#26680;&#26469;&#35299;&#20915;PDE&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#32806;&#21512;PDE&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#38590;&#28857;&#22312;&#20110;&#22788;&#29702;&#20989;&#25968;&#20043;&#38388;&#30340;&#32806;&#21512;&#26144;&#23556;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32806;&#21512;&#22810;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#65288;CMWNO&#65289;&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#23567;&#27874;&#31354;&#38388;&#20013;&#36827;&#34892;&#22810;&#23567;&#27874;&#20998;&#35299;&#21644;&#37325;&#26500;&#36807;&#31243;&#20013;&#35299;&#32806;&#21512;&#31215;&#20998;&#26680;&#12290;&#22312;&#35299;&#20915;Gray-Scott&#65288;GS&#65289;&#26041;&#31243;&#21644;&#38750;&#23616;&#37096;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#38382;&#39064;&#31561;&#32806;&#21512;PDE&#26041;&#38754;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#20808;&#21069;&#22522;&#20110;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;$L^2$&#35823;&#24046;&#34920;&#29616;&#20986;&#20102;$2\times \sim 4\times$&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coupled partial differential equations (PDEs) are key tasks in modeling the complex dynamics of many physical processes. Recently, neural operators have shown the ability to solve PDEs by learning the integral kernel directly in Fourier/Wavelet space, so the difficulty for solving the coupled PDEs depends on dealing with the coupled mappings between the functions. Towards this end, we propose a \textit{coupled multiwavelets neural operator} (CMWNO) learning scheme by decoupling the coupled integral kernels during the multiwavelet decomposition and reconstruction procedures in the Wavelet space. The proposed model achieves significantly higher accuracy compared to previous learning-based solvers in solving the coupled PDEs including Gray-Scott (GS) equations and the non-local mean field game (MFG) problem. According to our experimental results, the proposed model exhibits a $2\times \sim 4\times$ improvement relative $L$2 error compared to the best results from the state-of-the-art mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#31890;&#24230;&#39063;&#31890;&#29699;&#21644;&#26368;&#23567;&#29983;&#25104;&#26641;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#8220;&#22823;&#35268;&#27169;&#20248;&#20808;&#32423;&#8221;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#36991;&#20813;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#24182;&#21152;&#36895;&#20102;MST&#30340;&#26500;&#24314;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.01082</link><description>&lt;p&gt;
&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#39640;&#25928;&#26368;&#23567;&#29983;&#25104;&#26641;&#32858;&#31867;&#31639;&#27861;GBMST
&lt;/p&gt;
&lt;p&gt;
GBMST: An Efficient Minimum Spanning Tree Clustering Based on Granular-Ball Computing. (arXiv:2303.01082v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01082
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#31890;&#24230;&#39063;&#31890;&#29699;&#21644;&#26368;&#23567;&#29983;&#25104;&#26641;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#8220;&#22823;&#35268;&#27169;&#20248;&#20808;&#32423;&#8221;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#36991;&#20813;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#24182;&#21152;&#36895;&#20102;MST&#30340;&#26500;&#24314;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32858;&#31867;&#26041;&#27861;&#37117;&#22522;&#20110;&#21333;&#19968;&#31890;&#24230;&#30340;&#20449;&#24687;&#65292;&#22914;&#27599;&#20010;&#25968;&#25454;&#30340;&#36317;&#31163;&#21644;&#23494;&#24230;&#12290;&#36825;&#31181;&#26368;&#32454;&#31890;&#24230;&#30340;&#26041;&#27861;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#19988;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#31867;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31890;&#24230;&#39063;&#31890;&#29699;&#21644;&#26368;&#23567;&#29983;&#25104;&#26641;(MST)&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31895;&#31890;&#24230;&#39063;&#31890;&#29699;&#65292;&#28982;&#21518;&#20351;&#29992;&#39063;&#31890;&#29699;&#21644;MST&#23454;&#29616;&#22522;&#20110;&#8220;&#22823;&#35268;&#27169;&#20248;&#20808;&#32423;&#8221;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#36991;&#20813;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#24182;&#21152;&#24555;&#20102;MST&#30340;&#26500;&#24314;&#36807;&#31243;&#12290;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;&#25152;&#26377;&#20195;&#30721;&#24050;&#22312;https://github.com/xjnine/GBMST&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the existing clustering methods are based on a single granularity of information, such as the distance and density of each data. This most fine-grained based approach is usually inefficient and susceptible to noise. Therefore, we propose a clustering algorithm that combines multi-granularity Granular-Ball and minimum spanning tree (MST). We construct coarsegrained granular-balls, and then use granular-balls and MST to implement the clustering method based on "large-scale priority", which can greatly avoid the influence of outliers and accelerate the construction process of MST. Experimental results on several data sets demonstrate the power of the algorithm. All codes have been released at https://github.com/xjnine/GBMST.
&lt;/p&gt;</description></item><item><title>FuNVol&#26159;&#19968;&#20010;&#22810;&#36164;&#20135;&#38544;&#21547;&#27874;&#21160;&#29575;&#24066;&#22330;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#21644;&#31070;&#32463;SDE&#29983;&#25104;&#30495;&#23454;&#21382;&#21490;&#20215;&#26684;&#30340;IV&#34920;&#38754;&#24207;&#21015;&#65292;&#24182;&#22312;&#26080;&#38745;&#24577;&#22871;&#21033;&#30340;&#34920;&#38754;&#27425;&#27969;&#24418;&#20869;&#20135;&#29983;&#19968;&#33268;&#30340;&#24066;&#22330;&#24773;&#26223;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#27169;&#25311;&#34920;&#38754;&#36827;&#34892;&#23545;&#20914;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#29616;P&#65286;L&#19968;&#33268;&#30340;&#25439;&#30410;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.00859</link><description>&lt;p&gt;
FuNVol&#65306;&#20351;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#21644;&#31070;&#32463;SDE&#30340;&#22810;&#36164;&#20135;&#38544;&#21547;&#27874;&#21160;&#29575;&#24066;&#22330;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs. (arXiv:2303.00859v2 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00859
&lt;/p&gt;
&lt;p&gt;
FuNVol&#26159;&#19968;&#20010;&#22810;&#36164;&#20135;&#38544;&#21547;&#27874;&#21160;&#29575;&#24066;&#22330;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#20989;&#25968;&#20027;&#25104;&#20998;&#21644;&#31070;&#32463;SDE&#29983;&#25104;&#30495;&#23454;&#21382;&#21490;&#20215;&#26684;&#30340;IV&#34920;&#38754;&#24207;&#21015;&#65292;&#24182;&#22312;&#26080;&#38745;&#24577;&#22871;&#21033;&#30340;&#34920;&#38754;&#27425;&#27969;&#24418;&#20869;&#20135;&#29983;&#19968;&#33268;&#30340;&#24066;&#22330;&#24773;&#26223;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#27169;&#25311;&#34920;&#38754;&#36827;&#34892;&#23545;&#20914;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#29616;P&#65286;L&#19968;&#33268;&#30340;&#25439;&#30410;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#32467;&#21512;&#27010;&#29575;&#31215;&#20998;&#21464;&#25442;&#24809;&#32602;&#26469;&#29983;&#25104;&#22810;&#20010;&#36164;&#20135;&#30340;&#38544;&#21547;&#27874;&#21160;&#29575;&#34920;&#38754;&#24207;&#21015;&#65292;&#35813;&#26041;&#27861;&#24544;&#23454;&#20110;&#21382;&#21490;&#20215;&#26684;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;IV&#34920;&#38754;&#21644;&#20215;&#26684;&#30340;&#32852;&#21512;&#21160;&#24577;&#20135;&#29983;&#30340;&#24066;&#22330;&#24773;&#26223;&#19982;&#21382;&#21490;&#29305;&#24449;&#19968;&#33268;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#38745;&#24577;&#22871;&#21033;&#30340;&#34920;&#38754;&#27425;&#27969;&#24418;&#20869;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#27169;&#25311;&#34920;&#38754;&#36827;&#34892;&#23545;&#20914;&#20250;&#29983;&#25104;&#19982;&#23454;&#29616;P&#65286;L&#19968;&#33268;&#30340;&#25439;&#30410;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Here, we introduce a new approach for generating sequences of implied volatility (IV) surfaces across multiple assets that is faithful to historical prices. We do so using a combination of functional data analysis and neural stochastic differential equations (SDEs) combined with a probability integral transform penalty to reduce model misspecification. We demonstrate that learning the joint dynamics of IV surfaces and prices produces market scenarios that are consistent with historical features and lie within the sub-manifold of surfaces that are essentially free of static arbitrage. Finally, we demonstrate that delta hedging using the simulated surfaces generates profit and loss (P&amp;L) distributions that are consistent with realised P&amp;Ls.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#8212;&#8212;&#25152;&#26377;&#26641;&#30340;&#28151;&#21512;&#27169;&#22411;&#8212;&#8212;&#23427;&#22312;&#22788;&#29702;$n$&#20010;&#21464;&#37327;&#30340;&#24773;&#20917;&#26102;&#33021;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#20854;&#36793;&#38469;&#20272;&#35745;&#30340;&#31934;&#30830;&#35745;&#31639;&#26159;NP&#38590;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14202</link><description>&lt;p&gt;
&#25152;&#26377;&#26641;&#30340;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixtures of All Trees. (arXiv:2302.14202v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#8212;&#8212;&#25152;&#26377;&#26641;&#30340;&#28151;&#21512;&#27169;&#22411;&#8212;&#8212;&#23427;&#22312;&#22788;&#29702;$n$&#20010;&#21464;&#37327;&#30340;&#24773;&#20917;&#26102;&#33021;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#20854;&#36793;&#38469;&#20272;&#35745;&#30340;&#31934;&#30830;&#35745;&#31639;&#26159;NP&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#24418;&#22270;&#27169;&#22411;&#22240;&#21487;&#35745;&#31639;&#24615;&#32780;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#25215;&#35834;&#20110;&#29305;&#23450;&#30340;&#31232;&#30095;&#20381;&#36182;&#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#31216;&#20026;&#25152;&#26377;&#26641;&#30340;&#28151;&#21512;&#27169;&#22411;&#65306;&#21363;&#23545;$n$&#20010;&#21464;&#37327;&#19978;&#30340;&#25152;&#26377;&#21487;&#33021;($n^{n-2}$)&#30340;&#26641;&#24418;&#22270;&#27169;&#22411;&#36827;&#34892;&#28151;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21487;&#35745;&#31639;&#30340;&#26041;&#24335;&#19979;&#23545;&#28151;&#21512;&#27169;&#22411;(MoAT)&#36827;&#34892;&#32039;&#20945;&#30340;&#21442;&#25968;&#21270;&#65288;&#20351;&#29992;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#34920;&#31034;&#26041;&#24335;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#21487;&#35745;&#31639;&#20284;&#28982;&#20540;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#26641;&#24418;&#22270;&#27169;&#22411;&#30340;&#21487;&#35745;&#31639;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#36827;&#34892;&#36817;&#20284;&#25512;&#29702;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;MoAT&#27169;&#22411;&#20013;&#36793;&#38469;&#30340;&#31934;&#30830;&#35745;&#31639;&#26159;NP&#38590;&#38382;&#39064;&#12290;&#23454;&#38469;&#19978;&#65292;&#22312;&#19982;&#24378;&#22823;&#30340;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#22522;&#20934;&#30340;&#27604;&#36739;&#20013;&#65292;MoAT&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tree-shaped graphical models are widely used for their tractability. However, they unfortunately lack expressive power as they require committing to a particular sparse dependency structure. We propose a novel class of generative models called mixtures of all trees: that is, a mixture over all possible ($n^{n-2}$) tree-shaped graphical models over $n$ variables. We show that it is possible to parameterize this Mixture of All Trees (MoAT) model compactly (using a polynomial-size representation) in a way that allows for tractable likelihood computation and optimization via stochastic gradient descent. Furthermore, by leveraging the tractability of tree-shaped models, we devise fast-converging conditional sampling algorithms for approximate inference, even though our theoretical analysis suggests that exact computation of marginals in the MoAT model is NP-hard. Empirically, MoAT achieves state-of-the-art performance on density estimation benchmarks when compared against powerful probabili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#26377;&#19968;&#33268;&#30340;&#20248;&#21183;&#65292;&#20294;&#32477;&#23545;&#34920;&#29616;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#65292;&#40065;&#26834;&#24615;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.12095</link><description>&lt;p&gt;
&#35770;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65306;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#26377;&#19968;&#33268;&#30340;&#20248;&#21183;&#65292;&#20294;&#32477;&#23545;&#34920;&#29616;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#65292;&#40065;&#26834;&#24615;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#26368;&#36817;&#21457;&#24067;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26381;&#21153;&#65292;&#24182;&#22312;&#36807;&#21435;&#20960;&#20010;&#26376;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#23545;ChatGPT&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#20854;&#40065;&#26834;&#24615;&#65292;&#21363;&#23545;&#20110;&#26410;&#39044;&#26399;&#36755;&#20837;&#30340;&#34920;&#29616;&#65292;&#20173;&#19981;&#28165;&#26970;&#12290;&#40065;&#26834;&#24615;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#20013;&#29305;&#21035;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#35282;&#24230;&#23545;ChatGPT&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;AdvGLUE&#21644;ANLI&#22522;&#20934;&#26469;&#35780;&#20272;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#37319;&#29992;Flipkart&#35780;&#35770;&#21644;DDXPlus&#21307;&#23398;&#35786;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;OOD&#35780;&#20272;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20960;&#20010;&#27969;&#34892;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#23545;&#25239;&#24615;&#21644;OOD&#20998;&#31867;&#21644;&#32763;&#35793;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#20248;&#21183;&#12290;&#20294;&#26159;&#65292;&#32477;&#23545;&#30340;&#34920;&#29616;&#36828;&#38750;&#23436;&#32654;&#65292;&#36825;&#34920;&#26126;&#23545;&#25239;&#24615;&#21644;OOD&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#24515;&#38598;&#36827;&#34892;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#21387;&#32553;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#21319;&#32456;&#36523;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24212;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2302.11510</link><description>&lt;p&gt;
&#20351;&#29992;&#26680;&#24515;&#38598;&#30340;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#21387;&#32553;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20013;&#30340;&#32456;&#36523;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging. (arXiv:2302.11510v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#24515;&#38598;&#36827;&#34892;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#21387;&#32553;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#21319;&#32456;&#36523;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24212;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#26159;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#32456;&#36523;&#23398;&#20064;&#32467;&#21512;&#30340;&#19968;&#31181;&#27969;&#34892;&#31574;&#30053;&#12290;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#26088;&#22312;&#37325;&#36848;&#20197;&#21069;&#20219;&#21153;&#20013;&#30340;&#36873;&#25321;&#24615;&#32463;&#39564;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#30340;&#25216;&#26415;&#26159;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#20801;&#35768;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#32463;&#39564;&#12290;&#28982;&#32780;&#65292;&#23384;&#20648;&#25152;&#26377;&#20197;&#21069;&#20219;&#21153;&#30340;&#32463;&#39564;&#20250;&#20351;&#24471;&#20351;&#29992;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#30340;&#32456;&#36523;&#23398;&#20064;&#21464;&#24471;&#35745;&#31639;&#19978;&#38750;&#24120;&#26114;&#36149;&#21644;&#19981;&#20999;&#23454;&#38469;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selective experience replay is a popular strategy for integrating lifelong learning with deep reinforcement learning. Selective experience replay aims to recount selected experiences from previous tasks to avoid catastrophic forgetting. Furthermore, selective experience replay based techniques are model agnostic and allow experiences to be shared across different models. However, storing experiences from all previous tasks make lifelong learning using selective experience replay computationally very expensive and impractical as the number of tasks increase. To that end, we propose a reward distribution-preserving coreset compression technique for compressing experience replay buffers stored for selective experience replay.  We evaluated the coreset compression technique on the brain tumor segmentation (BRATS) dataset for the task of ventricle localization and on the whole-body MRI for localization of left knee cap, left kidney, right trochanter, left lung, and spleen. The coreset lifel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23376;&#22270;Weisfeiler-Lehman Tests&#65288;SWL&#65289;&#30340;&#35270;&#35282;&#23545;&#19968;&#33324;&#22522;&#20110;&#33410;&#28857;&#30340;&#23376;&#22270;GNN&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20005;&#26684;&#22686;&#38271;&#30340;SWL&#23436;&#25972;&#34920;&#36798;&#21147;&#23618;&#27425;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#22522;&#20110;&#33410;&#28857;&#30340;&#23376;&#22270;GNN&#37117;&#23646;&#20110;&#20845;&#20010;SWL&#31561;&#20215;&#31867;&#20043;&#19968;&#65292;&#20854;&#20013;SSWL&#23454;&#29616;&#20102;&#26368;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.07090</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;Weisfeiler-Lehman Tests&#30340;&#23376;&#22270;GNN&#23436;&#25972;&#34920;&#36798;&#21147;&#23618;&#27425;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Complete Expressiveness Hierarchy for Subgraph GNNs via Subgraph Weisfeiler-Lehman Tests. (arXiv:2302.07090v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23376;&#22270;Weisfeiler-Lehman Tests&#65288;SWL&#65289;&#30340;&#35270;&#35282;&#23545;&#19968;&#33324;&#22522;&#20110;&#33410;&#28857;&#30340;&#23376;&#22270;GNN&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20005;&#26684;&#22686;&#38271;&#30340;SWL&#23436;&#25972;&#34920;&#36798;&#21147;&#23618;&#27425;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#22522;&#20110;&#33410;&#28857;&#30340;&#23376;&#22270;GNN&#37117;&#23646;&#20110;&#20845;&#20010;SWL&#31561;&#20215;&#31867;&#20043;&#19968;&#65292;&#20854;&#20013;SSWL&#23454;&#29616;&#20102;&#26368;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23376;&#22270;GNN&#24050;&#25104;&#20026;&#24320;&#21457;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#35201;&#26041;&#21521;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26550;&#26500;&#65292;&#20294;&#30446;&#21069;&#23545;&#21508;&#31181;&#35774;&#35745;&#33539;&#20363;&#30340;&#34920;&#36798;&#33021;&#21147;&#24046;&#24322;&#20173;&#23384;&#22312;&#26377;&#38480;&#30340;&#29702;&#35299;&#65292;&#20063;&#19981;&#28165;&#26970;&#20160;&#20040;&#35774;&#35745;&#21407;&#21017;&#33021;&#20197;&#26368;&#23567;&#30340;&#26550;&#26500;&#22797;&#26434;&#24230;&#23454;&#29616;&#26368;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#65292;&#26412;&#25991;&#36890;&#36807;&#23376;&#22270;Weisfeiler-Lehman Tests&#65288;SWL&#65289;&#30340;&#35270;&#35282;&#23545;&#19968;&#33324;&#22522;&#20110;&#33410;&#28857;&#30340;&#23376;&#22270;GNN&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#32467;&#26524;&#26159;&#24314;&#31435;&#20102;&#19968;&#20010;SWL&#23436;&#25972;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#34920;&#36798;&#33021;&#21147;&#26159;&#20005;&#26684;&#22686;&#38271;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#22522;&#20110;&#33410;&#28857;&#30340;&#23376;&#22270;GNN&#37117;&#23646;&#20110;&#20845;&#20010;SWL&#31561;&#20215;&#31867;&#20043;&#19968;&#65292;&#20854;&#20013; $\mathsf{SSWL}$ &#23454;&#29616;&#20102;&#26368;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36825;&#20123;&#31561;&#20215;&#31867;&#22312;&#32534;&#30721;&#22270;&#36317;&#31163;&#21644;&#21452;&#32852;&#36890;&#24615;&#31561;&#23454;&#38469;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#20219;&#20309;&#33410;&#28857;&#30340;SWL&#31561;&#20215;&#31867;&#20197;&#21450;&#25972;&#20010;&#22270;&#24418;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, subgraph GNNs have emerged as an important direction for developing expressive graph neural networks (GNNs). While numerous architectures have been proposed, so far there is still a limited understanding of how various design paradigms differ in terms of expressive power, nor is it clear what design principle achieves maximal expressiveness with minimal architectural complexity. To address these fundamental questions, this paper conducts a systematic study of general node-based subgraph GNNs through the lens of Subgraph Weisfeiler-Lehman Tests (SWL). Our central result is to build a complete hierarchy of SWL with strictly growing expressivity. Concretely, we prove that any node-based subgraph GNN falls into one of the six SWL equivalence classes, among which $\mathsf{SSWL}$ achieves the maximal expressive power. We also study how these equivalence classes differ in terms of their practical expressiveness such as encoding graph distance and biconnectivity. Furthermore, we give
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05666</link><description>&lt;p&gt;
Jaccard&#24230;&#37327;&#25439;&#22833;&#65306;&#20351;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Jaccard&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. (arXiv:2302.05666v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
IoU&#25439;&#22833;&#26159;&#30452;&#25509;&#20248;&#21270;Jaccard&#25351;&#25968;&#30340;&#26367;&#20195;&#21697;&#12290;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#23558;IoU&#25439;&#22833;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#19968;&#37096;&#20998;&#65292;&#19982;&#20165;&#20248;&#21270;&#20687;&#32032;&#25439;&#22833;&#65288;&#22914;&#20132;&#21449;&#29109;&#25439;&#22833;&#65289;&#30456;&#27604;&#65292;&#23545;&#20110;Jaccard&#25351;&#25968;&#27979;&#37327;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#26174;&#30528;&#30340;IoU&#25439;&#22833;&#26159;&#36719;Jaccard&#25439;&#22833;&#21644;Lovasz-Softmax&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25439;&#22833;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#36719;&#26631;&#31614;&#19981;&#20860;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#65292;&#23427;&#20204;&#22312;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;&#36719;&#26631;&#31614;&#20860;&#23481;&#65292;&#19982;&#36719;Jaccard&#25439;&#22833;&#30456;&#21516;&#12290;&#20351;&#29992;JMLs&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26368;&#27969;&#34892;&#30340;&#36719;&#26631;&#31614;&#29992;&#20363;&#65306;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#19977;&#20010;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;Cityscapes&#12289;PASCAL VOC&#21644;DeepGlobe Land&#65289;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;DeepGlobe Land&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24212;&#29992;&#65288;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65289;&#20197;&#21450;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.04062</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Synthetic Data Generation: A Review. (arXiv:2302.04062v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04062
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24212;&#29992;&#65288;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65289;&#20197;&#21450;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#23384;&#22312;&#22810;&#31181;&#38382;&#39064;&#65292;&#22914;&#25968;&#25454;&#36136;&#37327;&#20302;&#65292;&#26377;&#38480;&#30340;&#25968;&#25454;&#28857;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27424;&#25311;&#21512;&#65292;&#30001;&#20110;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#30417;&#31649;&#38382;&#39064;&#38590;&#20197;&#35775;&#38382;&#25968;&#25454;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20197;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#26080;&#27861;&#20570;&#21040;&#30340;&#26041;&#24335;&#36827;&#34892;&#20849;&#20139;&#21644;&#20351;&#29992;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#29616;&#26377;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#35752;&#35770;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24037;&#20316;&#65306;&#65288;i&#65289;&#24212;&#29992;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65307;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65307;&#65288;iii&#65289;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data plays a crucial role in machine learning. However, in real-world applications, there are several problems with data, e.g., data are of low quality; a limited number of data points lead to under-fitting of the machine learning model; it is hard to access the data due to privacy, safety and regulatory concerns. Synthetic data generation offers a promising new avenue, as it can be shared and used in ways that real-world data cannot. This paper systematically reviews the existing works that leverage machine learning models for synthetic data generation. Specifically, we discuss the synthetic data generation works from several perspectives: (i) applications, including computer vision, speech, natural language, healthcare, and business; (ii) machine learning methods, particularly neural network architectures and deep generative models; (iii) privacy and fairness issue. In addition, we identify the challenges and opportunities in this emerging field and suggest future research directions
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21442;&#25968;&#21270;&#20102;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#30340;&#20195;&#20215;&#20989;&#25968;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#30456;&#27604;&#21407;&#22987;&#30340;&#32477;&#23545;&#20540;&#20195;&#20215;&#20989;&#25968;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#21487;&#35843;&#25972;&#21442;&#25968;&#30340;&#20195;&#20215;&#20989;&#25968;&#65292;&#36890;&#36807;&#35843;&#25972;&#21442;&#25968;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#19988;&#22312;&#22810;&#20010;&#26631;&#20934;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.10350</link><description>&lt;p&gt;
&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#30340;&#20195;&#20215;&#20989;&#25968;&#21442;&#25968;&#21270;&#21450;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Parameterizing the cost function of Dynamic Time Warping with application to time series classification. (arXiv:2301.10350v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21442;&#25968;&#21270;&#20102;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#30340;&#20195;&#20215;&#20989;&#25968;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#30456;&#27604;&#21407;&#22987;&#30340;&#32477;&#23545;&#20540;&#20195;&#20215;&#20989;&#25968;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#21487;&#35843;&#25972;&#21442;&#25968;&#30340;&#20195;&#20215;&#20989;&#25968;&#65292;&#36890;&#36807;&#35843;&#25972;&#21442;&#25968;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#19988;&#22312;&#22810;&#20010;&#26631;&#20934;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26102;&#38388;&#24207;&#21015;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#65292;&#23427;&#23558;&#20004;&#20010;&#24207;&#21015;&#20013;&#30340;&#28857;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#20123;&#23545;&#40784;&#25903;&#25345;&#26102;&#38388;&#32500;&#24230;&#30340;&#25197;&#26354;&#65292;&#20197;&#20801;&#35768;&#20197;&#19981;&#21516;&#36895;&#24230;&#23637;&#24320;&#30340;&#36807;&#31243;&#12290;&#36317;&#31163;&#26159;&#20219;&#20309;&#21487;&#20801;&#35768;&#26102;&#38388;&#32500;&#24230;&#25197;&#26354;&#30340;&#26368;&#23567;&#23545;&#40784;&#20195;&#20215;&#20043;&#21644;&#12290;&#20004;&#20010;&#28857;&#20043;&#38388;&#30340;&#23545;&#40784;&#20195;&#20215;&#26159;&#36825;&#20123;&#28857;&#20540;&#20043;&#38388;&#24046;&#24322;&#30340;&#20989;&#25968;&#12290;&#21407;&#22987;&#20195;&#20215;&#20989;&#25968;&#26159;&#36825;&#31181;&#24046;&#24322;&#30340;&#32477;&#23545;&#20540;&#12290;&#25552;&#20986;&#20102;&#20854;&#20182;&#20195;&#20215;&#20989;&#25968;&#12290;&#19968;&#31181;&#27969;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#26159;&#24046;&#20540;&#30340;&#24179;&#26041;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23545;&#20351;&#29992;&#19981;&#21516;&#20195;&#20215;&#20989;&#25968;&#30340;&#30456;&#23545;&#24433;&#21709;&#21644;&#35843;&#25972;&#20195;&#20215;&#20989;&#25968;&#20197;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#28508;&#21147;&#36827;&#34892;&#30340;&#31532;&#19968;&#27425;&#30740;&#31350;&#12290;&#26412;&#25991;&#20351;&#29992;&#21487;&#35843;&#25972;&#30340;&#20195;&#20215;&#20989;&#25968;{\lambda}{\gamma}&#65292;&#20854;&#20013;{\gamma}&#26159;&#21442;&#25968;&#65292;&#26469;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;{\gamma}&#20540;&#36234;&#39640;&#65292;&#23545;&#36739;&#22823;&#30340;&#25104;&#23545;&#24046;&#24322;&#36171;&#20104;&#26356;&#22823;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;{\gamma}&#20540;&#36234;&#20302;&#65292;&#21017;&#26356;&#21152;&#24378;&#35843;&#36739;&#23567;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#35843;&#25972;&#30340;&#20195;&#20215;&#20989;&#25968;&#22312;&#20960;&#20010;&#22522;&#20934;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#25552;&#39640;&#20102;&#20351;&#29992;DTW&#20316;&#20026;&#36317;&#31163;&#24230;&#37327;&#30340;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Time Warping (DTW) is a popular time series distance measure that aligns the points in two series with one another. These alignments support warping of the time dimension to allow for processes that unfold at differing rates. The distance is the minimum sum of costs of the resulting alignments over any allowable warping of the time dimension. The cost of an alignment of two points is a function of the difference in the values of those points. The original cost function was the absolute value of this difference. Other cost functions have been proposed. A popular alternative is the square of the difference. However, to our knowledge, this is the first investigation of both the relative impacts of using different cost functions and the potential to tune cost functions to different tasks. We do so in this paper by using a tunable cost function {\lambda}{\gamma} with parameter {\gamma}. We show that higher values of {\gamma} place greater weight on larger pairwise differences, while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#32447;&#26680;&#23398;&#20064;&#20013;&#30340;&#26032;&#31639;&#27861;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#20110;&#20197;&#21069;&#30340;&#32467;&#26524;&#12290;&#35813;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#21462;&#20915;&#20110;&#26680;&#30697;&#38453;&#29305;&#24449;&#20540;&#30340;&#34928;&#20943;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.12989</link><description>&lt;p&gt;
&#22312;&#32447;&#26680;&#23398;&#20064;&#20013;&#30340;&#25913;&#36827;&#26680;&#23545;&#40784;&#36951;&#25022;&#30028;
&lt;/p&gt;
&lt;p&gt;
Improved Kernel Alignment Regret Bound for Online Kernel Learning. (arXiv:2212.12989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#32447;&#26680;&#23398;&#20064;&#20013;&#30340;&#26032;&#31639;&#27861;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#20110;&#20197;&#21069;&#30340;&#32467;&#26524;&#12290;&#35813;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#21462;&#20915;&#20110;&#26680;&#30697;&#38453;&#29305;&#24449;&#20540;&#30340;&#34928;&#20943;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Hinge&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#22312;&#32447;&#26680;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#26680;&#23545;&#40784;&#36951;&#25022;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#20110;&#20197;&#21069;&#30340;&#32467;&#26524;&#12290;&#22914;&#26524;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#21576;&#25351;&#25968;&#34928;&#20943;&#65292;&#21017;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;$O(\ln^2{T})$&#65292;&#36951;&#25022;&#30028;&#20026;$O(\sqrt{\mathcal{A}_T})$&#12290;&#21542;&#21017;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;$O(\sqrt{\mathcal{A}_TT})$&#65292;&#36951;&#25022;&#30028;&#20026;$O((\mathcal{A}_TT)^{\frac{1}{4}})$&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;&#25209;&#37327;&#23398;&#20064;&#65292;&#24182;&#33719;&#24471;&#20102;$O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$&#30340;&#20313;&#37327;&#39118;&#38505;&#30028;&#65292;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we improve the kernel alignment regret bound for online kernel learning in the regime of the Hinge loss function. Previous algorithm achieves a regret of $O((\mathcal{A}_TT\ln{T})^{\frac{1}{4}})$ at a computational complexity (space and per-round time) of $O(\sqrt{\mathcal{A}_TT\ln{T}})$, where $\mathcal{A}_T$ is called \textit{kernel alignment}. We propose an algorithm whose regret bound and computational complexity are better than previous results. Our results depend on the decay rate of eigenvalues of the kernel matrix. If the eigenvalues of the kernel matrix decay exponentially, then our algorithm enjoys a regret of $O(\sqrt{\mathcal{A}_T})$ at a computational complexity of $O(\ln^2{T})$. Otherwise, our algorithm enjoys a regret of $O((\mathcal{A}_TT)^{\frac{1}{4}})$ at a computational complexity of $O(\sqrt{\mathcal{A}_TT})$. We extend our algorithm to batch learning and obtain a $O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$ excess risk bound which improves the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;HAC-Net&#65292;&#32467;&#21512;&#20102;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20004;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#34507;&#30333;&#36136;&#19982;&#37197;&#20307;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#24182;&#22312;&#20844;&#35748;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.12440</link><description>&lt;p&gt;
HAC-Net:&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28151;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#31934;&#24230;&#30340;&#34507;&#30333;&#36136;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
HAC-Net: A Hybrid Attention-Based Convolutional Neural Network for Highly Accurate Protein-Ligand Binding Affinity Prediction. (arXiv:2212.12440v3 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;HAC-Net&#65292;&#32467;&#21512;&#20102;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20004;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#34507;&#30333;&#36136;&#19982;&#37197;&#20307;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#24182;&#22312;&#20844;&#35748;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#22270;&#20687;&#26816;&#27979;&#21644;&#22270;&#35770;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#27010;&#24565;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#39044;&#27979;&#65292;&#36825;&#26159;&#33647;&#29289;&#21457;&#29616;&#21644;&#34507;&#30333;&#24037;&#31243;&#39046;&#22495;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;HAC-Net&#65292;&#21253;&#25324;&#19968;&#20010;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20004;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#32858;&#21512;&#33410;&#28857;&#29305;&#24449;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#12290;&#22312;&#34507;&#30333;&#36136;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#39046;&#22495;&#20844;&#35748;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;PDBbind v.2016 core set&#19978;&#65292;HAC-Net&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22810;&#20010;&#35757;&#32451;-&#27979;&#35797;&#21010;&#20998;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#27599;&#20010;&#21010;&#20998;&#37117;&#26368;&#22823;&#21270;&#20102;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#22797;&#21512;&#29289;&#30340;&#34507;&#30333;&#32467;&#26500;&#12289;&#34507;&#30333;&#24207;&#21015;&#25110;&#37197;&#20307;&#25193;&#23637;&#36830;&#25509;&#25351;&#32441;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#20284;&#24615;&#27979;&#24230;&#36827;&#34892;&#20102;&#21313;&#20493;&#20132;&#21449;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying deep learning concepts from image detection and graph theory has greatly advanced protein-ligand binding affinity prediction, a challenge with enormous ramifications for both drug discovery and protein engineering. We build upon these advances by designing a novel deep learning architecture consisting of a 3-dimensional convolutional neural network utilizing channel-wise attention and two graph convolutional networks utilizing attention-based aggregation of node features. HAC-Net (Hybrid Attention-Based Convolutional Neural Network) obtains state-of-the-art results on the PDBbind v.2016 core set, the most widely recognized benchmark in the field. We extensively assess the generalizability of our model using multiple train-test splits, each of which maximizes differences between either protein structures, protein sequences, or ligand extended-connectivity fingerprints of complexes in the training and test sets. Furthermore, we perform 10-fold cross-validation with a similarity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#21521;&#37327;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#26032;&#33539;&#24335;&#65292;&#20219;&#21153;&#21521;&#37327;&#21487;&#36890;&#36807;&#31639;&#26415;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#21644;&#32452;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#24615;&#33021;&#19988;&#23545;&#25511;&#21046;&#20219;&#21153;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2212.04089</link><description>&lt;p&gt;
&#20351;&#29992;&#20219;&#21153;&#31639;&#26415;&#32534;&#36753;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Editing Models with Task Arithmetic. (arXiv:2212.04089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#21521;&#37327;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#26032;&#33539;&#24335;&#65292;&#20219;&#21153;&#21521;&#37327;&#21487;&#36890;&#36807;&#31639;&#26415;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#21644;&#32452;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#24615;&#33021;&#19988;&#23545;&#25511;&#21046;&#20219;&#21153;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#21464;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#26041;&#24335;&#65288;&#27604;&#22914;&#25552;&#39640;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#25110;&#20943;&#36731;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#20559;&#24046;&#65289;&#26159;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26102;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22260;&#32469;&#8220;&#20219;&#21153;&#21521;&#37327;&#8221;&#26469;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26032;&#33539;&#24335;&#12290;&#20219;&#21153;&#21521;&#37327;&#25351;&#23450;&#20102;&#19968;&#20010;&#26041;&#21521;&#65292;&#21363;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65292;&#27839;&#30528;&#35813;&#26041;&#21521;&#31227;&#21160;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#32463;&#36807;&#24494;&#35843;&#20219;&#21153;&#21518;&#30340;&#30456;&#21516;&#27169;&#22411;&#30340;&#26435;&#37325;&#20013;&#20943;&#21435;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#26500;&#24314;&#20219;&#21153;&#21521;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#21521;&#37327;&#21487;&#20197;&#36890;&#36807;&#21542;&#23450;&#21644;&#21152;&#27861;&#31561;&#31639;&#26415;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#21542;&#23450;&#20219;&#21153;&#21521;&#37327;&#20250;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#23545;&#25511;&#21046;&#20219;&#21153;&#30340;&#27169;&#22411;&#34892;&#20026;&#24433;&#21709;&#19981;&#22823;&#12290;&#27492;&#22806;&#65292;&#23558;&#20219;&#21153;&#21521;&#37327;&#30456;&#21152;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#25511;&#21046;&#20219;&#21153;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#39640;&#33410;&#28857;&#34920;&#31034;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#36127;&#26679;&#26412;&#37319;&#26679;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.14394</link><description>&lt;p&gt;
&#38750;&#23545;&#27604;&#23398;&#20064;&#30340;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Link Prediction with Non-Contrastive Learning. (arXiv:2211.14394v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#39640;&#33410;&#28857;&#34920;&#31034;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#36127;&#26679;&#26412;&#37319;&#26679;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#19968;&#20010;&#26368;&#26032;&#28909;&#28857;&#26159;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#65292;&#26088;&#22312;&#25512;&#23548;&#20986;&#27809;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#26377;&#29992;&#33410;&#28857;&#34920;&#31034;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22270;&#24418;SSL&#26041;&#27861;&#37117;&#26159;&#23545;&#27604;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#36127;&#26679;&#26412;&#37319;&#26679;&#30340;&#25361;&#25112;&#65288;&#36895;&#24230;&#32531;&#24930;&#21644;&#27169;&#22411;&#25935;&#24863;&#24615;&#65289;&#65292;&#26368;&#36817;&#30340;&#25991;&#29486;&#24341;&#20837;&#20102;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#20351;&#29992;&#27491;&#26679;&#26412;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#38750;&#23545;&#27604;&#26041;&#27861;&#22312;&#38142;&#36335;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20256;&#36882;&#24615;&#21644;&#24402;&#32435;&#24615;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38750;&#23545;&#27604;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SSL&#26041;&#27861;&#26469;&#22312;&#38142;&#36335;&#39044;&#27979;&#20013;&#25552;&#39640;&#33410;&#28857;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent focal area in the space of graph neural networks (GNNs) is graph self-supervised learning (SSL), which aims to derive useful node representations without labeled data. Notably, many state-of-the-art graph SSL methods are contrastive methods, which use a combination of positive and negative samples to learn node representations. Owing to challenges in negative sampling (slowness and model sensitivity), recent literature introduced non-contrastive methods, which instead only use positive samples. Though such methods have shown promising performance in node-level tasks, their suitability for link prediction tasks, which are concerned with predicting link existence between pairs of nodes (and have broad applicability to recommendation systems contexts) is yet unexplored. In this work, we extensively evaluate the performance of existing non-contrastive methods for link prediction in both transductive and inductive settings. While most existing non-contrastive methods perform poorly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24179;&#34913;&#29702;&#35770;&#30340;&#21152;&#23494;&#36135;&#24065;&#20449;&#20219;&#32593;&#32476;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#27169;&#24335;&#30697;&#38453;&#25429;&#25417;&#23616;&#37096;&#25299;&#25169;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.13123</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#23494;&#30721;&#36135;&#24065;&#20449;&#20219;&#32593;&#32476;&#20013;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Motif-aware temporal GCN for fraud detection in signed cryptocurrency trust networks. (arXiv:2211.13123v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24179;&#34913;&#29702;&#35770;&#30340;&#21152;&#23494;&#36135;&#24065;&#20449;&#20219;&#32593;&#32476;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#27169;&#24335;&#30697;&#38453;&#25429;&#25417;&#23616;&#37096;&#25299;&#25169;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#26159;&#19968;&#31181;&#22788;&#29702;&#21487;&#34920;&#31034;&#20026;&#22270;&#30340;&#25968;&#25454;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#30001;&#20110;&#37329;&#34701;&#20132;&#26131;&#21487;&#20197;&#33258;&#28982;&#22320;&#26500;&#36896;&#25104;&#22270;&#24418;&#65292;&#22240;&#27492;GCN&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#26041;&#38754;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#21152;&#23494;&#36135;&#24065;&#20449;&#20219;&#32593;&#32476;&#20013;&#30340;&#27450;&#35784;&#26816;&#27979;&#12290;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#38745;&#24577;&#22270;&#19978;&#12290;&#32780;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21152;&#23494;&#36135;&#24065;&#32593;&#32476;&#30340;&#28436;&#21270;&#29305;&#24615;&#65292;&#24182;&#21033;&#29992;&#26412;&#22320;&#32467;&#26500;&#21644;&#24179;&#34913;&#29702;&#35770;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35745;&#31639;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#27169;&#24335;&#30697;&#38453;&#26469;&#25429;&#25417;&#23616;&#37096;&#25299;&#25169;&#20449;&#24687;&#65292;&#28982;&#21518;&#22312;GCN&#32858;&#21512;&#36807;&#31243;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#27599;&#20010;&#24555;&#29031;&#29983;&#25104;&#30340;&#23884;&#20837;&#26159;&#26102;&#38388;&#31383;&#21475;&#20869;&#23884;&#20837;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#30001;&#20110;&#20449;&#20219;&#32593;&#32476;&#22312;&#27599;&#20010;&#36793;&#32536;&#19978;&#37117;&#26377;&#31614;&#21517;&#65292;&#22240;&#27492;&#20351;&#29992;&#24179;&#34913;&#29702;&#35770;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#27169;&#24335;&#24863;&#30693;&#30340;&#26102;&#24577;GCN&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) is a class of artificial neural networks for processing data that can be represented as graphs. Since financial transactions can naturally be constructed as graphs, GCNs are widely applied in the financial industry, especially for financial fraud detection. In this paper, we focus on fraud detection on cryptocurrency truct networks. In the literature, most works focus on static networks. Whereas in this study, we consider the evolving nature of cryptocurrency networks, and use local structural as well as the balance theory to guide the training process. More specifically, we compute motif matrices to capture the local topological information, then use them in the GCN aggregation process. The generated embedding at each snapshot is a weighted average of embeddings within a time window, where the weights are learnable parameters. Since the trust networks is signed on each edge, balance theory is used to guide the training process. Experimental results 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21333;&#27425;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#65292;&#24182;&#32473;&#20986;&#20102;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.10890</link><description>&lt;p&gt;
&#21333;&#27425;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36866;&#29992;&#20110;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph. (arXiv:2211.10890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21333;&#27425;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#65292;&#24182;&#32473;&#20986;&#20102;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#25165;&#33021;&#26500;&#24314;&#20135;&#29983;&#23545;&#27604;&#30340;&#25439;&#22833;&#65292;&#36825;&#23545;&#20110;&#25429;&#25417;&#33410;&#28857;&#29305;&#24449;&#30340;&#20302;&#39057;&#20449;&#21495;&#26159;&#26377;&#25928;&#30340;&#12290;&#36825;&#31181;&#21452;&#36890;&#36947;&#35774;&#35745;&#24050;&#32463;&#22312;&#21516;&#26500;&#22270;&#19978;&#34920;&#29616;&#20986;&#23454;&#35777;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#22312;&#24322;&#26500;&#22270;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#30452;&#25509;&#36830;&#25509;&#30340;&#33410;&#28857;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#65292;&#23578;&#19981;&#28165;&#26970;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#21463;&#21040;GCL&#26041;&#27861;&#22312;&#24322;&#26500;&#22270;&#19978;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#30340;&#24433;&#21709;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20110;&#26159;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#23601;&#20986;&#29616;&#20102;&#65306;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#30340;GCL&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22312;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#19978;&#36890;&#36807;&#37051;&#22495;&#32858;&#21512;&#24471;&#21040;&#30340;&#29305;&#24449;&#30340;&#38598;&#20013;&#24615;&#36136;&#65292;&#24341;&#20837;&#20102;&#21333;&#36890;&#36947;&#22270;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing graph contrastive learning (GCL) techniques typically require two forward passes for a single instance to construct the contrastive loss, which is effective for capturing the low-frequency signals of node features. Such a dual-pass design has shown empirical success on homophilic graphs, but its effectiveness on heterophilic graphs, where directly connected nodes typically have different labels, is unknown. In addition, existing GCL approaches fail to provide strong performance guarantees. Coupled with the unpredictability of GCL approaches on heterophilic graphs, their applicability in real-world contexts is limited. Then, a natural question arises: Can we design a GCL method that works for both homophilic and heterophilic graphs with a performance guarantee? To answer this question, we theoretically study the concentration property of features obtained by neighborhood aggregation on homophilic and heterophilic graphs, introduce the single-pass graph contrastive learning loss
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#8212;&#8212;&#21367;&#31215;&#39640;&#26031;&#31070;&#32463;&#36807;&#31243;&#65288;ConvGNP&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#25918;&#32622;&#25928;&#29575;&#12290;ConvGNP&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#32852;&#21512;&#39640;&#26031;&#20998;&#24067;&#65292;&#36890;&#36807;&#23398;&#20064;&#31354;&#38388;&#21644;&#23395;&#33410;&#24615;&#38750;&#24179;&#31283;&#24615;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38750;&#24179;&#31283;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.10381</link><description>&lt;p&gt;
&#24102;&#26377;&#21367;&#31215;&#39640;&#26031;&#31070;&#32463;&#36807;&#31243;&#30340;&#29615;&#22659;&#20256;&#24863;&#22120;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Environmental Sensor Placement with Convolutional Gaussian Neural Processes. (arXiv:2211.10381v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#8212;&#8212;&#21367;&#31215;&#39640;&#26031;&#31070;&#32463;&#36807;&#31243;&#65288;ConvGNP&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#25918;&#32622;&#25928;&#29575;&#12290;ConvGNP&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#32852;&#21512;&#39640;&#26031;&#20998;&#24067;&#65292;&#36890;&#36807;&#23398;&#20064;&#31354;&#38388;&#21644;&#23395;&#33410;&#24615;&#38750;&#24179;&#31283;&#24615;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38750;&#24179;&#31283;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#20256;&#24863;&#22120;&#23545;&#20110;&#30417;&#27979;&#22825;&#27668;&#21644;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20687;&#21335;&#26497;&#36825;&#26679;&#30340;&#20559;&#36828;&#22320;&#21306;&#65292;&#26368;&#22823;&#21270;&#27979;&#37327;&#20449;&#24687;&#21644;&#26377;&#25928;&#25918;&#32622;&#20256;&#24863;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#26032;&#20256;&#24863;&#22120;&#25552;&#20379;&#30340;&#19981;&#30830;&#23450;&#24615;&#20943;&#23569;&#26469;&#35780;&#20272;&#25918;&#32622;&#20449;&#24687;&#12290;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#20294;&#38590;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#38750;&#24179;&#31283;&#34892;&#20026;&#24182;&#32553;&#25918;&#21040;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21367;&#31215;&#39640;&#26031;&#31070;&#32463;&#36807;&#31243;&#65288;ConvGNP&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;ConvGNP&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#20219;&#24847;&#30446;&#26631;&#20301;&#32622;&#30340;&#32852;&#21512;&#39640;&#26031;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20351;&#29992;&#27169;&#25311;&#30340;&#21335;&#26497;&#22320;&#21306;&#22320;&#38754;&#28201;&#24230;&#24322;&#24120;&#20316;&#20026;&#30495;&#23454;&#25968;&#25454;&#65292;ConvGNP&#23398;&#20064;&#20102;&#31354;&#38388;&#21644;&#23395;&#33410;&#24615;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#20248;&#20110;&#38750;&#24179;&#31283;GP&#22522;&#32447;&#12290;&#22312;&#27169;&#25311;&#30340;s&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Environmental sensors are crucial for monitoring weather conditions and the impacts of climate change. However, it is challenging to maximise measurement informativeness and place sensors efficiently, particularly in remote regions like Antarctica. Probabilistic machine learning models can evaluate placement informativeness by predicting the uncertainty reduction provided by a new sensor. Gaussian process (GP) models are widely used for this purpose, but they struggle with capturing complex non-stationary behaviour and scaling to large datasets. This paper proposes using a convolutional Gaussian neural process (ConvGNP) to address these issues. A ConvGNP uses neural networks to parameterise a joint Gaussian distribution at arbitrary target locations, enabling flexibility and scalability. Using simulated surface air temperature anomaly over Antarctica as ground truth, the ConvGNP learns spatial and seasonal non-stationarities, outperforming a non-stationary GP baseline. In a simulated s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;DRL&#26041;&#27861;&#65292;&#32852;&#21512;&#20248;&#21270;&#20102;MU-MISO&#31995;&#32479;&#20013;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;RIS&#30456;&#20301;&#31227;&#20301;&#65292;&#20197;&#26368;&#22823;&#21270;&#24635;&#19979;&#34892;&#36895;&#29575;&#12290;&#22312;&#35299;&#20915;&#20102;&#19981;&#23436;&#32654;&#30340;CSI&#21644;&#30828;&#20214;&#22833;&#35843;&#30340;&#25361;&#25112;&#21518;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22522;&#30784;DRL&#20195;&#29702;&#65292;&#24182;&#19988;&#26159;&#31532;&#19968;&#31687;&#24212;&#29992;DRL&#20248;&#21270;&#27874;&#26463;&#25104;&#24418;&#21644;RIS&#37197;&#32622;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2211.09702</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;RIS&#36741;&#21161;MU-MISO&#31995;&#32479;&#20013;&#19979;&#34892;&#27874;&#26463;&#25104;&#24418;&#21644;RIS&#37197;&#32622;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Based Joint Downlink Beamforming and RIS Configuration in RIS-aided MU-MISO Systems Under Hardware Impairments and Imperfect CSI. (arXiv:2211.09702v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;DRL&#26041;&#27861;&#65292;&#32852;&#21512;&#20248;&#21270;&#20102;MU-MISO&#31995;&#32479;&#20013;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;RIS&#30456;&#20301;&#31227;&#20301;&#65292;&#20197;&#26368;&#22823;&#21270;&#24635;&#19979;&#34892;&#36895;&#29575;&#12290;&#22312;&#35299;&#20915;&#20102;&#19981;&#23436;&#32654;&#30340;CSI&#21644;&#30828;&#20214;&#22833;&#35843;&#30340;&#25361;&#25112;&#21518;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22522;&#30784;DRL&#20195;&#29702;&#65292;&#24182;&#19988;&#26159;&#31532;&#19968;&#31687;&#24212;&#29992;DRL&#20248;&#21270;&#27874;&#26463;&#25104;&#24418;&#21644;RIS&#37197;&#32622;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#30456;&#20301;&#20381;&#36182;&#21453;&#23556;&#25391;&#24133;&#27169;&#22411;&#19979;&#65292;&#32852;&#21512;&#20248;&#21270;&#22810;&#29992;&#25143;&#22810;&#36755;&#20837;&#21333;&#36755;&#20986;(MU-MISO)&#31995;&#32479;&#20013;&#30340;&#21457;&#23556;&#27874;&#26463;&#25104;&#24418;&#21644;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;(RIS)&#30456;&#20301;&#31227;&#20301;&#65292;&#20197;&#26368;&#22823;&#21270;&#24635;&#19979;&#34892;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#23454;&#38469;&#30340;RIS&#24133;&#24230;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19981;&#23436;&#32654;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;(CSI)&#21644;&#30828;&#20214;&#22833;&#35843;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#19968;&#20010;&#22522;&#30784;&#30340;DRL&#20195;&#29702;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#23436;&#32654;&#30340;CSI&#21644;&#30456;&#20301;&#20381;&#36182;&#30340;RIS&#24133;&#24230;&#65292;&#20197;&#21450;&#19981;&#21305;&#37197;&#30340;CSI&#21644;&#29702;&#24819;&#30340;RIS&#21453;&#23556;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26174;&#33879;&#20248;&#20110;&#22522;&#30784;&#30340;DRL&#20195;&#29702;&#65292;&#24182;&#25509;&#36817;&#40644;&#37329;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#20462;&#25913;DRL&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21457;&#23556;&#27874;&#26463;&#25104;&#24418;&#21644;&#30456;&#20301;&#31227;&#20301;&#30340;&#32852;&#21512;&#35774;&#35745;&#20197;&#21450;&#30456;&#20301;&#20381;&#36182;&#24133;&#24230;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#23558;DRL&#24212;&#29992;&#20110;&#22788;&#29702;&#30828;&#20214;&#22833;&#35843;&#21644;&#19981;&#23436;&#32654;CSI&#30340;MU-MISO&#31995;&#32479;&#20013;&#65292;&#32852;&#21512;&#20248;&#21270;&#27874;&#26463;&#25104;&#24418;&#21644;RIS&#37197;&#32622;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel deep reinforcement learning (DRL) approach to jointly optimize transmit beamforming and reconfigurable intelligent surface (RIS) phase shifts in a multiuser multiple input single output (MU-MISO) system to maximize the sum downlink rate under the phase-dependent reflection amplitude model. Our approach addresses the challenge of imperfect channel state information (CSI) and hardware impairments by considering a practical RIS amplitude model. We compare the performance of our approach against a vanilla DRL agent in two scenarios: perfect CSI and phase-dependent RIS amplitudes, and mismatched CSI and ideal RIS reflections. The results demonstrate that the proposed framework significantly outperforms the vanilla DRL agent under mismatch and approaches the golden standard. Our contributions include modifications to the DRL approach to address the joint design of transmit beamforming and phase shifts and the phase-dependent amplitude model. To the best of our knowledge,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#35821;&#27861;&#30340;&#34920;&#36848;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#22810;&#39033;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013; emergent &#35821;&#35328;&#32467;&#26500;&#26159;&#33030;&#24369;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.17406</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; emergent &#35821;&#35328;&#32467;&#26500;&#26159;&#33030;&#24369;&#30340;
&lt;/p&gt;
&lt;p&gt;
Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#35821;&#27861;&#30340;&#34920;&#36848;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#22810;&#39033;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013; emergent &#35821;&#35328;&#32467;&#26500;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#24378;&#21170;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#24230;&#31561;&#24615;&#33021;&#25351;&#26631;&#24182;&#19981;&#33021;&#34913;&#37327;&#27169;&#22411;&#22312;&#20195;&#34920;&#22797;&#26434;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#20195;&#34920;&#35821;&#27861;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#34920;&#36848;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20123;&#31283;&#20581;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24230;&#37327;&#26041;&#24335;&#65292;&#36825;&#20123;&#24230;&#37327;&#26041;&#24335;&#21033;&#29992;&#26368;&#36817;&#22312;&#36890;&#36807;&#25506;&#27979;&#20219;&#21153;&#20174;LLM&#20013;&#25552;&#21462;&#35821;&#35328;&#32467;&#26500;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#21363;&#29992;&#20110;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#31616;&#21333;&#20219;&#21153;&#65292;&#22914;&#35821;&#27861;&#37325;&#26500;&#21644;&#26681;&#35782;&#21035;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#22235;&#31181;LLM&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#35821;&#26009;&#24211;&#19978;&#23545;&#35821;&#27861;&#20445;&#25345;&#25200;&#21160;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#26469;&#30740;&#31350;&#25152;&#25552;&#20986;&#30340;&#31283;&#20581;&#24230;&#37327;&#26041;&#24335;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been reported to have strong performance on natural language processing tasks. However, performance metrics such as accuracy do not measure the quality of the model in terms of its ability to robustly represent complex linguistic structure. In this paper, focusing on the ability of language models to represent syntax, we propose a framework to assess the consistency and robustness of linguistic representations. To this end, we introduce measures of robustness of neural network models that leverage recent advances in extracting linguistic constructs from LLMs via probing tasks, i.e., simple tasks used to extract meaningful information about a single facet of a language model, such as syntax reconstruction and root identification. Empirically, we study the performance of four LLMs across six different corpora on the proposed robustness measures by analysing their performance and robustness with respect to syntax-preserving perturbations. We provide evide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20855;&#26377;&#19968;&#33324;&#24418;&#24335;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#28201;&#21551;&#21160;&#25216;&#26415;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36817;&#20284;&#22320;&#35299;&#20915;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#25910;&#25947;&#36895;&#24230;&#27604;&#20854;&#31934;&#30830;&#23545;&#24212;&#29289;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2210.15659</link><description>&lt;p&gt;
&#35299;&#20915;&#20855;&#26377;&#19968;&#33324;&#24418;&#24335;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Primal-dual Approach for Solving Variational Inequalities with General-form Constraints. (arXiv:2210.15659v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20855;&#26377;&#19968;&#33324;&#24418;&#24335;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#28201;&#21551;&#21160;&#25216;&#26415;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36817;&#20284;&#22320;&#35299;&#20915;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#25910;&#25947;&#36895;&#24230;&#27604;&#20854;&#31934;&#30830;&#23545;&#24212;&#29289;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Yang&#31561;&#20154;&#26368;&#36817;&#36890;&#36807;&#19968;&#31181;&#19968;&#38454;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#20102;&#20855;&#26377;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#65288;VI&#65289;&#30340;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#25552;&#20986;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#31216;&#20026;ACVI&#20165;&#36866;&#29992;&#20110;&#21487;&#20197;&#35745;&#31639;&#20854;&#23376;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#24773;&#20917;&#65292;&#22240;&#27492;&#19968;&#33324;&#24773;&#20917;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#28201;&#21551;&#21160;&#25216;&#26415;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36817;&#20284;&#22320;&#35299;&#20915;&#23376;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22312;&#20808;&#21069;&#36845;&#20195;&#20013;&#25214;&#21040;&#30340;&#36817;&#20284;&#35299;&#21021;&#22987;&#21270;&#21464;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#24403;&#31639;&#23376;&#20026;$L$-Lipschitz&#19988;&#21333;&#35843;&#26102;&#65292;&#36825;&#31181;&#19981;&#31934;&#30830;&#30340;ACVI&#26041;&#27861;&#30340;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#30340;&#38388;&#38553;&#20989;&#25968;&#19979;&#38477;&#30340;&#36895;&#24230;&#20026;$\mathcal{O}(\frac{1}{\sqrt{K}})$&#65292;&#21069;&#25552;&#26159;&#38169;&#35823;&#20197;&#36866;&#24403;&#30340;&#36895;&#24230;&#19979;&#38477;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#36890;&#24120;&#36825;&#31181;&#25216;&#26415;&#27604;&#20854;&#31934;&#30830;&#23545;&#24212;&#29289;&#25910;&#25947;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Yang et al. (2023) recently addressed the open problem of solving Variational Inequalities (VIs) with equality and inequality constraints through a first-order gradient method. However, the proposed primal-dual method called ACVI is applicable when we can compute analytic solutions of its subproblems; thus, the general case remains an open problem. In this paper, we adopt a warm-starting technique where we solve the subproblems approximately at each iteration and initialize the variables with the approximate solution found at the previous iteration. We prove its convergence and show that the gap function of the last iterate of this inexact-ACVI method decreases at a rate of $\mathcal{O}(\frac{1}{\sqrt{K}})$ when the operator is $L$-Lipschitz and monotone, provided that the errors decrease at appropriate rates. Interestingly, we show that often in numerical experiments, this technique converges faster than its exact counterpart. Furthermore, for the cases when the inequality constraints
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#22312;&#25277;&#35937;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#36866;&#23452;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20248;&#38597;&#22320;&#35299;&#20915;RPM&#38382;&#39064;&#65292;&#24182;&#25581;&#31034;&#20102;&#22810;&#35270;&#35282;&#21644;&#22810;&#35780;&#20272;&#26159;&#25104;&#21151;&#25512;&#29702;&#30340;&#20851;&#38190;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.14914</link><description>&lt;p&gt;
&#24102;&#26377;&#36866;&#23452;&#30340;&#24402;&#32435;&#20559;&#24046;&#22686;&#24378;&#26426;&#22120;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#35270;&#35282;&#21644;&#22810;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias Boost Machine Abstract Reasoning Ability. (arXiv:2210.14914v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#22312;&#25277;&#35937;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#36866;&#23452;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20248;&#38597;&#22320;&#35299;&#20915;RPM&#38382;&#39064;&#65292;&#24182;&#25581;&#31034;&#20102;&#22810;&#35270;&#35282;&#21644;&#22810;&#35780;&#20272;&#26159;&#25104;&#21151;&#25512;&#29702;&#30340;&#20851;&#38190;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#33268;&#21147;&#20110;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#22312;&#25277;&#35937;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;RAVEN&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#30340;&#19981;&#21516;&#29256;&#26412;&#34987;&#25552;&#20986;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22914;&#26524;&#27809;&#26377;&#22797;&#26434;&#30340;&#35774;&#35745;&#25110;&#21253;&#21547;&#35821;&#20041;&#20449;&#24687;&#30340;&#39069;&#22806;&#20803;&#25968;&#25454;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#21518;&#22312;RPM&#38382;&#39064;&#30340;&#20915;&#31574;&#26041;&#38754;&#20173;&#21487;&#33021;&#29369;&#35947;&#19981;&#20915;&#12290;&#36890;&#36807;&#24443;&#24213;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#36866;&#23452;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20248;&#38597;&#22320;&#35299;&#20915;RPM&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#22686;&#21152;&#20219;&#20309;&#39069;&#22806;&#30340;&#20803;&#25968;&#25454;&#25110;&#29305;&#23450;&#39592;&#24178;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#25581;&#31034;&#20102;&#22810;&#35270;&#35282;&#21644;&#22810;&#35780;&#20272;&#26159;&#25104;&#21151;&#25512;&#29702;&#30340;&#20851;&#38190;&#23398;&#20064;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36830;&#25509;&#27169;&#22411;&#22312;&#27867;&#21270;&#26041;&#38754;&#22833;&#36133;&#30340;&#28508;&#22312;&#35299;&#37322;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#32467;&#26524;&#23558;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#26816;&#26597;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Great endeavors have been made to study AI's ability in abstract reasoning, along with which different versions of RAVEN's progressive matrices (RPM) are proposed as benchmarks. Previous works give inkling that without sophisticated design or extra meta-data containing semantic information, neural networks may still be indecisive in making decisions regarding RPM problems, after relentless training. Evidenced by thorough experiments and ablation studies, we showcase that end-to-end neural networks embodied with felicitous inductive bias, intentionally design or serendipitously match, can solve RPM problems elegantly, without the augment of any extra meta-data or preferences of any specific backbone. Our work also reveals that multi-viewpoint with multi-evaluation is a key learning strategy for successful reasoning. Finally, potential explanations for the failure of connectionist models in generalization are provided. We hope that these results will serve as inspections of AI's ability 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#37327;&#25552;&#31034;&#33021;&#25945;&#25480;&#27169;&#22411;&#26032;&#35821;&#35328;&#20197;&#21450;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#19979;&#25317;&#26377;&#38646;-shot&#32763;&#35793;&#33021;&#21147;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.14868</link><description>&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#37327;&#25552;&#31034;&#33021;&#25945;&#25480;&#27169;&#22411;&#26032;&#35821;&#35328;&#20197;&#21450;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#19979;&#25317;&#26377;&#38646;-shot&#32763;&#35793;&#33021;&#21147;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65306;MBXP&#21644;Multilingual HumanEval&#65292;&#20197;&#21450;MathQA-X&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;10&#31181;&#20197;&#19978;&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#36716;&#25442;&#26694;&#26550;&#23558;&#21407;&#22987;Python&#25968;&#25454;&#38598;&#20013;&#30340;&#25552;&#31034;&#21644;&#27979;&#35797;&#29992;&#20363;&#36716;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#20013;&#30340;&#30456;&#24212;&#25968;&#25454;&#12290;&#21033;&#29992;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#21333;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#20248;&#21183;&#12289;&#23569;&#37327;&#25552;&#31034;&#25945;&#25480;&#27169;&#22411;&#26032;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;&#38646;-shot&#32763;&#35793;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#24341;&#23548;&#65292;&#20197;&#33719;&#21462;&#22810;&#31181;&#35821;&#35328;&#30340;&#21512;&#25104;&#35268;&#33539;&#35299;&#65292;&#36825;&#20123;&#35299;&#21487;&#29992;&#20110;&#20854;&#20182;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#35780;&#20272;&#65292;&#22914;&#20195;&#30721;&#25554;&#20837;&#12289;&#40065;&#26834;&#24615;&#25110;&#25688;&#35201;&#20219;&#21153;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;
&lt;/p&gt;
&lt;p&gt;
We present new benchmarks on evaluation code generation models: MBXP and Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming languages and are generated using a scalable conversion framework that transpiles prompts and test cases from the original Python datasets into the corresponding data in the target language. Using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities even on mono-lingual settings. Furthermore, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks. Overall, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#39640;&#32500;&#29305;&#24449;&#19979;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24191;&#20041;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#20272;&#35745;&#26368;&#20339;&#20998;&#31163;&#36229;&#24179;&#38754;&#30340;&#26041;&#21521;&#65292;&#24778;&#20154;&#22320;&#21457;&#29616;&#36229;&#24179;&#38754;&#30340;&#25554;&#20540;&#21462;&#20915;&#20110;&#26631;&#31614;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2210.14347</link><description>&lt;p&gt;
&#22312;&#39640;&#32500;&#39640;&#26031;&#28508;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#25554;&#20540;&#21028;&#21035;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Interpolating Discriminant Functions in High-Dimensional Gaussian Latent Mixtures. (arXiv:2210.14347v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#39640;&#32500;&#29305;&#24449;&#19979;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24191;&#20041;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#20272;&#35745;&#26368;&#20339;&#20998;&#31163;&#36229;&#24179;&#38754;&#30340;&#26041;&#21521;&#65292;&#24778;&#20154;&#22320;&#21457;&#29616;&#36229;&#24179;&#38754;&#30340;&#25554;&#20540;&#21462;&#20915;&#20110;&#26631;&#31614;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#20302;&#32500;&#28508;&#22312;&#39640;&#26031;&#28151;&#21512;&#32467;&#26500;&#21644;&#38750;&#38646;&#22122;&#22768;&#30340;&#20551;&#35774;&#27169;&#22411;&#19979;&#38024;&#23545;&#39640;&#32500;&#29305;&#24449;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;&#20351;&#29992;&#24191;&#20041;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#26469;&#20272;&#35745;&#26368;&#20339;&#20998;&#31163;&#36229;&#24179;&#38754;&#30340;&#26041;&#21521;&#12290;&#25152;&#20272;&#35745;&#30340;&#36229;&#24179;&#38754;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#34987;&#35777;&#26126;&#26159;&#25554;&#20540;&#30340;&#12290;&#34429;&#28982;&#27491;&#22914;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#36817;&#32467;&#26524;&#25152;&#39044;&#26399;&#30340;&#37027;&#26679;&#65292;&#26041;&#21521;&#21521;&#37327;&#21487;&#20197;&#19968;&#33268;&#22320;&#20272;&#35745;&#65292;&#20294;&#19968;&#20010;&#22825;&#30495;&#30340;&#25554;&#20214;&#20272;&#35745;&#26410;&#33021;&#19968;&#33268;&#22320;&#20272;&#35745;&#25318;&#25130;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#26657;&#27491;&#38656;&#35201;&#19968;&#20010;&#29420;&#31435;&#30340;&#20445;&#30041;&#26679;&#26412;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#21487;&#20197;&#20351;&#36807;&#31243;&#20445;&#25345;&#26368;&#23567;&#21270;&#26368;&#22823;&#20540;&#26368;&#20248;&#12290;&#21518;&#32493;&#36807;&#31243;&#30340;&#25554;&#20540;&#24615;&#36136;&#21487;&#20197;&#20445;&#30041;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#21462;&#20915;&#20110;&#26631;&#31614;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers binary classification of high-dimensional features under a postulated model with a low-dimensional latent Gaussian mixture structure and non-vanishing noise. A generalized least squares estimator is used to estimate the direction of the optimal separating hyperplane. The estimated hyperplane is shown to interpolate on the training data. While the direction vector can be consistently estimated as could be expected from recent results in linear regression, a naive plug-in estimate fails to consistently estimate the intercept. A simple correction, that requires an independent hold-out sample, renders the procedure minimax optimal in many scenarios. The interpolation property of the latter procedure can be retained, but surprisingly depends on the way the labels are encoded.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#24863;&#30693;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;ImComplete&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24187;&#35937;&#20986;&#19982;&#29615;&#22659;&#32972;&#26223;&#30456;&#21327;&#35843;&#30340;&#35270;&#35273;&#23454;&#20363;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#21644;&#32467;&#26500;&#30340;&#20687;&#32032;&#32423;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.12350</link><description>&lt;p&gt;
&#23454;&#20363;&#24863;&#30693;&#22270;&#20687;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Instance-Aware Image Completion. (arXiv:2210.12350v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#24863;&#30693;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;ImComplete&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24187;&#35937;&#20986;&#19982;&#29615;&#22659;&#32972;&#26223;&#30456;&#21327;&#35843;&#30340;&#35270;&#35273;&#23454;&#20363;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#21644;&#32467;&#26500;&#30340;&#20687;&#32032;&#32423;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20462;&#22797;&#26159;&#19968;&#39033;&#26088;&#22312;&#22635;&#34917;&#24102;&#26377;&#32570;&#22833;&#21306;&#22495;&#30340;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#20351;&#23427;&#20204;&#20855;&#26377;&#21512;&#29702;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#20687;&#20462;&#22797;&#26041;&#27861;&#24448;&#24448;&#36890;&#36807;&#22635;&#20805;&#21608;&#22260;&#32441;&#29702;&#26469;&#22635;&#34917;&#32570;&#22833;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#21435;&#24187;&#35937;&#19968;&#20010;&#19982;&#29615;&#22659;&#32972;&#26223;&#30456;&#21327;&#35843;&#30340;&#35270;&#35273;&#23454;&#20363;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;&#65292;&#21517;&#20026;ImComplete&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24187;&#35937;&#32570;&#22833;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#19982;&#21407;&#22987;&#32972;&#26223;&#21327;&#35843;&#12290;ImComplete&#39318;&#20808;&#37319;&#29992;&#20102;&#19968;&#20010;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#32771;&#34385;&#21040;&#21487;&#35265;&#23454;&#20363;&#21644;&#32570;&#22833;&#21306;&#22495;&#30340;&#20301;&#32622;&#12290;&#28982;&#21518;&#65292;ImComplete&#23436;&#25104;&#20102;&#32570;&#22833;&#21306;&#22495;&#20869;&#30340;&#35821;&#20041;&#20998;&#21106;&#25513;&#27169;&#65292;&#25552;&#20379;&#20687;&#32032;&#32423;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#25351;&#23548;&#12290;&#26368;&#21518;&#65292;&#22270;&#20687;&#21512;&#25104;&#22359;&#29983;&#25104;&#20102;&#36924;&#30495;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image completion is a task that aims to fill in the missing region of a masked image with plausible contents. However, existing image completion methods tend to fill in the missing region with the surrounding texture instead of hallucinating a visual instance that is suitable in accordance with the context of the scene. In this work, we propose a novel image completion model, dubbed ImComplete, that hallucinates the missing instance that harmonizes well with - and thus preserves - the original context. ImComplete first adopts a transformer architecture that considers the visible instances and the location of the missing region. Then, ImComplete completes the semantic segmentation masks within the missing region, providing pixel-level semantic and structural guidance. Finally, the image synthesis blocks generate photo-realistic content. We perform a comprehensive evaluation of the results in terms of visual quality (LPIPS and FID) and contextual preservation scores (CLIPscore and object
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ImageNet&#21644;Kinetics&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#30005;&#24433;&#39044;&#21578;&#29255;&#31867;&#22411;&#20998;&#31867;&#20013;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#36890;&#36807;Dual Image and Video Transformer Architecture&#25552;&#39640;&#20102;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.07983</link><description>&lt;p&gt;
&#21452;&#37325;&#22270;&#20687;&#21644;&#35270;&#39057;Transformer&#30340;&#24212;&#29992;&#22312;&#22810;&#26631;&#31614;&#30005;&#24433;&#39044;&#21578;&#29255;&#31867;&#22411;&#20998;&#31867;&#20013;&#65292;&#25552;&#21319;&#20102;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Transfer Learning with a Dual Image and Video Transformer for Multi-label Movie Trailer Genre Classification. (arXiv:2210.07983v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ImageNet&#21644;Kinetics&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#30005;&#24433;&#39044;&#21578;&#29255;&#31867;&#22411;&#20998;&#31867;&#20013;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#36890;&#36807;Dual Image and Video Transformer Architecture&#25552;&#39640;&#20102;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ImageNet&#31354;&#38388;&#21644;Kinetics&#26102;&#31354;&#34920;&#31034;&#23545;&#20110;&#22810;&#26631;&#31614;&#30005;&#24433;&#39044;&#21578;&#29255;&#31867;&#22411;&#20998;&#31867;&#65288;MTGC&#65289;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;ImageNet&#21644;Kinetics&#39044;&#20808;&#35757;&#32451;&#30340;ConvNet&#21644;Transformer&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#24182;&#22312;Trailers12k&#19978;&#36827;&#34892;&#20102;&#23637;&#31034;&#65292;Trailers12k&#26159;&#19968;&#20010;&#30001;12,000&#20010;&#35270;&#39057;&#32452;&#25104;&#30340;&#25163;&#24037;&#26631;&#27880;&#30340;&#30005;&#24433;&#39044;&#21578;&#29255;&#25968;&#25454;&#38598;&#65292;&#24182;&#24102;&#26377;10&#31181;&#19981;&#21516;&#30340;&#31867;&#22411;&#21644;&#30456;&#20851;&#20803;&#25968;&#25454;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21487;&#20197;&#24433;&#21709;&#21487;&#36801;&#31227;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#20363;&#22914;&#24103;&#29575;&#65292;&#36755;&#20837;&#35270;&#39057;&#25193;&#23637;&#21644;&#26102;&#31354;&#24314;&#27169;&#12290;&#20026;&#20102;&#32553;&#23567;ImageNet/Kinetics&#21644;Trailers12k&#20043;&#38388;&#30340;&#26102;&#31354;&#32467;&#26500;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Dual Image and Video Transformer Architecture (DIViTA)&#65292;&#35813;&#26550;&#26500;&#25191;&#34892;&#25293;&#25668;&#26816;&#27979;&#65292;&#23558;&#39044;&#21578;&#29255;&#20998;&#21106;&#25104;&#39640;&#24230;&#30456;&#20851;&#30340;&#29255;&#27573;&#65292;&#20026;&#39044;&#20808;&#35757;&#32451;&#30340;&#39592;&#24178;&#25552;&#20379;&#26356;&#36830;&#36143;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#21487;&#36801;&#31227;&#24615;&#65288;ImageNet&#25552;&#39640;&#20102;1.83%&#65292;Kinetics&#25552;&#39640;&#20102;3.75%&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the transferability of ImageNet spatial and Kinetics spatio-temporal representations to multi-label Movie Trailer Genre Classification (MTGC). In particular, we present an extensive evaluation of the transferability of ConvNet and Transformer models pretrained on ImageNet and Kinetics to Trailers12k, a new manually-curated movie trailer dataset composed of 12,000 videos labeled with 10 different genres and associated metadata. We analyze different aspects that can influence transferability, such as frame rate, input video extension, and spatio-temporal modeling. In order to reduce the spatio-temporal structure gap between ImageNet/Kinetics and Trailers12k, we propose Dual Image and Video Transformer Architecture (DIViTA), which performs shot detection so as to segment the trailer into highly correlated clips, providing a more cohesive input for pretrained backbones and improving transferability (a 1.83% increase for ImageNet and 3.75% for Kinetics). Our results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#31616;&#21333;&#30340;&#22522;&#32447;&#30740;&#31350;&#26159;&#21542;&#38656;&#35201;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#26469;&#23454;&#29616;&#19981;&#38388;&#26029;&#23398;&#20064;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#26412;&#36523;&#36275;&#22815;&#24378;&#22823;&#65292;&#33021;&#22312;&#19981;&#38388;&#26029;&#23398;&#20064;&#20013;&#23454;&#29616;&#31454;&#20105;&#24615;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.04428</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#19981;&#38388;&#26029;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31616;&#21333;&#22522;&#32447;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Simple Baseline that Questions the Use of Pretrained-Models in Continual Learning. (arXiv:2210.04428v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#31616;&#21333;&#30340;&#22522;&#32447;&#30740;&#31350;&#26159;&#21542;&#38656;&#35201;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#26469;&#23454;&#29616;&#19981;&#38388;&#26029;&#23398;&#20064;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#26412;&#36523;&#36275;&#22815;&#24378;&#22823;&#65292;&#33021;&#22312;&#19981;&#38388;&#26029;&#23398;&#20064;&#20013;&#23454;&#29616;&#31454;&#20105;&#24615;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#25216;&#26415;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#38388;&#26029;&#23398;&#20064;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#20854;&#20013;&#26377;&#20123;&#26041;&#27861;&#35774;&#35745;&#20102;&#22312;&#39044;&#35757;&#32451;&#34920;&#31034;&#19978;&#30340;&#19981;&#38388;&#26029;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#19981;&#38388;&#26029;&#23398;&#20064;&#26102;&#21482;&#20801;&#35768;&#23545;&#39592;&#24178;&#27169;&#22411;&#36827;&#34892;&#26368;&#23567;&#21270;&#25110;&#32773;&#27809;&#26377;&#26356;&#26032;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#38388;&#26029;&#23398;&#20064;&#20013;&#26159;&#21542;&#38656;&#35201;&#22797;&#26434;&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23558;&#23427;&#20204;&#19982;&#25105;&#20204;&#35774;&#35745;&#30340;&#31616;&#21333;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35748;&#20026;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#26412;&#36523;&#24050;&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#22312;Split-CIFAR100&#21644;CoRe 50&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#31454;&#20105;&#24615;&#29978;&#33267;&#26356;&#22909;&#30340;&#19981;&#26029;&#23398;&#20064;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#22522;&#32447;&#65306;1&#65289;&#20351;&#29992;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#27599;&#20010;&#31867;&#21035;&#36935;&#21040;&#30340;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#35745;&#31639;&#23427;&#20204;&#30340;&#30456;&#24212;&#22343;&#20540;&#29305;&#24449;&#65292;2&#65289;&#39044;&#27979;&#36755;&#20837;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the success of pretraining techniques in representation learning, a number of continual learning methods based on pretrained models have been proposed. Some of these methods design continual learning mechanisms on the pre-trained representations and only allow minimum updates or even no updates of the backbone models during the training of continual learning. In this paper, we question whether the complexity of these models is needed to achieve good performance by comparing them to a simple baseline that we designed. We argue that the pretrained feature extractor itself can be strong enough to achieve a competitive or even better continual learning performance on Split-CIFAR100 and CoRe 50 benchmarks. To validate this, we conduct a very simple baseline that 1) use the frozen pretrained model to extract image features for every class encountered during the continual learning stage and compute their corresponding mean features on training data, and 2) predict the class of the input 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#29983;&#28079;&#36866;&#24212;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#23398;&#20064;&#30340;&#31574;&#30053;&#20013;&#26500;&#24314;&#22810;&#26679;&#31574;&#30053;&#30340;&#32452;&#21512;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#28436;&#31034;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#21516;&#26102;&#25972;&#21512;&#28436;&#31034;&#20013;&#30340;&#20849;&#24615;&#30693;&#35782;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20219;&#21153;&#25512;&#26029;&#65292;&#36824;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#20013;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#31934;&#31616;&#30340;&#21407;&#22411;&#31574;&#30053;&#38598;&#21512;&#24182;&#36890;&#36807;&#31574;&#30053;&#32452;&#21512;&#26469;&#36924;&#36817;&#25152;&#26377;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2209.11908</link><description>&lt;p&gt;
&#26469;&#33258;&#28436;&#31034;&#30340;&#24555;&#36895;&#29983;&#28079;&#36866;&#24212;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations. (arXiv:2209.11908v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#29983;&#28079;&#36866;&#24212;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#23398;&#20064;&#30340;&#31574;&#30053;&#20013;&#26500;&#24314;&#22810;&#26679;&#31574;&#30053;&#30340;&#32452;&#21512;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#28436;&#31034;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#21516;&#26102;&#25972;&#21512;&#28436;&#31034;&#20013;&#30340;&#20849;&#24615;&#30693;&#35782;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20219;&#21153;&#25512;&#26029;&#65292;&#36824;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#20013;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#31934;&#31616;&#30340;&#21407;&#22411;&#31574;&#30053;&#38598;&#21512;&#24182;&#36890;&#36807;&#31574;&#30053;&#32452;&#21512;&#26469;&#36924;&#36817;&#25152;&#26377;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#23398;&#20064;&#65288;LfD&#65289;&#26041;&#27861;&#20351;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#25152;&#38656;&#34892;&#20026;&#30340;&#28436;&#31034;&#26469;&#25945;&#25480;&#26426;&#22120;&#20154;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#20351;&#29992;&#38754;&#26356;&#24191;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LfD&#26694;&#26550;&#26080;&#27861;&#24555;&#36895;&#36866;&#24212;&#24322;&#26500;&#30340;&#20154;&#31867;&#28436;&#31034;&#65292;&#20063;&#19981;&#33021;&#22312;&#26222;&#36866;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LfD&#26694;&#26550;&#8212;&#8212;&#24555;&#36895;&#29983;&#28079;&#36866;&#24212;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;FLAIR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;(1)&#21033;&#29992;&#23398;&#20064;&#31574;&#30053;&#26500;&#24314;&#22810;&#26679;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#28436;&#31034;&#65292;&#20801;&#35768;&#24555;&#36895;&#30340;&#32456;&#31471;&#29992;&#25143;&#20010;&#24615;&#21270;&#65292;(2)&#25972;&#21512;&#28436;&#31034;&#20013;&#30340;&#20849;&#24615;&#30693;&#35782;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20219;&#21153;&#25512;&#26029;&#65307;(3)&#22312;&#32456;&#36523;&#37096;&#32626;&#20013;&#21482;&#22312;&#38656;&#35201;&#26102;&#25193;&#23637;&#20854;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#32452;&#21512;&#32500;&#25252;&#19968;&#20010;&#31934;&#31616;&#30340;&#21407;&#22411;&#31574;&#30053;&#38598;&#21512;&#65292;&#24182;&#33021;&#22815;&#36924;&#36817;&#25152;&#26377;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;FLAIR&#23454;&#29616;&#20102;&#36866;&#24212;&#24615;&#65288;&#21363;&#26426;&#22120;&#20154;&#36866;&#24212;&#20102;&#24322;&#26500;&#30340;&#12289;&#29305;&#23450;&#20110;&#29992;&#25143;&#30340;&#20219;&#21153;&#65289;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#20013;&#33410;&#30465;&#20102;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Demonstration (LfD) approaches empower end-users to teach robots novel tasks via demonstrations of the desired behaviors, democratizing access to robotics. However, current LfD frameworks are not capable of fast adaptation to heterogeneous human demonstrations nor the large-scale deployment in ubiquitous robotics applications. In this paper, we propose a novel LfD framework, Fast Lifelong Adaptive Inverse Reinforcement learning (FLAIR). Our approach (1) leverages learned strategies to construct policy mixtures for fast adaptation to new demonstrations, allowing for quick end-user personalization, (2) distills common knowledge across demonstrations, achieving accurate task inference; and (3) expands its model only when needed in lifelong deployments, maintaining a concise set of prototypical strategies that can approximate all behaviors via policy mixtures. We empirically validate that FLAIR achieves adaptability (i.e., the robot adapts to heterogeneous, user-specific task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35282;&#30697;&#38453;&#30340;&#20108;&#27425;&#26799;&#24230;&#65292;&#21487;&#20197;&#21152;&#36895;&#26799;&#24230;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#30740;&#31350;&#32773;&#36824;&#25512;&#27979;&#28023;&#26862;&#30697;&#38453;&#19982;&#23398;&#20064;&#29575;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2209.03282</link><description>&lt;p&gt;
&#20108;&#27425;&#26799;&#24230;&#65306;&#23558;&#26799;&#24230;&#31639;&#27861;&#21644;&#29275;&#39039;&#27861;&#34701;&#21512;&#20026;&#19968;&#20307;
&lt;/p&gt;
&lt;p&gt;
Quadratic Gradient: Combining Gradient Algorithms and Newton's Method as One. (arXiv:2209.03282v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35282;&#30697;&#38453;&#30340;&#20108;&#27425;&#26799;&#24230;&#65292;&#21487;&#20197;&#21152;&#36895;&#26799;&#24230;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#30740;&#31350;&#32773;&#36824;&#25512;&#27979;&#28023;&#26862;&#30697;&#38453;&#19982;&#23398;&#20064;&#29575;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19968;&#21015;&#19982;&#26799;&#24230;&#30456;&#21516;&#22823;&#23567;&#30340;&#21015;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20165;&#20351;&#29992;&#19968;&#20010;&#28014;&#28857;&#25968;&#26469;&#21152;&#36895;&#27599;&#20010;&#26799;&#24230;&#20803;&#32032;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#21487;&#33021;&#23545;&#29275;&#39039;&#27861;&#30340;&#32447;&#25628;&#32034;&#25216;&#26415;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#20010;&#19982;&#28023;&#26862;&#30697;&#38453;&#22823;&#23567;&#30456;&#21516;&#30340;&#27491;&#26041;&#24418;&#30697;&#38453;&#26469;&#32416;&#27491;&#28023;&#26862;&#30697;&#38453;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;Chiang&#25552;&#20986;&#20102;&#19968;&#31181;&#20171;&#20110;&#21015;&#21521;&#37327;&#21644;&#27491;&#26041;&#24418;&#30697;&#38453;&#20043;&#38388;&#30340;&#19996;&#35199;&#65292;&#21363;&#23545;&#35282;&#30697;&#38453;&#65292;&#26469;&#21152;&#36895;&#26799;&#24230;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#31216;&#20026;&#20108;&#27425;&#26799;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26500;&#24314;&#26032;&#29256;&#26412;&#30340;&#20108;&#27425;&#26799;&#24230;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26032;&#30340;&#20108;&#27425;&#26799;&#24230;&#19981;&#28385;&#36275;&#22266;&#23450;&#28023;&#26862;&#29275;&#39039;&#27861;&#30340;&#25910;&#25947;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23427;&#26377;&#26102;&#27604;&#21407;&#22987;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;Chiang&#25512;&#27979;&#28023;&#26862;&#30697;&#38453;&#19982;&#23398;&#20064;&#29575;f&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
It might be inadequate for the line search technique for Newton's method to use only one floating point number. A column vector of the same size as the gradient might be better than a mere float number to accelerate each of the gradient elements with different rates. Moreover, a square matrix of the same order as the Hessian matrix might be helpful to correct the Hessian matrix. Chiang applied something between a column vector and a square matrix, namely a diagonal matrix, to accelerate the gradient and further proposed a faster gradient variant called quadratic gradient. In this paper, we present a new way to build a new version of the quadratic gradient. This new quadratic gradient doesn't satisfy the convergence conditions of the fixed Hessian Newton's method. However, experimental results show that it sometimes has a better performance than the original one in convergence rate. Also, Chiang speculates that there might be a relation between the Hessian matrix and the learning rate f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#38024;&#23545;&#22270;&#24418;&#32467;&#26500;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#26356;&#20542;&#21521;&#20110;&#22686;&#21152;&#31867;&#38388;&#36793;&#32536;&#65292;&#36890;&#36807;&#36830;&#25509;&#19981;&#21516;&#31867;&#30340;&#33410;&#28857;&#26469;&#26356;&#26377;&#25928;&#22320;&#30772;&#22351;&#33410;&#28857;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;GNN &#30340;&#22266;&#26377;&#24179;&#28369;&#24615;&#20250;&#23548;&#33268;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#20002;&#22833;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#32423;&#20256;&#25773;&#30340;&#26032;&#22411;&#20195;&#29702;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.12815</link><description>&lt;p&gt;
&#24403;&#25915;&#20987;&#22270;&#24418;&#32467;&#26500;&#26102;&#65292;&#26799;&#24230;&#21578;&#35785;&#25105;&#20204;&#20160;&#20040;
&lt;/p&gt;
&lt;p&gt;
What Does the Gradient Tell When Attacking the Graph Structure. (arXiv:2208.12815v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#38024;&#23545;&#22270;&#24418;&#32467;&#26500;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#26356;&#20542;&#21521;&#20110;&#22686;&#21152;&#31867;&#38388;&#36793;&#32536;&#65292;&#36890;&#36807;&#36830;&#25509;&#19981;&#21516;&#31867;&#30340;&#33410;&#28857;&#26469;&#26356;&#26377;&#25928;&#22320;&#30772;&#22351;&#33410;&#28857;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;GNN &#30340;&#22266;&#26377;&#24179;&#28369;&#24615;&#20250;&#23548;&#33268;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#20002;&#22833;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#32423;&#20256;&#25773;&#30340;&#26032;&#22411;&#20195;&#29702;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#23481;&#26131;&#21463;&#21040;&#38024;&#23545;&#22270;&#24418;&#32467;&#26500;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#19968;&#20010;&#24694;&#24847;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#36793;&#32536;&#33539;&#22260;&#20869;&#65292;&#36890;&#36807;&#32473;&#20986;&#30340;&#35757;&#32451;&#26631;&#31614;&#65292;&#26469;&#30772;&#22351;&#21463;&#23475;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20043;&#21069;&#30340;&#32463;&#39564;&#24615;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#32773;&#26356;&#20542;&#21521;&#20110;&#28155;&#21152;&#36793;&#32536;&#32780;&#19981;&#26159;&#21024;&#38500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35777;&#26126;&#65292;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#20542;&#21521;&#20110;&#22686;&#21152;&#31867;&#38388;&#36793;&#32536;&#65292;&#36825;&#26159;&#30001;&#20110; GNN &#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#36825;&#20063;&#35299;&#37322;&#20102;&#20043;&#21069;&#30340;&#19968;&#20123;&#32463;&#39564;&#35266;&#23519;&#12290;&#36890;&#36807;&#36830;&#25509;&#19981;&#21516;&#31867;&#30340;&#33410;&#28857;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#30772;&#22351;&#33410;&#28857;&#29305;&#24449;&#65292;&#20174;&#32780;&#20351;&#27492;&#31867;&#25915;&#20987;&#26356;&#20855;&#20248;&#21183;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; GNN &#28040;&#24687;&#20256;&#36882;&#30340;&#22266;&#26377;&#24179;&#28369;&#24615;&#20250;&#23558;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#33410;&#28857;&#24046;&#24322;&#27169;&#31946;&#21270;&#65292;&#23548;&#33268;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#20002;&#22833;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#32423;&#20256;&#25773;&#30340;&#26032;&#22411;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has revealed that Graph Neural Networks (GNNs) are susceptible to adversarial attacks targeting the graph structure. A malicious attacker can manipulate a limited number of edges, given the training labels, to impair the victim model's performance. Previous empirical studies indicate that gradient-based attackers tend to add edges rather than remove them. In this paper, we present a theoretical demonstration revealing that attackers tend to increase inter-class edges due to the message passing mechanism of GNNs, which explains some previous empirical observations. By connecting dissimilar nodes, attackers can more effectively corrupt node features, making such attacks more advantageous. However, we demonstrate that the inherent smoothness of GNN's message passing tends to blur node dissimilarity in the feature space, leading to the loss of crucial information during the forward process. To address this issue, we propose a novel surrogate model with multi-level propagati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#20108;&#27425;&#26799;&#24230;&#30340;&#26041;&#27861;&#21040;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#22686;&#24378;&#26041;&#27861;&#33021;&#22815;&#20998;&#21035;&#27604;&#23427;&#20204;&#30340;&#21407;&#22987;&#26041;&#27861;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2208.06828</link><description>&lt;p&gt;
&#36890;&#36807;&#20108;&#27425;&#26799;&#24230;&#30340;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multinomial Logistic Regression Algorithms via Quadratic Gradient. (arXiv:2208.06828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#20108;&#27425;&#26799;&#24230;&#30340;&#26041;&#27861;&#21040;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#22686;&#24378;&#26041;&#27861;&#33021;&#22815;&#20998;&#21035;&#27604;&#23427;&#20204;&#30340;&#21407;&#22987;&#26041;&#27861;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#26159;&#19968;&#31181;&#22522;&#30784;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23427;&#23558;&#20108;&#20803;&#36923;&#36753;&#22238;&#24402;&#25512;&#24191;&#21040;&#22810;&#31867;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#19968;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31216;&#20026;&#8220;&#20108;&#27425;&#26799;&#24230;&#8221;&#30340;&#26356;&#24555;&#26799;&#24230;&#65292;&#21487;&#20197;&#21152;&#36895;&#20108;&#20803;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;Nesterov&#21152;&#36895;&#26799;&#24230;&#65288;NAG&#65289;&#26041;&#27861;&#29992;&#20110;&#20108;&#20803;&#36923;&#36753;&#22238;&#24402;&#12290;&#26412;&#25991;&#23558;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#21040;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#31639;&#27861;(Adagrad)&#65292;&#21487;&#20197;&#21152;&#36895;&#21407;&#22987;Adagrad&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20123;&#22810;&#31867;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#22686;&#24378;&#30340;NAG&#26041;&#27861;&#21644;&#22686;&#24378;&#30340;Adagrad&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20004;&#31181;&#22686;&#24378;&#26041;&#27861;&#20998;&#21035;&#27604;&#23427;&#20204;&#30340;&#21407;&#22987;&#26041;&#27861;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multinomial logistic regression, also known by other names such as multiclass logistic regression and softmax regression, is a fundamental classification method that generalizes binary logistic regression to multiclass problems. A recently work proposed a faster gradient called $\texttt{quadratic gradient}$ that can accelerate the binary logistic regression training, and presented an enhanced Nesterov's accelerated gradient (NAG) method for binary logistic regression.  In this paper, we extend this work to multiclass logistic regression and propose an enhanced Adaptive Gradient Algorithm (Adagrad) that can accelerate the original Adagrad method. We test the enhanced NAG method and the enhanced Adagrad method on some multiclass-problem datasets. Experimental results show that both enhanced methods converge faster than their original ones respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#24102;&#26377;&#22266;&#23450;&#27493;&#38271;&#30340;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#65288;LSA&#65289;&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;LSA&#21450;&#20854;Polyak-Ruppert&#24179;&#22343;&#21270;&#29256;&#26412;&#23450;&#20041;&#30340;&#36845;&#20195;&#30340;$p$&#38454;&#30697;&#21644;&#39640;&#27010;&#29575;&#20559;&#24046;&#30028;&#30340;&#25512;&#23548;&#12290;&#26412;&#25991;&#24471;&#21040;&#30340;&#26377;&#38480;&#26102;&#38388;&#23454;&#20363;&#30456;&#20851;&#30340;&#24179;&#22343;LSA&#36845;&#20195;&#30340;&#30028;&#38480;&#26159;&#23574;&#38160;&#30340;&#65292;&#24182;&#19982;&#23616;&#37096;&#28176;&#36817;&#26497;&#23567;&#26497;&#20540;&#38480;&#21046;&#30456;&#21516;&#65292;&#21516;&#26102;&#21097;&#20313;&#39033;&#23545;&#28151;&#21512;&#26102;&#38388;&#26377;&#30528;&#32039;&#23494;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2207.04475</link><description>&lt;p&gt;
&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#30340;Polyak-Ruppert&#24179;&#22343;&#36845;&#20195;&#30340;&#26377;&#38480;&#26102;&#38388;&#39640;&#27010;&#29575;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Finite-time High-probability Bounds for Polyak-Ruppert Averaged Iterates of Linear Stochastic Approximation. (arXiv:2207.04475v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#24102;&#26377;&#22266;&#23450;&#27493;&#38271;&#30340;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#65288;LSA&#65289;&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;LSA&#21450;&#20854;Polyak-Ruppert&#24179;&#22343;&#21270;&#29256;&#26412;&#23450;&#20041;&#30340;&#36845;&#20195;&#30340;$p$&#38454;&#30697;&#21644;&#39640;&#27010;&#29575;&#20559;&#24046;&#30028;&#30340;&#25512;&#23548;&#12290;&#26412;&#25991;&#24471;&#21040;&#30340;&#26377;&#38480;&#26102;&#38388;&#23454;&#20363;&#30456;&#20851;&#30340;&#24179;&#22343;LSA&#36845;&#20195;&#30340;&#30028;&#38480;&#26159;&#23574;&#38160;&#30340;&#65292;&#24182;&#19982;&#23616;&#37096;&#28176;&#36817;&#26497;&#23567;&#26497;&#20540;&#38480;&#21046;&#30456;&#21516;&#65292;&#21516;&#26102;&#21097;&#20313;&#39033;&#23545;&#28151;&#21512;&#26102;&#38388;&#26377;&#30528;&#32039;&#23494;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#24102;&#26377;&#22266;&#23450;&#27493;&#38271;&#30340;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#65288;LSA&#65289;&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#36825;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#26041;&#27861;&#12290;LSA&#34987;&#29992;&#26469;&#35745;&#31639;$d$&#32500;&#32447;&#24615;&#31995;&#32479;$\bar{\mathbf{A}}\theta = \bar{\mathbf{b}}$&#30340;&#36817;&#20284;&#35299;&#65292;&#20854;&#20013;$(\bar{\mathbf{A}},\bar{\mathbf{b}})$&#20165;&#33021;&#36890;&#36807;&#65288;&#28176;&#36817;&#65289;&#26080;&#20559;&#35266;&#27979;$\{(\mathbf{A}(Z_n),\mathbf{b}(Z_n))\}_{n \in \mathbb{N}}$&#26469;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#32771;&#34385;&#30340;&#24773;&#20917;&#26159;$\{Z_n\}_{n \in \mathbb{N}}$&#26159;i.i.d.&#24207;&#21015;&#25110;&#22343;&#21248;&#20960;&#20309;&#36941;&#21382;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;LSA&#21450;&#20854;Polyak-Ruppert&#24179;&#22343;&#21270;&#29256;&#26412;&#23450;&#20041;&#30340;&#36845;&#20195;&#36827;&#34892;$p$&#38454;&#30697;&#21644;&#39640;&#27010;&#29575;&#20559;&#24046;&#30028;&#30340;&#25512;&#23548;&#12290;&#25105;&#20204;&#33719;&#24471;&#30340;&#26377;&#38480;&#26102;&#38388;&#23454;&#20363;&#30456;&#20851;&#30340;&#24179;&#22343;LSA&#36845;&#20195;&#30340;&#30028;&#38480;&#26159;&#23574;&#38160;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#33719;&#24471;&#30340;&#20027;&#39033;&#19982;&#23616;&#37096;&#28176;&#36817;&#26497;&#23567;&#26497;&#20540;&#38480;&#21046;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#30340;&#21097;&#20313;&#39033;&#23545;&#28151;&#21512;&#26102;&#38388;&#26377;&#30528;&#32039;&#23494;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a finite-time analysis of linear stochastic approximation (LSA) algorithms with fixed step size, a core method in statistics and machine learning. LSA is used to compute approximate solutions of a $d$-dimensional linear system $\bar{\mathbf{A}} \theta = \bar{\mathbf{b}}$ for which $(\bar{\mathbf{A}}, \bar{\mathbf{b}})$ can only be estimated by (asymptotically) unbiased observations $\{(\mathbf{A}(Z_n),\mathbf{b}(Z_n))\}_{n \in \mathbb{N}}$. We consider here the case where $\{Z_n\}_{n \in \mathbb{N}}$ is an i.i.d. sequence or a uniformly geometrically ergodic Markov chain. We derive $p$-th moment and high-probability deviation bounds for the iterates defined by LSA and its Polyak-Ruppert-averaged version. Our finite-time instance-dependent bounds for the averaged LSA iterates are sharp in the sense that the leading term we obtain coincides with the local asymptotic minimax limit. Moreover, the remainder terms of our bounds admit a tight dependence on the mixing time 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26816;&#32034;&#21644;&#25490;&#21517;&#20004;&#38454;&#27573;&#24037;&#20316;&#27969;&#31243;&#12290;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#36825;&#20004;&#20010;&#32452;&#20214;&#37117;&#26159;&#29420;&#31435;&#35757;&#32451;&#25110;&#20351;&#29992;&#31616;&#21333;&#30340;&#32423;&#32852;&#31649;&#36947;&#65292;&#25928;&#26524;&#19981;&#20339;&#12290;&#26368;&#36817;&#19968;&#20123;&#24037;&#20316;&#25552;&#20986;&#32852;&#21512;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#25490;&#21517;&#22120;&#65292;&#20294;&#20173;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36824;&#38656;&#35201;&#25506;&#32034;&#26356;&#26377;&#25928;&#30340;&#21327;&#20316;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.14649</link><description>&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#21327;&#20316;&#26816;&#32034;&#22120;&#21644;&#25490;&#21517;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cooperative Retriever and Ranker in Deep Recommenders. (arXiv:2206.14649v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26816;&#32034;&#21644;&#25490;&#21517;&#20004;&#38454;&#27573;&#24037;&#20316;&#27969;&#31243;&#12290;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#36825;&#20004;&#20010;&#32452;&#20214;&#37117;&#26159;&#29420;&#31435;&#35757;&#32451;&#25110;&#20351;&#29992;&#31616;&#21333;&#30340;&#32423;&#32852;&#31649;&#36947;&#65292;&#25928;&#26524;&#19981;&#20339;&#12290;&#26368;&#36817;&#19968;&#20123;&#24037;&#20316;&#25552;&#20986;&#32852;&#21512;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#25490;&#21517;&#22120;&#65292;&#20294;&#20173;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36824;&#38656;&#35201;&#25506;&#32034;&#26356;&#26377;&#25928;&#30340;&#21327;&#20316;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;(DRS)&#22312;&#29616;&#20195;&#32593;&#32476;&#26381;&#21153;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#22788;&#29702;&#28023;&#37327;&#32593;&#32476;&#20869;&#23481;&#65292;DRS&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#24037;&#20316;&#27969;&#31243;&#65306;&#26816;&#32034;&#21644;&#25490;&#21517;&#65292;&#20197;&#29983;&#25104;&#20854;&#25512;&#33616;&#32467;&#26524;&#12290;&#26816;&#32034;&#22120;&#26088;&#22312;&#39640;&#25928;&#22320;&#20174;&#25972;&#20010;&#39033;&#30446;&#20013;&#36873;&#25321;&#19968;&#23567;&#32452;&#30456;&#20851;&#20505;&#36873;&#39033;&#65307;&#32780;&#25490;&#21517;&#22120;&#36890;&#24120;&#26356;&#31934;&#30830;&#20294;&#26102;&#38388;&#28040;&#32791;&#26356;&#22823;&#65292;&#24212;&#36827;&#19968;&#27493;&#20174;&#26816;&#32034;&#20505;&#36873;&#39033;&#20013;&#20248;&#21270;&#26368;&#20339;&#39033;&#30446;&#12290;&#20256;&#32479;&#19978;&#65292;&#20004;&#20010;&#32452;&#20214;&#35201;&#20040;&#29420;&#31435;&#35757;&#32451;&#65292;&#35201;&#20040;&#22312;&#31616;&#21333;&#30340;&#32423;&#32852;&#31649;&#36947;&#20869;&#35757;&#32451;&#65292;&#36825;&#23481;&#26131;&#20135;&#29983;&#21512;&#20316;&#25928;&#26524;&#24046;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#26368;&#36817;&#19968;&#20123;&#24037;&#20316;&#24314;&#35758;&#32852;&#21512;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#25490;&#21517;&#22120;&#65292;&#20294;&#20173;&#23384;&#22312;&#35768;&#22810;&#20005;&#37325;&#38480;&#21046;&#65306;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#30340;&#39033;&#20998;&#24067;&#36716;&#31227;&#12289;&#20551;&#38452;&#24615;&#21644;&#25490;&#21517;&#39034;&#24207;&#19981;&#23545;&#40784;&#31561;&#12290;&#22240;&#27492;&#65292;&#25506;&#32034;&#26816;&#32034;&#22120;&#21644;&#25490;&#21517;&#22120;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#20173;&#28982;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep recommender systems (DRS) are intensively applied in modern web services. To deal with the massive web contents, DRS employs a two-stage workflow: retrieval and ranking, to generate its recommendation results. The retriever aims to select a small set of relevant candidates from the entire items with high efficiency; while the ranker, usually more precise but time-consuming, is supposed to further refine the best items from the retrieved candidates. Traditionally, the two components are trained either independently or within a simple cascading pipeline, which is prone to poor collaboration effect. Though some latest works suggested to train retriever and ranker jointly, there still exist many severe limitations: item distribution shift between training and inference, false negative, and misalignment of ranking order. As such, it remains to explore effective collaborations between retriever and ranker.
&lt;/p&gt;</description></item><item><title>NovelCraft&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#24320;&#25918;&#19990;&#30028;&#20013;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#21457;&#29616;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#22312;&#22797;&#26434;&#30340;&#22330;&#26223;&#20013;&#25554;&#20837;&#26032;&#39062;&#29289;&#20307;&#30340;&#26816;&#27979;&#38656;&#35201;&#26356;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#20102;&#25511;&#21046;&#20551;&#38451;&#24615;&#26102;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#33021;&#27604;&#22797;&#26434;&#30340;&#26041;&#27861;&#26356;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2206.11736</link><description>&lt;p&gt;
NovelCraft&#65306;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#21457;&#29616;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds. (arXiv:2206.11736v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11736
&lt;/p&gt;
&lt;p&gt;
NovelCraft&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#24320;&#25918;&#19990;&#30028;&#20013;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#21457;&#29616;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#22312;&#22797;&#26434;&#30340;&#22330;&#26223;&#20013;&#25554;&#20837;&#26032;&#39062;&#29289;&#20307;&#30340;&#26816;&#27979;&#38656;&#35201;&#26356;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#20102;&#25511;&#21046;&#20551;&#38451;&#24615;&#26102;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#33021;&#27604;&#22797;&#26434;&#30340;&#26041;&#27861;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#25104;&#21151;&#25191;&#34892;&#20219;&#21153;&#65292;&#24517;&#39035;&#33021;&#22815;&#26816;&#27979;&#21644;&#36866;&#24212;&#26032;&#39062;&#24615;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#26032;&#39062;&#24615;&#26816;&#27979;&#30740;&#31350;&#36890;&#24120;&#21482;&#35780;&#20272;&#26088;&#22312;&#36827;&#34892;&#23545;&#35937;&#20998;&#31867;&#30340;&#37325;&#22797;&#21033;&#29992;&#25968;&#25454;&#38598;&#65288;&#22914;CIFAR-10&#65289;&#65292;&#20854;&#20013;&#22270;&#20687;&#32858;&#28966;&#20110;&#19968;&#20010;&#26126;&#26174;&#12289;&#23621;&#20013;&#30340;&#23545;&#35937;&#12290;&#38656;&#35201;&#26032;&#30340;&#22522;&#20934;&#26469;&#20195;&#34920;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#23548;&#33322;&#22797;&#26434;&#22330;&#26223;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26032;NovelCraft&#25968;&#25454;&#38598;&#21253;&#21547;&#23436;&#25104;&#20462;&#25913;&#21518;&#30340;Minecraft&#29615;&#22659;&#20013;&#30340;&#36339;&#36339;&#29699;&#35013;&#37197;&#20219;&#21153;&#30340;&#20195;&#29702;&#25152;&#30475;&#21040;&#30340;&#22270;&#20687;&#21644;&#31526;&#21495;&#19990;&#30028;&#29366;&#24577;&#30340;&#22810;&#27169;&#24335;&#24773;&#33410;&#25968;&#25454;&#12290;&#22312;&#26576;&#20123;&#24773;&#33410;&#20013;&#65292;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;3D&#22330;&#26223;&#20013;&#25554;&#20837;&#26032;&#39062;&#29289;&#20307;&#65292;&#36825;&#20123;&#29289;&#20307;&#21487;&#33021;&#24433;&#21709;&#28216;&#25103;&#29609;&#27861;&#24182;&#20986;&#29616;&#22312;&#21508;&#31181;&#22823;&#23567;&#21644;&#20301;&#32622;&#20013;&#12290;&#25105;&#20204;&#30340;&#35270;&#35273;&#26032;&#39062;&#24615;&#26816;&#27979;&#22522;&#20934;&#21457;&#29616;&#65292;&#25511;&#21046;&#20551;&#38451;&#24615;&#26102;&#65292;&#26368;&#22909;&#30340;&#38754;&#31215;&#19979;&#26354;&#32447;&#24230;&#37327;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#34987;&#26356;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#36229;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order for artificial agents to successfully perform tasks in changing environments, they must be able to both detect and adapt to novelty. However, visual novelty detection research often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object classification, where images focus on one distinct, well-centered object. New benchmarks are needed to represent the challenges of navigating the complex scenes of an open world. Our new NovelCraft dataset contains multimodal episodic data of the images and symbolic world-states seen by an agent completing a pogo stick assembly task within a modified Minecraft environment. In some episodes, we insert novel objects within the complex 3D scene that may impact gameplay and appear in a variety of sizes and positions. Our visual novelty detection benchmark finds that methods that rank best on popular area-under-the-curve metrics may be outperformed by simpler alternatives when controlling false positives matters most. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36947;&#36335;&#29992;&#25143;&#20132;&#20114;&#39044;&#27979;&#26041;&#38754;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;RMSE&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#30340;&#34892;&#20026;&#21487;&#33021;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#38656;&#35201;&#26356;&#32454;&#33268;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2206.11110</link><description>&lt;p&gt;
&#36229;&#36234;RMSE&#65306;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36947;&#36335;&#29992;&#25143;&#20132;&#20114;&#26159;&#21542;&#20135;&#29983;&#31867;&#20154;&#34892;&#20026;&#65311;
&lt;/p&gt;
&lt;p&gt;
Beyond RMSE: Do machine-learned models of road user interaction produce human-like behavior?. (arXiv:2206.11110v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36947;&#36335;&#29992;&#25143;&#20132;&#20114;&#39044;&#27979;&#26041;&#38754;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;RMSE&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#30340;&#34892;&#20026;&#21487;&#33021;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#38656;&#35201;&#26356;&#32454;&#33268;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20351;&#29992;&#21508;&#31181;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#21608;&#22260;&#36947;&#36335;&#29992;&#25143;&#30340;&#34892;&#20026;&#12290;&#22823;&#22810;&#25968;&#25991;&#29486;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20391;&#37325;&#20110;&#23398;&#20064;&#21644;&#25253;&#21578;&#27169;&#22411;&#30340;&#33021;&#21147;&#30340;&#23450;&#37327;&#35823;&#24046;&#24230;&#37327;&#65292;&#22914;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#12290;&#36825;&#31181;&#20391;&#37325;&#20110;&#23450;&#37327;&#35823;&#24046;&#24230;&#37327;&#30340;&#20570;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#27169;&#22411;&#30340;&#26356;&#37325;&#35201;&#30340;&#34892;&#20026;&#26041;&#38754;&#65292;&#24341;&#21457;&#20102;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#30830;&#23454;&#39044;&#27979;&#20102;&#31867;&#20154;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20197;&#31867;&#20284;&#20110;&#20256;&#32479;&#34892;&#20026;&#30740;&#31350;&#20013;&#20998;&#26512;&#20154;&#31867;&#25968;&#25454;&#30340;&#26041;&#24335;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#20171;&#32461;&#23450;&#37327;&#24230;&#37327;&#26469;&#35777;&#26126;&#33258;&#28982;&#34892;&#39542;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#19977;&#31181;&#19981;&#21516;&#34892;&#20026;&#29616;&#35937;&#30340;&#23384;&#22312;&#65306;1&#65289;&#35841;&#20808;&#36890;&#36807;&#21512;&#24182;&#28857;&#21462;&#20915;&#20110;&#36816;&#21160;&#23398; 2&#65289;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#65292;&#36710;&#36947;&#21464;&#26356;&#20197;&#36866;&#24212;&#21277;&#36947;&#36710;&#36742; 3&#65289;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#36991;&#20813;&#21069;&#36710;&#20914;&#31361;&#30340;&#36710;&#36742;&#21464;&#36947;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25191;&#34892;&#36825;&#19977;&#31181;&#29616;&#35937;&#26102;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;RMSE&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#34920;&#29616;&#20986;&#19981;&#21516;&#20110;&#20154;&#31867;&#34892;&#20026;&#30340;&#34892;&#20026;&#65292;&#36825;&#20984;&#26174;&#20102;&#22312;&#36947;&#36335;&#29992;&#25143;&#20132;&#20114;&#29615;&#22659;&#20013;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#33268;&#20998;&#26512;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles use a variety of sensors and machine-learned models to predict the behavior of surrounding road users. Most of the machine-learned models in the literature focus on quantitative error metrics like the root mean square error (RMSE) to learn and report their models' capabilities. This focus on quantitative error metrics tends to ignore the more important behavioral aspect of the models, raising the question of whether these models really predict human-like behavior. Thus, we propose to analyze the output of machine-learned models much like we would analyze human data in conventional behavioral research. We introduce quantitative metrics to demonstrate presence of three different behavioral phenomena in a naturalistic highway driving dataset: 1) The kinematics-dependence of who passes a merging point first 2) Lane change by an on-highway vehicle to accommodate an on-ramp vehicle 3) Lane changes by vehicles on the highway to avoid lead vehicle conflicts. Then, we analyz
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32437;&#21521;&#20998;&#21106;&#25968;&#25454;&#30340;&#36890;&#20449;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21608;&#26399;&#24615;&#20849;&#20139;&#21387;&#32553;&#30340;&#20013;&#38388;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#36890;&#20449;&#37327;&#20943;&#23569;&#65292;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.08330</link><description>&lt;p&gt;
Compressed-VFL: &#29992;&#20110;&#20855;&#26377;&#32437;&#21521;&#20998;&#21106;&#25968;&#25454;&#30340;&#36890;&#20449;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compressed-VFL: Communication-Efficient Learning with Vertically Partitioned Data. (arXiv:2206.08330v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08330
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32437;&#21521;&#20998;&#21106;&#25968;&#25454;&#30340;&#36890;&#20449;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21608;&#26399;&#24615;&#20849;&#20139;&#21387;&#32553;&#30340;&#20013;&#38388;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#36890;&#20449;&#37327;&#20943;&#23569;&#65292;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32437;&#21521;&#20998;&#21106;&#25968;&#25454;&#30340;&#36890;&#20449;&#39640;&#25928;&#35757;&#32451;&#30340;&#21387;&#32553;&#24335;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#65288;C-VFL&#65289;&#12290;&#22312;C-VFL&#20013;&#65292;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#21442;&#19982;&#26041;&#21033;&#29992;&#20960;&#20010;&#26412;&#22320;&#36845;&#20195;&#21327;&#21516;&#35757;&#32451;&#21508;&#33258;&#29305;&#24449;&#19978;&#30340;&#27169;&#22411;&#65292;&#24182;&#21608;&#26399;&#24615;&#22320;&#20849;&#20139;&#21387;&#32553;&#21518;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#25552;&#20379;&#20102;&#21387;&#32553;&#23545;&#20110;&#32437;&#21521;&#20998;&#21106;&#25968;&#25454;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#24433;&#21709;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21387;&#32553;&#35823;&#24046;&#34987;&#38480;&#21046;&#26102;&#65292;&#38750;&#20984;&#30446;&#26631;&#30340;&#25910;&#25947;&#36895;&#29575;&#20026; $O(\frac{1}{\sqrt{T}})$&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#24120;&#29992;&#21387;&#32553;&#25216;&#26415;&#65288;&#20363;&#22914;&#37327;&#21270;&#21644;&#21069;$k$&#22823;&#31232;&#30095;&#21270;&#65289;&#36827;&#34892;&#25910;&#25947;&#25152;&#38656;&#30340;&#20855;&#20307;&#35201;&#27714;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#65292;&#22312;&#19981;&#26126;&#26174;&#38477;&#20302;VFL&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21387;&#32553;&#21487;&#20197;&#23558;&#36890;&#20449;&#37327;&#20943;&#23569;&#36229;&#36807; $90\%$&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Compressed Vertical Federated Learning (C-VFL) for communication-efficient training on vertically partitioned data. In C-VFL, a server and multiple parties collaboratively train a model on their respective features utilizing several local iterations and sharing compressed intermediate results periodically. Our work provides the first theoretical analysis of the effect message compression has on distributed training over vertically partitioned data. We prove convergence of non-convex objectives at a rate of $O(\frac{1}{\sqrt{T}})$ when the compression error is bounded over the course of training. We provide specific requirements for convergence with common compression techniques, such as quantization and top-$k$ sparsification. Finally, we experimentally show compression can reduce communication by over $90\%$ without a significant decrease in accuracy over VFL without compression.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#30340;&#26080;&#32447;&#30005;-&#35270;&#35273;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;(MaxRay)&#65292;&#36890;&#36807;&#20174;&#26080;&#32447;&#30005;-&#35270;&#35273;&#23545;&#24212;&#20013;&#25552;&#21462;&#33258;&#22352;&#26631;&#26469;&#33258;&#20027;&#23398;&#20064;&#30446;&#26631;&#22312;&#26080;&#32447;&#30005;&#20013;&#30340;&#23450;&#20301;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26080;&#38656;&#26631;&#31614;&#30340;&#20934;&#30830;&#26080;&#32447;&#30005;&#30446;&#26631;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2206.06424</link><description>&lt;p&gt;
&#30475;&#12289;&#36752;&#23556;&#21644;&#23398;&#20064;&#65306;&#22522;&#20110;&#26080;&#30417;&#30563;&#30340;&#26080;&#32447;&#30005;-&#35270;&#35273;&#23545;&#24212;&#30340;&#33258;&#21161;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual Correspondence. (arXiv:2206.06424v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#30340;&#26080;&#32447;&#30005;-&#35270;&#35273;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;(MaxRay)&#65292;&#36890;&#36807;&#20174;&#26080;&#32447;&#30005;-&#35270;&#35273;&#23545;&#24212;&#20013;&#25552;&#21462;&#33258;&#22352;&#26631;&#26469;&#33258;&#20027;&#23398;&#20064;&#30446;&#26631;&#22312;&#26080;&#32447;&#30005;&#20013;&#30340;&#23450;&#20301;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26080;&#38656;&#26631;&#31614;&#30340;&#20934;&#30830;&#26080;&#32447;&#30005;&#30446;&#26631;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20195;&#31227;&#21160;&#36890;&#20449;&#32593;&#32476;&#23558;&#22312;&#24815;&#24120;&#36890;&#20449;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#23556;&#39057;&#24863;&#30693;&#21151;&#33021;&#65292;&#20174;&#32780;&#20351;&#23460;&#22806;&#30340;&#24863;&#30693;&#33539;&#22260;&#31354;&#21069;&#12290;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#20294;&#23545;&#20110;&#26080;&#32447;&#30005;&#24863;&#30693;&#20219;&#21153;&#30340;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#20197;&#30740;&#31350;&#26080;&#32447;&#30005;&#24863;&#30693;&#30340;&#24615;&#33021;&#21644;&#21069;&#26223;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MaxRay&#65306;&#19968;&#20010;&#21512;&#25104;&#30340;&#26080;&#32447;&#30005;-&#35270;&#35273;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#26377;&#21161;&#20110;&#31934;&#30830;&#30340;&#26080;&#32447;&#30005;&#30446;&#26631;&#23450;&#20301;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#36890;&#36807;&#20174;&#26080;&#32447;&#30005;-&#35270;&#35273;&#23545;&#24212;&#20013;&#25552;&#21462;&#33258;&#22352;&#26631;&#26469;&#23398;&#20064;&#26080;&#32447;&#30005;&#30446;&#26631;&#30340;&#26080;&#30417;&#30563;&#23450;&#20301;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#33258;&#21161;&#23450;&#20301;&#30340;&#22352;&#26631;&#26469;&#35757;&#32451;&#19968;&#20010;&#26080;&#32447;&#30005;&#23450;&#20301;&#22120;&#32593;&#32476;&#65292;&#24182;&#19982;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#37197;&#23545;&#30340;&#26080;&#32447;&#30005;-&#35270;&#35273;&#25968;&#25454;&#65292;&#21487;&#20197;&#26080;&#38656;&#26631;&#31614;&#26469;&#33258;&#21160;&#23398;&#20064;&#20934;&#30830;&#30340;&#26080;&#32447;&#30005;&#30446;&#26631;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next generation cellular networks will implement radio sensing functions alongside customary communications, thereby enabling unprecedented worldwide sensing coverage outdoors. Deep learning has revolutionised computer vision but has had limited application to radio perception tasks, in part due to lack of systematic datasets and benchmarks dedicated to the study of the performance and promise of radio sensing. To address this gap, we present MaxRay: a synthetic radio-visual dataset and benchmark that facilitate precise target localisation in radio. We further propose to learn to localise targets in radio without supervision by extracting self-coordinates from radio-visual correspondence. We use such self-supervised coordinates to train a radio localiser network. We characterise our performance against a number of state-of-the-art baselines. Our results indicate that accurate radio target localisation can be automatically learned from paired radio-visual data without labels, which is i
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26032;&#39062;&#20272;&#35745;&#37327;&#21644;&#33258;&#26631;&#20934;&#21270;&#30028;&#38480;&#30340;&#32447;&#24615;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#65292;&#20855;&#26377;$O(\sqrt{dT\log T})$&#36951;&#25022;&#30028;&#65292;&#32500;&#24230;&#30456;&#20851;&#30340;&#21152;&#27861;&#39033;&#20998;&#35299;&#36951;&#25022;&#32047;&#31215;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.05404</link><description>&lt;p&gt;
Squeeze All&#65306;&#32447;&#24615;&#19978;&#19979;&#25991;Bandit&#30340;&#26032;&#20272;&#35745;&#22120;&#21644;&#33258;&#26631;&#20934;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Squeeze All: Novel Estimator and Self-Normalized Bound for Linear Contextual Bandits. (arXiv:2206.05404v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05404
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26032;&#39062;&#20272;&#35745;&#37327;&#21644;&#33258;&#26631;&#20934;&#21270;&#30028;&#38480;&#30340;&#32447;&#24615;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#65292;&#20855;&#26377;$O(\sqrt{dT\log T})$&#36951;&#25022;&#30028;&#65292;&#32500;&#24230;&#30456;&#20851;&#30340;&#21152;&#27861;&#39033;&#20998;&#35299;&#36951;&#25022;&#32047;&#31215;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;$O(\sqrt{dT\log T})$&#36951;&#25022;&#30028;&#30340;&#32447;&#24615;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#65292;&#20854;&#20013;$d$&#26159;&#19978;&#19979;&#25991;&#30340;&#32500;&#25968;&#65292;$T$&#26159;&#26102;&#38388;&#36328;&#24230;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#19968;&#31181;&#26032;&#39062;&#30340;&#20272;&#35745;&#37327;&#65292;&#20854;&#20013;&#36890;&#36807;&#26174;&#24335;&#38543;&#26426;&#21270;&#23884;&#20837;&#20102;&#25506;&#32034;&#12290;&#26681;&#25454;&#38543;&#26426;&#24615;&#65292;&#25105;&#20204;&#30340;&#24314;&#35758;&#20272;&#35745;&#22120;&#21487;&#20197;&#20174;&#25152;&#26377;&#27494;&#22120;&#30340;&#19978;&#19979;&#25991;&#25110;&#20174;&#36873;&#23450;&#30340;&#19978;&#19979;&#25991;&#20013;&#33719;&#24471;&#36129;&#29486;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#24314;&#31435;&#20102;&#19968;&#20010;&#33258;&#26631;&#20934;&#21270;&#30028;&#38480;&#65292;&#23427;&#20801;&#35768;&#23558;&#32047;&#31215;&#36951;&#25022;&#20998;&#35299;&#20026;&#22522;&#20110;&#32500;&#24230;&#30340;\textit{&#21487;&#21152;}&#39033;&#65292;&#32780;&#19981;&#26159;&#20056;&#27861;&#39033;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#25105;&#20204;&#30340;&#38382;&#39064;&#35774;&#32622;&#19979;&#30340;$\Omega(\sqrt{dT})$&#30340;&#26032;&#30340;&#19979;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#36951;&#25022;&#21305;&#37197;&#19979;&#38480;&#65292;&#30452;&#21040;&#23545;&#25968;&#22240;&#23376;&#12290;&#25968;&#20540;&#23454;&#39564;&#25903;&#25345;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#32447;&#24615;Bandit&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a linear contextual bandit algorithm with $O(\sqrt{dT\log T})$ regret bound, where $d$ is the dimension of contexts and $T$ isthe time horizon. Our proposed algorithm is equipped with a novel estimator in which exploration is embedded through explicit randomization. Depending on the randomization, our proposed estimator takes contributions either from contexts of all arms or from selected contexts. We establish a self-normalized bound for our estimator, which allows a novel decomposition of the cumulative regret into \textit{additive} dimension-dependent terms instead of multiplicative terms. We also prove a novel lower bound of $\Omega(\sqrt{dT})$ under our problem setting. Hence, the regret of our proposed algorithm matches the lower bound up to logarithmic factors. The numerical experiments support the theoretical guarantees and show that our proposed method outperforms the existing linear bandit algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25552;&#20986;&#21807;&#19968;&#24179;&#34913;&#28857;&#22987;&#32456;&#23384;&#22312;&#19988;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#29575;&#20026;&#32447;&#24615;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#21487;&#36890;&#36807;&#36731;&#24494;&#36807;&#21442;&#25968;&#21270;&#24471;&#21040;&#28385;&#36275;&#12290;</title><link>http://arxiv.org/abs/2205.13814</link><description>&lt;p&gt;
&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#20840;&#23616;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Global Convergence of Over-parameterized Deep Equilibrium Models. (arXiv:2205.13814v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25552;&#20986;&#21807;&#19968;&#24179;&#34913;&#28857;&#22987;&#32456;&#23384;&#22312;&#19988;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#29575;&#20026;&#32447;&#24615;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#21487;&#36890;&#36807;&#36731;&#24494;&#36807;&#21442;&#25968;&#21270;&#24471;&#21040;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#36890;&#36807;&#26080;&#38480;&#28145;&#24230;&#30340;&#21152;&#26435;-&#32465;&#23450;&#27169;&#22411;&#20013;&#30340;&#24179;&#34913;&#28857;&#19982;&#36755;&#20837;&#27880;&#20837;&#26469;&#38544;&#24335;&#23450;&#20041;&#12290;&#23427;&#36890;&#36807;&#26681;&#26597;&#25214;&#30452;&#25509;&#27714;&#35299;&#24179;&#34913;&#28857;&#65292;&#24182;&#36890;&#36807;&#38544;&#24335;&#24494;&#20998;&#35745;&#31639;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;&#26080;&#38480;&#36816;&#31639;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36807;&#21442;&#25968;&#21270;DEQ&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#36890;&#36807;&#23545;&#21021;&#22987;&#24179;&#34913;&#28857;&#26045;&#21152;&#19968;&#23450;&#26465;&#20214;&#65292;&#25105;&#20204;&#34920;&#26126;&#21807;&#19968;&#24179;&#34913;&#28857;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22987;&#32456;&#23384;&#22312;&#65292;&#24182;&#19988;&#38024;&#23545;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#65292;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#29575;&#35777;&#26126;&#20026;&#32447;&#24615;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#23637;&#31034;&#25152;&#38656;&#36215;&#22987;&#26465;&#20214;&#36890;&#36807;&#36731;&#24494;&#36807;&#21442;&#25968;&#21270;&#24471;&#21040;&#28385;&#36275;&#65292;&#25105;&#20204;&#22312;&#38543;&#26426;DEQ&#19978;&#36827;&#34892;&#20102;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#20197;&#20811;&#26381;&#38750;&#28176;&#36817;&#20998;&#26512;&#26080;&#38480;&#28145;&#24230;&#21152;&#26435;&#32465;&#23450;&#27169;&#22411;&#30340;&#25216;&#26415;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
A deep equilibrium model (DEQ) is implicitly defined through an equilibrium point of an infinite-depth weight-tied model with an input-injection. Instead of infinite computations, it solves an equilibrium point directly with root-finding and computes gradients with implicit differentiation. The training dynamics of over-parameterized DEQs are investigated in this study. By supposing a condition on the initial equilibrium point, we show that the unique equilibrium point always exists during the training process, and the gradient descent is proved to converge to a globally optimal solution at a linear convergence rate for the quadratic loss function. In order to show that the required initial condition is satisfied via mild over-parameterization, we perform a fine-grained analysis on random DEQs. We propose a novel probabilistic framework to overcome the technical difficulty in the non-asymptotic analysis of infinite-depth weight-tied models.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#33258;&#21160;&#25512;&#26029;&#23398;&#29983;&#35823;&#35299;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;StudentSyn&#65292;&#36890;&#36807;&#35266;&#23519;&#23398;&#29983;&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#21512;&#25104;&#20182;&#20204;&#23545;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#23398;&#20064;&#23581;&#35797;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32534;&#31243;&#23548;&#24072;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.01265</link><description>&lt;p&gt;
&#20174;&#8220;&#35299;&#20915;&#26041;&#26696;&#21512;&#25104;&#8221;&#21040;&#8220;&#23398;&#29983;&#23581;&#35797;&#21512;&#25104;&#8221;&#65306;&#38754;&#21521;&#22522;&#20110;&#22359;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based Visual Programming Tasks. (arXiv:2205.01265v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01265
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#33258;&#21160;&#25512;&#26029;&#23398;&#29983;&#35823;&#35299;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;StudentSyn&#65292;&#36890;&#36807;&#35266;&#23519;&#23398;&#29983;&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#21512;&#25104;&#20182;&#20204;&#23545;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#23398;&#20064;&#23581;&#35797;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32534;&#31243;&#23548;&#24072;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22359;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#29615;&#22659;&#36234;&#26469;&#36234;&#34987;&#29992;&#26469;&#20171;&#32461;&#35745;&#31639;&#27010;&#24565;&#32473;&#21021;&#23398;&#32773;&#12290;&#37492;&#20110;&#32534;&#31243;&#20219;&#21153;&#26159;&#24320;&#25918;&#24335;&#21644;&#27010;&#24565;&#24615;&#30340;&#65292;&#21021;&#23398;&#32773;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#23398;&#20064;&#26102;&#24120;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32534;&#31243;&#23548;&#24072;&#26377;&#30528;&#24110;&#21161;&#25379;&#25166;&#30340;&#23398;&#29983;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#19988;&#38656;&#35201;&#20960;&#20010;&#32452;&#25104;&#37096;&#20998;&#26469;&#23454;&#29616;&#36825;&#19968;&#28508;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#29983;&#24314;&#27169;&#36825;&#19968;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#29305;&#21035;&#26159;&#33258;&#21160;&#25512;&#26029;&#23398;&#29983;&#35823;&#35299;&#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#39044;&#27979;&#65288;&#21512;&#25104;&#65289;&#20182;&#20204;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;StudentSyn&#65292;&#22260;&#32469;&#20197;&#19979;&#25361;&#25112;&#65306;&#20026;&#19968;&#20010;&#32473;&#23450;&#30340;&#23398;&#29983;&#65292;&#35266;&#23519;&#20182;&#20204;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#24341;&#29992;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#21518;&#65292;&#21512;&#25104;&#20182;&#20204;&#23545;&#26032;&#30446;&#26631;&#20219;&#21153;&#30340;&#23581;&#35797;&#12290;&#36825;&#20010;&#25361;&#25112;&#31867;&#20284;&#20110;&#31243;&#24207;&#21512;&#25104;&#65307;&#20294;&#26159;&#65292;&#36825;&#37324;&#30340;&#30446;&#26631;&#19981;&#26159;&#21512;&#25104;&#19968;&#20010;&#8220;&#35299;&#20915;&#26041;&#26696;&#8221;&#65288;&#21363;&#19987;&#23478;&#32534;&#20889;&#30340;&#31243;&#24207;&#65289;&#65292;&#32780;&#26159;&#21512;&#25104;&#19968;&#20010;&#8220;&#23398;&#29983;&#23581;&#35797;&#8221;&#65288;&#21363;&#19968;&#20010;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#35760;&#24405;&#30340;&#31243;&#24207;&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;StudentSyn&#30340;&#35814;&#32454;&#35828;&#26126;&#65292;&#24182;&#25551;&#36848;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#20960;&#20010;&#28508;&#22312;&#29992;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#23545;&#21021;&#23398;&#32773;&#21644;&#26377;&#32463;&#39564;&#30340;&#21442;&#19982;&#32773;&#30340;&#23454;&#39564;&#20998;&#26512;&#20102;&#25361;&#25112;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Block-based visual programming environments are increasingly used to introduce computing concepts to beginners. Given that programming tasks are open-ended and conceptual, novice students often struggle when learning in these environments. AI-driven programming tutors hold great promise in automatically assisting struggling students, and need several components to realize this potential. We investigate the crucial component of student modeling, in particular, the ability to automatically infer students' misconceptions for predicting (synthesizing) their behavior. We introduce a novel benchmark, StudentSyn, centered around the following challenge: For a given student, synthesize the student's attempt on a new target task after observing the student's attempt on a fixed reference task. This challenge is akin to that of program synthesis; however, instead of synthesizing a {solution} (i.e., program an expert would write), the goal here is to synthesize a {student attempt} (i.e., program t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;IOP-FL&#65292;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#20869;&#22806;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#21333;&#20010;&#23458;&#25143;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.08467</link><description>&lt;p&gt;
IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation&#65288;&#32852;&#37030;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#20869;&#22806;&#20010;&#24615;&#21270;&#26041;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation. (arXiv:2204.08467v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08467
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;IOP-FL&#65292;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#20869;&#22806;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#21333;&#20010;&#23458;&#25143;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#21307;&#30103;&#26426;&#26500;&#22312;&#26410;&#38598;&#20013;&#23458;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#30001;&#20110;&#26469;&#33258;&#21508;&#31181;&#25195;&#25551;&#20202;&#21644;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#30340;&#21307;&#23398;&#22270;&#20687;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#31181;&#20840;&#23616;&#27169;&#22411;&#26222;&#36941;&#23454;&#29616;&#27599;&#20010;&#23458;&#25143;&#30340;&#26368;&#20339;&#24615;&#33021;&#26159;&#22256;&#38590;&#30340;&#65292;&#22914;&#26524;&#21487;&#33021;&#30340;&#35805;&#12290;&#24403;&#23558;&#20840;&#23616;&#27169;&#22411;&#37096;&#32626;&#21040;&#22312;FL&#26399;&#38388;&#26410;&#21576;&#29616;&#30340;&#20998;&#24067;&#19978;&#30475;&#19981;&#35265;&#30340;&#23458;&#25143;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#20248;&#21270;&#27599;&#20010;&#23458;&#25143;&#22312;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;FL&#20013;&#30340;&#8220;&#20869;&#37096;&#21644;&#22806;&#37096;&#27169;&#22411;&#20010;&#24615;&#21270;&#8221;&#65288;IOP-FL&#65289;&#12290;&#25105;&#20204;&#30340;&#20869;&#37096;&#20010;&#24615;&#21270;&#20351;&#29992;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32047;&#31215;&#36890;&#29992;&#30693;&#35782;&#30340;&#20840;&#23616;&#26799;&#24230;&#21644;&#23458;&#25143;&#29305;&#23450;&#20248;&#21270;&#30340;&#26412;&#22320;&#26799;&#24230;&#65292;&#21033;&#29992;&#27599;&#20010;&#23458;&#25143;&#30340;&#26412;&#22320;&#36866;&#24212;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22806;&#37096;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#19968;&#20010;&#23458;&#25143;&#33258;&#36866;&#24212;&#26426;&#21046;&#26469;&#24212;&#23545;&#26469;&#33258;&#26410;&#30693;&#23458;&#25143;&#30340;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#21333;&#20010;&#23458;&#25143;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21516;&#26102;&#20445;&#25345;&#22312;&#19981;&#21516;&#23458;&#25143;&#20043;&#38388;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) allows multiple medical institutions to collaboratively learn a global model without centralizing client data. It is difficult, if possible at all, for such a global model to commonly achieve optimal performance for each individual client, due to the heterogeneity of medical images from various scanners and patient demographics. This problem becomes even more significant when deploying the global model to unseen clients outside the FL with unseen distributions not presented during federated training. To optimize the prediction accuracy of each individual client for medical imaging tasks, we propose a novel unified framework for both \textit{Inside and Outside model Personalization in FL} (IOP-FL). Our inside personalization uses a lightweight gradient-based approach that exploits the local adapted model for each client, by accumulating both the global gradients for common knowledge and the local gradients for client-specific optimization. Moreover, and important
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;&#37325;&#26500;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#22806;&#26679;&#26412;&#26816;&#27979;&#65292;&#22312;&#26368;&#22823;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#21644;&#20445;&#35777;&#37325;&#26500;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#37325;&#26500;&#12289;&#25968;&#25454;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#26631;&#20934;&#21270;L2&#36317;&#31163;&#31561;&#31574;&#30053;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#35760;&#22806;&#26679;&#26412;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2203.02194</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#37325;&#26500;&#33258;&#32534;&#30721;&#22120;&#30340;&#22806;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection. (arXiv:2203.02194v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;&#37325;&#26500;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#22806;&#26679;&#26412;&#26816;&#27979;&#65292;&#22312;&#26368;&#22823;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#21644;&#20445;&#35777;&#37325;&#26500;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#37325;&#26500;&#12289;&#25968;&#25454;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#26631;&#20934;&#21270;L2&#36317;&#31163;&#31561;&#31574;&#30053;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#35760;&#22806;&#26679;&#26412;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20998;&#31867;&#22120;&#38656;&#35201;&#26816;&#27979;&#36828;&#31163;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#22806;&#26679;&#26412;&#12290;&#37325;&#26500;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#21033;&#29992;&#36755;&#20837;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#26032;&#39062;&#24615;&#19982;&#27491;&#24120;&#24615;&#30340;&#24230;&#37327;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#30340;&#26412;&#36136;&#34920;&#36848;&#20026;&#20855;&#26377;&#23545;&#26465;&#20214;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#30340;&#20195;&#29702;&#26597;&#35810;&#30340;&#22235;&#20803;&#32452;&#22495;&#36716;&#25442;&#65292;&#20854;&#26377;&#20869;&#22312;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#25913;&#36827;&#26041;&#21521;&#34987;&#24418;&#24335;&#21270;&#20026;&#26368;&#22823;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#65292;&#21516;&#26102;&#30830;&#20445;&#20854;&#37325;&#26500;&#33021;&#21147;&#65292;&#20197;&#20805;&#24403;&#25152;&#25551;&#36848;&#30340;&#22495;&#36716;&#25442;&#22120;&#12290;&#20174;&#20013;&#65292;&#24341;&#20837;&#20102;&#31574;&#30053;&#65292;&#21253;&#25324;&#35821;&#20041;&#37325;&#26500;&#12289;&#25968;&#25454;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#26631;&#20934;&#21270;L2&#36317;&#31163;&#65292;&#20197;&#23454;&#36136;&#24615;&#25913;&#21892;&#21407;&#22987;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20849;&#21516;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#65292;&#22312;Wide-ResNet&#19978;&#65292;CIFAR-100&#19982;TinyImagenet-crop&#30340;FPR@95%TPR&#20026;0.2%&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#26631;&#35760;&#22806;&#26679;&#26412;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In some scenarios, classifier requires detecting out-of-distribution samples far from its training data. With desirable characteristics, reconstruction autoencoder-based methods deal with this problem by using input reconstruction error as a metric of novelty vs. normality. We formulate the essence of such approach as a quadruplet domain translation with an intrinsic bias to only query for a proxy of conditional data uncertainty. Accordingly, an improvement direction is formalized as maximumly compressing the autoencoder's latent space while ensuring its reconstructive power for acting as a described domain translator. From it, strategies are introduced including semantic reconstruction, data certainty decomposition and normalized L2 distance to substantially improve original methods, which together establish state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method works without any addit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#31181;&#8212;&#8212;&#20108;&#27425;&#26799;&#24230;&#65292;&#29992;&#20110;&#22312;&#21516;&#24577;&#21152;&#23494;&#39046;&#22495;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#24182;&#25104;&#21151;&#25552;&#21319;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#20165;&#38656;&#36739;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2201.10838</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#21450;&#20854;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#31181;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant. (arXiv:2201.10838v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.10838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#31181;&#8212;&#8212;&#20108;&#27425;&#26799;&#24230;&#65292;&#29992;&#20110;&#22312;&#21516;&#24577;&#21152;&#23494;&#39046;&#22495;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#24182;&#25104;&#21151;&#25552;&#21319;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#20165;&#38656;&#36739;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#21152;&#23494;&#25968;&#25454;&#19978;&#30340;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#19968;&#30452;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#31181;&#8212;&#8212;&#20108;&#27425;&#26799;&#24230;&#65292;&#29992;&#20110;&#22312;&#21516;&#24577;&#21152;&#23494;&#39046;&#22495;&#23454;&#29616;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#20854;&#26680;&#24515;&#21487;&#20197;&#30475;&#20316;&#26159;&#31616;&#21270;&#30340;&#22266;&#23450;Hessian&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#20998;&#21035;&#21521;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#65288;NAG&#65289;&#21644;&#33258;&#36866;&#24212;&#26799;&#24230;&#31639;&#27861;&#65288;Adagrad&#65289;&#22686;&#24378;&#20102;&#35813;&#26799;&#24230;&#21464;&#31181;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22686;&#24378;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#24378;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#27604;&#26420;&#32032;&#30340;&#19968;&#38454;&#26799;&#24230;&#26041;&#27861;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22686;&#24378;&#30340;NAG&#26041;&#27861;&#26469;&#23454;&#29616;&#21516;&#24577;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#24182;&#22312;&#20165;3&#27425;&#36845;&#20195;&#20013;&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logistic regression training over encrypted data has been an attractive idea to security concerns for years. In this paper, we propose a faster gradient variant called $\texttt{quadratic gradient}$ to implement logistic regression training in a homomorphic encryption domain, the core of which can be seen as an extension of the simplified fixed Hessian.  We enhance Nesterov's accelerated gradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with this gradient variant and evaluate the enhanced algorithms on several datasets. Experimental results show that the enhanced methods have a state-of-the-art performance in convergence speed compared to the naive first-order gradient methods. We then adopt the enhanced NAG method to implement homomorphic logistic regression training and obtain a comparable result by only $3$ iterations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#22312; COVID-19 &#20013;&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#26089;&#26399;&#21644;&#24265;&#20215;&#22320;&#35786;&#26029;&#35813;&#30149;&#65292;&#26377;&#21161;&#20110;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#23545;&#25239;&#35813;&#30149;&#12290;</title><link>http://arxiv.org/abs/2111.09537</link><description>&lt;p&gt;
&#26032;&#20896;&#32954;&#28814;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Prominence of Artificial Intelligence in COVID-19. (arXiv:2111.09537v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#22312; COVID-19 &#20013;&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#26089;&#26399;&#21644;&#24265;&#20215;&#22320;&#35786;&#26029;&#35813;&#30149;&#65292;&#26377;&#21161;&#20110;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#23545;&#25239;&#35813;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2019&#24180;12&#26376;&#65292;&#19968;&#31181;&#26032;&#22411;&#30149;&#27602;&#65292;COVID-19&#24050;&#32463;&#23548;&#33268;&#20102;&#26497;&#22810;&#30340;&#27515;&#20129;&#12290;&#19982;&#35199;&#29677;&#29273;&#27969;&#24863;1918&#24180;&#30456;&#27604;&#65292;&#19982;&#36825;&#31181;&#26032;&#20896;&#30149;&#27602;&#30340;&#26007;&#20105;&#20196;&#20154;&#22256;&#24785;&#21644;&#24656;&#24807;&#12290;&#32780;&#22312;&#21069;&#32447;&#21307;&#29983;&#21644;&#21307;&#23398;&#30740;&#31350;&#20154;&#21592;&#22312;&#25511;&#21046;&#36825;&#31181;&#39640;&#24230;&#20256;&#26579;&#30149;&#27602;&#30340;&#20256;&#25773;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#30340;&#21516;&#26102;&#65292;&#25216;&#26415;&#22312;&#36825;&#22330;&#25112;&#26007;&#20013;&#20063;&#35777;&#26126;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#21307;&#30103;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#37319;&#29992;&#65292;&#21487;&#20197;&#35786;&#26029;&#35768;&#22810;&#30142;&#30149;&#65292;&#29978;&#33267;&#20351;&#32463;&#39564;&#20016;&#23500;&#30340;&#21307;&#29983;&#24863;&#21040;&#22256;&#24785;&#12290;&#22240;&#27492;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#25506;&#35752;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#26089;&#26399;&#21644;&#24265;&#20215;&#22320;&#35786;&#26029;&#35813;&#30149;&#12290;&#22823;&#22810;&#25968;&#21457;&#23637;&#20013;&#22269;&#23478;&#38590;&#20197;&#20351;&#29992;&#20256;&#32479;&#26041;&#24335;&#36827;&#34892;&#27979;&#35797;&#65292;&#20294;&#21487;&#20197;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33719;&#24471;&#19981;&#21516;&#31867;&#22411;&#30340;&#21307;&#23398;&#22270;&#20687;&#20063;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#21160;&#21147;&#12290;&#22240;&#27492;&#65292;&#22823;&#37327;&#30340;&#25216;&#26415;&#21019;&#26032;&#34987;&#24341;&#20837;&#24182;&#24212;&#29992;&#20110;&#23454;&#36341;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In December 2019, a novel virus called COVID-19 had caused an enormous number of causalities to date. The battle with the novel Coronavirus is baffling and horrifying after the Spanish Flu 2019. While the front-line doctors and medical researchers have made significant progress in controlling the spread of the highly contiguous virus, technology has also proved its significance in the battle. Moreover, Artificial Intelligence has been adopted in many medical applications to diagnose many diseases, even baffling experienced doctors. Therefore, this survey paper explores the methodologies proposed that can aid doctors and researchers in early and inexpensive methods of diagnosis of the disease. Most developing countries have difficulties carrying out tests using the conventional manner, but a significant way can be adopted with Machine and Deep Learning. On the other hand, the access to different types of medical images has motivated the researchers. As a result, a mammoth number of tech
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#30340;&#35745;&#31639;&#21644;&#33021;&#37327;&#28040;&#32791;&#36235;&#21183;&#65292;&#37325;&#28857;&#20851;&#27880;&#25512;&#29702;&#25104;&#26412;&#32780;&#38750;&#35757;&#32451;&#25104;&#26412;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38500;&#20102;&#31639;&#27861;&#21019;&#26032;&#22806;&#65292;&#26356;&#20855;&#20307;&#21644;&#24378;&#22823;&#30340;&#30828;&#20214;&#36890;&#24120;&#20276;&#38543;&#30528;&#37325;&#35201;&#30340;&#33021;&#37327;&#25928;&#29575;&#20248;&#21270;&#65292;&#23548;&#33268;&#25512;&#29702;&#25104;&#26412;&#30340;&#25552;&#39640;&#21576;&#29616;&#26580;&#21644;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2109.05472</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#30340;&#35745;&#31639;&#21644;&#33021;&#37327;&#28040;&#32791;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Compute and Energy Consumption Trends in Deep Learning Inference. (arXiv:2109.05472v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.05472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#30340;&#35745;&#31639;&#21644;&#33021;&#37327;&#28040;&#32791;&#36235;&#21183;&#65292;&#37325;&#28857;&#20851;&#27880;&#25512;&#29702;&#25104;&#26412;&#32780;&#38750;&#35757;&#32451;&#25104;&#26412;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38500;&#20102;&#31639;&#27861;&#21019;&#26032;&#22806;&#65292;&#26356;&#20855;&#20307;&#21644;&#24378;&#22823;&#30340;&#30828;&#20214;&#36890;&#24120;&#20276;&#38543;&#30528;&#37325;&#35201;&#30340;&#33021;&#37327;&#25928;&#29575;&#20248;&#21270;&#65292;&#23548;&#33268;&#25512;&#29702;&#25104;&#26412;&#30340;&#25552;&#39640;&#21576;&#29616;&#26580;&#21644;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#20154;&#35748;&#20026;&#65292;&#28145;&#24230;&#23398;&#20064;&#31561;AI&#33539;&#24335;&#30340;&#36827;&#23637;&#19982;&#21442;&#25968;&#25968;&#37327;&#25351;&#25968;&#22686;&#38271;&#26377;&#20851;&#12290;&#26377;&#35768;&#22810;&#30740;&#31350;&#35777;&#23454;&#36825;&#20123;&#36235;&#21183;&#65292;&#20294;&#36825;&#26159;&#21542;&#24847;&#21619;&#30528;&#33021;&#37327;&#28040;&#32791;&#21576;&#25351;&#25968;&#22686;&#38271;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#25512;&#29702;&#25104;&#26412;&#19978;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#25104;&#26412;&#65292;&#22240;&#20026;&#21069;&#32773;&#21344;&#25454;&#20102;&#22823;&#37096;&#20998;&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20165;&#22240;&#20026;&#26377;&#20056;&#27861;&#22240;&#32032;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#38500;&#20102;&#31639;&#27861;&#21019;&#26032;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26356;&#20855;&#20307;&#21644;&#24378;&#22823;&#30340;&#30828;&#20214;&#65288;&#23548;&#33268;&#26356;&#39640;&#30340;FLOPS&#65289;&#65292;&#36825;&#36890;&#24120;&#20276;&#38543;&#30528;&#37325;&#35201;&#30340;&#33021;&#37327;&#25928;&#29575;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#23558;&#28966;&#28857;&#20174;&#31361;&#30772;&#24615;&#35770;&#25991;&#30340;&#31532;&#19968;&#27425;&#23454;&#29616;&#36716;&#31227;&#21040;&#20102;&#19968;&#20004;&#24180;&#21518;&#30340;&#25216;&#26415;&#29256;&#26412;&#30340;&#24041;&#22266;&#29256;&#26412;&#12290;&#22312;&#36825;&#20010;&#29420;&#29305;&#21644;&#20840;&#38754;&#30340;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30456;&#20851;&#27169;&#22411;&#65306;&#23545;&#20110;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#30475;&#21040;&#19968;&#20010;&#26356;&#26580;&#21644;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we study relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#36890;&#36807;&#31616;&#35201;&#25945;&#31243;&#21644;&#35299;&#37322;&#24615;&#21453;&#39304;&#20174;&#23569;&#37327;&#30340;&#35757;&#32451;&#31034;&#20363;&#20013;&#23398;&#20064;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#33021;&#25104;&#21151;&#22320;&#23558;&#20854;&#25512;&#24191;&#21040;&#35757;&#32451;&#33539;&#22260;&#20043;&#22806;&#30340;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23454;&#29616;&#20154;&#31867;&#23398;&#20064;&#26426;&#21046;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2107.06994</link><description>&lt;p&gt;
&#31995;&#32479;&#21270;&#30340;&#20154;&#31867;&#23398;&#20064;&#19982;&#25512;&#24191;&#65306;&#36890;&#36807;&#31616;&#35201;&#25945;&#31243;&#21644;&#35299;&#37322;&#24615;&#21453;&#39304;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Systematic human learning and generalization from a brief tutorial with explanatory feedback. (arXiv:2107.06994v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.06994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#36890;&#36807;&#31616;&#35201;&#25945;&#31243;&#21644;&#35299;&#37322;&#24615;&#21453;&#39304;&#20174;&#23569;&#37327;&#30340;&#35757;&#32451;&#31034;&#20363;&#20013;&#23398;&#20064;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#33021;&#25104;&#21151;&#22320;&#23558;&#20854;&#25512;&#24191;&#21040;&#35757;&#32451;&#33539;&#22260;&#20043;&#22806;&#30340;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23454;&#29616;&#20154;&#31867;&#23398;&#20064;&#26426;&#21046;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#38271;&#26399;&#20197;&#26469;&#34987;&#29992;&#26469;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#65292;&#25429;&#25417;&#34892;&#20026;&#21644;&#35748;&#30693;&#30340;&#20803;&#32032;&#21450;&#20854;&#31070;&#32463;&#22522;&#30784;&#12290; &#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#35768;&#22810;&#26041;&#38754;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#30340;&#26234;&#33021;&#27700;&#24179;&#65292;&#28982;&#32780;&#19982;&#20154;&#31867;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#20204;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20154;&#20204;&#19981;&#20165;&#21487;&#20197;&#22312;&#29087;&#24713;&#30340;&#39046;&#22495;&#36827;&#34892;&#25512;&#29702;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#24555;&#36895;&#23398;&#20064;&#25512;&#29702;&#26032;&#38382;&#39064;&#21644;&#24773;&#20917;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#26234;&#33021;&#20197;&#21450;&#23427;&#20204;&#30340;&#21738;&#20123;&#26041;&#38754;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#25104;&#24180;&#20154;&#20174;&#19968;&#20010;&#22522;&#20110;&#25968;&#29420;&#30340;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#30340;&#31616;&#35201;&#25945;&#23398;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12289;&#36890;&#36807;&#35299;&#37322;&#24615;&#21453;&#39304;&#32416;&#27491;&#38169;&#35823;&#31572;&#26696;&#21644;&#29421;&#31364;&#33539;&#22260;&#30340;&#35757;&#32451;&#31034;&#20363;&#65292;&#26469;&#25506;&#35752;&#36825;&#20010;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25484;&#25569;&#35813;&#20219;&#21153;&#30340;&#21442;&#19982;&#32773;&#21487;&#20197;&#22312;&#23569;&#25968;&#20960;&#27425;&#35797;&#39564;&#20013;&#23454;&#29616;&#65292;&#24182;&#21487;&#20197;&#25512;&#24191;&#21040;&#35757;&#32451;&#33539;&#22260;&#22806;&#30340;&#35868;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#20154;&#31867;&#23398;&#20064;&#26426;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have long been used to model human intelligence, capturing elements of behavior and cognition, and their neural basis. Recent advancements in deep learning have enabled neural network models to reach and even surpass human levels of intelligence in many respects, yet unlike humans, their ability to learn new tasks quickly remains a challenge. People can reason not only in familiar domains, but can also rapidly learn to reason through novel problems and situations, raising the question of how well modern neural network models capture human intelligence and in which ways they diverge. In this work, we explore this gap by investigating human adults' ability to learn an abstract reasoning task based on Sudoku from a brief instructional tutorial with explanatory feedback for incorrect responses using a narrow range of training examples. We find that participants who master the task do so within a small number of trials and generalize well to puzzles outside of the training r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#27969;&#24418;&#20013;&#23398;&#20064;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;&#65292;&#32780;&#26080;&#38656;&#22312;&#35757;&#32451;&#26399;&#38388;&#26631;&#35760;&#21464;&#25442;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#26412;&#22320;&#33258;&#36866;&#24212;&#30340;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#20998;&#31867;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.12096</link><description>&lt;p&gt;
&#23398;&#20064;&#25968;&#25454;&#27969;&#24418;&#19978;&#30340;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Learning Identity-Preserving Transformations on Data Manifolds. (arXiv:2106.12096v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.12096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#27969;&#24418;&#20013;&#23398;&#20064;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;&#65292;&#32780;&#26080;&#38656;&#22312;&#35757;&#32451;&#26399;&#38388;&#26631;&#35760;&#21464;&#25442;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#26412;&#22320;&#33258;&#36866;&#24212;&#30340;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#20998;&#31867;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23558;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;&#32435;&#20837;&#20854;&#27169;&#22411;&#20013;&#65292;&#20197;&#23558;&#20854;&#24615;&#33021;&#25512;&#24191;&#21040;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21464;&#25442;&#36890;&#24120;&#26159;&#20174;&#19968;&#32452;&#24050;&#30693;&#21487;&#20197;&#32500;&#25345;&#36755;&#20837;&#36523;&#20221;&#30340;&#20989;&#25968;&#20013;&#36873;&#25321;&#30340;&#65288;&#20363;&#22914;&#26059;&#36716;&#12289;&#24179;&#31227;&#12289;&#32763;&#36716;&#21644;&#32553;&#25918;&#65289;&#12290;&#28982;&#32780;&#65292;&#26377;&#35768;&#22810;&#33258;&#28982;&#21464;&#21270;&#26080;&#27861;&#36827;&#34892;&#26631;&#35760;&#30417;&#30563;&#25110;&#36890;&#36807;&#25968;&#25454;&#26816;&#26597;&#26469;&#23450;&#20041;&#12290;&#22914;&#28024;&#20837;&#24335;&#23398;&#20064;&#20551;&#35774;&#25152;&#31034;&#65292;&#35768;&#22810;&#36825;&#20123;&#33258;&#28982;&#21464;&#21270;&#23384;&#22312;&#20110;&#25110;&#38752;&#36817;&#20302;&#32500;&#38750;&#32447;&#24615;&#27969;&#24418;&#19978;&#12290;&#20960;&#31181;&#25216;&#26415;&#36890;&#36807;&#19968;&#32452;&#23398;&#20064;&#30340;&#26446;&#32676;&#31639;&#23376;&#26469;&#34920;&#31034;&#27969;&#24418;&#21464;&#21270;&#65292;&#36825;&#20123;&#31639;&#23376;&#23450;&#20041;&#20102;&#27969;&#24418;&#19978;&#30340;&#36816;&#21160;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#22312;&#20110;&#23427;&#20204;&#22312;&#35757;&#32451;&#20854;&#27169;&#22411;&#26102;&#38656;&#35201;&#21464;&#25442;&#26631;&#31614;&#65292;&#24182;&#19988;&#32570;&#20047;&#19968;&#31181;&#30830;&#23450;&#27599;&#20010;&#29305;&#23450;&#31639;&#23376;&#36866;&#29992;&#20110;&#27969;&#24418;&#30340;&#21738;&#20123;&#21306;&#22495;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#35813;&#26041;&#27861;&#30452;&#25509;&#20174;&#25968;&#25454;&#27969;&#24418;&#20013;&#23398;&#20064;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;&#65292;&#32780;&#26080;&#38656;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26631;&#35760;&#21464;&#25442;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#20462;&#25913;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#35813;&#33258;&#32534;&#30721;&#22120;&#22312;&#31471;&#21040;&#31471;&#35757;&#32451;&#20013;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#21644;&#19968;&#32452;&#26412;&#22320;&#33258;&#36866;&#24212;&#30340;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26377;&#29992;&#30340;&#21464;&#25442;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#20998;&#31867;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning techniques incorporate identity-preserving transformations into their models to generalize their performance to previously unseen data. These transformations are typically selected from a set of functions that are known to maintain the identity of an input when applied (e.g., rotation, translation, flipping, and scaling). However, there are many natural variations that cannot be labeled for supervision or defined through examination of the data. As suggested by the manifold hypothesis, many of these natural variations live on or near a low-dimensional, nonlinear manifold. Several techniques represent manifold variations through a set of learned Lie group operators that define directions of motion on the manifold. However, these approaches are limited because they require transformation labels when training their models and they lack a method for determining which regions of the manifold are appropriate for applying each specific operator. We address these limitati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25932;&#23545;&#40065;&#26834;&#24615;&#20272;&#35745;&#36807;&#39640;&#30340;&#24494;&#22937;&#21407;&#22240;&#65292;&#21457;&#29616;&#19981;&#24179;&#34913;&#26799;&#24230;&#20063;&#20250;&#23548;&#33268;&#40065;&#26834;&#24615;&#20272;&#35745;&#36807;&#39640;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#30028;&#20998;&#35299;&#25915;&#20987;&#65292;&#24182;&#35777;&#26126;11&#20010;&#38450;&#24481;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#23481;&#26131;&#21463;&#21040;&#19981;&#24179;&#34913;&#26799;&#24230;&#30340;&#24433;&#21709;&#65292;MD&#25915;&#20987;&#21487;&#20197;&#38477;&#20302;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2006.13726</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#26799;&#24230;&#65306;&#23545;&#25932;&#23545;&#40065;&#26834;&#24615;&#20272;&#35745;&#36807;&#39640;&#30340;&#24494;&#22937;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness. (arXiv:2006.13726v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.13726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25932;&#23545;&#40065;&#26834;&#24615;&#20272;&#35745;&#36807;&#39640;&#30340;&#24494;&#22937;&#21407;&#22240;&#65292;&#21457;&#29616;&#19981;&#24179;&#34913;&#26799;&#24230;&#20063;&#20250;&#23548;&#33268;&#40065;&#26834;&#24615;&#20272;&#35745;&#36807;&#39640;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#30028;&#20998;&#35299;&#25915;&#20987;&#65292;&#24182;&#35777;&#26126;11&#20010;&#38450;&#24481;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#23481;&#26131;&#21463;&#21040;&#19981;&#24179;&#34913;&#26799;&#24230;&#30340;&#24433;&#21709;&#65292;MD&#25915;&#20987;&#21487;&#20197;&#38477;&#20302;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25932;&#23545;&#40065;&#26834;&#24615;&#30740;&#31350;&#20013;&#65292;&#35780;&#20272;&#38450;&#24481;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20197;&#24448;&#21457;&#29616;&#22312;&#35768;&#22810;&#38450;&#24481;&#26041;&#27861;&#20013;&#23384;&#22312;&#38544;&#26214;&#30340;&#26799;&#24230;&#65292;&#36825;&#20123;&#26799;&#24230;&#20250;&#23548;&#33268;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#26356;&#24494;&#22937;&#30340;&#24773;&#20917;&#65292;&#31216;&#20026;&#19981;&#24179;&#34913;&#26799;&#24230;&#65292;&#23427;&#20063;&#20250;&#23548;&#33268;&#23545;&#25932;&#23545;&#40065;&#26834;&#24615;&#30340;&#36807;&#39640;&#20272;&#35745;&#12290;&#19981;&#24179;&#34913;&#26799;&#24230;&#29616;&#35937;&#21457;&#29983;&#22312;&#36793;&#30028;&#25439;&#22833;&#30340;&#19968;&#20010;&#26415;&#35821;&#30340;&#26799;&#24230;&#21344;&#20027;&#23548;&#22320;&#20301;&#24182;&#23558;&#25915;&#20987;&#25512;&#21521;&#27425;&#20248;&#26041;&#21521;&#26102;&#12290;&#20026;&#20102;&#21033;&#29992;&#19981;&#24179;&#34913;&#26799;&#24230;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#36793;&#30028;&#20998;&#35299;(MD)&#25915;&#20987;&#65292;&#23558;&#36793;&#30028;&#25439;&#22833;&#20998;&#35299;&#20026;&#21333;&#29420;&#30340;&#26415;&#35821;&#65292;&#28982;&#21518;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#20998;&#21035;&#25506;&#32034;&#36825;&#20123;&#26415;&#35821;&#30340;&#25915;&#20987;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;MD&#25915;&#20987;&#30340;&#22810;&#30446;&#26631;&#21644;&#38598;&#21512;&#29256;&#26412;&#12290;&#36890;&#36807;&#35843;&#26597;&#33258;2018&#24180;&#20197;&#26469;&#25552;&#20986;&#30340;24&#20010;&#38450;&#24481;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;11&#20010;&#27169;&#22411;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#23481;&#26131;&#21463;&#21040;&#19981;&#24179;&#34913;&#26799;&#24230;&#30340;&#24433;&#21709;&#65292;&#32780;&#25105;&#20204;&#30340;MD&#25915;&#20987;&#21487;&#20197;&#38477;&#20302;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the robustness of a defense model is a challenging task in adversarial robustness research. Obfuscated gradients have previously been found to exist in many defense methods and cause a false signal of robustness. In this paper, we identify a more subtle situation called Imbalanced Gradients that can also cause overestimated adversarial robustness. The phenomenon of imbalanced gradients occurs when the gradient of one term of the margin loss dominates and pushes the attack towards to a suboptimal direction. To exploit imbalanced gradients, we formulate a Margin Decomposition (MD) attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. We also propose a multi-targeted and ensemble version of our MD attack. By investigating 24 defense models proposed since 2018, we find that 11 models are susceptible to a certain degree of imbalanced gradients and our MD attack can decrease their robustnes
&lt;/p&gt;</description></item></channel></rss>