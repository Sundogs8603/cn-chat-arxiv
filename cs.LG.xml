<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;FP8&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.18313</link><description>&lt;p&gt;
FP8-LM&#65306;&#35757;&#32451;FP8&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FP8-LM: Training FP8 Large Language Models. (arXiv:2310.18313v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;FP8&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;FP8&#20302;&#27604;&#29305;&#25968;&#25454;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#22312;LLM&#35757;&#32451;&#20013;&#65292;&#22823;&#22810;&#25968;&#21464;&#37327;&#65288;&#22914;&#26799;&#24230;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#65289;&#21487;&#20197;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#25454;&#26684;&#24335;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#25913;&#21464;&#36229;&#21442;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FP8&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#26694;&#26550;&#29992;&#20110;&#35757;&#32451;LLMs&#12290;&#35813;&#26694;&#26550;&#20026;LLM&#30340;&#28151;&#21512;&#31934;&#24230;&#21644;&#20998;&#24067;&#24335;&#24182;&#34892;&#35757;&#32451;&#25552;&#20379;&#20102;&#19977;&#20010;&#32423;&#21035;&#30340;FP8&#21033;&#29992;&#12290;&#23427;&#36880;&#27493;&#24341;&#20837;8&#20301;&#26799;&#24230;&#65292;&#20248;&#21270;&#22120;&#29366;&#24577;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;H100 GPU&#24179;&#21488;&#19978;&#35757;&#32451;GPT-175B&#27169;&#22411;&#26399;&#38388;&#65292;&#25105;&#20204;&#30340;FP8&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#26694;&#26550;&#19981;&#20165;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;42%&#30340;&#30495;&#23454;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#65292;&#32780;&#19988;&#27604;&#24191;&#27867;&#37319;&#29992;&#30340;BF16&#26694;&#26550;&#65288;&#21363;Megatron-LM&#65289;&#36816;&#34892;&#36895;&#24230;&#24555;64%&#65292;&#27604;Nvidia Transformer Engine&#24555;17%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42% reduction in real memory usage but also ran 64% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17%. This l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.17658</link><description>&lt;p&gt;
&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21333;&#19968;&#32447;&#24615;&#23618;&#30340;&#36890;&#36947;&#30456;&#20851;(CD)&#25110;&#36890;&#36947;&#29420;&#31435;(CI)&#24314;&#27169;&#65292;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;CD&#21644;CI&#35270;&#20026;&#20004;&#31181;&#20114;&#34917;&#20294;&#20114;&#26021;&#30340;&#26041;&#27861;&#65292;&#26080;&#27861;&#21516;&#26102;&#21033;&#29992;&#36825;&#20004;&#20010;&#26497;&#31471;&#12290;&#32780;&#19988;&#65292;CD&#21644;CI&#37117;&#26159;&#38745;&#24577;&#31574;&#30053;&#65292;&#26080;&#27861;&#22312;&#27809;&#26377;&#22823;&#37327;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26159;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;CI&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;CSC&#65288;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#65289;&#65292;&#29992;&#20110;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#22686;&#24378;&#20102;CI&#31574;&#30053;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20943;&#23567;&#20102;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#30340;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#26469;&#26377;&#25928;&#22788;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15516</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#30340;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph Attention-based Deep Reinforcement Learning for solving the Chinese Postman Problem with Load-dependent costs. (arXiv:2310.15516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#30340;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#26469;&#26377;&#25928;&#22788;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#27169;&#22411;&#22312;&#35299;&#20915;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;DRL&#27714;&#35299;&#22120;&#36890;&#24120;&#26159;&#29992;&#26469;&#35299;&#20915;&#33410;&#28857;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#20363;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;TSP&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20851;&#20110;&#24212;&#29992;&#31070;&#32463;&#26041;&#27861;&#26469;&#35299;&#20915;&#24359;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#20363;&#22914;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;&#65288;CPP&#65289;&#65292;&#30340;&#30740;&#31350;&#21364;&#21313;&#20998;&#26377;&#38480;&#65292;&#22240;&#20026;&#19982;TSP&#30456;&#27604;&#65292;&#23427;&#20204;&#30340;&#35299;&#31354;&#38388;&#36890;&#24120;&#26356;&#21152;&#19981;&#35268;&#21017;&#21644;&#22797;&#26434;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;DRL&#26694;&#26550;&#65292;&#26469;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#65288;CPP-LC&#65289;&#30340;CPP&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36127;&#36733;&#32422;&#26463;&#30340;&#22797;&#26434;&#24359;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21019;&#26032;&#28857;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;CPP-LC&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#39034;&#24207;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;DRL&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21363;Arc-DRL&#27169;&#22411;&#65292;&#23427;&#30001;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;CPP-LC&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#20351;&#24471;DRL&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Deep reinforcement learning (DRL) models have shown promising results in solving routing problems. However, most DRL solvers are commonly proposed to solve node routing problems, such as the Traveling Salesman Problem (TSP). Meanwhile, there has been limited research on applying neural methods to arc routing problems, such as the Chinese Postman Problem (CPP), since they often feature irregular and complex solution spaces compared to TSP. To fill these gaps, this paper proposes a novel DRL framework to address the CPP with load-dependent costs (CPP-LC) (Corberan et al., 2018), which is a complex arc routing problem with load constraints. The novelty of our method is two-fold. First, we formulate the CPP-LC as a Markov Decision Process (MDP) sequential model. Subsequently, we introduce an autoregressive model based on DRL, namely Arc-DRL, consisting of an encoder and decoder to address the CPP-LC challenge effectively. Such a framework allows the DRL model to work efficiently 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#28216;&#25103;&#29702;&#35770;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;AI&#39537;&#21160;&#30340;&#21093;&#22842;&#20013;&#30340;&#19981;&#22242;&#32467;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#21463;&#23475;&#32773;&#38656;&#35201;&#35753;&#26410;&#26469;&#21463;&#23475;&#32773;&#35748;&#35782;&#21040;&#20182;&#20204;&#30340;&#21033;&#30410;&#21516;&#26679;&#38754;&#20020;&#20005;&#37325;&#21644;&#32039;&#36843;&#30340;&#23041;&#32961;&#65292;&#20197;&#28608;&#21169;&#26410;&#26469;&#21463;&#23475;&#32773;&#20197;&#22242;&#32467;&#25903;&#25345;&#24403;&#21069;&#21463;&#23475;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.06009</link><description>&lt;p&gt;
AI&#39537;&#21160;&#30340;&#21093;&#22842;&#20013;&#30340;&#20998;&#32780;&#27835;&#20043;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Divide-and-Conquer Dynamics in AI-Driven Disempowerment. (arXiv:2310.06009v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06009
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#28216;&#25103;&#29702;&#35770;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;AI&#39537;&#21160;&#30340;&#21093;&#22842;&#20013;&#30340;&#19981;&#22242;&#32467;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#21463;&#23475;&#32773;&#38656;&#35201;&#35753;&#26410;&#26469;&#21463;&#23475;&#32773;&#35748;&#35782;&#21040;&#20182;&#20204;&#30340;&#21033;&#30410;&#21516;&#26679;&#38754;&#20020;&#20005;&#37325;&#21644;&#32039;&#36843;&#30340;&#23041;&#32961;&#65292;&#20197;&#28608;&#21169;&#26410;&#26469;&#21463;&#23475;&#32773;&#20197;&#22242;&#32467;&#25903;&#25345;&#24403;&#21069;&#21463;&#23475;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20844;&#21496;&#35797;&#22270;&#21019;&#36896;&#20986;&#22312;&#22823;&#37096;&#20998;&#32463;&#27982;&#20215;&#20540;&#24037;&#20316;&#19978;&#36229;&#36234;&#20154;&#31867;&#30340;AI&#31995;&#32479;&#12290;&#24403;&#21069;&#30340;AI&#27169;&#22411;&#24050;&#32463;&#33258;&#21160;&#21270;&#21066;&#24369;&#20102;&#19968;&#20123;&#33402;&#26415;&#23478;&#12289;&#28436;&#21592;&#21644;&#20316;&#23478;&#30340;&#29983;&#35745;&#12290;&#20294;&#26159;&#22312;&#37027;&#20123;&#20248;&#20808;&#32771;&#34385;&#24403;&#21069;&#21361;&#23475;&#21644;&#26410;&#26469;&#21361;&#23475;&#20043;&#38388;&#23384;&#22312;&#30528;&#20869;&#35751;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#27169;&#22411;&#26469;&#30740;&#31350;&#36825;&#31181;&#19981;&#22242;&#32467;&#30340;&#21407;&#22240;&#21644;&#21518;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#26377;&#21161;&#20110;&#35299;&#37322;&#20026;&#20160;&#20040;&#22312;&#21382;&#21490;&#19978;&#65292;&#38754;&#20020;&#20849;&#21516;&#23041;&#32961;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#21457;&#29616;&#32852;&#21512;&#36215;&#26469;&#23545;&#25239;&#35813;&#23041;&#32961;&#26159;&#26377;&#21033;&#30340;&#65292;&#32780;&#35813;&#20849;&#21516;&#23041;&#32961;&#21448;&#21457;&#29616;&#20998;&#32780;&#27835;&#20043;&#26159;&#26377;&#21033;&#30340;&#12290;&#22312;&#29616;&#23454;&#21442;&#25968;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#20960;&#20010;&#39044;&#27979;&#65292;&#22312;&#21382;&#21490;&#32463;&#39564;&#35760;&#24405;&#20013;&#24471;&#21040;&#20102;&#21021;&#27493;&#30340;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI companies are attempting to create AI systems that outperform humans at most economically valuable work. Current AI models are already automating away the livelihoods of some artists, actors, and writers. But there is infighting between those who prioritize current harms and future harms. We construct a game-theoretic model of conflict to study the causes and consequences of this disunity. Our model also helps explain why throughout history, stakeholders sharing a common threat have found it advantageous to unite against it, and why the common threat has in turn found it advantageous to divide and conquer.  Under realistic parameter assumptions, our model makes several predictions that find preliminary corroboration in the historical-empirical record. First, current victims of AI-driven disempowerment need the future victims to realize that their interests are also under serious and imminent threat, so that future victims are incentivized to support current victims in solidarity. Se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;RNN LMs&#65289;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21482;&#33021;&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.05161</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Language Models as Probabilistic Finite-state Automata. (arXiv:2310.05161v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;RNN LMs&#65289;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21482;&#33021;&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20197;&#23481;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#26469;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#20351;&#25105;&#20204;&#31934;&#30830;&#22320;&#25551;&#36848;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#26080;&#26435;&#37325;&#24418;&#24335;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LMs&#24182;&#19981;&#25551;&#36848;&#26080;&#26435;&#37325;&#24418;&#24335;&#35821;&#35328;&#65292;&#32780;&#26159;&#23450;&#20041;&#20102;&#23545;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;RNN LMs&#21487;&#20197;&#34920;&#31034;&#21738;&#20123;&#31867;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#38472;&#36848;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;RNN&#31561;&#20215;&#20110;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#19968;&#20010;&#23376;&#31867;&#65292;&#22240;&#27492;&#21482;&#33021;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;RNNs&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;LMs&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#20026;&#20102;&#34920;&#31034;&#19968;&#20010;&#20219;&#24847;&#30830;&#23450;&#30340;&#26377;&#38480;&#29366;&#24577;LMs&#65292;&#20854;&#20013;&#26377;$N$&#20010;&#29366;&#24577;&#19988;&#23383;&#31526;&#38598;&#20026;$\Sigma$&#30340;RNN requir
&lt;/p&gt;
&lt;p&gt;
Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the representational capacity of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages -- rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requir
&lt;/p&gt;</description></item><item><title>GPS-AFL&#26159;&#19968;&#31181;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#28176;&#36827;&#24335;&#21442;&#19982;&#32773;&#36873;&#25321;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#22810;&#36718;&#35757;&#32451;&#20013;&#36880;&#28176;&#36873;&#25321;&#25968;&#25454;&#25152;&#26377;&#32773;&#65292;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#36873;&#25321;&#20559;&#24046;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.02651</link><description>&lt;p&gt;
&#22312;&#38656;&#35201;&#26102;&#32856;&#29992;&#65306;&#28176;&#36827;&#24335;&#21442;&#19982;&#32773;&#25307;&#21215;&#29992;&#20110;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hire When You Need to: Gradual Participant Recruitment for Auction-based Federated Learning. (arXiv:2310.02651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02651
&lt;/p&gt;
&lt;p&gt;
GPS-AFL&#26159;&#19968;&#31181;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#28176;&#36827;&#24335;&#21442;&#19982;&#32773;&#36873;&#25321;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#22810;&#36718;&#35757;&#32451;&#20013;&#36880;&#28176;&#36873;&#25321;&#25968;&#25454;&#25152;&#26377;&#32773;&#65292;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#36873;&#25321;&#20559;&#24046;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;DOs&#65289;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#65292;&#20197;&#21450;&#20182;&#20204;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#21160;&#26426;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20197;&#22768;&#35465;&#20026;&#22522;&#30784;&#30340;&#32852;&#37030;&#23398;&#20064;&#21442;&#19982;&#32773;&#36873;&#25321;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#23545;&#39640;&#22768;&#35465;DOs&#30340;&#28508;&#22312;&#36873;&#25321;&#20559;&#24046;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#20559;&#24046;&#21487;&#33021;&#23548;&#33268;&#36739;&#20302;&#22768;&#35465;&#30340;DOs&#34987;&#36807;&#26089;&#22320;&#25490;&#38500;&#22312;&#26410;&#26469;&#30340;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#36718;&#27425;&#20013;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#32467;&#26524;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#28176;&#36827;&#24335;&#21442;&#19982;&#32773;&#36873;&#25321;&#26041;&#26696;&#65288;GPS-AFL&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#28608;&#21169;&#26426;&#21046;&#19981;&#21516;&#65292;&#21518;&#32773;&#36890;&#24120;&#35748;&#20026;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#25152;&#26377;DOs&#24517;&#39035;&#19968;&#27425;&#24615;&#36873;&#25321;&#65292;GPS-AFL&#22312;&#22810;&#36718;&#35757;&#32451;&#20013;&#36880;&#28176;&#36873;&#25321;&#25152;&#38656;&#30340;DOs&#65292;&#38543;&#30528;&#36890;&#36807;&#37325;&#22797;&#20132;&#20114;&#36880;&#28176;&#25581;&#31034;&#26356;&#22810;&#20449;&#24687;&#12290;&#23427;&#30340;&#35774;&#35745;&#26088;&#22312;&#22312;&#25104;&#26412;&#21644;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of federated Learning (FL) depends on the quantity and quality of the data owners (DOs) as well as their motivation to join FL model training. Reputation-based FL participant selection methods have been proposed. However, they still face the challenges of the cold start problem and potential selection bias towards highly reputable DOs. Such a bias can result in lower reputation DOs being prematurely excluded from future FL training rounds, thereby reducing the diversity of training data and the generalizability of the resulting models. To address these challenges, we propose the Gradual Participant Selection scheme for Auction-based Federated Learning (GPS-AFL). Unlike existing AFL incentive mechanisms which generally assume that all DOs required for an FL task must be selected in one go, GPS-AFL gradually selects the required DOs over multiple rounds of training as more information is revealed through repeated interactions. It is designed to strike a balance between cost s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#22270;&#27169;&#22411;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#22270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#19981;&#21516;&#23646;&#24615;&#21644;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#12289;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00149</link><description>&lt;p&gt;
&#19968;&#20992;&#20999;&#65306;&#21521;&#33021;&#22815;&#35757;&#32451;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#21333;&#19968;&#22270;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
One for All: Towards Training One Graph Model for All Classification Tasks. (arXiv:2310.00149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00149
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#22270;&#27169;&#22411;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#22270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#19981;&#21516;&#23646;&#24615;&#21644;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#12289;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#30340;&#21333;&#19968;&#27169;&#22411;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#38271;&#26399;&#30446;&#26631;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#35821;&#35328;&#39046;&#22495;&#20869;&#25972;&#21512;&#21644;&#35299;&#20915;&#19981;&#21516;&#20219;&#21153;&#30340;&#24322;&#24120;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#23398;&#20064;&#39046;&#22495;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#30340;&#32479;&#19968;&#27169;&#22411;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22270;&#23398;&#20064;&#39046;&#22495;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#23646;&#24615;&#21644;&#36981;&#24490;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#24046;&#24322;&#20351;&#24471;&#24456;&#38590;&#23558;&#22270;&#34920;&#31034;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#20854;&#27425;&#65292;&#22270;&#19978;&#30340;&#20219;&#21153;&#20998;&#21270;&#20026;&#33410;&#28857;&#12289;&#38142;&#25509;&#21644;&#22270;&#20219;&#21153;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#23884;&#20837;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36866;&#24403;&#22270;&#25552;&#31034;&#33539;&#24335;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"&#19968;&#20992;&#20999;"&#65288;OFA&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#22270;&#27169;&#22411;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing a single model that addresses multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in integrating and solving different tasks within the language domain. However, a unified model for various tasks on graphs remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it hard to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. Striving to handle all the aforementioned challenges, we propose One for All (OFA), the first general framework that can use a single graph model to address the above challenges. Specifically, OFA proposes text-at
&lt;/p&gt;</description></item><item><title>SEPT&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#25429;&#25417;&#36712;&#36857;&#30340;&#36816;&#21160;&#23398;&#29305;&#24449;&#12289;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#20197;&#21450;&#36947;&#36335;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15289</link><description>&lt;p&gt;
SEPT: &#20026;&#36816;&#21160;&#39044;&#27979;&#30340;&#39640;&#25928;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SEPT: Towards Efficient Scene Representation Learning for Motion Prediction. (arXiv:2309.15289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15289
&lt;/p&gt;
&lt;p&gt;
SEPT&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#25429;&#25417;&#36712;&#36857;&#30340;&#36816;&#21160;&#23398;&#29305;&#24449;&#12289;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#20197;&#21450;&#36947;&#36335;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#39044;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#22797;&#26434;&#20132;&#36890;&#29615;&#22659;&#20013;&#23433;&#20840;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#25552;&#21462;&#20132;&#36890;&#20803;&#32032;&#20043;&#38388;&#30340;&#26377;&#25928;&#26102;&#31354;&#20851;&#31995;&#26159;&#20934;&#30830;&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#21463;&#21040;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;SEPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#24320;&#21457;&#22797;&#26434;&#20132;&#36890;&#22330;&#26223;&#20013;&#24378;&#22823;&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#30340;&#24314;&#27169;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21040;&#22312;&#22330;&#26223;&#36755;&#20837;&#19978;&#36827;&#34892;&#19977;&#20010;&#25513;&#30721;&#37325;&#26500;&#24314;&#27169;&#20219;&#21153;&#65292;&#21253;&#25324;&#20195;&#29702;&#36335;&#24452;&#21644;&#36947;&#36335;&#32593;&#32476;&#65292;&#39044;&#35757;&#32451;&#22330;&#26223;&#32534;&#30721;&#22120;&#20197;&#25429;&#25417;&#36712;&#36857;&#30340;&#36816;&#21160;&#23398;&#29305;&#24449;&#65292;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#20197;&#21450;&#36947;&#36335;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#28982;&#21518;&#22312;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;SEPT&#22312;Argoverse 1&#21644;Argoverse&#19978;&#26080;&#38656;&#31934;&#24515;&#35774;&#35745;&#30340;&#26550;&#26500;&#25110;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ICML 2023&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#35201;&#27714;&#21442;&#19982;&#32773;&#22312;&#20004;&#20010;&#26376;&#20869;&#25552;&#20379;&#24320;&#28304;&#23454;&#29616;&#30340;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65292;&#21560;&#24341;&#20102;28&#20010;&#21512;&#26684;&#30340;&#25552;&#20132;&#12290;</title><link>http://arxiv.org/abs/2309.15188</link><description>&lt;p&gt;
ICML 2023&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#65306;&#35774;&#35745;&#19982;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
ICML 2023 Topological Deep Learning Challenge : Design and Results. (arXiv:2309.15188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ICML 2023&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#35201;&#27714;&#21442;&#19982;&#32773;&#22312;&#20004;&#20010;&#26376;&#20869;&#25552;&#20379;&#24320;&#28304;&#23454;&#29616;&#30340;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65292;&#21560;&#24341;&#20102;28&#20010;&#21512;&#26684;&#30340;&#25552;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ICML 2023&#25299;&#25169;&#19982;&#20960;&#20309;&#26426;&#22120;&#23398;&#20064;&#30740;&#35752;&#20250;&#20013;&#20030;&#21150;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#25361;&#25112;&#12290;&#35813;&#27604;&#36187;&#35201;&#27714;&#21442;&#19982;&#32773;&#36890;&#36807;&#36129;&#29486;&#20110;python&#21253;TopoNetX&#65288;&#25968;&#25454;&#22788;&#29702;&#65289;&#21644;TopoModelX&#65288;&#28145;&#24230;&#23398;&#20064;&#65289;&#30340;&#24320;&#28304;&#23454;&#29616;&#26469;&#25552;&#20379;&#25991;&#29486;&#20013;&#30340;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#25361;&#25112;&#22312;&#20004;&#20010;&#26376;&#30340;&#26102;&#38388;&#20869;&#21560;&#24341;&#20102;28&#20010;&#21512;&#26684;&#30340;&#25552;&#20132;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25361;&#25112;&#30340;&#35774;&#35745;&#24182;&#24635;&#32467;&#20102;&#20854;&#20027;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the computational challenge on topological deep learning that was hosted within the ICML 2023 Workshop on Topology and Geometry in Machine Learning. The competition asked participants to provide open-source implementations of topological neural networks from the literature by contributing to the python packages TopoNetX (data processing) and TopoModelX (deep learning). The challenge attracted twenty-eight qualifying submissions in its two-month duration. This paper describes the design of the challenge and summarizes its main findings.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#29289;&#29702;&#20449;&#24687;&#35302;&#21457;&#29366;&#24577;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;PSMs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#21270;&#23398;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#30005;&#21378;&#31561;&#20256;&#36755;&#29616;&#35937;&#20013;&#23454;&#29616;&#23454;&#26102;&#20248;&#21270;&#21644;&#23481;&#38169;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#36827;&#34892;&#20256;&#36755;&#31995;&#32479;&#24314;&#27169;&#65292;PSMs&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.12211</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#35302;&#21457;&#29366;&#24577;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20256;&#36755;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Physics-informed State-space Neural Networks for Transport Phenomena. (arXiv:2309.12211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12211
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#29289;&#29702;&#20449;&#24687;&#35302;&#21457;&#29366;&#24577;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;PSMs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#21270;&#23398;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#30005;&#21378;&#31561;&#20256;&#36755;&#29616;&#35937;&#20013;&#23454;&#29616;&#23454;&#26102;&#20248;&#21270;&#21644;&#23481;&#38169;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#36827;&#34892;&#20256;&#36755;&#31995;&#32479;&#24314;&#27169;&#65292;PSMs&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#29289;&#29702;&#20449;&#24687;&#35302;&#21457;&#29366;&#24577;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;PSMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#23454;&#29616;&#23454;&#26102;&#20248;&#21270;&#12289;&#28789;&#27963;&#24615;&#21644;&#23481;&#38169;&#24615;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#21270;&#23398;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#30005;&#21378;&#31561;&#20197;&#20256;&#36755;&#20026;&#20027;&#23548;&#30340;&#31995;&#32479;&#12290;&#20256;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30001;&#20110;&#32570;&#20047;&#20687;&#36136;&#37327;&#23432;&#24658;&#36825;&#26679;&#30340;&#29289;&#29702;&#32422;&#26463;&#32780;&#26377;&#25152;&#19981;&#36275;&#12290;PSMs&#36890;&#36807;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#37096;&#20998;&#24494;&#20998;&#26041;&#31243;&#23545;&#29289;&#29702;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#24471;&#21040;&#20855;&#26377;&#29289;&#29702;&#32422;&#26463;&#30340;&#21487;&#36845;&#20195;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#36890;&#36807;&#20004;&#20010;&#20223;&#30495;&#23454;&#39564; - &#21152;&#28909;&#36890;&#36947;&#21644;&#20919;&#21364;&#31995;&#32479;&#22238;&#36335;&#65292;&#25105;&#20204;&#35777;&#26126;PSMs&#27604;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;&#20934;&#30830;&#24615;&#20043;&#22806;&#65292;PSMs&#36824;&#20855;&#26377;&#22810;&#31181;&#20196;&#20154;&#20449;&#26381;&#30340;&#29992;&#20363;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20854;&#20013;&#30340;&#20004;&#20010;&#65306;&#36890;&#36807;&#39034;&#24207;&#26356;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#21019;&#24314;&#38750;&#32447;&#24615;&#30417;&#25511;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
This work introduces Physics-informed State-space neural network Models (PSMs), a novel solution to achieving real-time optimization, flexibility, and fault tolerance in autonomous systems, particularly in transport-dominated systems such as chemical, biomedical, and power plants. Traditional data-driven methods fall short due to a lack of physical constraints like mass conservation; PSMs address this issue by training deep neural networks with sensor data and physics-informing using components' Partial Differential Equations (PDEs), resulting in a physics-constrained, end-to-end differentiable forward dynamics model. Through two in silico experiments - a heated channel and a cooling system loop - we demonstrate that PSMs offer a more accurate approach than purely data-driven models.  Beyond accuracy, there are several compelling use cases for PSMs. In this work, we showcase two: the creation of a nonlinear supervisory controller through a sequentially updated state-space representatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#28418;&#31227;&#25511;&#21046;&#39640;&#32500;RBMs&#12290;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11651</link><description>&lt;p&gt;
&#39640;&#32500;RBM&#30340;&#28418;&#31227;&#25511;&#21046;&#65306;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Drift Control of High-Dimensional RBM: A Computational Method Based on Neural Networks. (arXiv:2309.11651v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11651
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#28418;&#31227;&#25511;&#21046;&#39640;&#32500;RBMs&#12290;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#25490;&#38431;&#29702;&#35770;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29366;&#24577;&#31354;&#38388;&#20026;d&#32500;&#27491;&#21322;&#36724;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#12290;&#25511;&#21046;&#36807;&#31243;Z&#25353;&#29031;&#19968;&#20010;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#28436;&#21270;&#65292;&#20854;&#21327;&#26041;&#24046;&#30697;&#38453;&#26159;&#22806;&#29983;&#25351;&#23450;&#30340;&#65292;&#21453;&#23556;&#26041;&#21521;&#26159;&#20174;&#27491;&#21322;&#36724;&#36793;&#30028;&#34920;&#38754;&#21453;&#23556;&#12290;&#31995;&#32479;&#31649;&#29702;&#21592;&#26681;&#25454;Z&#30340;&#21382;&#21490;&#36873;&#25321;&#27599;&#20010;&#26102;&#38388;&#28857;t&#19978;&#30340;&#28418;&#31227;&#21521;&#37327;&#952;(t)&#65292;&#32780;&#26102;&#38388;&#28857;t&#19978;&#30340;&#25104;&#26412;&#29575;&#21462;&#20915;&#20110;Z(t)&#21644;&#952;(t)&#12290;&#22312;&#25105;&#20204;&#30340;&#21021;&#22987;&#38382;&#39064;&#34920;&#36848;&#20013;&#65292;&#30446;&#26631;&#26159;&#22312;&#26080;&#38480;&#35268;&#21010;&#26102;&#38388;&#33539;&#22260;&#20869;&#26368;&#23567;&#21270;&#26399;&#26395;&#36148;&#29616;&#25104;&#26412;&#65292;&#20043;&#21518;&#25105;&#20204;&#22788;&#29702;&#30456;&#24212;&#30340;&#20154;&#22343;&#25511;&#21046;&#38382;&#39064;&#12290;&#20511;&#37492;&#38889;&#28023;&#20142;&#31561;&#20154;&#65288;&#22269;&#23478;&#31185;&#23398;&#38498;&#23398;&#25253;&#65292;2018, 8505-8510&#65289;&#30340;&#26089;&#26399;&#24037;&#20316;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#25105;&#20204;&#30740;&#31350;&#30340;&#27979;&#35797;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#31934;&#24230;&#22312;&#19968;&#20010;&#23567;&#25968;&#33539;&#22260;&#20869;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by applications in queueing theory, we consider a stochastic control problem whose state space is the $d$-dimensional positive orthant. The controlled process $Z$ evolves as a reflected Brownian motion whose covariance matrix is exogenously specified, as are its directions of reflection from the orthant's boundary surfaces. A system manager chooses a drift vector $\theta(t)$ at each time $t$ based on the history of $Z$, and the cost rate at time $t$ depends on both $Z(t)$ and $\theta(t)$. In our initial problem formulation, the objective is to minimize expected discounted cost over an infinite planning horizon, after which we treat the corresponding ergodic control problem. Extending earlier work by Han et al. (Proceedings of the National Academy of Sciences, 2018, 8505-8510), we develop and illustrate a simulation-based computational method that relies heavily on deep neural network technology. For test problems studied thus far, our method is accurate to within a fraction o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#31163;&#32447;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21644;&#19978;&#19979;&#25991;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#24182;&#26368;&#22823;&#21270;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#25910;&#20837;&#12290;</title><link>http://arxiv.org/abs/2309.11518</link><description>&lt;p&gt;
&#22312;&#20869;&#23481;&#24066;&#22330;&#20013;&#30340;&#31163;&#32447;&#23398;&#20064;&#19979;&#30340;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Ad-load Balancing via Off-policy Learning in a Content Marketplace. (arXiv:2309.11518v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#31163;&#32447;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21644;&#19978;&#19979;&#25991;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#24182;&#26368;&#22823;&#21270;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#25910;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#26159;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#23588;&#20854;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#32972;&#26223;&#19979;&#65292;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#29992;&#25143;&#20307;&#39564;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#25910;&#20837;&#12290;&#20256;&#32479;&#30340;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#26041;&#27861;&#20381;&#36182;&#20110;&#38745;&#24577;&#20998;&#37197;&#31574;&#30053;&#65292;&#26080;&#27861;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21644;&#19978;&#19979;&#25991;&#22240;&#32032;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#23398;&#20064;&#21644;&#20381;&#25454;&#35760;&#24405;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#24191;&#21578;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#24191;&#21578;&#25910;&#20837;&#20043;&#38388;&#30340;&#20914;&#31361;&#30446;&#26631;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#30001;&#20110;&#29992;&#25143;&#24322;&#36136;&#24615;&#21644;&#29992;&#25143;&#22312;&#20250;&#35805;&#20013;&#30340;&#20301;&#32622;&#30340;&#20381;&#36182;&#24615;&#32780;&#24341;&#36215;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#22312;&#29305;&#23450;&#30340;&#20869;&#23481;&#33719;&#21462;&#20013;&#30830;&#23450;&#26368;&#20248;&#24191;&#21578;&#36127;&#36733;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ad-load balancing is a critical challenge in online advertising systems, particularly in the context of social media platforms, where the goal is to maximize user engagement and revenue while maintaining a satisfactory user experience. This requires the optimization of conflicting objectives, such as user satisfaction and ads revenue. Traditional approaches to ad-load balancing rely on static allocation policies, which fail to adapt to changing user preferences and contextual factors. In this paper, we present an approach that leverages off-policy learning and evaluation from logged bandit feedback. We start by presenting a motivating analysis of the ad-load balancing problem, highlighting the conflicting objectives between user satisfaction and ads revenue. We emphasize the nuances that arise due to user heterogeneity and the dependence on the user's position within a session. Based on this analysis, we define the problem as determining the optimal ad-load for a particular feed fetch.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#21270;&#23398;&#19987;&#21033;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21253;&#21547;10&#19975;&#20010;&#20998;&#23376;&#21450;&#20854;&#21151;&#33021;&#26631;&#31614;&#30340;&#21270;&#23398;&#21151;&#33021;&#65288;CheF&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#21151;&#33021;&#26631;&#31614;&#32463;&#36807;&#39564;&#35777;&#26159;&#39640;&#36136;&#37327;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.08765</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#19987;&#21033;&#23637;&#31034;&#20102;&#21151;&#33021;&#26631;&#31614;&#19982;&#21270;&#23398;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mining Patents with Large Language Models Demonstrates Congruence of Functional Labels and Chemical Structures. (arXiv:2309.08765v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08765
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#21270;&#23398;&#19987;&#21033;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21253;&#21547;10&#19975;&#20010;&#20998;&#23376;&#21450;&#20854;&#21151;&#33021;&#26631;&#31614;&#30340;&#21270;&#23398;&#21151;&#33021;&#65288;CheF&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#21151;&#33021;&#26631;&#31614;&#32463;&#36807;&#39564;&#35777;&#26159;&#39640;&#36136;&#37327;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32467;&#26500;&#39044;&#27979;&#21270;&#23398;&#21151;&#33021;&#26159;&#21270;&#23398;&#31185;&#23398;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#65292;&#20174;&#21457;&#29616;&#21644;&#37325;&#29992;&#26032;&#22411;&#33647;&#29289;&#21040;&#21019;&#36896;&#26032;&#26448;&#26009;&#12290;&#26368;&#36817;&#65292;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#36767;&#20102;&#28085;&#30422;&#35768;&#22810;&#19981;&#21516;&#21270;&#23398;&#21151;&#33021;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#21270;&#23398;&#19987;&#21033;&#30340;&#25361;&#25112;&#65292;&#20197;&#25972;&#21512;&#21644;&#21033;&#29992;&#36825;&#20123;&#36164;&#28304;&#25152;&#25429;&#25417;&#30340;&#20851;&#20110;&#21270;&#23398;&#21151;&#33021;&#30340;&#20449;&#24687;&#12290;&#21270;&#23398;&#19987;&#21033;&#21253;&#21547;&#22823;&#37327;&#20851;&#20110;&#21270;&#23398;&#21151;&#33021;&#30340;&#30693;&#35782;&#65292;&#20294;&#30001;&#20110;&#25552;&#21462;&#39640;&#36136;&#37327;&#21151;&#33021;&#26631;&#31614;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#23427;&#20204;&#20316;&#20026;&#25968;&#25454;&#38598;&#30340;&#26377;&#29992;&#24615;&#21382;&#26469;&#34987;&#24573;&#35270;&#12290;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;ChatGPT&#36741;&#21161;&#19987;&#21033;&#25688;&#35201;&#21644;&#35789;&#23884;&#20837;&#26631;&#31614;&#28165;&#29702;&#27969;&#31243;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21270;&#23398;&#21151;&#33021;&#65288;CheF&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;10&#19975;&#20010;&#20998;&#23376;&#21450;&#20854;&#19987;&#21033;&#34893;&#29983;&#30340;&#21151;&#33021;&#26631;&#31614;&#12290;&#36825;&#20123;&#21151;&#33021;&#26631;&#31614;&#32463;&#36807;&#39564;&#35777;&#26159;&#39640;&#36136;&#37327;&#30340;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
Predicting chemical function from structure is a major goal of the chemical sciences, from the discovery and repurposing of novel drugs to the creation of new materials. Recently, new machine learning algorithms are opening up the possibility of general predictive models spanning many different chemical functions. Here, we consider the challenge of applying large language models to chemical patents in order to consolidate and leverage the information about chemical functionality captured by these resources. Chemical patents contain vast knowledge on chemical function, but their usefulness as a dataset has historically been neglected due to the impracticality of extracting high-quality functional labels. Using a scalable ChatGPT-assisted patent summarization and word-embedding label cleaning pipeline, we derive a Chemical Function (CheF) dataset, containing 100K molecules and their patent-derived functional labels. The functional labels were validated to be of high quality, allowing us 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.06453</link><description>&lt;p&gt;
&#32553;&#23567;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#24046;&#36317;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model. (arXiv:2309.06453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#21477;&#23376;&#23884;&#20837;&#65288;CSE&#65289;&#20316;&#20026;&#20027;&#27969;&#25216;&#26415;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;CSE&#20013;&#26377;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#21363;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#21644;&#25439;&#22833;&#20989;&#25968;&#30456;&#21516;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#22238;&#31572;&#8220;&#21457;&#29983;&#20102;&#20160;&#20040;&#23548;&#33268;&#20102;&#24615;&#33021;&#24046;&#36317;&#8221;&#21644;&#8220;&#22914;&#20309;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#8221;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24443;&#24213;&#27604;&#36739;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;CSE&#22312;&#21508;&#33258;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#26469;&#22238;&#31572;&#8220;&#21457;&#29983;&#20102;&#20160;&#20040;&#8221;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with Contrastive learning of Sentence Embeddings (CSE) as the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, even when their sentence encoder and loss function are the same. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, alignment and uniformity only measure the results, which means they cannot answer "What happens during the training process that leads to the performance gap?" and "How can the performance gap be narrowed?". In this paper, we conduct empirical experiments to answer these "What" and "How" questions. We first answer the "What" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, We o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#22312;&#36825;&#31181;&#35266;&#23519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#65288;DeCA&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;</title><link>http://arxiv.org/abs/2308.13976</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Label Denoising through Cross-Model Agreement. (arXiv:2308.13976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#22312;&#36825;&#31181;&#35266;&#23519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#65288;DeCA&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20174;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#23398;&#20064;&#26159;&#38750;&#24120;&#24120;&#35265;&#30340;&#12290;&#35760;&#24518;&#36825;&#20123;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#40065;&#26834;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#65288;DeCA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#26368;&#23567;&#21270;&#30001;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;DeCA&#26041;&#27861;&#24212;&#29992;&#20110;&#20108;&#36827;&#21046;&#26631;&#31614;&#24773;&#26223;&#21644;&#22810;&#26631;&#31614;&#24773;&#26223;&#12290;&#23545;&#20110;&#20108;&#36827;&#21046;&#26631;&#31614;&#24773;&#26223;&#65292;&#25105;&#20204;&#36873;&#25321;&#38544;&#24335;&#21453;&#39304;&#25512;&#33616;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from corrupted labels is very common in real-world machine-learning applications. Memorizing such noisy labels could affect the learning of the model, leading to sub-optimal performances. In this work, we propose a novel framework to learn robust machine-learning models from noisy labels. Through an empirical study, we find that different models make relatively similar predictions on clean examples, while the predictions on noisy examples vary much more across different models. Motivated by this observation, we propose \em denoising with cross-model agreement \em (DeCA) which aims to minimize the KL-divergence between the true label distributions parameterized by two machine learning models while maximizing the likelihood of data observation. We employ the proposed DeCA on both the binary label scenario and the multiple label scenario. For the binary label scenario, we select implicit feedback recommendation as the downstream task and conduct experiments with four state-of-the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;H&amp;E Otsu thresholding&#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#24555;&#36895;&#21435;&#38500;&#26631;&#26412;&#27531;&#30041;&#29289;&#21644;&#20998;&#21106;&#32452;&#32455;&#65292;&#22312;&#34880;&#26579;&#20057;&#37240;&#27915;&#32418;&#26579;&#33394;&#30340;&#27963;&#26816;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13304</link><description>&lt;p&gt;
&#28608;&#20809;&#21644;&#26631;&#26412;&#28040;&#22833;&#20102;&#65281;&#24555;&#36895;&#21435;&#38500;&#26631;&#26412;&#27531;&#30041;&#29289;&#21644;&#32452;&#32455;&#20998;&#21106;&#22312;&#34880;&#26579;&#20057;&#37240;&#27915;&#32418;&#26579;&#33394;&#27963;&#26816;&#20013;
&lt;/p&gt;
&lt;p&gt;
Bang and the Artefacts are Gone! Rapid Artefact Removal and Tissue Segmentation in Haematoxylin and Eosin Stained Biopsies. (arXiv:2308.13304v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;H&amp;E Otsu thresholding&#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#24555;&#36895;&#21435;&#38500;&#26631;&#26412;&#27531;&#30041;&#29289;&#21644;&#20998;&#21106;&#32452;&#32455;&#65292;&#22312;&#34880;&#26579;&#20057;&#37240;&#27915;&#32418;&#26579;&#33394;&#30340;&#27963;&#26816;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;H&amp;E Otsu thresholding&#26041;&#26696;&#65292;&#29992;&#20110;&#24555;&#36895;&#26816;&#27979;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20013;&#30340;&#32452;&#32455;&#65292;&#21487;&#20197;&#28040;&#38500;&#31508;&#36857;&#21644;&#25195;&#25551;&#27531;&#30041;&#31561;&#21508;&#31181;&#19981;&#33391;&#26631;&#26412;&#27531;&#30041;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21040;&#33719;&#21462;&#20302;&#25918;&#22823;&#20493;&#29575;RGB&#20840;&#26223;&#22270;&#20687;&#30340;&#21452;&#23792;&#34920;&#31034;&#65292;&#20174;&#32780;&#21487;&#20197;&#31616;&#21333;&#22320;&#20351;&#29992;Otsu thresholding&#26041;&#27861;&#23558;&#32452;&#32455;&#19982;&#32972;&#26223;&#21644;&#27531;&#30041;&#29289;&#20998;&#31163;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#21508;&#31181;&#26426;&#26500;&#21644;WSI&#25968;&#23383;&#25195;&#25551;&#20202;&#30340;WSI&#21046;&#22791;&#30340;&#22270;&#20687;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#27599;&#24352;&#22270;&#20687;&#37117;&#21253;&#21547;&#22823;&#37327;&#30340;&#27531;&#30041;&#29289;&#65292;&#20854;&#20182;&#26041;&#27861;&#37117;&#26080;&#27861;&#22788;&#29702;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#32654;&#22937;&#20043;&#22788;&#22312;&#20110;&#20854;&#31616;&#27905;&#24615;&#65306;&#36890;&#36807;&#25805;&#20316;RGB&#39068;&#33394;&#31354;&#38388;&#24182;&#20351;&#29992;Otsu thresholding&#21487;&#20197;&#24555;&#36895;&#21435;&#38500;&#27531;&#30041;&#29289;&#24182;&#20998;&#21106;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present H&amp;E Otsu thresholding, a scheme for rapidly detecting tissue in whole-slide images (WSIs) that eliminates a wide range of undesirable artefacts such as pen marks and scanning artefacts. Our method involves obtaining a bid-modal representation of a low-magnification RGB overview image which enables simple Otsu thresholding to separate tissue from background and artefacts. We demonstrate our method on WSIs prepared from a wide range of institutions and WSI digital scanners, each containing substantial artefacts that cause other methods to fail. The beauty of our approach lies in its simplicity: manipulating RGB colour space and using Otsu thresholding allows for the rapid removal of artefacts and segmentation of tissue.
&lt;/p&gt;</description></item><item><title>LR-XFL&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#21644;&#27169;&#22411;&#26356;&#26032;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#23545;FL&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#25552;&#21319;&#21644;&#21152;&#26435;&#32858;&#21512;&#65292;&#24182;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12681</link><description>&lt;p&gt;
LR-XFL: &#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LR-XFL: Logical Reasoning-based Explainable Federated Learning. (arXiv:2308.12681v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12681
&lt;/p&gt;
&lt;p&gt;
LR-XFL&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#21644;&#27169;&#22411;&#26356;&#26032;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#23545;FL&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#25552;&#21319;&#21644;&#21152;&#26435;&#32858;&#21512;&#65292;&#24182;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21327;&#20316;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#38544;&#31169;&#20445;&#25252;&#30340;&#38656;&#27714;&#20351;&#24471;FL&#27169;&#22411;&#24456;&#38590;&#23454;&#29616;&#20840;&#23616;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064; (LR-XFL) &#26041;&#27861;&#65292;&#23558;&#36923;&#36753;&#25512;&#29702;&#34701;&#20837;FL&#20013;&#12290;&#22312;LR-XFL&#20013;&#65292;FL&#23458;&#25143;&#31471;&#26681;&#25454;&#20854;&#26412;&#22320;&#25968;&#25454;&#21019;&#24314;&#26412;&#22320;&#36923;&#36753;&#35268;&#21017;&#65292;&#24182;&#23558;&#20854;&#19982;&#27169;&#22411;&#26356;&#26032;&#19968;&#36215;&#21457;&#36865;&#21040;FL&#26381;&#21153;&#22120;&#12290;FL&#26381;&#21153;&#22120;&#36890;&#36807;&#36866;&#24403;&#30340;&#36923;&#36753;&#36830;&#25509;&#31526;&#23558;&#26412;&#22320;&#36923;&#36753;&#35268;&#21017;&#36830;&#25509;&#36215;&#26469;&#65292;&#35813;&#36830;&#25509;&#31526;&#22522;&#20110;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#23646;&#24615;&#36827;&#34892;&#25512;&#23548;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#26381;&#21153;&#22120;&#36824;&#26681;&#25454;&#23458;&#25143;&#31471;&#19978;&#20256;&#30340;&#36923;&#36753;&#35268;&#21017;&#21453;&#26144;&#30340;&#26412;&#22320;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20351;&#29992;&#26435;&#37325;&#20540;&#23545;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#36827;&#34892;&#32858;&#21512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LR-XFL&#22312;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;1.19&#65285;&#65292;5.81&#65285;&#21644;5.41&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging approach for training machine learning models collaboratively while preserving data privacy. The need for privacy protection makes it difficult for FL models to achieve global transparency and explainability. To address this limitation, we incorporate logic-based explanations into FL by proposing the Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local logic rules based on their local data and send them, along with model updates, to the FL server. The FL server connects the local logic rules through a proper logical connector that is derived based on properties of client data, without requiring access to the raw data. In addition, the server also aggregates the local model updates with weight values determined by the quality of the clients' local data as reflected by their uploaded logic rules. The results show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#23427;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04690</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Finite Element Operator Network for Solving Parametric PDEs. (arXiv:2308.04690v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#23427;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#25105;&#20204;&#29702;&#35299;&#21644;&#39044;&#27979;&#29289;&#29702;&#12289;&#24037;&#31243;&#21644;&#37329;&#34701;&#31561;&#20247;&#22810;&#39046;&#22495;&#33258;&#28982;&#29616;&#35937;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#21442;&#25968;PDE&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26377;&#38480;&#20803;&#27861;&#65292;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#37197;&#23545;&#30340;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#34920;&#26126;&#23427;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;FEONet&#26694;&#26550;&#22312;&#27169;&#25311;&#20855;&#26377;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#21644;&#22797;&#26434;&#22495;&#30340;&#21508;&#31181;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) underlie our understanding and prediction of natural phenomena across numerous fields, including physics, engineering, and finance. However, solving parametric PDEs is a complex task that necessitates efficient numerical methods. In this paper, we propose a novel approach for solving parametric PDEs using a Finite Element Operator Network (FEONet). Our proposed method leverages the power of deep learning in conjunction with traditional numerical methods, specifically the finite element method, to solve parametric PDEs in the absence of any paired input-output training data. We demonstrate the effectiveness of our approach on several benchmark problems and show that it outperforms existing state-of-the-art methods in terms of accuracy, generalization, and computational flexibility. Our FEONet framework shows potential for application in various fields where PDEs play a crucial role in modeling complex domains with diverse boundary conditions and sin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26085;&#24120;&#25235;&#21462;&#30340;1,404&#20010;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32858;&#31867;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#20998;&#26512;&#32593;&#32476;&#29983;&#24577;&#31995;&#32479;&#20013;&#20256;&#25773;&#30340;&#21465;&#36848;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#35782;&#21035;55,301&#20010;&#21465;&#36848;&#65292;&#25551;&#36848;&#20102;2022&#24180;&#20256;&#25773;&#26368;&#24191;&#27867;&#30340;&#21465;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36215;&#28304;&#21644;&#25918;&#22823;&#21465;&#36848;&#30340;&#32593;&#31449;&#12290;&#35813;&#31995;&#32479;&#21487;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#30340;&#26032;&#21465;&#36848;&#65292;&#24182;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26356;&#24555;&#22320;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.02068</link><description>&lt;p&gt;
&#34394;&#20551;&#32593;&#31449;&#65306;&#22312;&#35268;&#27169;&#19978;&#36861;&#36394;&#21644;&#24433;&#21709;&#34394;&#20551;&#26032;&#38395;&#25925;&#20107;&#30340;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale. (arXiv:2308.02068v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26085;&#24120;&#25235;&#21462;&#30340;1,404&#20010;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32858;&#31867;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#20998;&#26512;&#32593;&#32476;&#29983;&#24577;&#31995;&#32479;&#20013;&#20256;&#25773;&#30340;&#21465;&#36848;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#35782;&#21035;55,301&#20010;&#21465;&#36848;&#65292;&#25551;&#36848;&#20102;2022&#24180;&#20256;&#25773;&#26368;&#24191;&#27867;&#30340;&#21465;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36215;&#28304;&#21644;&#25918;&#22823;&#21465;&#36848;&#30340;&#32593;&#31449;&#12290;&#35813;&#31995;&#32479;&#21487;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#30340;&#26032;&#21465;&#36848;&#65292;&#24182;&#24110;&#21161;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26356;&#24555;&#22320;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#12289;&#23459;&#20256;&#21644;&#24443;&#22836;&#24443;&#23614;&#30340;&#35854;&#35328;&#22312;&#32593;&#32476;&#19978;&#22823;&#37327;&#20256;&#25773;&#65292;&#20854;&#20013;&#19968;&#20123;&#21465;&#36848;&#23545;&#20844;&#20849;&#20581;&#24247;&#12289;&#36873;&#20030;&#21644;&#20010;&#20154;&#23433;&#20840;&#20135;&#29983;&#21361;&#38505;&#30340;&#29616;&#23454;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#34394;&#20551;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#30028;&#22312;&#36861;&#36394;&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#26032;&#38395;&#21465;&#36848;&#26041;&#38754;&#20027;&#35201;&#32570;&#20047;&#33258;&#21160;&#21270;&#21644;&#31243;&#24207;&#21270;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;&#23545;1,404&#20010;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#30340;&#26085;&#24120;&#25235;&#21462;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;MPNet&#21644;DP-Means&#32858;&#31867;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#33258;&#21160;&#20998;&#31163;&#21644;&#20998;&#26512;&#22312;&#32447;&#29983;&#24577;&#31995;&#32479;&#20013;&#20256;&#25773;&#30340;&#21465;&#36848;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;1,404&#20010;&#32593;&#31449;&#19978;&#35782;&#21035;&#20102;55,301&#20010;&#21465;&#36848;&#65292;&#25551;&#36848;&#20102;2022&#24180;&#20256;&#25773;&#26368;&#24191;&#27867;&#30340;&#21465;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;&#36215;&#28304;&#21644;&#25918;&#22823;&#21465;&#36848;&#30340;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#32593;&#31449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25105;&#20204;&#30340;&#31995;&#32479;&#26469;&#26816;&#27979;&#28304;&#33258;&#19981;&#21487;&#38752;&#26032;&#38395;&#32593;&#31449;&#30340;&#26032;&#21465;&#36848;&#65292;&#24182;&#24110;&#21161;Politifact&#12289;&#36335;&#36879;&#31038;&#21644;&#32654;&#32852;&#31038;&#31561;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26356;&#24555;&#22320;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation, propaganda, and outright lies proliferate on the web, with some narratives having dangerous real-world consequences on public health, elections, and individual safety. However, despite the impact of misinformation, the research community largely lacks automated and programmatic approaches for tracking news narratives across online platforms. In this work, utilizing daily scrapes of 1,404 unreliable news websites, the large-language model MPNet, and DP-Means clustering, we introduce a system to automatically isolate and analyze the narratives spread within online ecosystems. Identifying 55,301 narratives on these 1,404 websites, we describe the most prevalent narratives spread in 2022 and identify the most influential websites that originate and magnify narratives. Finally, we show how our system can be utilized to detect new narratives originating from unreliable news websites and aid fact-checkers like Politifact, Reuters, and AP News in more quickly addressing misinfo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;CERN&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#20013;&#36873;&#25321;&#20013;&#24615;&#38271;&#23551;&#21629;&#31890;&#23376;&#34928;&#21464;&#30340;&#20107;&#20214;&#65292;&#24182;&#36890;&#36807;&#21152;&#36895;&#21345;&#36827;&#34892;&#21152;&#36895;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#21152;&#36895;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#24182;&#31526;&#21512;&#31532;&#20108;&#32423;&#35302;&#21457;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.05152</link><description>&lt;p&gt;
&#22312;&#24378;&#23376;&#23545;&#25758;&#26426;&#19978;&#29992;&#20110;&#38271;&#23551;&#21629;&#31890;&#23376;&#35302;&#21457;&#30340;FPGA&#24555;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fast Neural Network Inference on FPGAs for Triggering on Long-Lived Particles at Colliders. (arXiv:2307.05152v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;CERN&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#20013;&#36873;&#25321;&#20013;&#24615;&#38271;&#23551;&#21629;&#31890;&#23376;&#34928;&#21464;&#30340;&#20107;&#20214;&#65292;&#24182;&#36890;&#36807;&#21152;&#36895;&#21345;&#36827;&#34892;&#21152;&#36895;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#21152;&#36895;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#24182;&#31526;&#21512;&#31532;&#20108;&#32423;&#35302;&#21457;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#31890;&#23376;&#29289;&#29702;&#23398;&#38656;&#35201;&#19968;&#20010;&#22797;&#26434;&#30340;&#35302;&#21457;&#21644;&#37319;&#38598;&#31995;&#32479;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#20445;&#30041;&#24863;&#20852;&#36259;&#30340;&#30896;&#25758;&#20107;&#20214;&#20197;&#20379;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#21033;&#29992;FPGA&#21152;&#36895;&#21345;&#36827;&#34892;&#24322;&#26500;&#35745;&#31639;&#21487;&#33021;&#25104;&#20026;CERN&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#21363;&#23558;&#21040;&#26469;&#30340;&#39640;&#20142;&#24230;&#35745;&#21010;&#20013;&#35302;&#21457;&#31574;&#30053;&#30340;&#19968;&#31181;&#26032;&#36235;&#21183;&#25216;&#26415;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#22312;&#25506;&#27979;&#22120;&#20307;&#31215;&#20869;&#21457;&#29983;&#20013;&#24615;&#38271;&#23551;&#21629;&#31890;&#23376;&#34928;&#21464;&#30340;&#20107;&#20214;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#21830;&#19994;&#21487;&#33719;&#24471;&#30340;Xilinx FPGA&#21152;&#36895;&#21345;&#19978;&#36827;&#34892;&#21152;&#36895;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#25512;&#29702;&#26102;&#38388;&#36824;&#19982;&#22522;&#20110;CPU&#21644;GPU&#30340;&#30828;&#20214;&#35774;&#32622;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26032;&#31639;&#27861;&#22312;&#32771;&#34385;&#30340;&#22522;&#20934;&#29289;&#29702;&#22330;&#26223;&#20013;&#26159;&#39640;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;FPGA&#21345;&#19978;&#21152;&#36895;&#26102;&#20934;&#30830;&#24615;&#27809;&#26377;&#38477;&#20302;&#12290;&#25152;&#26377;&#27979;&#35797;&#30340;&#26550;&#26500;&#37117;&#31526;&#21512;&#31532;&#20108;&#32423;&#35302;&#21457;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental particle physics demands a sophisticated trigger and acquisition system capable to efficiently retain the collisions of interest for further investigation. Heterogeneous computing with the employment of FPGA cards may emerge as a trending technology for the triggering strategy of the upcoming high-luminosity program of the Large Hadron Collider at CERN. In this context, we present two machine-learning algorithms for selecting events where neutral long-lived particles decay within the detector volume studying their accuracy and inference time when accelerated on commercially available Xilinx FPGA accelerator cards. The inference time is also confronted with a CPU- and GPU-based hardware setup. The proposed new algorithms are proven efficient for the considered benchmark physics scenario and their accuracy is found to not degrade when accelerated on the FPGA cards. The results indicate that all tested architectures fit within the latency requirements of a second-level trigge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#25991;&#26412;&#20013;&#30340;&#28909;&#38376;&#20803;&#32032;&#26469;&#20943;&#23569;GPU&#20869;&#23384;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.14048</link><description>&lt;p&gt;
H$_2$O: &#39640;&#25928;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28909;&#38376;&#20803;&#32032;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. (arXiv:2306.14048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#25991;&#26412;&#20013;&#30340;&#28909;&#38376;&#20803;&#32032;&#26469;&#20943;&#23569;GPU&#20869;&#23384;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;, &#20294;&#26159;&#30001;&#20110;&#25104;&#26412;&#36807;&#39640;&#65292;&#23427;&#20204;&#29305;&#21035;&#38590;&#20197;&#29992;&#20110;&#23545;&#35805;&#31995;&#32479;&#21644;&#25925;&#20107;&#21019;&#20316;&#31561;&#38656;&#35201;&#29983;&#25104;&#38271;&#20869;&#23481;&#30340;&#24212;&#29992;&#12290;&#38500;&#20102;&#27169;&#22411;&#21442;&#25968;&#22806;&#65292;&#36890;&#24120;&#36824;&#38656;&#35201;&#22312;GPU&#20869;&#23384;&#20013;&#23384;&#20648;&#22823;&#37327;&#20020;&#26102;&#29366;&#24577;&#20449;&#24687;&#65292;&#31216;&#20026;KV cache&#65292;&#23427;&#19982;&#24207;&#21015;&#38271;&#24230;&#21644;&#25209;&#37327;&#22823;&#23567;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#29616;KV cache&#30340;&#26041;&#27861;&#65292;&#23427;&#26174;&#33879;&#22320;&#20943;&#23569;&#20102;&#20854;&#20869;&#23384;&#21344;&#29992;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#21457;&#29616;&#65292;&#21363;&#22312;&#35745;&#31639;&#27880;&#24847;&#21147;&#20998;&#25968;&#26102;&#65292;&#23567;&#37096;&#20998;&#26631;&#35760;&#36129;&#29486;&#26368;&#22823;&#20215;&#20540;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#26631;&#35760;&#20026;&#28909;&#38376;&#20803;&#32032;(H$_2$)&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;(i) H$_2$&#30340;&#20986;&#29616;&#26159;&#33258;&#28982;&#32780;&#28982;&#30340;&#65292;&#24182;&#19988;&#19982;&#25991;&#26412;&#20013;&#26631;&#35760;&#30340;&#39057;&#32321;&#20849;&#29616;&#24378;&#30456;&#20851;&#65307;(ii)&#21435;&#38500;&#23427;&#20204;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;H$_2$O&#65292;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM&#30340;&#28909;&#38376;&#20803;&#32032;&#39044;&#27979;&#22120;&#12290;H$_2$O&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#32473;&#23450;&#24207;&#21015;&#20013;&#30340;&#28909;&#38376;&#20803;&#32032;&#65292;&#24182;&#22240;&#27492;&#20445;&#25345;&#19968;&#20010;&#26356;&#23567;&#30340;KV cache,&#23558;GPU&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;50%&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;,H$_2$O&#21487;&#20197;&#22312;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#23436;&#25972;KV cache&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insi
&lt;/p&gt;</description></item><item><title>HypLL&#26159;&#19968;&#20010;&#20351;&#29992;&#24076;&#20122;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#22522;&#20110;PyTorch&#65292;&#26088;&#22312;&#20351;&#20854;&#26131;&#20110;&#20351;&#29992;&#65292;&#25645;&#24314;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#65292;&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.06154</link><description>&lt;p&gt;
HypLL: &#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
HypLL: The Hyperbolic Learning Library. (arXiv:2306.06154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06154
&lt;/p&gt;
&lt;p&gt;
HypLL&#26159;&#19968;&#20010;&#20351;&#29992;&#24076;&#20122;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#22522;&#20110;PyTorch&#65292;&#26088;&#22312;&#20351;&#20854;&#26131;&#20110;&#20351;&#29992;&#65292;&#25645;&#24314;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#65292;&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#22810;&#23186;&#20307;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#65292;&#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#27491;&#36805;&#36895;&#24341;&#36215;&#20851;&#27880;&#12290;&#28145;&#24230;&#32593;&#32476;&#36890;&#24120;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#38544;&#21547;&#22320;&#20551;&#35774;&#25968;&#25454;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#26102;&#65292;&#24076;&#20122;&#20960;&#20309;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21487;&#35775;&#38382;&#30340;&#24320;&#28304;&#24211;&#29992;&#20110;&#26500;&#24314;&#31867;&#20284;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HypLL, &#21363;&#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#20197;&#23558;&#24076;&#20122;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;HypLL&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#29305;&#21035;&#24378;&#35843;&#20854;&#26131;&#29992;&#24615;&#35774;&#35745;&#65292;&#20197;&#21560;&#24341;&#24191;&#27867;&#30340;&#21463;&#20247;&#20851;&#27880;&#36825;&#20010;&#26032;&#30340;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/maxvanspengler/hyperbolic_learning_library&#12290;&#21387;&#32553;&#25991;&#20214;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://d
&lt;/p&gt;
&lt;p&gt;
Deep learning in hyperbolic space is quickly gaining traction in the fields of machine learning, multimedia, and computer vision. Deep networks commonly operate in Euclidean space, implicitly assuming that data lies on regular grids. Recent advances have shown that hyperbolic geometry provides a viable alternative foundation for deep learning, especially when data is hierarchical in nature and when working with few embedding dimensions. Currently however, no accessible open-source library exists to build hyperbolic network modules akin to well-known deep learning libraries. We present HypLL, the Hyperbolic Learning Library to bring the progress on hyperbolic deep learning together. HypLL is built on top of PyTorch, with an emphasis in its design for easy-of-use, in order to attract a broad audience towards this new and open-ended research direction. The code is available at: https://github.com/maxvanspengler/hyperbolic_learning_library. The compressed archive is available at: https://d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25104;&#21151;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29983;&#25104;&#24314;&#27169;&#30340;&#20248;&#24322;&#24615;&#36136;&#19982;&#39640;&#25928;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;&#20316;&#32773;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#35774;&#35745;&#20102;&#28040;&#38500;&#36845;&#20195;&#30340;&#20272;&#35745;&#22120;&#24182;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01843</link><description>&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Training of Autoencoders. (arXiv:2306.01843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25104;&#21151;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29983;&#25104;&#24314;&#27169;&#30340;&#20248;&#24322;&#24615;&#36136;&#19982;&#39640;&#25928;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;&#20316;&#32773;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#35774;&#35745;&#20102;&#28040;&#38500;&#36845;&#20195;&#30340;&#20272;&#35745;&#22120;&#24182;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#20855;&#26377;&#20248;&#24322;&#30340;&#32479;&#35745;&#24615;&#36136;&#65292;&#23588;&#20854;&#26159;&#22312;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#20013;&#38750;&#24120;&#27969;&#34892;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#27969;&#24418;&#20551;&#35774;&#65292;&#29983;&#25104;&#33258;&#32534;&#30721;&#22120;&#26377;&#26395;&#27604;&#24402;&#19968;&#21270;&#27969;&#26356;&#39640;&#25928;&#12290;&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#30340;&#25104;&#21151;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#23558;&#36825;&#20004;&#31181;&#33539;&#24335;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35782;&#21035;&#24182;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#29616;&#26377;&#30340;&#33258;&#30001;&#26684;&#24335;&#32593;&#32476;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#36807;&#20110;&#32531;&#24930;&#65292;&#20381;&#36182;&#20110;&#36845;&#20195;&#26041;&#26696;&#65292;&#20854;&#25104;&#26412;&#38543;&#28508;&#22312;&#32500;&#24230;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#20272;&#35745;&#22120;&#65292;&#28040;&#38500;&#20102;&#36845;&#20195;&#65292;&#20174;&#32780;&#20351;&#25104;&#26412;&#20445;&#25345;&#19981;&#21464;&#65288;&#27599;&#20010;&#25209;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#22823;&#32422;&#26159;&#26222;&#36890;&#33258;&#32534;&#30721;&#22120;&#30340;&#20004;&#20493;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#26420;&#32032;&#22320;&#23558;&#26368;&#22823;&#20284;&#28982;&#24212;&#29992;&#20110;&#33258;&#32534;&#30721;&#22120;&#21487;&#33021;&#23548;&#33268;&#21457;&#25955;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#24819;&#27861;&#26469;&#25512;&#21160;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#29983;&#25104;&#22270;&#20687;&#12289;&#25554;&#20540;&#21644;&#21464;&#25442;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood training has favorable statistical properties and is popular for generative modeling, especially with normalizing flows. On the other hand, generative autoencoders promise to be more efficient than normalizing flows due to the manifold hypothesis. In this work, we introduce successful maximum likelihood training of unconstrained autoencoders for the first time, bringing the two paradigms together. To do so, we identify and overcome two challenges: Firstly, existing maximum likelihood estimators for free-form networks are unacceptably slow, relying on iteration schemes whose cost scales linearly with latent dimension. We introduce an improved estimator which eliminates iteration, resulting in constant cost (roughly double the runtime per batch of a vanilla autoencoder). Secondly, we demonstrate that naively applying maximum likelihood to autoencoders can lead to divergent solutions and use this insight to motivate a stable maximum likelihood training objective. We per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24189;&#28789;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;GBN&#65289;&#20013;&#30340;&#8220;&#24189;&#28789;&#22122;&#22768;&#8221;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;Ghost Noise Injection (GNI)&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36991;&#20813;&#23567;&#25209;&#37327;&#35757;&#32451;&#24102;&#26469;&#30340;&#35757;&#32451;-&#27979;&#35797;&#24046;&#24322;&#25928;&#24212;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#20379;&#26356;&#22909;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17205</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24189;&#28789;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Ghost Noise for Regularizing Deep Neural Networks. (arXiv:2305.17205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24189;&#28789;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;GBN&#65289;&#20013;&#30340;&#8220;&#24189;&#28789;&#22122;&#22768;&#8221;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;Ghost Noise Injection (GNI)&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36991;&#20813;&#23567;&#25209;&#37327;&#35757;&#32451;&#24102;&#26469;&#30340;&#35757;&#32451;-&#27979;&#35797;&#24046;&#24322;&#25928;&#24212;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#20379;&#26356;&#22909;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#31283;&#23450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#36807;&#31243;&#24182;&#25552;&#39640;&#27979;&#35797;&#24615;&#33021;&#12290;BN&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#21462;&#20915;&#20110;&#25209;&#37327;&#22823;&#23567;&#65292;&#26174;&#24335;&#20351;&#29992;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#20250;&#36890;&#36807;&#25209;&#37327;&#24402;&#19968;&#21270;&#25552;&#39640;&#27867;&#21270;&#24615;&#65292;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#37117;&#20855;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#24341;&#20837;&#30340;&#8220;&#24189;&#28789;&#22122;&#22768;&#8221;&#19982;&#24402;&#19968;&#21270;&#36827;&#34892;&#20998;&#31163;&#65292;&#24182;&#23450;&#37327;&#20998;&#26512;&#22122;&#22768;&#30340;&#20998;&#24067;&#21450;&#20854;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#30740;&#31350;GBN&#30340;&#26377;&#25928;&#24615;&#12290;&#21463;&#25105;&#20204;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#31216;&#20026;Ghost Noise Injection (GNI)&#65292;&#27169;&#20223;&#20102;GBN&#20013;&#30340;&#22122;&#22768;&#65292;&#32780;&#36991;&#20813;&#20102;&#23567;&#25209;&#37327;&#35757;&#32451;&#24102;&#26469;&#30340;&#26377;&#23475;&#30340;&#35757;&#32451;-&#27979;&#35797;&#24046;&#24322;&#25928;&#24212;&#12290;&#23454;&#39564;&#35777;&#26126;GNI&#21487;&#20197;&#27604;GBN&#25552;&#20379;&#26356;&#22909;&#30340;&#27867;&#21270;&#25928;&#30410;&#12290;Ghost Noise Injection&#22312;&#20854;&#20182;&#38750;&#22122;&#22768;&#35774;&#32622;&#20013;&#65292;&#20363;&#22914;&#23618;&#24402;&#19968;&#21270;&#32593;&#32476;&#20063;&#33021;&#22815;&#20135;&#29983;&#31215;&#26497;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) is widely used to stabilize the optimization process and improve the test performance of deep neural networks. The regularization effect of BN depends on the batch size and explicitly using smaller batch sizes with Batch Normalization, a method known as Ghost Batch Normalization (GBN), has been found to improve generalization in many settings. We investigate the effectiveness of GBN by disentangling the induced "Ghost Noise" from normalization and quantitatively analyzing the distribution of noise as well as its impact on model performance. Inspired by our analysis, we propose a new regularization technique called Ghost Noise Injection (GNI) that imitates the noise in GBN without incurring the detrimental train-test discrepancy effects of small batch training. We experimentally show that GNI can provide a greater generalization benefit than GBN. Ghost Noise Injection can also be beneficial in otherwise non-noisy settings such as layer-normalized networks, provi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#25351;&#25968;&#31215;&#20998;&#22120;&#65292;&#23427;&#22312;&#22788;&#29702;&#21018;&#24615;&#31995;&#32479;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#25552;&#20379;&#25968;&#20540;&#35823;&#24046;&#30340;&#27010;&#29575;&#35299;&#37322;&#65292;&#24182;&#19988;&#33021;&#22815;&#34987;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.14978</link><description>&lt;p&gt;
&#27010;&#29575;&#25351;&#25968;&#31215;&#20998;&#22120;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Exponential Integrators. (arXiv:2305.14978v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#25351;&#25968;&#31215;&#20998;&#22120;&#65292;&#23427;&#22312;&#22788;&#29702;&#21018;&#24615;&#31995;&#32479;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#25552;&#20379;&#25968;&#20540;&#35823;&#24046;&#30340;&#27010;&#29575;&#35299;&#37322;&#65292;&#24182;&#19988;&#33021;&#22815;&#34987;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#27714;&#35299;&#22120;&#20026;&#21160;&#24577;&#31995;&#32479;&#30340;&#27169;&#25311;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#25512;&#26029;&#25552;&#20379;&#20102;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#21018;&#24615;&#31995;&#32479;&#20013;&#65292;&#23427;&#20204;&#20687;&#26631;&#20934;&#27714;&#35299;&#22120;&#19968;&#26679;&#20250;&#36935;&#21040;&#24615;&#33021;&#24809;&#32602;&#65292;&#22240;&#20026;&#38656;&#35201;&#37319;&#21462;&#23567;&#27493;&#38271;&#19981;&#26159;&#20026;&#20102;&#25968;&#20540;&#31934;&#24230;&#65292;&#32780;&#26159;&#20026;&#20102;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#27010;&#29575;&#25351;&#25968;&#31215;&#20998;&#22120;&#26497;&#22823;&#22320;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#24555;&#36895;&#12289;&#32447;&#24615;&#21160;&#24577;&#21152;&#20837;&#20808;&#39564;&#20013;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#31867;&#20855;&#26377;&#26377;&#21033;&#24615;&#36136;&#30340;&#27010;&#29575;&#31215;&#20998;&#22120;&#12290;&#21363;&#23427;&#20204;&#34987;&#35777;&#26126;&#26159;L-&#31283;&#23450;&#30340;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#20250;&#38477;&#20302;&#21040;&#32463;&#20856;&#30340;&#25351;&#25968;&#31215;&#20998;&#22120;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25968;&#20540;&#35823;&#24046;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#36890;&#36807;&#22312;&#20808;&#21069;&#20272;&#35745;&#20540;&#30340;&#21521;&#37327;&#22330;&#38597;&#21487;&#27604;&#19978;&#24378;&#21152;&#20998;&#27573;&#21322;&#32447;&#24615;&#65292;&#35813;&#26041;&#27861;&#36824;&#25512;&#24191;&#21040;&#20219;&#24847;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#21018;&#24615;&#38382;&#39064;&#20013;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#27010;&#29575;&#25351;&#25968;&#31215;&#20998;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic solvers provide a flexible and efficient framework for simulation, uncertainty quantification, and inference in dynamical systems. However, like standard solvers, they suffer performance penalties for certain stiff systems, where small steps are required not for reasons of numerical accuracy but for the sake of stability. This issue is greatly alleviated in semi-linear problems by the probabilistic exponential integrators developed in this paper. By including the fast, linear dynamics in the prior, we arrive at a class of probabilistic integrators with favorable properties. Namely, they are proven to be L-stable, and in a certain case reduce to a classic exponential integrator -- with the added benefit of providing a probabilistic account of the numerical error. The method is also generalized to arbitrary non-linear systems by imposing piece-wise semi-linearity on the prior via Jacobians of the vector field at the previous estimates, resulting in probabilistic exponential
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13030</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Adaptive action supervision in reinforcement learning from real-world multi-agent demonstrations. (arXiv:2305.13030v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#23545;&#30495;&#23454;&#19990;&#30028;&#29983;&#29289;&#22810;&#26234;&#33021;&#20307;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#29983;&#25104;&#28789;&#27963;&#21644;&#22810;&#26679;&#21270;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#65307;&#28982;&#32780;&#65292;&#22312;&#24314;&#27169;&#30495;&#23454;&#19990;&#30028;&#29983;&#29289;&#22810;&#26234;&#33021;&#20307;&#26102;&#65292;&#22312;&#28304;&#65288;&#21363;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65289;&#21644;&#30446;&#26631;&#65288;&#21363; RL &#30340;&#32593;&#32476;&#31354;&#38388;&#65289;&#20043;&#38388;&#23384;&#22312;&#22495;&#24046;&#24322;&#65292;&#24182;&#19988;&#28304;&#29615;&#22659;&#21442;&#25968;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30340;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892; RL &#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#32467;&#21512; RL &#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#30340;&#28436;&#31034;&#21160;&#20316;&#26469;&#22312; RL &#20013;&#21033;&#29992;&#26410;&#30693;&#28304;&#21160;&#24577;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#20026;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling of real-world biological multi-agents is a fundamental problem in various scientific and engineering fields. Reinforcement learning (RL) is a powerful framework to generate flexible and diverse behaviors in cyberspace; however, when modeling real-world biological multi-agents, there is a domain gap between behaviors in the source (i.e., real-world data) and the target (i.e., cyberspace for RL), and the source environment parameters are usually unknown. In this paper, we propose a method for adaptive action supervision in RL from real-world demonstrations in multi-agent scenarios. We adopt an approach that combines RL and supervised learning by selecting actions of demonstrations in RL based on the minimum distance of dynamic time warping for utilizing the information of the unknown source dynamics. This approach can be easily applied to many existing neural network architectures and provide us with an RL model balanced between reproducibility as imitation and generalization ab
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#26222;&#21450;&#31243;&#24230;&#65292;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#32593;&#31449;&#19978;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2305.09820</link><description>&lt;p&gt;
&#26426;&#22120;&#21046;&#36896;&#30340;&#23186;&#20307;&#65306;&#30417;&#27979;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#19978;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#30340;&#21160;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites. (arXiv:2305.09820v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#26222;&#21450;&#31243;&#24230;&#65292;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#32593;&#31449;&#19978;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20687;ChatGPT&#36825;&#26679;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26085;&#30410;&#27969;&#34892;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#38395;&#32593;&#31449;&#24320;&#22987;&#21033;&#29992;&#23427;&#20204;&#29983;&#25104;&#25991;&#31456;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#33021;&#22312;&#22768;&#35465;&#33391;&#22909;&#30340;&#32593;&#31449;&#19978;&#20135;&#29983;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#25991;&#31456;&#65292;&#32780;&#19988;&#19981;&#33391;&#26032;&#38395;&#32593;&#31449;&#20063;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;LLM&#25209;&#37327;&#29983;&#20135;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#20102;&#24320;&#22987;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#22823;&#35268;&#27169;&#30740;&#31350;&#21512;&#25104;&#25991;&#31456;&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#20013;&#26222;&#21450;&#29575;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;DeBERTa&#30340;&#21512;&#25104;&#26032;&#38395;&#26816;&#27979;&#22120;&#65292;&#24182;&#23545;3074&#20010;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#36229;&#36807;1291&#19975;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;2022&#24180;1&#26376;1&#26085;&#33267;2023&#24180;4&#26376;1&#26085;&#26399;&#38388;&#65292;&#21512;&#25104;&#26032;&#38395;&#25991;&#31456;&#30340;&#30456;&#23545;&#25968;&#37327;&#22312;&#20027;&#27969;&#32593;&#31449;&#19978;&#22686;&#21152;&#20102;79.4&#65285;&#65292;&#32780;&#22312;&#34394;&#20551;&#20449;&#24687;&#32593;&#31449;&#19978;&#22686;&#21152;&#20102;342&#65285;&#12290;&#20998;&#26512;ChatGPT&#21457;&#24067;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20013;&#26029;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#23427;&#30340;&#21457;&#24067;&#23548;&#33268;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#26174;&#33879;&#22686;&#21152;&#65292;&#20294;&#34394;&#20551;&#20449;&#24687;&#32593;&#31449;&#19978;&#30340;&#21512;&#25104;&#25991;&#31456;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#30340;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing popularity of generative large language models (LLMs) like ChatGPT, an increasing number of news websites have begun utilizing them to generate articles. However, not only can these language models produce factually inaccurate articles on reputable websites but disreputable news sites can utilize these LLMs to mass produce misinformation. To begin to understand this phenomenon, we present one of the first large-scale studies of the prevalence of synthetic articles within online news media. To do this, we train a DeBERTa-based synthetic news detector and classify over 12.91 million articles from 3,074 misinformation and mainstream news websites. We find that between January 1, 2022 and April 1, 2023, the relative number of synthetic news articles increased by 79.4% on mainstream websites while increasing by 342% on misinformation sites. Analyzing the impact of the release of ChatGPT using an interrupted-time-series, we show that while its release resulted in a marked
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;JaxPruner&#65292;&#19968;&#27454;&#29992;&#20110;&#30740;&#31350;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#24320;&#28304;&#24211;&#12290;JaxPruner&#25552;&#20379;&#20102;&#27969;&#34892;&#30340;&#21098;&#26525;&#21644;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#30340;&#31616;&#26126;&#23454;&#29616;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#24310;&#36831;&#24320;&#38144;&#65292;&#24182;&#21487;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;JAX&#24211;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.14082</link><description>&lt;p&gt;
JaxPruner&#65306;&#19968;&#20010;&#29992;&#20110;&#31232;&#30095;&#24615;&#30740;&#31350;&#30340;&#31616;&#26126;&#24211;
&lt;/p&gt;
&lt;p&gt;
JaxPruner: A concise library for sparsity research. (arXiv:2304.14082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JaxPruner&#65292;&#19968;&#27454;&#29992;&#20110;&#30740;&#31350;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#24320;&#28304;&#24211;&#12290;JaxPruner&#25552;&#20379;&#20102;&#27969;&#34892;&#30340;&#21098;&#26525;&#21644;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#30340;&#31616;&#26126;&#23454;&#29616;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#24310;&#36831;&#24320;&#38144;&#65292;&#24182;&#21487;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;JAX&#24211;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JaxPruner&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;JAX&#30340;&#24320;&#28304;&#21098;&#26525;&#21644;&#31232;&#30095;&#35757;&#32451;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#27969;&#34892;&#30340;&#21098;&#26525;&#21644;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#30340;&#31616;&#26126;&#23454;&#29616;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#24310;&#36831;&#24320;&#38144;&#65292;&#21152;&#36895;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#12290;JaxPruner&#23454;&#29616;&#30340;&#31639;&#27861;&#20351;&#29992;&#36890;&#29992;API&#65292;&#24182;&#19982;&#27969;&#34892;&#30340;&#20248;&#21270;&#24211;Optax&#26080;&#32541;&#21327;&#20316;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;JAX&#24211;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#20195;&#30721;&#24211;&#20013;&#25552;&#20379;&#31034;&#20363;&#24182;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20379;&#22522;&#20934;&#23454;&#39564;&#26469;&#23637;&#31034;&#36825;&#31181;&#38598;&#25104;&#30340;&#20415;&#25463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the popular optimization library Optax, which, in turn, enables easy integration with existing JAX based libraries. We demonstrate this ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX and provide baseline experiments on popular benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2304.05805</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65306;&#38271;&#26399;&#35760;&#24518;&#26377;&#22810;&#22823;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
GDP nowcasting with artificial neural networks: How much does long-term memory matter?. (arXiv:2304.05805v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#19981;&#21516;&#30340;&#32479;&#35745;&#27169;&#22411;&#24212;&#29992;&#20110;&#32654;&#22269;&#32463;&#27982;&#23395;&#24230;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65288;GDP&#65289;&#22686;&#38271;&#39044;&#27979;&#12290;&#20351;&#29992;&#27599;&#26376;&#30340;FRED-MD&#25968;&#25454;&#24211;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#65288;DFM&#65289;&#21644;&#22235;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#39044;&#27979;&#34920;&#29616;&#65306;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;1D CNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#12290;&#23454;&#35777;&#20998;&#26512;&#21576;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#35780;&#20272;&#21608;&#26399;&#30340;&#32467;&#26524;&#12290;&#31532;&#19968;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2019&#24180;&#31532;4&#23395;&#24230;&#65289;&#20855;&#26377;&#24179;&#34913;&#30340;&#32463;&#27982;&#22686;&#38271;&#65292;&#32780;&#31532;&#20108;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2022&#24180;&#31532;3&#23395;&#24230;&#65289;&#36824;&#21253;&#25324;COVID-19&#34928;&#36864;&#26399;&#38388;&#30340;&#26102;&#38388;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#30456;&#23545;&#36739;&#20302;&#30340;&#38408;&#20540;&#20540;&#65288;&#32422;&#20845;&#20010;&#23395;&#24230;&#25110;&#21313;&#20843;&#20010;&#26376;&#65289;&#20197;&#21518;&#65292;&#36825;&#31181;&#25928;&#24212;&#20250;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26399;&#65288;&#22914;COVID-19&#34928;&#36864;&#26399;&#38388;&#65289;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#20250;&#21464;&#24471;&#36739;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our study, we apply different statistical models to nowcast quarterly GDP growth for the US economy. Using the monthly FRED-MD database, we compare the nowcasting performance of the dynamic factor model (DFM) and four artificial neural networks (ANNs): the multilayer perceptron (MLP), the one-dimensional convolutional neural network (1D CNN), the long short-term memory network (LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the results from two distinctively different evaluation periods. The first (2010:Q1 -- 2019:Q4) is characterized by balanced economic growth, while the second (2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. According to our results, longer input sequences result in more accurate nowcasts in periods of balanced economic growth. However, this effect ceases above a relatively low threshold value of around six quarters (eighteen months). During periods of economic turbulence (e.g., during the COVID-19 recession), long
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#39044;&#27979;&#26399;&#36135;&#20215;&#26684;&#36235;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#20219;&#21153;&#35774;&#35745;&#21644;&#36830;&#32493;&#35757;&#32451;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.16532</link><description>&lt;p&gt;
&#24322;&#26500;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26399;&#36135;&#37327;&#21270;&#25237;&#36164;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Futures Quantitative Investment with Heterogeneous Continual Graph Neural Network. (arXiv:2303.16532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16532
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39044;&#27979;&#26399;&#36135;&#20215;&#26684;&#36235;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#20219;&#21153;&#35774;&#35745;&#21644;&#36830;&#32493;&#35757;&#32451;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#35745;&#37327;&#27169;&#22411;&#39044;&#27979;&#26399;&#36135;&#20215;&#26684;&#36235;&#21183;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#32771;&#34385;&#21040;&#26399;&#36135;&#21382;&#21490;&#25968;&#25454;&#20197;&#21450;&#19981;&#21516;&#26399;&#36135;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#27492;&#31867;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22235;&#20010;&#24322;&#26500;&#20219;&#21153;&#26469;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#29305;&#24449;&#65306;&#20215;&#26684;&#22238;&#24402;&#12289;&#31227;&#21160;&#24179;&#22343;&#20215;&#26684;&#22238;&#24402;&#12289;&#30701;&#26102;&#38388;&#20869;&#30340;&#20215;&#26684;&#24046;&#22238;&#24402;&#21644;&#21464;&#21270;&#28857;&#26816;&#27979;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#26631;&#31614;&#65292;&#25105;&#20204;&#37319;&#29992;&#36830;&#32493;&#35757;&#32451;&#30340;&#26041;&#24335;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a challenging problem to predict trends of futures prices with traditional econometric models as one needs to consider not only futures' historical data but also correlations among different futures. Spatial-temporal graph neural networks (STGNNs) have great advantages in dealing with such kind of spatial-temporal data. However, we cannot directly apply STGNNs to high-frequency future data because future investors have to consider both the long-term and short-term characteristics when doing decision-making. To capture both the long-term and short-term features, we exploit more label information by designing four heterogeneous tasks: price regression, price moving average regression, price gap regression (within a short interval), and change-point detection, which involve both long-term and short-term scenes. To make full use of these labels, we train our model in a continual manner. Traditional continual GNNs define the gradient of prices as the parameter important to overcome ca
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#20869;&#37096;&#27979;&#37327;&#20013;&#37325;&#26500;&#26925;&#22278;&#38382;&#39064;&#20013;&#30340;&#23548;&#30005;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#36830;&#32493;&#21644;&#32463;&#39564;&#25439;&#22833;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#25968;&#25454;&#22122;&#22768;&#30340;&#20248;&#24322;&#31283;&#23450;&#24615;&#20197;&#21450;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16454</link><description>&lt;p&gt;
&#28151;&#21512;&#26368;&#23567;&#20108;&#20056;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#37096;&#27979;&#37327;&#19979;&#30340;&#23548;&#30005;&#29575;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Conductivity Imaging from Internal Measurements with Mixed Least-Squares Deep Neural Networks. (arXiv:2303.16454v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#20869;&#37096;&#27979;&#37327;&#20013;&#37325;&#26500;&#26925;&#22278;&#38382;&#39064;&#20013;&#30340;&#23548;&#30005;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#36830;&#32493;&#21644;&#32463;&#39564;&#25439;&#22833;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#25968;&#25454;&#22122;&#22768;&#30340;&#20248;&#24322;&#31283;&#23450;&#24615;&#20197;&#21450;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#19968;&#20010;&#20869;&#37096;&#27979;&#37327;&#20013;&#37325;&#26500;&#26925;&#22278;&#38382;&#39064;&#20013;&#30340;&#23548;&#30005;&#29575;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#25511;&#21046;&#26041;&#31243;&#30340;&#28151;&#21512;&#25913;&#36896;&#65292;&#24182;&#21033;&#29992;&#26631;&#20934;&#30340;&#26368;&#23567;&#20108;&#20056;&#30446;&#26631;&#20989;&#25968;&#21516;&#26102;&#36817;&#20284;&#23548;&#30005;&#29575;&#21644;&#36890;&#37327;&#65292;&#20197;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#35797;&#25506;&#20989;&#25968;&#12290;&#25105;&#20204;&#23545;&#36830;&#32493;&#21644;&#32463;&#39564;&#25439;&#22833;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#21253;&#25324;&#36890;&#36807;&#22122;&#22768;&#27700;&#24179;&#12289;&#21508;&#31181;&#24809;&#32602;&#21442;&#25968;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21442;&#25968;&#65288;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#21442;&#25968;&#36793;&#30028;&#65289;&#26174;&#24335;&#22320;&#20272;&#35745;&#35823;&#24046;&#30340;&#20005;&#26684;&#20272;&#35745;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#35813;&#26041;&#27861;&#30340;&#19981;&#21516;&#29305;&#28857;&#65292;&#20363;&#22914;&#23545;&#20110;&#25968;&#25454;&#22122;&#22768;&#30340;&#20248;&#24322;&#31283;&#23450;&#24615;&#20197;&#21450;&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we develop a novel approach using deep neural networks to reconstruct the conductivity distribution in elliptic problems from one internal measurement. The approach is based on a mixed reformulation of the governing equation and utilizes the standard least-squares objective to approximate the conductivity and flux simultaneously, with deep neural networks as ansatz functions. We provide a thorough analysis of the neural network approximations for both continuous and empirical losses, including rigorous error estimates that are explicit in terms of the noise level, various penalty parameters and neural network architectural parameters (depth, width and parameter bound). We also provide extensive numerical experiments in two- and multi-dimensions to illustrate distinct features of the approach, e.g., excellent stability with respect to data noise and capability of solving high-dimensional problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Supervision Interpolation &#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#26494;&#21644;&#25512;&#24191; Mixup &#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25554;&#20540;&#22686;&#24378;&#35270;&#35282;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; LossMix &#30340;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#25110;&#32773;&#35828;LossMix &#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2303.10343</link><description>&lt;p&gt;
LossMix&#65306;&#31616;&#21270;&#21644;&#24191;&#27867;&#24212;&#29992; Mixup &#20110;&#30446;&#26631;&#26816;&#27979;&#21644;&#26356;&#22810;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LossMix: Simplify and Generalize Mixup for Object Detection and Beyond. (arXiv:2303.10343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Supervision Interpolation &#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#26494;&#21644;&#25512;&#24191; Mixup &#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25554;&#20540;&#22686;&#24378;&#35270;&#35282;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; LossMix &#30340;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#25110;&#32773;&#35828;LossMix &#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#28151;&#21512;&#22686;&#24378;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20294;&#30001;&#20110;&#31354;&#38388;&#38169;&#20301;&#12289;&#21069;&#26223;/&#32972;&#26223;&#21306;&#20998;&#20197;&#21450;&#22810;&#20010;&#23454;&#20363;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#26131;&#24212;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#30417;&#30563;&#25554;&#20540;&#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#26494;&#21644;&#25512;&#24191; Mixup &#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25554;&#20540;&#22686;&#24378;&#35270;&#35282;&#65292;&#28982;&#21518;&#22312;&#36825;&#20010;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102; LossMix&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;insight&#26159;&#65292;&#36890;&#36807;&#25554;&#20540;&#25439;&#22833;&#35823;&#24046;&#26469;&#35843;&#25972;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#35268;&#33539;&#28151;&#21512;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;ground truth&#26631;&#31614;&#12290;&#22312;PASCAL VOC&#21644;MS COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LossMix&#22987;&#32456;&#20248;&#20110;&#24403;&#21069;&#27969;&#34892;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#24182;&#19988;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#39046;&#22495;m...
&lt;/p&gt;
&lt;p&gt;
The success of data mixing augmentations in image classification tasks has been well-received. However, these techniques cannot be readily applied to object detection due to challenges such as spatial misalignment, foreground/background distinction, and plurality of instances. To tackle these issues, we first introduce a novel conceptual framework called Supervision Interpolation, which offers a fresh perspective on interpolation-based augmentations by relaxing and generalizing Mixup. Building on this framework, we propose LossMix, a simple yet versatile and effective regularization that enhances the performance and robustness of object detectors and more. Our key insight is that we can effectively regularize the training on mixed data by interpolating their loss errors instead of ground truth labels. Empirical results on the PASCAL VOC and MS COCO datasets demonstrate that LossMix consistently outperforms currently popular mixing strategies. Furthermore, we design a two-stage domain m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#30340;&#22522;&#20934;&#20197;&#21450;&#19968;&#31181;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#21644;&#20960;&#31181;&#22522;&#32447;&#26041;&#27861;&#12290;&#30740;&#31350;&#27169;&#25311;&#20102;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38543;&#26426;&#24341;&#20837;&#30340;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25439;&#22833;&#26816;&#26597;&#26469;&#26816;&#27979;&#36825;&#20123;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.06999</link><description>&lt;p&gt;
&#36890;&#36807;&#25439;&#22833;&#26816;&#26597;&#22312;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Identifying Label Errors in Object Detection Datasets by Loss Inspection. (arXiv:2303.06999v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#30340;&#22522;&#20934;&#20197;&#21450;&#19968;&#31181;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#21644;&#20960;&#31181;&#22522;&#32447;&#26041;&#27861;&#12290;&#30740;&#31350;&#27169;&#25311;&#20102;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38543;&#26426;&#24341;&#20837;&#30340;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25439;&#22833;&#26816;&#26597;&#26469;&#26816;&#27979;&#36825;&#20123;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#30340;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#26631;&#27880;&#26159;&#19968;&#20010;&#26543;&#29157;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#22312;&#27880;&#37322;&#36807;&#31243;&#20013;&#24456;&#23481;&#26131;&#24341;&#20837;&#38169;&#35823;&#65292;&#24182;&#19988;&#22312;&#23457;&#26680;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#34987;&#24573;&#35270;&#65292;&#23548;&#33268;&#20934;&#30830;&#24230;&#20302;&#19979;&#30340;&#22522;&#20934;&#21644;&#22522;&#20110;&#22122;&#22768;&#26631;&#31614;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#38477;&#20302;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#30340;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#30340;&#22522;&#20934;&#20197;&#21450;&#19968;&#20010;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#21644;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#24050;&#32463;&#26631;&#35760;&#33391;&#22909;&#30340;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#27169;&#25311;&#20102;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#38543;&#26426;&#24341;&#20837;&#30340;&#26631;&#31614;&#38169;&#35823;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#20551;&#35774;&#24050;&#32463;&#25552;&#20379;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#24182;&#32771;&#34385;&#20004;&#20010;&#38454;&#27573;&#30340;&#20998;&#31867;&#25439;&#22833;&#21644;&#22238;&#24402;&#25439;&#22833;&#30340;&#24635;&#21644;&#12290;&#36825;&#20123;&#25439;&#22833;&#22522;&#20110;&#39044;&#27979;&#21644;&#21253;&#25324;&#27169;&#25311;&#26631;&#31614;&#38169;&#35823;&#30340;&#22122;&#22768;&#26631;&#31614;&#36827;&#34892;&#35745;&#31639;&#65292;&#26088;&#22312;&#26816;&#27979;&#21518;&#32773;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#19977;&#20010;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#19968;&#20010;&#26080;&#28145;&#24230;&#23398;&#20064;&#30340;&#26420;&#32032;&#26041;&#27861;&#65292;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Labeling datasets for supervised object detection is a dull and time-consuming task. Errors can be easily introduced during annotation and overlooked during review, yielding inaccurate benchmarks and performance degradation of deep neural networks trained on noisy labels. In this work, we for the first time introduce a benchmark for label error detection methods on object detection datasets as well as a label error detection method and a number of baselines. We simulate four different types of randomly introduced label errors on train and test sets of well-labeled object detection datasets. For our label error detection method we assume a two-stage object detector to be given and consider the sum of both stages' classification and regression losses. The losses are computed with respect to the predictions and the noisy labels including simulated label errors, aiming at detecting the latter. We compare our method to three baselines: a naive one without deep learning, the object detector'
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#23545;&#27604;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#38752;&#30340;&#36127;&#26679;&#26412;&#23545;&#26469;&#25913;&#36827;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.09532</link><description>&lt;p&gt;
&#22522;&#20110;&#20266;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#20110;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pseudo Contrastive Learning for Graph-based Semi-supervised Learning. (arXiv:2302.09532v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#23545;&#27604;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#38752;&#30340;&#36127;&#26679;&#26412;&#23545;&#26469;&#25913;&#36827;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#26631;&#31614;&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24615;&#33021;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26681;&#25454;&#33258;&#20449;&#30340;&#39044;&#27979;&#29983;&#25104;&#38468;&#21152;&#30340;&#20266;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#31867;&#30446;&#26631;&#23545;&#32473;&#23450;&#26631;&#31614;&#30340;&#25935;&#24863;&#24615;&#65292;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#36136;&#37327;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#21487;&#38752;&#30340;&#20998;&#31867;&#30417;&#30563;&#8220;&#19968;&#20010;&#33410;&#28857;&#23646;&#20110;&#29305;&#23450;&#31867;&#8221;&#65292;&#25105;&#20204;&#26356;&#21916;&#27426;&#23481;&#38169;&#24615;&#23545;&#27604;&#30417;&#30563;&#8220;&#20004;&#20010;&#33410;&#28857;&#19981;&#23646;&#20110;&#21516;&#19968;&#31867;&#8221;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#25918;&#26494;&#30340;&#29256;&#26412;&#65292;&#21363;&#35782;&#21035;&#21487;&#38752;&#30340;&#36127;&#26679;&#26412;&#23545;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;GNNs&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#20266;&#23545;&#27604;&#23398;&#20064;(PCL)&#12290;&#23427;&#23558;&#30446;&#26631;&#20026;&#30456;&#21516;&#31867;&#30340;&#27491;&#20266;&#26631;&#31614;&#21644;&#36127;&#20266;&#26631;&#31614;&#30340;&#20004;&#20010;&#33410;&#28857;&#20998;&#24320;&#12290;&#20026;&#20102;&#23558;&#25299;&#25169;&#30693;&#35782;&#32435;&#20837;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25299;&#25169;&#21152;&#26435;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pseudo Labeling is a technique used to improve the performance of semi-supervised Graph Neural Networks (GNNs) by generating additional pseudo-labels based on confident predictions. However, the quality of generated pseudo-labels has been a longstanding concern due to the sensitivity of the classification objective with respect to the given labels. To avoid the untrustworthy classification supervision indicating ``a node belongs to a specific class,'' we favor the fault-tolerant contrasting supervision demonstrating ``two nodes do not belong to the same class.'' Thus, the problem of generating high-quality pseudo-labels is then transformed into a relaxed version, i.e., identifying reliable negative pairs. To achieve this, we propose a general framework for GNNs, termed Pseudo Contrastive Learning (PCL). It separates two nodes whose positive and negative pseudo-labels target the same class. To incorporate topological knowledge into learning, we devise a topologically weighted contrastiv
&lt;/p&gt;</description></item><item><title>Mithridates &#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#25913;&#21464;&#35757;&#32451;&#31649;&#36947;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33258;&#28982;&#25269;&#25239;&#21147;&#65292;&#35753;&#20174;&#19994;&#20154;&#21592;&#33021;&#22815;&#22238;&#31572;&#22914;&#20309;&#35780;&#20272;&#27169;&#22411;&#23545;&#21518;&#38376;&#20013;&#27602;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#20197;&#21450;&#22914;&#20309;&#25552;&#39640;&#25269;&#25239;&#21147;&#30340;&#21487;&#34892;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04977</link><description>&lt;p&gt;
Mithridates&#65306;&#22686;&#24378;&#23545;&#21518;&#38376;&#23398;&#20064;&#30340;&#33258;&#28982;&#25269;&#25239;
&lt;/p&gt;
&lt;p&gt;
Mithridates: Boosting Natural Resistance to Backdoor Learning. (arXiv:2302.04977v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04977
&lt;/p&gt;
&lt;p&gt;
Mithridates &#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#25913;&#21464;&#35757;&#32451;&#31649;&#36947;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33258;&#28982;&#25269;&#25239;&#21147;&#65292;&#35753;&#20174;&#19994;&#20154;&#21592;&#33021;&#22815;&#22238;&#31572;&#22914;&#20309;&#35780;&#20272;&#27169;&#22411;&#23545;&#21518;&#38376;&#20013;&#27602;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#20197;&#21450;&#22914;&#20309;&#25552;&#39640;&#25269;&#25239;&#21147;&#30340;&#21487;&#34892;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28508;&#22312;&#19981;&#21487;&#20449;&#26469;&#28304;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20013;&#27602;&#25915;&#20987;&#12290;&#35757;&#32451;&#36755;&#20837;&#30340;&#19968;&#20010;&#23567;&#30340;&#24694;&#24847;&#21046;&#20316;&#30340;&#23376;&#38598;&#21487;&#20197;&#23548;&#33268;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#8220;&#21518;&#38376;&#8221;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#38169;&#35823;&#20998;&#31867;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#30340;&#36755;&#20837;&#65289;&#65292;&#38500;&#20102;&#20854;&#20027;&#20219;&#21153;&#12290;&#34429;&#28982;&#21518;&#38376;&#25915;&#20987;&#20173;&#28982;&#20027;&#35201;&#26159;&#19968;&#31181;&#20551;&#35774;&#30340;&#23041;&#32961;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#38656;&#35201;&#23545;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#36827;&#34892;&#24040;&#22823;&#30340;&#25913;&#21464;&#65292;&#32780;&#19988;&#23545;&#20110;&#23454;&#38469;&#37096;&#32626;&#26469;&#35828;&#22826;&#22797;&#26434;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33258;&#28982;&#25269;&#25239;&#21147;&#65292;&#21363;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30340;&#25269;&#25239;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102; Mithridates&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#22238;&#31572;&#20004;&#20010;&#21487;&#34892;&#30340;&#38382;&#39064;&#65306;&#65288;1&#65289;&#25105;&#30340;&#27169;&#22411;&#22312;&#25269;&#24481;&#21518;&#38376;&#20013;&#27602;&#25915;&#20987;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;&#65288;2&#65289;&#22914;&#20309;&#25552;&#39640;&#23427;&#30340;&#25269;&#25239;&#21147;&#32780;&#19981;&#25913;&#21464;&#35757;&#32451;&#31649;&#36947;&#65311;Mithridates &#21033;&#29992;&#36229;&#21442;&#25968;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models trained on data from potentially untrusted sources are vulnerable to poisoning. A small, maliciously crafted subset of the training inputs can cause the model to learn a "backdoor" task (e.g., misclassify inputs with a certain feature) in addition to its main task. While backdoor attacks remain largely a hypothetical threat, state-of-the-art defenses require massive changes to the existing ML pipelines and are too complex for practical deployment.  In this paper, we take a pragmatic view and investigate natural resistance of ML pipelines to backdoor attacks, i.e., resistance that can be achieved without changes to how models are trained. We design, implement, and evaluate Mithridates, a new method that helps practitioners answer two actionable questions: (1) how well does my model resist backdoor poisoning attacks?, and (2) how can I increase its resistance without changing the training pipeline? Mithridates leverages hyperparameter search $\unicode{x2013}$
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#25581;&#31034;&#20102;&#26679;&#26412;&#25968;&#12289;&#38544;&#31169;&#32423;&#21035;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#19979;&#28216;&#25968;&#25454;&#38598;&#20197;&#21450;&#21487;&#23398;&#20064;&#21442;&#25968;&#23376;&#38598;&#31561;&#22240;&#32032;&#23545;&#20998;&#31867;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.01190</link><description>&lt;p&gt;
&#20851;&#20110;&#24046;&#20998;&#38544;&#31169;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Efficacy of Differentially Private Few-shot Image Classification. (arXiv:2302.01190v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#25581;&#31034;&#20102;&#26679;&#26412;&#25968;&#12289;&#38544;&#31169;&#32423;&#21035;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#19979;&#28216;&#25968;&#25454;&#38598;&#20197;&#21450;&#21487;&#23398;&#20064;&#21442;&#25968;&#23376;&#38598;&#31561;&#22240;&#32032;&#23545;&#20998;&#31867;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#20123;DP&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#26368;&#20339;&#30340;&#38750;&#31169;&#26377;&#27169;&#22411;&#12290;&#36825;&#20123;DP&#27169;&#22411;&#36890;&#24120;&#22312;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30456;&#23545;&#22823;&#19988;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#30340;&#31169;&#26377;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#21644;&#32852;&#21512;&#23398;&#20064;&#65292;&#37325;&#35201;&#30340;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#22320;&#34920;&#29616;&#65288;i.e. &#33719;&#21462;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#21487;&#33021;&#26377;&#38382;&#39064;&#65289;&#65292;&#19988;&#33021;&#22815;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#65288;&#21363;&#29992;&#20110;&#21508;&#31181;&#19987;&#19994;&#35774;&#32622;&#65289;&#36827;&#34892;&#33391;&#22909;&#30340;&#20998;&#31867;&#12290;&#20026;&#20102;&#20102;&#35299;&#23569;&#26679;&#26412;DP&#20309;&#26102;&#26377;&#25928;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#35814;&#23613;&#30340;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#27599;&#31867;&#26679;&#26412;&#25968;&#12289;&#38544;&#31169;&#32423;&#21035;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#19979;&#28216;&#25968;&#25454;&#38598;&#20197;&#21450;&#21487;&#23398;&#20064;&#21442;&#25968;&#23376;&#38598;&#31561;&#23545;&#23569;&#26679;&#26412;DP&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#26131;&#21463;&#25915;&#20987;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been significant recent progress in training differentially private (DP) models which achieve accuracy that approaches the best non-private models. These DP models are typically pretrained on large public datasets and then fine-tuned on private downstream datasets that are relatively large and similar in distribution to the pretraining data. However, in many applications including personalization and federated learning, it is crucial to perform well (i) in the few-shot setting, as obtaining large amounts of labeled data may be problematic; and (ii) on datasets from a wide variety of domains for use in various specialist settings. To understand under which conditions few-shot DP can be effective, we perform an exhaustive set of experiments that reveals how the accuracy and vulnerability to attack of few-shot DP image classification models are affected as the number of shots per class, privacy level, model architecture, downstream dataset, and subset of learnable parameters in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.09010</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#25351;&#25968;&#26631;&#20934;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39118;&#38505;&#20013;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#21644;&#31995;&#32479;&#21442;&#25968;&#25200;&#21160;&#30340;&#24433;&#21709;&#32780;&#19981;&#22815;&#31283;&#20581;&#12290;&#22240;&#27492;,&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#65292;&#26679;&#26412;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#27169;&#22411;&#39118;&#38505;&#25935;&#24863;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#21464;&#20307;&#65292;&#20854;&#23454;&#29616;&#36807;&#31243;&#31867;&#20284;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#25968;&#26631;&#20934;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#31574;&#30053;&#39118;&#38505;&#25935;&#24863;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#33945;&#29305;&#21345;&#32599;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#22312;&#32447;(&#26102;&#38388;&#24046;&#20998;)&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#25968;&#26631;&#20934;&#30340;&#20351;&#29992;&#33021;&#22815;&#25512;&#24191;&#24120;&#29992;&#30340;&#29305;&#23450;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#22312;&#25670;&#21160;&#26438;&#21644;&#25670;&#25670;&#26438;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#23454;&#29616;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;</title><link>http://arxiv.org/abs/2212.01071</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#39578;&#25200;&#34892;&#20026;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#65292;&#36825;&#23548;&#33268;&#20102;&#34394;&#20551;&#26816;&#27979;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#20013;&#24341;&#20154;&#27880;&#30446;&#30340;&#39046;&#22495;&#12290;&#25968;&#25454;&#30340;&#22270;&#24418;&#29305;&#24615;&#20197;&#21450;&#22823;&#37327;&#33410;&#28857;&#23548;&#33268;&#20102;&#35768;&#22810;&#38556;&#30861;&#65292;&#21253;&#25324;&#30697;&#38453;&#20013;&#22823;&#37327;&#26080;&#20851;&#29305;&#24449;&#30340;&#39640;&#31163;&#25955;&#24230;&#21644;&#19981;&#24179;&#34913;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31639;&#27861;&#30340;&#32452;&#21512;&#65292;&#21363;SGAN&#12290;&#26412;&#25991;&#23558;&#23569;&#37327;&#26631;&#31614;&#24212;&#29992;&#20110;SGAN&#20316;&#20026;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#34394;&#20551;&#36134;&#25143;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media grows faster, harassment becomes more prevalent which leads to considered fake detection a fascinating field among researchers. The graph nature of data with the large number of nodes caused different obstacles including a considerable amount of unrelated features in matrices as high dispersion and imbalance classes in the dataset. To deal with these issues Auto-encoders and a combination of semi-supervised learning and the GAN algorithm which is called SGAN were used. This paper is deploying a smaller number of labels and applying SGAN as a classifier. The result of this test showed that the accuracy had reached 91\% in detecting fake accounts using only 100 labeled samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DARE-GP&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#26234;&#33021;&#38899;&#31665;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36867;&#36991;&#19982;&#26234;&#33021;&#38899;&#31665;&#30456;&#36830;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2211.09273</link><description>&lt;p&gt;
&#38024;&#23545;&#22768;&#23398;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#36867;&#36991;&#30340;&#23454;&#26102;&#35821;&#38899;&#24773;&#24863;&#26816;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning. (arXiv:2211.09273v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DARE-GP&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#26234;&#33021;&#38899;&#31665;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36867;&#36991;&#19982;&#26234;&#33021;&#38899;&#31665;&#30456;&#36830;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#30417;&#35270;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#28041;&#21450;&#21040;&#24191;&#27867;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#22312;&#26222;&#36941;&#23384;&#22312;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#22810;&#20256;&#24863;&#22120;&#30340;&#25903;&#25345;&#19979;&#65292;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#32780;&#36825;&#20123;&#35774;&#22791;&#21487;&#20197;&#25903;&#25345;&#36825;&#20123;&#30417;&#35270;&#29992;&#20363;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#36825;&#26679;&#30340;&#24212;&#29992;&#26696;&#20363;&#65306;&#20351;&#29992;&#19982;&#26234;&#33021;&#38899;&#31665;&#30456;&#36830;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19981;&#24433;&#21709;&#26234;&#33021;&#38899;&#31665;&#30340;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36867;&#36991;&#19982;&#26234;&#33021;&#38899;&#31665;&#30456;&#36830;&#30340;&#40657;&#30418;SER&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#23545;&#25239;&#36867;&#36991;&#30340;&#35270;&#35282;&#26469;&#32771;&#34385;&#36825;&#20010;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21517;&#20026;&#8220;&#36890;&#36807;&#36951;&#20256;&#35268;&#21010;&#20987;&#36133;&#22768;&#23398;&#24773;&#24863;&#35782;&#21035;&#30340;DARE-GP&#8221;&#65292;&#23427;&#20351;&#29992;&#36951;&#20256;&#35268;&#21010;&#29983;&#25104;&#38750;&#20405;&#20837;&#24615;&#30340;&#28155;&#21152;&#38899;&#39057;&#25200;&#21160;&#65288;AAPs&#65289;&#12290;&#36890;&#36807;&#32422;&#26463;&#36825;&#20123;AAP&#30340;&#36827;&#21270;&#65292;&#21487;&#20197;&#20445;&#25252;&#36716;&#24405;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;SER&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;AAP&#30340;&#21152;&#24615;&#29305;&#24615;&#65292;&#20197;&#21450;&#20026;&#29305;&#23450;&#30446;&#26631;&#29983;&#25104;&#36825;&#20123;AAPs&#30340;&#26041;&#27861;&#65292;&#20351;DARE-GP&#25104;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotional Surveillance is an emerging area with wide-reaching privacy concerns. These concerns are exacerbated by ubiquitous IoT devices with multiple sensors that can support these surveillance use cases. The work presented here considers one such use case: the use of a speech emotion recognition (SER) classifier tied to a smart speaker. This work demonstrates the ability to evade black-box SER classifiers tied to a smart speaker without compromising the utility of the smart speaker. This privacy concern is considered through the lens of adversarial evasion of machine learning. Our solution, Defeating Acoustic Recognition of Emotion via Genetic Programming (DARE-GP), uses genetic programming to generate non-invasive additive audio perturbations (AAPs). By constraining the evolution of these AAPs, transcription accuracy can be protected while simultaneously degrading SER classifier performance. The additive nature of these AAPs, along with an approach that generates these AAPs for a fi
&lt;/p&gt;</description></item><item><title>&#26497;&#21270;&#32534;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#20998;&#31867;&#38382;&#39064;&#30340;&#31616;&#21333;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#33021;&#20445;&#30041;&#32570;&#22833;&#20449;&#24687;&#12289;&#26080;&#38656;&#25554;&#34917;&#65292;&#35753;&#20915;&#31574;&#26641;&#33258;&#30001;&#36873;&#25321;&#22914;&#20309;&#22788;&#29702;&#32570;&#22833;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.01905</link><description>&lt;p&gt;
&#26497;&#21270;&#32534;&#30721;&#65306;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#32570;&#22833;&#20540;&#20998;&#31867;&#30340;&#31616;&#21333;&#22522;&#32447;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Polar Encoding: A Simple Baseline Approach for Classification with Missing Values. (arXiv:2210.01905v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01905
&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#32534;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#20998;&#31867;&#38382;&#39064;&#30340;&#31616;&#21333;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#33021;&#20445;&#30041;&#32570;&#22833;&#20449;&#24687;&#12289;&#26080;&#38656;&#25554;&#34917;&#65292;&#35753;&#20915;&#31574;&#26641;&#33258;&#30001;&#36873;&#25321;&#22914;&#20309;&#22788;&#29702;&#32570;&#22833;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26497;&#21270;&#32534;&#30721;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#20998;&#31867;&#21644;&#25968;&#20540;&#22411;$[0,1]$&#20540;&#23646;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#19968;&#31181;&#24456;&#22909;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#19982;&#20219;&#20309;&#20998;&#31867;&#31639;&#27861;&#37197;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#20445;&#30041;&#32570;&#22833;&#20449;&#24687;&#65292;&#38750;&#24120;&#31616;&#21333;&#26131;&#29992;&#24182;&#19988;&#24615;&#33021;&#33391;&#22909;&#12290;&#19982;&#29616;&#26377;&#30340;&#32570;&#22833;&#25351;&#31034;&#26041;&#27861;&#19981;&#21516;&#65292;&#26497;&#21270;&#32534;&#30721;&#19981;&#38656;&#35201;&#25554;&#34917;&#65292;&#30830;&#20445;&#32570;&#22833;&#20540;&#19982;&#38750;&#32570;&#22833;&#20540;&#31561;&#36317;&#31163;&#65292;&#35753;&#20915;&#31574;&#26641;&#31639;&#27861;&#33258;&#30001;&#36873;&#25321;&#22914;&#20309;&#20998;&#21106;&#32570;&#22833;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#8220;&#23646;&#24615;&#20013;&#21253;&#21547;&#32570;&#22833;&#24615;&#8221;&#65288;MIA&#65289;&#30340;&#23454;&#38469;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20998;&#31867;&#21644;$[0,1]$&#20540;&#23646;&#24615;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#21333;&#19968;&#23646;&#24615;&#31867;&#22411;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#23545;&#24212;&#20110;&#32463;&#20856;&#30340;&#37325;&#24515;&#22352;&#26631;&#27010;&#24565;&#65292;&#36825;&#25552;&#20379;&#20102;&#26497;&#21270;&#32534;&#30721;&#30340;&#27169;&#31946;&#21270;&#24418;&#24335;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose polar encoding, a representation of categorical and numerical $[0,1]$-valued attributes with missing values to be used in a classification context. We argue that this is a good baseline approach, because it can be used with any classification algorithm, preserves missingness information, is very simple to apply and offers good performance. In particular, unlike the existing missing-indicator approach, it does not require imputation, ensures that missing values are equidistant from non-missing values, and lets decision tree algorithms choose how to split missing values, thereby providing a practical realisation of the "missingness incorporated in attributes" (MIA) proposal. Furthermore, we show that categorical and $[0,1]$-valued attributes can be viewed as special cases of a single attribute type, corresponding to the classical concept of barycentric coordinates, and that this offers a natural interpretation of polar encoding as a fuzzified form of one-hot encoding. With an 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.08012</link><description>&lt;p&gt;
&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21033;&#29992;&#32452;&#21512;&#24615;&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#25512;&#24191;&#21040;&#26032;&#39062;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#20551;&#35774;&#25105;&#20204;&#30340;&#32463;&#39564;&#21487;&#20197;&#20998;&#35299;&#20026;&#22522;&#26412;&#30340;&#21407;&#23376;&#32452;&#20214;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#20197;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#37325;&#26032;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#21442;&#19982;&#26032;&#39062;&#32463;&#39564;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#23398;&#20064;&#20197;&#32452;&#21512;&#26041;&#24335;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#30340;&#34892;&#20026;&#31216;&#20026;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#65288;CLBs&#65289;&#12290;&#23398;&#20064;CLBs&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#65288;BP&#65289;&#12290;&#23613;&#31649;&#36825;&#26159;&#20154;&#31867;&#36731;&#26494;&#23436;&#25104;&#30340;&#26234;&#33021;&#22766;&#20030;&#65292;&#20294;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#35828;&#24182;&#38750;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#25105;&#20204;&#24314;&#35758;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#30740;&#31350;&#20195;&#29702;&#21830;&#36890;&#36807;&#35299;&#20915;BP&#30340;&#39046;&#22495;&#26080;&#20851;&#29256;&#26412;&#26469;&#23637;&#31034;CLBs&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21463;&#21040;&#25351;&#20195;&#28216;&#25103;&#30340;&#35821;&#35328;&#28044;&#29616;&#21644;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#25193;&#23637;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Human beings use compositionality to generalise from past experiences to novel experiences. We assume a separation of our experiences into fundamental atomic components that can be recombined in novel ways to support our ability to engage with novel experiences. We frame this as the ability to learn to generalise compositionally, and we will refer to behaviours making use of this ability as compositional learning behaviours (CLBs). A central problem to learning CLBs is the resolution of a binding problem (BP). While it is another feat of intelligence that human beings perform with ease, it is not the case for state-of-the-art artificial agents. Thus, in order to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP. We take inspiration from the language emergence and grounding framework of referential games and propose a meta-learning extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2205.15677</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#21028;&#21035;&#22120;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#21487;&#24494;&#22686;&#24378;&#25216;&#26415;&#25913;&#21892;&#20102;GAN&#35757;&#32451;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#20294;&#26159;&#65292;&#22686;&#24378;&#25216;&#26415;&#38544;&#24335;&#22320;&#24341;&#20837;&#20102;&#19981;&#33391;&#19981;&#21464;&#24615;&#22240;&#32032;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#30001;&#25968;&#25454;&#36716;&#25442;&#24341;&#36215;&#30340;&#26631;&#31614;&#31354;&#38388;&#35821;&#20041;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#21028;&#21035;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#26368;&#32456;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#21464;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#32487;&#25215;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#21487;&#20197;&#39044;&#27979;&#22686;&#24378;&#25968;&#25454;&#30340;&#21442;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#25968;&#25454;&#30340;&#39044;&#27979;&#30446;&#26631;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#21306;&#21035;&#24320;&#26469;&#12290;&#25105;&#20204;&#36824;&#40723;&#21169;&#29983;&#25104;&#22120;&#23545;&#25239;&#22320;&#29983;&#25104;&#20854;&#22686;&#24378;&#21442;&#25968;&#21487;&#20197;&#34987;&#21028;&#21035;&#22120;&#20934;&#30830;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#37327;&#21644;&#26356;&#39640;&#25928;&#30340;&#21028;&#21035;&#22120;&#65292;&#25552;&#39640;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generative adversarial networks (GANs) with limited data is challenging because discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversari
&lt;/p&gt;</description></item></channel></rss>