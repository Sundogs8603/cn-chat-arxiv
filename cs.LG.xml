<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#25509;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#35814;&#23613;&#38381;&#29615;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#36817;&#20284;&#20026;&#19981;&#21516;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#20869;&#30340;&#20302;&#32500;&#25511;&#21046;&#22120;&#65292;&#24179;&#34913;&#20102;&#36924;&#36817;&#31934;&#24230;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2311.04843</link><description>&lt;p&gt;
&#36328;&#36234;&#32500;&#24230;&#65306;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#21487;&#20449;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging Dimensions: Confident Reachability for High-Dimensional Controllers
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.04843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#25509;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#35814;&#23613;&#38381;&#29615;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#36817;&#20284;&#20026;&#19981;&#21516;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#20869;&#30340;&#20302;&#32500;&#25511;&#21046;&#22120;&#65292;&#24179;&#34913;&#20102;&#36924;&#36817;&#31934;&#24230;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#36827;&#34892;&#23454;&#29616;&#12290;&#36825;&#26679;&#30340;&#25511;&#21046;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#36890;&#36807;&#22270;&#20687;&#20316;&#20026;&#20027;&#35201;&#24863;&#30693;&#27169;&#24335;&#22312;&#30495;&#23454;&#31995;&#32479;&#19978;&#25191;&#34892;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#36825;&#31181;&#25511;&#21046;&#22120;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24037;&#20855;&#22312;&#22788;&#29702;&#20855;&#26377;&#25968;&#21315;&#20010;&#32500;&#24230;&#30340;&#36755;&#20837;&#26102;&#26080;&#27861;&#25193;&#23637;&#65292;&#29305;&#21035;&#26159;&#24403;&#21508;&#20010;&#36755;&#20837;&#65288;&#22914;&#20687;&#32032;&#65289;&#32570;&#20047;&#26126;&#30830;&#30340;&#29289;&#29702;&#24847;&#20041;&#26102;&#12290;&#26412;&#25991;&#22312;&#36830;&#25509;&#35814;&#23613;&#30340;&#38381;&#29615;&#39564;&#35777;&#19982;&#39640;&#32500;&#25511;&#21046;&#22120;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#65292;&#39640;&#32500;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#21487;&#20197;&#29992;&#19981;&#21516;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#20869;&#30340;&#20960;&#20010;&#20302;&#32500;&#25511;&#21046;&#22120;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#24179;&#34913;&#20302;&#32500;&#25511;&#21046;&#22120;&#30340;&#36924;&#36817;&#31934;&#24230;&#21644;&#21487;&#39564;&#35777;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#26032;&#30340;&#39564;&#35777;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;&#12290;&#28982;&#21518;&#65292;&#22914;&#26524;&#20302;&#32500;&#21487;&#36798;&#24615;&#32467;&#26524;&#24050;&#32463;&#24471;&#21040;&#20102;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous systems are increasingly implemented using end-to-end learning-based controllers. Such controllers make decisions that are executed on the real system with images as one of the primary sensing modalities. Deep neural networks form a fundamental building block of such controllers. Unfortunately, the existing neural-network verification tools do not scale to inputs with thousands of dimensions -- especially when the individual inputs (such as pixels) are devoid of clear physical meaning. This paper takes a step towards connecting exhaustive closed-loop verification with high-dimensional controllers. Our key insight is that the behavior of a high-dimensional controller can be approximated with several low-dimensional controllers in different regions of the state space. To balance the approximation accuracy and verifiability of our low-dimensional controllers, we leverage the latest verification-aware knowledge distillation. Then, if low-dimensional reachability results are infl
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#27450;&#35784;&#20132;&#26131;&#35782;&#21035;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02595</link><description>&lt;p&gt;
QFNN-FFD&#65306;&#29992;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#30340;&#37327;&#23376;&#32852;&#37030;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02595
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#27450;&#35784;&#20132;&#26131;&#35782;&#21035;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#65292;&#36825;&#26159;&#19968;&#20010;&#34701;&#21512;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21069;&#27839;&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#26032;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#12290;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;FL&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;QFNN-FFD&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#35782;&#21035;&#27450;&#35784;&#20132;&#26131;&#30340;&#26041;&#27861;&#12290;&#22312;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#23454;&#26045;&#21452;&#38454;&#27573;&#35757;&#32451;&#27169;&#22411;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24615;&#33021;&#26041;&#27861;&#12290;QFNN-FFD&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#65292;&#26631;&#24535;&#30528;&#37329;&#34701;&#31185;&#25216;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#22823;&#36827;&#27493;&#65292;&#24182;&#20026;&#20197;&#38544;&#31169;&#20026;&#37325;&#28857;&#30340;&#27450;&#35784;&#26816;&#27979;&#24314;&#31435;&#20102;&#26032;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02595v1 Announce Type: cross  Abstract: This study introduces the Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine Learning (QML) and quantum computing with Federated Learning (FL) to innovate financial fraud detection. Using quantum technologies' computational power and FL's data privacy, QFNN-FFD presents a secure, efficient method for identifying fraudulent transactions. Implementing a dual-phase training model across distributed clients surpasses existing methods in performance. QFNN-FFD significantly improves fraud detection and ensures data confidentiality, marking a significant advancement in fintech solutions and establishing a new standard for privacy-focused fraud detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#36890;&#36807;&#22312;&#26799;&#24230;&#26356;&#26032;&#20013;&#24212;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25200;&#21160;&#65292;&#20445;&#25252;&#26377;&#29992;&#21333;&#20803;&#20197;&#38450;&#36951;&#24536;&#65292;&#21516;&#26102;&#24674;&#22797;&#19981;&#22826;&#26377;&#29992;&#21333;&#20803;&#30340;&#21487;&#22609;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00781</link><description>&lt;p&gt;
&#22788;&#29702;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#36890;&#36807;&#22312;&#26799;&#24230;&#26356;&#26032;&#20013;&#24212;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25200;&#21160;&#65292;&#20445;&#25252;&#26377;&#29992;&#21333;&#20803;&#20197;&#38450;&#36951;&#24536;&#65292;&#21516;&#26102;&#24674;&#22797;&#19981;&#22826;&#26377;&#29992;&#21333;&#20803;&#30340;&#21487;&#22609;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#26082;&#36973;&#21463;&#26377;&#29992;&#21333;&#20803;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21448;&#22240;&#20725;&#21270;&#21644;&#26080;&#29992;&#21333;&#20803;&#23548;&#33268;&#21487;&#22609;&#24615;&#20002;&#22833;&#12290;&#34429;&#28982;&#35768;&#22810;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#33021;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#25345;&#32493;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;UPGD&#32467;&#21512;&#20102;&#26799;&#24230;&#26356;&#26032;&#21644;&#25200;&#21160;&#65292;&#23427;&#23545;&#26356;&#26377;&#29992;&#30340;&#21333;&#20803;&#24212;&#29992;&#36739;&#23567;&#30340;&#20462;&#25913;&#65292;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#36951;&#24536;&#65292;&#23545;&#19981;&#22826;&#26377;&#29992;&#30340;&#21333;&#20803;&#24212;&#29992;&#36739;&#22823;&#30340;&#20462;&#25913;&#65292;&#24674;&#22797;&#23427;&#20204;&#30340;&#21487;&#22609;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00781v1 Announce Type: cross  Abstract: Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36801;&#31227;&#23398;&#20064;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20026;&#36807;&#31243;&#25511;&#21046;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00247</link><description>&lt;p&gt;
&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#20419;&#36827;&#36807;&#31243;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36801;&#31227;&#23398;&#20064;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20026;&#36807;&#31243;&#25511;&#21046;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36801;&#31227;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#20026;&#36807;&#31243;&#25511;&#21046;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#36807;&#31243;&#24037;&#19994;&#39046;&#22495;&#24212;&#29992;DRL&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#24341;&#20837;&#36801;&#31227;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#24314;&#35758;&#21644;&#23637;&#26395;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36801;&#31227;&#23398;&#20064;&#19982;DRL&#32467;&#21512;&#36215;&#26469;&#21152;&#24378;&#36807;&#31243;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00247v1 Announce Type: cross  Abstract: This paper provides insights into deep reinforcement learning (DRL) for process control from the perspective of transfer learning. We analyze the challenges of applying DRL in the field of process industries and the necessity of introducing transfer learning. Furthermore, recommendations and prospects are provided for future research directions on how transfer learning can be integrated with DRL to empower process control.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TransDeformer&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#33136;&#26894;&#36718;&#24275;&#30340;&#39640;&#31354;&#38388;&#20934;&#30830;&#24615;&#37325;&#24314;&#65292;&#24182;&#36328;&#24739;&#32773;&#23454;&#29616;&#20102;&#32593;&#26684;&#23545;&#24212;&#65292;&#20026;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#25552;&#20379;&#20102;&#21487;&#38752;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#21464;&#20307;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.00231</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24418;&#29366;&#21464;&#24418;&#32593;&#32476;&#29992;&#20110;&#26080;&#20266;&#24433;&#20960;&#20309;&#37325;&#26500;&#39592;&#30406;&#33136;&#26894;MR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00231
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TransDeformer&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#33136;&#26894;&#36718;&#24275;&#30340;&#39640;&#31354;&#38388;&#20934;&#30830;&#24615;&#37325;&#24314;&#65292;&#24182;&#36328;&#24739;&#32773;&#23454;&#29616;&#20102;&#32593;&#26684;&#23545;&#24212;&#65292;&#20026;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#25552;&#20379;&#20102;&#21487;&#38752;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#21464;&#20307;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33136;&#26894;&#26894;&#38388;&#30424;&#36864;&#21464;&#65292;&#26159;&#33136;&#26894;&#38388;&#30424;&#28176;&#36827;&#24615;&#32467;&#26500;&#24615;&#30952;&#25439;&#65292;&#34987;&#35748;&#20026;&#22312;&#33136;&#37096;&#30140;&#30171;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20840;&#29699;&#20581;&#24247;&#20851;&#27880;&#28966;&#28857;&#12290;&#20174;MR&#22270;&#20687;&#20013;&#33258;&#21160;&#37325;&#24314;&#33136;&#26894;&#20960;&#20309;&#24418;&#29366;&#65292;&#23558;&#20351;&#21307;&#23398;&#21442;&#25968;&#30340;&#24555;&#36895;&#27979;&#37327;&#25104;&#20026;&#21487;&#33021;&#65292;&#20197;&#35780;&#20272;&#33136;&#26894;&#29366;&#24577;&#65292;&#20174;&#32780;&#30830;&#23450;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#25216;&#26415;&#36890;&#24120;&#20250;&#29983;&#25104;&#38169;&#35823;&#30340;&#20998;&#21106;&#25110;&#19981;&#36866;&#21512;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#30340;&#26080;&#32467;&#26500;&#28857;&#20113;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransDeformer&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#39640;&#31354;&#38388;&#20934;&#30830;&#24230;&#21644;&#24739;&#32773;&#38388;&#32593;&#26684;&#23545;&#24212;&#30340;&#26041;&#24335;&#37325;&#24314;&#33136;&#26894;&#36718;&#24275;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;TransDeformer&#30340;&#21464;&#31181;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#26032;&#30340;&#27880;&#24847;&#21147;&#20844;&#24335;&#65292;&#23558;&#22270;&#20687;&#29305;&#24449;&#21644;&#26631;&#35760;&#21270;&#30340;&#36718;&#24275;&#29305;&#24449;&#38598;&#25104;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00231v1 Announce Type: cross  Abstract: Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13890</link><description>&lt;p&gt;
&#20197;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#23545;&#27604;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#23545;&#27604;&#21058;&#21487;&#20197;&#23450;&#20301;&#32959;&#30244;&#24182;&#35266;&#23519;&#20854;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#36825;&#23545;&#20110;&#30284;&#30151;&#34920;&#24449;&#21644;&#27835;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#21058;&#30340;&#20351;&#29992;&#19981;&#20165;&#19982;&#19981;&#33391;&#20581;&#24247;&#39118;&#38505;&#30456;&#20851;&#65292;&#32780;&#19988;&#23545;&#20110;&#24576;&#23381;&#24739;&#32773;&#12289;&#32958;&#21151;&#33021;&#38556;&#30861;&#24739;&#32773;&#25110;&#20854;&#20182;&#19981;&#33391;&#21453;&#24212;&#24739;&#32773;&#23384;&#22312;&#38480;&#21046;&#12290;&#30001;&#20110;&#23545;&#27604;&#21058;&#25668;&#21462;&#26159;&#30149;&#28790;&#24694;&#24615;&#12289;&#30284;&#30151;&#22797;&#21457;&#39118;&#38505;&#21644;&#27835;&#30103;&#21453;&#24212;&#30340;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22240;&#27492;&#20943;&#23569;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#36827;&#34892;DCE-MRI&#26102;&#38388;&#24207;&#21015;&#30340;&#33719;&#21462;&#26102;&#38388;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#30340;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#21464;&#24322;&#24615;&#30340;Fr\'echet&#25918;&#23556;&#32452;&#23398;&#36317;&#31163;&#20316;&#20026;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13890v1 Announce Type: cross  Abstract: Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow to localize tumors and observe their contrast kinetics, which is essential for cancer characterization and respective treatment decision-making. However, contrast agent administration is not only associated with adverse health risks, but also restricted for patients during pregnancy, and for those with kidney malfunction, or other adverse reactions. With contrast uptake as key biomarker for lesion malignancy, cancer recurrence risk, and treatment response, it becomes pivotal to reduce the dependency on intravenous contrast agent administration. To this end, we propose a multi-conditional latent diffusion model capable of acquisition time-conditioned image synthesis of DCE-MRI temporal sequences. To evaluate medical image synthesis, we additionally propose and validate the Fr\'echet radiomics distance as an image quality measure based on biomarker variability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#26426;&#22120;&#36817;&#20284;&#36951;&#24536;&#23457;&#35745;&#20219;&#21153;&#30340;&#26126;&#30830;&#23450;&#20041;&#21644;&#26377;&#25928;&#25351;&#26631;&#65292;&#36890;&#36807;&#25913;&#21464;&#23457;&#35745;&#25361;&#25112;&#30340;&#26041;&#24335;&#20026;&#38750;&#25104;&#21592;&#25512;&#26029;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#23457;&#35745;&#25351;&#26631;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;</title><link>https://arxiv.org/abs/2403.12830</link><description>&lt;p&gt;
&#26426;&#22120;&#36817;&#20284;&#36951;&#24536;&#24050;&#32463;&#24471;&#21040;&#36866;&#24403;&#35780;&#20272;&#20102;&#21527;&#65311;&#20174;&#23457;&#35745;&#21040;&#21103;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#26426;&#22120;&#36817;&#20284;&#36951;&#24536;&#23457;&#35745;&#20219;&#21153;&#30340;&#26126;&#30830;&#23450;&#20041;&#21644;&#26377;&#25928;&#25351;&#26631;&#65292;&#36890;&#36807;&#25913;&#21464;&#23457;&#35745;&#25361;&#25112;&#30340;&#26041;&#24335;&#20026;&#38750;&#25104;&#21592;&#25512;&#26029;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#23457;&#35745;&#25351;&#26631;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#26085;&#30410;&#20851;&#27880;&#65292;&#24443;&#24213;&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#34880;&#32479;&#30340;&#26426;&#22120;&#36817;&#20284;&#36951;&#24536;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;MLaaS&#25552;&#20379;&#32773;&#24076;&#26395;&#23558;&#20854;&#35270;&#20026;&#31526;&#21512;&#30417;&#31649;&#21512;&#35268;&#24615;&#30340;&#26368;&#32456;&#20445;&#38556;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#33267;&#20851;&#37325;&#35201;&#65292;&#38544;&#31169;&#31038;&#21306;&#39564;&#35777;&#26426;&#22120;&#36817;&#20284;&#36951;&#24536;&#25928;&#26524;&#30340;&#26041;&#27861;&#21457;&#23637;&#21644;&#23454;&#26045;&#30340;&#36895;&#24230;&#20196;&#20154;&#22833;&#26395;&#65292;&#36825;&#19968;&#20851;&#38190;&#39046;&#22495;&#32463;&#24120;&#26410;&#33021;&#24471;&#21040;&#36275;&#22815;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26126;&#30830;&#23450;&#20041;&#19988;&#26377;&#25928;&#30340;&#25351;&#26631;&#65292;&#20026;&#40657;&#30418;&#36951;&#24536;&#23457;&#35745;&#20219;&#21153;&#25552;&#20379;&#35299;&#20915;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#23457;&#35745;&#25361;&#25112;&#36716;&#21270;&#20026;&#38750;&#25104;&#21592;&#25512;&#26029;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20986;&#39640;&#25928;&#30340;&#23457;&#35745;&#25351;&#26631;&#12290;&#36890;&#36807;&#20165;&#20381;&#36182;&#21407;&#22987;&#21644;&#24050;&#36951;&#24536;&#27169;&#22411;--&#28040;&#38500;&#20102;&#35757;&#32451;&#39069;&#22806;&#38452;&#24433;&#27169;&#22411;&#30340;&#38656;&#35201;--&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12830v1 Announce Type: new  Abstract: The growing concerns surrounding data privacy and security have underscored the critical necessity for machine unlearning--aimed at fully removing data lineage from machine learning models. MLaaS providers expect this to be their ultimate safeguard for regulatory compliance. Despite its critical importance, the pace at which privacy communities have been developing and implementing strong methods to verify the effectiveness of machine unlearning has been disappointingly slow, with this vital area often receiving insufficient focus. This paper seeks to address this shortfall by introducing well-defined and effective metrics for black-box unlearning auditing tasks. We transform the auditing challenge into a question of non-membership inference and develop efficient metrics for auditing. By relying exclusively on the original and unlearned models--eliminating the need to train additional shadow models--our approach simplifies the evaluation
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;R2D2&#65292;&#29992;&#20110;&#35299;&#20915;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05452</link><description>&lt;p&gt;
&#29992;&#20110;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#24555;&#36895;&#31934;&#23494;&#25104;&#20687;&#30340;R2D2&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;R2D2&#65292;&#29992;&#20110;&#35299;&#20915;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#30005;&#24178;&#28041;&#25104;&#20687;&#38656;&#35201;&#35299;&#20915;&#26469;&#33258;&#22823;&#25968;&#25454;&#37327;&#30340;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#36870;&#38382;&#39064;&#12290;&#26368;&#36817;&#22522;&#20110;&#20248;&#21270;&#29702;&#35770;&#30340;&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#25104;&#20687;&#31934;&#24230;&#33021;&#21147;&#65292;&#36828;&#36828;&#36229;&#20986;&#20102;CLEAN&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#30001;&#25163;&#24037;&#35774;&#35745;&#30340;&#27491;&#21017;&#21270;&#31639;&#23376;&#25512;&#21160;&#30340;&#20808;&#36827;&#36817;&#31471;&#31639;&#27861;&#65292;&#22914;SARA&#31995;&#21015;&#65292;&#20197;&#21450;&#30001;&#23398;&#20064;&#27491;&#21017;&#21270;&#21435;&#22122;&#22120;&#25512;&#21160;&#30340;&#28151;&#21512;&#25554;&#25300;&#65288;PnP&#65289;&#31639;&#27861;&#65292;&#22914;AIRI&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#21644;PnP&#32467;&#26500;&#39640;&#24230;&#36845;&#20195;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22788;&#29702;&#26410;&#26469;&#20202;&#22120;&#39044;&#26399;&#30340;&#26497;&#31471;&#25968;&#25454;&#22823;&#23567;&#30340;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#8220;&#29992;&#20110;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#27531;&#24046;&#23545;&#27531;&#24046;DNN&#31995;&#21015;&#8221;&#12290;R2D2&#30340;&#37325;&#24314;&#34987;&#24418;&#25104;&#20026;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#32780;&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05452v1 Announce Type: cross  Abstract: Radio-interferometric (RI) imaging entails solving high-resolution high-dynamic range inverse problems from large data volumes. Recent image reconstruction techniques grounded in optimization theory have demonstrated remarkable capability for imaging precision, well beyond CLEAN's capability. These range from advanced proximal algorithms propelled by handcrafted regularization operators, such as the SARA family, to hybrid plug-and-play (PnP) algorithms propelled by learned regularization denoisers, such as AIRI. Optimization and PnP structures are however highly iterative, which hinders their ability to handle the extreme data sizes expected from future instruments. To address this scalability challenge, we introduce a novel deep learning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic range imaging'. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Netw
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38544;&#31169;&#28431;&#26007;&#26041;&#27861;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#33021;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#20998;&#24067;&#26465;&#20214;&#19979;&#22343;&#26377;&#25928;&#22320;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04778</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38544;&#31169;&#28431;&#26007;&#30340;&#39640;&#25928;&#20984;&#24046;&#20998;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Efficient Difference-of-Convex Solver for Privacy Funnel
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38544;&#31169;&#28431;&#26007;&#26041;&#27861;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#33021;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#20998;&#24067;&#26465;&#20214;&#19979;&#22343;&#26377;&#25928;&#22320;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38544;&#31169;&#28431;&#26007;&#65288;PF&#65289;&#26041;&#27861;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#20854;&#20984;&#24046;&#20998;&#65288;DC&#65289;&#32467;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;DC&#20998;&#31163;&#23548;&#33268;&#20102;&#38381;&#24335;&#26356;&#26032;&#26041;&#31243;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#20998;&#24067;&#35774;&#32622;&#12290;&#23545;&#20110;&#24050;&#30693;&#20998;&#24067;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#38750;&#36138;&#23146;&#27714;&#35299;&#22120;&#30340;&#25910;&#25947;&#24615;&#65288;&#23616;&#37096;&#31283;&#23450;&#28857;&#65289;&#65292;&#24182;&#22312;&#32463;&#39564;&#19978;&#23637;&#31034;&#23427;&#22312;&#34920;&#24449;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;DC&#26041;&#27861;&#27934;&#23519;&#21147;&#36866;&#29992;&#20110;&#20855;&#26377;&#26631;&#35760;&#32463;&#39564;&#26679;&#26412;&#30340;&#26410;&#30693;&#20998;&#24067;&#35774;&#32622;&#12290;&#21033;&#29992;&#36825;&#20123;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#27714;&#35299;&#22120;&#28385;&#36275;&#20102;PF&#30340;&#22522;&#26412;Markov&#20851;&#31995;&#65292;&#19982;&#20197;&#24448;&#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#27714;&#35299;&#22120;&#30456;&#27604;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04778v1 Announce Type: new  Abstract: We propose an efficient solver for the privacy funnel (PF) method, leveraging its difference-of-convex (DC) structure. The proposed DC separation results in a closed-form update equation, which allows straightforward application to both known and unknown distribution settings. For known distribution case, we prove the convergence (local stationary points) of the proposed non-greedy solver, and empirically show that it outperforms the state-of-the-art approaches in characterizing the privacy-utility trade-off. The insights of our DC approach apply to unknown distribution settings where labeled empirical samples are available instead. Leveraging the insights, our alternating minimization solver satisfies the fundamental Markov relation of PF in contrast to previous variational inference-based solvers. Empirically, we evaluate the proposed solver with MNIST and Fashion-MNIST datasets. Our results show that under a comparable reconstruction 
&lt;/p&gt;</description></item><item><title>SOFIM&#21033;&#29992;&#27491;&#21017;&#21270;Fisher&#20449;&#24687;&#30697;&#38453;&#21644;Sherman-Morrison&#30697;&#38453;&#27714;&#36870;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#38543;&#26426;&#20248;&#21270;&#20013;&#26799;&#24230;&#26356;&#26032;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#24102;&#26469;&#30340;&#38750;&#24179;&#31283;&#30446;&#26631;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02833</link><description>&lt;p&gt;
SOFIM: &#20351;&#29992;&#27491;&#21017;&#21270;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02833
&lt;/p&gt;
&lt;p&gt;
SOFIM&#21033;&#29992;&#27491;&#21017;&#21270;Fisher&#20449;&#24687;&#30697;&#38453;&#21644;Sherman-Morrison&#30697;&#38453;&#27714;&#36870;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#38543;&#26426;&#20248;&#21270;&#20013;&#26799;&#24230;&#26356;&#26032;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#24102;&#26469;&#30340;&#38750;&#24179;&#31283;&#30446;&#26631;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27491;&#21017;&#21270;Fisher&#20449;&#24687;&#30697;&#38453;&#65288;FIM&#65289;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;SOFIM&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;FIM&#26469;&#36924;&#36817;Hessian&#30697;&#38453;&#65292;&#20197;&#25214;&#21040;&#22823;&#35268;&#27169;&#38543;&#26426;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#29275;&#39039;&#26799;&#24230;&#26356;&#26032;&#12290;&#21487;&#20197;&#35270;&#20026;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;NGD&#65289;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#21270;FIM&#21644;&#30452;&#25509;&#36890;&#36807;Sherman-Morrison&#30697;&#38453;&#27714;&#36870;&#25214;&#21040;&#26799;&#24230;&#26356;&#26032;&#26041;&#21521;&#26469;&#35299;&#20915;&#23384;&#20648;&#21644;&#35745;&#31639;&#23436;&#25972;FIM&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20687;&#24191;&#21463;&#27426;&#36814;&#30340;Adam&#26041;&#27861;&#19968;&#26679;&#65292;SOFIM&#21033;&#29992;&#26799;&#24230;&#30340;&#31532;&#19968;&#26102;&#21051;&#26469;&#22788;&#29702;&#30001;&#24322;&#26500;&#25968;&#25454;&#24341;&#36215;&#30340;&#36328;&#23567;&#25209;&#27425;&#38750;&#24179;&#31283;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#27491;&#21017;&#21270;FIM&#21644;Sherman-Morrison&#30697;&#38453;&#27714;&#36870;&#30340;&#21033;&#29992;&#23548;&#33268;&#20102;&#25910;&#25947;&#36895;&#29575;&#30340;&#25913;&#21892;&#65292;&#21516;&#26102;space&#21644;time&#22797;&#26434;&#24230;&#19982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02833v1 Announce Type: new  Abstract: This paper introduces a new stochastic optimization method based on the regularized Fisher information matrix (FIM), named SOFIM, which can efficiently utilize the FIM to approximate the Hessian matrix for finding Newton's gradient update in large-scale stochastic optimization of machine learning models. It can be viewed as a variant of natural gradient descent (NGD), where the challenge of storing and calculating the full FIM is addressed through making use of the regularized FIM and directly finding the gradient update direction via Sherman-Morrison matrix inversion. Additionally, like the popular Adam method, SOFIM uses the first moment of the gradient to address the issue of non-stationary objectives across mini-batches due to heterogeneous data. The utilization of the regularized FIM and Sherman-Morrison matrix inversion leads to the improved convergence rate with the same space and time complexities as stochastic gradient descent (
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;</title><link>https://arxiv.org/abs/2402.10962</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#27979;&#37327;&#21644;&#25511;&#21046;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Measuring and Controlling Persona Drift in Language Model Dialogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#25215;&#25285;&#29305;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#12290;&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#65292;&#23427;&#20204;&#23558;&#26159;&#31283;&#23450;&#30340;&#65292;&#22240;&#27492;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#22312;&#25972;&#20010;&#23545;&#35805;&#36807;&#31243;&#20013;&#32487;&#32493;&#26681;&#25454;&#35268;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#36890;&#36807;&#20004;&#20010;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#33258;&#25105;&#23545;&#35805;&#26469;&#35780;&#20272;&#8220;&#20154;&#35774;&#8221;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#27169;&#22411;&#22914;LLaMA2-chat-70B&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;&#20843;&#36718;&#23545;&#35805;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#12290;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#30001;&#20110;&#38271;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#34928;&#20943;&#65292;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#19982;&#20004;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;ResQuNNs&#65292;&#22312;quanvolutional&#23618;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#24615;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36328;&#23618;&#26799;&#24230;&#35775;&#38382;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09146</link><description>&lt;p&gt;
ResQuNNs: &#23454;&#29616;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;ResQuNNs&#65292;&#22312;quanvolutional&#23618;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#24615;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36328;&#23618;&#26799;&#24230;&#35775;&#38382;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QuNNs&#65289;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;quanvolutional&#23618;&#24182;&#35299;&#20915;&#19982;&#20854;&#30456;&#20851;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;quanvolutional&#23618;&#34429;&#28982;&#26377;&#21161;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#20294;&#24448;&#24448;&#26159;&#38745;&#24577;&#30340;&#65292;&#36866;&#24212;&#24615;&#26377;&#38480;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#36825;&#20123;&#23618;&#20869;&#37096;&#36827;&#34892;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;QuNNs&#30340;&#28789;&#27963;&#24615;&#21644;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#21487;&#35757;&#32451;&#30340;quanvolutional&#23618;&#30340;&#24341;&#20837;&#32473;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38590;&#20197;&#22312;&#36825;&#20123;&#23618;&#20043;&#38388;&#35775;&#38382;&#26799;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Residual Quanvolutional Neural Networks (ResQuNNs)&#65292;&#21033;&#29992;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#22312;&#36825;&#20123;&#23618;&#20043;&#38388;&#28155;&#21152;&#36339;&#36807;&#36830;&#25509;&#20197;&#20419;&#36827;&#26799;&#24230;&#30340;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09146v1 Announce Type: new Abstract: In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#26680;&#22238;&#24402;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#35299;&#26512;&#20102;&#27169;&#22411;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#34394;&#20551;&#25968;&#25454;&#19982;&#24615;&#33021;&#23436;&#20840;&#23849;&#28291;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#32531;&#35299;&#20102;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;</title><link>https://arxiv.org/abs/2402.07712</link><description>&lt;p&gt;
&#27169;&#22411;&#23849;&#28291;&#35299;&#23494;&#65306;&#22238;&#24402;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model Collapse Demystified: The Case of Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#26680;&#22238;&#24402;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#35299;&#26512;&#20102;&#27169;&#22411;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#34394;&#20551;&#25968;&#25454;&#19982;&#24615;&#33021;&#23436;&#20840;&#23849;&#28291;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#32531;&#35299;&#20102;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;"&#27169;&#22411;&#23849;&#28291;"&#29616;&#35937;&#25351;&#30340;&#26159;&#27169;&#22411;&#22312;&#36882;&#24402;&#22320;&#35757;&#32451;&#33258;&#36523;&#19978;&#19968;&#20195;&#21448;&#19968;&#20195;&#29983;&#25104;&#30340;&#25968;&#25454;&#26102;&#65292;&#20854;&#24615;&#33021;&#36880;&#28176;&#38477;&#20302;&#65292;&#26368;&#32456;&#21464;&#24471;&#23436;&#20840;&#26080;&#29992;&#65292;&#21363;&#27169;&#22411;&#23849;&#28291;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#26680;&#22238;&#24402;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#33719;&#24471;&#20102;&#32467;&#26524;&#65292;&#26174;&#31034;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#34394;&#20551;&#25968;&#25454;&#19982;&#27169;&#22411;&#24615;&#33021;&#23436;&#20840;&#23849;&#28291;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20132;&#21449;&#28857;&#12290;&#22312;&#22810;&#39033;&#24335;&#34928;&#20943;&#30340;&#20809;&#35889;&#21644;&#28304;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20462;&#25913;&#21518;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#23637;&#31034;&#20102;&#20174;&#24555;&#36895;&#21040;&#32531;&#24930;&#36895;&#29575;&#30340;&#26032;&#20132;&#21449;&#29616;&#35937;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#31616;&#21333;&#31574;&#30053;&#26469;&#32531;&#35299;&#27169;&#22411;&#23849;&#28291;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of large language models like ChatGPT, the phenomenon of "model collapse" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;MRI&#25195;&#25551;&#20013;&#20934;&#30830;&#22320;&#20998;&#21106;&#33034;&#39635;&#31070;&#32463;&#26681;&#20998;&#25903;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00724</link><description>&lt;p&gt;
&#33034;&#39635;&#31070;&#32463;&#26681;&#20998;&#25903;&#30340;&#33258;&#21160;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Automatic Segmentation of the Spinal Cord Nerve Rootlets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;MRI&#25195;&#25551;&#20013;&#20934;&#30830;&#22320;&#20998;&#21106;&#33034;&#39635;&#31070;&#32463;&#26681;&#20998;&#25903;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35782;&#21035;&#33034;&#39635;&#31070;&#32463;&#26681;&#20998;&#25903;&#23545;&#20110;&#25551;&#32472;&#33034;&#39635;&#30340;&#21151;&#33021;&#27963;&#21160;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;T2&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25195;&#25551;&#20013;&#35821;&#20041;&#20998;&#21106;&#33034;&#39635;&#31070;&#32463;&#26681;&#20998;&#25903;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#24320;&#25918;&#23384;&#21462;&#30340;MRI&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#65292;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#20102;&#19968;&#20010;&#19977;&#32500;&#22810;&#31867;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20998;&#21106;C2-C8&#32972;&#38754;&#31070;&#32463;&#26681;&#20998;&#25903;&#12290;&#27599;&#20010;&#36755;&#20986;&#31867;&#21035;&#23545;&#24212;&#19968;&#20010;&#33034;&#39635;&#27700;&#24179;&#12290;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#30340;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#19981;&#21516;&#22330;&#22320;&#12289;&#19981;&#21516;&#20250;&#35805;&#21644;&#19981;&#21516;&#20998;&#36776;&#29575;&#20043;&#38388;&#30340;&#21464;&#24322;&#24615;&#12290;&#27979;&#35797;&#32467;&#26524;&#30340;Dice&#20998;&#25968;&#20026;0.67+-0.16&#65288;&#22343;&#20540;+-&#26631;&#20934;&#24046;&#65292;&#38024;&#23545;&#19981;&#21516;&#31070;&#32463;&#26681;&#20998;&#25903;&#27700;&#24179;&#65289;&#65292;&#34920;&#26126;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36824;&#23637;&#31034;&#20102;&#20302;&#21378;&#21830;&#38388;&#21644;&#22330;&#22320;&#38388;&#30340;&#21464;&#24322;&#24615;&#65288;&#21464;&#24322;&#31995;&#25968;&lt;=1.41%&#65289;&#65292;&#20197;&#21450;&#20302;&#20250;&#35805;&#38388;&#21464;&#24322;&#24615;&#65288;&#21464;&#24322;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
Precise identification of spinal nerve rootlets is relevant to delineate spinal levels for the study of functional activity in the spinal cord. The goal of this study was to develop an automatic method for the semantic segmentation of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI) scans. Images from two open-access MRI datasets were used to train a 3D multi-class convolutional neural network using an active learning approach to segment C2-C8 dorsal nerve rootlets. Each output class corresponds to a spinal level. The method was tested on 3T T2-weighted images from datasets unseen during training to assess inter-site, inter-session, and inter-resolution variability. The test Dice score was 0.67 +- 0.16 (mean +- standard deviation across rootlets levels), suggesting a good performance. The method also demonstrated low inter-vendor and inter-site variability (coefficient of variation &lt;= 1.41 %), as well as low inter-session variability (coefficient of variation &lt;= 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#38543;&#26426;&#35797;&#39564;&#35774;&#35745;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#65292;&#33021;&#22815;&#37327;&#21270;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#24378;&#24230;&#65292;&#24182;&#20272;&#35745;&#20854;&#19979;&#30028;&#65292;&#26377;&#25928;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#35782;&#21035;&#28151;&#28102;&#12290;</title><link>https://arxiv.org/abs/2312.03871</link><description>&lt;p&gt;
&#38544;&#34109;&#32780;&#21487;&#37327;&#21270;&#65306;&#20351;&#29992;&#38543;&#26426;&#35797;&#39564;&#30340;&#28151;&#28102;&#24378;&#24230;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Hidden yet quantifiable: A lower bound for confounding strength using randomized trials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03871
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38543;&#26426;&#35797;&#39564;&#35774;&#35745;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#65292;&#33021;&#22815;&#37327;&#21270;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#24378;&#24230;&#65292;&#24182;&#20272;&#35745;&#20854;&#19979;&#30028;&#65292;&#26377;&#25928;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#35782;&#21035;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#33410;&#22863;&#31934;&#20934;&#21307;&#23398;&#26102;&#20195;&#65292;&#35266;&#23519;&#24615;&#30740;&#31350;&#22312;&#27491;&#30830;&#35780;&#20272;&#20020;&#24202;&#23454;&#36341;&#20013;&#26032;&#30103;&#27861;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#20174;&#38750;&#38543;&#26426;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#22240;&#26524;&#32467;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#35797;&#39564;&#26469;&#37327;&#21270;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#30340;&#26032;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26469;&#26816;&#27979;&#24378;&#24230;&#36229;&#36807;&#32473;&#23450;&#38408;&#20540;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26816;&#39564;&#26469;&#20272;&#35745;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#24378;&#24230;&#30340;&#28176;&#36817;&#26377;&#25928;&#19979;&#30028;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#32479;&#35745;&#26816;&#39564;&#30340;&#21151;&#25928;&#21644;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#19979;&#30028;&#22914;&#20309;&#33021;&#22815;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#27491;&#30830;&#35782;&#21035;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#30340;&#23384;&#22312;&#21644;&#19981;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03871v2 Announce Type: replace-cross  Abstract: In the era of fast-paced precision medicine, observational studies play a major role in properly evaluating new treatments in clinical practice. Yet, unobserved confounding can significantly compromise causal conclusions drawn from non-randomized data. We propose a novel strategy that leverages randomized trials to quantify unobserved confounding. First, we design a statistical test to detect unobserved confounding with strength above a given threshold. Then, we use the test to estimate an asymptotically valid lower bound on the unobserved confounding strength. We evaluate the power and validity of our statistical test on several synthetic and semi-synthetic datasets. Further, we show how our lower bound can correctly identify the absence and presence of unobserved confounding in a real-world setting.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#20984;&#30446;&#26631;&#21644;&#20984;&#32422;&#26463;&#30340;&#26368;&#22351;&#24773;&#20917;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2310.10117</link><description>&lt;p&gt;
&#20855;&#26377;&#20984;&#20840;&#23616;&#21644;&#23616;&#37096;&#32422;&#26463;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Convex Global and Local Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#20984;&#30446;&#26631;&#21644;&#20984;&#32422;&#26463;&#30340;&#26368;&#22351;&#24773;&#20917;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#38382;&#39064;&#37117;&#24102;&#26377;&#32422;&#26463;&#65292;&#20854;&#24212;&#29992;&#39046;&#22495;&#28041;&#21450;&#26080;&#27861;&#19982;&#20182;&#20154;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#25935;&#24863;&#25968;&#25454;&#65292;&#20363;&#22914;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#12290;&#22312;&#36825;&#31181;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#21327;&#20316;&#23398;&#20064;&#28041;&#21450;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#29992;&#20110;&#24102;&#32422;&#26463;&#30340;ML&#38382;&#39064;&#65292;&#25110;&#31616;&#31216;&#24102;&#32422;&#26463;&#30340;FL&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;FL&#25216;&#26415;&#24471;&#21040;&#20102;&#24191;&#27867;&#21457;&#23637;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#20165;&#22788;&#29702;&#26080;&#32422;&#26463;&#30340;FL&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#35299;&#20915;&#24102;&#32422;&#26463;FL&#38382;&#39064;&#30340;&#36890;&#29992;&#31639;&#27861;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#36817;&#31471;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#65288;AL&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21463;&#38480;&#21046;ML&#38382;&#39064;&#30340;&#26032;FL&#31639;&#27861;&#12290;&#20551;&#35774;&#20984;&#30446;&#26631;&#21644;&#20984;&#32422;&#26463;&#20197;&#21450;&#20854;&#20182;&#19968;&#20123;&#28201;&#21644;&#26465;&#20214;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26368;&#22351;&#24773;&#20917;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10117v2 Announce Type: replace  Abstract: In practice, many machine learning (ML) problems come with constraints, and their applied domains involve distributed sensitive data that cannot be shared with others, e.g., in healthcare. Collaborative learning in such practical scenarios entails federated learning (FL) for ML problems with constraints, or FL with constraints for short. Despite the extensive developments of FL techniques in recent years, these techniques only deal with unconstrained FL problems. To fill this gap, we take the first step toward building a general algorithmic framework for solving FL problems with constraints. In particular, we propose a new FL algorithm for constrained ML problems based on the proximal augmented Lagrangian (AL) method. Assuming convex objective and convex constraints plus other mild conditions, we establish the worst-case complexity of the proposed algorithm. Our numerical experiments show the effectiveness of our algorithm in perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#30340;&#26032;&#22411;&#21512;&#35268;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;(CV-CRC)&#65292;&#23427;&#25193;&#23637;&#20102;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#33021;&#22815;&#25511;&#21046;&#26356;&#24191;&#27867;&#30340;&#39118;&#38505;&#20989;&#25968;&#65292;&#24182;&#22312;&#39044;&#27979;&#22120;&#38598;&#21512;&#30340;&#24179;&#22343;&#39118;&#38505;&#19978;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.11974</link><description>&lt;p&gt;
&#20132;&#21449;&#39564;&#35777;&#21512;&#35268;&#39118;&#38505;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Cross-Validation Conformal Risk Control. (arXiv:2401.11974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#30340;&#26032;&#22411;&#21512;&#35268;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;(CV-CRC)&#65292;&#23427;&#25193;&#23637;&#20102;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#33021;&#22815;&#25511;&#21046;&#26356;&#24191;&#27867;&#30340;&#39118;&#38505;&#20989;&#25968;&#65292;&#24182;&#22312;&#39044;&#27979;&#22120;&#38598;&#21512;&#30340;&#24179;&#22343;&#39118;&#38505;&#19978;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#35268;&#39118;&#38505;&#25511;&#21046;&#65288;CRC&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#25216;&#26415;&#65292;&#23427;&#24212;&#29992;&#20110;&#20256;&#32479;&#30340;&#28857;&#39044;&#27979;&#22120;&#19978;&#65292;&#20197;&#25552;&#20379;&#26657;&#20934;&#20445;&#35777;&#12290;&#22312;CRC&#20013;&#25512;&#24191;&#19968;&#33268;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#65292;&#36890;&#36807;&#20174;&#28857;&#39044;&#27979;&#22120;&#20013;&#25552;&#21462;&#19968;&#20010;&#39044;&#27979;&#22120;&#38598;&#21512;&#26469;&#25511;&#21046;&#39118;&#38505;&#20989;&#25968;&#65288;&#22914;&#35823;&#35206;&#30422;&#27010;&#29575;&#25110;&#38169;&#35823;&#36127;&#20363;&#29575;&#65289;&#65292;&#20174;&#32780;&#30830;&#20445;&#26657;&#20934;&#24615;&#12290;&#21407;&#22987;&#30340;CRC&#38656;&#35201;&#23558;&#21487;&#29992;&#25968;&#25454;&#38598;&#20998;&#20026;&#35757;&#32451;&#21644;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#24403;&#25968;&#25454;&#21487;&#29992;&#24615;&#26377;&#38480;&#26102;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#22120;&#38598;&#21512;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#32780;&#19981;&#26159;&#21407;&#22987;CRC&#30340;&#26032;&#22411;CRC&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#20132;&#21449;&#39564;&#35777;CRC&#65288;CV-CRC&#65289;&#23558;CP&#30340;&#19968;&#31181;&#29256;&#26412;&#25193;&#23637;&#21040;CRC&#65292;&#21487;&#20197;&#25511;&#21046;&#26356;&#24191;&#27867;&#30340;&#39118;&#38505;&#20989;&#25968;&#12290;CV-CRC&#34987;&#35777;&#26126;&#22312;&#39044;&#27979;&#22120;&#38598;&#21512;&#30340;&#24179;&#22343;&#39118;&#38505;&#19978;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;CV-CRC&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal risk control (CRC) is a recently proposed technique that applies post-hoc to a conventional point predictor to provide calibration guarantees. Generalizing conformal prediction (CP), with CRC, calibration is ensured for a set predictor that is extracted from the point predictor to control a risk function such as the probability of miscoverage or the false negative rate. The original CRC requires the available data set to be split between training and validation data sets. This can be problematic when data availability is limited, resulting in inefficient set predictors. In this paper, a novel CRC method is introduced that is based on cross-validation, rather than on validation as the original CRC. The proposed cross-validation CRC (CV-CRC) extends a version of the jackknife-minmax from CP to CRC, allowing for the control of a broader range of risk functions. CV-CRC is proved to offer theoretical guarantees on the average risk of the set predictor. Furthermore, numerical exper
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11648</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#20998;&#23618;&#27491;&#21017;&#21270;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#30340;&#35786;&#26029;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#21046;&#23450;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20027;&#21160;&#26410;&#26469;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35768;&#22810;&#30740;&#31350;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;EHR&#25968;&#25454;&#22266;&#26377;&#30340;&#24322;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#65292;&#24517;&#28982;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NECHO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20998;&#23618;&#27491;&#21017;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#19968;&#23545;&#21452;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#34701;&#21512;&#28085;&#30422;&#21307;&#23398;&#20195;&#30721;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#26041;&#38754;&#20449;&#24687;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22260;&#32469;&#30528;&#21307;&#23398;&#20195;&#30721;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21307;&#23398;&#26412;&#20307;&#20013;&#30340;&#29238;&#32423;&#20449;&#24687;&#26469;&#35268;&#33539;&#29305;&#23450;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;EHR&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#25968;&#21315;&#20301;AI&#20316;&#32773;&#23545;&#26410;&#26469;AI&#30340;&#39044;&#27979;&#26174;&#31034;&#65292;&#21040;2028&#24180;&#65292;AI&#31995;&#32479;&#26377;50%&#30340;&#20960;&#29575;&#23454;&#29616;&#22810;&#20010;&#37324;&#31243;&#30865;&#65292;&#21253;&#25324;&#33258;&#20027;&#26500;&#24314;&#20840;&#26032;&#30340;&#20184;&#27454;&#22788;&#29702;&#32593;&#31449;&#12289;&#21019;&#20316;&#19968;&#39318;&#19982;&#30693;&#21517;&#38899;&#20048;&#23478;&#30340;&#26032;&#27468;&#38590;&#20197;&#21306;&#20998;&#30340;&#27468;&#26354;&#65292;&#24182;&#33258;&#20027;&#19979;&#36733;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#26080;&#38656;&#36741;&#21161;&#30340;&#26426;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#30340;&#20960;&#29575;&#20272;&#35745;&#20026;10%&#21040;2047&#24180;&#20026;50%&#12290;</title><link>http://arxiv.org/abs/2401.02843</link><description>&lt;p&gt;
&#25968;&#21315;&#20301;AI&#20316;&#32773;&#23545;&#26410;&#26469;AI&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Thousands of AI Authors on the Future of AI. (arXiv:2401.02843v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02843
&lt;/p&gt;
&lt;p&gt;
&#25968;&#21315;&#20301;AI&#20316;&#32773;&#23545;&#26410;&#26469;AI&#30340;&#39044;&#27979;&#26174;&#31034;&#65292;&#21040;2028&#24180;&#65292;AI&#31995;&#32479;&#26377;50%&#30340;&#20960;&#29575;&#23454;&#29616;&#22810;&#20010;&#37324;&#31243;&#30865;&#65292;&#21253;&#25324;&#33258;&#20027;&#26500;&#24314;&#20840;&#26032;&#30340;&#20184;&#27454;&#22788;&#29702;&#32593;&#31449;&#12289;&#21019;&#20316;&#19968;&#39318;&#19982;&#30693;&#21517;&#38899;&#20048;&#23478;&#30340;&#26032;&#27468;&#38590;&#20197;&#21306;&#20998;&#30340;&#27468;&#26354;&#65292;&#24182;&#33258;&#20027;&#19979;&#36733;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#26080;&#38656;&#36741;&#21161;&#30340;&#26426;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#30340;&#20960;&#29575;&#20272;&#35745;&#20026;10%&#21040;2047&#24180;&#20026;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#35843;&#26597;&#20013;&#65292;2778&#21517;&#22312;&#39030;&#32423;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20250;&#35758;&#19978;&#21457;&#34920;&#36807;&#35770;&#25991;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;AI&#36827;&#23637;&#30340;&#36895;&#24230;&#12289;&#39640;&#32423;AI&#31995;&#32479;&#30340;&#24615;&#36136;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#39044;&#27979;&#12290;&#24635;&#20307;&#39044;&#27979;&#26174;&#31034;&#65292;&#21040;2028&#24180;&#65292;AI&#31995;&#32479;&#33267;&#23569;&#26377;50%&#30340;&#20960;&#29575;&#23454;&#29616;&#22810;&#20010;&#37324;&#31243;&#30865;&#65292;&#21253;&#25324;&#33258;&#20027;&#26500;&#24314;&#19968;&#20010;&#20840;&#26032;&#30340;&#20184;&#27454;&#22788;&#29702;&#32593;&#31449;&#12289;&#21019;&#20316;&#19968;&#39318;&#21487;&#20197;&#19982;&#30693;&#21517;&#38899;&#20048;&#23478;&#30340;&#26032;&#27468;&#38590;&#20197;&#21306;&#20998;&#30340;&#27468;&#26354;&#65292;&#24182;&#33258;&#20027;&#19979;&#36733;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22914;&#26524;&#31185;&#23398;&#25345;&#32493;&#19981;&#21463;&#24178;&#25200;&#65292;2027&#24180;&#26080;&#38656;&#36741;&#21161;&#30340;&#26426;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#30340;&#20960;&#29575;&#20272;&#35745;&#20026;10%&#65292;&#21040;2047&#24180;&#20026;50%&#12290;&#21518;&#32773;&#30340;&#20272;&#35745;&#27604;&#25105;&#20204;&#19968;&#24180;&#21069;&#36827;&#34892;&#30340;&#31867;&#20284;&#35843;&#26597;[Grace et al., 2022]&#25552;&#21069;&#20102;13&#24180;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#20154;&#31867;&#32844;&#19994;&#23436;&#20840;&#21487;&#33258;&#21160;&#21270;&#30340;&#20960;&#29575;&#39044;&#35745;&#35201;&#21040;2037&#24180;&#36798;&#21040;10%&#65292;&#21040;2116&#24180;&#25165;&#36798;&#21040;50%&#65288;&#19982;2022&#24180;&#35843;&#26597;&#20013;&#30340;2164&#24180;&#30456;&#27604;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).  Most
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#32422;&#26463;&#19979;&#30340;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#23454;&#36341;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.11456</link><description>&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#30340;&#36845;&#20195;&#20559;&#22909;&#23398;&#20064;&#65306;&#22312;KL&#32422;&#26463;&#19979;&#23558;&#29702;&#35770;&#19982;&#23454;&#36341;&#32852;&#31995;&#36215;&#26469;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. (arXiv:2312.11456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#32422;&#26463;&#19979;&#30340;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#23454;&#36341;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#23545;&#40784;&#36807;&#31243;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26631;&#20934;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#21363;&#21453;&#21521;KL&#27491;&#21017;&#21270;&#30340;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#29992;&#20110;RLHF&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#23545;&#36825;&#20010;&#20844;&#24335;&#30340;&#20005;&#26684;&#29702;&#35770;&#20998;&#26512;&#20173;&#28982;&#24456;&#24320;&#25918;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#22312;&#31163;&#32447;&#12289;&#22312;&#32447;&#21644;&#28151;&#21512;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#26397;&#30528;&#23454;&#38469;&#24212;&#29992;&#30340;&#26041;&#21521;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23545;&#20449;&#24687;&#29702;&#35770;&#31574;&#30053;&#25913;&#36827;&#39044;&#35328;&#30340;&#31283;&#20581;&#36817;&#20284;&#65292;&#33258;&#28982;&#22320;&#20135;&#29983;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;RLHF&#31639;&#27861;&#12290;&#36825;&#21253;&#25324;&#22312;&#32447;&#22330;&#26223;&#20013;&#30340;&#36845;&#20195;&#29256;&#26412;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#31639;&#27861;&#65292;&#20197;&#21450;&#31163;&#32447;&#24773;&#26223;&#19979;&#30340;&#22810;&#27493;&#25298;&#32477;&#25277;&#26679;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#23545;&#40784;&#23454;&#39564;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.  Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32431;&#24046;&#20998;&#38544;&#31169;&#21644;&#39640;&#26031;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#35745;&#31639;MCMC&#31169;&#26377;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36817;&#20284;&#37319;&#26679;&#25200;&#21160;&#31639;&#27861;&#65292;&#32467;&#21512;Metropolis-Hastings&#31639;&#27861;&#21644;&#23616;&#37096;&#21270;&#27493;&#39588;&#65292;&#23454;&#29616;&#20102;&#23545;&#38544;&#31169;&#30340;&#20445;&#25252;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.14661</link><description>&lt;p&gt;
&#22522;&#20110;&#32431;&#24046;&#20998;&#38544;&#31169;&#21644;&#39640;&#26031;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#35745;&#31639;MCMC&#31169;&#26377;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tractable MCMC for Private Learning with Pure and Gaussian Differential Privacy. (arXiv:2310.14661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32431;&#24046;&#20998;&#38544;&#31169;&#21644;&#39640;&#26031;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#35745;&#31639;MCMC&#31169;&#26377;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36817;&#20284;&#37319;&#26679;&#25200;&#21160;&#31639;&#27861;&#65292;&#32467;&#21512;Metropolis-Hastings&#31639;&#27861;&#21644;&#23616;&#37096;&#21270;&#27493;&#39588;&#65292;&#23454;&#29616;&#20102;&#23545;&#38544;&#31169;&#30340;&#20445;&#25252;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#21363;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#25351;&#25968;&#26426;&#21046;&#65292;&#25552;&#20379;&#949;-&#32431;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20445;&#35777;&#65292;&#24182;&#19981;&#21463;&#65288;&#949;&#65292;&#948;&#65289;-&#36817;&#20284;DP&#24341;&#20837;&#30340;&#28508;&#22312;&#26080;&#30028;&#38544;&#31169;&#27844;&#28431;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#38656;&#35201;&#24212;&#29992;&#36817;&#20284;&#37319;&#26679;&#26041;&#27861;&#65292;&#22914;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#65292;&#20174;&#32780;&#37325;&#26032;&#24341;&#20837;&#20102;&#23545;&#38544;&#31169;&#20445;&#35777;&#30340;&#948;-&#36817;&#20284;&#35823;&#24046;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#37319;&#26679;&#25200;&#21160;&#65288;&#21363;ASAP&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#19982;&#28385;&#36275;&#32431;DP&#25110;&#32431;&#39640;&#26031;DP&#65288;&#21363;&#948;=0&#65289;&#30340;&#21442;&#32771;&#20998;&#24067;&#26377;&#30028;Wasserstein&#26080;&#31351;&#36317;&#31163;&#30340;MCMC&#26679;&#26412;&#21152;&#22122;&#22768;&#12290;&#28982;&#21518;&#21033;&#29992;Metropolis-Hastings&#31639;&#27861;&#29983;&#25104;&#26679;&#26412;&#24182;&#35777;&#26126;&#31639;&#27861;&#22312;W$_\infty$&#36317;&#31163;&#19978;&#25910;&#25947;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26032;&#25216;&#26415;&#19982;&#32454;&#33268;&#30340;&#23616;&#37096;&#21270;&#27493;&#39588;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#31532;&#19968;&#20010;&#21487;&#35745;&#31639;MCMC&#31169;&#26377;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Posterior sampling, i.e., exponential mechanism to sample from the posterior distribution, provides $\varepsilon$-pure differential privacy (DP) guarantees and does not suffer from potentially unbounded privacy breach introduced by $(\varepsilon,\delta)$-approximate DP. In practice, however, one needs to apply approximate sampling methods such as Markov chain Monte Carlo (MCMC), thus re-introducing the unappealing $\delta$-approximation error into the privacy guarantees. To bridge this gap, we propose the Approximate SAample Perturbation (abbr. ASAP) algorithm which perturbs an MCMC sample with noise proportional to its Wasserstein-infinity ($W_\infty$) distance from a reference distribution that satisfies pure DP or pure Gaussian DP (i.e., $\delta=0$). We then leverage a Metropolis-Hastings algorithm to generate the sample and prove that the algorithm converges in W$_\infty$ distance. We show that by combining our new techniques with a careful localization step, we obtain the first ne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#30340;&#24179;&#34913;K-Means&#32858;&#31867;&#30340;&#27010;&#29575;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#26368;&#20248;&#35299;&#26469;&#35745;&#31639;&#26657;&#20934;&#21518;&#39564;&#27010;&#29575;&#65292;&#23454;&#29616;&#22312;D-Wave AQC&#19978;&#35782;&#21035;&#27169;&#31946;&#35299;&#20915;&#26041;&#26696;&#21644;&#25968;&#25454;&#28857;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.12153</link><description>&lt;p&gt;
&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#30340;&#24179;&#34913;K-Means&#30340;&#27010;&#29575;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing. (arXiv:2310.12153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#30340;&#24179;&#34913;K-Means&#32858;&#31867;&#30340;&#27010;&#29575;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#26368;&#20248;&#35299;&#26469;&#35745;&#31639;&#26657;&#20934;&#21518;&#39564;&#27010;&#29575;&#65292;&#23454;&#29616;&#22312;D-Wave AQC&#19978;&#35782;&#21035;&#27169;&#31946;&#35299;&#20915;&#26041;&#26696;&#21644;&#25968;&#25454;&#28857;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#65288;AQC&#65289;&#26159;&#19968;&#31181;&#26377;&#26395;&#29992;&#20110;&#31163;&#25955;&#19988;&#36890;&#24120;&#20026;NP&#22256;&#38590;&#20248;&#21270;&#38382;&#39064;&#30340;&#37327;&#23376;&#35745;&#31639;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;AQC&#20801;&#35768;&#23454;&#29616;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#65292;&#36825;&#20419;&#20351;&#20102;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#24320;&#21457;&#37327;&#23376;&#34920;&#31034;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#38656;&#35201;&#20174;&#22122;&#22768;AQC&#36827;&#34892;&#22810;&#27425;&#27979;&#37327;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#20165;&#21033;&#29992;&#26368;&#20339;&#27979;&#37327;&#65292;&#20002;&#24323;&#20102;&#20854;&#20182;&#27979;&#37327;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#36827;&#34892;&#27010;&#29575;&#24179;&#34913;k-means&#32858;&#31867;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#38750;&#26368;&#20248;&#35299;&#26469;&#35745;&#31639;&#26657;&#20934;&#21518;&#39564;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#35745;&#31639;&#25104;&#26412;&#24456;&#20302;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#27169;&#31946;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#25968;&#25454;&#28857;&#65292;&#25105;&#20204;&#22312;D-Wave AQC&#19978;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adiabatic quantum computing (AQC) is a promising quantum computing approach for discrete and often NP-hard optimization problems. Current AQCs allow to implement problems of research interest, which has sparked the development of quantum representations for many machine learning and computer vision tasks. Despite requiring multiple measurements from the noisy AQC, current approaches only utilize the best measurement, discarding information contained in the remaining ones. In this work, we explore the potential of using this information for probabilistic balanced k-means clustering. Instead of discarding non-optimal solutions, we propose to use them to compute calibrated posterior probabilities with little additional compute cost. This allows us to identify ambiguous solutions and data points, which we demonstrate on a D-Wave AQC on synthetic and real data.
&lt;/p&gt;</description></item><item><title>IMITATE&#26159;&#19968;&#31181;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#21307;&#23398;&#25253;&#21578;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.07355</link><description>&lt;p&gt;
IMITATE: &#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training. (arXiv:2310.07355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07355
&lt;/p&gt;
&lt;p&gt;
IMITATE&#26159;&#19968;&#31181;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#21307;&#23398;&#25253;&#21578;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#39046;&#22495;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#20174;&#20020;&#24202;&#25253;&#21578;&#21644;&#30456;&#20851;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#24573;&#35270;&#20102;&#21033;&#29992;&#20020;&#24202;&#25253;&#21578;&#22266;&#26377;&#30340;&#23618;&#32423;&#32467;&#26500;&#30340;&#26426;&#20250;&#65292;&#36825;&#20123;&#25253;&#21578;&#36890;&#24120;&#34987;&#20998;&#20026;&#25551;&#36848;&#24615;&#20869;&#23481;&#30340;&#8220;&#21457;&#29616;&#8221;&#21644;&#32467;&#35770;&#24615;&#35266;&#23519;&#30340;&#8220;&#21360;&#35937;&#8221;&#12290;&#24403;&#21069;&#30340;&#21307;&#23398;VLP&#26041;&#27861;&#24448;&#24448;&#23558;&#25253;&#21578;&#31616;&#21270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#23454;&#20307;&#25110;&#20998;&#25955;&#30340;&#26631;&#35760;&#65292;&#32780;&#27809;&#26377;&#21033;&#29992;&#36825;&#31181;&#20016;&#23500;&#30340;&#12289;&#32467;&#26500;&#21270;&#30340;&#26684;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;VLP&#26694;&#26550;&#65292;&#21517;&#20026;IMITATE&#65292;&#29992;&#20110;&#20174;&#21307;&#23398;&#25253;&#21578;&#20013;&#23398;&#20064;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#20174;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#20998;&#21035;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32422;&#26463;&#30446;&#26631;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21040;CCA&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2310.01012</link><description>&lt;p&gt;
CCA&#23478;&#26063;&#30340;&#39640;&#25928;&#31639;&#27861;&#65306;&#26080;&#32422;&#26463;&#30446;&#26631;&#19982;&#26080;&#20559;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32422;&#26463;&#30446;&#26631;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21040;CCA&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#26041;&#27861;&#22312;&#22810;&#35270;&#35282;&#23398;&#20064;&#20013;&#20855;&#26377;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#27491;&#21017;&#21270;&#32447;&#24615;CCA&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#30340;&#25512;&#24191;&#65292;&#24182;&#19982;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#65288;GEP&#65289;&#26694;&#26550;&#32479;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32447;&#24615;&#26041;&#27861;&#30340;&#20256;&#32479;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#28145;&#24230;CCA&#30340;&#25193;&#23637;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#35757;&#32451;&#36807;&#31243;&#32531;&#24930;&#19988;&#22797;&#26434;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#25551;&#36848;GEPs&#30340;&#39030;&#32423;&#23376;&#31354;&#38388;&#30340;&#26032;&#39062;&#26080;&#32422;&#26463;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#36129;&#29486;&#26159;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#24212;&#29992;&#20110;&#30456;&#24212;&#30340;CCA&#30446;&#26631;&#65292;&#20174;&#32780;&#33719;&#24471;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#25152;&#26377;&#26631;&#20934;CCA&#21644;&#28145;&#24230;CCA&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;&#36825;&#26679;&#30340;&#36895;&#24230;&#20351;&#25105;&#20204;&#33021;&#22815;&#39318;&#27425;&#36827;&#34892;&#22823;&#35268;&#27169;&#29983;&#29289;&#25968;&#25454;&#30340;PLS&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Canonical Correlation Analysis (CCA) family of methods is foundational in multi-view learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear methods are computationally infeasible for large-scale data. Extensions to Deep CCA show great promise, but current training procedures are slow and complicated. First we propose a novel unconstrained objective that characterizes the top subspace of GEPs. Our core contribution is a family of fast algorithms for stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic gradient descent (SGD) to the corresponding CCA objectives. These methods show far faster convergence and recover higher correlations than the previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This speed allows us to perform a first-of-its-kind PLS analysis of an extremely large bio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21463;&#20010;&#20307;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.15375</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling. (arXiv:2309.15375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15375
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21463;&#20010;&#20307;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#21495;&#22270;&#65288;ECG&#25110;EKG&#65289;&#26159;&#19968;&#31181;&#27979;&#37327;&#24515;&#33039;&#30005;&#27963;&#21160;&#30340;&#21307;&#23398;&#27979;&#35797;&#12290;ECG&#24120;&#29992;&#20110;&#35786;&#26029;&#21644;&#30417;&#27979;&#21508;&#31181;&#24515;&#33039;&#30142;&#30149;&#65292;&#21253;&#25324;&#24515;&#24459;&#22833;&#24120;&#12289;&#24515;&#32908;&#26775;&#22622;&#21644;&#24515;&#21147;&#34928;&#31469;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ECG&#38656;&#35201;&#20020;&#24202;&#27979;&#37327;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21307;&#30103;&#26426;&#26500;&#30340;&#24212;&#29992;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21333;&#23548;&#32852;ECG&#24050;&#32463;&#22312;&#20329;&#25140;&#24335;&#35774;&#22791;&#19978;&#24212;&#29992;&#24191;&#27867;&#12290;&#21478;&#19968;&#31181;ECG&#30340;&#26367;&#20195;&#26041;&#27861;&#26159;&#20809;&#27978;&#24230;&#33033;&#25615;&#26816;&#27979;&#65288;PPG&#65289;&#65292;&#23427;&#37319;&#29992;&#38750;&#20405;&#20837;&#24615;&#12289;&#20302;&#25104;&#26412;&#30340;&#20809;&#23398;&#26041;&#27861;&#26469;&#27979;&#37327;&#24515;&#33039;&#29983;&#29702;&#23398;&#65292;&#20351;&#20854;&#25104;&#20026;&#25429;&#25417;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#24515;&#33039;&#20449;&#21495;&#30340;&#21512;&#36866;&#36873;&#25321;&#12290;&#34429;&#28982;ECG&#21644;PPG&#20043;&#38388;&#20855;&#26377;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#21518;&#32773;&#24182;&#27809;&#26377;&#25552;&#20379;&#26126;&#26174;&#30340;&#20020;&#24202;&#35786;&#26029;&#20215;&#20540;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21463;&#20010;&#20307;&#38480;&#21046;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#20174;&#32780;&#23454;&#29616;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
An electrocardiogram (ECG or EKG) is a medical test that measures the heart's electrical activity. ECGs are often used to diagnose and monitor a wide range of heart conditions, including arrhythmias, heart attacks, and heart failure. On the one hand, the conventional ECG requires clinical measurement, which restricts its deployment to medical facilities. On the other hand, single-lead ECG has become popular on wearable devices using administered procedures. An alternative to ECG is Photoplethysmography (PPG), which uses non-invasive, low-cost optical methods to measure cardiac physiology, making it a suitable option for capturing vital heart signs in daily life. As a result, it has become increasingly popular in health monitoring and is used in various clinical and commercial wearable devices. While ECG and PPG correlate strongly, the latter does not offer significant clinical diagnostic value. Here, we propose a subject-independent attention-based deep state-space model to translate P
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23545;&#31216;&#20445;&#25345;&#27880;&#24847;&#21147;&#32593;&#32476; (Spa-Net) &#23545;&#19981;&#31283;&#23450;&#30340;&#37325;&#31890;&#23376;&#36827;&#34892;&#37325;&#24314;&#65292;&#24182;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#22788;&#29702;&#22810;&#31181;&#36755;&#20837;&#29289;&#20307;&#31867;&#22411;&#21644;&#20840;&#23616;&#20107;&#20214;&#29305;&#24449;&#12290;&#22312;&#39030;&#22840;&#20811;&#23545;&#30340;&#21322;&#36731;&#23376;&#34928;&#21464;&#21644;&#19982;&#24076;&#26684;&#26031;&#29627;&#33394;&#23376;&#20849;&#21516;&#20135;&#29983;&#30340;&#39030;&#22840;&#20811;&#23545;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.01886</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23545;&#31216;&#20445;&#25345;&#27880;&#24847;&#21147;&#32593;&#32476;&#37325;&#24314;&#19981;&#31283;&#23450;&#30340;&#37325;&#31890;&#23376;
&lt;/p&gt;
&lt;p&gt;
Reconstruction of Unstable Heavy Particles Using Deep Symmetry-Preserving Attention Networks. (arXiv:2309.01886v2 [hep-ex] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01886
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#31216;&#20445;&#25345;&#27880;&#24847;&#21147;&#32593;&#32476; (Spa-Net) &#23545;&#19981;&#31283;&#23450;&#30340;&#37325;&#31890;&#23376;&#36827;&#34892;&#37325;&#24314;&#65292;&#24182;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#22788;&#29702;&#22810;&#31181;&#36755;&#20837;&#29289;&#20307;&#31867;&#22411;&#21644;&#20840;&#23616;&#20107;&#20214;&#29305;&#24449;&#12290;&#22312;&#39030;&#22840;&#20811;&#23545;&#30340;&#21322;&#36731;&#23376;&#34928;&#21464;&#21644;&#19982;&#24076;&#26684;&#26031;&#29627;&#33394;&#23376;&#20849;&#21516;&#20135;&#29983;&#30340;&#39030;&#22840;&#20811;&#23545;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#24314;&#19981;&#31283;&#23450;&#30340;&#37325;&#31890;&#23376;&#38656;&#35201;&#31934;&#23494;&#30340;&#25216;&#26415;&#26469;&#31579;&#36873;&#20986;&#23558;&#25506;&#27979;&#22120;&#29289;&#20307;&#20998;&#37197;&#32473;&#24213;&#23618;&#31890;&#23376;&#30340;&#22823;&#37327;&#21487;&#33021;&#25490;&#21015;&#12290;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#27880;&#24847;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#31216;&#20445;&#25345;&#27880;&#24847;&#21147;&#32593;&#32476; (Spa-Net)&#65292;&#20808;&#21069;&#24050;&#24212;&#29992;&#20110;&#21482;&#20135;&#29983;&#24378;&#23376;&#21943;&#27880;&#30340;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#20013;&#30340;&#39030;&#22840;&#20811;&#23545;&#34928;&#21464;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102; Spa-Net &#26550;&#26500;&#26469;&#32771;&#34385;&#22810;&#31181;&#36755;&#20837;&#29289;&#20307;&#31867;&#22411;&#65292;&#22914;&#36731;&#23376;&#65292;&#20197;&#21450;&#20840;&#23616;&#20107;&#20214;&#29305;&#24449;&#65292;&#22914;&#22833;&#21435;&#30340;&#27178;&#21521;&#21160;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#22238;&#24402;&#21644;&#20998;&#31867;&#36755;&#20986;&#20197;&#34917;&#20805;&#31890;&#23376;&#20998;&#37197;&#12290;&#25105;&#20204;&#22312;&#39030;&#22840;&#20811;&#23545;&#30340;&#21322;&#36731;&#23376;&#34928;&#21464;&#20197;&#21450;&#19982;&#24076;&#26684;&#26031;&#29627;&#33394;&#23376;&#20849;&#21516;&#20135;&#29983;&#30340;&#39030;&#22840;&#20811;&#23545;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#20102; Spa-Net &#25193;&#23637;&#33021;&#21147;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#33021;&#21147;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65306;ttH &#25628;&#32034;&#12289;&#39030;&#22840;&#20811;&#23545;&#30340;&#27979;&#37327;&#31561;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing unstable heavy particles requires sophisticated techniques to sift through the large number of possible permutations for assignment of detector objects to the underlying partons. An approach based on a generalized attention mechanism, symmetry preserving attention networks (Spa-Net), has been previously applied to top quark pair decays at the Large Hadron Collider which produce only hadronic jets. Here we extend the Spa-Net architecture to consider multiple input object types, such as leptons, as well as global event features, such as the missing transverse momentum. In addition, we provide regression and classification outputs to supplement the parton assignment. We explore the performance of the extended capability of Spa-Net in the context of semi-leptonic decays of top quark pairs as well as top quark pairs produced in association with a Higgs boson. We find significant improvements in the power of three representative studies: a search for ttH, a measurement of the 
&lt;/p&gt;</description></item><item><title>GRASP&#26159;&#19968;&#31181;&#26032;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#65292;&#26681;&#25454;&#26679;&#26412;&#30340;&#20195;&#34920;&#24615;&#36873;&#25321;&#26368;&#36866;&#21512;&#23398;&#20064;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#32447;&#28176;&#36827;&#24335;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13646</link><description>&lt;p&gt;
GRASP: &#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#28176;&#36827;&#24335;&#23398;&#20064;&#30340;&#37325;&#28436;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
GRASP: A Rehearsal Policy for Efficient Online Continual Learning. (arXiv:2308.13646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13646
&lt;/p&gt;
&lt;p&gt;
GRASP&#26159;&#19968;&#31181;&#26032;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#65292;&#26681;&#25454;&#26679;&#26412;&#30340;&#20195;&#34920;&#24615;&#36873;&#25321;&#26368;&#36866;&#21512;&#23398;&#20064;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#32447;&#28176;&#36827;&#24335;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28176;&#36827;&#23398;&#20064;&#28041;&#21450;&#20174;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#27969;&#20013;&#36880;&#27493;&#32047;&#31215;&#30693;&#35782;&#12290;&#28176;&#36827;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#38750;&#24179;&#31283;&#30340;&#25968;&#25454;&#27969;&#20250;&#23548;&#33268;&#20043;&#21069;&#23398;&#21040;&#30340;&#33021;&#21147;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#37325;&#28436;&#26159;&#19968;&#31181;&#24120;&#29992;&#19988;&#26377;&#25928;&#30340;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#36807;&#21435;&#30340;&#35266;&#27979;&#32467;&#26524;&#23384;&#20648;&#22312;&#32531;&#20914;&#21306;&#20013;&#65292;&#24182;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23558;&#23427;&#20204;&#19982;&#26032;&#30340;&#35266;&#27979;&#32467;&#26524;&#28151;&#21512;&#12290;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24212;&#35813;&#36873;&#25321;&#21738;&#20123;&#23384;&#20648;&#26679;&#26412;&#36827;&#34892;&#37325;&#28436;&#65311;&#36873;&#25321;&#26368;&#36866;&#21512;&#23398;&#20064;&#30340;&#26679;&#26412;&#32780;&#19981;&#26159;&#38543;&#26426;&#36873;&#25321;&#26679;&#26412;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#23398;&#20064;&#36895;&#24230;&#26174;&#33879;&#21152;&#24555;&#12290;&#23545;&#20110;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#31616;&#21333;&#30340;&#31867;&#22343;&#34913;&#38543;&#26426;&#36873;&#25321;&#31574;&#30053;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#19968;&#31181;&#26032;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;GRASP&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#38382;&#39064;&#12290;GRASP&#39318;&#20808;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#36880;&#28176;&#36873;&#25321;&#36739;&#19981;&#20855;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) in deep neural networks (DNNs) involves incrementally accumulating knowledge in a DNN from a growing data stream. A major challenge in CL is that non-stationary data streams cause catastrophic forgetting of previously learned abilities. Rehearsal is a popular and effective way to mitigate this problem, which is storing past observations in a buffer and mixing them with new observations during learning. This leads to a question: Which stored samples should be selected for rehearsal? Choosing samples that are best for learning, rather than simply selecting them at random, could lead to significantly faster learning. For class incremental learning, prior work has shown that a simple class balanced random selection policy outperforms more sophisticated methods. Here, we revisit this question by exploring a new sample selection policy called GRASP. GRASP selects the most prototypical (class representative) samples first and then gradually selects less prototypical (h
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#21518;&#32493;&#39564;&#35777;&#30340;&#27491;&#24335;&#27979;&#35797;&#31243;&#24207;&#65292;&#29992;&#20110;&#26816;&#27979;&#27169;&#22411;&#20998;&#37197;&#22266;&#23450;&#39044;&#27979;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#24320;&#21457;&#21487;&#38752;&#30340;&#26426;&#21046;&#65292;&#21487;&#20197;&#30830;&#23450;&#32473;&#23450;&#27169;&#22411;&#26159;&#21542;&#33021;&#20026;&#20915;&#31574;&#23545;&#35937;&#25552;&#20379;&#21518;&#32493;&#25514;&#26045;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27169;&#22411;&#20998;&#37197;&#22266;&#23450;&#39044;&#27979;&#21487;&#33021;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#30830;&#20445;&#21518;&#32493;&#25514;&#26045;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#36151;&#27454;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#21518;&#32493;&#25514;&#26045;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12820</link><description>&lt;p&gt;
&#19981;&#25490;&#38500;&#39044;&#27979;&#65306;&#22522;&#20110;&#21487;&#36798;&#38598;&#30340;&#21518;&#32493;&#39564;&#35777;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prediction without Preclusion: Recourse Verification with Reachable Sets. (arXiv:2308.12820v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#21518;&#32493;&#39564;&#35777;&#30340;&#27491;&#24335;&#27979;&#35797;&#31243;&#24207;&#65292;&#29992;&#20110;&#26816;&#27979;&#27169;&#22411;&#20998;&#37197;&#22266;&#23450;&#39044;&#27979;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#24320;&#21457;&#21487;&#38752;&#30340;&#26426;&#21046;&#65292;&#21487;&#20197;&#30830;&#23450;&#32473;&#23450;&#27169;&#22411;&#26159;&#21542;&#33021;&#20026;&#20915;&#31574;&#23545;&#35937;&#25552;&#20379;&#21518;&#32493;&#25514;&#26045;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27169;&#22411;&#20998;&#37197;&#22266;&#23450;&#39044;&#27979;&#21487;&#33021;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#30830;&#20445;&#21518;&#32493;&#25514;&#26045;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#36151;&#27454;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#21518;&#32493;&#25514;&#26045;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24120;&#34987;&#29992;&#20110;&#20915;&#23450;&#35841;&#26377;&#36164;&#26684;&#24471;&#21040;&#36151;&#27454;&#12289;&#38754;&#35797;&#25110;&#20844;&#20849;&#31119;&#21033;&#12290;&#26631;&#20934;&#25216;&#26415;&#29992;&#20110;&#26500;&#24314;&#36825;&#20123;&#27169;&#22411;&#26102;&#65292;&#20250;&#20351;&#29992;&#20851;&#20110;&#20154;&#30340;&#29305;&#24449;&#65292;&#20294;&#24573;&#35270;&#20182;&#20204;&#30340;&#21487;&#25805;&#20316;&#24615;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#20998;&#37197;&#22266;&#23450;&#30340;&#39044;&#27979;&#65292;&#36825;&#24847;&#21619;&#30528;&#34987;&#25298;&#32477;&#36151;&#27454;&#12289;&#38754;&#35797;&#25110;&#31119;&#21033;&#30340;&#28040;&#36153;&#32773;&#21487;&#33021;&#27704;&#20037;&#34987;&#25490;&#38500;&#22312;&#33719;&#24471;&#20449;&#36151;&#12289;&#23601;&#19994;&#25110;&#25588;&#21161;&#30340;&#26426;&#20250;&#20043;&#22806;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27491;&#24335;&#30340;&#27979;&#35797;&#31243;&#24207;&#26469;&#26816;&#27979;&#20998;&#37197;&#22266;&#23450;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21518;&#32493;&#39564;&#35777;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#26426;&#21046;&#21487;&#38752;&#22320;&#30830;&#23450;&#32473;&#23450;&#27169;&#22411;&#26159;&#21542;&#33021;&#25552;&#20379;&#23545;&#20915;&#31574;&#23545;&#35937;&#30340;&#21518;&#32493;&#25163;&#27573;&#65292;&#36825;&#20123;&#25163;&#27573;&#30001;&#29992;&#25143;&#25351;&#23450;&#30340;&#21487;&#25805;&#20316;&#24615;&#32422;&#26463;&#30830;&#23450;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#24037;&#20855;&#22914;&#20309;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#30830;&#20445;&#21518;&#32493;&#25514;&#26045;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#36151;&#27454;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#21518;&#32493;&#25514;&#26045;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#27169;&#22411;&#22914;&#20309;&#26080;&#24847;&#20013;&#20998;&#37197;&#22266;&#23450;&#39044;&#27979;&#65292;&#20174;&#32780;&#27704;&#20037;&#31105;&#27490;&#20351;&#29992;&#32773;&#33719;&#24471;&#30456;&#20851;&#26435;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are often used to decide who will receive a loan, a job interview, or a public benefit. Standard techniques to build these models use features about people but overlook their actionability. In turn, models can assign predictions that are fixed, meaning that consumers who are denied loans, interviews, or benefits may be permanently locked out from access to credit, employment, or assistance. In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar acces
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#19982;&#22806;&#37096;&#24615;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#21644;&#22996;&#25176;-&#20195;&#29702;&#26694;&#26550;&#65292;&#19978;&#23618;&#23398;&#20064;&#36866;&#24403;&#30340;&#22870;&#21169;&#21442;&#25968;&#21270;&#65292;&#19979;&#23618;&#23398;&#20064;&#20195;&#29702;&#20154;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.02585</link><description>&lt;p&gt;
&#23558;&#20195;&#29702;&#31574;&#30053;&#19982;&#22806;&#37096;&#24615;&#23545;&#40784;&#65306;&#36890;&#36807;&#21452;&#23618;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Aligning Agent Policy with Externalities: Reward Design via Bilevel RL. (arXiv:2308.02585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#19982;&#22806;&#37096;&#24615;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#21644;&#22996;&#25176;-&#20195;&#29702;&#26694;&#26550;&#65292;&#19978;&#23618;&#23398;&#20064;&#36866;&#24403;&#30340;&#22870;&#21169;&#21442;&#25968;&#21270;&#65292;&#19979;&#23618;&#23398;&#20064;&#20195;&#29702;&#20154;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#24120;&#22312;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#30340;&#24320;&#22987;&#22788;&#20551;&#35774;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#36825;&#31181;&#22266;&#23450;&#22870;&#21169;&#33539;&#24335;&#19979;&#30340;&#23398;&#20064;&#20013;&#65292;&#21487;&#33021;&#20250;&#24573;&#30053;&#37325;&#35201;&#30340;&#31574;&#30053;&#20248;&#21270;&#32771;&#34385;&#22240;&#32032;&#65292;&#27604;&#22914;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#21644;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#31038;&#20250;&#31119;&#21033;&#12289;&#21487;&#25345;&#32493;&#24615;&#25110;&#24066;&#22330;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340; emergent &#34892;&#20026;&#21644;&#21487;&#33021;&#19981;&#23545;&#40784;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#25968;&#23398;&#21270;&#22320;&#27010;&#25324;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#19982;&#36825;&#31181;&#22806;&#22312;&#24615;&#23545;&#40784;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#19982;&#22996;&#25176;-&#20195;&#29702;&#26694;&#26550;&#30456;&#32852;&#31995;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#22996;&#25176;&#20154;&#22312;&#19978;&#23618;&#30830;&#23450;&#31995;&#32479;&#30340;&#26356;&#24191;&#27867;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#20195;&#29702;&#20154;&#22312;&#19979;&#23618;&#35299;&#20915;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#19978;&#23618;&#20219;&#21153;&#26159;&#23398;&#20064;&#19968;&#20010;&#19982;&#26356;&#24191;&#27867;&#30446;&#26631;&#30456;&#23545;&#24212;&#30340;&#36866;&#24403;&#22870;&#21169;&#21442;&#25968;&#21270;&#65292;&#19979;&#23618;&#20219;&#21153;&#26159;&#23398;&#20064;&#20195;&#29702;&#20154;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), a reward function is often assumed at the outset of a policy optimization procedure. Learning in such a fixed reward paradigm in RL can neglect important policy optimization considerations, such as state space coverage and safety. Moreover, it can fail to encompass broader impacts in terms of social welfare, sustainability, or market stability, potentially leading to undesirable emergent behavior and potentially misaligned policy. To mathematically encapsulate the problem of aligning RL policy optimization with such externalities, we consider a bilevel optimization problem and connect it to a principal-agent framework, where the principal specifies the broader goals and constraints of the system at the upper level and the agent solves a Markov Decision Process (MDP) at the lower level. The upper-level deals with learning a suitable reward parametrization corresponding to the broader goals and the lower-level deals with learning the policy for the agent. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;Stein&#26041;&#27861;&#25512;&#23548;&#20986;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#65292;&#36890;&#36807;&#39640;&#26031;&#24179;&#28369;&#25216;&#26415;&#23558;&#24179;&#28369;&#24230;&#37327;&#36716;&#21270;&#20026;Wasserstein&#36317;&#31163;&#12290;&#36890;&#36807;&#29305;&#27530;&#21270;&#32467;&#26524;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#24191;&#20041;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#39640;&#26031;&#38543;&#26426;&#22330;&#36924;&#36817;&#30340;&#39318;&#20010;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.16308</link><description>&lt;p&gt;
&#36890;&#36807;Stein&#26041;&#27861;&#23545;&#39640;&#26031;&#38543;&#26426;&#22330;&#36827;&#34892;&#36924;&#36817;&#21450;&#20854;&#22312;&#24191;&#20041;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Gaussian random field approximation via Stein's method with applications to wide random neural networks. (arXiv:2306.16308v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;Stein&#26041;&#27861;&#25512;&#23548;&#20986;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#65292;&#36890;&#36807;&#39640;&#26031;&#24179;&#28369;&#25216;&#26415;&#23558;&#24179;&#28369;&#24230;&#37327;&#36716;&#21270;&#20026;Wasserstein&#36317;&#31163;&#12290;&#36890;&#36807;&#29305;&#27530;&#21270;&#32467;&#26524;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#24191;&#20041;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#39640;&#26031;&#38543;&#26426;&#22330;&#36924;&#36817;&#30340;&#39318;&#20010;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;Stein&#26041;&#27861;&#25512;&#23548;&#20986;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#65288;$W_1$&#65289;&#30340;&#19978;&#30028;&#65292;&#35813;&#36317;&#31163;&#26159;&#36830;&#32493;&#38543;&#26426;&#22330;&#19982;&#39640;&#26031;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#26031;&#24179;&#28369;&#25216;&#26415;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#24179;&#28369;&#24230;&#37327;&#20013;&#30340;&#19978;&#30028;&#36716;&#21270;&#20026;$W_1$&#36317;&#31163;&#12290;&#24179;&#28369;&#24615;&#26159;&#22522;&#20110;&#20351;&#29992;Laplacian&#31639;&#23376;&#30340;&#24130;&#26500;&#24314;&#30340;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#35774;&#35745;&#25104;&#19982;Cameron-Martin&#25110;Reproducing Kernel Hilbert Space&#30456;&#20851;&#32852;&#30340;&#39640;&#26031;&#36807;&#31243;&#20855;&#26377;&#26131;&#25805;&#20316;&#30340;&#29305;&#24449;&#12290;&#36825;&#20010;&#29305;&#24449;&#20351;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#20043;&#21069;&#25991;&#29486;&#20013;&#32771;&#34385;&#30340;&#19968;&#32500;&#21306;&#38388;&#22411;&#25351;&#26631;&#38598;&#12290;&#36890;&#36807;&#29305;&#21270;&#25105;&#20204;&#30340;&#19968;&#33324;&#32467;&#26524;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#22312;&#20219;&#24847;&#28145;&#24230;&#21644;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;&#24191;&#20041;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#39640;&#26031;&#38543;&#26426;&#22330;&#36924;&#36817;&#30340;&#39318;&#20010;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#19978;&#30028;&#26126;&#30830;&#22320;&#29992;&#32593;&#32476;&#23485;&#24230;&#21644;&#38543;&#26426;&#26435;&#37325;&#30340;&#30697;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive upper bounds on the Wasserstein distance ($W_1$), with respect to $\sup$-norm, between any continuous $\mathbb{R}^d$ valued random field indexed by the $n$-sphere and the Gaussian, based on Stein's method. We develop a novel Gaussian smoothing technique that allows us to transfer a bound in a smoother metric to the $W_1$ distance. The smoothing is based on covariance functions constructed using powers of Laplacian operators, designed so that the associated Gaussian process has a tractable Cameron-Martin or Reproducing Kernel Hilbert Space. This feature enables us to move beyond one dimensional interval-based index sets that were previously considered in the literature. Specializing our general result, we obtain the first bounds on the Gaussian random field approximation of wide random neural networks of any depth and Lipschitz activation functions at the random field level. Our bounds are explicitly expressed in terms of the widths of the network and moments of the random wei
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#20026;&#34920;&#26684;MDPs&#25512;&#23548;&#20986;&#39640;&#27010;&#29575;&#12289;&#23454;&#20363;&#30456;&#20851;&#30340;&#35823;&#24046;&#36793;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#31163;&#32447;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.14063</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#37319;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data. (arXiv:2306.14063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#20026;&#34920;&#26684;MDPs&#25512;&#23548;&#20986;&#39640;&#27010;&#29575;&#12289;&#23454;&#20363;&#30456;&#20851;&#30340;&#35823;&#24046;&#36793;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#31163;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#31163;&#32447;RL&#26041;&#27861;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#29702;&#35770;&#20445;&#35777;&#26159;&#23454;&#29616;&#25968;&#25454;&#38656;&#27714;&#37327;&#36739;&#22823;&#30340;RL&#31639;&#27861;&#23454;&#38469;&#21487;&#34892;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#32467;&#26524;&#20381;&#36182;&#20110;&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#65292;&#21363;&#21253;&#25324;&#19968;&#20010;&#30001;&#21333;&#19968;&#35760;&#24405;&#31574;&#30053;&#25910;&#38598;&#30340;i.i.d.&#36712;&#36857;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#65292;&#21363;&#25968;&#25454;&#38598;&#21487;&#20197;&#26159;&#33258;&#36866;&#24212;&#25910;&#38598;&#30340;&#12290;&#25105;&#20204;&#20026;&#34920;&#26684;MDPs&#20013;&#30340;TMIS&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#20272;&#35745;&#22120;&#22312;&#36825;&#20010;&#24191;&#20041;&#35774;&#32622;&#20013;&#24320;&#21457;&#29702;&#35770;&#65292;&#25512;&#23548;&#20854;&#20272;&#35745;&#35823;&#24046;&#30340;&#39640;&#27010;&#29575;&#12289;&#23454;&#20363;&#30456;&#20851;&#36793;&#30028;&#12290;&#25105;&#20204;&#36824;&#22238;&#25910;&#20102;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#31163;&#32447;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#27169;&#25311;&#65292;&#20197;&#32463;&#39564;&#20998;&#26512;&#36825;&#20123;&#20272;&#35745;&#22120;&#22312;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#27169;&#24335;&#19979;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing theoretical guarantees on the sample complexity of offline RL methods is an important step towards making data-hungry RL algorithms practically viable. Currently, most results hinge on unrealistic assumptions about the data distribution -- namely that it comprises a set of i.i.d. trajectories collected by a single logging policy. We consider a more general setting where the dataset may have been gathered adaptively. We develop theory for the TMIS Offline Policy Evaluation (OPE) estimator in this generalized setting for tabular MDPs, deriving high-probability, instance-dependent bounds on its estimation error. We also recover minimax-optimal offline learning in the adaptive setting. Finally, we conduct simulations to empirically analyze the behavior of these estimators under adaptive and non-adaptive regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2305.12081</link><description>&lt;p&gt;
AnyPredict: &#34920;&#26684;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyPredict: Foundation Model for Tabular Prediction. (arXiv:2305.12081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#25972;&#21512;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#65292;&#20197;&#20811;&#26381;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#34920;&#26684;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#20027;&#35201;&#38382;&#39064;&#21253;&#25324; (1) &#32570;&#20047;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#24102;&#26377;&#26631;&#20934;&#26631;&#31614;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450; (2) &#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27169;&#24335;&#19981;&#21305;&#37197;&#21644;&#39044;&#27979;&#30446;&#26631;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110; AnyPredict &#30340;&#34920;&#26684;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#39046;&#22495;&#20869;&#21644;&#24191;&#27867;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#25968;&#25454;&#24341;&#25806;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26469;&#25972;&#21512;&#34920;&#26684;&#26679;&#26412;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#27169;&#24335;&#34920;&#26684;&#20043;&#38388;&#30340;&#38556;&#30861;&#65292;&#24182;&#20351;&#29992;&#8220;&#23398;&#20064;&#65292;&#27880;&#37322;&#21644;&#23457;&#35745;&#8221;&#27969;&#31243;&#23558;&#39046;&#22495;&#22806;&#25968;&#25454;&#19982;&#30446;&#26631;&#20219;&#21153;&#23545;&#40784;&#12290;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#20351;&#39044;&#35757;&#32451;&#30340; AnyPredict &#33021;&#22815;&#25903;&#25345;&#27599;&#20010;&#34920;&#26684;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated significant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains.  This paper proposes a method for building training data at scale for tabular prediction foundation models (AnyPredict) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a ``learn, annotate, and audit'' pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#32452;&#21333;&#30446;&#26631;&#38382;&#39064;&#36827;&#34892;&#35299;&#20915;&#65292;&#24182;&#20171;&#32461;&#20102;R2&#25928;&#29992;&#20989;&#25968;&#20316;&#20026;&#36866;&#24403;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#25928;&#29992;&#20989;&#25968;&#21333;&#35843;&#19988;&#27425;&#27169;&#65292;&#21487;&#20197;&#20351;&#29992;&#36138;&#24515;&#20248;&#21270;&#31639;&#27861;&#35745;&#31639;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.11774</link><description>&lt;p&gt;
&#20351;&#29992;R2&#25928;&#29992;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization Using the R2 Utility. (arXiv:2305.11774v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#32452;&#21333;&#30446;&#26631;&#38382;&#39064;&#36827;&#34892;&#35299;&#20915;&#65292;&#24182;&#20171;&#32461;&#20102;R2&#25928;&#29992;&#20989;&#25968;&#20316;&#20026;&#36866;&#24403;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#25928;&#29992;&#20989;&#25968;&#21333;&#35843;&#19988;&#27425;&#27169;&#65292;&#21487;&#20197;&#20351;&#29992;&#36138;&#24515;&#20248;&#21270;&#31639;&#27861;&#35745;&#31639;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#25551;&#36848;&#22810;&#30446;&#26631;&#20043;&#38388;&#26368;&#20339;&#26435;&#34913;&#30340;&#28857;&#38598;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#30690;&#37327;&#20540;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#19994;&#32773;&#24120;&#24120;&#20351;&#29992;&#26631;&#37327;&#21270;&#20989;&#25968;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#32452;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#36825;&#32452;&#26631;&#37327;&#21270;&#38382;&#39064;&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#21333;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32422;&#23450;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#36890;&#29992;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31574;&#30053;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#21407;&#22987;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#36716;&#21270;&#20026;&#23450;&#20041;&#22312;&#38598;&#21512;&#19978;&#30340;&#21333;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#26032;&#38382;&#39064;&#30340;&#36866;&#24403;&#31867;&#21035;&#30340;&#30446;&#26631;&#20989;&#25968;&#26159;R2&#25928;&#29992;&#20989;&#25968;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;&#26631;&#37327;&#21270;&#20248;&#21270;&#38382;&#39064;&#30340;&#21152;&#26435;&#31215;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#25928;&#29992;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#21644;&#27425;&#27169;&#30340;&#38598;&#21512;&#20989;&#25968;&#65292;&#21487;&#20197;&#36890;&#36807;&#36138;&#24515;&#20248;&#21270;&#31639;&#27861;&#26377;&#25928;&#22320;&#35745;&#31639;&#20986;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of multi-objective optimization is to identify a collection of points which describe the best possible trade-offs between the multiple objectives. In order to solve this vector-valued optimization problem, practitioners often appeal to the use of scalarization functions in order to transform the multi-objective problem into a collection of single-objective problems. This set of scalarized problems can then be solved using traditional single-objective optimization techniques. In this work, we formalise this convention into a general mathematical framework. We show how this strategy effectively recasts the original multi-objective optimization problem into a single-objective optimization problem defined over sets. An appropriate class of objective functions for this new problem is the R2 utility function, which is defined as a weighted integral over the scalarized optimization problems. We show that this utility function is a monotone and submodular set function, which can be op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;Koopman&#31639;&#23376;&#36924;&#36817;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Koopman&#31639;&#23376;&#30340;&#32447;&#24615;&#24615;&#36136;&#21644;&#23545;&#25511;&#21046;&#22120;&#21644;&#38381;&#29615;&#31995;&#32479;&#32467;&#26500;&#30340;&#20102;&#35299;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#38381;&#29615;&#21644;&#35013;&#32622;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.15318</link><description>&lt;p&gt;
&#38381;&#29615;Koopman&#31639;&#23376;&#36924;&#36817;&#27861;
&lt;/p&gt;
&lt;p&gt;
Closed-Loop Koopman Operator Approximation. (arXiv:2303.15318v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;Koopman&#31639;&#23376;&#36924;&#36817;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Koopman&#31639;&#23376;&#30340;&#32447;&#24615;&#24615;&#36136;&#21644;&#23545;&#25511;&#21046;&#22120;&#21644;&#38381;&#29615;&#31995;&#32479;&#32467;&#26500;&#30340;&#20102;&#35299;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#38381;&#29615;&#21644;&#35013;&#32622;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24050;&#30693;&#25511;&#21046;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#21453;&#39304;&#25511;&#21046;&#31995;&#32479;&#30340;Koopman&#27169;&#22411;&#12290;Koopman&#31639;&#23376;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#35270;&#20026;&#26080;&#38480;&#32500;&#32447;&#24615;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#26080;&#38480;&#25968;&#37327;&#30340;&#26144;&#23556;&#20989;&#25968;&#36827;&#34892;&#37325;&#20889;&#12290;&#36890;&#36807;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#26144;&#23556;&#20989;&#25968;&#24182;&#22312;&#26144;&#23556;&#31354;&#38388;&#20013;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;Koopman&#31639;&#23376;&#30340;&#26377;&#38480;&#32500;&#36817;&#20284;&#12290;&#29616;&#26377;&#26041;&#27861;&#26088;&#22312;&#35782;&#21035;&#24320;&#29615;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26576;&#20123;&#31995;&#32479;&#65288;&#22914;&#19981;&#31283;&#23450;&#31995;&#32479;&#65289;&#65292;&#20197;&#24320;&#29615;&#26041;&#24335;&#36816;&#34892;&#23454;&#39564;&#26159;&#19981;&#20999;&#23454;&#38469;&#25110;&#19981;&#21487;&#33021;&#30340;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;Koopman&#31639;&#23376;&#30340;&#32447;&#24615;&#24615;&#36136;&#65292;&#32467;&#21512;&#25511;&#21046;&#22120;&#21644;&#38381;&#29615;&#31995;&#32479;&#30340;&#32467;&#26500;&#65292;&#21516;&#26102;&#35782;&#21035;&#38381;&#29615;&#21644;&#35013;&#32622;&#31995;&#32479;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#38381;&#29615;Koopman&#31639;&#23376;&#36924;&#36817;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method to identify a Koopman model of a feedback-controlled system given a known controller. The Koopman operator allows a nonlinear system to be rewritten as an infinite-dimensional linear system by viewing it in terms of an infinite set of lifting functions. A finite-dimensional approximation of the Koopman operator can be identified from data by choosing a finite subset of lifting functions and solving a regression problem in the lifted space. Existing methods are designed to identify open-loop systems. However, it is impractical or impossible to run experiments on some systems, such as unstable systems, in an open-loop fashion. The proposed method leverages the linearity of the Koopman operator, along with knowledge of the controller and the structure of the closed-loop system, to simultaneously identify the closed-loop and plant systems. The advantages of the proposed closed-loop Koopman operator approximation method are demonstrated experimentally using a ro
&lt;/p&gt;</description></item></channel></rss>