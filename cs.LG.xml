<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;MIDA&#26041;&#27861;&#65292;&#20197;&#22810;&#24037;&#20316;&#27969;&#21487;&#20449;&#24230;&#21644;&#25968;&#25454;&#21487;&#35266;&#27979;&#24615;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#36731;&#37327;&#32423;&#30340;&#36816;&#34892;&#26102;&#38598;&#25104;&#25968;&#25454;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23450;&#20041;&#25968;&#25454;&#21487;&#35266;&#27979;&#31574;&#30053;&#21644;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#24322;&#26500;&#31185;&#23398;&#29615;&#22659;&#19979;&#22810;&#20010;&#25903;&#25345;&#24037;&#20855;&#21644;&#39640;&#25928;&#30340;HPC&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2308.09004</link><description>&lt;p&gt;
&#20197;&#22810;&#24037;&#20316;&#27969;&#21487;&#20449;&#24230;&#21644;&#25968;&#25454;&#21487;&#35266;&#27979;&#24615;&#20026;&#22522;&#30784;&#30340;&#36731;&#37327;&#32423;&#25968;&#25454;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Lightweight Data Integration using Multi-workflow Provenance and Data Observability. (arXiv:2308.09004v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;MIDA&#26041;&#27861;&#65292;&#20197;&#22810;&#24037;&#20316;&#27969;&#21487;&#20449;&#24230;&#21644;&#25968;&#25454;&#21487;&#35266;&#27979;&#24615;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#36731;&#37327;&#32423;&#30340;&#36816;&#34892;&#26102;&#38598;&#25104;&#25968;&#25454;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23450;&#20041;&#25968;&#25454;&#21487;&#35266;&#27979;&#31574;&#30053;&#21644;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#24322;&#26500;&#31185;&#23398;&#29615;&#22659;&#19979;&#22810;&#20010;&#25903;&#25345;&#24037;&#20855;&#21644;&#39640;&#25928;&#30340;HPC&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#31185;&#23398;&#21457;&#29616;&#38656;&#35201;&#36328;&#22810;&#20010;&#35745;&#31639;&#35774;&#26045;&#36827;&#34892;&#22810;&#23398;&#31185;&#21512;&#20316;&#65292;&#21253;&#25324;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#26426;&#22120;&#21644;&#36793;&#32536;&#21040;&#20113;&#30340;&#36830;&#32493;&#20307;&#12290;&#38598;&#25104;&#30340;&#25968;&#25454;&#20998;&#26512;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#36890;&#36807;&#25903;&#25345;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#12289;FAIR&#12289;&#21487;&#22797;&#29616;&#24615;&#21644;&#29992;&#25143;&#25805;&#25511;&#12290;&#28982;&#32780;&#65292;&#31185;&#23398;&#30340;&#24322;&#26500;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22914;&#22788;&#29702;&#22810;&#20010;&#25903;&#25345;&#24037;&#20855;&#12289;&#36328;&#35774;&#26045;&#29615;&#22659;&#21644;&#39640;&#25928;&#30340;HPC&#25191;&#34892;&#12290;&#22312;&#25968;&#25454;&#21487;&#35266;&#27979;&#24615;&#12289;&#36866;&#37197;&#22120;&#31995;&#32479;&#35774;&#35745;&#21644;&#21487;&#20449;&#24230;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIDA&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#36816;&#34892;&#26102;&#22810;&#24037;&#20316;&#27969;&#38598;&#25104;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;MIDA&#23450;&#20041;&#20102;&#21508;&#31181;&#24182;&#34892;&#31995;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#30340;&#25968;&#25454;&#21487;&#35266;&#27979;&#31574;&#30053;&#21644;&#36866;&#24212;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#21487;&#35266;&#27979;&#24615;&#65292;&#23427;&#22312;&#21518;&#21488;&#25318;&#25130;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20202;&#22120;&#21270;&#65292;&#21516;&#26102;&#38598;&#25104;&#39046;&#22495;&#12289;&#21487;&#20449;&#24230;&#21644;&#36965;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern large-scale scientific discovery requires multidisciplinary collaboration across diverse computing facilities, including High Performance Computing (HPC) machines and the Edge-to-Cloud continuum. Integrated data analysis plays a crucial role in scientific discovery, especially in the current AI era, by enabling Responsible AI development, FAIR, Reproducibility, and User Steering. However, the heterogeneous nature of science poses challenges such as dealing with multiple supporting tools, cross-facility environments, and efficient HPC execution. Building on data observability, adapter system design, and provenance, we propose MIDA: an approach for lightweight runtime Multi-workflow Integrated Data Analysis. MIDA defines data observability strategies and adaptability methods for various parallel systems and machine learning tools. With observability, it intercepts the dataflows in the background without requiring instrumentation while integrating domain, provenance, and telemetry 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DealMVC&#30340;&#21452;&#21521;&#23545;&#27604;&#26657;&#20934;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#22270;&#32858;&#31867;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#23545;&#27604;&#26657;&#20934;&#25439;&#22833;&#26469;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09000</link><description>&lt;p&gt;
DealMVC: &#38754;&#21521;&#22810;&#35270;&#35282;&#32858;&#31867;&#30340;&#21452;&#21521;&#23545;&#27604;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
DealMVC: Dual Contrastive Calibration for Multi-view Clustering. (arXiv:2308.09000v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DealMVC&#30340;&#21452;&#21521;&#23545;&#27604;&#26657;&#20934;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#22270;&#32858;&#31867;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#23545;&#27604;&#26657;&#20934;&#25439;&#22833;&#26469;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23545;&#27604;&#32858;&#31867;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#35270;&#35282;&#19968;&#33268;&#24615;&#20449;&#24687;&#25366;&#25496;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#38480;&#21046;&#32858;&#31867;&#24615;&#33021;&#36827;&#19968;&#27493;&#25552;&#21319;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#19981;&#21516;&#35270;&#22270;&#20013;&#30456;&#21516;&#26679;&#26412;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20132;&#21449;&#35270;&#22270;&#22330;&#26223;&#20013;&#31867;&#20284;&#20294;&#19981;&#21516;&#30340;&#26679;&#26412;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#21452;&#21521;&#23545;&#27604;&#26657;&#20934;&#32593;&#32476;&#65288;DealMVC&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#34701;&#21512;&#26426;&#21046;&#65292;&#33719;&#24471;&#20840;&#23616;&#30340;&#36328;&#35270;&#22270;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#40784;&#35270;&#22270;&#29305;&#24449;&#30456;&#20284;&#24615;&#22270;&#21644;&#39640;&#32622;&#20449;&#24230;&#20266;&#26631;&#31614;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;&#23545;&#27604;&#26657;&#20934;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21033;&#29992;&#22810;&#35270;&#22270;&#20449;&#24687;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23616;&#37096;&#23545;&#27604;&#26657;&#20934;&#25439;&#22833;&#65292;&#29992;&#20110;&#32422;&#26463;&#25104;&#23545;&#35270;&#22270;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#12290;&#29305;&#24449;&#32467;&#26500;&#26159; ...
&lt;/p&gt;
&lt;p&gt;
Benefiting from the strong view-consistent information mining capacity, multi-view contrastive clustering has attracted plenty of attention in recent years. However, we observe the following drawback, which limits the clustering performance from further improvement. The existing multi-view models mainly focus on the consistency of the same samples in different views while ignoring the circumstance of similar but different samples in cross-view scenarios. To solve this problem, we propose a novel Dual contrastive calibration network for Multi-View Clustering (DealMVC). Specifically, we first design a fusion mechanism to obtain a global cross-view feature. Then, a global contrastive calibration loss is proposed by aligning the view feature similarity graph and the high-confidence pseudo-label graph. Moreover, to utilize the diversity of multi-view information, we propose a local contrastive calibration loss to constrain the consistency of pair-wise view features. The feature structure is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#23398;&#20064;&#22686;&#24378; (ReST) &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReST&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.08998</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#22686;&#24378; (ReST) &#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforced Self-Training (ReST) for Language Modeling. (arXiv:2308.08998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#23398;&#20064;&#22686;&#24378; (ReST) &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReST&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064; (RLHF)&#65292;&#21487;&#20197;&#36890;&#36807;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#38271;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064; (RL) &#26469;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784; LLM&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22686;&#24378;&#33258;&#23398;&#20064; (ReST)&#12290;&#32473;&#23450;&#21021;&#22987;&#30340;LLM&#31574;&#30053;&#65292;ReST&#36890;&#36807;&#20174;&#31574;&#30053;&#20013;&#29983;&#25104;&#26679;&#26412;&#26469;&#20135;&#29983;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;LLM&#31574;&#30053;&#12290;ReST&#27604;&#20856;&#22411;&#30340;&#22312;&#32447;RLHF&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#31163;&#32447;&#29983;&#25104;&#30340;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#25968;&#25454;&#12290;&#34429;&#28982;ReST&#26159;&#36866;&#29992;&#20110;&#25152;&#26377;&#29983;&#25104;&#23398;&#20064;&#35774;&#32622;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ReST&#21487;&#20197;&#20197;&#35745;&#31639;&#21644;&#37319;&#26679;&#39640;&#25928;&#30340;&#26041;&#24335;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#22312;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#19978;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#25391;&#33633;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#21033;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#26102;&#38388;&#29305;&#24615;&#26469;&#25552;&#39640;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#29289;&#29702;&#38382;&#39064;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08989</link><description>&lt;p&gt;
&#31070;&#32463;&#25391;&#33633;&#22120;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Neural oscillators for generalization of physics-informed machine learning. (arXiv:2308.08989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#25391;&#33633;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#21033;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#26102;&#38388;&#29305;&#24615;&#26469;&#25552;&#39640;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#29289;&#29702;&#38382;&#39064;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20854;&#22312;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#34920;&#31034;&#30340;&#22797;&#26434;&#29289;&#29702;&#38382;&#39064;&#26102;&#12290;&#26412;&#25991;&#26088;&#22312;&#22686;&#24378;PIML&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#22312;&#26410;&#25506;&#32034;&#21306;&#22495;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;PDE&#35299;&#30340;&#22266;&#26377;&#22240;&#26524;&#20851;&#31995;&#21644;&#26102;&#38388;&#39034;&#24207;&#29305;&#24615;&#65292;&#23558;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#32452;&#30340;&#24490;&#29615;&#31070;&#32463;&#26550;&#26500;&#19982;PIML&#27169;&#22411;&#34701;&#21512;&#65292;&#31216;&#20043;&#20026;&#31070;&#32463;&#25391;&#33633;&#22120;&#12290;&#36890;&#36807;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#32531;&#35299;&#26799;&#24230;&#29190;&#28856;&#21644;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#31070;&#32463;&#25391;&#33633;&#22120;&#20419;&#36827;&#20102;PIML&#20219;&#21153;&#20013;&#30340;&#25913;&#36827;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#28041;&#21450;&#26102;&#38388;&#20381;&#36182;&#30340;&#38750;&#32447;&#24615;PDE&#21644;&#21452;&#35843;&#21644;&#26753;&#26041;&#31243;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A primary challenge of physics-informed machine learning (PIML) is its generalization beyond the training domain, especially when dealing with complex physical problems represented by partial differential equations (PDEs). This paper aims to enhance the generalization capabilities of PIML, facilitating practical, real-world applications where accurate predictions in unexplored regions are crucial. We leverage the inherent causality and temporal sequential characteristics of PDE solutions to fuse PIML models with recurrent neural architectures based on systems of ordinary differential equations, referred to as neural oscillators. Through effectively capturing long-time dependencies and mitigating the exploding and vanishing gradient problem, neural oscillators foster improved generalization in PIML tasks. Extensive experimentation involving time-dependent nonlinear PDEs and biharmonic beam equations demonstrates the efficacy of the proposed approach. Incorporating neural oscillators out
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#38024;&#23545;&#29983;&#29289;&#28151;&#21512;&#31995;&#32479;&#20013;&#30340;&#20223;&#29983;&#24046;&#36317;&#36827;&#34892;&#20102;&#37327;&#21270;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#40060;&#31867;&#21644;&#20223;&#29983;&#35825;&#39285;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#23545;&#40060;&#31867;&#23545;&#30340;&#27169;&#25311;&#65292;&#30740;&#31350;&#20154;&#21592;&#35777;&#26126;&#20102;&#20182;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#19982;&#30495;&#23454;&#40060;&#31867;&#23436;&#20840;&#30456;&#20284;&#30340;&#31038;&#20250;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.08978</link><description>&lt;p&gt;
&#37327;&#21270;&#29983;&#29289;&#28151;&#21512;&#31995;&#32479;&#20013;&#30340;&#20223;&#29983;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Quantifying the biomimicry gap in biohybrid systems. (arXiv:2308.08978v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08978
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#38024;&#23545;&#29983;&#29289;&#28151;&#21512;&#31995;&#32479;&#20013;&#30340;&#20223;&#29983;&#24046;&#36317;&#36827;&#34892;&#20102;&#37327;&#21270;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#40060;&#31867;&#21644;&#20223;&#29983;&#35825;&#39285;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#23545;&#40060;&#31867;&#23545;&#30340;&#27169;&#25311;&#65292;&#30740;&#31350;&#20154;&#21592;&#35777;&#26126;&#20102;&#20182;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#19982;&#30495;&#23454;&#40060;&#31867;&#23436;&#20840;&#30456;&#20284;&#30340;&#31038;&#20250;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21160;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#20223;&#29983;&#28151;&#21512;&#31995;&#32479;&#24050;&#25104;&#20026;&#25506;&#32034;&#21644;&#35782;&#21035;&#32676;&#20307;&#21160;&#29289;&#34892;&#20026;&#26426;&#21046;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#23558;&#31038;&#20250;&#20114;&#21160;&#27169;&#22411;&#20174;&#27169;&#25311;&#36716;&#31227;&#21040;&#29616;&#23454;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#39564;&#35777;&#24314;&#27169;&#20551;&#35774;&#12290;&#36825;&#19968;&#25361;&#25112;&#28304;&#20110;&#25152;&#35859;&#30340;&#8220;&#20223;&#29983;&#24046;&#36317;&#8221;&#30340;&#36328;&#36234;&#65292;&#35813;&#24046;&#36317;&#20351;&#29992;&#19981;&#23436;&#32654;&#30340;&#26426;&#22120;&#20154;&#22797;&#21046;&#21697;&#12289;&#36890;&#20449;&#32447;&#32034;&#21644;&#29289;&#29702;&#32422;&#26463;&#65292;&#36825;&#20123;&#22240;&#32032;&#26410;&#34987;&#32435;&#20837;&#27169;&#25311;&#20013;&#65292;&#21487;&#33021;&#22312;&#21160;&#29289;&#36523;&#19978;&#24341;&#21457;&#19981;&#30495;&#23454;&#30340;&#34892;&#20026;&#21453;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#32418;&#22068;&#26080;&#39035;&#39790;&#40060;&#65288;Hemigrammus rhodostomus&#65289;&#30340;&#20223;&#29983;&#35825;&#39285;&#21644;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#26469;&#29983;&#25104;&#20223;&#29983;&#31038;&#20250;&#20114;&#21160;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;&#30001;&#40060;&#21644;&#26426;&#22120;&#20154;&#35825;&#39285;&#32452;&#25104;&#30340;&#29983;&#29289;&#28151;&#21512;&#20307;&#12289;&#19968;&#23545;&#30495;&#23454;&#40060;&#21644;&#19968;&#23545;&#40060;&#30340;&#27169;&#25311;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29983;&#29289;&#28151;&#21512;&#31995;&#32479;&#20135;&#29983;&#20102;&#19982;&#30495;&#23454;&#40060;&#31867;&#23436;&#20840;&#30456;&#20284;&#30340;&#20223;&#30495;&#31038;&#20250;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biohybrid systems in which robotic lures interact with animals have become compelling tools for probing and identifying the mechanisms underlying collective animal behavior. One key challenge lies in the transfer of social interaction models from simulations to reality, using robotics to validate the modeling hypotheses. This challenge arises in bridging what we term the "biomimicry gap", which is caused by imperfect robotic replicas, communication cues and physics constrains not incorporated in the simulations that may elicit unrealistic behavioral responses in animals. In this work, we used a biomimetic lure of a rummy-nose tetra fish (Hemigrammus rhodostomus) and a neural network (NN) model for generating biomimetic social interactions. Through experiments with a biohybrid pair comprising a fish and the robotic lure, a pair of real fish, and simulations of pairs of fish, we demonstrate that our biohybrid system generates high-fidelity social interactions mirroring those of genuine f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#20013;&#30340;&#27969;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#24314;&#31435;&#20102;&#19968;&#20010;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#26469;&#25551;&#36848;&#39118;&#38505;&#21644;&#27425;&#20248;&#24615;&#24230;&#37327;&#31561;&#32479;&#35745;&#37327;&#65292;&#33719;&#24471;&#20102;&#31283;&#23450;&#24615;&#23398;&#20064;&#29575;&#38408;&#20540;&#21644;&#25910;&#25947;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#25193;&#25955;&#31995;&#25968;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;SGD&#36845;&#20195;&#30340;&#32479;&#35745;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#26631;&#20934;&#31034;&#20363;&#21644;&#25968;&#20540;&#27169;&#25311;&#65292;&#39564;&#35777;&#20102;&#35813;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08977</link><description>&lt;p&gt;
&#25171;&#30772;&#39640;&#32500;&#38899;&#31526;&#65306;&#20851;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#19978; SGD &#23398;&#20064;&#21160;&#21147;&#23398;&#30340; ODE &#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hitting the High-Dimensional Notes: An ODE for SGD learning dynamics on GLMs and multi-index models. (arXiv:2308.08977v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#20013;&#30340;&#27969;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#24314;&#31435;&#20102;&#19968;&#20010;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#26469;&#25551;&#36848;&#39118;&#38505;&#21644;&#27425;&#20248;&#24615;&#24230;&#37327;&#31561;&#32479;&#35745;&#37327;&#65292;&#33719;&#24471;&#20102;&#31283;&#23450;&#24615;&#23398;&#20064;&#29575;&#38408;&#20540;&#21644;&#25910;&#25947;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#25193;&#25955;&#31995;&#25968;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;SGD&#36845;&#20195;&#30340;&#32479;&#35745;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#26631;&#20934;&#31034;&#20363;&#21644;&#25968;&#20540;&#27169;&#25311;&#65292;&#39564;&#35777;&#20102;&#35813;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#24212;&#29992;&#20110;&#20855;&#26377;&#19968;&#33324;&#25968;&#25454;&#21327;&#26041;&#24046;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#65288;&#20363;&#22914;&#36923;&#36753;&#22238;&#24402;&#12289;&#30456;&#20301;&#24674;&#22797;&#65289;&#26102;&#65292;&#27969;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#39640;&#32500;&#38480;&#21046;&#19979;&#30340;&#21160;&#21147;&#23398;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; SGD &#30340;&#30830;&#23450;&#24615;&#31561;&#25928;&#24418;&#24335;&#65292;&#21363;&#19968;&#32452;&#25551;&#36848;&#39118;&#38505;&#21644;&#20854;&#20182;&#27425;&#20248;&#24615;&#24230;&#37327;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#12290;&#24403;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#19982;&#25968;&#25454;&#25968;&#37327;&#25104;&#27491;&#27604;&#22686;&#38271;&#26102;&#65292;&#35813;&#31561;&#25928;&#24615;&#20197;&#26497;&#22823;&#27010;&#29575;&#21457;&#29983;&#12290;&#35813;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471; SGD &#31283;&#23450;&#24615;&#30340;&#23398;&#20064;&#29575;&#38408;&#20540;&#20197;&#21450;&#25910;&#25947;&#20445;&#35777;&#12290;&#38500;&#20102;&#30830;&#23450;&#24615;&#31561;&#25928;&#24615;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#31616;&#21270;&#25193;&#25955;&#31995;&#25968;&#30340; SDE&#65288;&#22343;&#21248;&#21270; SGD&#65289;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512; SGD &#36845;&#20195;&#30340;&#24120;&#35268;&#32479;&#35745;&#21160;&#21147;&#23398;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20123;&#26631;&#20934;&#31034;&#20363;&#19978;&#28436;&#31034;&#20102;&#35813;&#29702;&#35770;&#65292;&#24182;&#23637;&#31034;&#20102;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the dynamics of streaming stochastic gradient descent (SGD) in the high-dimensional limit when applied to generalized linear models and multi-index models (e.g. logistic regression, phase retrieval) with general data-covariance. In particular, we demonstrate a deterministic equivalent of SGD in the form of a system of ordinary differential equations that describes a wide class of statistics, such as the risk and other measures of sub-optimality. This equivalence holds with overwhelming probability when the model parameter count grows proportionally to the number of data. This framework allows us to obtain learning rate thresholds for stability of SGD as well as convergence guarantees. In addition to the deterministic equivalent, we introduce an SDE with a simplified diffusion coefficient (homogenized SGD) which allows us to analyze the dynamics of general statistics of SGD iterates. Finally, we illustrate this theory on some standard examples and show numerical simulations w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35270;&#35282;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#25200;&#21160;&#24402;&#22240;&#29305;&#24449;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#25581;&#31034;&#20102;&#24402;&#22240;&#29305;&#24449;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.08949</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#37325;&#35270;&#35282;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Dual-Perspective Approach to Evaluating Feature Attribution Methods. (arXiv:2308.08949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08949
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35270;&#35282;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#25200;&#21160;&#24402;&#22240;&#29305;&#24449;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#25581;&#31034;&#20102;&#24402;&#22240;&#29305;&#24449;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24314;&#31435;&#19968;&#20010;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#32479;&#19968;&#26694;&#26550;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20960;&#20010;&#35270;&#35282;&#26469;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#35270;&#35282;&#26159;&#35266;&#23519;&#25200;&#21160;&#24402;&#22240;&#29305;&#24449;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65288;&#21363;&#24544;&#23454;&#24230;&#65289;&#12290;&#23613;&#31649;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#27934;&#35265;&#65292;&#20294;&#29616;&#26377;&#30340;&#24544;&#23454;&#24230;&#35780;&#20272;&#23384;&#22312;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25581;&#31034;&#30340;&#32570;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24544;&#23454;&#24230;&#33539;&#24335;&#20869;&#30340;&#20004;&#20010;&#26032;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;&#30452;&#35266;&#30340;&#23646;&#24615;&#65306;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#27491;&#30830;&#24615;&#35780;&#20272;&#24402;&#22240;&#29305;&#24449;&#30495;&#27491;&#26159;&#39044;&#27979;&#24615;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#32780;&#23436;&#25972;&#24615;&#26816;&#26597;&#25152;&#24471;&#24402;&#22240;&#22914;&#20309;&#24456;&#22909;&#22320;&#25581;&#31034;&#25152;&#26377;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;&#36825;&#20004;&#20010;&#35270;&#35282;&#22522;&#20110;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;&#30340;&#23450;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods attempt to explain neural network predictions by identifying relevant features. However, establishing a cohesive framework for assessing feature attribution remains a challenge. There are several views through which we can evaluate attributions. One principal lens is to observe the effect of perturbing attributed features on the model's behavior (i.e., faithfulness). While providing useful insights, existing faithfulness evaluations suffer from shortcomings that we reveal in this paper. In this work, we propose two new perspectives within the faithfulness paradigm that reveal intuitive properties: soundness and completeness. Soundness assesses the degree to which attributed features are truly predictive features, while completeness examines how well the resulting attribution reveals all the predictive features. The two perspectives are based on a firm mathematical foundation and provide quantitative metrics that are computable through efficient algorithms. W
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26089;&#26399;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#22320;&#22270;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#20102;&#20892;&#20316;&#29289;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20122;&#30000;&#32423;&#21035;&#30340;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20840;&#29699;&#35206;&#30422;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#24378;&#35843;&#20102;&#36755;&#20837;&#27169;&#24577;&#23545;&#20110;&#20135;&#37327;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08948</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20892;&#20316;&#29289;&#20135;&#37327;&#65306;&#22312;&#30000;&#22320;&#21644;&#20122;&#30000;&#32423;&#21035;&#19978;&#23545;&#36755;&#20837;&#27169;&#24577;&#21644;&#27169;&#22411;&#30340;&#24191;&#27867;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Predicting Crop Yield With Machine Learning: An Extensive Analysis Of Input Modalities And Models On a Field and sub-field Level. (arXiv:2308.08948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26089;&#26399;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#22320;&#22270;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#20102;&#20892;&#20316;&#29289;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20122;&#30000;&#32423;&#21035;&#30340;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20840;&#29699;&#35206;&#30422;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#24378;&#35843;&#20102;&#36755;&#20837;&#27169;&#24577;&#23545;&#20110;&#20135;&#37327;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26089;&#26399;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#22320;&#22270;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#20122;&#30000;&#32423;&#21035;&#19978;&#20351;&#29992;&#20892;&#20316;&#29289;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;Sentinel-2&#21355;&#26143;&#22270;&#20687;&#20316;&#20026;&#20027;&#35201;&#30340;&#36755;&#20837;&#25968;&#25454;&#27169;&#24577;&#65292;&#20197;&#21450;&#20854;&#20182;&#34917;&#20805;&#30340;&#27169;&#24577;&#65292;&#21253;&#25324;&#22825;&#27668;&#12289;&#22303;&#22756;&#21644;DEM&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#20855;&#26377;&#20840;&#29699;&#35206;&#30422;&#33539;&#22260;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#20351;&#35813;&#26694;&#26550;&#20855;&#26377;&#20840;&#29699;&#20280;&#32553;&#24615;&#12290;&#25105;&#20204;&#26126;&#30830;&#24378;&#35843;&#20102;&#36755;&#20837;&#27169;&#24577;&#23545;&#20110;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#26368;&#20339;&#32452;&#21512;&#30340;&#36755;&#20837;&#27169;&#24577;&#21462;&#20915;&#20110;&#22320;&#21306;&#12289;&#20316;&#29289;&#21644;&#36873;&#25321;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a simple yet effective early fusion method for crop yield prediction that handles multiple input modalities with different temporal and spatial resolutions. We use high-resolution crop yield maps as ground truth data to train crop and machine learning model agnostic methods at the sub-field level. We use Sentinel-2 satellite imagery as the primary modality for input data with other complementary modalities, including weather, soil, and DEM data. The proposed method uses input modalities available with global coverage, making the framework globally scalable. We explicitly highlight the importance of input modalities for crop yield prediction and emphasize that the best-performing combination of input modalities depends on region, crop, and chosen model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#31934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2308.08945</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpretable Graph Neural Networks for Tabular Data. (arXiv:2308.08945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#31934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#34920;&#26684;&#26684;&#24335;&#30340;&#25968;&#25454;&#32463;&#24120;&#20986;&#29616;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36817;&#26399;&#34987;&#25193;&#23637;&#20197;&#26377;&#25928;&#22788;&#29702;&#27492;&#31867;&#25968;&#25454;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26412;&#36136;&#19978;&#20135;&#29983;&#20102;&#40657;&#30418;&#27169;&#22411;&#65292;&#20197;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#23384;&#22312;&#65292;&#20351;&#24471;&#29992;&#25143;&#26080;&#27861;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#65288;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#38480;&#21046;&#23398;&#20064;&#31639;&#27861;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#20934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;IGNNet&#19982;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;XGBoost&#65292;Random Forests&#21644;TabNet&#65289;&#24615;&#33021;&#30456;&#24403;&#12290;&#21516;&#26102;&#65292;&#32467;&#26524;&#26174;&#31034;&#20174;IGNNet&#33719;&#24471;&#30340;&#35299;&#37322;&#19982;&#30495;&#23454;&#24773;&#20917;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in tabular format is frequently occurring in real-world applications. Graph Neural Networks (GNNs) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. However, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. We propose an approach, called IGNNet (Interpretable Graph Neural Network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. A large-scale empirical investigation is presented, showing that IGNNet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including XGBoost, Random Forests, and TabNet. At the same time, the results show that the explanations obtained from IGNNet are aligned with the true
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24322;&#26500;&#25968;&#25454;&#31354;&#38388;&#20013;&#24341;&#20837;&#22240;&#26524;&#23545;&#25239;&#25200;&#21160;&#24182;&#24212;&#29992;&#23545;&#25239;&#35757;&#32451;&#65292;&#32467;&#21512;&#20010;&#20307;&#20844;&#24179;&#24615;&#12289;&#22240;&#26524;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20197;&#35299;&#20915;&#22312;&#22788;&#29702;&#31163;&#25955;&#25935;&#24863;&#23646;&#24615;&#26102;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08938</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#25968;&#25454;&#31354;&#38388;&#20013;&#23454;&#29616;&#20010;&#20307;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#22240;&#26524;&#23545;&#25239;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces. (arXiv:2308.08938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24322;&#26500;&#25968;&#25454;&#31354;&#38388;&#20013;&#24341;&#20837;&#22240;&#26524;&#23545;&#25239;&#25200;&#21160;&#24182;&#24212;&#29992;&#23545;&#25239;&#35757;&#32451;&#65292;&#32467;&#21512;&#20010;&#20307;&#20844;&#24179;&#24615;&#12289;&#22240;&#26524;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20197;&#35299;&#20915;&#22312;&#22788;&#29702;&#31163;&#25955;&#25935;&#24863;&#23646;&#24615;&#26102;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36127;&#36131;&#20219;&#30340;AI&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#20844;&#24179;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#22240;&#26524;&#24615;&#31561;&#23646;&#24615;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#21508;&#33258;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#21516;&#26102;&#25506;&#32034;&#21644;&#38598;&#25104;&#36825;&#20123;&#23646;&#24615;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20010;&#20307;&#20844;&#24179;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#31163;&#25955;&#25935;&#24863;&#23646;&#24615;&#26102;&#12290;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#27169;&#22411;&#21644;&#25935;&#24863;&#23646;&#24615;&#26469;&#21019;&#24314;&#20844;&#24179;&#24230;&#37327;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#34913;&#37327;&#20010;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#24212;&#29992;&#23545;&#25239;&#35757;&#32451;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#23558;&#20010;&#20307;&#20844;&#24179;&#24615;&#12289;&#22240;&#26524;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#30340;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;
&lt;/p&gt;
&lt;p&gt;
As responsible AI gains importance in machine learning algorithms, properties such as fairness, adversarial robustness, and causality have received considerable attention in recent years. However, despite their individual significance, there remains a critical gap in simultaneously exploring and integrating these properties. In this paper, we propose a novel approach that examines the relationship between individual fairness, adversarial robustness, and structural causal models in heterogeneous data spaces, particularly when dealing with discrete sensitive attributes. We use causal structural models and sensitive attributes to create a fair metric and apply it to measure semantic similarity among individuals. By introducing a novel causal adversarial perturbation and applying adversarial training, we create a new regularizer that combines individual fairness, causality, and robustness in the classifier. Our method is evaluated on both real-world and synthetic datasets, demonstrating it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#37326;&#28779;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#30340;&#35745;&#31639;&#20195;&#20215;&#39640;&#12289;&#26102;&#38388;&#28040;&#32791;&#22823;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22238;&#24402;&#27169;&#22411;&#21644;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#65292;&#32467;&#21512;&#21355;&#26143;&#22320;&#24418;&#35201;&#32032;&#22320;&#22270;&#21644;&#21382;&#21490;&#28779;&#28798;&#25968;&#25454;&#65292;&#33021;&#22815;&#22522;&#20110;&#22320;&#35980;&#22270;&#20687;&#24555;&#36895;&#19988;&#30456;&#23545;&#20934;&#30830;&#22320;&#39044;&#27979;&#28779;&#28798;&#30340;&#29123;&#28903;&#25345;&#32493;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.08936</link><description>&lt;p&gt;
&#29992;&#22238;&#24402;&#26041;&#27861;&#20272;&#31639;&#28779;&#28798;&#25345;&#32493;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Estimating fire Duration using regression methods. (arXiv:2308.08936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#37326;&#28779;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#30340;&#35745;&#31639;&#20195;&#20215;&#39640;&#12289;&#26102;&#38388;&#28040;&#32791;&#22823;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22238;&#24402;&#27169;&#22411;&#21644;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#65292;&#32467;&#21512;&#21355;&#26143;&#22320;&#24418;&#35201;&#32032;&#22320;&#22270;&#21644;&#21382;&#21490;&#28779;&#28798;&#25968;&#25454;&#65292;&#33021;&#22815;&#22522;&#20110;&#22320;&#35980;&#22270;&#20687;&#24555;&#36895;&#19988;&#30456;&#23545;&#20934;&#30830;&#22320;&#39044;&#27979;&#28779;&#28798;&#30340;&#29123;&#28903;&#25345;&#32493;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37326;&#28779;&#39044;&#27979;&#38382;&#39064;&#36890;&#24120;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#20027;&#35201;&#28041;&#21450;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#21644;&#32454;&#32990;&#33258;&#21160;&#26426;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#19968;&#30452;&#20197;&#26469;&#37117;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#24456;&#38590;&#36805;&#36895;&#20316;&#20986;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39640;&#35745;&#31639;&#20195;&#20215;&#21644;&#26102;&#38388;&#28040;&#32791;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#12289;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#12289;XGBoost&#22238;&#24402;&#27169;&#22411;&#20197;&#21450;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#32534;&#30721;&#22120;&#31561;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#24050;&#30693;&#37326;&#28779;&#30340;&#29123;&#28903;&#25345;&#32493;&#26102;&#38388;&#12290;&#27169;&#22411;&#30340;&#36755;&#20837;&#22522;&#20110;&#21355;&#26143;&#25552;&#20379;&#30340;&#22320;&#24418;&#35201;&#32032;&#22320;&#22270;&#21644;&#35813;&#22320;&#21306;&#23545;&#24212;&#30340;&#21382;&#21490;&#28779;&#28798;&#25968;&#25454;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22788;&#29702;&#36755;&#20837;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#33021;&#22815;&#26681;&#25454;&#22320;&#35980;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#19988;&#30456;&#23545;&#20934;&#30830;&#30340;&#26410;&#26469;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wildfire forecasting problems usually rely on complex grid-based mathematical models, mostly involving Computational fluid dynamics(CFD) and Celluar Automata, but these methods have always been computationally expensive and difficult to deliver a fast decision pattern. In this paper, we provide machine learning based approaches that solve the problem of high computational effort and time consumption. This paper predicts the burning duration of a known wildfire by RF(random forest), KNN, and XGBoost regression models and also image-based, like CNN and Encoder. Model inputs are based on the map of landscape features provided by satellites and the corresponding historical fire data in this area. This model is trained by happened fire data and landform feature maps and tested with the most recent real value in the same area. By processing the input differently to obtain the optimal outcome, the system is able to make fast and relatively accurate future predictions based on landscape images
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#39044;&#35757;&#32451;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#29702;&#35770;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#26367;&#20195;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#25216;&#26415;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08934</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Data Imbalance in Molecular Property Prediction with Pre-training. (arXiv:2308.08934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#39044;&#35757;&#32451;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#29702;&#35770;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#26367;&#20195;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#25216;&#26415;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#21644;&#20998;&#26512;&#26448;&#26009;&#30340;&#21508;&#31181;&#23646;&#24615;&#26159;&#26448;&#26009;&#21457;&#23637;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#30005;&#27744;&#12289;&#21322;&#23548;&#20307;&#12289;&#20652;&#21270;&#21058;&#21644;&#33647;&#29289;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#23646;&#24615;&#26159;&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#21644;&#27169;&#25311;&#26469;&#30830;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#27599;&#20010;&#20505;&#36873;&#26448;&#26009;&#25191;&#34892;&#36825;&#26679;&#30340;&#35745;&#31639;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#29702;&#35770;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22312;&#19968;&#23567;&#37096;&#20998;&#29702;&#35770;&#35745;&#31639;&#32467;&#26524;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20313;&#26448;&#26009;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#31181;&#31216;&#20026;&#39044;&#35757;&#32451;&#30340;&#25216;&#26415;&#34987;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#39044;&#35757;&#32451;&#26159;&#25351;&#22312;&#30446;&#26631;&#20219;&#21153;&#20043;&#21069;&#65292;&#20808;&#22312;&#19968;&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#20010;&#36807;&#31243;&#26088;&#22312;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#31283;&#23450;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revealing and analyzing the various properties of materials is an essential and critical issue in the development of materials, including batteries, semiconductors, catalysts, and pharmaceuticals. Traditionally, these properties have been determined through theoretical calculations and simulations. However, it is not practical to perform such calculations on every single candidate material. Recently, a combination method of the theoretical calculation and machine learning has emerged, that involves training machine learning models on a subset of theoretical calculation results to construct a surrogate model that can be applied to the remaining materials. On the other hand, a technique called pre-training is used to improve the accuracy of machine learning models. Pre-training involves training the model on pretext task, which is different from the target task, before training the model on the target task. This process aims to extract the input data features, stabilizing the learning pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#20223;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#20570;&#24066;&#20013;&#24212;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20511;&#37492;&#19987;&#19994;&#20154;&#31867;&#20570;&#24066;&#21830;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#32467;&#21512;&#23376;&#20248;&#20449;&#21495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#30452;&#25509;&#31574;&#30053;&#20132;&#20114;&#65292;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#22810;&#20215;&#26684;&#27700;&#24179;&#30340;&#20570;&#24066;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.08918</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#27979;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#20223;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#21160;&#20570;&#24066;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
IMM: An Imitative Reinforcement Learning Approach with Predictive Representation Learning for Automatic Market Making. (arXiv:2308.08918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#20223;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#20570;&#24066;&#20013;&#24212;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20511;&#37492;&#19987;&#19994;&#20154;&#31867;&#20570;&#24066;&#21830;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#32467;&#21512;&#23376;&#20248;&#20449;&#21495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#30452;&#25509;&#31574;&#30053;&#20132;&#20114;&#65292;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#22810;&#20215;&#26684;&#27700;&#24179;&#30340;&#20570;&#24066;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20570;&#24066;&#65288;MM&#65289;&#22312;&#37329;&#34701;&#20132;&#26131;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#22312;&#30830;&#20445;&#24066;&#22330;&#27969;&#21160;&#24615;&#26041;&#38754;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#21151;&#33021;&#12290;&#22312;&#39034;&#24207;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#33021;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#22312;&#37327;&#21270;&#20132;&#26131;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;RL&#30340;MM&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#20248;&#21270;&#21333;&#19968;&#20215;&#26684;&#27700;&#24179;&#31574;&#30053;&#65292;&#32780;&#23545;&#20110;&#39057;&#32321;&#25764;&#38144;&#35746;&#21333;&#21644;&#20002;&#22833;&#38431;&#21015;&#20248;&#20808;&#32423;&#31561;&#38382;&#39064;&#26080;&#27861;&#35299;&#20915;&#12290;&#28041;&#21450;&#22810;&#20010;&#20215;&#26684;&#27700;&#24179;&#30340;&#31574;&#30053;&#26356;&#31526;&#21512;&#23454;&#38469;&#20132;&#26131;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#20215;&#26684;&#27700;&#24179;&#31574;&#30053;&#28041;&#21450;&#21040;&#20840;&#38754;&#30340;&#20132;&#26131;&#34892;&#20026;&#31354;&#38388;&#30340;&#22797;&#26434;&#24615;&#65292;&#26377;&#25928;&#35757;&#32451;&#30408;&#21033;&#30340;RL&#20195;&#29702;&#20154;&#22312;MM&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#21463;&#21040;&#19987;&#19994;&#20570;&#24066;&#21830;&#39640;&#25928;&#24037;&#20316;&#27969;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RL&#26694;&#26550;&#65292;&#21363;&#27169;&#20223;&#24066;&#22330;&#20570;&#24066;&#21830;&#65288;IMM&#65289;&#65292;&#23427;&#21033;&#29992;&#23376;&#20248;&#20449;&#21495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#30452;&#25509;&#31574;&#30053;&#20132;&#20114;&#30340;&#26041;&#24335;&#26469;&#24320;&#21457;&#22810;&#20215;&#26684;&#27700;&#24179;&#30340;MM&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Market making (MM) has attracted significant attention in financial trading owing to its essential function in ensuring market liquidity. With strong capabilities in sequential decision-making, Reinforcement Learning (RL) technology has achieved remarkable success in quantitative trading. Nonetheless, most existing RL-based MM methods focus on optimizing single-price level strategies which fail at frequent order cancellations and loss of queue priority. Strategies involving multiple price levels align better with actual trading scenarios. However, given the complexity that multi-price level strategies involves a comprehensive trading action space, the challenge of effectively training profitable RL agents for MM persists. Inspired by the efficient workflow of professional human market makers, we propose Imitative Market Maker (IMM), a novel RL framework leveraging both knowledge from suboptimal signal-based experts and direct policy interactions to develop multi-price level MM strategi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#25351;&#26631;&#25552;&#20379;&#29420;&#29305;&#30340;&#32467;&#26500;&#26469;&#32531;&#35299;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.08915</link><description>&lt;p&gt;
&#36229;&#36234;&#20849;&#20139;&#65306;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection. (arXiv:2308.08915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#25351;&#26631;&#25552;&#20379;&#29420;&#29305;&#30340;&#32467;&#26500;&#26469;&#32531;&#35299;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;(KPI)&#20197;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;(MTS)&#30340;&#24418;&#24335;&#36827;&#34892;&#30417;&#27979;&#65292;&#20197;&#30830;&#20445;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#21644;&#26381;&#21153;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#20934;&#30830;&#26816;&#27979;MTS&#30340;&#24322;&#24120;&#23545;&#20110;&#21518;&#32493;&#30340;&#25925;&#38556;&#25490;&#38500;&#38750;&#24120;&#20851;&#38190;&#12290;&#24322;&#24120;&#30340;&#31232;&#32570;&#24615;&#21644;&#25163;&#21160;&#26631;&#35760;&#23548;&#33268;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#30340;MTS&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#26041;&#27861;&#20248;&#21270;&#20102;&#19968;&#20010;&#28085;&#30422;&#25152;&#26377;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;/&#25439;&#22833;&#30340;&#25972;&#20307;&#30446;&#26631;/&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;&#20043;&#38388;&#20914;&#31361;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23548;&#33268;MTS&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25439;&#22833;&#20013;&#25379;&#25166;&#12290;&#36825;&#19968;&#20851;&#38190;&#26041;&#38754;&#26174;&#33879;&#24433;&#21709;&#26816;&#27979;&#24615;&#33021;&#65292;&#20294;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#20223;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;(MMoE)&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CAD&#65292;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;KPI&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;CAD&#20026;&#27599;&#20010;&#25351;&#26631;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#32467;&#26500;&#65292;&#20197;&#32531;&#35299;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive key performance indicators (KPIs) are monitored as multivariate time series data (MTS) to ensure the reliability of the software applications and service system. Accurately detecting the abnormality of MTS is very critical for subsequent fault elimination. The scarcity of anomalies and manual labeling has led to the development of various self-supervised MTS anomaly detection (AD) methods, which optimize an overall objective/loss encompassing all metrics' regression objectives/losses. However, our empirical study uncovers the prevalence of conflicts among metrics' regression objectives, causing MTS models to grapple with different losses. This critical aspect significantly impacts detection performance but has been overlooked in existing approaches. To address this problem, by mimicking the design of multi-gate mixture-of-experts (MMoE), we introduce CAD, a Conflict-aware multivariate KPI Anomaly Detection algorithm. CAD offers an exclusive structure for each metric to mitigate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#30140;&#30171;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#20197;&#20415;&#30740;&#31350;&#30140;&#30171;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#12290;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21487;&#20197;&#23558;&#22823;&#22411;&#22270;&#35889;&#36716;&#21270;&#20026;&#20302;&#32500;&#21521;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.08904</link><description>&lt;p&gt;
&#20026;&#30140;&#30171;&#24320;&#21457;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Development of a Knowledge Graph Embeddings Model for Pain. (arXiv:2308.08904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#30140;&#30171;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#20197;&#20415;&#30740;&#31350;&#30140;&#30171;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#12290;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21487;&#20197;&#23558;&#22823;&#22411;&#22270;&#35889;&#36716;&#21270;&#20026;&#20302;&#32500;&#21521;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30140;&#30171;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#27010;&#24565;&#30456;&#20114;&#20851;&#32852;&#65292;&#27604;&#22914;&#21487;&#33021;&#24341;&#36215;&#30140;&#30171;&#30340;&#30142;&#30149;&#65292;&#21487;&#33021;&#32531;&#35299;&#30140;&#30171;&#30340;&#33647;&#29289;&#31561;&#31561;&#12290;&#20026;&#20102;&#23436;&#20840;&#29702;&#35299;&#20010;&#20307;&#25110;&#25972;&#20010;&#20154;&#32676;&#25152;&#32463;&#21382;&#30340;&#30140;&#30171;&#32972;&#26223;&#65292;&#25105;&#20204;&#38656;&#35201;&#30740;&#31350;&#19982;&#30140;&#30171;&#30456;&#20851;&#30340;&#25152;&#26377;&#27010;&#24565;&#21450;&#20854;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24403;&#24314;&#27169;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#35760;&#24405;&#30340;&#30140;&#30171;&#26102;&#65292;&#36825;&#23588;&#20026;&#26377;&#29992;&#12290;&#30693;&#35782;&#22270;&#35889;&#36890;&#36807;&#19968;&#20010;&#30456;&#20114;&#38142;&#25509;&#30340;&#32593;&#32476;&#26469;&#34920;&#31034;&#27010;&#24565;&#21644;&#23427;&#20204;&#30340;&#20851;&#31995;&#65292;&#20197;&#19968;&#31181;&#35745;&#31639;&#21487;&#22788;&#29702;&#30340;&#26041;&#24335;&#23454;&#29616;&#35821;&#20041;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#35889;&#21487;&#33021;&#36807;&#22823;&#20197;&#33267;&#20110;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;&#32780;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26377;&#21161;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#23558;&#22270;&#35889;&#34920;&#31034;&#20026;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#12290;&#26500;&#24314;&#36825;&#26679;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#25152;&#38656;&#30340;&#19982;&#30140;&#30171;&#30456;&#20851;&#30340;&#21508;&#31181;&#20851;&#31995;&#21487;&#20197;o
&lt;/p&gt;
&lt;p&gt;
Pain is a complex concept that can interconnect with other concepts such as a disorder that might cause pain, a medication that might relieve pain, and so on. To fully understand the context of pain experienced by either an individual or across a population, we may need to examine all concepts related to pain and the relationships between them. This is especially useful when modeling pain that has been recorded in electronic health records. Knowledge graphs represent concepts and their relations by an interlinked network, enabling semantic and context-based reasoning in a computationally tractable form. These graphs can, however, be too large for efficient computation. Knowledge graph embeddings help to resolve this by representing the graphs in a low-dimensional vector space. These embeddings can then be used in various downstream tasks such as classification and link prediction. The various relations associated with pain which are required to construct such a knowledge graph can be o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;U&#22411;&#20998;&#23618;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#20197;&#25552;&#39640;&#36793;&#32536;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08896</link><description>&lt;p&gt;
&#20026;U&#22411;&#24182;&#34892;&#20998;&#23618;&#23398;&#20064;&#36827;&#34892;&#26368;&#20248;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Optimal Resource Allocation for U-Shaped Parallel Split Learning. (arXiv:2308.08896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;U&#22411;&#20998;&#23618;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#20197;&#25552;&#39640;&#36793;&#32536;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#23398;&#20064;&#65288;SL&#65289;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#20844;&#24320;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#32780;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;SL&#19981;&#21487;&#36991;&#20813;&#22320;&#27844;&#28431;&#20102;&#26631;&#31614;&#38544;&#31169;&#65292;&#22240;&#20026;&#23614;&#37096;&#27169;&#22411;&#65288;&#20855;&#26377;&#26368;&#21518;&#20960;&#23618;&#65289;&#24212;&#35813;&#25918;&#22312;&#26381;&#21153;&#22120;&#19978;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#21033;&#29992;U&#22411;&#26550;&#26500;&#23558;&#26089;&#26399;&#23618;&#21644;&#26368;&#21518;&#23618;&#37117;&#25918;&#22312;&#29992;&#25143;&#31471;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;U&#22411;&#20998;&#23618;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#20197;&#25552;&#39640;&#36793;&#32536;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#22810;&#20010;&#29992;&#25143;&#19982;&#36793;&#32536;&#26381;&#21153;&#22120;&#36827;&#34892;SL&#36890;&#20449;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#65292;&#31216;&#20026;LSCRA&#65292;&#23427;&#21487;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#21644;&#20998;&#23618;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;LSCRA&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;U&#22411;PSL&#21487;&#20197;&#36798;&#21040;&#19982;&#20854;&#20182;S&#26041;&#27861;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split learning (SL) has emerged as a promising approach for model training without revealing the raw data samples from the data owners. However, traditional SL inevitably leaks label privacy as the tail model (with the last layers) should be placed on the server. To overcome this limitation, one promising solution is to utilize U-shaped architecture to leave both early layers and last layers on the user side. In this paper, we develop a novel parallel U-shaped split learning and devise the optimal resource optimization scheme to improve the performance of edge networks. In the proposed framework, multiple users communicate with an edge server for SL. We analyze the end-to-end delay of each client during the training process and design an efficient resource allocation algorithm, called LSCRA, which finds the optimal computing resource allocation and split layers. Our experimental results show the effectiveness of LSCRA and that U-shaped PSL can achieve a similar performance with other S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#30340;&#32467;&#26500;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#37325;&#39640;&#26031;&#29275;&#39039;&#26041;&#21521;&#39044;&#35328;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20598;&#24418;&#24335;&#35745;&#31639;&#36825;&#20123;&#39044;&#35328;&#65292;&#24471;&#21040;&#20102;&#26082;&#20855;&#26377;&#35745;&#31639;&#22909;&#22788;&#21448;&#20855;&#26377;&#26032;&#35265;&#35299;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#39044;&#35328;&#20316;&#20026;&#38543;&#26426;&#26799;&#24230;&#30340;&#19979;&#38477;&#26041;&#21521;&#30340;&#20248;&#21183;&#65292;&#24182;&#30740;&#31350;&#20102;&#35745;&#31639;&#19978;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.08886</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21452;&#39640;&#26031;&#29275;&#39039;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Dual Gauss-Newton Directions for Deep Learning. (arXiv:2308.08886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#30340;&#32467;&#26500;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#37325;&#39640;&#26031;&#29275;&#39039;&#26041;&#21521;&#39044;&#35328;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20598;&#24418;&#24335;&#35745;&#31639;&#36825;&#20123;&#39044;&#35328;&#65292;&#24471;&#21040;&#20102;&#26082;&#20855;&#26377;&#35745;&#31639;&#22909;&#22788;&#21448;&#20855;&#26377;&#26032;&#35265;&#35299;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#39044;&#35328;&#20316;&#20026;&#38543;&#26426;&#26799;&#24230;&#30340;&#19979;&#38477;&#26041;&#21521;&#30340;&#20248;&#21183;&#65292;&#24182;&#30740;&#31350;&#20102;&#35745;&#31639;&#19978;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#39640;&#26031;&#29275;&#39039;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#30340;&#32467;&#26500;&#65292;&#21363;&#30001;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#32447;&#24615;&#32593;&#32476;&#32452;&#25104;&#65292;&#20197;&#23548;&#20986;&#27604;&#38543;&#26426;&#26799;&#24230;&#26356;&#22909;&#30340;&#26041;&#21521;&#39044;&#35328;&#65292;&#22522;&#20110;&#37096;&#20998;&#32447;&#24615;&#21270;&#30340;&#24605;&#24819;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23427;&#20204;&#30340;&#23545;&#20598;&#24418;&#24335;&#26469;&#35745;&#31639;&#36825;&#26679;&#30340;&#26041;&#21521;&#39044;&#35328;&#65292;&#20174;&#32780;&#33719;&#24471;&#35745;&#31639;&#19978;&#30340;&#22909;&#22788;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#39044;&#35328;&#23450;&#20041;&#20102;&#21487;&#20197;&#29992;&#20316;&#29616;&#26377;&#20248;&#21270;&#31639;&#27861;&#20013;&#38543;&#26426;&#26799;&#24230;&#30340;&#26367;&#20195;&#21697;&#30340;&#19979;&#38477;&#26041;&#21521;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;&#23545;&#20598;&#24418;&#24335;&#30340;&#20248;&#21183;&#20197;&#21450;&#28041;&#21450;&#35745;&#31639;&#36825;&#20123;&#39044;&#35328;&#30340;&#35745;&#31639;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by Gauss-Newton-like methods, we study the benefit of leveraging the structure of deep learning objectives, namely, the composition of a convex loss function and of a nonlinear network, in order to derive better direction oracles than stochastic gradients, based on the idea of partial linearization. In a departure from previous works, we propose to compute such direction oracles via their dual formulation, leading to both computational benefits and new insights. We demonstrate that the resulting oracles define descent directions that can be used as a drop-in replacement for stochastic gradients, in existing optimization algorithms. We empirically study the advantage of using the dual formulation as well as the computational trade-offs involved in the computation of such oracles.
&lt;/p&gt;</description></item><item><title>FE-PINN&#26159;&#19968;&#31181;&#23398;&#20064;&#24213;&#23618;&#29289;&#29702;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#22312;&#20027;&#35757;&#32451;&#20043;&#21069;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#35299;&#20915;&#38382;&#39064;&#30340;&#27169;&#24335;&#12290;&#19982;&#20256;&#32479;PINN&#30456;&#27604;&#65292;FE-PINN&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#35757;&#32451;&#21644;&#26356;&#39640;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.08873</link><description>&lt;p&gt;
&#29305;&#24449;&#24378;&#21270;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;FE-PINN&#65289;&#65306;&#22312;&#30446;&#26631;&#20219;&#21153;&#20043;&#21069;&#23398;&#20064;&#24213;&#23618;&#29289;&#29702;&#29305;&#24449;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Feature Enforcing PINN (FE-PINN): A Framework to Learn the Underlying-Physics Features Before Target Task. (arXiv:2308.08873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08873
&lt;/p&gt;
&lt;p&gt;
FE-PINN&#26159;&#19968;&#31181;&#23398;&#20064;&#24213;&#23618;&#29289;&#29702;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#22312;&#20027;&#35757;&#32451;&#20043;&#21069;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#35299;&#20915;&#38382;&#39064;&#30340;&#27169;&#24335;&#12290;&#19982;&#20256;&#32479;PINN&#30456;&#27604;&#65292;FE-PINN&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#35757;&#32451;&#21644;&#26356;&#39640;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#24378;&#21270;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;FE-PINN&#65289;&#30340;&#26032;&#22411;&#26080;&#25968;&#25454;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#20027;&#35757;&#32451;&#24490;&#29615;&#20043;&#21069;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#20219;&#20309;&#38382;&#39064;&#30340;&#24213;&#23618;&#27169;&#24335;&#12290;&#30001;&#20110;&#23384;&#22312;&#20559;&#24494;&#20998;&#27531;&#24046;&#21644;&#36793;&#30028;&#26465;&#20214;&#22343;&#26041;&#35823;&#24046;&#20004;&#20010;&#39033;&#65292;&#26222;&#36890;PINN&#30340;&#25439;&#22833;&#20989;&#25968;&#19981;&#24179;&#34913;&#12290;FE-PINN&#36890;&#36807;&#21482;&#38656;&#19968;&#20998;&#38047;&#30340;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#32791;&#26102;&#25968;&#23567;&#26102;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;FE-PINN&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#23436;&#25104;&#36825;&#20010;&#36807;&#31243;&#12290;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#23398;&#20064;&#26377;&#20851;&#24213;&#23618;&#29289;&#29702;&#30340;&#26377;&#29992;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#20197;&#23436;&#21892;&#35745;&#31639;&#12290;FE-PINN&#24212;&#29992;&#20110;&#19977;&#20010;&#22522;&#20934;&#38382;&#39064;&#65306;&#22278;&#26609;&#20307;&#19978;&#30340;&#27969;&#21160;&#12289;&#20108;&#32500;&#28909;&#20256;&#23548;&#20197;&#21450;&#35745;&#31639;&#20837;&#21475;&#36895;&#24230;&#30340;&#36870;&#38382;&#39064;&#12290;FE-PINN&#21487;&#20197;&#20998;&#21035;&#21152;&#36895;15&#20493;&#12289;2&#20493;&#21644;5&#20493;&#22320;&#35299;&#20915;&#27599;&#20010;&#26696;&#20363;&#12290;&#21478;&#22806;
&lt;/p&gt;
&lt;p&gt;
In this work, a new data-free framework called Feature Enforcing Physics Informed Neural Network (FE-PINN) is introduced. This framework is capable of learning the underlying pattern of any problem with low computational cost before the main training loop. The loss function of vanilla PINN due to the existence of two terms of partial differential residuals and boundary condition mean squared error is imbalanced. FE-PINN solves this challenge with just one minute of training instead of time-consuming hyperparameter tuning for loss function that can take hours. The FE-PINN accomplishes this process by performing a sequence of sub-tasks. The first sub-task learns useful features about the underlying physics. Then, the model trains on the target task to refine the calculations. FE-PINN is applied to three benchmarks, flow over a cylinder, 2D heat conduction, and an inverse problem of calculating inlet velocity. FE-PINN can solve each case with, 15x, 2x, and 5x speed up accordingly. Another
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26631;&#31614;&#32570;&#22833;&#38750;&#38543;&#26426;&#65288;MNAR&#65289;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#36716;&#25442;&#36319;&#36394;&#30340;&#20266;&#20462;&#27491;&#25351;&#23548;&#65288;PRG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#20449;&#24687;&#21644;&#21160;&#24577;&#21019;&#24314;&#30340;&#22270;&#27169;&#22411;&#65292;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20013;&#30340;&#26631;&#31614;&#32570;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08872</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#38543;&#26426;&#32570;&#22833;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Semi-supervised Learning with Non-random Missing Labels. (arXiv:2308.08872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26631;&#31614;&#32570;&#22833;&#38750;&#38543;&#26426;&#65288;MNAR&#65289;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#36716;&#25442;&#36319;&#36394;&#30340;&#20266;&#20462;&#27491;&#25351;&#23548;&#65288;PRG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#20449;&#24687;&#21644;&#21160;&#24577;&#21019;&#24314;&#30340;&#22270;&#27169;&#22411;&#65292;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20013;&#30340;&#26631;&#31614;&#32570;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35299;&#20915;&#26631;&#31614;&#32570;&#22833;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;SSL&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20256;&#32479;&#30340;&#24773;&#26223;&#65292;&#24573;&#30053;&#20102;&#19968;&#20010;&#23454;&#38469;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#21363;&#26631;&#31614;&#32570;&#22833;&#38750;&#38543;&#26426;&#65288;MNAR&#65289;&#12290;&#22312;MNAR&#20013;&#65292;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#23646;&#20110;&#19981;&#21516;&#30340;&#31867;&#21035;&#20998;&#24067;&#65292;&#23548;&#33268;&#20102;&#26377;&#20559;&#30340;&#26631;&#31614;&#25554;&#34917;&#65292;&#38477;&#20302;&#20102;SSL&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#21035;&#36716;&#25442;&#36319;&#36394;&#30340;&#20266;&#20462;&#27491;&#25351;&#23548;&#65288;PRG&#65289;&#26041;&#27861;&#26469;&#22788;&#29702;MNAR&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#24335;&#24471;&#21040;&#31867;&#21035;&#32423;&#21035;&#30340;&#24341;&#23548;&#20449;&#24687;&#65292;&#35813;&#20449;&#24687;&#22522;&#20110;&#21160;&#24577;&#21019;&#24314;&#30340;&#22270;&#22312;&#31867;&#21035;&#36319;&#36394;&#30697;&#38453;&#20043;&#19978;&#24314;&#27169;&#12290;PRG&#26041;&#27861;&#36890;&#36807;&#23558;&#31867;&#21035;&#20998;&#24067;&#30340;&#21382;&#21490;&#20449;&#24687;&#21644;&#20266;&#20462;&#27491;&#36807;&#31243;&#24341;&#36215;&#30340;&#31867;&#21035;&#36716;&#25442;&#32479;&#19968;&#36215;&#26469;&#65292;&#20197;&#20445;&#25345;&#27169;&#22411;&#23545;&#25152;&#26377;&#31867;&#21035;&#20998;&#37197;&#20266;&#26631;&#31614;&#30340;&#26080;&#20559;&#28909;&#24773;&#65292;&#20174;&#32780;&#25552;&#39640;&#20266;&#26631;&#31614;&#22312;&#24120;&#35265;&#31867;&#21035;&#21644;&#32597;&#35265;&#31867;&#21035;&#19978;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) tackles the label missing problem by enabling the effective usage of unlabeled data. While existing SSL methods focus on the traditional setting, a practical and challenging scenario called label Missing Not At Random (MNAR) is usually ignored. In MNAR, the labeled and unlabeled data fall into different class distributions resulting in biased label imputation, which deteriorates the performance of SSL models. In this work, class transition tracking based Pseudo-Rectifying Guidance (PRG) is devised for MNAR. We explore the class-level guidance information obtained by the Markov random walk, which is modeled on a dynamically created graph built over the class tracking matrix. PRG unifies the historical information of class distribution and class transitions caused by the pseudo-rectifying procedure to maintain the model's unbiased enthusiasm towards assigning pseudo-labels to all classes, so as the quality of pseudo-labels on both popular classes and rare c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#30001;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#23454;&#29616;&#19982;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#22312;&#26102;&#38388;&#27573;&#20381;&#36182;&#24615;&#26041;&#38754;&#36798;&#21040;&#21516;&#26679;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08858</link><description>&lt;p&gt;
&#27809;&#26377;&#27169;&#22411;&#30340;&#31639;&#27861;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Model-Free Algorithm with Improved Sample Efficiency for Zero-Sum Markov Games. (arXiv:2308.08858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#30001;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#23454;&#29616;&#19982;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#22312;&#26102;&#38388;&#27573;&#20381;&#36182;&#24615;&#26041;&#38754;&#36798;&#21040;&#21516;&#26679;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#38382;&#39064;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#26377;&#38480;&#26102;&#38388;&#27573;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$O(H^3SAB/\epsilon^2)$&#25214;&#21040;$\epsilon$-&#26368;&#20248;&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#65292;&#20854;&#20013;$H$&#26159;&#26102;&#38388;&#27573;&#65292;$S$&#26159;&#29366;&#24577;&#25968;&#37327;&#65288;$A$&#21644;$B$&#20998;&#21035;&#34920;&#31034;&#20004;&#20010;&#29609;&#23478;&#30340;&#21160;&#20316;&#25968;&#37327;&#65289;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#36825;&#26679;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#30001;&#30340;&#38454;&#27573;&#24615;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#23427;&#23454;&#29616;&#20102;&#19982;&#26368;&#20339;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#22240;&#27492;&#39318;&#27425;&#35777;&#26126;&#20102;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#22312;&#26102;&#38388;&#27573;&#20381;&#36182;&#24615;&#26041;&#38754;&#20139;&#21463;&#19982;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;&#23545;&#20110;$H$&#30340;&#20381;&#36182;&#24615;&#30340;&#20027;&#35201;&#25913;&#36827;&#26469;&#28304;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The problem of two-player zero-sum Markov games has recently attracted increasing interests in theoretical studies of multi-agent reinforcement learning (RL). In particular, for finite-horizon episodic Markov decision processes (MDPs), it has been shown that model-based algorithms can find an $\epsilon$-optimal Nash Equilibrium (NE) with the sample complexity of $O(H^3SAB/\epsilon^2)$, which is optimal in the dependence of the horizon $H$ and the number of states $S$ (where $A$ and $B$ denote the number of actions of the two players, respectively). However, none of the existing model-free algorithms can achieve such an optimality. In this work, we propose a model-free stage-based Q-learning algorithm and show that it achieves the same sample complexity as the best model-based algorithm, and hence for the first time demonstrate that model-free algorithms can enjoy the same optimality in the $H$ dependence as model-based algorithms. The main improvement of the dependency on $H$ arises by
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#38271;&#23614;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;"&#25216;&#24039;&#21253;"&#12290;&#36890;&#36807;&#22810;&#31181;&#20808;&#36827;&#30340;&#35774;&#35745;&#65292;&#22914;&#25968;&#25454;&#22686;&#24378;&#12289;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#20998;&#31867;&#22120;&#35774;&#35745;&#31561;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#33016;&#37096;X&#23556;&#32447;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08853</link><description>&lt;p&gt;
Bag of Tricks for Long-Tailed Multi-Label Classification on Chest X-Rays
&lt;/p&gt;
&lt;p&gt;
Bag of Tricks for Long-Tailed Multi-Label Classification on Chest X-Rays. (arXiv:2308.08853v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#38271;&#23614;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;"&#25216;&#24039;&#21253;"&#12290;&#36890;&#36807;&#22810;&#31181;&#20808;&#36827;&#30340;&#35774;&#35745;&#65292;&#22914;&#25968;&#25454;&#22686;&#24378;&#12289;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#20998;&#31867;&#22120;&#35774;&#35745;&#31561;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#33016;&#37096;X&#23556;&#32447;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#33016;&#37096;&#25918;&#23556;&#23398;&#30340;&#20998;&#31867;&#23545;&#20110;&#26631;&#20934;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#22825;&#29983;&#30340;&#38271;&#23614;&#21644;&#22810;&#26631;&#31614;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#23581;&#35797;&#21516;&#26102;&#32771;&#34385;&#21040;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#20849;&#29616;&#25152;&#24102;&#26469;&#30340;&#38590;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#25552;&#39640;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#35786;&#26029;&#30340;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#26222;&#21450;&#65292;&#22914;&#20309;&#23558;&#36825;&#20123;&#26032;&#33539;&#24335;&#32435;&#20837;&#24403;&#21069;&#26694;&#26550;&#20013;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;ICCV CVAMD 2023 CXR-LT&#31454;&#36187;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#12289;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#20998;&#31867;&#22120;&#35774;&#35745;&#12289;&#25439;&#22833;&#20989;&#25968;&#21152;&#26435;&#37325;&#12289;&#22806;&#37096;&#25968;&#25454;&#34917;&#20805;&#31561;&#22810;&#20010;&#20808;&#36827;&#35774;&#35745;&#22312;CXR&#35786;&#26029;&#26041;&#38754;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#31616;&#21333;&#30340;&#27979;&#35797;&#26102;&#38388;&#25968;&#25454;&#22686;&#24378;&#21644;&#38598;&#25104;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical classification of chest radiography is particularly challenging for standard machine learning algorithms due to its inherent long-tailed and multi-label nature. However, few attempts take into account the coupled challenges posed by both the class imbalance and label co-occurrence, which hinders their value to boost the diagnosis on chest X-rays (CXRs) in the real-world scenarios. Besides, with the prevalence of pretraining techniques, how to incorporate these new paradigms into the current framework lacks of the systematical study. This technical report presents a brief description of our solution in the ICCV CVAMD 2023 CXR-LT Competition. We empirically explored the effectiveness for CXR diagnosis with the integration of several advanced designs about data augmentation, feature extractor, classifier design, loss function reweighting, exogenous data replenishment, etc. In addition, we improve the performance through simple test-time data augmentation and ensemble. Our framewo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#37325;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861; (ADMM) &#21644;&#21322;&#24179;&#28369;&#29275;&#39039; (SSN) &#22522;&#20110;&#22686;&#24191;&#23545;&#20598;&#27861; (ALM) &#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#31639;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#26680;&#24515;&#22270;&#24418;Lasso&#27169;&#22411;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#22823;&#32500;&#24230;&#30340;&#20219;&#21153;&#20013;&#33410;&#30465;&#36229;&#36807;70\%&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08852</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#23398;&#20064;&#20855;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#26680;&#24515;&#22270;&#24418;Lasso&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning the hub graphical Lasso model with the structured sparsity via an efficient algorithm. (arXiv:2308.08852v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08852
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#37325;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861; (ADMM) &#21644;&#21322;&#24179;&#28369;&#29275;&#39039; (SSN) &#22522;&#20110;&#22686;&#24191;&#23545;&#20598;&#27861; (ALM) &#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#31639;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#26680;&#24515;&#22270;&#24418;Lasso&#27169;&#22411;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#22823;&#32500;&#24230;&#30340;&#20219;&#21153;&#20013;&#33410;&#30465;&#36229;&#36807;70\%&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#27169;&#22411;&#22312;&#20174;&#29983;&#29289;&#20998;&#26512;&#21040;&#25512;&#33616;&#31995;&#32479;&#31561;&#20247;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26680;&#24515;&#33410;&#28857;&#30340;&#22270;&#24418;&#27169;&#22411;&#22312;&#25968;&#25454;&#32500;&#24230;&#36739;&#22823;&#26102;&#35745;&#31639;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#39640;&#25928;&#20272;&#35745;&#26680;&#24515;&#22270;&#24418;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#39318;&#20808;&#36890;&#36807;&#21452;&#37325;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861; (ADMM) &#29983;&#25104;&#19968;&#20010;&#33391;&#22909;&#30340;&#21021;&#22987;&#28857;&#65292;&#28982;&#21518;&#20351;&#29992;&#21322;&#24179;&#28369;&#29275;&#39039; (SSN) &#22522;&#20110;&#22686;&#24191;&#23545;&#20598;&#27861; (ALM) &#30340;&#26041;&#27861;&#36827;&#34892;&#28909;&#21551;&#21160;&#65292;&#20197;&#35745;&#31639;&#20986;&#33021;&#22815;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#31934;&#30830;&#21040;&#36275;&#22815;&#31243;&#24230;&#30340;&#35299;&#12290;&#24191;&#20041;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#31232;&#30095;&#32467;&#26500;&#30830;&#20445;&#20102;&#35813;&#31639;&#27861;&#33021;&#22815;&#38750;&#24120;&#39640;&#25928;&#22320;&#33719;&#24471;&#19968;&#20010;&#33391;&#22909;&#30340;&#35299;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#20840;&#38754;&#23454;&#39564;&#20013;&#65292;&#35813;&#31639;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;&#29305;&#21035;&#26159;&#22312;&#26576;&#20123;&#39640;&#32500;&#20219;&#21153;&#20013;&#65292;&#23427;&#21487;&#20197;&#33410;&#30465;&#36229;&#36807;70\%&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#21516;&#26102;&#20173;&#28982;&#21487;&#20197;&#36798;&#21040;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphical models have exhibited their performance in numerous tasks ranging from biological analysis to recommender systems. However, graphical models with hub nodes are computationally difficult to fit, particularly when the dimension of the data is large. To efficiently estimate the hub graphical models, we introduce a two-phase algorithm. The proposed algorithm first generates a good initial point via a dual alternating direction method of multipliers (ADMM), and then warm starts a semismooth Newton (SSN) based augmented Lagrangian method (ALM) to compute a solution that is accurate enough for practical tasks. The sparsity structure of the generalized Jacobian ensures that the algorithm can obtain a nice solution very efficiently. Comprehensive experiments on both synthetic data and real data show that it obviously outperforms the existing state-of-the-art algorithms. In particular, in some high dimensional tasks, it can save more than 70\% of the execution time, meanwhile still ach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#22810;&#31934;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#21442;&#25968;&#21270;&#32593;&#26684;&#21270;&#21644;&#27169;&#25311;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34746;&#26059;&#31649;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#26032;&#30340;&#21453;&#24212;&#22120;&#35774;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20811;&#26381;&#20102;&#27809;&#26377;&#26799;&#24230;&#30340;&#38750;&#23616;&#37096;&#20248;&#21270;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2308.08841</link><description>&lt;p&gt;
&#36890;&#36807;CFD&#32806;&#21512;&#22810;&#31934;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#19979;&#30340;&#26032;&#21453;&#24212;&#22120;&#35774;&#35745;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Assisted Discovery of Novel Reactor Designs via CFD-Coupled Multi-fidelity Bayesian Optimisation. (arXiv:2308.08841v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#22810;&#31934;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#21442;&#25968;&#21270;&#32593;&#26684;&#21270;&#21644;&#27169;&#25311;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34746;&#26059;&#31649;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#26032;&#30340;&#21453;&#24212;&#22120;&#35774;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20811;&#26381;&#20102;&#27809;&#26377;&#26799;&#24230;&#30340;&#38750;&#23616;&#37096;&#20248;&#21270;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28155;&#21152;&#21046;&#36896;&#25216;&#26415;&#20351;&#24471;&#26356;&#20808;&#36827;&#30340;&#21453;&#24212;&#22120;&#20960;&#20309;&#32467;&#26500;&#25104;&#20026;&#21487;&#33021;&#65292;&#20026;&#26356;&#22823;&#21644;&#26356;&#22797;&#26434;&#30340;&#35774;&#35745;&#31354;&#38388;&#25552;&#20379;&#20102;&#28508;&#22312;&#26426;&#20250;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#30830;&#23450;&#21644;&#20248;&#21270;&#26377;&#24076;&#26395;&#30340;&#37197;&#32622;&#23545;&#20110;&#29616;&#26377;&#30340;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#26041;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#37492;&#20110;&#31639;&#27861;&#25913;&#36827;&#21644;&#28155;&#21152;&#21046;&#36896;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34746;&#26059;&#31649;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21464;&#25130;&#38754;&#21644;&#34746;&#26059;&#36335;&#24452;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31995;&#21015;&#39640;&#32500;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#30830;&#20445;&#21487;&#34892;&#30340;&#38750;&#23616;&#37096;&#20248;&#21270;&#65292;&#22312;&#27809;&#26377;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24212;&#29992;&#22810;&#31934;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25551;&#36848;&#20102;&#22810;&#20010;&#36830;&#32493;&#30340;&#31934;&#24230;&#65292;&#24182;&#19982;&#21442;&#25968;&#21270;&#32593;&#26684;&#21270;&#21644;&#27169;&#25311;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20302;&#36136;&#37327;&#12289;
&lt;/p&gt;
&lt;p&gt;
Additive manufacturing has enabled the production of more advanced reactor geometries, resulting in the potential for significantly larger and more complex design spaces. Identifying and optimising promising configurations within broader design spaces presents a significant challenge for existing human-centric design approaches. As such, existing parameterisations of coiled-tube reactor geometries are low-dimensional with expensive optimisation limiting more complex solutions. Given algorithmic improvements and the onset of additive manufacturing, we propose two novel coiled-tube parameterisations enabling the variation of cross-section and coil path, resulting in a series of high dimensional, complex optimisation problems. To ensure tractable, non-local optimisation where gradients are not available, we apply multi-fidelity Bayesian optimisation. Our approach characterises multiple continuous fidelities and is coupled with parameterised meshing and simulation, enabling lower quality, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25511;&#21046;&#38544;&#23494;&#32852;&#21512;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#23398;&#20064;&#21644;&#28151;&#28102;&#26469;&#23454;&#29616;&#26368;&#20248;&#26597;&#35810;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#24694;&#24847;&#35328;&#35770;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08825</link><description>&lt;p&gt;
&#25511;&#21046;&#38544;&#23494;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Controlling Federated Learning for Covertness. (arXiv:2308.08825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25511;&#21046;&#38544;&#23494;&#32852;&#21512;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#23398;&#20064;&#21644;&#28151;&#28102;&#26469;&#23454;&#29616;&#26368;&#20248;&#26597;&#35810;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#24694;&#24847;&#35328;&#35770;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#32773;&#36890;&#36807;&#37325;&#22797;&#26597;&#35810;&#20998;&#24067;&#24335;&#39044;&#35328;&#26426;&#25552;&#20379;&#30340;&#22122;&#22768;&#26799;&#24230;&#35780;&#20272;&#26469;&#26368;&#23567;&#21270;&#20989;&#25968;$f$&#12290;&#21516;&#26102;&#65292;&#23398;&#20064;&#32773;&#36824;&#35797;&#22270;&#38544;&#34255;$\arg\min f$&#65292;&#20197;&#20813;&#34987;&#24694;&#24847;&#31363;&#21548;&#32773;&#35266;&#23519;&#21040;&#20854;&#26597;&#35810;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;\textit{&#38544;&#23494;}&#25110;\textit{&#23398;&#20064;&#32773;&#31169;&#23494;}&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#24517;&#39035;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#24615;&#22312;&#23398;&#20064;&#21644;&#28151;&#28102;&#20043;&#38388;&#21160;&#24577;&#36873;&#25321;&#12290;&#23558;&#25511;&#21046;&#38544;&#23494;&#20248;&#21270;&#30340;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20102;&#21160;&#24577;&#35268;&#21010;&#31639;&#23376;&#20855;&#26377;&#36229;&#27169;&#32467;&#26500;&#65292;&#20174;&#32780;&#24471;&#20986;&#26368;&#20248;&#31574;&#30053;&#20855;&#26377;&#21333;&#35843;&#38408;&#20540;&#32467;&#26500;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#20102;&#35299;&#36801;&#31227;&#27010;&#29575;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#26368;&#20248;&#26597;&#35810;&#31574;&#30053;&#12290;&#20316;&#20026;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#24694;&#24847;&#35328;&#35770;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A learner aims to minimize a function $f$ by repeatedly querying a distributed oracle that provides noisy gradient evaluations. At the same time, the learner seeks to hide $\arg\min f$ from a malicious eavesdropper that observes the learner's queries. This paper considers the problem of \textit{covert} or \textit{learner-private} optimization, where the learner has to dynamically choose between learning and obfuscation by exploiting the stochasticity. The problem of controlling the stochastic gradient algorithm for covert optimization is modeled as a Markov decision process, and we show that the dynamic programming operator has a supermodular structure implying that the optimal policy has a monotone threshold structure. A computationally efficient policy gradient algorithm is proposed to search for the optimal querying policy without knowledge of the transition probabilities. As a practical application, our methods are demonstrated on a hate speech classification task in a federated se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#22270;&#24418;&#20027;&#21160;&#23398;&#20064;&#20013;&#26469;&#33258;&#24694;&#24847;&#37051;&#22495;&#30340;&#35821;&#20041;&#28151;&#28102;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#24102;&#26377;&#35821;&#20041;&#29305;&#24449;&#30340;&#33410;&#28857;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#21644;&#19981;&#30456;&#20284;&#24230;&#26469;&#20849;&#21516;&#35780;&#20272;&#33410;&#28857;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26679;&#26412;&#38598;&#30340;&#20934;&#21017;&#21644;&#26597;&#35810;&#31574;&#30053;&#26469;&#32500;&#25345;&#25152;&#36873;&#33410;&#28857;&#30340;&#22810;&#26679;&#24615;&#21644;&#31867;&#21035;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.08823</link><description>&lt;p&gt;
&#32531;&#35299;&#22270;&#24418;&#20027;&#21160;&#23398;&#20064;&#20013;&#26469;&#33258;&#24694;&#24847;&#37051;&#22495;&#30340;&#35821;&#20041;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
Mitigating Semantic Confusion from Hostile Neighborhood for Graph Active Learning. (arXiv:2308.08823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#22270;&#24418;&#20027;&#21160;&#23398;&#20064;&#20013;&#26469;&#33258;&#24694;&#24847;&#37051;&#22495;&#30340;&#35821;&#20041;&#28151;&#28102;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#24102;&#26377;&#35821;&#20041;&#29305;&#24449;&#30340;&#33410;&#28857;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#21644;&#19981;&#30456;&#20284;&#24230;&#26469;&#20849;&#21516;&#35780;&#20272;&#33410;&#28857;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26679;&#26412;&#38598;&#30340;&#20934;&#21017;&#21644;&#26597;&#35810;&#31574;&#30053;&#26469;&#32500;&#25345;&#25152;&#36873;&#33410;&#28857;&#30340;&#22810;&#26679;&#24615;&#21644;&#31867;&#21035;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#20027;&#21160;&#23398;&#20064;&#65288;Graph Active Learning&#65292;&#31616;&#31216;GAL&#65289;&#26088;&#22312;&#23547;&#25214;&#22270;&#20013;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#21270;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Graph Neural Networks&#65292;&#31616;&#31216;GNNs&#65289;&#30340;&#24615;&#33021;&#65292;&#21560;&#24341;&#20102;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38750;&#24120;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#65292;&#29616;&#26377;&#30340;GAL&#31574;&#30053;&#21487;&#33021;&#20250;&#24341;&#20837;&#35821;&#20041;&#28151;&#28102;&#21040;&#36873;&#25321;&#30340;&#35757;&#32451;&#38598;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#22270;&#24418;&#22024;&#26434;&#30340;&#24773;&#20917;&#19979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20551;&#35774;&#25152;&#26377;&#32858;&#21512;&#29305;&#24449;&#37117;&#26159;&#26377;&#29992;&#30340;&#65292;&#24573;&#35270;&#20102;&#22312;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#19979;&#31867;&#38388;&#36793;&#32536;&#20043;&#38388;&#30340;&#35821;&#20041;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#30340;&#35821;&#20041;&#24863;&#30693;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65288;Semantic-aware Active learning framework for Graphs&#65292;&#31616;&#31216;SAG&#65289;&#26469;&#32531;&#35299;&#35821;&#20041;&#28151;&#28102;&#38382;&#39064;&#12290;&#24341;&#20837;&#33410;&#28857;&#20043;&#38388;&#24102;&#26377;&#35821;&#20041;&#29305;&#24449;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#21644;&#19981;&#30456;&#20284;&#24230;&#26469;&#20849;&#21516;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#21147;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26679;&#26412;&#38598;&#30340;&#20934;&#21017;&#21644;&#26597;&#35810;&#31574;&#30053;&#65292;&#20998;&#21035;&#29992;&#20110;&#32500;&#25345;&#25152;&#36873;&#33410;&#28857;&#30340;&#22810;&#26679;&#24615;&#21644;&#31867;&#21035;&#24179;&#34913;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;SAG&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Active Learning (GAL), which aims to find the most informative nodes in graphs for annotation to maximize the Graph Neural Networks (GNNs) performance, has attracted many research efforts but remains non-trivial challenges. One major challenge is that existing GAL strategies may introduce semantic confusion to the selected training set, particularly when graphs are noisy. Specifically, most existing methods assume all aggregating features to be helpful, ignoring the semantically negative effect between inter-class edges under the message-passing mechanism. In this work, we present Semantic-aware Active learning framework for Graphs (SAG) to mitigate the semantic confusion problem. Pairwise similarities and dissimilarities of nodes with semantic features are introduced to jointly evaluate the node influence. A new prototype-based criterion and query policy are also designed to maintain diversity and class balance of the selected nodes, respectively. Extensive experiments on the pu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#20808;&#39564;&#21644;&#26174;&#33879;&#24615;&#22270;&#37325;&#26032;&#25773;&#25918;&#65292;&#23454;&#29616;&#23545;&#20043;&#21069;&#24050;&#35265;&#31867;&#21035;&#30340;&#21512;&#29702;&#37325;&#24314;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08812</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#24067;&#20808;&#39564;&#19982;&#26174;&#33879;&#24615;&#22270;&#37325;&#26032;&#25773;&#25918;&#30340;&#34701;&#21512;&#65292;&#29992;&#20110;&#36830;&#32493;&#19977;&#32500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual 3D Reconstruction. (arXiv:2308.08812v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#20808;&#39564;&#21644;&#26174;&#33879;&#24615;&#22270;&#37325;&#26032;&#25773;&#25918;&#65292;&#23454;&#29616;&#23545;&#20043;&#21069;&#24050;&#35265;&#31867;&#21035;&#30340;&#21512;&#29702;&#37325;&#24314;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#22270;&#20687;&#19977;&#32500;&#37325;&#24314;&#26159;&#30740;&#31350;&#22914;&#20309;&#26681;&#25454;&#21333;&#35270;&#35282;&#22270;&#20687;&#39044;&#27979;&#19977;&#32500;&#29289;&#20307;&#24418;&#29366;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20010;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#33719;&#21462;&#26469;&#39044;&#27979;&#24418;&#29366;&#30340;&#21487;&#35265;&#21644;&#36974;&#25377;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#38754;&#20020;&#21019;&#24314;&#38024;&#23545;&#25152;&#26377;&#21487;&#33021;&#31867;&#21035;&#30340;&#20840;&#38754;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#20351;&#29992;&#21464;&#20998;&#20808;&#39564;&#30340;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26032;&#31867;&#21035;&#21518;&#20173;&#21487;&#20197;&#21512;&#29702;&#37325;&#24314;&#20197;&#21069;&#35265;&#36807;&#30340;&#31867;&#21035;&#12290;&#21464;&#20998;&#20808;&#39564;&#20195;&#34920;&#25277;&#35937;&#24418;&#29366;&#24182;&#36991;&#20813;&#36951;&#24536;&#65292;&#32780;&#26174;&#33879;&#24615;&#22270;&#20197;&#36739;&#23569;&#30340;&#20869;&#23384;&#20351;&#29992;&#20445;&#30041;&#23545;&#35937;&#23646;&#24615;&#12290;&#36825;&#23545;&#20110;&#23384;&#20648;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#36164;&#28304;&#38480;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26174;&#33879;&#24615;&#22270;&#30340;&#32463;&#39564;&#37325;&#25918;&#65292;&#20197;&#25429;&#25417;&#20840;&#23616;&#21644;&#29420;&#29305;&#30340;&#23545;&#35937;&#29305;&#24449;&#12290;&#35814;&#32454;&#30340;&#23454;&#39564;&#26174;&#31034;&#19982;&#24050;&#24314;&#31435;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-image 3D reconstruction is a research challenge focused on predicting 3D object shapes from single-view images. This task requires significant data acquisition to predict both visible and occluded portions of the shape. Furthermore, learning-based methods face the difficulty of creating a comprehensive training dataset for all possible classes. To this end, we propose a continual learning-based 3D reconstruction method where our goal is to design a model using Variational Priors that can still reconstruct the previously seen classes reasonably even after training on new classes. Variational Priors represent abstract shapes and combat forgetting, whereas saliency maps preserve object attributes with less memory usage. This is vital due to resource constraints in storing extensive training data. Additionally, we introduce saliency map-based experience replay to capture global and distinct object features. Thorough experiments show competitive results compared to established method
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#20559;&#31227;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#26377;&#25928;&#22788;&#29702;&#22312;&#21327;&#21464;&#37327;&#21644;&#26631;&#31614;&#20559;&#31227;&#19979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08810</link><description>&lt;p&gt;
&#26631;&#31614;&#20559;&#31227;&#36866;&#37197;&#22120;&#29992;&#20110;&#21327;&#21464;&#37327;&#21644;&#26631;&#31614;&#20559;&#31227;&#19979;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Label Shift Adapter for Test-Time Adaptation under Covariate and Label Shifts. (arXiv:2308.08810v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08810
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#20559;&#31227;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#26377;&#25928;&#22788;&#29702;&#22312;&#21327;&#21464;&#37327;&#21644;&#26631;&#31614;&#20559;&#31227;&#19979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#36866;&#24212;&#24615; (TTA)&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20197;&#25209;&#27425;&#20026;&#21333;&#20301;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#23613;&#31649;&#23454;&#38469;&#24773;&#20917;&#20013;&#26631;&#31614;&#20998;&#24067;&#24448;&#24448;&#23384;&#22312;&#19981;&#24179;&#34913;&#29616;&#35937;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;TTA&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#37117;&#20855;&#26377;&#24179;&#34913;&#30340;&#26631;&#31614;&#20998;&#24067;&#12290;&#30001;&#20110;&#26576;&#20123;&#31867;&#21035;&#22312;&#26576;&#20123;&#39046;&#22495;&#20013;&#20986;&#29616;&#26356;&#39057;&#32321;&#65288;&#22914;&#22478;&#24066;&#20013;&#30340;&#24314;&#31569;&#29289;&#65292;&#26862;&#26519;&#20013;&#30340;&#26641;&#26408;&#65289;&#65292;&#26631;&#31614;&#20998;&#24067;&#22312;&#39046;&#22495;&#21457;&#29983;&#21464;&#21270;&#26102;&#20250;&#33258;&#28982;&#22320;&#21457;&#29983;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;TTA&#26041;&#27861;&#27809;&#26377;&#24456;&#22909;&#22320;&#35299;&#20915;&#21327;&#21464;&#37327;&#21644;&#26631;&#31614;&#20559;&#31227;&#20849;&#23384;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#20559;&#31227;&#36866;&#37197;&#22120;&#65292;&#23427;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;TTA&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#26377;&#25928;&#22788;&#29702;TTA&#36807;&#31243;&#20013;&#30340;&#26631;&#31614;&#20559;&#31227;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20272;&#35745;&#30446;&#26631;&#39046;&#22495;&#30340;&#26631;&#31614;&#20998;&#24067;&#65292;&#23558;&#20854;&#36755;&#20837;&#21040;&#26631;&#31614;&#20559;&#31227;&#36866;&#37197;&#22120;&#20013;&#12290;&#38543;&#21518;&#65292;&#26631;&#31614;&#20559;&#31227;&#36866;&#37197;&#22120;&#20135;&#29983;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation (TTA) aims to adapt a pre-trained model to the target domain in a batch-by-batch manner during inference. While label distributions often exhibit imbalances in real-world scenarios, most previous TTA approaches typically assume that both source and target domain datasets have balanced label distribution. Due to the fact that certain classes appear more frequently in certain domains (e.g., buildings in cities, trees in forests), it is natural that the label distribution shifts as the domain changes. However, we discover that the majority of existing TTA methods fail to address the coexistence of covariate and label shifts. To tackle this challenge, we propose a novel label shift adapter that can be incorporated into existing TTA approaches to deal with label shifts during the TTA process effectively. Specifically, we estimate the label distribution of the target domain to feed it into the label shift adapter. Subsequently, the label shift adapter produces optimal pa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#38750;&#20010;&#24615;&#21270;&#26041;&#27861;PARE&#65292;&#36890;&#36807;&#39044;&#27979;&#26368;&#39640;&#27969;&#34892;&#24230;&#30340;&#39033;&#30446;&#36827;&#34892;&#25512;&#33616;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#24573;&#30053;&#39033;&#30446;&#27969;&#34892;&#24230;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;PARE&#30340;&#24615;&#33021;&#20248;&#20110;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08799</link><description>&lt;p&gt;
&#25429;&#25417;&#27969;&#34892;&#36235;&#21183;&#65306;&#22686;&#24378;&#39033;&#30446;&#25512;&#33616;&#30340;&#31616;&#21270;&#38750;&#20010;&#24615;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Capturing Popularity Trends: A Simplistic Non-Personalized Approach for Enhanced Item Recommendation. (arXiv:2308.08799v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#38750;&#20010;&#24615;&#21270;&#26041;&#27861;PARE&#65292;&#36890;&#36807;&#39044;&#27979;&#26368;&#39640;&#27969;&#34892;&#24230;&#30340;&#39033;&#30446;&#36827;&#34892;&#25512;&#33616;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#24573;&#30053;&#39033;&#30446;&#27969;&#34892;&#24230;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;PARE&#30340;&#24615;&#33021;&#20248;&#20110;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#20391;&#37325;&#20110;&#36890;&#36807;&#21382;&#21490;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#65292;&#36825;&#21487;&#33021;&#20250;&#20405;&#29359;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#20102;&#39033;&#30446;&#27969;&#34892;&#24230;&#30340;&#26102;&#38388;&#27874;&#21160;&#23545;&#29992;&#25143;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Popularity-Aware Recommender&#65288;PARE&#65289;&#65292;&#36890;&#36807;&#39044;&#27979;&#23558;&#36798;&#21040;&#26368;&#39640;&#27969;&#34892;&#24230;&#30340;&#39033;&#30446;&#26469;&#36827;&#34892;&#38750;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;PARE&#30001;&#22235;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#20998;&#21035;&#20851;&#27880;&#19981;&#21516;&#30340;&#26041;&#38754;&#65306;&#27969;&#34892;&#24230;&#21382;&#21490;&#12289;&#26102;&#38388;&#24433;&#21709;&#12289;&#21608;&#26399;&#24615;&#24433;&#21709;&#21644;&#38468;&#21152;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#34701;&#21512;&#22235;&#20010;&#27169;&#22359;&#30340;&#36755;&#20986;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26126;&#30830;&#24314;&#27169;&#39033;&#30446;&#27969;&#34892;&#24230;&#30340;&#24037;&#20316;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PARE&#30340;&#24615;&#33021;&#19982;&#22797;&#26434;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have been gaining increasing research attention over the years. Most existing recommendation methods focus on capturing users' personalized preferences through historical user-item interactions, which may potentially violate user privacy. Additionally, these approaches often overlook the significance of the temporal fluctuation in item popularity that can sway users' decision-making. To bridge this gap, we propose Popularity-Aware Recommender (PARE), which makes non-personalized recommendations by predicting the items that will attain the highest popularity. PARE consists of four modules, each focusing on a different aspect: popularity history, temporal impact, periodic impact, and side information. Finally, an attention layer is leveraged to fuse the outputs of four modules. To our knowledge, this is the first work to explicitly model item popularity in recommendation systems. Extensive experiments show that PARE performs on par or even better than sophisticated st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#38750;&#24179;&#31283;&#21160;&#21147;&#31995;&#32479;&#28436;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#26816;&#27979;&#26410;&#26469;&#30340;&#32763;&#36710;&#28857;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#27979;&#19982;&#29289;&#29702;&#32422;&#26463;&#30340;&#20559;&#31163;&#26469;&#39044;&#27979;&#32763;&#36710;&#28857;&#65292;&#20174;&#32780;&#20351;&#24471;&#39044;&#27979;&#32467;&#26524;&#20855;&#26377;&#20005;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.08794</link><description>&lt;p&gt;
&#21151;&#33021;&#31354;&#38388;&#20013;&#38750;&#24179;&#31283;&#21160;&#21147;&#23398;&#20013;&#30340;&#32763;&#36710;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Tipping Point Forecasting in Non-Stationary Dynamics on Function Spaces. (arXiv:2308.08794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#38750;&#24179;&#31283;&#21160;&#21147;&#31995;&#32479;&#28436;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#26816;&#27979;&#26410;&#26469;&#30340;&#32763;&#36710;&#28857;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#27979;&#19982;&#29289;&#29702;&#32422;&#26463;&#30340;&#20559;&#31163;&#26469;&#39044;&#27979;&#32763;&#36710;&#28857;&#65292;&#20174;&#32780;&#20351;&#24471;&#39044;&#27979;&#32467;&#26524;&#20855;&#26377;&#20005;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#36710;&#28857;&#26159;&#38750;&#24179;&#31283;&#21644;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#28436;&#21270;&#20013;&#30340;&#31361;&#21464;&#12289;&#21095;&#28872;&#19988;&#24120;&#24120;&#19981;&#21487;&#36870;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#39044;&#35745;&#28201;&#23460;&#27668;&#20307;&#27987;&#24230;&#30340;&#22686;&#21152;&#20250;&#23548;&#33268;&#20302;&#20113;&#35206;&#30422;&#30340;&#24613;&#21095;&#20943;&#23569;&#65292;&#34987;&#31216;&#20026;&#27668;&#20505;&#23398;&#30340;&#32763;&#36710;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24490;&#29615;&#31070;&#32463;&#31639;&#23376;&#65288;RNO&#65289;&#23398;&#20064;&#36825;&#31181;&#38750;&#24179;&#31283;&#21160;&#21147;&#31995;&#32479;&#30340;&#28436;&#21270;&#65292;RNO&#21487;&#20197;&#23398;&#20064;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#22312;&#20165;&#35757;&#32451;RNO&#22312;&#32763;&#36710;&#28857;&#20043;&#21069;&#30340;&#21160;&#21147;&#23398;&#25968;&#25454;&#20043;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#26410;&#26469;&#30340;&#32763;&#36710;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#27979;&#19982;&#29289;&#29702;&#32422;&#26463;&#65288;&#22914;&#23432;&#24658;&#37327;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#65289;&#20559;&#31163;&#26469;&#39044;&#27979;&#32763;&#36710;&#28857;&#65292;&#20174;&#32780;&#20351;&#24471;&#23545;&#36825;&#20123;&#31361;&#21464;&#30340;&#39044;&#27979;&#20276;&#38543;&#30528;&#19968;&#31181;&#20005;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tipping points are abrupt, drastic, and often irreversible changes in the evolution of non-stationary and chaotic dynamical systems. For instance, increased greenhouse gas concentrations are predicted to lead to drastic decreases in low cloud cover, referred to as a climatological tipping point. In this paper, we learn the evolution of such non-stationary dynamical systems using a novel recurrent neural operator (RNO), which learns mappings between function spaces. After training RNO on only the pre-tipping dynamics, we employ it to detect future tipping points using an uncertainty-based approach. In particular, we propose a conformal prediction framework to forecast tipping points by monitoring deviations from physics constraints (such as conserved quantities and partial differential equations), enabling forecasting of these abrupt changes along with a rigorous measure of uncertainty. We illustrate our proposed methodology on non-stationary ordinary and partial differential equations,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22810;EV&#20805;&#30005;/&#25918;&#30005;&#21644;&#22312;&#26368;&#20339;&#21151;&#29575;&#27969;&#19979;&#36816;&#34892;&#30340;&#24452;&#21521;&#20998;&#24067;&#24335;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#20998;&#37197;&#21151;&#29575;&#27969;&#26469;&#35299;&#20915;EV&#20805;&#30005;&#25511;&#21046;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.08792</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#30005;&#21160;&#36710;&#20805;&#30005;&#25511;&#21046;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Reinforcement Learning for Electric Vehicles Charging Control on Distribution Networks. (arXiv:2308.08792v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22810;EV&#20805;&#30005;/&#25918;&#30005;&#21644;&#22312;&#26368;&#20339;&#21151;&#29575;&#27969;&#19979;&#36816;&#34892;&#30340;&#24452;&#21521;&#20998;&#24067;&#24335;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#20998;&#37197;&#21151;&#29575;&#27969;&#26469;&#35299;&#20915;EV&#20805;&#30005;&#25511;&#21046;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#21160;&#36710;&#65288;EVs&#65289;&#30340;&#26222;&#21450;&#65292;&#32500;&#25345;&#30005;&#32593;&#31283;&#23450;&#24615;&#21464;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;EV&#20805;&#30005;&#25511;&#21046;&#31574;&#30053;&#26469;&#31649;&#29702;EV&#30340;&#36710;&#8212;&#32593;&#65288;V2G&#65289;&#21644;&#32593;&#8212;&#36710;&#65288;G2V&#65289;&#27169;&#24335;&#20043;&#38388;&#30340;&#20999;&#25442;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL&#65289;&#24050;&#32463;&#35777;&#26126;&#22312;EV&#20805;&#30005;&#25511;&#21046;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;MADRL&#30340;&#26041;&#27861;&#26410;&#33021;&#32771;&#34385;EV&#20805;&#30005;/&#25918;&#30005;&#30340;&#33258;&#28982;&#21151;&#29575;&#27969;&#20197;&#21450;&#24573;&#35270;&#39550;&#39542;&#21592;&#38544;&#31169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22810;EV&#20805;&#30005;/&#25918;&#30005;&#19982;&#22312;&#26368;&#20339;&#21151;&#29575;&#27969;&#65288;OPF&#65289;&#19979;&#36816;&#34892;&#30340;&#24452;&#21521;&#20998;&#24067;&#24335;&#32593;&#32476;&#65288;RDN&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#26102;&#20998;&#37197;&#21151;&#29575;&#27969;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#26469;&#25551;&#36848;RDN&#36127;&#33655;&#12290;&#23558;EV&#20805;&#30005;&#25511;&#21046;&#38382;&#39064;&#21046;&#23450;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20197;&#25214;&#21040;&#19968;&#31181;&#20248;&#21270;&#30340;&#20805;&#30005;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#36127;&#36733;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing popularity of electric vehicles (EVs), maintaining power grid stability has become a significant challenge. To address this issue, EV charging control strategies have been developed to manage the switch between vehicle-to-grid (V2G) and grid-to-vehicle (G2V) modes for EVs. In this context, multi-agent deep reinforcement learning (MADRL) has proven its effectiveness in EV charging control. However, existing MADRL-based approaches fail to consider the natural power flow of EV charging/discharging in the distribution network and ignore driver privacy. To deal with these problems, this paper proposes a novel approach that combines multi-EV charging/discharging with a radial distribution network (RDN) operating under optimal power flow (OPF) to distribute power flow in real time. A mathematical model is developed to describe the RDN load. The EV charging control problem is formulated as a Markov Decision Process (MDP) to find an optimal charging control strategy that balanc
&lt;/p&gt;</description></item><item><title>APPFLx&#26159;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#30340;&#36328;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#32852;&#21512;&#23398;&#20064;&#24179;&#21488;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#31616;&#21270;&#30340;&#23454;&#39564;&#21551;&#21160;&#36807;&#31243;&#21644;&#21487;&#35270;&#21270;&#30340;&#23454;&#39564;&#29983;&#21629;&#21608;&#26399;&#36861;&#36394;&#21151;&#33021;&#65292;&#20351;&#24471;&#39046;&#22495;&#19987;&#23478;&#21644;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2308.08786</link><description>&lt;p&gt;
APPFLx&#65306;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#30340;&#36328;&#19981;&#21516;&#25968;&#25454;&#28304;&#32852;&#21512;&#23398;&#20064;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
APPFLx: Providing Privacy-Preserving Cross-Silo Federated Learning as a Service. (arXiv:2308.08786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08786
&lt;/p&gt;
&lt;p&gt;
APPFLx&#26159;&#19968;&#20010;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#30340;&#36328;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#32852;&#21512;&#23398;&#20064;&#24179;&#21488;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#31616;&#21270;&#30340;&#23454;&#39564;&#21551;&#21160;&#36807;&#31243;&#21644;&#21487;&#35270;&#21270;&#30340;&#23454;&#39564;&#29983;&#21629;&#21608;&#26399;&#36861;&#36394;&#21151;&#33021;&#65292;&#20351;&#24471;&#39046;&#22495;&#19987;&#23478;&#21644;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#21512;&#20316;&#35757;&#32451;&#24378;&#20581;&#21644;&#27867;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#25935;&#24863;&#30340;&#26412;&#22320;&#25968;&#25454;&#65288;&#22914;&#21307;&#30103;&#21644;&#37329;&#34701;&#25968;&#25454;&#65289;&#12290;&#20026;&#20102;&#31616;&#21270;&#21644;&#21152;&#24555;&#38544;&#31169;&#20445;&#25252;&#30340;&#36328;&#19981;&#21516;&#25968;&#25454;&#28304;&#32852;&#21512;&#23398;&#20064;&#30340;&#37319;&#29992;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;APPFLx&#65292;&#36825;&#26159;&#19968;&#20010;&#21363;&#21487;&#29992;&#24179;&#21488;&#65292;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#30340;&#36328;&#19981;&#21516;&#25968;&#25454;&#28304;&#32852;&#21512;&#23398;&#20064;&#26381;&#21153;&#12290;APPFLx&#20351;&#29992;Globus&#36523;&#20221;&#39564;&#35777;&#20801;&#35768;&#29992;&#25143;&#36731;&#26494;&#23433;&#20840;&#22320;&#36992;&#35831;&#21487;&#20449;&#20219;&#30340;&#21512;&#20316;&#32773;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#20960;&#31181;&#21516;&#27493;&#21644;&#24322;&#27493;&#30340;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#65292;&#31616;&#21270;&#20102;&#32852;&#21512;&#23398;&#20064;&#23454;&#39564;&#30340;&#21551;&#21160;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#36861;&#36394;&#21644;&#21487;&#35270;&#21270;&#32852;&#21512;&#23398;&#20064;&#23454;&#39564;&#30340;&#29983;&#21629;&#21608;&#26399;&#65292;&#20351;&#39046;&#22495;&#19987;&#23478;&#21644;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#33021;&#22815;&#22312;&#19968;&#20010;&#24179;&#21488;&#19978;&#36731;&#26494;&#22320;&#21327;&#35843;&#21644;&#35780;&#20272;&#36328;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#32852;&#21512;&#23398;&#20064;&#12290;APPFLx&#21487;&#22312;&#32447;&#35775;&#38382;https://appflx.link
&lt;/p&gt;
&lt;p&gt;
Cross-silo privacy-preserving federated learning (PPFL) is a powerful tool to collaboratively train robust and generalized machine learning (ML) models without sharing sensitive (e.g., healthcare of financial) local data. To ease and accelerate the adoption of PPFL, we introduce APPFLx, a ready-to-use platform that provides privacy-preserving cross-silo federated learning as a service. APPFLx employs Globus authentication to allow users to easily and securely invite trustworthy collaborators for PPFL, implements several synchronous and asynchronous FL algorithms, streamlines the FL experiment launch process, and enables tracking and visualizing the life cycle of FL experiments, allowing domain experts and ML practitioners to easily orchestrate and evaluate cross-silo FL under one platform. APPFLx is available online at https://appflx.link
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21464;&#23398;&#20064;&#26694;&#26550;EDNIL&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#22836;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21560;&#25910;&#25968;&#25454;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08778</link><description>&lt;p&gt;
&#22810;&#22836;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#19981;&#21464;&#23398;&#20064;&#30340;&#29615;&#22659;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
Environment Diversification with Multi-head Neural Network for Invariant Learning. (arXiv:2308.08778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21464;&#23398;&#20064;&#26694;&#26550;EDNIL&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#22836;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21560;&#25910;&#25968;&#25454;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#36827;&#34892;&#35757;&#32451;&#65307;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#20559;&#31227;&#20250;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#19981;&#21464;&#23398;&#20064;&#65292;&#29992;&#20110;&#25552;&#21462;&#23545;&#20998;&#24067;&#21464;&#21270;&#19981;&#25935;&#24863;&#30340;&#19981;&#21464;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#22836;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21464;&#23398;&#20064;&#26694;&#26550;EDNIL&#65292;&#29992;&#20110;&#21560;&#25910;&#25968;&#25454;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#23545;&#29615;&#22659;&#26377;&#20808;&#39564;&#30693;&#35782;&#25110;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#24378;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#35813;&#31639;&#27861;&#19982;&#26368;&#36817;&#25506;&#35752;&#21464;&#20307;&#29305;&#24449;&#21644;&#19981;&#21464;&#29305;&#24449;&#24615;&#36136;&#30340;&#30740;&#31350;&#26377;&#29702;&#35770;&#32852;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;EDNIL&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#26102;&#20855;&#26377;&#26356;&#24378;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are often trained with empirical risk minimization; however, it has been shown that a shift between training and testing distributions can cause unpredictable performance degradation. On this issue, a research direction, invariant learning, has been proposed to extract invariant features insensitive to the distributional changes. This work proposes EDNIL, an invariant learning framework containing a multi-head neural network to absorb data biases. We show that this framework does not require prior knowledge about environments or strong assumptions about the pre-trained model. We also reveal that the proposed algorithm has theoretical connections to recent studies discussing properties of variant and invariant features. Finally, we demonstrate that models trained with EDNIL are empirically more robust against distributional shifts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#21387;&#32553;&#12289;&#35821;&#35328;&#20844;&#24179;&#24615;&#21644;&#36879;&#26126;&#24615;&#31561;&#26041;&#38754;&#30340;&#35201;&#27714;&#65292;&#24182;&#21457;&#29616;&#24046;&#20998;&#38544;&#31169;&#19982;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#31232;&#30095;&#24615;&#20043;&#38388;&#23384;&#22312;&#30456;&#20114;&#21046;&#32422;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.08774</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#12289;&#35821;&#35328;&#20844;&#24179;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#65306;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21487;&#33021;&#24615;&#21644;&#21487;&#33021;&#24615;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models. (arXiv:2308.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#21387;&#32553;&#12289;&#35821;&#35328;&#20844;&#24179;&#24615;&#21644;&#36879;&#26126;&#24615;&#31561;&#26041;&#38754;&#30340;&#35201;&#27714;&#65292;&#24182;&#21457;&#29616;&#24046;&#20998;&#38544;&#31169;&#19982;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#31232;&#30095;&#24615;&#20043;&#38388;&#23384;&#22312;&#30456;&#20114;&#21046;&#32422;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22914;mBERT&#12289;XLM-R&#21644;BLOOM&#26088;&#22312;&#23454;&#29616;&#22810;&#35821;&#35328;&#27010;&#25324;&#25110;&#21387;&#32553;&#65292;&#20197;&#20415;&#20110;&#36716;&#31227;&#21040;&#22823;&#37327;&#65288;&#21487;&#33021;&#26410;&#30693;&#30340;&#65289;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36824;&#24212;&#35813;&#20855;&#22791;&#38544;&#31169;&#24615;&#12289;&#35821;&#35328;&#20844;&#24179;&#24615;&#21644;&#36879;&#26126;&#24615;&#65292;&#21363;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#35201;&#27714;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#21527;&#65311;&#25105;&#20204;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#21387;&#32553;&#21644;&#35821;&#35328;&#20844;&#24179;&#24615;&#19982;&#24046;&#20998;&#38544;&#31169;&#26159;&#20860;&#23481;&#30340;&#65292;&#20294;&#24046;&#20998;&#38544;&#31169;&#19982;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#31232;&#30095;&#24615;&#26159;&#30456;&#24726;&#30340;&#65292;&#21518;&#32773;&#26159;&#36879;&#26126;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#36824;&#23545;&#20004;&#20010;&#24120;&#35265;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#38544;&#31169;&#20445;&#35777;&#19979;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#21387;&#32553;&#21644;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#31232;&#30095;&#24615;&#65292;&#26356;&#35814;&#32454;&#22320;&#25506;&#35752;&#20102;&#36825;&#20123;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#20849;&#21516;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#23454;&#38469;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingual generalization or compression to facilitate transfer to a large number of (potentially unseen) languages. However, these models should ideally also be private, linguistically fair, and transparent, by relating their predictions to training data. Can these requirements be simultaneously satisfied? We show that multilingual compression and linguistic fairness are compatible with differential privacy, but that differential privacy is at odds with training data influence sparsity, an objective for transparency. We further present a series of experiments on two common NLP tasks and evaluate multilingual compression and training data influence sparsity under different privacy guarantees, exploring these trade-offs in more detail. Our results suggest that we need to develop ways to jointly optimize for these objectives in order to find practical trade-offs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#36710;&#21066;&#36807;&#31243;&#20013;&#20992;&#20855;&#30952;&#25439;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#21512;&#28201;&#24230;&#31561;&#36755;&#20837;&#29305;&#24449;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#20992;&#20855;&#30340;&#29366;&#24577;&#65292;&#20992;&#20855;&#28201;&#24230;&#26159;&#21028;&#26029;&#20992;&#20855;&#30952;&#25439;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.08765</link><description>&lt;p&gt;
&#29992;&#20110;&#36710;&#21066;&#20013;&#20992;&#20855;&#30952;&#25439;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for tool wear prediction in turning. (arXiv:2308.08765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08765
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#36710;&#21066;&#36807;&#31243;&#20013;&#20992;&#20855;&#30952;&#25439;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#21512;&#28201;&#24230;&#31561;&#36755;&#20837;&#29305;&#24449;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#20992;&#20855;&#30340;&#29366;&#24577;&#65292;&#20992;&#20855;&#28201;&#24230;&#26159;&#21028;&#26029;&#20992;&#20855;&#30952;&#25439;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26694;&#26550;&#65292;&#20197;&#20415;&#22312;&#36710;&#21066;&#36807;&#31243;&#20013;&#20026;&#20992;&#20855;&#30952;&#25439;&#39044;&#27979;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#37319;&#29992;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#21152;&#36895;&#24230;&#12289;&#22768;&#23398;&#12289;&#28201;&#24230;&#21644;&#20027;&#36724;&#36716;&#36895;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#20108;&#20998;&#31867;&#35757;&#32451;&#65292;&#29992;&#20110;&#39044;&#27979;&#20999;&#21066;&#36807;&#31243;&#21518;&#20992;&#20855;&#30340;&#29366;&#24577;&#65292;&#20197;&#20108;&#20998;&#31867;&#24418;&#24335;&#34920;&#31034;&#20999;&#21066;&#24037;&#20855;&#26159;&#21542;&#21487;&#29992;&#25110;&#25439;&#22351;&#12290;&#35757;&#32451;&#36807;&#31243;&#20043;&#21518;&#65292;&#20351;&#29992;Shapley&#20934;&#21017;&#35299;&#37322;&#35757;&#32451;&#30340;ML&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#21644;&#20998;&#31867;&#20013;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#35299;&#37322;ML&#20998;&#31867;&#22120;&#39044;&#27979;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#25152;&#26377;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#26045;Shapley&#20934;&#21017;&#20043;&#21518;&#65292;&#30830;&#23450;&#20102;&#20992;&#20855;&#28201;&#24230;&#20316;&#20026;&#21028;&#26029;&#20992;&#20855;&#30952;&#25439;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims develop an Explainable Artificial Intelligence (XAI) framework to facilitate human-understandable solutions for tool wear prediction during turning. A random forest algorithm was used as the supervised Machine Learning (ML) classifier for training and binary classification using acceleration, acoustics, temperature, and spindle speed during the orthogonal tube turning process as input features. The ML classifier was used to predict the condition of the tool after the cutting process, which was determined in a binary class form indicating if the cutting tool was available or failed. After the training process, the Shapley criterion was used to explain the predictions of the trained ML classifier. Specifically, the significance of each input feature in the decision-making and classification was identified to explain the reasoning of the ML classifier predictions. After implementing the Shapley criterion on all testing datasets, the tool temperature was identified as th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;LightGBM&#31639;&#27861;&#21644;&#29305;&#24449;&#24037;&#31243;&#24320;&#23637;&#20102;&#39640;&#25928;&#21830;&#19994;&#38134;&#34892;&#23458;&#25143;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#30740;&#31350;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#29305;&#24449;&#23646;&#24615;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;0.734&#65292;AUC&#36798;&#21040;0.772&#65292;&#20026;&#21830;&#19994;&#38134;&#34892;&#30340;&#20449;&#36151;&#25480;&#20104;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2308.08762</link><description>&lt;p&gt;
&#22522;&#20110;LightGBM&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#39640;&#25928;&#21830;&#19994;&#38134;&#34892;&#23458;&#25143;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Efficient Commercial Bank Customer Credit Risk Assessment Based on LightGBM and Feature Engineering. (arXiv:2308.08762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;LightGBM&#31639;&#27861;&#21644;&#29305;&#24449;&#24037;&#31243;&#24320;&#23637;&#20102;&#39640;&#25928;&#21830;&#19994;&#38134;&#34892;&#23458;&#25143;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#30740;&#31350;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#29305;&#24449;&#23646;&#24615;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;0.734&#65292;AUC&#36798;&#21040;0.772&#65292;&#20026;&#21830;&#19994;&#38134;&#34892;&#30340;&#20449;&#36151;&#25480;&#20104;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#25511;&#21046;&#20449;&#29992;&#39118;&#38505;&#26159;&#21830;&#19994;&#38134;&#34892;&#31283;&#23450;&#36816;&#20316;&#30340;&#20851;&#38190;&#29615;&#33410;&#12290;&#26412;&#25991;&#20027;&#35201;&#22522;&#20110;Kaggle&#19978;&#19968;&#23478;&#22806;&#22269;&#21830;&#19994;&#38134;&#34892;&#30340;&#23458;&#25143;&#20449;&#24687;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;LightGBM&#31639;&#27861;&#26500;&#24314;&#20998;&#31867;&#22120;&#23545;&#23458;&#25143;&#36827;&#34892;&#20998;&#31867;&#65292;&#24110;&#21161;&#38134;&#34892;&#21028;&#26029;&#23458;&#25143;&#20449;&#29992;&#36829;&#32422;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#20027;&#35201;&#22788;&#29702;&#29305;&#24449;&#24037;&#31243;&#65292;&#22914;&#32570;&#22833;&#20540;&#22788;&#29702;&#12289;&#32534;&#30721;&#12289;&#19981;&#24179;&#34913;&#26679;&#26412;&#31561;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#25928;&#26524;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#26032;&#30340;&#29305;&#24449;&#23646;&#24615;&#65292;&#20351;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;0.734&#65292;AUC&#36798;&#21040;0.772&#65292;&#36825;&#36229;&#36807;&#20102;&#35768;&#22810;&#22522;&#20110;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#22120;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20026;&#21830;&#19994;&#38134;&#34892;&#30340;&#20449;&#36151;&#25480;&#20104;&#25552;&#20379;&#19968;&#20123;&#21442;&#32771;&#65292;&#21516;&#26102;&#20063;&#20026;&#20854;&#20182;&#31867;&#20284;&#30740;&#31350;&#25552;&#20379;&#19968;&#20123;&#29305;&#24449;&#22788;&#29702;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective control of credit risk is a key link in the steady operation of commercial banks. This paper is mainly based on the customer information dataset of a foreign commercial bank in Kaggle, and we use LightGBM algorithm to build a classifier to classify customers, to help the bank judge the possibility of customer credit default. This paper mainly deals with characteristic engineering, such as missing value processing, coding, imbalanced samples, etc., which greatly improves the machine learning effect. The main innovation of this paper is to construct new feature attributes on the basis of the original dataset so that the accuracy of the classifier reaches 0.734, and the AUC reaches 0.772, which is more than many classifiers based on the same dataset. The model can provide some reference for commercial banks' credit granting, and also provide some feature processing ideas for other similar studies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReProHRL&#30340;&#23618;&#27425;&#21270;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#22810;&#30446;&#26631;&#23548;&#33322;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#29289;&#20307;&#26816;&#27979;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReProHRL&#26041;&#27861;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08737</link><description>&lt;p&gt;
ReProHRL: &#38754;&#21521;&#22810;&#30446;&#26631;&#23548;&#33322;&#30340;&#30495;&#23454;&#19990;&#30028;&#23618;&#27425;&#21270;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReProHRL: Towards Multi-Goal Navigation in the Real World using Hierarchical Agents. (arXiv:2308.08737v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReProHRL&#30340;&#23618;&#27425;&#21270;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#22810;&#30446;&#26631;&#23548;&#33322;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#29289;&#20307;&#26816;&#27979;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReProHRL&#26041;&#27861;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#24050;&#25104;&#21151;&#29992;&#20110;&#39640;&#31934;&#24230;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#22810;&#30446;&#26631;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38590;&#20197;&#23398;&#20064;&#21040;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#19990;&#30028;&#36827;&#34892;&#24494;&#35843;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#30495;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"ReProHRL"&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#20998;&#23618;&#22810;&#30446;&#26631;&#23548;&#33322;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#29289;&#20307;&#26816;&#27979;&#22120;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#23398;&#20064;&#22810;&#30446;&#26631;&#23548;&#33322;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;ReProHRL&#26041;&#27861;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;&#34429;&#28982;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#21333;&#30446;&#26631;&#23548;&#33322;&#30340;&#31616;&#21333;&#29615;&#22659;&#20013;&#22343;&#23454;&#29616;&#20102;100%&#30340;&#25104;&#21151;&#29575;&#65292;&#20294;&#26159;&#22312;&#22810;&#30446;&#26631;&#23548;&#33322;&#38382;&#39064;&#19978;&#65292;ReProHRL&#26041;&#27861;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots have been successfully used to perform tasks with high precision. In real-world environments with sparse rewards and multiple goals, learning is still a major challenge and Reinforcement Learning (RL) algorithms fail to learn good policies. Training in simulation environments and then fine-tuning in the real world is a common approach. However, adapting to the real-world setting is a challenge. In this paper, we present a method named Ready for Production Hierarchical RL (ReProHRL) that divides tasks with hierarchical multi-goal navigation guided by reinforcement learning. We also use object detectors as a pre-processing step to learn multi-goal navigation and transfer it to the real world. Empirical results show that the proposed ReProHRL method outperforms the state-of-the-art baseline in simulation and real-world environments in terms of both training time and performance. Although both methods achieve a 100% success rate in a simple environment for single goal-based navigati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#21644;&#27604;&#36739;&#20102;&#20808;&#21069;&#26085;&#24535;&#20998;&#26512;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20844;&#20849;&#26085;&#24535;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.08736</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#26085;&#24535;&#34920;&#31034;&#26041;&#27861;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#25928;&#26524;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Log Representation for Log-based Anomaly Detection. (arXiv:2308.08736v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#21644;&#27604;&#36739;&#20102;&#20808;&#21069;&#26085;&#24535;&#20998;&#26512;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20844;&#20849;&#26085;&#24535;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#26159;&#20154;&#20204;&#20102;&#35299;&#36719;&#20214;&#31995;&#32479;&#36816;&#34892;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#26469;&#28304;&#12290;&#30001;&#20110;&#29616;&#20195;&#36719;&#20214;&#26550;&#26500;&#21644;&#32500;&#25252;&#26041;&#27861;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24037;&#20316;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#26085;&#24535;&#20998;&#26512;&#12290;&#22312;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26085;&#24535;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#25991;&#26412;&#26085;&#24535;&#25968;&#25454;&#36716;&#25442;&#20026;&#25968;&#23383;&#29305;&#24449;&#21521;&#37327;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#19988;&#24517;&#19981;&#21487;&#23569;&#30340;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#23545;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#65292;&#36825;&#38480;&#21046;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#36873;&#25321;&#20854;&#33258;&#21160;&#21270;&#26085;&#24535;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#20013;&#26368;&#20339;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#24182;&#27604;&#36739;&#20102;&#20808;&#21069;&#26085;&#24535;&#20998;&#26512;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20845;&#31181;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#19971;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22235;&#31181;&#20844;&#20849;&#26085;&#24535;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logs are an essential source of information for people to understand the running status of a software system. Due to the evolving modern software architecture and maintenance methods, more research efforts have been devoted to automated log analysis. In particular, machine learning (ML) has been widely used in log analysis tasks. In ML-based log analysis tasks, converting textual log data into numerical feature vectors is a critical and indispensable step. However, the impact of using different log representation techniques on the performance of the downstream models is not clear, which limits researchers and practitioners' opportunities of choosing the optimal log representation techniques in their automated log analysis workflows. Therefore, this work investigates and compares the commonly adopted log representation techniques from previous log analysis research. Particularly, we select six log representation techniques and evaluate them with seven ML models and four public log datas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#21160;&#24577;&#26426;&#21046;&#22312;DyNNs&#20013;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#21160;&#24577;&#26426;&#21046;&#35774;&#35745;&#23545;DyNNs&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08709</link><description>&lt;p&gt;
&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;&#65306;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#21160;&#24577;&#26426;&#21046;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dynamic Neural Network is All You Need: Understanding the Robustness of Dynamic Mechanisms in Neural Networks. (arXiv:2308.08709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#21160;&#24577;&#26426;&#21046;&#22312;DyNNs&#20013;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#21160;&#24577;&#26426;&#21046;&#35774;&#35745;&#23545;DyNNs&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#34987;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#26085;&#24120;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;DNNs&#24050;&#34987;&#37096;&#32626;&#22312;&#23454;&#26102;&#31995;&#32479;&#20013;&#65292;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#21644;&#21709;&#24212;&#26102;&#38388;&#24050;&#25104;&#20026;&#24403;&#21153;&#20043;&#24613;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22330;&#26223;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#23558;&#21160;&#24577;&#26426;&#21046;&#24341;&#20837;&#38745;&#24577;DNNs(SDNN)&#20197;&#21019;&#24314;&#33021;&#22815;&#26681;&#25454;&#36755;&#20837;&#22797;&#26434;&#24615;&#25191;&#34892;&#21160;&#24577;&#35745;&#31639;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;(DyNNs)&#12290;&#23613;&#31649;&#22312;&#23454;&#26102;&#31995;&#32479;&#20013;&#23558;&#21160;&#24577;&#26426;&#21046;&#24341;&#20837;SDNNs&#26356;&#21487;&#21462;&#65292;&#20294;&#35780;&#20272;&#21160;&#24577;&#26426;&#21046;&#24341;&#20837;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#20063;&#21464;&#24471;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;SDNNs&#21644;DyNNs&#20043;&#38388;&#30340;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30740;&#31350;&#21160;&#24577;&#26426;&#21046;&#22312;DyNNs&#20013;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#21160;&#24577;&#26426;&#21046;&#35774;&#35745;&#23545;DyNNs&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have been used to solve different day-to-day problems. Recently, DNNs have been deployed in real-time systems, and lowering the energy consumption and response time has become the need of the hour. To address this scenario, researchers have proposed incorporating dynamic mechanism to static DNNs (SDNN) to create Dynamic Neural Networks (DyNNs) performing dynamic amounts of computation based on the input complexity. Although incorporating dynamic mechanism into SDNNs would be preferable in real-time systems, it also becomes important to evaluate how the introduction of dynamic mechanism impacts the robustness of the models. However, there has not been a significant number of works focusing on the robustness trade-off between SDNNs and DyNNs. To address this issue, we propose to investigate the robustness of dynamic mechanism in DyNNs and how dynamic mechanism design impacts the robustness of DyNNs. For that purpose, we evaluate three research questions. These
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26469;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#38382;&#39064;&#12290;&#30740;&#31350;&#20013;&#23545;&#20960;&#31181;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#25512;&#23548;&#20986;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23578;&#19981;&#20855;&#22791;&#24847;&#35782;&#65292;&#20294;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#26080;&#26126;&#26174;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2308.08708</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24847;&#35782;&#65306;&#26469;&#33258;&#24847;&#35782;&#31185;&#23398;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. (arXiv:2308.08708v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26469;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#38382;&#39064;&#12290;&#30740;&#31350;&#20013;&#23545;&#20960;&#31181;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#25512;&#23548;&#20986;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23578;&#19981;&#20855;&#22791;&#24847;&#35782;&#65292;&#20294;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#26080;&#26126;&#26174;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#25110;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#33021;&#20855;&#26377;&#24847;&#35782;&#25104;&#20026;&#31185;&#23398;&#30028;&#20851;&#27880;&#30340;&#35805;&#39064;&#65292;&#20063;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#25285;&#24551;&#12290;&#26412;&#25253;&#21578;&#25552;&#20986;&#24182;&#20030;&#20363;&#20102;&#19968;&#31181;&#20005;&#35880;&#19988;&#32463;&#39564;&#22522;&#30784;&#30340;&#20154;&#24037;&#26234;&#33021;&#24847;&#35782;&#26041;&#27861;&#65306;&#26681;&#25454;&#25105;&#20204;&#30446;&#21069;&#26368;&#21487;&#20449;&#30340;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#23545;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20960;&#31181;&#24191;&#27867;&#35748;&#21487;&#30340;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#65292;&#21253;&#25324;&#24490;&#29615;&#22788;&#29702;&#29702;&#35770;&#12289;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#29702;&#35770;&#12289;&#39640;&#38454;&#29702;&#35770;&#12289;&#39044;&#27979;&#22788;&#29702;&#29702;&#35770;&#21644;&#27880;&#24847;&#27169;&#24335;&#29702;&#35770;&#12290;&#20174;&#36825;&#20123;&#29702;&#35770;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20123;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#20855;&#22791;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25351;&#31034;&#24615;&#29305;&#24449;&#26469;&#35780;&#20272;&#20102;&#20960;&#20010;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#31995;&#32479;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20855;&#26377;&#24847;&#35782;&#65292;&#20294;&#21516;&#26102;&#20063;&#26174;&#31034;&#20986;&#27809;&#26377;&#26126;&#26174;&#30340;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#30340;&#21487;&#35777;&#26126;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#21644;&#35266;&#27979;&#21487;&#33021;&#24615;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#26500;&#24314;&#36817;&#20284;&#27169;&#22411;&#20197;&#23454;&#29616;&#20934;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08705</link><description>&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#19982;&#65288;&#20934;&#65289;&#25928;&#29575;&#65306;&#20449;&#24687;&#20849;&#20139;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Multi-agent RL with (Quasi-)Efficiency: The Blessing of Information Sharing. (arXiv:2308.08705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#30340;&#21487;&#35777;&#26126;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#21644;&#35266;&#27979;&#21487;&#33021;&#24615;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#26500;&#24314;&#36817;&#20284;&#27169;&#22411;&#20197;&#23454;&#29616;&#20934;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;POSGs&#65289;&#30340;&#21487;&#35777;&#26126;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#20026;&#20102;&#35268;&#36991;&#24050;&#30693;&#30340;&#38590;&#24230;&#38382;&#39064;&#21644;&#20351;&#29992;&#35745;&#31639;&#19981;&#21487;&#34892;&#30340;&#39044;&#35328;&#26426;&#65292;&#25105;&#20204;&#20513;&#23548;&#21033;&#29992;Agent&#20043;&#38388;&#30340;&#28508;&#22312;&#8220;&#20449;&#24687;&#20849;&#20139;&#8221;&#65292;&#36825;&#26159;&#23454;&#35777;MARL&#20013;&#30340;&#24120;&#35265;&#20570;&#27861;&#65292;&#20063;&#26159;&#20855;&#22791;&#36890;&#20449;&#21151;&#33021;&#30340;&#22810;Agent&#25511;&#21046;&#31995;&#32479;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#33509;&#24178;&#35745;&#31639;&#22797;&#26434;&#24615;&#32467;&#26524;&#65292;&#26469;&#35777;&#26126;&#20449;&#24687;&#20849;&#20139;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#21450;&#35266;&#27979;&#21487;&#33021;&#24615;&#20551;&#35774;&#20026;&#20102;&#27714;&#35299;POSGs&#20013;&#30340;&#35745;&#31639;&#25928;&#29575;&#24050;&#32463;&#20351;&#24471;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#21333;Agent&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#20934;&#25928;&#29575;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#36827;&#19968;&#27493;&#8220;&#36817;&#20284;&#8221;&#20849;&#20139;&#30340;&#20844;&#20849;&#20449;&#24687;&#26500;&#24314;POSG&#30340;&#8220;&#36817;&#20284;&#27169;&#22411;&#8221;&#65292;&#22312;&#35813;&#27169;&#22411;&#20013;&#35745;&#21010;&#19968;&#20010;&#36817;&#20284;&#22343;&#34913;&#65288;&#20174;&#35299;&#20915;&#21407;&#22987;POSG&#30340;&#35282;&#24230;&#65289;&#21487;&#20197;&#23454;&#29616;&#20934;&#25928;&#29575;&#65292;&#21363;&#20934;&#22810;&#39033;&#24335;&#26102;&#38388;&#65292;&#21069;&#25552;&#26159;&#19978;&#36848;&#20551;&#35774;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study provable multi-agent reinforcement learning (MARL) in the general framework of partially observable stochastic games (POSGs). To circumvent the known hardness results and the use of computationally intractable oracles, we advocate leveraging the potential \emph{information-sharing} among agents, a common practice in empirical MARL, and a standard model for multi-agent control systems with communications. We first establish several computation complexity results to justify the necessity of information-sharing, as well as the observability assumption that has enabled quasi-efficient single-agent RL with partial observations, for computational efficiency in solving POSGs. We then propose to further \emph{approximate} the shared common information to construct an {approximate model} of the POSG, in which planning an approximate equilibrium (in terms of solving the original POSG) can be quasi-efficient, i.e., of quasi-polynomial-time, under the aforementioned assumptions. Furthermo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PiZero&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#24049;&#21019;&#24314;&#30340;&#25277;&#35937;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#32423;&#35268;&#21010;&#65292;&#19981;&#21463;&#30495;&#23454;&#29615;&#22659;&#38480;&#21046;&#65292;&#21487;&#22312;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#22788;&#29702;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#35774;&#32622;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#21487;&#27604;&#30340;&#20808;&#21069;&#26041;&#27861;&#32780;&#26080;&#38656;&#20551;&#35774;&#35775;&#38382;&#29615;&#22659;&#27169;&#25311;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.08693</link><description>&lt;p&gt;
&#35745;&#21010;&#22312;&#24819;&#35937;&#20013;&#65306;&#22522;&#20110;&#23398;&#20064;&#25277;&#35937;&#25628;&#32034;&#31354;&#38388;&#30340;&#39640;&#32423;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning in the imagination: High-level planning on learned abstract search spaces. (arXiv:2308.08693v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PiZero&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#24049;&#21019;&#24314;&#30340;&#25277;&#35937;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#32423;&#35268;&#21010;&#65292;&#19981;&#21463;&#30495;&#23454;&#29615;&#22659;&#38480;&#21046;&#65292;&#21487;&#22312;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#22788;&#29702;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#35774;&#32622;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#21487;&#27604;&#30340;&#20808;&#21069;&#26041;&#27861;&#32780;&#26080;&#38656;&#20551;&#35774;&#35775;&#38382;&#29615;&#22659;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;PiZero&#65292;&#23427;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#24049;&#21019;&#24314;&#30340;&#25277;&#35937;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#35745;&#21010;&#65292;&#35813;&#25628;&#32034;&#31354;&#38388;&#19982;&#30495;&#23454;&#29615;&#22659;&#23436;&#20840;&#35299;&#32806;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#20197;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#39640;&#32423;&#35268;&#21010;&#65292;&#24182;&#20197;&#22797;&#21512;&#25110;&#26102;&#38388;&#25193;&#23637;&#21160;&#20316;&#30340;&#24418;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#22312;&#38656;&#35201;&#25191;&#34892;&#22823;&#37327;&#22522;&#26412;&#24494;&#25805;&#20316;&#20197;&#25191;&#34892;&#30456;&#20851;&#23439;&#25805;&#20316;&#30340;&#29615;&#22659;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#21487;&#27604;&#30340;&#20808;&#21069;&#26041;&#27861;&#26356;&#36890;&#29992;&#65292;&#22240;&#20026;&#23427;&#22788;&#29702;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#23548;&#33322;&#20219;&#21153;&#21644;Sokoban&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#27809;&#26377;&#20551;&#35774;&#35775;&#38382;&#29615;&#22659;&#27169;&#25311;&#22120;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#21487;&#27604;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method, called PiZero, that gives an agent the ability to plan in an abstract search space of its own creation that is completely decoupled from the real environment. Unlike prior approaches, this enables the agent to perform high-level planning at arbitrary timescales and reason in terms of compound or temporally-extended actions, which can be useful in environments where large numbers of base-level micro-actions are needed to perform relevant macro-actions. In addition, our method is more general than comparable prior methods because it handles settings with continuous action spaces and partial observability. We evaluate our method on multiple domains, including navigation tasks and Sokoban. Experimentally, it outperforms comparable prior methods without assuming access to an environment simulator.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#36807;&#25311;&#21512;&#25351;&#25968;&#65288;OI&#65289;&#65292;&#36890;&#36807;&#23545;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;MNIST&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;OI&#30340;&#23454;&#29992;&#24615;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#36807;&#25311;&#21512;&#34892;&#20026;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#36739;&#23567;&#21644;&#26356;&#19987;&#19994;&#30340;&#25968;&#25454;&#38598;&#30340;&#32531;&#35299;&#24433;&#21709;&#12290;OI&#20026;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2308.08682</link><description>&lt;p&gt;
&#37327;&#21270;&#36807;&#25311;&#21512;: &#24341;&#20837;&#36807;&#25311;&#21512;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Quantifying Overfitting: Introducing the Overfitting Index. (arXiv:2308.08682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#36807;&#25311;&#21512;&#25351;&#25968;&#65288;OI&#65289;&#65292;&#36890;&#36807;&#23545;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;MNIST&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;OI&#30340;&#23454;&#29992;&#24615;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#36807;&#25311;&#21512;&#34892;&#20026;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#36739;&#23567;&#21644;&#26356;&#19987;&#19994;&#30340;&#25968;&#25454;&#38598;&#30340;&#32531;&#35299;&#24433;&#21709;&#12290;OI&#20026;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#36807;&#25311;&#21512;&#26159;&#25351;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#29616;&#35937;&#65292;&#19968;&#30452;&#26159;&#20010;&#19981;&#23481;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#36807;&#25311;&#21512;&#25351;&#25968;&#65288;OI&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#20542;&#21521;&#12290;&#36890;&#36807;&#23545;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;BUS&#65289;&#21644;MNIST&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;MobileNet&#12289;U-Net&#12289;ResNet&#12289;Darknet&#21644;ViT-32&#31561;&#26550;&#26500;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;OI&#30340;&#23454;&#29992;&#24615;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#19981;&#21516;&#26550;&#26500;&#20043;&#38388;&#30340;&#36807;&#25311;&#21512;&#34892;&#20026;&#30340;&#21464;&#21270;&#65292;&#24182;&#24378;&#35843;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#36739;&#23567;&#21644;&#26356;&#19987;&#19994;&#30340;&#25968;&#25454;&#38598;&#30340;&#32531;&#35299;&#24433;&#21709;&#12290;ViT-32&#22312;MNIST&#19978;&#30340;&#34920;&#29616;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#26576;&#20123;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#24615;&#12290;&#36890;&#36807;&#25552;&#20379;&#23458;&#35266;&#35270;&#35282;&#26469;&#35780;&#20272;&#36807;&#25311;&#21512;&#65292;OI&#20026;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving domain of machine learning, ensuring model generalizability remains a quintessential challenge. Overfitting, where a model exhibits superior performance on training data but falters on unseen data, is a recurrent concern. This paper introduces the Overfitting Index (OI), a novel metric devised to quantitatively assess a model's tendency to overfit. Through extensive experiments on the Breast Ultrasound Images Dataset (BUS) and the MNIST dataset using architectures such as MobileNet, U-Net, ResNet, Darknet, and ViT-32, we illustrate the utility and discernment of the OI. Our results underscore the variable overfitting behaviors across architectures and highlight the mitigative impact of data augmentation, especially on smaller and more specialized datasets. The ViT-32's performance on MNIST further emphasizes the robustness of certain models and the dataset's comprehensive nature. By providing an objective lens to gauge overfitting, the OI offers a promising aven
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#30340;&#36731;&#37327;&#32423;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#26082;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21448;&#22823;&#24133;&#20943;&#23569;&#20102;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2308.08669</link><description>&lt;p&gt;
SkinDistilViT: &#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#30340;&#36731;&#37327;&#32423;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
SkinDistilViT: Lightweight Vision Transformer for Skin Lesion Classification. (arXiv:2308.08669v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#30340;&#36731;&#37327;&#32423;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#26082;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21448;&#22823;&#24133;&#20943;&#23569;&#20102;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#21450;&#26089;&#21457;&#29616;&#65292;&#30382;&#32932;&#30284;&#26159;&#19968;&#31181;&#21487;&#27835;&#30103;&#30340;&#30142;&#30149;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#38024;&#23545;&#30382;&#32932;&#30284;&#20998;&#31867;&#38382;&#39064;&#30340;&#29983;&#20135;&#19987;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#31181;&#35270;&#35273;Transformer&#26469;&#21305;&#37197;&#19987;&#23478;&#27880;&#37322;&#30340;&#40657;&#33394;&#32032;&#30244;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#20154;&#31867;&#34920;&#29616;&#12290;&#30001;&#20110;&#25512;&#29702;&#25104;&#26412;&#65292;&#22312;&#26102;&#38388;&#21644;&#20869;&#23384;&#26041;&#38754;&#37117;&#38750;&#24120;&#37325;&#35201;&#65292;&#25105;&#20204;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#33719;&#24471;&#19968;&#20010;&#22312;&#31934;&#24230;&#26041;&#38754;&#20445;&#30041;&#20102;98.33%&#30340;&#32769;&#24072;&#30340;&#24179;&#34913;&#22810;&#31867;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#25104;&#26412;&#21482;&#26159;&#32769;&#24072;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#23601;&#20869;&#23384;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#32769;&#24072;&#27169;&#22411;&#23567;&#20102;49.60%&#12290;&#20174;&#26102;&#38388;&#19978;&#30475;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;GPU&#19978;&#24555;&#20102;69.25%&#65292;&#22312;CPU&#19978;&#24555;&#20102;97.96%&#12290;&#36890;&#36807;&#22312;Transformer&#30340;&#27599;&#20010;&#23618;&#32423;&#19978;&#28155;&#21152;&#20998;&#31867;&#22836;&#24182;&#37319;&#29992;&#32423;&#32852;&#33976;&#39311;&#36807;&#31243;&#65292;&#25105;&#20204;&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#24179;&#34913;&#22810;&#31867;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;2.1%&#65292;&#21516;&#26102;&#21019;&#24314;&#20102;&#19968;&#31995;&#21015;&#30340;&#27169;&#22411;&#65292;&#20854;&#23610;&#23544;&#21508;&#19981;&#30456;&#21516;&#20294;&#24615;&#33021;&#30456;&#24403;&#12290;&#25105;&#20204;&#22312;https://github.com/Longman-Stan/SkinDistilVit&#19978;&#25552;&#20379;&#20102;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin cancer is a treatable disease if discovered early. We provide a production-specific solution to the skin cancer classification problem that matches human performance in melanoma identification by training a vision transformer on melanoma medical images annotated by experts. Since inference cost, both time and memory wise is important in practice, we employ knowledge distillation to obtain a model that retains 98.33% of the teacher's balanced multi-class accuracy, at a fraction of the cost. Memory-wise, our model is 49.60% smaller than the teacher. Time-wise, our solution is 69.25% faster on GPU and 97.96% faster on CPU. By adding classification heads at each level of the transformer and employing a cascading distillation process, we improve the balanced multi-class accuracy of the base model by 2.1%, while creating a range of models of various sizes but comparable performance. We provide the code at https://github.com/Longman-Stan/SkinDistilVit.
&lt;/p&gt;</description></item><item><title>BREATHE&#26159;&#19968;&#20010;&#22522;&#20110;&#20108;&#38454;&#26799;&#24230;&#21644;&#24322;&#26041;&#24046;&#27169;&#25311;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25628;&#32034;&#20256;&#32479;&#30340;&#21521;&#37327;&#35774;&#35745;&#31354;&#38388;&#21644;&#22522;&#20110;&#22270;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#24182;&#22312;&#21508;&#33258;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.08666</link><description>&lt;p&gt;
BREATHE: &#22522;&#20110;&#20108;&#38454;&#26799;&#24230;&#21644;&#24322;&#26041;&#24046;&#27169;&#25311;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
BREATHE: Second-Order Gradients and Heteroscedastic Emulation based Design Space Exploration. (arXiv:2308.08666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08666
&lt;/p&gt;
&lt;p&gt;
BREATHE&#26159;&#19968;&#20010;&#22522;&#20110;&#20108;&#38454;&#26799;&#24230;&#21644;&#24322;&#26041;&#24046;&#27169;&#25311;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25628;&#32034;&#20256;&#32479;&#30340;&#21521;&#37327;&#35774;&#35745;&#31354;&#38388;&#21644;&#22522;&#20110;&#22270;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#24182;&#22312;&#21508;&#33258;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#31181;&#31185;&#23398;&#30740;&#31350;&#21644;&#29289;&#29702;&#23454;&#39564;&#20013;&#19981;&#26029;&#21162;&#21147;&#25506;&#32034;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#26597;&#24448;&#24448;&#28041;&#21450;&#22797;&#26434;&#30340;&#27169;&#25311;&#22120;&#25110;&#32791;&#26102;&#30340;&#23454;&#39564;&#65292;&#20351;&#24471;&#25506;&#32034;&#21644;&#35266;&#23519;&#26032;&#30340;&#35774;&#35745;&#26679;&#26412;&#21464;&#24471;&#22256;&#38590;&#12290;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#23616;&#38480;&#30340;&#20808;&#21069;&#24037;&#20316;&#36890;&#24120;&#26159;&#26679;&#26412;&#20302;&#25928;&#30340;&#65292;&#24182;&#19988;&#23616;&#38480;&#20110;&#21521;&#37327;&#25628;&#32034;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26694;&#26550;&#65292;&#31216;&#20026;BREATHE&#65292;&#23427;&#19981;&#20165;&#25628;&#32034;&#20256;&#32479;&#30340;&#22522;&#20110;&#21521;&#37327;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#36824;&#25628;&#32034;&#22522;&#20110;&#22270;&#30340;&#35774;&#35745;&#31354;&#38388;&#20197;&#33719;&#21462;&#34920;&#29616;&#26368;&#20339;&#30340;&#22270;&#24418;&#12290;&#23427;&#21033;&#29992;&#20108;&#38454;&#26799;&#24230;&#24182;&#20027;&#21160;&#35757;&#32451;&#24322;&#26041;&#24046;&#27169;&#22411;&#20197;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#20248;&#21270;&#12290;&#22312;&#21333;&#30446;&#26631;&#21521;&#37327;&#20248;&#21270;&#24212;&#29992;&#20013;&#65292;&#20854;&#24615;&#33021;&#27604;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#31561;&#19979;&#19968;&#20010;&#26368;&#20339;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;64.1&#65285;&#12290;&#22312;&#22522;&#20110;&#22270;&#30340;&#25628;&#32034;&#20013;&#65292;BREATHE&#20248;&#20110;&#19979;&#19968;&#20010;&#26368;&#20339;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers constantly strive to explore larger and more complex search spaces in various scientific studies and physical experiments. However, such investigations often involve sophisticated simulators or time-consuming experiments that make exploring and observing new design samples challenging. Previous works that target such applications are typically sample-inefficient and restricted to vector search spaces. To address these limitations, this work proposes a constrained multi-objective optimization (MOO) framework, called BREATHE, that searches not only traditional vector-based design spaces but also graph-based design spaces to obtain best-performing graphs. It leverages second-order gradients and actively trains a heteroscedastic surrogate model for sample-efficient optimization. In a single-objective vector optimization application, it leads to 64.1% higher performance than the next-best baseline, random forest regression. In graph-based search, BREATHE outperforms the next-bes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19982;&#27599;&#20010;&#38750;&#27954;&#22269;&#23478;&#30456;&#20851;&#30340;&#22320;&#29702;&#26631;&#31614;&#30340;Flickr&#22270;&#20687;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#12289;&#20154;&#20026;&#20013;&#24515;&#30340;&#35270;&#35273;&#25968;&#25454;&#20013;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#29616;&#26377;&#25968;&#25454;&#23384;&#22312;&#20559;&#35265;&#65292;&#24182;&#20986;&#29616;&#20102;&#8220;&#20182;&#32773;&#21270;&#8221;&#29616;&#35937;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#26469;&#33719;&#21462;&#20195;&#34920;&#38750;&#27954;&#20154;&#21644;&#20182;&#20204;&#29615;&#22659;&#30340;&#22270;&#20687;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08656</link><description>&lt;p&gt;
Flickr Africa: &#22312;&#22823;&#35268;&#27169;&#12289;&#20154;&#20026;&#20013;&#24515;&#30340;&#35270;&#35273;&#25968;&#25454;&#20013;&#32771;&#23519;&#22320;&#29702;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Flickr Africa: Examining Geo-Diversity in Large-Scale, Human-Centric Visual Data. (arXiv:2308.08656v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19982;&#27599;&#20010;&#38750;&#27954;&#22269;&#23478;&#30456;&#20851;&#30340;&#22320;&#29702;&#26631;&#31614;&#30340;Flickr&#22270;&#20687;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#12289;&#20154;&#20026;&#20013;&#24515;&#30340;&#35270;&#35273;&#25968;&#25454;&#20013;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#29616;&#26377;&#25968;&#25454;&#23384;&#22312;&#20559;&#35265;&#65292;&#24182;&#20986;&#29616;&#20102;&#8220;&#20182;&#32773;&#21270;&#8221;&#29616;&#35937;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#26469;&#33719;&#21462;&#20195;&#34920;&#38750;&#27954;&#20154;&#21644;&#20182;&#20204;&#29615;&#22659;&#30340;&#22270;&#20687;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#35265;&#20250;&#24433;&#21709;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#22320;&#29702;&#32972;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35843;&#26597;&#26631;&#20934;&#20114;&#32852;&#32593;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;&#19982;&#38750;&#27954;&#27599;&#20010;&#22269;&#23478;&#30456;&#20851;&#30340;&#24102;&#26377;&#22320;&#29702;&#26631;&#31614;&#30340;Flickr&#22270;&#20687;&#26469;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#20154;&#20026;&#20013;&#24515;&#22270;&#20687;&#22320;&#29702;&#22810;&#26679;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#21487;&#29992;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#20869;&#23481;&#65292;&#24182;&#19982;&#19982;&#20043;&#23545;&#24212;&#30340;&#27431;&#27954;&#22269;&#23478;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#21450;&#26681;&#25454;&#32454;&#31890;&#24230;&#22269;&#20869;&#36130;&#23500;&#20272;&#35745;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#24180;&#19968;&#27425;&#30340;&#26102;&#38388;&#20998;&#26512;&#65292;&#20197;&#25581;&#31034;&#20986;&#29616;&#30340;&#25968;&#25454;&#36235;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21576;&#29616;&#20102;&#19968;&#20010;&#8220;&#20182;&#32773;&#21270;&#8221;&#29616;&#35937;&#30340;&#21457;&#29616;&#65292;&#34920;&#26126;&#38750;&#27954;&#22823;&#37327;&#22270;&#20687;&#26159;&#30001;&#38750;&#26412;&#22320;&#25668;&#24433;&#24072;&#25293;&#25668;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#24037;&#20316;&#65292;&#20197;&#25429;&#25417;&#20855;&#26377;&#38750;&#27954;&#20154;&#21644;&#20182;&#20204;&#29615;&#22659;&#20195;&#34920;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biases in large-scale image datasets are known to influence the performance of computer vision models as a function of geographic context. To investigate the limitations of standard Internet data collection methods in low- and middle-income countries, we analyze human-centric image geo-diversity on a massive scale using geotagged Flickr images associated with each nation in Africa. We report the quantity and content of available data with comparisons to population-matched nations in Europe as well as the distribution of data according to fine-grained intra-national wealth estimates. Temporal analyses are performed at two-year intervals to expose emerging data trends. Furthermore, we present findings for an ``othering'' phenomenon as evidenced by a substantial number of images from Africa being taken by non-local photographers. The results of our study suggest that further work is required to capture image data representative of African people and their environments and, ultimately, to 
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#20449;&#24687;&#21270;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#22320;&#38663;&#21709;&#24212;&#35780;&#20272;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#27169;&#24335;&#35782;&#21035;&#25216;&#26415;&#65292;&#22312;&#23454;&#26102;&#24615;&#33021;&#35201;&#27714;&#30340;&#24212;&#29992;&#20013;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.08655</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#21270;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#22320;&#38663;&#21709;&#24212;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Physics Informed Recurrent Neural Networks for Seismic Response Evaluation of Nonlinear Systems. (arXiv:2308.08655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08655
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#21270;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#22320;&#38663;&#21709;&#24212;&#35780;&#20272;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#27169;&#24335;&#35782;&#21035;&#25216;&#26415;&#65292;&#22312;&#23454;&#26102;&#24615;&#33021;&#35201;&#27714;&#30340;&#24212;&#29992;&#20013;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#24037;&#31243;&#20013;&#30340;&#21160;&#24577;&#21709;&#24212;&#35780;&#20272;&#26159;&#30830;&#23450;&#32467;&#26500;&#22312;&#22320;&#38663;&#12289;&#39118;&#25110;&#20914;&#20987;&#31561;&#21160;&#24577;&#33655;&#36733;&#20316;&#29992;&#19979;&#30340;&#21709;&#24212;&#65292;&#22914;&#26500;&#20214;&#21147;&#12289;&#33410;&#28857;&#20301;&#31227;&#31561;&#30340;&#36807;&#31243;&#12290;&#36825;&#26159;&#32467;&#26500;&#20998;&#26512;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#23427;&#20351;&#24037;&#31243;&#24072;&#33021;&#22815;&#35780;&#20272;&#22312;&#26497;&#31471;&#33655;&#36733;&#26465;&#20214;&#19979;&#30340;&#32467;&#26500;&#24615;&#33021;&#65292;&#24182;&#23545;&#32467;&#26500;&#30340;&#35774;&#35745;&#21644;&#23433;&#20840;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#20256;&#32479;&#30340;&#21160;&#24577;&#21709;&#24212;&#35780;&#20272;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#26377;&#38480;&#20803;&#20998;&#26512;&#65288;FEA&#65289;&#36827;&#34892;&#25968;&#20540;&#27169;&#25311;&#65292;&#20854;&#20013;&#32467;&#26500;&#20351;&#29992;&#26377;&#38480;&#20803;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#36816;&#21160;&#26041;&#31243;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#65292;&#20294;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#24182;&#19988;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#23454;&#26102;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#29305;&#21035;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24050;&#32463;&#23558;&#20854;&#24212;&#29992;&#20110;&#32467;&#26500;&#24037;&#31243;&#20013;&#30340;&#21160;&#24577;&#21709;&#24212;&#35780;&#20272;&#12290;&#36825;&#20123;&#25216;&#26415;&#21033;&#29992;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#27169;&#24335;&#35782;&#21035;&#33021;&#21147;&#26469;&#27169;&#25311;&#32467;&#26500;&#21709;&#24212;&#65292;&#24182;&#33021;&#22815;&#22312;&#23454;&#26102;&#24615;&#33021;&#35201;&#27714;&#30340;&#24212;&#29992;&#20013;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic response evaluation in structural engineering is the process of determining the response of a structure, such as member forces, node displacements, etc when subjected to dynamic loads such as earthquakes, wind, or impact. This is an important aspect of structural analysis, as it enables engineers to assess structural performance under extreme loading conditions and make informed decisions about the design and safety of the structure. Conventional methods for dynamic response evaluation involve numerical simulations using finite element analysis (FEA), where the structure is modeled using finite elements, and the equations of motion are solved numerically. Although effective, this approach can be computationally intensive and may not be suitable for real-time applications. To address these limitations, recent advancements in machine learning, specifically artificial neural networks, have been applied to dynamic response evaluation in structural engineering. These techniques leve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#39640;&#20809;&#35889;&#20016;&#24230;&#39044;&#27979;&#30340;&#37325;&#26500;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#36127;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#26500;&#24314;&#31232;&#30095;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#26368;&#22823;&#20284;&#28982;&#21387;&#32553;&#21521;&#37327;&#20943;&#23569;&#20449;&#24687;&#25439;&#22833;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20809;&#35889;&#37325;&#24314;&#35823;&#24046;&#21644;&#21387;&#32553;&#29575;&#26041;&#38754;&#30456;&#27604;&#26631;&#20934;&#20462;&#21098;&#12289;&#26368;&#23567;&#20108;&#20056;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08653</link><description>&lt;p&gt;
&#29992;&#20110;&#31232;&#30095;&#39640;&#20809;&#35889;&#20016;&#24230;&#39044;&#27979;&#30340;&#37325;&#26500;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Reproducing Kernel Hilbert Space Pruning for Sparse Hyperspectral Abundance Prediction. (arXiv:2308.08653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#39640;&#20809;&#35889;&#20016;&#24230;&#39044;&#27979;&#30340;&#37325;&#26500;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#36127;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#26500;&#24314;&#31232;&#30095;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#26368;&#22823;&#20284;&#28982;&#21387;&#32553;&#21521;&#37327;&#20943;&#23569;&#20449;&#24687;&#25439;&#22833;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20809;&#35889;&#37325;&#24314;&#35823;&#24046;&#21644;&#21387;&#32553;&#29575;&#26041;&#38754;&#30456;&#27604;&#26631;&#20934;&#20462;&#21098;&#12289;&#26368;&#23567;&#20108;&#20056;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#36317;&#31163;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#39640;&#20809;&#35889;&#27979;&#37327;&#25968;&#25454;&#21487;&#20197;&#32473;&#20986;&#22330;&#26223;&#20013;&#29289;&#21697;&#12289;&#26448;&#26009;&#21644;&#21270;&#23398;&#21697;&#30340;&#35814;&#32454;&#24773;&#20917;&#65292;&#20294;&#30001;&#20110;&#20808;&#36827;&#20256;&#24863;&#22120;&#30340;&#39640;&#31354;&#38388;&#21644;&#20809;&#35889;&#20998;&#36776;&#29575;&#65292;&#20998;&#26512;&#21464;&#24471;&#22256;&#38590;&#12289;&#32531;&#24930;&#21644;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#31232;&#30095;&#24615;&#23545;&#20110;&#23454;&#29616;&#20809;&#35889;&#21387;&#32553;&#21644;&#20998;&#26512;&#30340;&#26410;&#26469;&#38750;&#24120;&#37325;&#35201;&#12290;&#24050;&#32463;&#35266;&#23519;&#21040;&#29615;&#22659;&#21644;&#22823;&#27668;&#25928;&#24212;&#65292;&#21253;&#25324;&#25955;&#23556;&#65292;&#20250;&#20135;&#29983;&#38750;&#32447;&#24615;&#25928;&#24212;&#65292;&#23545;&#20110;&#29616;&#26377;&#30340;&#28304;&#20998;&#31163;&#21644;&#21387;&#32553;&#26041;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#36127;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#36827;&#34892;&#20462;&#21098;&#21644;&#26500;&#24314;&#31232;&#30095;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#22823;&#20284;&#28982;&#21387;&#32553;&#21521;&#37327;&#26469;&#20943;&#23569;&#20449;&#24687;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26631;&#20934;&#20462;&#21098;&#21644;&#26368;&#23567;&#20108;&#20056;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32771;&#23519;&#20102;&#24635;&#20307;&#20809;&#35889;&#37325;&#24314;&#35823;&#24046;&#21644;&#21387;&#32553;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral measurements from long range sensors can give a detailed picture of the items, materials, and chemicals in a scene but analysis can be difficult, slow, and expensive due to high spatial and spectral resolutions of state-of-the-art sensors. As such, sparsity is important to enable the future of spectral compression and analytics. It has been observed that environmental and atmospheric effects, including scattering, can produce nonlinear effects posing challenges for existing source separation and compression methods. We present a novel transformation into Hilbert spaces for pruning and constructing sparse representations via non-negative least squares minimization. Then we introduce max likelihood compression vectors to decrease information loss. Our approach is benchmarked against standard pruning and least squares as well as deep learning methods. Our methods are evaluated in terms of overall spectral reconstruction error and compression rate using real and synthetic dat
&lt;/p&gt;</description></item><item><title>AdaptEx&#26159;&#19968;&#20010;&#33258;&#21161;&#19978;&#19979;&#25991;&#36172;&#21338;&#24179;&#21488;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#20010;&#24615;&#21270;&#29992;&#25143;&#20307;&#39564;&#24182;&#25552;&#20379;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20256;&#32479;&#27979;&#35797;&#26041;&#27861;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;&#23427;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#20869;&#23481;&#21644;&#8220;&#20919;&#21551;&#21160;&#8221;&#24773;&#20917;&#19979;&#24555;&#36895;&#36845;&#20195;&#12290;</title><link>http://arxiv.org/abs/2308.08650</link><description>&lt;p&gt;
AdaptEx&#65306;&#19968;&#20010;&#33258;&#21161;&#19978;&#19979;&#25991;&#36172;&#21338;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
AdaptEx: A Self-Service Contextual Bandit Platform. (arXiv:2308.08650v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08650
&lt;/p&gt;
&lt;p&gt;
AdaptEx&#26159;&#19968;&#20010;&#33258;&#21161;&#19978;&#19979;&#25991;&#36172;&#21338;&#24179;&#21488;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#20010;&#24615;&#21270;&#29992;&#25143;&#20307;&#39564;&#24182;&#25552;&#20379;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20256;&#32479;&#27979;&#35797;&#26041;&#27861;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;&#23427;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#20869;&#23481;&#21644;&#8220;&#20919;&#21551;&#21160;&#8221;&#24773;&#20917;&#19979;&#24555;&#36895;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AdaptEx&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;Expedia Group&#24191;&#27867;&#20351;&#29992;&#30340;&#33258;&#21161;&#19978;&#19979;&#25991;&#36172;&#21338;&#24179;&#21488;&#65292;&#23427;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#20197;&#35268;&#27169;&#21270;&#30340;&#26041;&#24335;&#20010;&#24615;&#21270;&#29992;&#25143;&#20307;&#39564;&#12290;AdaptEx&#32771;&#34385;&#20102;&#27599;&#20010;&#35775;&#38382;&#32773;&#30340;&#29420;&#29305;&#19978;&#19979;&#25991;&#65292;&#36873;&#25321;&#20102;&#26368;&#20248;&#30340;&#21464;&#20307;&#65292;&#24182;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#27599;&#27425;&#20114;&#21160;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26082;&#33021;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#65292;&#21516;&#26102;&#21448;&#33021;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20256;&#32479;&#27979;&#35797;&#26041;&#27861;&#25152;&#38656;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;&#35813;&#24179;&#21488;&#33021;&#22815;&#22312;&#20869;&#23481;&#19981;&#26029;&#21464;&#21270;&#21644;&#25345;&#32493;&#8220;&#20919;&#21551;&#21160;&#8221;&#24773;&#20917;&#19979;&#65292;&#20248;&#38597;&#22320;&#24555;&#36895;&#36845;&#20195;&#26397;&#30528;&#26368;&#20248;&#35299;&#21069;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents AdaptEx, a self-service contextual bandit platform widely used at Expedia Group, that leverages multi-armed bandit algorithms to personalize user experiences at scale. AdaptEx considers the unique context of each visitor to select the optimal variants and learns quickly from every interaction they make. It offers a powerful solution to improve user experiences while minimizing the costs and time associated with traditional testing methods. The platform unlocks the ability to iterate towards optimal product solutions quickly, even in ever-changing content and continuous "cold start" situations gracefully.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedHR&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;pFedHR&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#20351;&#29992;&#19981;&#20860;&#23481;&#25968;&#25454;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.08643</link><description>&lt;p&gt;
&#36890;&#36807;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Federated Learning via Heterogeneous Model Reassembly. (arXiv:2308.08643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedHR&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;pFedHR&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#20351;&#29992;&#19981;&#20860;&#23481;&#25968;&#25454;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#27169;&#22411;&#24322;&#26500;&#30340;&#23454;&#38469;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20855;&#26377;&#19981;&#21516;&#32593;&#32476;&#32467;&#26500;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedHR&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24322;&#26500;&#27169;&#22411;&#20010;&#24615;&#21270;&#38382;&#39064;&#35270;&#20026;&#26381;&#21153;&#22120;&#31471;&#30340;&#27169;&#22411;&#21305;&#37197;&#20248;&#21270;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;pFedHR&#33021;&#22815;&#33258;&#21160;&#19988;&#21160;&#24577;&#22320;&#29983;&#25104;&#20855;&#26377;&#26368;&#23567;&#20154;&#24037;&#24178;&#39044;&#30340;&#20449;&#24687;&#20016;&#23500;&#19988;&#22810;&#26679;&#21270;&#30340;&#20010;&#24615;&#21270;&#20505;&#36873;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#25216;&#26415;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#36731;&#20102;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#20844;&#20849;&#25968;&#25454;&#19982;&#23458;&#25143;&#31471;&#25968;&#25454;&#36896;&#25104;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;IID&#21644;&#38750;IID&#35774;&#32622;&#19979;&#65292;pFedHR&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;pFedHR&#26377;&#25928;&#38477;&#20302;&#20102;&#20351;&#29992;&#19981;&#20860;&#23481;&#25968;&#25454;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of usi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#32771;&#34385;&#20102;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#28789;&#27963;&#21644;&#22266;&#23450;&#38271;&#24230;&#32422;&#26463;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.08641</link><description>&lt;p&gt;
&#38750;&#21333;&#35843;&#36882;&#22686;&#30340;&#39034;&#24207;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Non-monotone Sequential Submodular Maximization. (arXiv:2308.08641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#32771;&#34385;&#20102;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#28789;&#27963;&#21644;&#22266;&#23450;&#38271;&#24230;&#32422;&#26463;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23376;&#27169;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#39034;&#24207;&#23376;&#27169;&#26368;&#22823;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#19968;&#20010;&#24453;&#36873;&#38598;&#21512;V&#20013;&#36873;&#25321;&#24182;&#23545;&#19968;&#32452;k&#20010;&#39033;&#36827;&#34892;&#25490;&#24207;&#65292;&#20351;&#24471;k&#20010;&#65288;&#21487;&#33021;&#20026;&#38750;&#21333;&#35843;&#36882;&#22686;&#30340;&#65289;&#23376;&#27169;&#20989;&#25968;f1&#65292;...&#65292;fk&#65288;&#20854;&#20013;&#27599;&#20010;&#20989;&#25968;fj&#23558;&#36825;&#20010;&#24207;&#21015;&#30340;&#21069;j&#20010;&#39033;&#20316;&#20026;&#36755;&#20837;&#65289;&#30340;&#21152;&#26435;&#27714;&#21644;&#26368;&#22823;&#21270;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#35843;&#35774;&#32622;&#19978;&#65292;&#20551;&#35774;&#23376;&#27169;&#20989;&#25968;&#26159;&#38750;&#36882;&#20943;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27604;&#22914;&#22810;&#26679;&#24615;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#21521;&#29616;&#26377;&#38598;&#21512;&#28155;&#21152;&#39033;&#21487;&#33021;&#20250;&#23545;&#25972;&#20307;&#25928;&#29992;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#28789;&#27963;&#21644;&#22266;&#23450;&#38271;&#24230;&#32422;&#26463;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study a fundamental problem in submodular optimization, which is called sequential submodular maximization. Specifically, we aim to select and rank a group of $k$ items from a ground set $V$ such that the weighted summation of $k$ (possibly non-monotone) submodular functions $f_1, \cdots ,f_k: 2^V \rightarrow \mathbb{R}^+$ is maximized, here each function $f_j$ takes the first $j$ items from this sequence as input. The existing research on sequential submodular maximization has predominantly concentrated on the monotone setting, assuming that the submodular functions are non-decreasing. However, in various real-world scenarios, like diversity-aware recommendation systems, adding items to an existing set might negatively impact the overall utility. In response, this paper pioneers the examination of the aforementioned problem with non-monotone submodular functions and offers effective solutions for both flexible and fixed length constraints, as well as a special case w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23454;&#29616;&#27169;&#22411;&#20877;&#24179;&#34913;&#26469;&#20943;&#36731;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#22833;&#34913;&#27169;&#22411;&#36827;&#34892;&#28508;&#31354;&#38388;&#25506;&#32034;&#29983;&#25104;&#22343;&#34913;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#22343;&#34913;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#35265;&#20943;&#36731;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#31181;&#26063;&#20844;&#24179;&#30340;&#35757;&#32451;&#20013;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20844;&#24179;&#24230;&#25351;&#26631;&#19978;&#25913;&#36827;&#20102;&#36817;5&#20493;&#12290;</title><link>http://arxiv.org/abs/2308.08638</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#20877;&#24179;&#34913;&#23454;&#29616;&#20844;&#24179;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fair GANs through model rebalancing with synthetic data. (arXiv:2308.08638v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23454;&#29616;&#27169;&#22411;&#20877;&#24179;&#34913;&#26469;&#20943;&#36731;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#22833;&#34913;&#27169;&#22411;&#36827;&#34892;&#28508;&#31354;&#38388;&#25506;&#32034;&#29983;&#25104;&#22343;&#34913;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#22343;&#34913;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#35265;&#20943;&#36731;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#31181;&#26063;&#20844;&#24179;&#30340;&#35757;&#32451;&#20013;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20844;&#24179;&#24230;&#25351;&#26631;&#19978;&#25913;&#36827;&#20102;&#36817;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28982;&#32780;&#25910;&#38598;&#20195;&#34920;&#36866;&#24403;&#30340;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#26082;&#26114;&#36149;&#21448;&#22256;&#38590;&#65292;&#36825;&#23548;&#33268;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#20559;&#35265;&#65292;&#36827;&#32780;&#22312;&#27169;&#22411;&#20013;&#36827;&#19968;&#27493;&#20256;&#25773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#24179;&#34913;&#27169;&#22411;&#20998;&#24067;&#26469;&#20943;&#36731;&#29616;&#26377;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29616;&#26377;&#22833;&#34913;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#28508;&#31354;&#38388;&#25506;&#32034;&#29983;&#25104;&#22343;&#34913;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#22343;&#34913;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#35265;&#20943;&#36731;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;&#22833;&#34913;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#20063;&#33021;&#26174;&#31034;&#20986;&#20844;&#24179;&#24230;&#25351;&#26631;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;FFHQ&#25968;&#25454;&#38598;&#36827;&#34892;&#31181;&#26063;&#20844;&#24179;&#35757;&#32451;&#30340;Stylegan2&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#32467;&#26524;&#65292;&#24182;&#19988;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20844;&#24179;&#24230;&#25351;&#26631;&#19978;&#25552;&#21319;&#20102;&#36817;5&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models require large amounts of training data. This often poses a problem as the collection of datasets can be expensive and difficult, in particular datasets that are representative of the appropriate underlying distribution (e.g. demographic). This introduces biases in datasets which are further propagated in the models. We present an approach to mitigate biases in an existing generative adversarial network by rebalancing the model distribution. We do so by generating balanced data from an existing unbalanced deep generative model using latent space exploration and using this data to train a balanced generative model. Further, we propose a bias mitigation loss function that shows improvements in the fairness metric even when trained with unbalanced datasets. We show results for the Stylegan2 models while training on the FFHQ dataset for racial fairness and see that the proposed approach improves on the fairness metric by almost 5 times, whilst maintaining image qualit
&lt;/p&gt;</description></item><item><title>FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.08634</link><description>&lt;p&gt;
FedPop: &#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
FedPop: Federated Population-based Hyperparameter Tuning. (arXiv:2308.08634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08634
&lt;/p&gt;
&lt;p&gt;
FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#33539;&#24335;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#38598;&#20013;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;ML&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;ML&#27969;&#31243;&#31867;&#20284;&#65292;FL&#20013;&#30340;&#23458;&#25143;&#31471;&#26412;&#22320;&#20248;&#21270;&#21644;&#26381;&#21153;&#22120;&#32858;&#21512;&#36807;&#31243;&#23545;&#36229;&#21442;&#25968;&#65288;HP&#65289;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#23613;&#31649;&#22312;&#38598;&#20013;&#24335;ML&#20013;&#23545;&#35843;&#20248;HP&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;FL&#26102;&#20250;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#8220;&#35843;&#20248;&#21518;&#35757;&#32451;&#8221;&#26694;&#26550;&#23545;&#20110;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;FL&#19981;&#21512;&#36866;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#24050;&#32463;&#25552;&#20986;&#29992;&#20110;FL&#20013;&#30340;HP&#35843;&#20248;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#26356;&#26032;&#30340;HP&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#65288;FedPop&#65289;&#30340;&#26032;&#22411;HP&#35843;&#20248;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;FedPop&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;HP&#65292;&#27492;&#31639;&#27861;&#36866;&#29992;&#20110;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#21508;&#31181;HP&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning (ML) paradigm, in which multiple clients collaboratively train ML models without centralizing their local data. Similar to conventional ML pipelines, the client local optimization and server aggregation procedure in FL are sensitive to the hyperparameter (HP) selection. Despite extensive research on tuning HPs for centralized ML, these methods yield suboptimal results when employed in FL. This is mainly because their "training-after-tuning" framework is unsuitable for FL with limited client computation power. While some approaches have been proposed for HP-Tuning in FL, they are limited to the HPs for client local updates. In this work, we propose a novel HP-tuning algorithm, called Federated Population-based Hyperparameter Tuning (FedPop), to address this vital yet challenging problem. FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both client and server sides
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;LSTM&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22635;&#20805;GRACE&#21355;&#26143;&#21152;&#36895;&#35745;&#25968;&#25454;&#38388;&#26029;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#19977;&#20010;&#36724;&#19978;&#30340;&#21152;&#36895;&#35745;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08621</link><description>&lt;p&gt;
&#22522;&#20110;LSTM&#30340;GRACE&#21152;&#36895;&#35745;&#25968;&#25454;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LSTM-Based Forecasting Model for GRACE Accelerometer Data. (arXiv:2308.08621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;LSTM&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22635;&#20805;GRACE&#21355;&#26143;&#21152;&#36895;&#35745;&#25968;&#25454;&#38388;&#26029;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#19977;&#20010;&#36724;&#19978;&#30340;&#21152;&#36895;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#21147;&#24674;&#22797;&#19982;&#27668;&#20505;&#23454;&#39564;&#65288;GRACE&#65289;&#21355;&#26143;&#20219;&#21153;&#20174;2002&#24180;&#21040;2017&#24180;&#65292;&#20026;&#30417;&#27979;&#22320;&#29699;&#37325;&#21147;&#22330;&#21464;&#21270;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#22320;&#29699;&#29289;&#29702;&#23398;&#21644;&#27700;&#25991;&#23398;&#31561;&#22810;&#31181;&#24212;&#29992;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#38543;&#21518;&#65292;&#22312;2018&#24180;&#65292;GRACE Follow-On&#32487;&#32493;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#12290;&#20174;&#21355;&#26143;&#19978;&#19981;&#21516;&#20202;&#22120;&#30340;&#38598;&#25104;&#23548;&#20986;&#30340;&#26376;&#24230;&#22320;&#29699;&#37325;&#21147;&#22330;&#25968;&#25454;&#65292;&#30001;&#20110;&#22810;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;GRACE&#20219;&#21153;&#24320;&#22987;&#20197;&#26469;&#26576;&#20123;&#20202;&#22120;&#30340;&#35266;&#27979;&#38388;&#26029;&#65292;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;&#29616;&#22312;&#24050;&#32463;&#26377;&#20108;&#21313;&#22810;&#24180;&#30340;GRACE&#21644;GRACE Follow-On&#25968;&#25454;&#21487;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22635;&#20805;&#25968;&#25454;&#38388;&#26029;&#24182;&#39044;&#27979;GRACE&#21152;&#36895;&#35745;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#21152;&#36895;&#35745;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#26469;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#19977;&#20010;&#36724;&#19978;&#21152;&#36895;&#35745;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#39044;&#22788;&#29702;&#21152;&#36895;&#35745;&#25968;&#25454;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gravity Recovery and Climate Experiment (GRACE) satellite mission, spanning from 2002 to 2017, has provided a valuable dataset for monitoring variations in Earth's gravity field, enabling diverse applications in geophysics and hydrology. The mission was followed by GRACE Follow-On in 2018, continuing data collection efforts. The monthly Earth gravity field, derived from the integration different instruments onboard satellites, has shown inconsistencies due to various factors, including gaps in observations for certain instruments since the beginning of the GRACE mission.  With over two decades of GRACE and GRACE Follow-On data now available, this paper proposes an approach to fill the data gaps and forecast GRACE accelerometer data. Specifically, we focus on accelerometer data and employ Long Short-Term Memory (LSTM) networks to train a model capable of predicting accelerometer data for all three axes.  In this study, we describe the methodology used to preprocess the accelerometer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#8212;&#8212;&#24605;&#32500;&#22270;&#65288;GoT&#65289;&#65292;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#26029;&#21319;&#32423;&#30340;&#25361;&#25112;&#20013;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#20110;GPT-4&#65292;&#24182;&#19988;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26041;&#27861;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.08614</link><description>&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#22270;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought. (arXiv:2308.08614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#8212;&#8212;&#24605;&#32500;&#22270;&#65288;GoT&#65289;&#65292;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#26029;&#21319;&#32423;&#30340;&#25361;&#25112;&#20013;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#20110;GPT-4&#65292;&#24182;&#19988;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26041;&#27861;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#26631;&#20934;&#26597;&#35810;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38754;&#23545;&#38656;&#35201;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24613;&#21095;&#19979;&#38477;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#25552;&#31034;&#24037;&#31243;&#39046;&#22495;&#65292;&#20197;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#31216;&#20026;&#8220;&#24605;&#32500;&#22270;&#65288;GoT&#65289;&#8221;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#26029;&#21319;&#32423;&#30340;&#25361;&#25112;&#19978;&#36827;&#34892;&#27979;&#35797;&#65306;24&#28857;&#28216;&#25103;&#65292;&#39640;&#38454;&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#35299;&#26512;&#65292;&#20197;&#21450;&#36882;&#24402;&#25968;&#21015;&#30340;&#20844;&#24335;&#25512;&#23548;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;GPT-4&#65292;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;$89.7\%$&#12289;$86\%$&#21644;$56\%$&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26041;&#27861;&#8220;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#8221;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#20934;&#30830;&#24615;&#25552;&#21319;&#20102;$23\%$&#12289;$24\%$&#21644;$15\%$&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large-scale models, such as GPT-4, have showcased remarkable capabilities in addressing standard queries. However, when facing complex problems that require multi-step logical reasoning, their accuracy dramatically decreases. Current research has explored the realm of \textit{prompting engineering} to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique, dubbed \textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalating challenges: the 24-point game, resolution of high-degree polynomial equations, and derivation of formulas for recursive sequences, our method outperformed GPT-4, achieving accuracy improvements of $89.7\%$, $86\%$, and $56\%$ for each respective task. Moreover, when juxtaposed with the state-of-the-art (SOTA) prompting method, \textit{Tree of Thought (ToT)}, our approach registered an average accuracy boost of $23\%$, $24\%$, and $15\%$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#20248;&#21270;&#20809;&#20239;&#31995;&#32479;&#22312;&#20892;&#19994;&#20013;&#30340;&#23433;&#35013;&#20915;&#31574;&#21046;&#23450;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12289;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#24182;&#22686;&#21152;&#20892;&#19994;&#21033;&#28070;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.08611</link><description>&lt;p&gt;
&#20892;&#19994;&#20013;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#65306;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Renewable Energy in Agriculture: A Deep Reinforcement Learning-based Approach. (arXiv:2308.08611v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#20248;&#21270;&#20809;&#20239;&#31995;&#32479;&#22312;&#20892;&#19994;&#20013;&#30340;&#23433;&#35013;&#20915;&#31574;&#21046;&#23450;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12289;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#24182;&#22686;&#21152;&#20892;&#19994;&#21033;&#28070;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#26469;&#20248;&#21270;&#20892;&#19994;&#39046;&#22495;&#20809;&#20239;&#31995;&#32479;&#23433;&#35013;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;DQN&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#20892;&#19994;&#25237;&#36164;&#32773;&#26681;&#25454;&#23433;&#35013;&#39044;&#31639;&#12289;&#25919;&#24220;&#28608;&#21169;&#25514;&#26045;&#12289;&#33021;&#28304;&#38656;&#27714;&#12289;&#31995;&#32479;&#25104;&#26412;&#21644;&#38271;&#26399;&#25928;&#30410;&#31561;&#22240;&#32032;&#20316;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#22870;&#21169;&#26426;&#21046;&#65292;DQN&#23398;&#20064;&#22914;&#20309;&#22312;&#20809;&#20239;&#31995;&#32479;&#38598;&#25104;&#26041;&#38754;&#20570;&#20986;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#12290;&#35813;&#20998;&#26512;&#25552;&#20379;&#20102;&#28145;&#20837;&#20102;&#35299;DQN&#22914;&#20309;&#25903;&#25345;&#25237;&#36164;&#32773;&#22312;&#20892;&#19994;&#20013;&#36827;&#34892;&#20809;&#20239;&#31995;&#32479;&#23433;&#35013;&#20915;&#31574;&#30340;&#32508;&#21512;&#29702;&#35299;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#20419;&#36827;&#21487;&#25345;&#32493;&#21644;&#39640;&#25928;&#30340;&#20892;&#19994;&#23454;&#36341;&#65292;&#21516;&#26102;&#20026;&#26410;&#26469;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#21033;&#29992;DQN&#65292;&#20892;&#19994;&#25237;&#36164;&#32773;&#21487;&#20197;&#20570;&#20986;&#20248;&#21270;&#20915;&#31574;&#65292;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#65292;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#65292;&#24182;&#22686;&#21152;&#21033;&#28070;&#12290;&#35813;&#30740;&#31350;&#23545;&#25512;&#21160;&#20892;&#19994;&#39046;&#22495;&#30340;&#36827;&#27493;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article investigates the use of Deep Q-Networks (DQNs) to optimize decision-making for photovoltaic (PV) systems installations in the agriculture sector. The study develops a DQN framework to assist agricultural investors in making informed decisions considering factors such as installation budget, government incentives, energy requirements, system cost, and long-term benefits. By implementing a reward mechanism, the DQN learns to make data-driven decisions on PV integration. The analysis provides a comprehensive understanding of how DQNs can support investors in making decisions about PV installations in agriculture. This research has significant implications for promoting sustainable and efficient farming practices while also paving the way for future advancements in this field. By leveraging DQNs, agricultural investors can make optimized decisions that improve energy efficiency, reduce environmental impact, and enhance profitability. This study contributes to the advancement o
&lt;/p&gt;</description></item><item><title>PEvoLM&#26159;&#19968;&#31181;&#34507;&#30333;&#36136;&#24207;&#21015;&#36827;&#21270;&#20449;&#24687;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#36716;&#25442;&#20026;&#25968;&#23383;&#21521;&#37327;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.08578</link><description>&lt;p&gt;
PEvoLM: &#34507;&#30333;&#36136;&#24207;&#21015;&#36827;&#21270;&#20449;&#24687;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PEvoLM: Protein Sequence Evolutionary Information Language Model. (arXiv:2308.08578v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08578
&lt;/p&gt;
&lt;p&gt;
PEvoLM&#26159;&#19968;&#31181;&#34507;&#30333;&#36136;&#24207;&#21015;&#36827;&#21270;&#20449;&#24687;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#36716;&#25442;&#20026;&#25968;&#23383;&#21521;&#37327;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#34507;&#30333;&#36136;&#24207;&#21015;&#25968;&#25454;&#24211;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#22810;&#24207;&#21015;&#27604;&#23545;(PSI-BLAST&#31561;)&#36890;&#36807;&#32791;&#36153;&#26102;&#38388;&#30340;&#25968;&#25454;&#24211;&#25628;&#32034;&#26469;&#33719;&#21462;&#36827;&#21270;&#20449;&#24687;&#12290;&#36825;&#20123;&#25628;&#32034;&#24341;&#25806;&#29983;&#25104;&#30340;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;(PSSMs)&#23545;&#20110;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#35745;&#31639;&#29983;&#29289;&#23398;&#39046;&#22495;&#30340;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#36755;&#20837;&#12290;&#34507;&#30333;&#24207;&#21015;&#26159;&#30001;&#31216;&#20026;&#27688;&#22522;&#37240;(AAs)&#30340;&#36830;&#32493;&#26631;&#35760;&#25110;&#23383;&#31526;&#32452;&#25104;&#30340;&#12290;&#31867;&#27604;&#33258;&#28982;&#35821;&#35328;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;NLP&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#24212;&#29992;&#20110;&#29983;&#29289;&#20449;&#24687;&#23398;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;(ELMo)&#65292;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#36716;&#25442;&#20026;&#25968;&#23383;&#21521;&#37327;&#34920;&#31034;&#12290;&#23613;&#31649;&#21407;&#22987;ELMo&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#23618;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(LSTMs)&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#27491;&#21521;&#22788;&#29702;&#30340;&#21452;&#36335;&#24452;&#26550;&#26500;&#65292;
&lt;/p&gt;
&lt;p&gt;
With the exponential increase of the protein sequence databases over time, multiple-sequence alignment (MSA) methods, like PSI-BLAST, perform exhaustive and time-consuming database search to retrieve evolutionary information. The resulting position-specific scoring matrices (PSSMs) of such search engines represent a crucial input to many machine learning (ML) models in the field of bioinformatics and computational biology. A protein sequence is a collection of contiguous tokens or characters called amino acids (AAs). The analogy to natural language allowed us to exploit the recent advancements in the field of Natural Language Processing (NLP) and therefore transfer NLP state-of-the-art algorithms to bioinformatics. This research presents an Embedding Language Model (ELMo), converting a protein sequence to a numerical vector representation. While the original ELMo trained a 2-layer bidirectional Long Short-Term Memory (LSTMs) network following a two-path architecture, one for the forwar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#27604;&#20998;&#26512;&#20102;12&#31181;&#33258;&#28982;&#21551;&#21457;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#32467;&#21512;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#29305;&#24449;&#38598;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2308.08574</link><description>&lt;p&gt;
&#33258;&#28982;&#21551;&#21457;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance. (arXiv:2308.08574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#27604;&#20998;&#26512;&#20102;12&#31181;&#33258;&#28982;&#21551;&#21457;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#32467;&#21512;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#29305;&#24449;&#38598;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#23545;&#20110;&#26377;&#25928;&#38450;&#27490;&#39118;&#38505;&#23398;&#29983;&#22833;&#36133;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#22871;12&#20010;&#33258;&#28982;&#21551;&#21457;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#23454;&#20363;&#30340;&#28857;&#20987;&#27969;&#25968;&#25454;&#12289;&#35838;&#20869;&#21333;&#19968;&#35838;&#31243;&#34920;&#29616;&#20197;&#21450;&#21516;&#26102;&#21442;&#21152;&#22810;&#20010;&#35838;&#31243;&#26102;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#21033;&#29992;&#33258;&#28982;&#21551;&#21457;&#30340;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#32467;&#21512;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#29305;&#24449;&#38598;&#22823;&#23567;&#30340;2/3&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting student performance is key in leveraging effective pre-failure interventions for at-risk students. In this paper, I have analyzed the relative performance of a suite of 12 nature-inspired algorithms when used to predict student performance across 3 datasets consisting of instance-based clickstream data, intra-course single-course performance, and performance when taking multiple courses simultaneously. I found that, for all datasets, leveraging an ensemble approach using NIAs for feature selection and traditional ML algorithms for classification increased predictive accuracy while also reducing feature set size by 2/3.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26681;&#25454;&#27979;&#37327;&#25968;&#25454;&#37325;&#24314;&#21160;&#24577;&#33655;&#36733;&#12290;&#35813;&#27169;&#22411;&#21487;&#22788;&#29702;&#19981;&#23436;&#25972;&#21644;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#22788;&#29702;&#27979;&#37327;&#31995;&#32479;&#30340;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2308.08571</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#21160;&#24577;&#33655;&#36733;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
A physics-informed machine learning model for reconstruction of dynamic loads. (arXiv:2308.08571v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26681;&#25454;&#27979;&#37327;&#25968;&#25454;&#37325;&#24314;&#21160;&#24577;&#33655;&#36733;&#12290;&#35813;&#27169;&#22411;&#21487;&#22788;&#29702;&#19981;&#23436;&#25972;&#21644;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#22788;&#29702;&#27979;&#37327;&#31995;&#32479;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#36328;&#24230;&#26725;&#26753;&#22312;&#20854;&#23551;&#21629;&#21608;&#26399;&#20869;&#21463;&#21040;&#22810;&#31181;&#21160;&#24577;&#28608;&#21169;&#12290;&#20026;&#20102;&#32771;&#34385;&#36825;&#20123;&#28608;&#21169;&#23545;&#32467;&#26500;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#22312;&#35774;&#35745;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#36733;&#33655;&#27169;&#22411;&#26469;&#27169;&#25311;&#32467;&#26500;&#21487;&#33021;&#32463;&#21382;&#30340;&#26465;&#20214;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#19981;&#21516;&#30340;&#31616;&#21270;&#20551;&#35774;&#65292;&#24182;&#36890;&#24120;&#30001;&#20174;&#27979;&#37327;&#25968;&#25454;&#20013;&#38543;&#26426;&#30830;&#23450;&#30340;&#21442;&#25968;&#24341;&#23548;&#65292;&#20351;&#20854;&#36755;&#20986;&#26412;&#36136;&#19978;&#19981;&#30830;&#23450;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#27010;&#29575;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#27979;&#37327;&#30340;&#25376;&#24230;&#12289;&#36895;&#24230;&#25110;&#21152;&#36895;&#24230;&#37325;&#24314;&#21160;&#24577;&#21147;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#19981;&#23436;&#25972;&#21644;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32771;&#34385;&#27979;&#37327;&#31995;&#32479;&#20013;&#30340;&#22122;&#22768;&#12290;&#36890;&#36807;&#23545;&#22823;&#24102;&#19996;&#26725;&#36827;&#34892;&#31354;&#27668;&#21160;&#21147;&#23398;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-span bridges are subjected to a multitude of dynamic excitations during their lifespan. To account for their effects on the structural system, several load models are used during design to simulate the conditions the structure is likely to experience. These models are based on different simplifying assumptions and are generally guided by parameters that are stochastically identified from measurement data, making their outputs inherently uncertain. This paper presents a probabilistic physics-informed machine-learning framework based on Gaussian process regression for reconstructing dynamic forces based on measured deflections, velocities, or accelerations. The model can work with incomplete and contaminated data and offers a natural regularization approach to account for noise in the measurement system. An application of the developed framework is given by an aerodynamic analysis of the Great Belt East Bridge. The aerodynamic response is calculated numerically based on the quasi-st
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#21307;&#23398;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65288;CMISR&#65289;&#65292;&#37319;&#29992;&#20840;&#23616;&#21453;&#39304;&#30340;&#38381;&#29615;&#26694;&#26550;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#27424;&#20998;&#36776;&#29575;&#21644;&#36229;&#20998;&#36776;&#29575;&#20803;&#32032;&#12290;CMISR&#22312;&#31283;&#24577;&#19979;&#20855;&#26377;&#38646;&#24674;&#22797;&#35823;&#24046;&#65292;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;MISR&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08567</link><description>&lt;p&gt;
CMISR&#65306;&#24490;&#29615;&#21307;&#23398;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
CMISR: Circular Medical Image Super-Resolution. (arXiv:2308.08567v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08567
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#21307;&#23398;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65288;CMISR&#65289;&#65292;&#37319;&#29992;&#20840;&#23616;&#21453;&#39304;&#30340;&#38381;&#29615;&#26694;&#26550;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#27424;&#20998;&#36776;&#29575;&#21644;&#36229;&#20998;&#36776;&#29575;&#20803;&#32032;&#12290;CMISR&#22312;&#31283;&#24577;&#19979;&#20855;&#26377;&#38646;&#24674;&#22797;&#35823;&#24046;&#65292;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;MISR&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#21307;&#23398;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;MISR&#65289;&#26041;&#27861;&#37319;&#29992;&#38544;&#24335;&#30340;&#27424;&#20998;&#36776;&#29575;&#65288;UR&#65289;&#21333;&#20803;&#21644;&#26126;&#30830;&#30340;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#21333;&#20803;&#30340;&#24320;&#29615;&#26550;&#26500;&#12290;UR&#21333;&#20803;&#21487;&#20197;&#22987;&#32456;&#32473;&#20986;&#12289;&#20551;&#35774;&#25110;&#20272;&#35745;&#65292;&#32780;SR&#21333;&#20803;&#26681;&#25454;&#21508;&#31181;SR&#31639;&#27861;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#12290;&#38381;&#29615;&#21453;&#39304;&#26426;&#21046;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#21069;&#30340;MISR&#26041;&#27861;&#65292;&#24182;&#33021;&#26377;&#25928;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#21453;&#39304;&#26426;&#21046;&#21487;&#20197;&#20998;&#20026;&#20004;&#31867;&#65306;&#23616;&#37096;&#21453;&#39304;&#21644;&#20840;&#23616;&#21453;&#39304;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21453;&#39304;&#30340;&#38381;&#29615;&#26694;&#26550;&#65292;&#21363;&#24490;&#29615;MISR&#65288;CMISR&#65289;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;UR&#21644;SR&#20803;&#32032;&#12290;&#24314;&#31435;CMISR&#30340;&#25968;&#23398;&#27169;&#22411;&#21644;&#38381;&#29615;&#26041;&#31243;&#12290;&#36890;&#36807;&#27888;&#21202;&#32423;&#25968;&#36924;&#36817;&#30340;&#25968;&#23398;&#35777;&#26126;&#34920;&#26126;&#65292;CMISR&#22312;&#31283;&#24577;&#19979;&#20855;&#26377;&#38646;&#24674;&#22797;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;CMISR&#20855;&#26377;&#21363;&#25554;&#21363;&#29992;&#30340;&#29305;&#24615;&#65292;&#21487;&#20197;&#24314;&#31435;&#22312;&#20219;&#20309;&#29616;&#26377;&#30340;MISR&#31639;&#27861;&#19978;&#12290;&#20998;&#21035;&#25552;&#20986;&#20102;&#20116;&#31181;CMISR&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical methods of medical image super-resolution (MISR) utilize open-loop architecture with implicit under-resolution (UR) unit and explicit super-resolution (SR) unit. The UR unit can always be given, assumed, or estimated, while the SR unit is elaborately designed according to various SR algorithms. The closed-loop feedback mechanism is widely employed in current MISR approaches and can efficiently improve their performance. The feedback mechanism may be divided into two categories: local and global feedback. Therefore, this paper proposes a global feedback-based closed-cycle framework, circular MISR (CMISR), with unambiguous UR and SR elements. Mathematical model and closed-loop equation of CMISR are built. Mathematical proof with Taylor-series approximation indicates that CMISR has zero recovery error in steady-state. In addition, CMISR holds plug-and-play characteristic which can be established on any existing MISR algorithms. Five CMISR algorithms are respectively proposed bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#26041;&#20301;&#26694;&#26550;&#65288;KMF&#65289;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25552;&#21462;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#20027;&#39064;&#26469;&#22686;&#24378;&#26631;&#31614;&#35821;&#20041;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08563</link><description>&lt;p&gt;
KMF: &#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#26041;&#20301;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#38646;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification. (arXiv:2308.08563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#26041;&#20301;&#26694;&#26550;&#65288;KMF&#65289;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25552;&#21462;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#20027;&#39064;&#26469;&#22686;&#24378;&#26631;&#31614;&#35821;&#20041;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38646;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#65288;ZNC&#65289;&#22312;&#22270;&#25968;&#25454;&#20998;&#26512;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#39044;&#27979;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35266;&#23519;&#21040;&#30340;&#26410;&#30693;&#31867;&#21035;&#30340;&#33410;&#28857;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#23558;&#29305;&#24449;&#30340;&#21407;&#22411;&#21644;&#26631;&#31614;&#30340;&#35821;&#20041;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#20174;&#24050;&#35266;&#23519;&#21040;&#30340;&#31867;&#21035;&#21040;&#26410;&#35266;&#23519;&#21040;&#30340;&#31867;&#21035;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#29305;&#24449;-&#35821;&#20041;&#23545;&#40784;&#20013;&#22810;&#26041;&#20301;&#35821;&#20041;&#26041;&#21521;&#30340;&#23384;&#22312;&#65292;&#21363;&#33410;&#28857;&#30340;&#20869;&#23481;&#36890;&#24120;&#28085;&#30422;&#19982;&#22810;&#20010;&#26631;&#31614;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#19981;&#21516;&#20027;&#39064;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21306;&#20998;&#21644;&#21028;&#26029;&#24433;&#21709;&#35748;&#30693;&#33021;&#21147;&#30340;&#35821;&#20041;&#22240;&#32032;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#26041;&#20301;&#26694;&#26550;&#65288;KMF&#65289;&#65292;&#36890;&#36807;&#25552;&#21462;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30340;&#20027;&#39064;&#26469;&#22686;&#24378;&#26631;&#31614;&#35821;&#20041;&#30340;&#20016;&#23500;&#24615;&#12290;&#28982;&#21518;&#65292;&#23558;&#27599;&#20010;&#33410;&#28857;&#30340;&#20869;&#23481;&#37325;&#26500;&#20026;&#20027;&#39064;&#32423;&#21035;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Zero-Shot Node Classification (ZNC) has been an emerging and crucial task in graph data analysis. This task aims to predict nodes from unseen classes which are unobserved in the training process. Existing work mainly utilizes Graph Neural Networks (GNNs) to associate features' prototypes and labels' semantics thus enabling knowledge transfer from seen to unseen classes. However, the multi-faceted semantic orientation in the feature-semantic alignment has been neglected by previous work, i.e. the content of a node usually covers diverse topics that are relevant to the semantics of multiple labels. It's necessary to separate and judge the semantic factors that tremendously affect the cognitive ability to improve the generality of models. To this end, we propose a Knowledge-Aware Multi-Faceted framework (KMF) that enhances the richness of label semantics via the extracted KG (Knowledge Graph)-based topics. And then the content of each node is reconstructed to a topic-level repre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QMLS&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32553;&#30701;&#33647;&#29289;&#30740;&#21457;&#30340;&#26102;&#38388;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#36890;&#36807;&#29983;&#25104;&#21629;&#20013;&#29289;&#21644;&#20248;&#21270;&#20998;&#23376;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08561</link><description>&lt;p&gt;
&#26410;&#26469;&#33647;&#29289;&#21457;&#29616;&#30340;&#23454;&#26045;&#65306;&#22522;&#20110;&#37327;&#23376;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#25311;(QMLS)&#12290;
&lt;/p&gt;
&lt;p&gt;
Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS). (arXiv:2308.08561v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QMLS&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32553;&#30701;&#33647;&#29289;&#30740;&#21457;&#30340;&#26102;&#38388;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#36890;&#36807;&#29983;&#25104;&#21629;&#20013;&#29289;&#21644;&#20248;&#21270;&#20998;&#23376;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30740;&#21457;&#30340;&#30740;&#31350;&#19982;&#24320;&#21457;(R&amp;D)&#38454;&#27573;&#26159;&#19968;&#20010;&#28459;&#38271;&#32780;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#25913;&#38761;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#27010;&#24565;QMLS&#65292;&#23558;&#25972;&#20010;R&amp;D&#38454;&#27573;&#32553;&#30701;&#21040;&#19977;&#21040;&#20845;&#20010;&#26376;&#65292;&#25104;&#26412;&#20165;&#20026;&#20116;&#21040;&#20843;&#19975;&#32654;&#20803;&#12290;&#23545;&#20110;&#21629;&#20013;&#20135;&#29983;&#65292;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#29983;&#25104;(MLMG)&#26681;&#25454;&#30446;&#26631;&#34507;&#30333;&#30340;&#20998;&#23376;&#32467;&#26500;&#29983;&#25104;&#21487;&#33021;&#30340;&#21629;&#20013;&#29289;&#65292;&#32780;&#37327;&#23376;&#27169;&#25311;(QS)&#26681;&#25454;&#19982;&#30446;&#26631;&#34507;&#30333;&#30340;&#21453;&#24212;&#21644;&#32467;&#21512;&#25928;&#26524;&#36807;&#28388;&#21407;&#22987;&#23454;&#39564;&#20013;&#30340;&#20998;&#23376;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#38085;&#20248;&#21270;&#65292;&#20174;MLMG&#21644;QS&#29983;&#25104;&#21644;&#36807;&#28388;&#30340;&#32467;&#26524;&#20998;&#23376;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21464;&#24322;(MLMV)&#23558;&#37027;&#20123;&#20986;&#29616;&#22312;&#20004;&#20010;&#36807;&#31243;&#20013;&#30340;&#20998;&#23376;&#21046;&#25104;&#25968;&#21313;&#31181;&#20998;&#23376;&#21464;&#20307;&#65292;&#32780;&#20854;&#20182;&#20998;&#23376;&#21482;&#21046;&#25104;&#20960;&#31181;&#21464;&#20307;&#12290;&#26368;&#21518;&#65292;&#25152;&#26377;&#20248;&#21270;&#30340;&#20998;&#23376;&#23558;&#32463;&#36807;&#22810;&#36718;&#39640;&#26631;&#20934;&#30340;QS&#36807;&#28388;&#65292;&#20197;&#30830;&#20445;&#21453;&#24212;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Research &amp; Development (R&amp;D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&amp;D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction ef
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#20998;&#26512;&#21382;&#21490;&#25968;&#25454;&#21644;&#21306;&#22359;&#38142;&#21442;&#25968;&#65292;&#25214;&#20986;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#24182;&#35782;&#21035;&#39118;&#38505;&#21152;&#23494;&#36135;&#24065;&#12290;</title><link>http://arxiv.org/abs/2308.08554</link><description>&lt;p&gt;
AI&#36741;&#21161;&#35843;&#26597;&#21306;&#22359;&#38142;&#21442;&#25968;&#65306;&#39118;&#38505;&#21152;&#23494;&#36135;&#24065;&#21644;&#20215;&#26684;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors. (arXiv:2308.08554v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#20998;&#26512;&#21382;&#21490;&#25968;&#25454;&#21644;&#21306;&#22359;&#38142;&#21442;&#25968;&#65292;&#25214;&#20986;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#24182;&#35782;&#21035;&#39118;&#38505;&#21152;&#23494;&#36135;&#24065;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21152;&#23494;&#36135;&#24065;&#24050;&#25104;&#20026;&#25237;&#36164;&#32773;&#21644;&#23398;&#32773;&#24191;&#27867;&#30740;&#31350;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#20026;&#20102;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#65292;&#20102;&#35299;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#24182;&#35782;&#21035;&#39118;&#38505;&#21152;&#23494;&#36135;&#24065;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#37325;&#28857;&#20998;&#26512;&#21382;&#21490;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#23545;&#21306;&#22359;&#38142;&#21442;&#25968;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#24182;&#25214;&#21040;&#39118;&#38505;&#21152;&#23494;&#36135;&#24065;&#12290;&#25105;&#20204;&#23545;&#21382;&#21490;&#21152;&#23494;&#36135;&#24065;&#30340;&#38142;&#19978;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#27979;&#37327;&#20102;&#20215;&#26684;&#19982;&#20854;&#20182;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#32858;&#31867;&#21644;&#20998;&#31867;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#21152;&#23494;&#36135;&#24065;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#39118;&#38505;&#25110;&#38750;&#39118;&#38505;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#37096;&#20998;&#21152;&#23494;&#36135;&#24065;&#65288;39&#65285;&#65289;&#24050;&#36864;&#20986;&#24066;&#22330;&#65292;&#32780;&#21482;&#26377;&#24456;&#23567;&#19968;&#37096;&#20998;&#65288;10&#65285;&#65289;&#23384;&#27963;&#20102;1000&#22810;&#22825;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrencies have become a popular and widely researched topic of interest in recent years for investors and scholars. In order to make informed investment decisions, it is essential to comprehend the factors that impact cryptocurrency prices and to identify risky cryptocurrencies. This paper focuses on analyzing historical data and using artificial intelligence algorithms on on-chain parameters to identify the factors affecting a cryptocurrency's price and to find risky cryptocurrencies. We conducted an analysis of historical cryptocurrencies' on-chain data and measured the correlation between the price and other parameters. In addition, we used clustering and classification in order to get a better understanding of a cryptocurrency and classify it as risky or not. The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. Our analysis revealed a significant negative
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#26356;&#28789;&#27963;&#35760;&#24518;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#20855;&#26377;&#38271;&#26399;&#35760;&#24518;&#30340;&#36164;&#20135;&#20215;&#26684;&#27874;&#21160;&#24615;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#31895;&#31961;&#27874;&#21160;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08550</link><description>&lt;p&gt;
&#20855;&#26377;&#26356;&#28789;&#27963;&#35760;&#24518;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65306;&#27604;&#31895;&#31961;&#27874;&#21160;&#24615;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Networks with more flexible memory: better predictions than rough volatility. (arXiv:2308.08550v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#26356;&#28789;&#27963;&#35760;&#24518;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#20855;&#26377;&#38271;&#26399;&#35760;&#24518;&#30340;&#36164;&#20135;&#20215;&#26684;&#27874;&#21160;&#24615;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#31895;&#31961;&#27874;&#21160;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25193;&#23637;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#20026;&#20854;&#36755;&#20986;&#30340;&#27599;&#20010;&#32500;&#24230;&#21253;&#25324;&#22810;&#20010;&#28789;&#27963;&#30340;&#26102;&#38388;&#23610;&#24230;&#65292;&#20174;&#32780;&#26426;&#26800;&#22320;&#25913;&#21892;&#20102;&#20854;&#23545;&#20855;&#26377;&#38271;&#26399;&#35760;&#24518;&#25110;&#39640;&#24230;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#30340;&#36807;&#31243;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#26222;&#36890;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTMs&#65289;&#21644;&#25193;&#23637;&#30340;LSTMs&#22312;&#39044;&#27979;&#20855;&#26377;&#38271;&#26399;&#35760;&#24518;&#30340;&#36164;&#20135;&#20215;&#26684;&#27874;&#21160;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#25193;&#23637;&#30340;LSTMs&#25152;&#38656;&#30340;&#26102;&#26399;&#25968;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#32780;&#20855;&#26377;&#30456;&#21516;&#36229;&#21442;&#25968;&#30340;&#27169;&#22411;&#22312;&#39564;&#35777;&#21644;&#27979;&#35797;&#25439;&#22833;&#30340;&#21464;&#21270;&#36739;&#23567;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#65292;&#22312;&#20351;&#29992;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#65292;&#39564;&#35777;&#25439;&#22833;&#26368;&#23567;&#30340;&#27169;&#22411;&#27604;&#31895;&#31961;&#27874;&#21160;&#24615;&#39044;&#27979;&#30340;&#20934;&#30830;&#29575;&#39640;&#32422;20%&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend recurrent neural networks to include several flexible timescales for each dimension of their output, which mechanically improves their abilities to account for processes with long memory or with highly disparate time scales. We compare the ability of vanilla and extended long short term memory networks (LSTMs) to predict asset price volatility, known to have a long memory. Generally, the number of epochs needed to train extended LSTMs is divided by two, while the variation of validation and test losses among models with the same hyperparameters is much smaller. We also show that the model with the smallest validation loss systemically outperforms rough volatility predictions by about 20% when trained and tested on a dataset with multiple time series.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#27599;&#26085;&#26032;&#38395;&#24773;&#32490;&#23545;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31995;&#32479;&#20174;&#26032;&#38395;&#20013;&#25552;&#21462;&#24773;&#32490;&#24433;&#21709;&#22240;&#32032;&#12290;&#30740;&#31350;&#21457;&#29616;&#25237;&#36164;&#32773;&#24773;&#32490;&#23545;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.08549</link><description>&lt;p&gt;
&#27599;&#26085;&#26032;&#38395;&#24773;&#32490;&#23545;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of Daily News Sentiment on Stock Price Forecasting. (arXiv:2308.08549v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#27599;&#26085;&#26032;&#38395;&#24773;&#32490;&#23545;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31995;&#32479;&#20174;&#26032;&#38395;&#20013;&#25552;&#21462;&#24773;&#32490;&#24433;&#21709;&#22240;&#32032;&#12290;&#30740;&#31350;&#21457;&#29616;&#25237;&#36164;&#32773;&#24773;&#32490;&#23545;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32929;&#31080;&#30340;&#26410;&#26469;&#20215;&#26684;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24341;&#20837;&#39069;&#22806;&#30340;&#22240;&#32032;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#25105;&#20204;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#38752;&#32929;&#31080;&#30340;&#21382;&#21490;&#20215;&#26684;&#25968;&#25454;&#26469;&#39044;&#27979;&#20854;&#26410;&#26469;&#20215;&#26684;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25237;&#36164;&#32773;&#24773;&#32490;&#21463;&#21040;&#26377;&#20851;&#20844;&#21496;&#30340;&#27599;&#26085;&#26032;&#38395;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#23545;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#20449;&#24687;&#26469;&#28304;&#21487;&#20197;&#33719;&#21462;&#36825;&#20123;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20805;&#26021;&#30528;&#22823;&#37327;&#30340;&#22122;&#22768;&#65292;&#20351;&#24471;&#20934;&#30830;&#25552;&#21462;&#24773;&#24863;&#21464;&#24471;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#35774;&#35745;&#19968;&#20010;&#39640;&#25928;&#30340;&#31995;&#32479;&#65292;&#20174;&#20851;&#20110;NITY50&#32929;&#31080;&#30340;&#26032;&#38395;&#20013;&#33719;&#21462;&#24773;&#32490;&#65292;&#24182;&#30740;&#31350;&#36825;&#20123;&#32929;&#31080;&#30340;&#36130;&#32463;&#26032;&#38395;&#24773;&#32490;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#23545;&#20854;&#20215;&#26684;&#30340;&#24433;&#21709;&#31243;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#24314;&#19968;&#20010;&#21382;&#26102;&#32422;3.7&#24180;&#30340;&#26032;&#38395;&#25968;&#25454;&#24211;&#65292;&#21253;&#21547;&#20102;&#36817;50&#19975;&#26465;&#26032;&#38395;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting future prices of a stock is an arduous task to perform. However, incorporating additional elements can significantly improve our predictions, rather than relying solely on a stock's historical price data to forecast its future price. Studies have demonstrated that investor sentiment, which is impacted by daily news about the company, can have a significant impact on stock price swings. There are numerous sources from which we can get this information, but they are cluttered with a lot of noise, making it difficult to accurately extract the sentiments from them. Hence the focus of our research is to design an efficient system to capture the sentiments from the news about the NITY50 stocks and investigate how much the financial news sentiment of these stocks are affecting their prices over a period of time. This paper presents a robust data collection and preprocessing framework to create a news database for a timeline of around 3.7 years, consisting of almost half a million n
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#20010;&#21322;&#39034;&#24207;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#65288;TOSM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;CT&#21644;MRI&#20013;&#30340;&#19977;&#32500;&#20307;&#37325;&#24314;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#26469;&#20943;&#23569;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#65292;&#28982;&#21518;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26356;&#26032;&#25968;&#25454;&#20998;&#24067;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#19977;&#32500;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2308.08511</link><description>&lt;p&gt;
&#20004;&#20010;&#21322;&#39034;&#24207;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#19977;&#32500;&#19981;&#36866;&#23450;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems. (arXiv:2308.08511v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08511
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#20010;&#21322;&#39034;&#24207;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#65288;TOSM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;CT&#21644;MRI&#20013;&#30340;&#19977;&#32500;&#20307;&#37325;&#24314;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#26469;&#20943;&#23569;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#65292;&#28982;&#21518;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26356;&#26032;&#25968;&#25454;&#20998;&#24067;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#19977;&#32500;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#21644;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#26159;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#30340;&#25216;&#26415;&#12290;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;CT&#21644;MRI&#20013;&#36935;&#21040;&#30340;&#19981;&#21516;&#21453;&#38382;&#39064;&#65288;&#22914;&#31232;&#30095;&#35270;&#37326;CT&#21644;&#24555;&#36895;MRI&#37325;&#24314;&#65289;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#29616;&#31934;&#30830;&#30340;&#19977;&#32500;&#20307;&#37325;&#24314;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#37325;&#24314;&#20108;&#32500;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;&#37325;&#24314;&#30340;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20013;&#23548;&#33268;&#30456;&#37051;&#20999;&#29255;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#20010;&#21322;&#39034;&#24207;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#65288;TOSM&#65289;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;TOSM&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#30456;&#36739;&#20110;&#30452;&#25509;&#22312;&#19977;&#32500;&#20307;&#31215;&#19978;&#24037;&#20316;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#37325;&#24314;&#38454;&#27573;&#65292;TOSM&#20250;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26356;&#26032;&#25968;&#25454;&#20998;&#24067;&#65292;&#21033;&#29992;&#19977;&#20010;&#26041;&#21521;&#30340;&#20114;&#34917;&#24471;&#20998;&#65288;sag&#65289;
&lt;/p&gt;
&lt;p&gt;
Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucial technologies in the field of medical imaging. Score-based models have proven to be effective in addressing different inverse problems encountered in CT and MRI, such as sparse-view CT and fast MRI reconstruction. However, these models face challenges in achieving accurate three dimensional (3D) volumetric reconstruction. The existing score-based models primarily focus on reconstructing two dimensional (2D) data distribution, leading to inconsistencies between adjacent slices in the reconstructed 3D volumetric images. To overcome this limitation, we propose a novel two-and-a-half order score-based model (TOSM). During the training phase, our TOSM learns data distributions in 2D space, which reduces the complexity of training compared to directly working on 3D volumes. However, in the reconstruction phase, the TOSM updates the data distribution in 3D space, utilizing complementary scores along three directions (sag
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#23432;&#21452;&#37325;&#31283;&#20581;&#31574;&#30053;&#65288;CDR&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#26377;&#27602;&#25554;&#34917;&#38382;&#39064;&#12290;CDR&#36890;&#36807;&#23457;&#26597;&#25554;&#34917;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#26469;&#36807;&#28388;&#25554;&#34917;&#65292;&#32467;&#26524;&#26174;&#31034;CDR&#20855;&#26377;&#38477;&#20302;&#26041;&#24046;&#21644;&#25913;&#36827;&#23614;&#37096;&#30028;&#38480;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#24182;&#20943;&#23569;&#26377;&#27602;&#25554;&#34917;&#30340;&#39057;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08461</link><description>&lt;p&gt;
CDR&#65306;&#29992;&#20110;&#21435;&#20559;&#25512;&#33616;&#30340;&#20445;&#23432;&#21452;&#37325;&#31283;&#20581;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CDR: Conservative Doubly Robust Learning for Debiased Recommendation. (arXiv:2308.08461v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#23432;&#21452;&#37325;&#31283;&#20581;&#31574;&#30053;&#65288;CDR&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#26377;&#27602;&#25554;&#34917;&#38382;&#39064;&#12290;CDR&#36890;&#36807;&#23457;&#26597;&#25554;&#34917;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#26469;&#36807;&#28388;&#25554;&#34917;&#65292;&#32467;&#26524;&#26174;&#31034;CDR&#20855;&#26377;&#38477;&#20302;&#26041;&#24046;&#21644;&#25913;&#36827;&#23614;&#37096;&#30028;&#38480;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#24182;&#20943;&#23569;&#26377;&#27602;&#25554;&#34917;&#30340;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#24448;&#24448;&#26159;&#35266;&#23519;&#24615;&#30340;&#32780;&#19981;&#26159;&#23454;&#39564;&#24615;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#20013;&#26222;&#36941;&#23384;&#22312;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#20559;&#24046;&#38382;&#39064;&#24050;&#25104;&#20026;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#21452;&#37325;&#31283;&#20581;&#23398;&#20064;&#65288;DR&#65289;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#30340;&#29305;&#24615;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;DR&#26041;&#27861;&#22312;&#23384;&#22312;&#25152;&#35859;&#30340;&#26377;&#27602;&#25554;&#34917;&#65288;Poisonous Imputation&#65289;&#26102;&#21463;&#21040;&#20005;&#37325;&#24433;&#21709;&#65292;&#25554;&#34917;&#26126;&#26174;&#20559;&#31163;&#30495;&#23454;&#25968;&#25454;&#24182;&#36866;&#24471;&#20854;&#21453;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#23432;&#21452;&#37325;&#31283;&#20581;&#31574;&#30053;&#65288;CDR&#65289;&#65292;&#36890;&#36807;&#23457;&#26597;&#25554;&#34917;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#26469;&#36807;&#28388;&#25554;&#34917;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;CDR&#21487;&#20197;&#38477;&#20302;&#26041;&#24046;&#24182;&#25913;&#36827;&#23614;&#37096;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;CDR&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#30830;&#23454;&#20943;&#23569;&#20102;&#26377;&#27602;&#25554;&#34917;&#30340;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recommendation systems (RS), user behavior data is observational rather than experimental, resulting in widespread bias in the data. Consequently, tackling bias has emerged as a major challenge in the field of recommendation systems. Recently, Doubly Robust Learning (DR) has gained significant attention due to its remarkable performance and robust properties. However, our experimental findings indicate that existing DR methods are severely impacted by the presence of so-called Poisonous Imputation, where the imputation significantly deviates from the truth and becomes counterproductive.  To address this issue, this work proposes Conservative Doubly Robust strategy (CDR) which filters imputations by scrutinizing their mean and variance. Theoretical analyses show that CDR offers reduced variance and improved tail bounds.In addition, our experimental investigations illustrate that CDR significantly enhances performance and can indeed reduce the frequency of poisonous imputation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#35780;&#20272;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#25298;&#32477;&#26354;&#32447;&#26041;&#27861;&#12290;&#20351;&#29992;&#24863;&#30693;&#37327;&#21270;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#26469;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#21644;&#21307;&#23398;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08381</link><description>&lt;p&gt;
&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#25298;&#32477;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Precision and Recall Reject Curves for Classification. (arXiv:2308.08381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#35780;&#20272;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#25298;&#32477;&#26354;&#32447;&#26041;&#27861;&#12290;&#20351;&#29992;&#24863;&#30693;&#37327;&#21270;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#26469;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#21644;&#21307;&#23398;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#20998;&#31867;&#22330;&#26223;&#20013;&#65292;&#21482;&#20351;&#29992;&#27169;&#22411;&#39640;&#24230;&#30830;&#23450;&#30340;&#20998;&#31867;&#23454;&#20363;&#26159;&#21487;&#21462;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#36825;&#20123;&#39640;&#24230;&#30830;&#23450;&#30340;&#23454;&#20363;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20934;&#30830;&#24230;&#25298;&#32477;&#26354;&#32447;&#12290;&#25298;&#32477;&#26354;&#32447;&#20801;&#35768;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#30830;&#23450;&#24230;&#24230;&#37327;&#22312;&#25509;&#21463;&#25110;&#25298;&#32477;&#20998;&#31867;&#30340;&#19968;&#31995;&#21015;&#38408;&#20540;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#24230;&#21487;&#33021;&#24182;&#19981;&#36866;&#21512;&#25152;&#26377;&#24212;&#29992;&#31243;&#24207;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#30456;&#21453;&#65292;&#31934;&#30830;&#24230;&#25110;&#21484;&#22238;&#29575;&#21487;&#33021;&#26356;&#21487;&#21462;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#23384;&#22312;&#31867;&#21035;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#20013;&#65292;&#20363;&#22914;&#65292;&#22312;&#19981;&#24179;&#34913;&#30340;&#22522;&#20934;&#25968;&#25454;&#21644;&#21307;&#23398;&#23454;&#38469;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#25298;&#32477;&#26354;&#32447;&#65306;&#21484;&#22238;&#29575;-&#25298;&#32477;&#26354;&#32447;&#21644;&#31934;&#30830;&#24230;-&#25298;&#32477;&#26354;&#32447;&#12290;&#36890;&#36807;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#20013;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#20154;&#24037;&#22522;&#20934;&#25968;&#25454;&#19978;&#23558;&#25552;&#20986;&#30340;&#26354;&#32447;&#19982;&#20934;&#30830;&#24230;&#25298;&#32477;&#26354;&#32447;&#20316;&#20026;&#22522;&#20934;&#36827;&#34892;&#39564;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23384;&#22312;&#31867;&#21035;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#22522;&#20934;&#25968;&#25454;&#21644;&#21307;&#23398;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
For some classification scenarios, it is desirable to use only those classification instances that a trained model associates with a high certainty. To obtain such high-certainty instances, previous work has proposed accuracy-reject curves. Reject curves allow to evaluate and compare the performance of different certainty measures over a range of thresholds for accepting or rejecting classifications. However, the accuracy may not be the most suited evaluation metric for all applications, and instead precision or recall may be preferable. This is the case, for example, for data with imbalanced class distributions. We therefore propose reject curves that evaluate precision and recall, the recall-reject curve and the precision-reject curve. Using prototype-based classifiers from learning vector quantization, we first validate the proposed curves on artificial benchmark data against the accuracy reject curve as a baseline. We then show on imbalanced benchmarks and medical, real-world data 
&lt;/p&gt;</description></item><item><title>HyperSNN&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#25511;&#21046;&#24212;&#29992;&#30340;&#39640;&#25928;&#31283;&#20581;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#32500;&#35745;&#31639;&#65292;&#23558;&#33021;&#37327;&#28040;&#32791;&#38477;&#20302;&#33267;1.36%-9.96%&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#23427;&#36866;&#29992;&#20110;&#20132;&#20114;&#24335;&#12289;&#31227;&#21160;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#20419;&#36827;&#20102;&#33021;&#37327;&#39640;&#25928;&#21644;&#31283;&#20581;&#30340;&#31995;&#32479;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.08222</link><description>&lt;p&gt;
HyperSNN&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#25511;&#21046;&#24212;&#29992;&#30340;&#39640;&#25928;&#31283;&#20581;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HyperSNN: A new efficient and robust deep learning model for resource constrained control applications. (arXiv:2308.08222v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08222
&lt;/p&gt;
&lt;p&gt;
HyperSNN&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#25511;&#21046;&#24212;&#29992;&#30340;&#39640;&#25928;&#31283;&#20581;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#32500;&#35745;&#31639;&#65292;&#23558;&#33021;&#37327;&#28040;&#32791;&#38477;&#20302;&#33267;1.36%-9.96%&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#23427;&#36866;&#29992;&#20110;&#20132;&#20114;&#24335;&#12289;&#31227;&#21160;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#20419;&#36827;&#20102;&#33021;&#37327;&#39640;&#25928;&#21644;&#31283;&#20581;&#30340;&#31995;&#32479;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#36793;&#32536;&#35745;&#31639;&#22312;&#26234;&#33021;&#23478;&#20855;&#12289;&#26426;&#22120;&#20154;&#21644;&#26234;&#33021;&#23478;&#23621;&#31561;&#39046;&#22495;&#30340;&#26085;&#30410;&#37319;&#29992;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25511;&#21046;&#20219;&#21153;&#26041;&#27861;HyperSNN&#65292;&#23427;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#39640;&#32500;&#35745;&#31639;&#30456;&#32467;&#21512;&#12290; HyperSNN&#29992;8&#20301;&#25972;&#25968;&#21152;&#27861;&#26367;&#20195;&#20102;&#26114;&#36149;&#30340;32&#20301;&#28014;&#28857;&#20056;&#27861;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#33021;&#37327;&#28040;&#32791;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#31283;&#20581;&#24615;&#21644;&#21487;&#33021;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;AI Gym&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;Cartpole&#12289;Acrobot&#12289;MountainCar&#21644;Lunar Lander&#12290; HyperSNN&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#65292;&#20294;&#20165;&#28040;&#32791;1.36&#65285;&#33267;9.96&#65285;&#30340;&#33021;&#37327;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#20351;&#29992;HyperSNN&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;HyperSNN&#29305;&#21035;&#36866;&#29992;&#20110;&#20132;&#20114;&#24335;&#12289;&#31227;&#21160;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#20419;&#36827;&#20102;&#33021;&#37327;&#39640;&#25928;&#21644;&#31283;&#20581;&#30340;&#31995;&#32479;&#35774;&#35745;&#12290;&#32780;&#19988;&#65292;&#23427;&#20026;.....
&lt;/p&gt;
&lt;p&gt;
In light of the increasing adoption of edge computing in areas such as intelligent furniture, robotics, and smart homes, this paper introduces HyperSNN, an innovative method for control tasks that uses spiking neural networks (SNNs) in combination with hyperdimensional computing. HyperSNN substitutes expensive 32-bit floating point multiplications with 8-bit integer additions, resulting in reduced energy consumption while enhancing robustness and potentially improving accuracy. Our model was tested on AI Gym benchmarks, including Cartpole, Acrobot, MountainCar, and Lunar Lander. HyperSNN achieves control accuracies that are on par with conventional machine learning methods but with only 1.36% to 9.96% of the energy expenditure. Furthermore, our experiments showed increased robustness when using HyperSNN. We believe that HyperSNN is especially suitable for interactive, mobile, and wearable devices, promoting energy-efficient and robust system design. Furthermore, it paves the way for th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21387;&#32553;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;(SCALLION&#21644;SCAFCOM)&#65292;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#32463;&#20856;&#30340;&#38543;&#26426;&#25511;&#21046;&#24179;&#22343;&#27861;&#24182;&#25552;&#20986;&#20102;&#31561;&#20215;&#20294;&#26356;&#39640;&#25928;/&#31616;&#21270;&#30340;&#24418;&#24335;&#65292;&#20943;&#23569;&#20102;&#19978;&#34892;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.08165</link><description>&lt;p&gt;
&#24102;&#26377;&#36890;&#20449;&#21387;&#32553;&#30340;&#38543;&#26426;&#25511;&#21046;&#24179;&#22343;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stochastic Controlled Averaging for Federated Learning with Communication Compression. (arXiv:2308.08165v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21387;&#32553;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;(SCALLION&#21644;SCAFCOM)&#65292;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#32463;&#20856;&#30340;&#38543;&#26426;&#25511;&#21046;&#24179;&#22343;&#27861;&#24182;&#25552;&#20986;&#20102;&#31561;&#20215;&#20294;&#26356;&#39640;&#25928;/&#31616;&#21270;&#30340;&#24418;&#24335;&#65292;&#20943;&#23569;&#20102;&#19978;&#34892;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#21387;&#32553;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#36890;&#36807;&#26080;&#32447;&#20256;&#36755;&#30340;&#20449;&#24687;&#37327;&#30340;&#25216;&#26415;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#20943;&#36731;&#36890;&#20449;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#36890;&#20449;&#21387;&#32553;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#21387;&#32553;&#24341;&#36215;&#30340;&#20449;&#24687;&#22833;&#30495;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#30340;&#29305;&#24615;&#65292;&#22914;&#37096;&#20998;&#21442;&#19982;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#26377;&#25152;&#21457;&#23637;&#65292;&#21387;&#32553;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#19981;&#33021;&#36866;&#24212;&#20219;&#24847;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#25110;&#37096;&#20998;&#21442;&#19982;&#65292;&#35201;&#20040;&#35201;&#27714;&#23545;&#21387;&#32553;&#26377;&#20005;&#26684;&#30340;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20855;&#26377;&#24320;&#38144;&#20943;&#21322;&#30340;&#19978;&#34892;&#36890;&#20449;&#25104;&#26412;&#30340;&#32463;&#20856;&#38543;&#26426;&#25511;&#21046;&#24179;&#22343;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21387;&#32553;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;SCALLION&#21644;SCAFCOM&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication compression, a technique aiming to reduce the information volume to be transmitted over the air, has gained great interests in Federated Learning (FL) for the potential of alleviating its communication overhead. However, communication compression brings forth new challenges in FL due to the interplay of compression-incurred information distortion and inherent characteristics of FL such as partial participation and data heterogeneity. Despite the recent development, the performance of compressed FL approaches has not been fully exploited. The existing approaches either cannot accommodate arbitrary data heterogeneity or partial participation, or require stringent conditions on compression.  In this paper, we revisit the seminal stochastic controlled averaging method by proposing an equivalent but more efficient/simplified formulation with halved uplink communication costs. Building upon this implementation, we propose two compressed FL algorithms, SCALLION and SCAFCOM, to s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#22122;&#22768;&#26597;&#35810;&#20013;&#29983;&#25104;&#20934;&#30830;&#30340;&#20107;&#20214;&#36793;&#30028;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.07293</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#65288;DiffSED&#65289;
&lt;/p&gt;
&lt;p&gt;
DiffSED: Sound Event Detection with Denoising Diffusion. (arXiv:2308.07293v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#22122;&#22768;&#26597;&#35810;&#20013;&#29983;&#25104;&#20934;&#30830;&#30340;&#20107;&#20214;&#36793;&#30028;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#65288;SED&#65289;&#26088;&#22312;&#22312;&#32473;&#23450;&#26080;&#32422;&#26463;&#38899;&#39057;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#24863;&#20852;&#36259;&#20107;&#20214;&#30340;&#26102;&#38388;&#36793;&#30028;&#21644;&#31867;&#21035;&#26631;&#31614;&#12290;&#29616;&#26377;&#30340;&#25152;&#26377;&#26041;&#27861;&#37117;&#26159;&#20174;&#37492;&#21035;&#23398;&#20064;&#30340;&#35282;&#24230;&#32771;&#34385;SED&#38382;&#39064;&#65292;&#37319;&#29992;&#20998;&#21106;&#21644;&#20998;&#31867;&#65288;&#21363;&#24103;&#32423;&#65289;&#31574;&#30053;&#25110;&#26356;&#26377;&#21407;&#21017;&#30340;&#20107;&#20214;&#32423;&#24314;&#27169;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#21462;&#29983;&#25104;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#37325;&#26032;&#23450;&#20041;SED&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#23494;&#38598;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#22522;&#20110;&#30446;&#26631;&#38899;&#39057;&#26679;&#26412;&#65292;&#20174;&#22122;&#22768;&#25552;&#35758;&#20013;&#29983;&#25104;&#20934;&#30830;&#30340;&#22768;&#38899;&#26102;&#38388;&#36793;&#30028;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#22312;&#20248;&#38597;&#30340;Transformer&#35299;&#30721;&#22120;&#26694;&#26550;&#20013;&#23558;&#22122;&#22768;&#28508;&#22312;&#26597;&#35810;&#36716;&#25442;&#20026;&#22320;&#38754;&#30495;&#23454;&#29256;&#26412;&#65292;&#20174;&#32780;&#23398;&#20064;&#36870;&#36716;&#22122;&#22768;&#36807;&#31243;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#21363;&#20351;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20351;&#29992;&#22122;&#22768;&#26597;&#35810;&#65292;&#27169;&#22411;&#20063;&#33021;&#29983;&#25104;&#20934;&#30830;&#30340;&#20107;&#20214;&#36793;&#30028;&#12290;&#22312;Urban-SED&#21644;EPIC-Sounds&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound Event Detection (SED) aims to predict the temporal boundaries of all the events of interest and their class labels, given an unconstrained audio sample. Taking either the splitand-classify (i.e., frame-level) strategy or the more principled event-level modeling approach, all existing methods consider the SED problem from the discriminative learning perspective. In this work, we reformulate the SED problem by taking a generative learning perspective. Specifically, we aim to generate sound temporal boundaries from noisy proposals in a denoising diffusion process, conditioned on a target audio sample. During training, our model learns to reverse the noising process by converting noisy latent queries to the groundtruth versions in the elegant Transformer decoder framework. Doing so enables the model generate accurate event boundaries from even noisy queries during inference. Extensive experiments on the Urban-SED and EPIC-Sounds datasets demonstrate that our model significantly outpe
&lt;/p&gt;</description></item><item><title>AudioFormer&#26159;&#19968;&#31181;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#34920;&#31034;&#26469;&#25429;&#25417;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07221</link><description>&lt;p&gt;
AudioFormer: &#36890;&#36807;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#38899;&#39057;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes. (arXiv:2308.07221v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07221
&lt;/p&gt;
&lt;p&gt;
AudioFormer&#26159;&#19968;&#31181;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#34920;&#31034;&#26469;&#25429;&#25417;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AudioFormer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#26469;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#38543;&#21518;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#20197;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#30340;&#24418;&#24335;&#65292;&#20511;&#21161;&#29616;&#26377;&#30340;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#27169;&#22411;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#19968;&#20010;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411; (MLM)&#65292;&#20174;&#32780;&#33719;&#24471;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#21019;&#20102;&#19968;&#31181;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604; (MPC) &#23398;&#20064;&#26041;&#27861;&#30340;&#25972;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21516;&#19968;&#38899;&#39057;&#36755;&#20837;&#20013;&#22810;&#20010;&#31163;&#25955;&#22768;&#23398;&#20195;&#30721;&#38388;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#35270;&#20026;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#31867;&#20284;&#22635;&#31354;&#39064;&#30340;&#26041;&#27861;&#35757;&#32451;&#19968;&#20010;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#26368;&#32456;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MPC&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21040;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures c
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#20892;&#19994;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#20892;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.06668</link><description>&lt;p&gt;
&#26234;&#33021;&#20892;&#19994;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#65306;&#22522;&#30784;&#30693;&#35782;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges. (arXiv:2308.06668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06668
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#20892;&#19994;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#20892;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#38388;&#65292;&#20892;&#19994;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#23637;&#31034;&#20986;&#22312;&#21508;&#31181;&#20892;&#19994;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65306;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#12289;&#38590;&#20197;&#33719;&#21462;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#24320;&#21457;&#21644;&#32500;&#25252;&#65292;&#32780;&#19988;&#22823;&#22810;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#65292;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36328;&#36234;&#20102;&#21508;&#20010;&#39046;&#22495;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#23569;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#23436;&#25104;&#21508;&#31181;&#22810;&#26679;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#20892;&#19994;&#39046;&#22495;&#20013;&#24212;&#29992;&#23578;&#26410;&#26377;&#22826;&#22810;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#20892;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agricultu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EGP&#30340;&#21019;&#26032;&#30340;&#29109;&#24341;&#23548;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#20248;&#20808;&#21098;&#38500;&#29109;&#36739;&#20302;&#30340;&#23618;&#20013;&#30340;&#36830;&#25509;&#26469;&#26377;&#25928;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#31454;&#20105;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.06619</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?. (arXiv:2308.06619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EGP&#30340;&#21019;&#26032;&#30340;&#29109;&#24341;&#23548;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#20248;&#20808;&#21098;&#38500;&#29109;&#36739;&#20302;&#30340;&#23618;&#20013;&#30340;&#36830;&#25509;&#26469;&#26377;&#25928;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#31454;&#20105;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20943;&#23567;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#22312;&#26377;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#25216;&#26415;&#20063;&#24456;&#38590;&#20174;&#27169;&#22411;&#20013;&#23436;&#20840;&#21435;&#38500;&#25972;&#20010;&#23618;&#65306;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#35299;&#20915;&#30340;&#20219;&#21153;&#21527;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EGP&#30340;&#21019;&#26032;&#30340;&#29109;&#24341;&#23548;&#21098;&#26525;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#23567;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;EGP&#30340;&#20851;&#38190;&#37325;&#28857;&#26159;&#20248;&#20808;&#21098;&#38500;&#29109;&#36739;&#20302;&#30340;&#23618;&#20013;&#30340;&#36830;&#25509;&#65292;&#26368;&#32456;&#23436;&#20840;&#21435;&#38500;&#36825;&#20123;&#23618;&#12290;&#36890;&#36807;&#22312;ResNet-18&#21644;Swin-T&#31561;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;EGP&#33021;&#22815;&#26377;&#25928;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#25581;&#31034;&#20102;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#20248;&#21183;&#32972;&#21518;&#30340;&#26426;&#21046;&#65292;&#36824;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#22797;&#26434;&#30340;&#20851;&#31995;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38236;&#20687;&#25193;&#25955;&#27169;&#22411;(MDMs)&#65292;&#29992;&#20110;&#22312;&#31163;&#25955;&#20998;&#31867;&#25968;&#25454;&#21644;&#36830;&#32493;&#39046;&#22495;&#20013;&#36827;&#34892;&#29983;&#25104;&#20219;&#21153;&#12290;MDMs&#21463;&#38480;&#21046;&#25277;&#26679;&#38382;&#39064;&#30340;&#38236;&#20687;Langevin&#31639;&#27861;&#21551;&#21457;&#65292;&#24182;&#25552;&#20379;&#20102;&#36866;&#24212;&#31616;&#21333;&#25193;&#25955;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.06342</link><description>&lt;p&gt;
&#38236;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mirror Diffusion Models. (arXiv:2308.06342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38236;&#20687;&#25193;&#25955;&#27169;&#22411;(MDMs)&#65292;&#29992;&#20110;&#22312;&#31163;&#25955;&#20998;&#31867;&#25968;&#25454;&#21644;&#36830;&#32493;&#39046;&#22495;&#20013;&#36827;&#34892;&#29983;&#25104;&#20219;&#21153;&#12290;MDMs&#21463;&#38480;&#21046;&#25277;&#26679;&#38382;&#39064;&#30340;&#38236;&#20687;Langevin&#31639;&#27861;&#21551;&#21457;&#65292;&#24182;&#25552;&#20379;&#20102;&#36866;&#24212;&#31616;&#21333;&#25193;&#25955;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#36830;&#32493;&#39046;&#22495;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#31163;&#25955;&#30340;&#20998;&#31867;&#25968;&#25454;&#20013;&#24212;&#29992;&#25193;&#25955;&#20173;&#28982;&#26159;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#22312;&#36830;&#32493;&#39046;&#22495;&#30340;&#29983;&#25104;&#20013;&#24120;&#24120;&#38656;&#35201;&#36827;&#34892;&#21098;&#20999;&#65292;&#36825;&#23601;&#38656;&#35201;&#19968;&#20010;&#23558;&#25193;&#25955;&#36866;&#24212;&#32422;&#26463;&#22495;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#21463;&#38480;&#21046;&#25277;&#26679;&#38382;&#39064;&#30340;&#38236;&#20687;Langevin&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#26412;&#29702;&#35770;&#25253;&#21578;&#20013;&#25105;&#20204;&#25552;&#20986;&#20102;&#38236;&#20687;&#25193;&#25955;&#27169;&#22411;(MDMs)&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;MDMs&#22312;simplex&#25193;&#25955;&#30340;&#32972;&#26223;&#19979;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#28909;&#38376;&#39046;&#22495;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06053</link><description>&lt;p&gt;
&#22312;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#19978;&#20855;&#26377;MiRo&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#35774;&#22791;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#20174;&#25345;&#32493;&#30340;&#20219;&#21153;&#27969;&#20013;&#36880;&#27493;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#20102;&#35760;&#20303;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#26087;&#26679;&#26412;&#23384;&#20648;&#22312;&#19968;&#20010;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#20013;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#21040;&#26469;&#26102;&#36827;&#34892;&#22238;&#25918;&#12290;&#37319;&#29992;&#25345;&#32493;&#23398;&#20064;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#36793;&#32536;&#35774;&#22791;&#36890;&#24120;&#23545;&#33021;&#28304;&#25935;&#24863;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#19981;&#25439;&#23475;&#33021;&#28304;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#27169;&#22411;&#20934;&#30830;&#24230;&#65292;&#21363;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#20197;&#33719;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#36890;&#36807;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#36164;&#28304;&#29366;&#24577;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#65292;&#20174;&#32780;&#23558;&#25105;&#20204;&#30340;&#35265;&#35299;&#31934;&#30830;&#22320;&#25972;&#21512;&#21040;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#25104;&#26412;&#25928;&#30410;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;Miro&#36824;&#23545;&#24102;&#26377;&#26126;&#30830;&#20934;&#30830;&#24230;-&#33021;&#37327;&#24179;&#34913;&#30340;&#21442;&#25968;&#36827;&#34892;&#22312;&#32447;&#20998;&#26512;&#65292;&#24182;&#20197;&#20302;&#24320;&#38144;&#22320;&#36866;&#24212;&#26368;&#20339;&#20540;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23454;&#20363;&#33258;&#36866;&#24212;&#25512;&#29702;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32553;&#25918;&#21644;&#20301;&#31227;&#28145;&#24230;&#29305;&#24449;&#65288;SSF&#65289;&#23454;&#29616;&#20102;&#22788;&#29702;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06051</link><description>&lt;p&gt;
&#38754;&#21521;&#23454;&#20363;&#33258;&#36866;&#24212;&#25512;&#29702;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Instance-adaptive Inference for Federated Learning. (arXiv:2308.06051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23454;&#20363;&#33258;&#36866;&#24212;&#25512;&#29702;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32553;&#25918;&#21644;&#20301;&#31227;&#28145;&#24230;&#29305;&#24449;&#65288;SSF&#65289;&#23454;&#29616;&#20102;&#22788;&#29702;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#27719;&#38598;&#26412;&#22320;&#35757;&#32451;&#26469;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#20943;&#36731;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#12290;&#36229;&#36234;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22312;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#20063;&#21487;&#20197;&#35266;&#23519;&#21040;&#23458;&#25143;&#31471;&#20869;&#37096;&#30340;&#24322;&#36136;&#24615;&#65292;&#20005;&#37325;&#24433;&#21709;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;FedIns&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#23454;&#29616;&#20102;&#23454;&#20363;&#33258;&#36866;&#24212;&#25512;&#29702;&#26469;&#22788;&#29702;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#19981;&#20351;&#29992;&#24222;&#22823;&#30340;&#23454;&#20363;&#33258;&#36866;&#24212;&#27169;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#8212;&#8212;&#32553;&#25918;&#21644;&#20301;&#31227;&#28145;&#24230;&#29305;&#24449;&#65288;SSF&#65289;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#36827;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#35757;&#32451;&#19968;&#20010;SSF&#27744;&#65292;&#22312;&#26381;&#21153;&#22120;&#31471;&#27719;&#38598;&#36825;&#20123;SSF&#27744;&#65292;&#20174;&#32780;&#20173;&#28982;&#20445;&#25345;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed learning paradigm that enables multiple clients to learn a powerful global model by aggregating local training. However, the performance of the global model is often hampered by non-i.i.d. distribution among the clients, requiring extensive efforts to mitigate inter-client data heterogeneity. Going beyond inter-client data heterogeneity, we note that intra-client heterogeneity can also be observed on complex real-world data and seriously deteriorate FL performance. In this paper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client data heterogeneity by enabling instance-adaptive inference in the FL framework. Instead of huge instance-adaptive models, we resort to a parameter-efficient fine-tuning method, i.e., scale and shift deep features (SSF), upon a pre-trained model. Specifically, we first train an SSF pool for each client, and aggregate these SSF pools on the server side, thus still maintaining a low communication cost. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30828;&#24615;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#27969;&#24418;&#21644;&#23450;&#20041;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;&#26469;&#25915;&#20987;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#33030;&#24369;&#24615;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2308.05681</link><description>&lt;p&gt;
&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#38754;&#20020;&#30340;&#30828;&#24615;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#21644;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient. (arXiv:2308.05681v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30828;&#24615;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#27969;&#24418;&#21644;&#23450;&#20041;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;&#26469;&#25915;&#20987;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#33030;&#24369;&#24615;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#26041;&#27861;&#35201;&#27714;&#35201;&#20040;&#23436;&#20840;&#20102;&#35299;&#21463;&#23475;&#32773;&#65288;&#21363;&#30333;&#30418;&#25915;&#20987;&#65289;&#65292;&#35201;&#20040;&#26377;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65288;&#21363;&#22522;&#20110;&#36716;&#31227;&#30340;&#25915;&#20987;&#65289;&#65292;&#25110;&#32773;&#39057;&#32321;&#26597;&#35810;&#27169;&#22411;&#65288;&#21363;&#40657;&#30418;&#25915;&#20987;&#65289;&#12290;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#37117;&#38750;&#24120;&#38480;&#21046;&#24615;&#65292;&#24341;&#21457;&#20102;&#23545;&#33030;&#24369;&#24615;&#30340;&#36136;&#30097;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33030;&#24369;&#24615;&#30830;&#23454;&#23384;&#22312;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#20219;&#21153;&#65306;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#21463;&#23475;&#32773;&#27169;&#22411;&#25110;&#35757;&#32451;&#25968;&#25454;&#25110;&#26631;&#31614;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#30828;&#24615;&#26080;&#30418;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#36816;&#21160;&#27969;&#24418;&#65292;&#28982;&#21518;&#23450;&#20041;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#25915;&#20987;&#30340;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;&#65288;SMI&#26799;&#24230;&#65289;&#12290;&#25105;&#20204;&#30340;&#26799;&#24230;&#21253;&#21547;&#36816;&#21160;&#21160;&#21147;&#23398;&#30340;&#20449;&#24687;&#65292;&#36825;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#20551;&#35774;&#25439;&#22833;&#26799;&#24230;&#26159;&#36890;&#36807;&#35745;&#31639;&#32780;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming 
&lt;/p&gt;</description></item><item><title>PTransIPs&#26159;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#27688;&#22522;&#37240;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#65292;&#24182;&#32467;&#21512;&#22823;&#22411;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#23884;&#20837;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#35782;&#21035;&#30967;&#37240;&#21270;&#20301;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.05115</link><description>&lt;p&gt;
&#22522;&#20110;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;Transformer&#30340;&#30967;&#37240;&#21270;&#20301;&#28857;&#35782;&#21035;&#26041;&#27861;(PTransIPs)
&lt;/p&gt;
&lt;p&gt;
PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer. (arXiv:2308.05115v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05115
&lt;/p&gt;
&lt;p&gt;
PTransIPs&#26159;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#27688;&#22522;&#37240;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#65292;&#24182;&#32467;&#21512;&#22823;&#22411;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#23884;&#20837;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#35782;&#21035;&#30967;&#37240;&#21270;&#20301;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30967;&#37240;&#21270;&#26159;&#35768;&#22810;&#22522;&#30784;&#32454;&#32990;&#36807;&#31243;&#30340;&#26680;&#24515;&#65292;&#24433;&#21709;&#30528;&#21508;&#31181;&#30142;&#30149;&#30340;&#21457;&#29983;&#21644;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#30967;&#37240;&#21270;&#20301;&#28857;&#30340;&#35782;&#21035;&#26159;&#29702;&#35299;&#32454;&#32990;&#21644;&#30149;&#27602;&#24863;&#26579;&#30340;&#20998;&#23376;&#26426;&#21046;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#21487;&#33021;&#20026;&#26032;&#30340;&#27835;&#30103;&#38774;&#28857;&#25552;&#20379;&#22522;&#30784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTransIPs&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#30967;&#37240;&#21270;&#20301;&#28857;&#12290;PTransIPs&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#27688;&#22522;&#37240;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#65292;&#24182;&#26681;&#25454;&#24207;&#21015;&#20013;&#27688;&#22522;&#37240;&#30340;&#31867;&#22411;&#21644;&#20301;&#32622;&#25552;&#21462;&#29420;&#29305;&#30340;&#32534;&#30721;&#12290;&#23427;&#36824;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#23884;&#20837;&#20316;&#20026;&#39069;&#22806;&#30340;&#25968;&#25454;&#36755;&#20837;&#12290;PTransIPs&#36827;&#19968;&#27493;&#36890;&#36807;&#32467;&#21512;&#20855;&#26377;&#27531;&#24046;&#36830;&#25509;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#65292;&#37197;&#22791;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20840;&#36830;&#25509;&#23618;&#36755;&#20986;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phosphorylation is central to numerous fundamental cellular processes, influencing the onset and progression of a variety of diseases. Identification of phosphorylation sites is thus an important step for understanding the molecular mechanisms of cells and virus infection, which potentially leads to new therapeutic targets. In this study, we present PTransIPs, a novel deep learning model for the identification of phosphorylation sites. PTransIPs treats amino acids in protein sequences as words in natural language, extracting unique encodings based on the types along with position of amino acids in the sequence. It also incorporates embeddings from large pre-trained protein models as additional data inputs. PTransIPS is further trained on a combination model of convolutional neural network with residual connections and Transformer model equipped with multi-head attention mechanisms. At last, the model outputs classification results through a fully connected layer. The results of indepen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Adversarial ModSecurity&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#26469;&#23545;&#25239;SQL&#27880;&#20837;&#25915;&#20987;&#30340;&#38450;&#28779;&#22681;&#12290;&#36890;&#36807;&#23558;&#26680;&#24515;&#35268;&#21017;&#38598;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#35782;&#21035;&#24182;&#38450;&#24481;&#23545;&#25239;&#24615;SQL&#27880;&#20837;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AdvModSec&#22312;&#35757;&#32451;&#21518;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#36825;&#31867;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2308.04964</link><description>&lt;p&gt;
Adversarial ModSecurity: &#20351;&#29992;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;SQL&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial ModSecurity: Countering Adversarial SQL Injections with Robust Machine Learning. (arXiv:2308.04964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04964
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Adversarial ModSecurity&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#26469;&#23545;&#25239;SQL&#27880;&#20837;&#25915;&#20987;&#30340;&#38450;&#28779;&#22681;&#12290;&#36890;&#36807;&#23558;&#26680;&#24515;&#35268;&#21017;&#38598;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#35782;&#21035;&#24182;&#38450;&#24481;&#23545;&#25239;&#24615;SQL&#27880;&#20837;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AdvModSec&#22312;&#35757;&#32451;&#21518;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#36825;&#31867;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ModSecurity&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#26631;&#20934;&#30340;&#24320;&#28304;Web&#24212;&#29992;&#38450;&#28779;&#22681;(WAF)&#65292;&#30001;OWASP&#22522;&#37329;&#20250;&#32500;&#25252;&#12290;&#23427;&#36890;&#36807;&#19982;&#26680;&#24515;&#35268;&#21017;&#38598;&#36827;&#34892;&#21305;&#37197;&#26469;&#26816;&#27979;&#24694;&#24847;&#35831;&#27714;&#65292;&#35782;&#21035;&#20986;&#24120;&#35265;&#30340;&#25915;&#20987;&#27169;&#24335;&#12290;&#27599;&#20010;&#35268;&#21017;&#22312;CRS&#20013;&#37117;&#34987;&#25163;&#21160;&#20998;&#37197;&#19968;&#20010;&#26435;&#37325;&#65292;&#22522;&#20110;&#30456;&#24212;&#25915;&#20987;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#22914;&#26524;&#35302;&#21457;&#35268;&#21017;&#30340;&#26435;&#37325;&#20043;&#21644;&#36229;&#36807;&#32473;&#23450;&#30340;&#38408;&#20540;&#65292;&#23601;&#20250;&#34987;&#26816;&#27979;&#20026;&#24694;&#24847;&#35831;&#27714;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#22312;&#26816;&#27979;SQL&#27880;&#20837;&#25915;&#20987;&#26041;&#38754;&#24456;&#19981;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#24448;&#24448;&#20250;&#38459;&#27490;&#35768;&#22810;&#21512;&#27861;&#35831;&#27714;&#65292;&#21516;&#26102;&#36824;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;SQL&#27880;&#20837;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#25925;&#24847;&#25805;&#32437;&#20197;&#36867;&#36991;&#26816;&#27979;&#30340;&#25915;&#20987;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;AdvModSec&#30340;&#24378;&#22823;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#23558;CRS&#35268;&#21017;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#32463;&#36807;&#35757;&#32451;&#20197;&#26816;&#27979;&#23545;&#25239;&#24615;SQL&#27880;&#20837;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AdvModSec&#22312;&#38024;&#23545;&#35813;&#25915;&#20987;&#30340;&#27969;&#37327;&#19978;&#36827;&#34892;&#35757;&#32451;&#21518;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
ModSecurity is widely recognized as the standard open-source Web Application Firewall (WAF), maintained by the OWASP Foundation. It detects malicious requests by matching them against the Core Rule Set, identifying well-known attack patterns. Each rule in the CRS is manually assigned a weight, based on the severity of the corresponding attack, and a request is detected as malicious if the sum of the weights of the firing rules exceeds a given threshold. In this work, we show that this simple strategy is largely ineffective for detecting SQL injection (SQLi) attacks, as it tends to block many legitimate requests, while also being vulnerable to adversarial SQLi attacks, i.e., attacks intentionally manipulated to evade detection. To overcome these issues, we design a robust machine learning model, named AdvModSec, which uses the CRS rules as input features, and it is trained to detect adversarial SQLi attacks. Our experiments show that AdvModSec, being trained on the traffic directed towa
&lt;/p&gt;</description></item><item><title>OmniDataComposer&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21644;&#26080;&#38480;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#35843;&#25968;&#25454;&#32467;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#21644;&#21512;&#24182;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#31561;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#65292;&#24182;&#23454;&#29616;&#36328;&#27169;&#24577;&#25968;&#25454;&#26657;&#27491;&#12290;</title><link>http://arxiv.org/abs/2308.04126</link><description>&lt;p&gt;
OmniDataComposer: &#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21644;&#26080;&#38480;&#25968;&#25454;&#29983;&#25104;&#30340;&#32479;&#19968;&#25968;&#25454;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation. (arXiv:2308.04126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04126
&lt;/p&gt;
&lt;p&gt;
OmniDataComposer&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21644;&#26080;&#38480;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#35843;&#25968;&#25454;&#32467;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#21644;&#21512;&#24182;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#31561;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#65292;&#24182;&#23454;&#29616;&#36328;&#27169;&#24577;&#25968;&#25454;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;OmniDataComposer&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21644;&#26080;&#38480;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#21644;&#31616;&#21270;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26368;&#26680;&#24515;&#30340;&#31361;&#30772;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#22788;&#29702;&#21644;&#21512;&#24182;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#21327;&#35843;&#25968;&#25454;&#32467;&#26500;&#65292;&#21253;&#25324;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#35270;&#39057;/&#22270;&#20687;&#23383;&#24149;&#25552;&#21462;&#12289;&#23494;&#38598;&#23383;&#24149;&#25552;&#21462;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#12289;Recognize Anything Model&#65288;RAM&#65289;&#21644;&#29289;&#20307;&#36319;&#36394;&#31561;&#22810;&#31181;&#25805;&#20316;&#30340;&#36827;&#23637;&#12290;OmniDataComposer&#33021;&#22815;&#35782;&#21035;&#36229;&#36807;6400&#31181;&#23545;&#35937;&#31867;&#21035;&#65292;&#26174;&#33879;&#25193;&#22823;&#20102;&#35270;&#35273;&#20449;&#24687;&#30340;&#33539;&#22260;&#12290;&#23427;&#23558;&#36825;&#20123;&#22810;&#26679;&#30340;&#27169;&#24577;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20419;&#36827;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#22686;&#24378;&#65292;&#24182;&#20419;&#36827;&#36328;&#27169;&#24577;&#25968;&#25454;&#26657;&#27491;&#12290;&#26368;&#32456;&#36755;&#20986;&#23558;&#27599;&#20010;&#35270;&#39057;&#36755;&#20837;&#36716;&#21270;&#20026;&#35814;&#32454;&#30340;&#39034;&#24207;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents OmniDataComposer, an innovative approach for multimodal data fusion and unlimited data generation with an intent to refine and uncomplicate interplay among diverse data modalities. Coming to the core breakthrough, it introduces a cohesive data structure proficient in processing and merging multimodal data inputs, which include video, audio, and text. Our crafted algorithm leverages advancements across multiple operations such as video/image caption extraction, dense caption extraction, Automatic Speech Recognition (ASR), Optical Character Recognition (OCR), Recognize Anything Model(RAM), and object tracking. OmniDataComposer is capable of identifying over 6400 categories of objects, substantially broadening the spectrum of visual information. It amalgamates these diverse modalities, promoting reciprocal enhancement among modalities and facilitating cross-modal data correction. \textbf{The final output metamorphoses each video input into an elaborate sequential docum
&lt;/p&gt;</description></item><item><title>DOMINO&#26159;&#19968;&#31181;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32500;&#20998;&#31867;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65307;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03295</link><description>&lt;p&gt;
DOMINO: &#22810;&#20010;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32500;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data. (arXiv:2308.03295v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03295
&lt;/p&gt;
&lt;p&gt;
DOMINO&#26159;&#19968;&#31181;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32500;&#20998;&#31867;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65307;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#21033;&#29992;&#24322;&#26500;&#36830;&#25509;&#30340;&#20256;&#24863;&#22120;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24120;&#34987;&#29992;&#20110;&#20998;&#26512;&#26412;&#22320;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#20998;&#24067;&#20559;&#31227;&#12290;&#24403;&#19968;&#20010;&#27169;&#22411;&#37096;&#32626;&#22312;&#19982;&#20854;&#35757;&#32451;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#26102;&#65292;&#20998;&#24067;&#20559;&#31227;&#20250;&#20005;&#37325;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#25429;&#25417;&#22810;&#20010;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#36825;&#38656;&#35201;&#36229;&#20986;&#24403;&#20170;&#36793;&#32536;&#35774;&#22791;&#23481;&#37327;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#32780;&#22823;&#33041;&#21551;&#21457;&#30340;&#39640;&#32500;&#35745;&#31639;(HDC)&#20316;&#20026;&#36793;&#32536;&#23398;&#20064;&#30340;&#19968;&#31181;&#36731;&#37327;&#32423;&#35299;&#20915;&#26041;&#26696;&#24050;&#34987;&#24341;&#20837;&#65292;&#20294;&#29616;&#26377;&#30340;HDC&#22312;&#38754;&#23545;&#20998;&#24067;&#20559;&#31227;&#25361;&#25112;&#26102;&#20173;&#28982;&#33030;&#24369;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DOMINO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;HDC&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid evolution of the Internet of Things, many real-world applications utilize heterogeneously connected sensors to capture time-series information. Edge-based machine learning (ML) methodologies are often employed to analyze locally collected data. However, a fundamental issue across data-driven ML approaches is distribution shift. It occurs when a model is deployed on a data distribution different from what it was trained on, and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) have been proposed to capture spatial and temporal dependencies in multi-sensor time series data, requiring intensive computational resources beyond the capacity of today's edge devices. While brain-inspired hyperdimensional computing (HDC) has been introduced as a lightweight solution for edge-based learning, existing HDCs are also vulnerable to the distribution shift challenge. In this paper, we propose DOMINO, a novel HDC learning fr
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;PanStarrs DR1&#12289;2MASS&#21644;WISE&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21306;&#20998;L&#31867;&#21644;T&#31867;&#26837;&#30702;&#26143;&#21644;&#20854;&#20182;&#20809;&#35889;&#21644;&#20142;&#24230;&#31867;&#21035;&#30340;&#22825;&#20307;&#12290;&#36825;&#26377;&#21161;&#20110;&#24314;&#31435;&#19968;&#20010;&#22343;&#21248;&#19988;&#23436;&#25972;&#30340;&#26837;&#30702;&#26143;&#26679;&#26412;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#21487;&#38752;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.03045</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#29616;&#20195;&#26143;&#31354;&#35843;&#26597;&#25968;&#25454;&#20013;&#25628;&#32034;L&#65286;T&#31867;&#26837;&#30702;&#26143;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods for the search for L&amp;T brown dwarfs in the data of modern sky surveys. (arXiv:2308.03045v2 [astro-ph.SR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03045
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;PanStarrs DR1&#12289;2MASS&#21644;WISE&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21306;&#20998;L&#31867;&#21644;T&#31867;&#26837;&#30702;&#26143;&#21644;&#20854;&#20182;&#20809;&#35889;&#21644;&#20142;&#24230;&#31867;&#21035;&#30340;&#22825;&#20307;&#12290;&#36825;&#26377;&#21161;&#20110;&#24314;&#31435;&#19968;&#20010;&#22343;&#21248;&#19988;&#23436;&#25972;&#30340;&#26837;&#30702;&#26143;&#26679;&#26412;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#21487;&#38752;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#21508;&#31181;&#20272;&#35745;&#65292;&#26837;&#30702;&#26143;&#65288;BD&#65289;&#24212;&#21344;&#38134;&#27827;&#31995;&#20013;&#25152;&#26377;&#22825;&#20307;&#30340;25&#65285;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26837;&#30702;&#26143;&#34987;&#21457;&#29616;&#24182;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#65292;&#26080;&#35770;&#26159;&#20010;&#20307;&#36824;&#26159;&#25972;&#20307;&#12290;&#36825;&#20123;&#30740;&#31350;&#38656;&#35201;&#22343;&#21248;&#21644;&#23436;&#25972;&#30340;&#26837;&#30702;&#26143;&#26679;&#26412;&#12290;&#30001;&#20110;&#20854;&#24369;&#20449;&#21495;&#65292;&#26837;&#30702;&#26143;&#30340;&#20809;&#35889;&#30740;&#31350;&#30456;&#24403;&#32791;&#36153;&#31934;&#21147;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20809;&#35889;&#35266;&#27979;&#30830;&#35748;&#30340;&#22823;&#37327;&#21487;&#38752;&#30340;&#26837;&#30702;&#26143;&#26679;&#26412;&#20284;&#20046;&#22312;&#30446;&#21069;&#26159;&#19981;&#21487;&#20225;&#21450;&#30340;&#12290;&#24050;&#32463;&#23581;&#35797;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#21033;&#29992;&#20854;&#39068;&#33394;&#20316;&#20026;&#20915;&#31574;&#35268;&#21017;&#24212;&#29992;&#20110;&#22823;&#37327;&#30340;&#21208;&#27979;&#25968;&#25454;&#26469;&#25628;&#32034;&#21644;&#21019;&#24314;&#19968;&#32452;&#26837;&#30702;&#26143;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#12289;XGBoost&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#21644;TabNet&#31561;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;PanStarrs DR1&#12289;2MASS&#21644;WISE&#25968;&#25454;&#19978;&#21306;&#20998;L&#31867;&#21644;T&#31867;&#26837;&#30702;&#26143;&#19982;&#20854;&#20182;&#20809;&#35889;&#21644;&#20142;&#24230;&#31867;&#21035;&#30340;&#22825;&#20307;&#12290;&#35752;&#35770;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
According to various estimates, brown dwarfs (BD) should account for up to 25 percent of all objects in the Galaxy. However, few of them are discovered and well-studied, both individually and as a population. Homogeneous and complete samples of brown dwarfs are needed for these kinds of studies. Due to their weakness, spectral studies of brown dwarfs are rather laborious. For this reason, creating a significant reliable sample of brown dwarfs, confirmed by spectroscopic observations, seems unattainable at the moment. Numerous attempts have been made to search for and create a set of brown dwarfs using their colours as a decision rule applied to a vast amount of survey data. In this work, we use machine learning methods such as Random Forest Classifier, XGBoost, SVM Classifier and TabNet on PanStarrs DR1, 2MASS and WISE data to distinguish L and T brown dwarfs from objects of other spectral and luminosity classes. The explanation of the models is discussed. We also compare our models wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#30495;&#23454;&#21644;&#21487;&#20449;&#30340;&#25200;&#21160;&#25110;&#24322;&#24120;&#22270;&#20687;&#26469;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;Robusta&#65292;&#19968;&#31181;&#40065;&#26834;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#20026;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#21106;&#27169;&#22411;&#25552;&#20379;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#21644;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02535</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#29992;&#20110;&#40065;&#26834;&#35821;&#20041;&#20998;&#21106;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Training Datasets for Robust Semantic Segmentation. (arXiv:2308.02535v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#30495;&#23454;&#21644;&#21487;&#20449;&#30340;&#25200;&#21160;&#25110;&#24322;&#24120;&#22270;&#20687;&#26469;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;Robusta&#65292;&#19968;&#31181;&#40065;&#26834;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#20026;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#21106;&#27169;&#22411;&#25552;&#20379;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#21644;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#21644;&#22270;&#20687;&#21040;&#26631;&#31614;&#20998;&#21106;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#26469;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;Robusta&#65292;&#29992;&#20110;&#29983;&#25104;&#30495;&#23454;&#21644;&#21487;&#20449;&#30340;&#25200;&#21160;&#25110;&#24322;&#24120;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19979;&#28216;&#20998;&#21106;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#12289;&#20998;&#24067;&#21464;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation techniques have shown significant progress in recent years, but their robustness to real-world perturbations and data samples not seen during training remains a challenge, particularly in safety-critical applications. In this paper, we propose a novel approach to improve the robustness of semantic segmentation techniques by leveraging the synergy between label-to-image generators and image-to-label segmentation models. Specifically, we design and train Robusta, a novel robust conditional generative adversarial network to generate realistic and plausible perturbed or outlier images that can be used to train reliable segmentation models. We conduct in-depth studies of the proposed generative model, assess the performance and robustness of the downstream segmentation network, and demonstrate that our approach can significantly enhance the robustness of semantic segmentation techniques in the face of real-world perturbations, distribution shifts, and out-of-distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01050</link><description>&lt;p&gt;
&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#39118;&#38505;&#35780;&#20272;&#30340;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness. (arXiv:2308.01050v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#26377;&#28508;&#21147;&#25552;&#20379;&#35832;&#22810;&#31038;&#20250;&#25928;&#30410;&#65292;&#22914;&#20943;&#23569;&#36947;&#36335;&#20107;&#25925;&#21644;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21382;&#21490;&#25968;&#25454;&#21644;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#37327;&#21270;AVs&#30340;&#39118;&#38505;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;AVs&#22312;&#21508;&#31181;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#65288;ODDs&#65289;&#20013;&#34892;&#20026;&#30340;&#39118;&#38505;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#23545;&#8220;&#19981;&#33391;&#8221;&#36947;&#36335;&#29992;&#25143;&#36827;&#34892;&#21453;&#20107;&#23454;&#27169;&#25311;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#21487;&#33021;&#23548;&#33268;&#30896;&#25758;&#30340;&#26368;&#23567;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#37327;&#12290;&#35813;&#27010;&#24565;&#26377;&#21161;&#20110;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#35780;&#20272;AVs&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;AV&#30340;&#34892;&#20026;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20173;&#28982;&#36866;&#29992;&#20110;&#26368;&#22351;&#21644;&#26368;&#20339;&#24773;&#20917;&#20998;&#26512;&#65292;&#20351;&#35813;&#26041;&#27861;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#20063;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.00629</link><description>&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems - &#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20248;&#21270;&#20915;&#31574;&#31995;&#32479;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26799;&#24230;&#26041;&#27861;&#65292;&#38656;&#35201;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21453;&#39304;&#31232;&#32570;&#25110;&#32773;&#26080;&#20449;&#24687;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26080;&#23548;&#25968;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#26799;&#24230;&#21453;&#39304;&#36136;&#37327;&#30340;&#20381;&#36182;&#65292;&#20294;&#22312;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#24448;&#24448;&#38590;&#20197;&#25193;&#23637;&#12290;&#22914;&#26524;&#31995;&#32479;&#38656;&#35201;&#22810;&#20010;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#26469;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#21152;&#21095;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#24230;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#35282;&#33394;&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#39640;&#25928;&#22320;&#20248;&#21270;&#30001;&#22823;&#37327;&#21442;&#25968;&#21442;&#25968;&#21270;&#30340;&#22810;&#23618;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;(HA-GP-UCB)&#22312;&#25928;&#26524;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
&lt;/p&gt;</description></item><item><title>&#22312;&#24433;&#20687;&#23548;&#24341;&#30340;&#24494;&#21019;&#21307;&#30103;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;X&#23556;&#32447;&#25237;&#24433;&#36827;&#34892;&#36752;&#23556;&#36879;&#26126;&#29289;&#20307;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20248;&#21270;&#35270;&#22270;&#21512;&#25104;&#22312;&#23436;&#25104;&#27492;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.00214</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#35843;&#25972;&#23618;&#26512;&#25104;&#20687;&#65288;NeTT&#65289;&#21644;&#25513;&#34109;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;mNeRF&#65289;&#30340;&#31283;&#20581;&#21333;&#35270;&#38181;&#24418;X&#23556;&#32447;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF). (arXiv:2308.00214v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00214
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24433;&#20687;&#23548;&#24341;&#30340;&#24494;&#21019;&#21307;&#30103;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;X&#23556;&#32447;&#25237;&#24433;&#36827;&#34892;&#36752;&#23556;&#36879;&#26126;&#29289;&#20307;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20248;&#21270;&#35270;&#22270;&#21512;&#25104;&#22312;&#23436;&#25104;&#27492;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24433;&#20687;&#23548;&#24341;&#30340;&#24494;&#21019;&#21307;&#30103;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#20219;&#21153;&#21487;&#20197;&#30475;&#20316;&#26159;&#23039;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#21033;&#29992;X&#23556;&#32447;&#25237;&#24433;&#26469;&#36798;&#21040;3D&#31354;&#38388;&#20013;&#30340;&#30446;&#26631;&#12290;&#36817;&#26399;&#22312;&#21487;&#24494;&#20998;&#28210;&#26579;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#20351;&#24471;RGB&#30456;&#26426;&#35270;&#22270;&#21512;&#25104;&#21644;&#23039;&#24577;&#20272;&#35745;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;X&#23556;&#32447;&#25237;&#24433;&#36827;&#34892;&#36752;&#23556;&#36879;&#26126;&#29289;&#20307;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20248;&#21270;&#35270;&#22270;&#21512;&#25104;&#22312;&#23436;&#25104;&#27492;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#39318;&#20808;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65288;DiffDRR&#65289;&#65292;&#33021;&#22815;&#22312;TensorFlow&#20013;&#39640;&#25928;&#35745;&#31639;&#25968;&#23383;&#37325;&#24314;&#25918;&#23556;&#22270;&#20687;&#65288;DRRs&#65289;&#24182;&#21033;&#29992;&#33258;&#21160;&#24494;&#20998;&#12290;&#32467;&#21512;&#32463;&#20856;&#30340;CBCT&#37325;&#24314;&#31639;&#27861;&#65292;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#23039;&#24577;&#20272;&#35745;&#65292;&#20351;&#29992;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26469;&#37327;&#21270;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#23039;&#24577;&#21512;&#25104;&#30340;DRR&#19982;&#30446;&#26631;&#22788;&#30495;&#23454;&#36879;&#35270;&#22270;&#20687;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many tasks performed in image-guided, mini-invasive, medical procedures can be cast as pose estimation problems, where an X-ray projection is utilized to reach a target in 3D space. Recent advances in the differentiable rendering of optically reflective materials have enabled state-of-the-art performance in RGB camera view synthesis and pose estimation. Expanding on these prior works, we introduce new methods for pose estimation of radiolucent objects using X-ray projections, and we demonstrate the critical role of optimal view synthesis in performing this task. We first develop an algorithm (DiffDRR) that efficiently computes Digitally Reconstructed Radiographs (DRRs) and leverages automatic differentiation within TensorFlow. In conjunction with classic CBCT reconstruction algorithms, we perform pose estimation by gradient descent using a loss function that quantifies the similarity of the DRR synthesized from a randomly initialized pose and the true fluoroscopic image at the target p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36828;&#31243;&#29983;&#29289;&#24863;&#24212;&#25216;&#26415;rPPG&#30340;&#20844;&#24320;&#28304;&#22522;&#20934;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20844;&#24179;&#35780;&#20272;rPPG&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#24182;&#35299;&#20915;&#19982;&#30382;&#32932;&#39068;&#33394;&#12289;&#30456;&#26426;&#29305;&#24615;&#21644;&#29615;&#22659;&#20809;&#31561;&#22240;&#32032;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.12644</link><description>&lt;p&gt;
&#36828;&#31243;&#29983;&#29289;&#24863;&#24212;&#65306;&#20844;&#24320;&#28304;&#22522;&#20934;&#26694;&#26550;&#29992;&#20110;&#20844;&#24179;&#35780;&#20272;rPPG
&lt;/p&gt;
&lt;p&gt;
Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG. (arXiv:2307.12644v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36828;&#31243;&#29983;&#29289;&#24863;&#24212;&#25216;&#26415;rPPG&#30340;&#20844;&#24320;&#28304;&#22522;&#20934;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20844;&#24179;&#35780;&#20272;rPPG&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#24182;&#35299;&#20915;&#19982;&#30382;&#32932;&#39068;&#33394;&#12289;&#30456;&#26426;&#29305;&#24615;&#21644;&#29615;&#22659;&#20809;&#31561;&#22240;&#32032;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
rPPG&#65288;&#36828;&#31243;&#20809;&#30005;&#23481;&#31215;&#33033;&#25615;&#22270;&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#25668;&#20687;&#22836;&#25429;&#25417;&#21040;&#30340;&#34880;&#32418;&#34507;&#30333;&#30340;&#20809;&#21560;&#25910;&#29305;&#24615;&#26469;&#27979;&#37327;&#21644;&#20998;&#26512;BVP&#65288;&#34880;&#23481;&#37327;&#33033;&#25615;&#65289;&#30340;&#25216;&#26415;&#12290;&#20998;&#26512;&#25152;&#27979;&#37327;&#30340;BVP&#21487;&#20197;&#24471;&#20986;&#21508;&#31181;&#29983;&#29702;&#20449;&#21495;&#65292;&#22914;&#24515;&#29575;&#12289;&#21387;&#21147;&#27700;&#24179;&#21644;&#34880;&#21387;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#22914;&#36828;&#31243;&#21307;&#30103;&#12289;&#36828;&#31243;&#24739;&#32773;&#30417;&#25252;&#21644;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#26089;&#26399;&#39044;&#27979;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#21162;&#21147;&#21644;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#19982;&#30382;&#32932;&#39068;&#33394;&#12289;&#30456;&#26426;&#29305;&#24615;&#12289;&#29615;&#22659;&#20809;&#21644;&#20854;&#20182;&#22122;&#22768;&#21644;&#20266;&#36857;&#26469;&#28304;&#26377;&#20851;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#36843;&#20999;&#38656;&#35201;&#20844;&#27491;&#21487;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
rPPG (Remote photoplethysmography) is a technology that measures and analyzes BVP (Blood Volume Pulse) by using the light absorption characteristics of hemoglobin captured through a camera. Analyzing the measured BVP can derive various physiological signals such as heart rate, stress level, and blood pressure, which can be applied to various applications such as telemedicine, remote patient monitoring, and early prediction of cardiovascular disease. rPPG is rapidly evolving and attracting great attention from both academia and industry by providing great usability and convenience as it can measure biosignals using a camera-equipped device without medical or wearable devices. Despite extensive efforts and advances in this field, serious challenges remain, including issues related to skin color, camera characteristics, ambient lighting, and other sources of noise and artifacts, which degrade accuracy performance. We argue that fair and evaluable benchmarking is urgently required to overc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24341;&#20837;&#20102;&#19977;&#37327;&#23376;&#20301;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#20132;&#20114;&#23618;&#30340;&#37327;&#23376;&#21367;&#31215;&#32593;&#32476;&#65292;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#32416;&#32544;&#33021;&#21147;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#21644;&#19968;&#32500;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.11792</link><description>&lt;p&gt;
&#20855;&#26377;&#20132;&#20114;&#23618;&#30340;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32463;&#20856;&#25968;&#25454;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data. (arXiv:2307.11792v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24341;&#20837;&#20102;&#19977;&#37327;&#23376;&#20301;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#20132;&#20114;&#23618;&#30340;&#37327;&#23376;&#21367;&#31215;&#32593;&#32476;&#65292;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#32416;&#32544;&#33021;&#21147;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#21644;&#19968;&#32500;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#37327;&#23376;&#35745;&#31639;&#26426;&#20855;&#26377;&#24322;&#24120;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#65292;&#20960;&#20046;&#27809;&#26377;&#38169;&#35823;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#25215;&#35834;&#20043;&#19979;&#65292;&#23545;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#22810;&#37327;&#23376;&#20301;&#30456;&#20114;&#20316;&#29992;&#30340;&#24433;&#21709;&#36827;&#34892;&#24191;&#27867;&#30740;&#31350;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24341;&#20837;&#20102;&#19977;&#37327;&#23376;&#20301;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#20132;&#20114;&#23618;&#30340;&#37327;&#23376;&#21367;&#31215;&#32593;&#32476;&#65292;&#22686;&#21152;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#32416;&#32544;&#33021;&#21147;&#65292;&#29992;&#20110;&#23545;&#22270;&#20687;&#21644;&#19968;&#32500;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;MNIST&#12289;Fashion MNIST&#21644;Iris&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#29992;&#20110;&#36827;&#34892;&#20108;&#20803;&#21644;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#24182;&#21457;&#29616;&#36229;&#36234;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning (QML) has come into the limelight due to the exceptional computational abilities of quantum computers. With the promises of near error-free quantum computers in the not-so-distant future, it is important that the effect of multi-qubit interactions on quantum neural networks is studied extensively. This paper introduces a Quantum Convolutional Network with novel Interaction layers exploiting three-qubit interactions increasing the network's expressibility and entangling capability, for classifying both image and one-dimensional data. The proposed approach is tested on three publicly available datasets namely MNIST, Fashion MNIST, and Iris datasets, to perform binary and multiclass classifications and is found to supersede the performance of the existing state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#20197;&#21450;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10352</link><description>&lt;p&gt;
&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#20197;&#21450;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#21106;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#24050;&#25104;&#20026;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#30340;Wasserstein&#36317;&#31163;&#30340;&#19968;&#31181;&#27969;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#24191;&#27867;&#24212;&#29992;&#21253;&#25324;&#22270;&#20687;&#22788;&#29702;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#24314;&#27169;&#65292;&#24120;&#24120;&#38656;&#35201;&#20248;&#21270;&#19968;&#20123;&#21442;&#25968;&#20197;&#26368;&#23567;&#21270;SW&#65292;&#35813;&#21442;&#25968;&#20805;&#24403;&#31163;&#25955;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;&#22240;&#20026;&#20855;&#26377;&#23494;&#24230;&#30340;&#27979;&#24230;&#22312;&#25968;&#20540;&#19978;&#26159;&#26080;&#27861;&#23454;&#29616;&#30340;&#65289;&#12290;&#25152;&#26377;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#37117;&#23384;&#22312;&#30456;&#21516;&#30340;&#23376;&#38382;&#39064;&#65292;&#21363;&#26368;&#23567;&#21270;&#20999;&#21106;Wasserstein&#33021;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;$\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$&#30340;&#23646;&#24615;&#65292;&#21363;&#20004;&#20010;&#20855;&#26377;&#19982;&#19968;&#20010;&#27979;&#24230;&#30340;&#25903;&#25745;&#30456;&#21516;&#25968;&#37327;&#30340;&#31163;&#25955;&#22343;&#21248;&#27979;&#24230;&#20043;&#38388;&#30340;SW&#36317;&#31163;&#20316;&#20026;&#25903;&#25745;$Y \in \mathbb{R}^{n \times d}$&#20989;&#25968;&#30340;&#33021;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#33021;&#37327;&#30340;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#65292;&#20197;&#21450;&#20854;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;$\mathcal{E}_p$&#65288;&#20351;&#29992;SW&#20013;&#30340;&#26399;&#26395;&#20272;&#35745;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#26469;&#24341;&#23548;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.07944</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#26631;&#31614;&#26469;&#37325;&#26032;&#23457;&#35270;&#39046;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#26469;&#24341;&#23548;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#19982;&#20266;&#26631;&#31614;&#25216;&#26415;&#30340;&#36741;&#21161;&#24050;&#32463;&#25104;&#20026;&#39046;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#65292;&#21407;&#22240;&#26159;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#20302;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#20849;&#23384;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#21516;&#26102;&#23398;&#20064;&#26816;&#27979;&#25152;&#26377;&#31867;&#21035;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#24341;&#23548;&#19981;&#21516;&#20998;&#24067;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#29615;&#22659;&#24046;&#24322;&#65288;&#20363;&#22914;&#65292;&#20809;&#26463;&#25968;&#37327;&#65289;&#24102;&#26469;&#30340;&#24178;&#25200;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#22495;&#26816;&#26597;&#65288;CDE&#65289;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#23454;&#20363;&#22797;&#21046;&#31896;&#36148;&#21040;&#28304;&#29615;&#22659;&#20013;&#24182;&#27979;&#37327;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#20266;&#26631;&#31614;&#30340;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#21644;&#32531;&#35299;&#29289;&#20307;&#30340;&#36716;&#31227;&#65288;&#20363;&#22914;&#65292;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (DA) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing DA methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel ReDB framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination (CDE) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#22240;&#34920;&#36798;&#21644;&#30456;&#20851;&#24615;&#31526;&#21495;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22522;&#22240;&#20043;&#38388;&#30340;&#31526;&#21495;&#19981;&#30830;&#23450;&#20849;&#34920;&#36798;&#12290;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#31526;&#21495;&#19981;&#30830;&#23450;&#36129;&#29486;&#30340;&#27010;&#29575;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#37327;&#21270;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20013;&#21508;&#31181;&#36830;&#25509;&#30340;&#32467;&#26500;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#35299;&#37322;&#20854;&#23545;&#32593;&#32476;&#20960;&#20309;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.07738</link><description>&lt;p&gt;
&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20013;&#30340;&#36127;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Negative probabilities in Gene Regulatory Networks. (arXiv:2307.07738v1 [q-bio.MN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07738
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#22240;&#34920;&#36798;&#21644;&#30456;&#20851;&#24615;&#31526;&#21495;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22522;&#22240;&#20043;&#38388;&#30340;&#31526;&#21495;&#19981;&#30830;&#23450;&#20849;&#34920;&#36798;&#12290;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#31526;&#21495;&#19981;&#30830;&#23450;&#36129;&#29486;&#30340;&#27010;&#29575;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#37327;&#21270;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20013;&#21508;&#31181;&#36830;&#25509;&#30340;&#32467;&#26500;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#35299;&#37322;&#20854;&#23545;&#32593;&#32476;&#20960;&#20309;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24050;&#30693;&#34920;&#36798;&#21644;&#32473;&#23450;&#30456;&#20851;&#24615;&#31526;&#21495;&#26469;&#35782;&#21035;&#22522;&#22240;&#20043;&#38388;&#23384;&#22312;&#30340;&#31526;&#21495;&#19981;&#30830;&#23450;&#20849;&#34920;&#36798;&#30340;&#33258;&#28982;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#22522;&#22240;&#20043;&#38388;&#30340;&#20146;&#21644;&#20851;&#31995;&#20449;&#24687;&#65288;&#21363;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20013;&#30340;&#36830;&#25509;&#24615;&#65289;&#21644;&#23427;&#20204;&#20419;&#36827;/&#25233;&#21046;&#20849;&#34920;&#36798;&#34507;&#30333;&#20135;&#29983;&#30340;&#30693;&#35782;&#65292;&#24182;&#23547;&#27714;&#33021;&#22815;&#35299;&#37322;&#34507;&#30333;&#36136;&#24179;&#34913;&#20998;&#24067;&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#25552;&#35758;&#23558;&#23427;&#20204;&#30340;&#8220;&#20419;&#36827; vs. &#25233;&#21046;&#8221;&#21151;&#33021;&#23553;&#35013;&#22312;&#19968;&#20010;&#31526;&#21495;&#19981;&#30830;&#23450;&#30340;&#27010;&#29575;&#36716;&#31227;&#30697;&#38453;&#20013;&#65292;&#35813;&#30697;&#38453;&#30340;&#34892;&#21644;&#20026;&#19968;&#65292;&#20294;&#26159;&#38500;&#27492;&#20043;&#22806;&#31526;&#21495;&#19981;&#30830;&#23450;&#12290;&#26500;&#24314;&#20855;&#26377;&#31526;&#21495;&#19981;&#30830;&#23450;&#36129;&#29486;&#30340;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#30340;&#34920;&#31034;&#30340;&#30446;&#30340;&#26159;&#37327;&#21270;&#21508;&#31181;&#36830;&#25509;&#30340;&#32467;&#26500;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#35299;&#37322;&#36825;&#20123;&#36830;&#25509;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31361;&#26174;&#35843;&#25511;&#26426;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a natural framework to identify sign-indefinite co-expressions between genes based on the known expressions and given the sign of their respective correlations. Specifically, given information concerning the affinity among genes (i.e., connectivity in the gene regulatory network) and knowledge whether they promote/inhibit co-expression of the respective protein production, we seek rates that may explain the observed stationary distributions at the level of proteins. We propose to encapsulate their ``promotion vs.\ inhibition'' functionality in a sign-indefinite probability transition matrix--a matrix whose row-sums equal to one, but is otherwise sign indefinite. The purpose of constructing such a representation for the interaction network with sign-indefinite contributions in protein regulation, is to quantify the structure and significance of various links, and to explain how these may affect the geometry of the network, highlighting the significance of the regulatory fun
&lt;/p&gt;</description></item><item><title>PC-Droid&#26159;&#19968;&#20010;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#26032;&#30340;&#25193;&#25955;&#20844;&#24335;&#21644;&#30740;&#31350;&#26356;&#36817;&#26399;&#30340;&#31215;&#20998;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#21943;&#27880;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#19981;&#20165;&#33021;&#25552;&#20379;&#26356;&#24555;&#30340;&#29983;&#25104;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#37117;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06836</link><description>&lt;p&gt;
PC-Droid: &#26356;&#24555;&#30340;&#25193;&#25955;&#36895;&#24230;&#21644;&#25913;&#36827;&#30340;&#31890;&#23376;&#20113;&#29983;&#25104;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
PC-Droid: Faster diffusion and improved quality for particle cloud generation. (arXiv:2307.06836v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06836
&lt;/p&gt;
&lt;p&gt;
PC-Droid&#26159;&#19968;&#20010;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#26032;&#30340;&#25193;&#25955;&#20844;&#24335;&#21644;&#30740;&#31350;&#26356;&#36817;&#26399;&#30340;&#31215;&#20998;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#21943;&#27880;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#19981;&#20165;&#33021;&#25552;&#20379;&#26356;&#24555;&#30340;&#29983;&#25104;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#37117;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;PC-JeDi&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PC-Droid&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#24133;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#21943;&#27880;&#31890;&#23376;&#20113;&#12290;&#36890;&#36807;&#21033;&#29992;&#26032;&#30340;&#25193;&#25955;&#20844;&#24335;&#12289;&#30740;&#31350;&#26356;&#36817;&#26399;&#30340;&#31215;&#20998;&#27714;&#35299;&#22120;&#65292;&#24182;&#21516;&#26102;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#21943;&#27880;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20004;&#31181;&#20307;&#31995;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#33976;&#39311;&#30340;&#28508;&#21147;&#26469;&#30740;&#31350;&#29983;&#25104;&#36895;&#24230;&#21644;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26356;&#24555;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#36229;&#36234;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29983;&#25104;&#26102;&#38388;&#27604;PC-JeDi&#24555;&#19978;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the success of PC-JeDi we introduce PC-Droid, a substantially improved diffusion model for the generation of jet particle clouds. By leveraging a new diffusion formulation, studying more recent integration solvers, and training on all jet types simultaneously, we are able to achieve state-of-the-art performance for all types of jets across all evaluation metrics. We study the trade-off between generation speed and quality by comparing two attention based architectures, as well as the potential of consistency distillation to reduce the number of diffusion steps. Both the faster architecture and consistency models demonstrate performance surpassing many competing models, with generation time up to two orders of magnitude faster than PC-JeDi.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20851;&#31995;&#30340;&#23376;&#22270;&#23884;&#20837;&#19982;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;RaSECo&#65292;&#29992;&#20110;&#39044;&#27979;&#22810;&#20851;&#31995;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#19981;&#21516;&#30340;&#33647;&#29289;&#22270;&#21644;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#65292;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#20110;&#26032;&#33647;&#29289;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01507</link><description>&lt;p&gt;
&#20851;&#27880;&#20851;&#31995;&#30340;&#23376;&#22270;&#23884;&#20837;&#19982;&#23545;&#27604;&#23398;&#20064;&#22312;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Relation-aware subgraph embedding with co-contrastive learning for drug-drug interaction prediction. (arXiv:2307.01507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20851;&#31995;&#30340;&#23376;&#22270;&#23884;&#20837;&#19982;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;RaSECo&#65292;&#29992;&#20110;&#39044;&#27979;&#22810;&#20851;&#31995;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#19981;&#21516;&#30340;&#33647;&#29289;&#22270;&#21644;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#65292;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#20110;&#26032;&#33647;&#29289;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#27880;&#20851;&#31995;&#30340;&#23376;&#22270;&#23884;&#20837;&#23545;&#20110;&#39044;&#27979;&#22810;&#20851;&#31995;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#38750;&#24120;&#26377;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#22823;&#37096;&#20998;&#37117;&#23616;&#38480;&#20110;&#23398;&#20064;&#29616;&#26377;&#33647;&#29289;&#30340;&#23376;&#22270;&#23884;&#20837;&#65292;&#23548;&#33268;&#22312;&#28041;&#21450;&#26032;&#33647;&#29289;&#30340;&#27979;&#35797;DDIs&#20013;&#20986;&#29616;&#20005;&#37325;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#27880;&#20851;&#31995;&#30340;&#23376;&#22270;&#23884;&#20837;&#19982;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#22411;DDI&#39044;&#27979;&#26041;&#27861;&#65292;&#21363;RaSECo&#12290;RaSECo&#26500;&#24314;&#20102;&#20004;&#20010;&#24322;&#26500;&#33647;&#29289;&#22270;&#65306;&#22810;&#20851;&#31995;DDI&#22270;&#21644;&#22522;&#20110;&#22810;&#23646;&#24615;&#30340;&#33647;&#29289;&#30456;&#20284;&#24230;&#65288;DDS&#65289;&#22270;&#12290;&#36825;&#20004;&#20010;&#22270;&#20998;&#21035;&#29992;&#20110;&#23398;&#20064;&#21644;&#20256;&#25773;&#33647;&#29289;&#30340;&#23376;&#22270;&#23884;&#20837;&#65292;&#20174;&#32780;&#30830;&#20445;&#25152;&#26377;&#33647;&#29289;&#65292;&#21253;&#25324;&#26032;&#33647;&#29289;&#65292;&#33021;&#22815;&#32858;&#21512;&#26377;&#25928;&#30340;&#23376;&#22270;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20132;&#21449;&#35270;&#22270;&#23545;&#27604;&#26426;&#21046;&#26469;&#22686;&#24378;&#33647;&#29289;&#23545;&#65288;DP&#65289;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation-aware subgraph embedding is promising for predicting multi-relational drug-drug interactions (DDIs). Typically, most existing methods begin by constructing a multi-relational DDI graph and then learning relation-aware subgraph embeddings (RaSEs) of drugs from the DDI graph. However, most existing approaches are usually limited in learning RaSEs of new drugs, leading to serious over-fitting when the test DDIs involve such drugs. To alleviate this issue, We propose a novel DDI prediction method based on relation-aware subgraph embedding with co-contrastive learning, RaSECo. RaSECo constructs two heterogeneous drug graphs: a multi-relational DDI graph and a multi-attributes-based drug-drug similarity (DDS) graph. The two graphs are used respectively for learning and propagating the RaSEs of drugs, thereby ensuring that all drugs, including new ones, can aggregate effective RaSEs. Additionally, we employ a cross-view contrastive mechanism to enhance drug-pair (DP) embedding. RaSEC
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#20197;&#31616;&#21270;&#36879;&#38236;&#31867;&#26143;&#20307;&#22312;&#22810;&#27874;&#27573;&#22270;&#20687;&#20013;&#30340;&#35782;&#21035;&#12290;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#24456;&#38590;&#27867;&#21270;&#65292;&#24182;&#19988;&#23384;&#22312;&#22823;&#37327;&#34394;&#20551;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01090</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#32593;&#32476;&#22312;&#22810;&#27874;&#27573;&#22270;&#20687;&#20013;&#31616;&#21270;&#36879;&#38236;&#31867;&#26143;&#20307;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Streamlined Lensed Quasar Identification in Multiband Images via Ensemble Networks. (arXiv:2307.01090v2 [astro-ph.GA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01090
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#20197;&#31616;&#21270;&#36879;&#38236;&#31867;&#26143;&#20307;&#22312;&#22810;&#27874;&#27573;&#22270;&#20687;&#20013;&#30340;&#35782;&#21035;&#12290;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#24456;&#38590;&#27867;&#21270;&#65292;&#24182;&#19988;&#23384;&#22312;&#22823;&#37327;&#34394;&#20551;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#21382;&#24378;&#24341;&#21147;&#36879;&#38236;&#25928;&#24212;&#30340;&#31867;&#26143;&#20307;&#23545;&#23431;&#23449;&#33192;&#32960;&#36895;&#29575;&#12289;&#21069;&#26223;&#20559;&#25240;&#22120;&#20869;&#30340;&#26263;&#29289;&#36136;&#20998;&#24067;&#20197;&#21450;&#31867;&#26143;&#20307;&#23487;&#20027;&#26143;&#31995;&#31561;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#36879;&#38236;&#26143;&#20307;&#30340;&#25968;&#37327;&#36807;&#22810;&#65292;&#23558;&#23427;&#20204;&#22312;&#22825;&#25991;&#22270;&#20687;&#20013;&#20934;&#30830;&#35782;&#21035;&#20986;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#20808;&#36827;&#30340;&#21367;&#31215;&#32593;&#32476;&#65288;&#22914;ResNet&#12289;Inception&#12289;NASNet&#12289;MobileNet&#12289;EfficientNet&#21644;RegNet&#65289;&#19982;&#22522;&#20110;Hyper Suprime-Cam&#65288;HSC&#65289;&#22810;&#27874;&#27573;&#22270;&#20687;&#30340;&#29616;&#23454;&#26143;&#31995;-&#31867;&#26143;&#20307;&#36879;&#38236;&#27169;&#25311;&#35757;&#32451;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#65288;ViTs&#65289;&#36827;&#34892;&#38598;&#25104;&#12290;&#34429;&#28982;&#21333;&#20010;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#36798;&#21040;97.3%&#20197;&#19978;&#65292;&#20013;&#20301;&#35823;&#25253;&#29575;&#20026;3.6%&#65292;&#20294;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#24456;&#38590;&#27867;&#21270;&#65292;&#27599;&#20010;&#20998;&#31867;&#22120;&#37117;&#20250;&#36873;&#25321;&#20986;&#22823;&#37327;&#34394;&#20551;&#28304;&#12290; &#19968;&#20010;&#37325;&#35201;&#30340;&#21407;&#22240;&#26159;&#36879;&#38236;&#31867;&#26143;&#20307;&#30340;&#25968;&#37327;&#27604;&#38750;&#36879;&#38236;&#31867;&#26143;&#20307;&#23569;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quasars experiencing strong lensing offer unique viewpoints on subjects related to the cosmic expansion rate, the dark matter profile within the foreground deflectors, and the quasar host galaxies. Unfortunately, identifying them in astronomical images is challenging since they are overwhelmed by the abundance of non-lenses. To address this, we have developed a novel approach by ensembling cutting-edge convolutional networks (CNNs) -- for instance, ResNet, Inception, NASNet, MobileNet, EfficientNet, and RegNet -- along with vision transformers (ViTs) trained on realistic galaxy-quasar lens simulations based on the Hyper Suprime-Cam (HSC) multiband images. While the individual model exhibits remarkable performance when evaluated against the test dataset, achieving an area under the receiver operating characteristic curve of $&gt;$97.3% and a median false positive rate of 3.6%, it struggles to generalize in real data, indicated by numerous spurious sources picked by each classifier. A signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#35821;&#35328;&#25509;&#21475;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#22270;&#20687;&#21644;&#30446;&#26631;&#20449;&#24687;&#30340;&#31574;&#30053;&#20197;&#21450;&#23569;&#37327;&#30340;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#31283;&#20581;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00117</link><description>&lt;p&gt;
&#25351;&#23548;&#30446;&#26631;&#34920;&#24449;&#65306;&#19968;&#31181;&#21322;&#30417;&#30563;&#35821;&#35328;&#25509;&#21475;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Goal Representations for Instruction Following: A Semi-Supervised Language Interface to Control. (arXiv:2307.00117v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#35821;&#35328;&#25509;&#21475;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#22270;&#20687;&#21644;&#30446;&#26631;&#20449;&#24687;&#30340;&#31574;&#30053;&#20197;&#21450;&#23569;&#37327;&#30340;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#31283;&#20581;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#25353;&#29031;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#34892;&#21160;&#65292;&#20363;&#22914;&#8220;&#23558;&#27611;&#24062;&#25918;&#22312;&#24494;&#27874;&#28809;&#26049;&#36793;&#8221;&#12290;&#20294;&#26159;&#33719;&#21462;&#22823;&#37327;&#24102;&#26377;&#35821;&#35328;&#25351;&#20196;&#26631;&#31614;&#30340;&#26631;&#27880;&#25968;&#25454;&#38750;&#24120;&#22256;&#38590;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#33719;&#21462;&#23545;&#22270;&#20687;&#30446;&#26631;&#20316;&#20986;&#21709;&#24212;&#30340;&#31574;&#30053;&#35201;&#23481;&#26131;&#24471;&#22810;&#65292;&#22240;&#20026;&#20219;&#20309;&#33258;&#20027;&#23581;&#35797;&#25110;&#28436;&#31034;&#37117;&#21487;&#20197;&#22312;&#20107;&#21518;&#29992;&#26368;&#32456;&#29366;&#24577;&#20316;&#20026;&#30446;&#26631;&#36827;&#34892;&#26631;&#35760;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21482;&#21033;&#29992;&#23569;&#37327;&#30340;&#35821;&#35328;&#25968;&#25454;&#65292;&#21033;&#29992;&#32852;&#21512;&#22270;&#20687;&#21644;&#30446;&#26631;&#20449;&#24687;&#30340;&#31574;&#30053;&#26469;&#22788;&#29702;&#35821;&#35328;&#25509;&#21475;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#22312;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#25110;&#32852;&#21512;&#35757;&#32451;&#35821;&#35328;-&#30446;&#26631;-&#26465;&#20214;&#31574;&#30053;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#27809;&#26377;&#26377;&#25928;&#22320;&#25193;&#23637;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;&#32780;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#31181;&#23558;&#35821;&#35328;&#19982;&#30446;&#26631;&#22270;&#20687;&#23545;&#40784;&#30340;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#31283;&#20581;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our goal is for robots to follow natural language instructions like "put the towel next to the microwave." But getting large amounts of labeled data, i.e. data that contains demonstrations of tasks labeled with the language instruction, is prohibitive. In contrast, obtaining policies that respond to image goals is much easier, because any autonomous trial or demonstration can be labeled in hindsight with its final state as the goal. In this work, we contribute a method that taps into joint image- and goal- conditioned policies with language using only a small amount of language data. Prior work has made progress on this using vision-language models or by jointly training language-goal-conditioned policies, but so far neither method has scaled effectively to real-world robot tasks without significant human annotation. Our method achieves robust performance in the real world by learning an embedding from the labeled data that aligns language not to the goal image, but rather to the desir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#35757;&#32451;&#20013;&#32500;&#24230;&#26080;&#20851;&#30340;&#22256;&#38590;&#36127;&#26679;&#26412;&#28151;&#21512;&#26041;&#27861;&#65288;DINS&#65289;&#65292;&#36890;&#36807;&#23545;&#37319;&#26679;&#21306;&#22495;&#30340;&#26032;&#35270;&#35282;&#36827;&#34892;&#37325;&#26032;&#23457;&#35270;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DINS&#20248;&#20110;&#20854;&#20182;&#36127;&#37319;&#26679;&#26041;&#27861;&#65292;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15905</link><description>&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#20013;&#32500;&#24230;&#26080;&#20851;&#30340;&#22256;&#38590;&#36127;&#26679;&#26412;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dimension Independent Mixup for Hard Negative Sample in Collaborative Filtering. (arXiv:2306.15905v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#35757;&#32451;&#20013;&#32500;&#24230;&#26080;&#20851;&#30340;&#22256;&#38590;&#36127;&#26679;&#26412;&#28151;&#21512;&#26041;&#27861;&#65288;DINS&#65289;&#65292;&#36890;&#36807;&#23545;&#37319;&#26679;&#21306;&#22495;&#30340;&#26032;&#35270;&#35282;&#36827;&#34892;&#37325;&#26032;&#23457;&#35270;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DINS&#20248;&#20110;&#20854;&#20182;&#36127;&#37319;&#26679;&#26041;&#27861;&#65292;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22522;&#20110;&#36807;&#21435;&#30340;&#20114;&#21160;&#39044;&#27979;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#36127;&#37319;&#26679;&#22312;&#20351;&#29992;&#38544;&#24335;&#21453;&#39304;&#35757;&#32451;&#22522;&#20110;CF&#30340;&#27169;&#22411;&#26102;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#21306;&#22495;&#30340;&#26032;&#35270;&#35282;&#26469;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#30446;&#21069;&#30340;&#37319;&#26679;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#37319;&#26679;&#25110;&#32447;&#37319;&#26679;&#19978;&#65292;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#26377;&#30456;&#24403;&#22823;&#19968;&#37096;&#20998;&#22256;&#38590;&#37319;&#26679;&#21306;&#22495;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#26080;&#20851;&#30340;&#22256;&#38590;&#36127;&#26679;&#26412;&#28151;&#21512;&#26041;&#27861;&#65288;DINS&#65289;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#35757;&#32451;&#22522;&#20110;CF&#30340;&#27169;&#22411;&#30340;&#21306;&#22495;&#37319;&#26679;&#26041;&#27861;&#12290;DINS&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#22256;&#38590;&#36793;&#30028;&#23450;&#20041;&#12289;&#32500;&#24230;&#26080;&#20851;&#28151;&#21512;&#21644;&#22810;&#36339;&#27744;&#21270;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DINS&#20248;&#20110;&#20854;&#20182;&#36127;&#37319;&#26679;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) is a widely employed technique that predicts user preferences based on past interactions. Negative sampling plays a vital role in training CF-based models with implicit feedback. In this paper, we propose a novel perspective based on the sampling area to revisit existing sampling methods. We point out that current sampling methods mainly focus on Point-wise or Line-wise sampling, lacking flexibility and leaving a significant portion of the hard sampling area un-explored. To address this limitation, we propose Dimension Independent Mixup for Hard Negative Sampling (DINS), which is the first Area-wise sampling method for training CF-based models. DINS comprises three modules: Hard Boundary Definition, Dimension Independent Mixup, and Multi-hop Pooling. Experiments with real-world datasets on both matrix factorization and graph-based models demonstrate that DINS outperforms other negative sampling methods, establishing its effectiveness and superiority. Our wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#23558;&#22270;&#20687;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38544;&#24739;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25351;&#20986;&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#39640;&#32500;&#24615;&#26159;&#23545;&#25239;&#25915;&#20987;&#30340;&#20016;&#23500;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#20026;&#35270;&#35273;&#25915;&#20987;&#32773;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#23545;&#25239;&#30446;&#26631;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13213</link><description>&lt;p&gt;
&#35270;&#35273;&#23545;&#25239;&#26679;&#26412;&#36234;&#29425;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38544;&#24739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Visual Adversarial Examples Jailbreak Large Language Models. (arXiv:2306.13213v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#23558;&#22270;&#20687;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38544;&#24739;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25351;&#20986;&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#39640;&#32500;&#24615;&#26159;&#23545;&#25239;&#25915;&#20987;&#30340;&#20016;&#23500;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#20026;&#35270;&#35273;&#25915;&#20987;&#32773;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#23545;&#25239;&#30446;&#26631;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#22270;&#20687;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#39640;&#24230;&#20851;&#27880;&#12290;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#26222;&#21450;&#65292;&#20363;&#22914;Flamingo&#12289;BLIP-2&#21644;GPT-4&#65292;&#26631;&#24535;&#30528;&#35270;&#35273;&#21644;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#20808;&#36827;&#21457;&#23637;&#30456;&#20114;&#34701;&#21512;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#28041;&#21450;&#30340;&#39118;&#38505;&#20173;&#26410;&#24471;&#21040;&#35814;&#32454;&#30740;&#31350;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#36825;&#19968;&#36235;&#21183;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#65292;&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#39640;&#32500;&#24615;&#22312;&#26412;&#36136;&#19978;&#20351;&#20854;&#25104;&#20026;&#23545;&#25239;&#25915;&#20987;&#30340;&#20016;&#23500;&#39046;&#22495;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#25193;&#22823;&#20102;LLMs&#30340;&#25915;&#20987;&#38754;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;LLMs&#30340;&#24191;&#27867;&#21151;&#33021;&#20063;&#20026;&#35270;&#35273;&#25915;&#20987;&#32773;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#23454;&#29616;&#23545;&#25239;&#30446;&#26631;&#30340;&#21487;&#33021;&#24615;&#65292;&#23558;&#23433;&#20840;&#22833;&#36133;&#30340;&#24433;&#21709;&#25193;&#23637;&#21040;&#20102;&#31616;&#21333;&#30340;&#38169;&#35823;&#20998;&#31867;&#20043;&#22806;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#20123;&#39118;&#38505;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;VLM&#35270;&#35273;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a surge of interest in introducing vision into Large Language Models (LLMs). The proliferation of large Visual Language Models (VLMs), such as Flamingo, BLIP-2, and GPT-4, signifies an exciting convergence of advancements in both visual and language foundation models. Yet, the risks associated with this integrative approach are largely unexamined. In this paper, we shed light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the additional visual input space intrinsically makes it a fertile ground for adversarial attacks. This unavoidably expands the attack surfaces of LLMs. Second, we highlight that the broad functionality of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. To elucidate these risks, we study adversarial examples in the visual input space of a VLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#27169;&#25311;&#24179;&#21488;&#19978;&#23545;&#20004;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20197;&#24320;&#21457;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#12290;&#36890;&#36807;&#23450;&#21046;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#23545;&#20934;&#30830;&#24230;&#12289;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#24230;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.11466</link><description>&lt;p&gt;
&#22312;&#21508;&#31181;&#27169;&#25311;&#39550;&#39542;&#25805;&#20316;&#20013;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20840;&#38754;&#22521;&#35757;&#21644;&#35780;&#20272;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Training and Evaluation on Deep Reinforcement Learning for Automated Driving in Various Simulated Driving Maneuvers. (arXiv:2306.11466v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#27169;&#25311;&#24179;&#21488;&#19978;&#23545;&#20004;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20197;&#24320;&#21457;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#12290;&#36890;&#36807;&#23450;&#21046;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#23545;&#20934;&#30830;&#24230;&#12289;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#24230;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24320;&#21457;&#21644;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#29978;&#33267;&#21361;&#38505;&#30340;&#65292;&#32780;&#27169;&#25311;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39550;&#39542;&#25805;&#20316;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#36890;&#36807;&#23398;&#20064;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#22788;&#29702;&#22797;&#26434;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;&#20219;&#21153;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#24320;&#21457;&#33258;&#21160;&#39550;&#39542;&#65292;&#22312;&#36825;&#26041;&#38754;&#30340;&#20855;&#20307;&#30740;&#31350;&#36824;&#19981;&#22810;&#12290;&#26412;&#30740;&#31350;&#22312;highway-env&#27169;&#25311;&#24179;&#21488;&#19978;&#23454;&#26045;&#12289;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#20004;&#20010;DRL&#31639;&#27861;&#65292;Deep Q-networks&#65288;DQN&#65289;&#21644;Trust Region Policy Optimization&#65288;TRPO&#65289;&#65292;&#20197;&#22521;&#35757;&#33258;&#21160;&#39550;&#39542;&#12290;&#21516;&#26102;&#24320;&#21457;&#20102;&#26377;&#25928;&#19988;&#23450;&#21046;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#24230;&#65288;&#36710;&#36742;&#22312;&#36947;&#36335;&#19978;&#30340;&#34892;&#39542;&#24773;&#20917;&#65289;&#12289;&#25928;&#29575;&#65288;&#36710;&#36742;&#30340;&#34892;&#39542;&#36895;&#24230;&#65289;&#12289;&#23433;&#20840;&#24615;&#65288;&#36710;&#36742;&#36991;&#20813;&#19982;&#38556;&#30861;&#29289;&#30896;&#25758;&#30340;&#21487;&#33021;&#24615;&#65289;&#21644;&#33298;&#36866;&#24230;&#65288;&#36710;&#36742;&#39550;&#39542;&#30340;&#33298;&#36866;&#31243;&#24230;&#65289;&#26469;&#35780;&#20272;&#24050;&#23454;&#26045;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing and testing automated driving models in the real world might be challenging and even dangerous, while simulation can help with this, especially for challenging maneuvers. Deep reinforcement learning (DRL) has the potential to tackle complex decision-making and controlling tasks through learning and interacting with the environment, thus it is suitable for developing automated driving while not being explored in detail yet. This study carried out a comprehensive study by implementing, evaluating, and comparing the two DRL algorithms, Deep Q-networks (DQN) and Trust Region Policy Optimization (TRPO), for training automated driving on the highway-env simulation platform. Effective and customized reward functions were developed and the implemented algorithms were evaluated in terms of onlane accuracy (how well the car drives on the road within the lane), efficiency (how fast the car drives), safety (how likely the car is to crash into obstacles), and comfort (how much the car ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#21442;&#32771;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#29992;&#25143;&#25511;&#21046;&#30340;&#36523;&#20221;&#12290;&#35813;&#26550;&#26500;&#20351;&#29992;&#21435;&#20013;&#24515;&#21270;&#26631;&#35782;&#31526;&#36827;&#34892;&#36523;&#20221;&#39564;&#35777;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#23454;&#29616;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#65292;&#24182;&#33021;&#26681;&#25454;&#38656;&#27714;&#38598;&#25104;&#21508;&#31181;&#39069;&#22806;&#30340;&#20803;&#32032;&#65292;&#26159;&#19968;&#20010;&#36866;&#29992;&#33539;&#22260;&#24191;&#27867;&#30340; BCFL &#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.10841</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#25903;&#25345;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#21442;&#32771;&#26550;&#26500;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Blockchain-Enabled Federated Learning: A Reference Architecture Design, Implementation, and Verification. (arXiv:2306.10841v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#21442;&#32771;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#29992;&#25143;&#25511;&#21046;&#30340;&#36523;&#20221;&#12290;&#35813;&#26550;&#26500;&#20351;&#29992;&#21435;&#20013;&#24515;&#21270;&#26631;&#35782;&#31526;&#36827;&#34892;&#36523;&#20221;&#39564;&#35777;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#23454;&#29616;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#65292;&#24182;&#33021;&#26681;&#25454;&#38656;&#27714;&#38598;&#25104;&#21508;&#31181;&#39069;&#22806;&#30340;&#20803;&#32032;&#65292;&#26159;&#19968;&#20010;&#36866;&#29992;&#33539;&#22260;&#24191;&#27867;&#30340; BCFL &#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;BCFL&#65289;&#21442;&#32771;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#23558;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#30340;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#23562;&#37325;&#25968;&#25454;&#38544;&#31169;&#21644;&#29992;&#25143;&#25511;&#21046;&#30340;&#36523;&#20221;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#25112;&#30053;&#24615;&#22320;&#37319;&#29992;&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#26631;&#35782;&#31526;&#65288;DID&#65289;&#30340;&#36523;&#20221;&#39564;&#35777;&#31995;&#32479;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#20351;&#29992;&#20854;&#33258;&#20027; DID &#23433;&#20840;&#22320;&#35748;&#35777;&#24182;&#33719;&#24471;&#23545;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#36825;&#20123;&#20449;&#24687;&#34987;&#35760;&#24405;&#22312;&#21306;&#22359;&#38142;&#19978;&#12290;&#36890;&#36807;&#25191;&#34892;&#26234;&#33021;&#21512;&#32422;&#26469;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#26159;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340; BCFL &#21442;&#32771;&#26550;&#26500;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#38656;&#27714;&#21644;&#29992;&#20363;&#38598;&#25104;&#21508;&#31181;&#39069;&#22806;&#30340;&#20803;&#32032;&#65292;&#20351;&#20854;&#25104;&#20026;&#24191;&#27867;&#36866;&#29992;&#30340; BCFL &#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative reference architecture for blockchain-enabled federated learning (BCFL), a state-of-the-art approach that amalgamates the strengths of federated learning and blockchain technology. This results in a decentralized, collaborative machine learning system that respects data privacy and user-controlled identity. Our architecture strategically employs a decentralized identifier (DID)-based authentication system, allowing participants to authenticate and then gain access to the federated learning platform securely using their self-sovereign DIDs, which are recorded on the blockchain. Ensuring robust security and efficient decentralization through the execution of smart contracts is a key aspect of our approach. Moreover, our BCFL reference architecture provides significant extensibility, accommodating the integration of various additional elements, as per specific requirements and use cases, thereby rendering it an adaptable solution for a wide range of BCFL 
&lt;/p&gt;</description></item><item><title>&#31471;&#21040;&#31471;&#39550;&#39542;&#27169;&#22411;&#23384;&#22312;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#27425;&#35201;&#32452;&#20214;&#30340;&#26356;&#25913;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#25152;&#23384;&#22312;&#30340;&#20004;&#31181;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;TF ++&#65292;&#22312;CARLA&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.07957</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39550;&#39542;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Hidden Biases of End-to-End Driving Models. (arXiv:2306.07957v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07957
&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#39550;&#39542;&#27169;&#22411;&#23384;&#22312;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#27425;&#35201;&#32452;&#20214;&#30340;&#26356;&#25913;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#25152;&#23384;&#22312;&#30340;&#20004;&#31181;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;TF ++&#65292;&#22312;CARLA&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31471;&#21040;&#31471;&#30340;&#39550;&#39542;&#31995;&#32479;&#22312;CARLA&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#20027;&#35201;&#36129;&#29486;&#30340;&#22522;&#30784;&#19978;&#65292;&#36825;&#20123;&#31995;&#32479;&#20063;&#20250;&#24341;&#20837;&#23545;&#27425;&#35201;&#31995;&#32479;&#32452;&#20214;&#30340;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#30340;&#25913;&#36827;&#28304;&#24182;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#37117;&#23384;&#22312;&#20004;&#31181;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#23545;&#20110;&#22312;CARLA&#19978;&#35266;&#23519;&#21040;&#30340;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#65306;(1) &#36890;&#36807;&#23545;&#30446;&#26631;&#28857;&#36319;&#38543;&#30340;&#24378;&#24402;&#32435;&#20559;&#35265;&#26469;&#36827;&#34892;&#27178;&#21521;&#24674;&#22797;&#65292;(2) &#36890;&#36807;&#22810;&#27169;&#24577;&#33322;&#36335;&#28857;&#39044;&#27979;&#30340;&#32437;&#21521;&#24179;&#22343;&#26469;&#20943;&#36895;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#20559;&#35265;&#30340;&#32570;&#28857;&#65292;&#24182;&#30830;&#23450;&#20102;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TF ++&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#22312;Longest6&#21644;LAV&#22522;&#20934;&#27979;&#35797;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#22312;Longest6&#19978;&#27604;&#26368;&#20339;&#21069;&#26399;&#24037;&#20316;&#25552;&#39640;&#20102;14&#20010;&#39550;&#39542;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end driving systems have recently made rapid progress, in particular on CARLA. Independent of their major contribution, they introduce changes to minor system components. Consequently, the source of improvements is unclear. We identify two biases that recur in nearly all state-of-the-art methods and are critical for the observed progress on CARLA: (1) lateral recovery via a strong inductive bias towards target point following, and (2) longitudinal averaging of multimodal waypoint predictions for slowing down. We investigate the drawbacks of these biases and identify principled alternatives. By incorporating our insights, we develop TF++, a simple end-to-end method that ranks first on the Longest6 and LAV benchmarks, gaining 14 driving score over the best prior work on Longest6.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#35748;&#30693;&#38169;&#35823;&#30340;&#29305;&#28857;&#65292;&#32780;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#21017;&#36890;&#36807;&#23398;&#20064;&#36991;&#20813;&#36825;&#31867;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#25506;&#27979;LLMs&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#26032;&#29983;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07622</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#25512;&#29702;&#20559;&#24046;&#8212;&#8212;&#20197;&#21450;&#22312;GPT-4&#20013;&#28040;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4. (arXiv:2306.07622v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#35748;&#30693;&#38169;&#35823;&#30340;&#29305;&#28857;&#65292;&#32780;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#21017;&#36890;&#36807;&#23398;&#20064;&#36991;&#20813;&#36825;&#31867;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#25506;&#27979;LLMs&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#26032;&#29983;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30446;&#21069;&#22788;&#20110;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#20852;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#65288;&#23588;&#20854;&#26159;GPT-3&#65289;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#65292;&#20197;&#21450;&#36981;&#24490;&#36825;&#31181;&#34892;&#20026;&#32780;&#26469;&#30340;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLM&#65292;&#29305;&#21035;&#26159;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#23624;&#26381;&#20110;&#36825;&#20123;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Cognitive Reflection Test&#65288;CRT&#65289;&#21450;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#30452;&#35273;&#20915;&#31574;&#30340;&#35821;&#20041;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#31350;&#20102;&#31867;&#20154;&#30452;&#35273;&#20915;&#31574;&#30340;&#31283;&#23450;&#20542;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#24515;&#29702;&#23398;&#26041;&#27861;&#35843;&#26597;LLM&#26377;&#28508;&#21147;&#25581;&#31034;&#21542;&#21017;&#26410;&#30693;&#30340;&#26032;&#29983;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition -- and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Moreover, we probe how sturdy the inclination for intuitive-like decision-making is. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;WaffleCLIP&#65292;&#19968;&#20010;&#20351;&#29992;&#38543;&#26426;&#23383;&#31526;&#21644;&#21333;&#35789;&#25551;&#36848;&#31526;&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#35273;&#20998;&#31867;&#30340;&#26694;&#26550;&#65292;&#23427;&#21462;&#20195;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#21487;&#20197;&#20316;&#20026;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#25193;&#23637;&#30340;&#26367;&#20195;&#36873;&#25321;&#21644;&#39564;&#35777;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07282</link><description>&lt;p&gt;
&#22312;&#24615;&#33021;&#26041;&#38754;&#25671;&#25670;&#19981;&#23450;&#65306;&#20351;&#29992;&#38543;&#26426;&#21333;&#35789;&#21644;&#24191;&#20041;&#27010;&#24565;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Waffling around for Performance: Visual Classification with Random Words and Broad Concepts. (arXiv:2306.07282v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;WaffleCLIP&#65292;&#19968;&#20010;&#20351;&#29992;&#38543;&#26426;&#23383;&#31526;&#21644;&#21333;&#35789;&#25551;&#36848;&#31526;&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#35273;&#20998;&#31867;&#30340;&#26694;&#26550;&#65292;&#23427;&#21462;&#20195;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#21487;&#20197;&#20316;&#20026;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#25193;&#23637;&#30340;&#26367;&#20195;&#36873;&#25321;&#21644;&#39564;&#35777;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#35832;&#22914;CLIP&#20043;&#31867;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#20998;&#31867;&#24615;&#33021;&#21487;&#20197;&#21463;&#30410;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#22914;GPT-3&#65289;&#30340;&#39069;&#22806;&#35821;&#20041;&#30693;&#35782;&#12290;&#29305;&#21035;&#26159;&#22312;LLM&#29983;&#25104;&#30340;&#31867;&#21035;&#25551;&#36848;&#31526;&#20013;&#36827;&#34892;&#24179;&#22343;&#65292;&#20363;&#22914;&#8220;&#20855;&#26377;&#22278;&#24418;&#24418;&#29366;&#30340;&#21326;&#22827;&#39292;&#8221;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#34892;&#20026;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;WaffleCLIP&#65292;&#36825;&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#35270;&#35273;&#20998;&#31867;&#26694;&#26550;&#65292;&#23427;&#20165;&#29992;&#38543;&#26426;&#23383;&#31526;&#21644;&#21333;&#35789;&#25551;&#36848;&#31526;&#26367;&#25442;LLM&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#12290;&#22312;&#19981;&#26597;&#35810;&#22806;&#37096;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#22823;&#37327;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;&#36825;&#20351;&#24471;WaffleCLIP&#26082;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#20302;&#25104;&#26412;&#26367;&#20195;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#26410;&#26469;&#22522;&#20110;LLM&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#30340;&#19968;&#31181;&#21512;&#29702;&#26816;&#26597;&#12290;&#25105;&#20204;&#23545;&#24341;&#20837;LLM&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#30340;&#38468;&#21152;&#35821;&#20041;&#30340;&#24433;&#21709;&#21644;&#32570;&#38519;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#26524;&#21487;&#29992;&#65292;se&#30340;&#33021;&#21160;&#24615;&#21487;&#20197;&#20316;&#20026;&#35299;&#37322;&#36825;&#20123;&#32570;&#38519;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The visual classification performance of vision-language models such as CLIP has been shown to benefit from additional semantic knowledge from large language models (LLMs) such as GPT-3. In particular, averaging over LLM-generated class descriptors, e.g. "waffle, which has a round shape", can notably improve generalization performance. In this work, we critically study this behavior and propose WaffleCLIP, a framework for zero-shot visual classification which simply replaces LLM-generated descriptors with random character and word descriptors. Without querying external models, we achieve comparable performance gains on a large number of visual classification tasks. This allows WaffleCLIP to both serve as a low-cost alternative, as well as a sanity check for any future LLM-based vision-language model extensions. We conduct an extensive experimental study on the impact and shortcomings of additional semantics introduced with LLM-generated descriptors, and showcase how - if available - se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#24773;&#32490;&#21464;&#21270;&#24182;&#25913;&#36827;&#24773;&#32490;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23402;&#29983;&#32593;&#32476;&#36827;&#34892;&#24230;&#37327;&#23398;&#20064;&#65292;&#25512;&#29702;&#20986;&#24773;&#32490;&#21464;&#21270;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#19982;&#24515;&#24773;&#26631;&#31614;&#19968;&#36215;&#29992;&#20110;&#24515;&#24773;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24773;&#32490;&#21464;&#21270;&#20449;&#24687;&#30340;&#21152;&#20837;&#21487;&#20197;&#25552;&#39640;&#24515;&#24773;&#39044;&#27979;&#25928;&#26524;&#65292;&#24378;&#35843;&#20102;&#24314;&#27169;&#24515;&#24773;-&#24773;&#32490;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#26377;&#25928;&#30340;&#24515;&#24773;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06979</link><description>&lt;p&gt;
&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#24773;&#32490;&#21464;&#21270;&#39044;&#27979;&#26041;&#27861;&#21644;&#25913;&#36827;&#30340;&#24773;&#32490;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Weakly Supervised Approach to Emotion-change Prediction and Improved Mood Inference. (arXiv:2306.06979v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#24773;&#32490;&#21464;&#21270;&#24182;&#25913;&#36827;&#24773;&#32490;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23402;&#29983;&#32593;&#32476;&#36827;&#34892;&#24230;&#37327;&#23398;&#20064;&#65292;&#25512;&#29702;&#20986;&#24773;&#32490;&#21464;&#21270;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#19982;&#24515;&#24773;&#26631;&#31614;&#19968;&#36215;&#29992;&#20110;&#24515;&#24773;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24773;&#32490;&#21464;&#21270;&#20449;&#24687;&#30340;&#21152;&#20837;&#21487;&#20197;&#25552;&#39640;&#24515;&#24773;&#39044;&#27979;&#25928;&#26524;&#65292;&#24378;&#35843;&#20102;&#24314;&#27169;&#24515;&#24773;-&#24773;&#32490;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#26377;&#25928;&#30340;&#24515;&#24773;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#37096;&#20998;&#24773;&#24863;&#35745;&#31639;&#30740;&#31350;&#20851;&#27880;&#20110;&#25512;&#26029;&#24773;&#32490;&#12289;&#30740;&#31350;&#24515;&#24773;&#25110;&#29702;&#35299;&#24515;&#24773;-&#24773;&#32490;&#30456;&#20114;&#20316;&#29992;&#65292;&#20294;&#23545;&#27492;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#36828;&#36828;&#19981;&#22815;&#12290;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#20174;&#39044;&#35757;&#32451;&#30340;&#23402;&#29983;&#32593;&#32476;&#20013;&#25512;&#29702;&#20986;&#24773;&#32490;&#21464;&#21270;&#65288;&#8710;&#65289;&#20449;&#24687;&#26469;&#25512;&#26029;&#24515;&#24773;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#27880;&#37322;&#26631;&#31614;&#65292;&#24182;&#23581;&#35797;&#22312;&#19982;&#24515;&#24773;&#29305;&#24449;&#23545;&#40784;&#30340;&#38271;&#26102;&#35270;&#39057;&#21098;&#36753;&#20013;&#36827;&#34892;&#24515;&#24773;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#24773;&#32490;&#21464;&#21270;&#65288;&#8710;&#65289;&#26631;&#31614;&#19982;&#24515;&#24773;&#26631;&#31614;&#19968;&#36215;&#29992;&#20110;&#24515;&#24773;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20165;&#20351;&#29992;&#24515;&#24773;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#65292;&#20351;&#29992;&#24773;&#32490;&#21464;&#21270;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#21487;&#20197;&#25913;&#21892;&#24515;&#24773;&#39044;&#27979;&#25928;&#26524;&#65292;&#24378;&#35843;&#20102;&#22312;&#26377;&#25928;&#30340;&#24515;&#24773;&#25512;&#29702;&#20013;&#23545;&#24515;&#24773;-&#24773;&#32490;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whilst a majority of affective computing research focuses on inferring emotions, examining mood or understanding the \textit{mood-emotion interplay} has received significantly less attention. Building on prior work, we (a) deduce and incorporate emotion-change ($\Delta$) information for inferring mood, without resorting to annotated labels, and (b) attempt mood prediction for long duration video clips, in alignment with the characterisation of mood. We generate the emotion-change ($\Delta$) labels via metric learning from a pre-trained Siamese Network, and use these in addition to mood labels for mood classification. Experiments evaluating \textit{unimodal} (training only using mood labels) vs \textit{multimodal} (training using mood plus $\Delta$ labels) models show that mood prediction benefits from the incorporation of emotion-change information, emphasising the importance of modelling the mood-emotion interplay for effective mood inference.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;MARL&#31639;&#27861;&#30340;iPLAN&#26041;&#27861;&#65292;&#21487;&#22312;&#39640;&#23494;&#24230;&#19988;&#24322;&#26500;&#20132;&#36890;&#22330;&#26223;&#19979;&#36827;&#34892;&#24847;&#22270;&#24863;&#30693;&#35268;&#21010;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#25512;&#26029;&#38468;&#36817;&#39550;&#39542;&#32773;&#30340;&#24847;&#22270;&#65292;&#24182;&#36890;&#36807;&#34892;&#20026;&#25110;&#30636;&#26102;&#28608;&#21169;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2306.06236</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24322;&#26500;&#20132;&#36890;&#24847;&#22270;&#24863;&#30693;&#35268;&#21010;&#31639;&#27861;iPLAN
&lt;/p&gt;
&lt;p&gt;
iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning. (arXiv:2306.06236v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06236
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;MARL&#31639;&#27861;&#30340;iPLAN&#26041;&#27861;&#65292;&#21487;&#22312;&#39640;&#23494;&#24230;&#19988;&#24322;&#26500;&#20132;&#36890;&#22330;&#26223;&#19979;&#36827;&#34892;&#24847;&#22270;&#24863;&#30693;&#35268;&#21010;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#25512;&#26029;&#38468;&#36817;&#39550;&#39542;&#32773;&#30340;&#24847;&#22270;&#65292;&#24182;&#36890;&#36807;&#34892;&#20026;&#25110;&#30636;&#26102;&#28608;&#21169;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#23494;&#24230;&#21644;&#24322;&#26500;&#20132;&#36890;&#22330;&#26223;&#20013;&#20445;&#38556;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;AVs&#65289;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#38754;&#20020;&#36739;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#25512;&#26029;&#38468;&#36817;&#39550;&#39542;&#32773;&#30340;&#34892;&#20026;&#25110;&#24847;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36712;&#36857;&#21644;&#24847;&#22270;&#39044;&#27979;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#23494;&#24230;&#21644;&#24322;&#26500;&#20132;&#36890;&#22330;&#26223;&#20013;&#36827;&#34892;&#24847;&#22270;&#24863;&#30693;&#35268;&#21010;&#12290;&#25105;&#20204;&#30340;iPLAN&#26041;&#27861;&#20351;&#26234;&#33021;&#20307;&#20165;&#20174;&#20854;&#26412;&#22320;&#35266;&#27979;&#20013;&#25512;&#26029;&#38468;&#36817;&#39550;&#39542;&#32773;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#28608;&#21169;&#22240;&#32032;&#65306;&#34892;&#20026;&#28608;&#21169;&#29992;&#20110;&#26234;&#33021;&#20307;&#30340;&#38271;&#26399;&#35268;&#21010;&#65292;&#22522;&#20110;&#23427;&#20204;&#30340;&#39550;&#39542;&#34892;&#20026;&#25110;&#20010;&#24615;&#65307;&#30636;&#26102;&#28608;&#21169;&#29992;&#20110;&#26234;&#33021;&#20307;&#30340;&#30701;&#26399;&#35268;&#21010;&#65292;&#20197;&#22522;&#20110;&#24403;&#21069;&#20132;&#36890;&#29366;&#24577;&#36827;&#34892;&#30896;&#25758;&#36991;&#20813;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#27969;&#25512;&#29702;&#27169;&#22359;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#25512;&#26029;&#23545;&#25163;&#30340;&#28608;&#21169;&#24182;&#23558;&#20854;&#25512;&#26029;&#20449;&#24687;&#32435;&#20837;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Navigating safely and efficiently in dense and heterogeneous traffic scenarios is challenging for autonomous vehicles (AVs) due to their inability to infer the behaviors or intentions of nearby drivers. In this work, we propose a distributed multi-agent reinforcement learning (MARL) algorithm with trajectory and intent prediction in dense and heterogeneous traffic scenarios. Our approach for intent-aware planning, iPLAN, allows agents to infer nearby drivers' intents solely from their local observations. We model two distinct incentives for agents' strategies: Behavioral incentives for agents' long-term planning based on their driving behavior or personality; Instant incentives for agents' short-term planning for collision avoidance based on the current traffic state. We design a two-stream inference module that allows agents to infer their opponents' incentives and incorporate their inferred information into decision-making. We perform experiments on two simulation environments, Non-C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20013;&#20986;&#29616;&#30340;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#25104;&#21151;&#20462;&#22797;&#22810;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#36328;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#36716;&#25442;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2306.06157</link><description>&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#26694;&#26550;&#36716;&#25442;&#30340;&#25925;&#38556;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Fault Localization for Framework Conversions of Image Recognition Models. (arXiv:2306.06157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20013;&#20986;&#29616;&#30340;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#25104;&#21151;&#20462;&#22797;&#22810;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#36328;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#36716;&#25442;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#26102;&#65292;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#23558;&#27169;&#22411;&#20174;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#65288;&#20363;&#22914;&#65292;&#20174;TensorFlow&#21040;PyTorch&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23481;&#26131;&#20986;&#38169;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#30446;&#26631;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#30830;&#23450;&#36825;&#31181;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#25105;&#20204;&#23545;&#19977;&#20010;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#30340;DNNs&#65288;MobileNetV2&#12289;ResNet101&#21644;InceptionV3&#65289;&#36827;&#34892;&#20102;&#19981;&#21516;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;PyTorch&#12289;Keras&#12289;TensorFlow&#65288;TF&#65289;&#21644;TFLite&#65289;&#20043;&#38388;&#36827;&#34892;&#20102;&#36716;&#25442;&#65292;&#24182;&#21457;&#29616;&#20102;&#35768;&#22810;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#39640;&#36798;100&#65285;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23450;&#20301;&#25925;&#38556;&#21644;&#20462;&#22797;&#26377;&#32570;&#38519;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#65292;&#37325;&#28857;&#25918;&#22312;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#20998;&#26512;&#38454;&#27573;&#65306;1&#65289;&#36716;&#25442;&#24037;&#20855;&#65292;2&#65289;&#27169;&#22411;&#21442;&#25968;&#65292;3&#65289;&#27169;&#22411;&#36229;&#21442;&#25968;&#65292;4&#65289;&#22270;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35768;&#22810;&#38024;&#23545;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#36716;&#25442;&#24037;&#20855;&#30340;&#25512;&#33616;&#12289;&#35843;&#35797;&#25216;&#24039;&#20197;&#21450;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#25104;&#21151;&#20462;&#22797;&#25152;&#26377;&#27979;&#35797;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;MobileNetV2&#65292;ResNet101&#21644;InceptionV3 &#30340;&#26377;&#32570;&#38519;&#30340;&#36716;&#25442;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying Deep Neural Networks (DNNs), developers often convert models from one deep learning framework to another (e.g., TensorFlow to PyTorch). However, this process is error-prone and can impact target model accuracy. To identify the extent of such impact, we perform and briefly present a differential analysis against three DNNs used for image recognition (MobileNetV2, ResNet101, and InceptionV3), converted across four well-known deep learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which revealed numerous model crashes and output label discrepancies of up to 100%. To mitigate such errors, we present a novel approach towards fault localization and repair of buggy deep learning framework conversions, focusing on pre-trained image recognition models. Our technique consists of four primary stages of analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters, and 4) graph representation. In addition, we propose a number of strategies towards faul
&lt;/p&gt;</description></item><item><title>AMEE&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#35780;&#20215;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#27604;&#36739;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#22810;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#20449;&#24687;&#20215;&#20540;&#65292;&#24110;&#21161;&#35299;&#20915;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#35299;&#37322;&#26041;&#27861;&#36873;&#25321;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05501</link><description>&lt;p&gt;
AMEE&#65306;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#35299;&#37322;&#35780;&#20215;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AMEE: A Robust Framework for Explanation Evaluation in Time Series Classification. (arXiv:2306.05501v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05501
&lt;/p&gt;
&lt;p&gt;
AMEE&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#35780;&#20215;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#27604;&#36739;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#22810;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#20449;&#24687;&#20215;&#20540;&#65292;&#24110;&#21161;&#35299;&#20915;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#35299;&#37322;&#26041;&#27861;&#36873;&#25321;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#37327;&#35780;&#20272;&#21644;&#25490;&#21517;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#21040;&#21355;&#29983;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#20851;&#38190;&#39046;&#22495;&#30340;&#26222;&#36941;&#25968;&#25454;&#31867;&#22411;&#12290;&#26368;&#36817;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35299;&#37322;&#26041;&#27861;&#30340;&#30740;&#31350;&#20852;&#36259;&#28608;&#22686;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#35299;&#37322;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#35299;&#37322;&#25216;&#26415;&#22312;&#29305;&#23450;&#38382;&#39064;&#19978;&#20135;&#29983;&#20998;&#27495;&#26102;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;&#20351;&#29992;&#21738;&#31181;&#25216;&#26415;&#12290;&#27604;&#36739;&#35299;&#37322;&#20197;&#25214;&#21040;&#27491;&#30830;&#31572;&#26696;&#24182;&#19981;&#23481;&#26131;&#12290;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65306;&#22914;&#20309;&#23450;&#37327;&#21644;&#31283;&#20581;&#22320;&#35780;&#20272;&#32473;&#23450;&#35299;&#37322;&#26041;&#27861;&#30340;&#20449;&#24687;&#20215;&#20540;&#65288;&#21363;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#24615;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#24182;&#25490;&#27604;&#36739;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AMEE&#65292;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#35299;&#37322;&#35780;&#20215;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#27604;&#36739;&#22810;&#31181;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#22312;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#20013;&#22686;&#21152;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
This paper aims to provide a framework to quantitatively evaluate and rank explanation methods for the time series classification task, which deals with a prevalent data type in critical domains such as healthcare and finance. The recent surge of research interest in explanation methods for time series classification has provided a great variety of explanation techniques. Nevertheless, when these explanation techniques disagree on a specific problem, it remains unclear which of them to use. Comparing the explanations to find the right answer is non-trivial. Two key challenges remain: how to quantitatively and robustly evaluate the informativeness (i.e., relevance for the classification task) of a given explanation method, and how to compare explanation methods side-by-side. We propose AMEE, a Model-Agnostic Explanation Evaluation framework for quantifying and comparing multiple saliency-based explanations for time series classification. Perturbation is added to the input time series gu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#20449;&#24687;&#30452;&#25509;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#36827;&#34892;&#27604;&#36739;&#65292;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26399;&#26395;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#22312;&#21407;&#28857;&#38468;&#36817;&#30340;&#36755;&#20837;&#26377;&#30528;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04847</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#37325;&#36807;&#31243;&#23558;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#23884;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Embedding stochastic differential equations into neural networks via dual processes. (arXiv:2306.04847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04847
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#20449;&#24687;&#30452;&#25509;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#36827;&#34892;&#27604;&#36739;&#65292;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26399;&#26395;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#22312;&#21407;&#28857;&#38468;&#36817;&#30340;&#36755;&#20837;&#26377;&#30528;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26399;&#26395;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36755;&#20837;&#21644;&#36755;&#20986;&#25968;&#25454;&#38598;&#65307;&#30456;&#21453;&#65292;&#20174;&#26102;&#38388;&#28436;&#21270;&#26041;&#31243;&#33719;&#24471;&#30340;&#20449;&#24687;&#65292;&#21363;&#30456;&#24212;&#30340;&#21452;&#37325;&#36807;&#31243;&#65292;&#30452;&#25509;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#36827;&#34892;&#27604;&#36739;&#12290;&#20316;&#20026;&#28436;&#31034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#29992;&#20110;Ornstein-Uhlenbeck&#36807;&#31243;&#21644;&#22122;&#22768;van der Pol&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#23398;&#20064;&#30340;&#32593;&#32476;&#30340;&#26174;&#30528;&#29305;&#24449;&#26159;&#22312;&#21407;&#28857;&#38468;&#36817;&#30340;&#36755;&#20837;&#30340;&#20934;&#30830;&#24230;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#22240;&#20026;&#23398;&#20064;&#30340;&#32593;&#32476;&#19981;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach to constructing a neural network for predicting expectations of stochastic differential equations. The proposed method does not need data sets of inputs and outputs; instead, the information obtained from the time-evolution equations, i.e., the corresponding dual process, is directly compared with the weights in the neural network. As a demonstration, we construct neural networks for the Ornstein-Uhlenbeck process and the noisy van der Pol system. The remarkable feature of learned networks with the proposed method is the accuracy of inputs near the origin. Hence, it would be possible to avoid the overfitting problem because the learned network does not depend on training data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#25193;&#25955;&#39033;&#20915;&#23450;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#65292;&#36827;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;AT&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.02618</link><description>&lt;p&gt;
&#25552;&#39640;&#25193;&#25955;&#20197;&#25913;&#21892;&#40065;&#26834;&#24615;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhance Diffusion to Improve Robust Generalization. (arXiv:2306.02618v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#25193;&#25955;&#39033;&#20915;&#23450;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#65292;&#36827;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;AT&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#20154;&#31867;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#31181;&#26368;&#24378;&#30340;&#38450;&#24481;&#26426;&#21046;&#26159;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;AT&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;AT&#30740;&#31350;&#20013;&#22914;&#20309;&#35774;&#32622;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#36229;&#21442;&#25968;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#65292;&#23450;&#21046;&#21270;&#35774;&#32622;&#22952;&#30861;&#19981;&#21516;&#27169;&#22411;&#35774;&#35745;&#22312;AT&#30740;&#31350;&#20013;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#20854;&#27425;&#65292;&#32463;&#36807;&#40065;&#26834;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#26102;&#38754;&#20020;&#22256;&#38590;&#65292;&#24182;&#19988;&#21463;&#21040;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#20027;&#35201;&#30340;AT&#26694;&#26550; - &#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;PGD-AT&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#36817;&#20284;PGD-AT&#30340;&#21160;&#24577;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;SDE&#30340;&#25193;&#25955;&#39033;&#20915;&#23450;&#20102;&#40065;&#26834;&#27867;&#21270;&#12290;&#35813;&#29702;&#35770;&#21457;&#29616;&#30340;&#19968;&#20010;&#30452;&#25509;&#25512;&#35770;&#26159;&#65292;&#40065;&#26834;&#27867;&#21270;&#19982;&#23398;&#20064;&#29575;&#21644;&#25209;&#27425;&#22823;&#23567;&#20043;&#27604;&#21576;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to human imperceptible adversarial perturbations. One of the strongest defense mechanisms is \emph{Adversarial Training} (AT). In this paper, we aim to address two predominant problems in AT. First, there is still little consensus on how to set hyperparameters with a performance guarantee for AT research, and customized settings impede a fair comparison between different model designs in AT research. Second, the robustly trained neural networks struggle to generalize well and suffer from tremendous overfitting. This paper focuses on the primary AT framework - Projected Gradient Descent Adversarial Training (PGD-AT). We approximate the dynamic of PGD-AT by a continuous-time Stochastic Differential Equation (SDE), and show that the diffusion term of this SDE determines the robust generalization. An immediate implication of this theoretical finding is that robust generalization is positively correlated with the ratio between learning rate and batch siz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#29616;&#26377;&#26041;&#27861;&#31934;&#32454;&#21270;&#30340;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#26080;&#28145;&#24230;&#20381;&#36182;&#24615;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.01992</link><description>&lt;p&gt;
&#20851;&#20110;ReLU&#32593;&#32476;&#30340;&#22823;&#23567;&#26080;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
On Size-Independent Sample Complexity of ReLU Networks. (arXiv:2306.01992v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#29616;&#26377;&#26041;&#27861;&#31934;&#32454;&#21270;&#30340;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#26080;&#28145;&#24230;&#20381;&#36182;&#24615;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#27867;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#23398;&#20064;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#22312;&#26435;&#37325;&#30697;&#38453;&#19978;&#32473;&#23450;&#33539;&#25968;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20272;&#35745;&#30456;&#20851;&#20989;&#25968;&#31867;&#30340;Rademacher&#22797;&#26434;&#24230;&#12290;&#20043;&#21069;Golowich-Rakhlin-Shamir (2020)&#33719;&#24471;&#20102;&#19968;&#20010;&#19981;&#20381;&#36182;&#20110;&#32593;&#32476;&#22823;&#23567;&#30340;&#65288;&#19982;Frobenius&#33539;&#25968;&#30340;&#20056;&#31215;&#25104;&#27604;&#20363;&#65289;&#19978;&#30028;&#65292;&#38500;&#20102;&#19968;&#20010;&#24179;&#26041;&#26681;&#28145;&#24230;&#30340;&#22240;&#23376;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#31934;&#32454;&#21270;&#30340;&#32467;&#26524;&#65292;&#36890;&#24120;&#26681;&#26412;&#27809;&#26377;&#26126;&#26174;&#30340;&#28145;&#24230;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the sample complexity of learning ReLU neural networks from the point of view of generalization. Given norm constraints on the weight matrices, a common approach is to estimate the Rademacher complexity of the associated function class. Previously Golowich-Rakhlin-Shamir (2020) obtained a bound independent of the network size (scaling with a product of Frobenius norms) except for a factor of the square-root depth. We give a refinement which often has no explicit depth-dependence at all.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35299;&#37322;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#33258;&#27965;&#24615;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#36827;&#34892;&#36719;&#20214;&#34917;&#19969;&#36873;&#25321;&#65292;&#20174;&#32780;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#35299;&#37322;&#30340;&#36719;&#20214;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00108</link><description>&lt;p&gt;
&#22810;&#25968;&#35268;&#21017;&#65306;&#36890;&#36807;&#33258;&#27965;&#24615;&#23454;&#29616;&#26356;&#22909;&#30340;&#20462;&#34917;
&lt;/p&gt;
&lt;p&gt;
Majority Rule: better patching via Self-Consistency. (arXiv:2306.00108v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35299;&#37322;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#33258;&#27965;&#24615;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#36827;&#34892;&#36719;&#20214;&#34917;&#19969;&#36873;&#25321;&#65292;&#20174;&#32780;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#35299;&#37322;&#30340;&#36719;&#20214;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#34987;&#24341;&#23548;&#35299;&#20915;&#19968;&#20123;&#38656;&#35201;&#8220;&#23569;&#37327;&#25552;&#31034;&#8221;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#31034;&#20363;&#38382;&#39064;-&#35299;&#20915;&#26041;&#26696;&#23545;&#12290;&#29616;&#22312;&#65292;&#22914;&#26524;&#23569;&#37327;&#25552;&#31034;&#20063;&#21253;&#25324;&#8220;&#24605;&#32500;&#38142;&#8221;&#65288;CoT&#65289;&#35299;&#37322;&#65292;&#24418;&#24335;&#20026;&#38382;&#39064;-&#35299;&#37322;-&#35299;&#20915;&#26041;&#26696;&#65292;LLMs&#23558;&#20250;&#20135;&#29983;&#19968;&#20010;&#8220;&#35299;&#37322;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#26368;&#36817;&#65292;&#19968;&#39033;&#26356;&#20986;&#33394;&#30340;&#25216;&#26415;&#8220;&#33258;&#27965;&#24615;&#8221;&#65288;S-C&#65289;&#20986;&#29616;&#20102;&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#31181;&#30452;&#35273;&#65306;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#35768;&#22810;&#21487;&#33021;&#30340;&#35299;&#37322;&#12290;&#24403;LLM&#34987;&#21453;&#22797;&#37319;&#26679;&#20197;&#29983;&#25104;&#38382;&#39064;&#30340;&#35299;&#37322;-&#35299;&#20915;&#26041;&#26696;&#23545;&#26102;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#27744;&#20013;&#26368;&#24120;&#21457;&#29983;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;&#24573;&#30053;&#35299;&#37322;&#65289;&#26356;&#26377;&#21487;&#33021;&#26159;&#27491;&#30830;&#30340;&#65281;&#20294;&#26159;&#65292;&#36825;&#31181;&#39640;&#24615;&#33021;&#30340;S-C&#65288;&#29978;&#33267;CoT&#65289;&#26041;&#27861;&#22312;&#36719;&#20214;&#24037;&#31243;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#21463;&#21040;&#35299;&#37322;&#30340;&#32570;&#20047;&#30340;&#38480;&#21046;&#65307;&#22823;&#22810;&#25968;&#36719;&#20214;&#25968;&#25454;&#38598;&#37117;&#32570;&#20047;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;S-C&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#35299;&#37322;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#20110;&#36719;&#20214;&#34917;&#19969;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language models (LLMs) can be induced to solve non-trivial problems with "few-shot" prompts including illustrative problem-solution examples. Now if the few-shots also include "chain of thought" (CoT) explanations, which are of the form problem-explanation-solution, LLMs will generate a "explained" solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] (S-C) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant S-C (or even CoT) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the S-C a
&lt;/p&gt;</description></item><item><title>InGram&#26159;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26032;&#20851;&#31995;&#21644;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#27719;&#24635;&#37051;&#23621;&#23884;&#20837;&#29983;&#25104;&#20851;&#31995;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19987</link><description>&lt;p&gt;
InGram&#65306;&#36890;&#36807;&#20851;&#31995;&#22270;&#36827;&#34892;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
InGram: Inductive Knowledge Graph Embedding via Relation Graphs. (arXiv:2305.19987v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19987
&lt;/p&gt;
&lt;p&gt;
InGram&#26159;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26032;&#20851;&#31995;&#21644;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#27719;&#24635;&#37051;&#23621;&#23884;&#20837;&#29983;&#25104;&#20851;&#31995;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#34987;&#35270;&#20026;&#39044;&#27979;&#35757;&#32451;&#26399;&#38388;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#23454;&#20307;&#20043;&#38388;&#30340;&#32570;&#22833;&#19977;&#20803;&#32452;&#30340;&#20219;&#21153;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;InGram&#65292;&#23427;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26032;&#20851;&#31995;&#21644;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#22522;&#20110;&#20851;&#31995;&#22270;&#21644;&#21407;&#22987;&#30693;&#35782;&#22270;&#35889;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#27719;&#24635;&#37051;&#23621;&#23884;&#20837;&#20197;&#29983;&#25104;&#20851;&#31995;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;InGram&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.18256</link><description>&lt;p&gt;
&#29992;Transformer&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#21644;&#25968;&#20540;&#30693;&#35782;&#22270;&#20013;&#30340;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#20102;&#19968;&#20010;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#19977;&#20803;&#32452;&#19982;&#38480;&#23450;&#35789;&#38598;&#21512;&#30456;&#20851;&#32852;; &#19968;&#20010;&#38480;&#23450;&#35789;&#30001;&#20851;&#31995;&#21644;&#23454;&#20307;&#32452;&#25104;&#65292;&#20026;&#19977;&#20803;&#32452;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20551;&#23450;&#23454;&#20307;&#26159;&#31163;&#25955;&#23545;&#35937;&#65292;&#20294;&#26377;&#20123;&#20449;&#24687;&#24212;&#20351;&#29992;&#25968;&#20540;&#34920;&#31034;&#65292;&#20363;&#22914;(J.R.R.&#65292;&#20986;&#29983;&#20110;&#65292;1892)&#12290;&#21516;&#26102;&#65292;&#19977;&#20803;&#32452;(J.R.R.&#65292;&#23601;&#35835;&#20110;&#65292;&#29275;&#27941;&#22823;&#23398;)&#21487;&#20197;&#19982;&#38480;&#23450;&#35789;(&#24320;&#22987;&#26102;&#38388;&#65292;1911)&#30456;&#20851;&#32852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21253;&#21547;&#19977;&#20803;&#32452;&#25110;&#38480;&#23450;&#35789;&#20013;&#25968;&#20540;&#25991;&#23383;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;Transformer&#21644;&#19968;&#20010;&#39044;&#27979;Transformer&#65292;&#26469;&#23398;&#20064;&#34920;&#31034;&#65292;&#19981;&#20165;&#22522;&#20110;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36824;&#22522;&#20110;&#25968;&#20540;&#20449;&#24687;&#12290;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#38480;&#23450;&#35789;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#39304;&#36865;&#32473;Transformer&#26469;&#33719;&#24471;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#65292;&#31579;&#36873;&#20986;&#38024;&#23545; SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#28508;&#22312;&#27835;&#30103;&#33647;&#29289;&#12290;&#20854;&#20013;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;</title><link>http://arxiv.org/abs/2305.18088</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#30340;&#33647;&#29289;&#37325;&#29992;&#20197;&#38774;&#21521;COVID-19 3CL Protease
&lt;/p&gt;
&lt;p&gt;
Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach. (arXiv:2305.18088v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#65292;&#31579;&#36873;&#20986;&#38024;&#23545; SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#28508;&#22312;&#27835;&#30103;&#33647;&#29289;&#12290;&#20854;&#20013;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#24050;&#32463;&#25104;&#20026;&#20840;&#29699;&#20581;&#24247;&#21361;&#26426;&#65292;&#36843;&#20999;&#38656;&#35201;&#24555;&#36895;&#37492;&#23450;&#28508;&#22312;&#30340;&#27835;&#30103;&#33647;&#29289;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#33647;&#29289;&#37325;&#29992;&#26159;&#30465;&#26102;&#30465;&#21147;&#30340;&#21807;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;Zinc&#25968;&#25454;&#24211;&#23545;&#20840;&#29699;&#24050;&#25209;&#20934;&#65288;&#21253;&#25324;FDA&#25209;&#20934;&#65289;&#30340;5903&#31181;&#33647;&#29289;&#36827;&#34892;&#31579;&#36873;&#65292;&#20316;&#20026;&#28508;&#22312;&#30340;COVID-19&#27835;&#30103;&#33647;&#29289;&#65292;&#20197;&#38774;&#21521;SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#12290;&#25105;&#20204;&#20351;&#29992;Autodock-Vina&#36827;&#34892;&#20998;&#23376;&#23545;&#25509;&#65292;&#26816;&#26597;&#33647;&#29289;&#20998;&#23376;&#30340;&#21151;&#25928;&#12290;&#20026;&#20102;&#25552;&#39640;&#33647;&#29289;&#37325;&#29992;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#20915;&#31574;&#26641;&#12289;&#39069;&#22806;&#26641;&#12289;MLP&#12289;KNN&#12289;XGBoost&#21644;&#26799;&#24230;&#25552;&#21319;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#24314;&#27169;&#32467;&#21512;&#33647;&#29289;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#35745;&#31639;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#12290;&#36825;&#20123;&#27169;&#25311;&#32467;&#26524;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has created a global health crisis, driving the need for the rapid identification of potential therapeutics. To meet this challenge, drug repurposing is the only solution with saving cost and time. In this study, we used the Zinc database to screen the world-approved including FDA-approved 5903 drugs for repurposing as potential COVID-19 treatments targeting the main protease 3CL of SARS-CoV-2. We performed molecular docking using Autodock-Vina to check the efficacy of drug molecules. To enhance the efficiency of drug repurposing approach, we modeled the binding affinities using several machine learning regression approaches for QSAR modeling such as decision tree, extra trees, MLP, KNN, XGBoost, and gradient boosting. The computational results demonstrated that Decision Tree Regression (DTR) model has improved statistical measures of R2 and RMSE. These simulated results helped to identify drugs with high binding affinity and favorable binding energies. From the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#25968;&#20540;&#31639;&#26415;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;PCA&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#23545;&#20197;&#24448;&#26041;&#27861;&#65292;&#20854;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#19978;&#22343;&#26377;&#25552;&#21319;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#21644;&#39640;&#25928;&#21516;&#24577;&#30005;&#36335;&#65292;&#35745;&#31639;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17341</link><description>&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#31354;&#38388;&#30340;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#26469;&#25913;&#36827;&#38544;&#31169;&#20445;&#25252;PCA
&lt;/p&gt;
&lt;p&gt;
Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication. (arXiv:2305.17341v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#25968;&#20540;&#31639;&#26415;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;PCA&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#23545;&#20197;&#24448;&#26041;&#27861;&#65292;&#20854;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#19978;&#22343;&#26377;&#25552;&#21319;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#21644;&#39640;&#25928;&#21516;&#24577;&#30005;&#36335;&#65292;&#35745;&#31639;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36817;&#20284;&#25968;&#20540;&#31639;&#26415;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;PCA&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#34987;&#31216;&#20026;PowerMethod&#30340;PCA&#24120;&#35268;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20197;&#21327;&#26041;&#24046;&#30697;&#38453;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#19982;&#25968;&#25454;&#38598;&#30340;&#31532;&#19968;&#20027;&#25104;&#20998;&#23545;&#24212;&#30340;&#36817;&#20284;&#29305;&#24449;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65288;&#22914;Pandas CSCML 21&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20197;&#19979;&#20248;&#21270;&#65306;&#65288;i&#65289;&#20248;&#21270;&#20102;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#25216;&#26415;&#65288;Jiang&#31561;&#20154;SIGSAC 2018&#65289;&#65292;&#35813;&#25216;&#26415;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35745;&#31639;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65307;&#65288;ii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#21516;&#24577;&#30005;&#36335;&#26469;&#21516;&#24577;&#35745;&#31639;&#21327;&#26041;&#24046;&#30697;&#38453;&#65307;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#29992;&#20110;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal Component Analysis (PCA) is a pivotal technique in the fields of machine learning and data analysis. In this study, we present a novel approach for privacy-preserving PCA using an approximate numerical arithmetic homomorphic encryption scheme. We build our method upon a proposed PCA routine known as the PowerMethod, which takes the covariance matrix as input and produces an approximate eigenvector corresponding to the first principal component of the dataset. Our method surpasses previous approaches (e.g., Pandas CSCML 21) in terms of efficiency, accuracy, and scalability.  To achieve such efficiency and accuracy, we have implemented the following optimizations: (i) We optimized a homomorphic matrix multiplication technique (Jiang et al. SIGSAC 2018) that will play a crucial role in the computation of the covariance matrix. (ii) We devised an efficient homomorphic circuit for computing the covariance matrix homomorphically. (iii) We designed a novel and efficient homomorphic 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NASimEmu&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#26234;&#33021;&#20307;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#27169;&#25311;&#22120;&#21644;&#20223;&#30495;&#22120;&#30340;&#32467;&#21512;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20223;&#30495;&#22120;&#20013;&#37096;&#32626;&#65292;&#20174;&#32780;&#39564;&#35777;&#25152;&#20351;&#29992;&#30340;&#25277;&#35937;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#26694;&#26550;&#30340;&#35774;&#35745;&#26088;&#22312;&#22521;&#35757;&#36890;&#29992;&#30340;&#26234;&#33021;&#20307;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#22330;&#26223;&#20013;&#36827;&#34892;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.17246</link><description>&lt;p&gt;
NASimEmu: &#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#22330;&#26223;&#30340;&#26234;&#33021;&#20307;&#30340;&#32593;&#32476;&#25915;&#20987;&#27169;&#25311;&#22120;&#19982;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
NASimEmu: Network Attack Simulator &amp; Emulator for Training Agents Generalizing to Novel Scenarios. (arXiv:2305.17246v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NASimEmu&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#26234;&#33021;&#20307;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#27169;&#25311;&#22120;&#21644;&#20223;&#30495;&#22120;&#30340;&#32467;&#21512;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20223;&#30495;&#22120;&#20013;&#37096;&#32626;&#65292;&#20174;&#32780;&#39564;&#35777;&#25152;&#20351;&#29992;&#30340;&#25277;&#35937;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#26694;&#26550;&#30340;&#35774;&#35745;&#26088;&#22312;&#22521;&#35757;&#36890;&#29992;&#30340;&#26234;&#33021;&#20307;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#22330;&#26223;&#20013;&#36827;&#34892;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29992;&#20110;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22521;&#35757;&#25915;&#20987;&#22411;&#28183;&#36879;&#27979;&#35797;&#26234;&#33021;&#20307;&#30340;&#26694;&#26550;&#24448;&#24448;&#38590;&#20197;&#20351;&#26234;&#33021;&#20307;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21407;&#22240;&#22312;&#20110;&#22522;&#20110;&#27169;&#25311;&#30340;&#26694;&#26550;&#20013;&#23384;&#22312;&#29616;&#23454;&#24046;&#36317;&#65292;&#32780;&#22522;&#20110;&#20223;&#30495;&#30340;&#26694;&#26550;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#26694;&#26550;&#36890;&#24120;&#20351;&#29992;&#19981;&#29616;&#23454;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;NASimEmu&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#20855;&#26377;&#20849;&#20139;&#25509;&#21475;&#30340;&#27169;&#25311;&#22120;&#21644;&#20223;&#30495;&#22120;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#26234;&#33021;&#20307;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20223;&#30495;&#22120;&#20013;&#37096;&#32626;&#65292;&#20174;&#32780;&#39564;&#35777;&#25152;&#20351;&#29992;&#30340;&#25277;&#35937;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20419;&#36827;&#20102;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#22330;&#26223;&#20013;&#36827;&#34892;&#36716;&#31227;&#30340;&#36890;&#29992;&#26234;&#33021;&#20307;&#30340;&#24320;&#21457;&#12290;&#23545;&#20110;&#27169;&#25311;&#37096;&#20998;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;NASim&#24182;&#22686;&#24378;&#20102;&#20854;&#23454;&#29616;&#12290;&#20223;&#30495;&#22120;&#20351;&#29992;&#34892;&#19994;&#32423;&#24037;&#20855;&#23454;&#26045;&#65292;&#22914;Vagrant&#12289;VirtualBox&#21644;Metasp&#12290;
&lt;/p&gt;
&lt;p&gt;
Current frameworks for training offensive penetration testing agents with deep reinforcement learning struggle to produce agents that perform well in real-world scenarios, due to the reality gap in simulation-based frameworks and the lack of scalability in emulation-based frameworks. Additionally, existing frameworks often use an unrealistic metric that measures the agents' performance on the training data. NASimEmu, a new framework introduced in this paper, addresses these issues by providing both a simulator and an emulator with a shared interface. This approach allows agents to be trained in simulation and deployed in the emulator, thus verifying the realism of the used abstraction. Our framework promotes the development of general agents that can transfer to novel scenarios unseen during their training. For the simulation part, we adopt an existing simulator NASim and enhance its realism. The emulator is implemented with industry-level tools, such as Vagrant, VirtualBox, and Metasp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#37325;&#26500;&#20986;&#28151;&#27788;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#65292;&#24182;&#25512;&#26029;&#20854;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15111</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#25968;&#25454;&#20013;&#37325;&#26500;&#12289;&#39044;&#27979;&#21644;&#31283;&#23450;&#28151;&#27788;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Reconstruction, forecasting, and stability of chaotic dynamics from partial data. (arXiv:2305.15111v2 [nlin.AO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15111
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#37325;&#26500;&#20986;&#28151;&#27788;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#65292;&#24182;&#25512;&#26029;&#20854;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#26041;&#31243;&#30340;&#26041;&#27861;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#39044;&#27979;&#21644;&#35745;&#31639;&#28151;&#27788;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#36825;&#31687;&#35745;&#31639;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#65288;i&#65289;&#25512;&#26029;&#26410;&#35266;&#27979;&#21040;&#65288;&#38544;&#34255;&#30340;&#65289;&#28151;&#27788;&#21464;&#37327;&#30340;&#21160;&#21147;&#23398;&#65288;&#20840;&#29366;&#24577;&#37325;&#26500;&#65289;&#65307;&#65288;ii&#65289;&#23545;&#20840;&#29366;&#24577;&#30340;&#28436;&#21270;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#65307;&#21644;&#65288;iii&#65289;&#25512;&#26029;&#20840;&#29366;&#24577;&#30340;&#31283;&#23450;&#24615;&#36136;&#12290;&#36825;&#20123;&#20219;&#21153;&#26159;&#36890;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#26469;&#23436;&#25104;&#30340;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#35266;&#27979;&#25968;&#25454;&#20165;&#38480;&#20110;&#29366;&#24577;&#30340;&#37096;&#20998;&#65306;&#65288;i&#65289;&#20302;&#21040;&#39640;&#20998;&#36776;&#29575;&#30340;LSTM&#65288;LH-LSTM&#65289;&#65292;&#23427;&#23558;&#37096;&#20998;&#35266;&#27979;&#20316;&#20026;&#35757;&#32451;&#36755;&#20837;&#65292;&#24182;&#22312;&#35745;&#31639;&#25439;&#22833;&#26102;&#38656;&#35201;&#35775;&#38382;&#25972;&#20010;&#31995;&#32479;&#29366;&#24577;&#65307;&#21644;&#65288;ii&#65289;&#29289;&#29702;&#20449;&#24687;LSTM&#65288;PI-LSTM&#65289;&#65292;&#20854;&#35774;&#35745;&#26159;&#23558;&#37096;&#20998;&#35266;&#27979;&#19982;&#21160;&#21147;&#31995;&#32479;&#28436;&#21270;&#26041;&#31243;&#30340;&#31215;&#20998;&#24418;&#24335;&#30456;&#32467;&#21512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;LSTM&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#28151;&#27788;&#31995;&#32479;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The forecasting and computation of the stability of chaotic systems from partial observations are tasks for which traditional equation-based methods may not be suitable. In this computational paper, we propose data-driven methods to (i) infer the dynamics of unobserved (hidden) chaotic variables (full-state reconstruction); (ii) time forecast the evolution of the full state; and (iii) infer the stability properties of the full state. The tasks are performed with long short-term memory (LSTM) networks, which are trained with observations (data) limited to only part of the state: (i) the low-to-high resolution LSTM (LH-LSTM), which takes partial observations as training input, and requires access to the full system state when computing the loss; and (ii) the physics-informed LSTM (PI-LSTM), which is designed to combine partial observations with the integral formulation of the dynamical system's evolution equations. First, we derive the Jacobian of the LSTMs. Second, we analyse a chaotic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#21307;&#23398;&#20307;&#24449;&#21644;&#22238;&#24402;&#25554;&#20540;&#30340;&#26041;&#27861;&#65288;MedLens&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#30149;&#21382;&#20013;&#21307;&#23398;&#20307;&#24449;&#25968;&#25454;&#32570;&#22833;&#29575;&#36807;&#39640;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11742</link><description>&lt;p&gt;
MedLens: &#36890;&#36807;&#36873;&#25321;&#21307;&#23398;&#20307;&#24449;&#21644;&#22238;&#24402;&#25554;&#20540;&#26469;&#25552;&#39640;&#27515;&#20129;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MedLens: Improve mortality prediction via medical signs selecting and regression interpolation. (arXiv:2305.11742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#21307;&#23398;&#20307;&#24449;&#21644;&#22238;&#24402;&#25554;&#20540;&#30340;&#26041;&#27861;&#65288;MedLens&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#30149;&#21382;&#20013;&#21307;&#23398;&#20307;&#24449;&#25968;&#25454;&#32570;&#22833;&#29575;&#36807;&#39640;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#24182;&#25552;&#21069;&#39044;&#27979;&#27515;&#20129;&#29575;&#23545;&#21450;&#26102;&#25552;&#20379;&#24739;&#32773;&#25252;&#29702;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#22823;&#37327;&#21307;&#23398;&#20307;&#24449;&#34987;&#29992;&#20110;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#20020;&#24202;&#20307;&#24449;&#30340;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#34987;&#36739;&#23569;&#35752;&#35770;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#21307;&#23398;&#20307;&#24449;&#21644;&#22823;&#37327;&#24739;&#32773;&#20303;&#38498;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#29575;&#21644;&#30456;&#20851;&#20998;&#25968;&#36827;&#34892;&#28145;&#20837;&#27979;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#32508;&#21512;&#32570;&#22833;&#29575;&#38750;&#24120;&#39640;&#65292;&#22823;&#37327;&#26080;&#29992;&#30340;&#20307;&#24449;&#21487;&#33021;&#20250;&#25439;&#23475;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#21482;&#26377;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#25165;&#33021;&#25552;&#39640;&#19981;&#21516;&#39044;&#27979;&#31639;&#27861;&#30340;&#22522;&#32447;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;MedLens&#65292;&#36890;&#36807;&#32479;&#35745;&#33258;&#21160;&#36873;&#25321;&#37325;&#35201;&#21307;&#23398;&#20307;&#24449;&#65292;&#24182;&#20351;&#29992;&#28789;&#27963;&#30340;&#25554;&#20540;&#26041;&#27861;&#22788;&#29702;&#39640;&#32570;&#22833;&#29575;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring the health status of patients and predicting mortality in advance is vital for providing patients with timely care and treatment. Massive medical signs in electronic health records (EHR) are fitted into advanced machine learning models to make predictions. However, the data-quality problem of original clinical signs is less discussed in the literature. Based on an in-depth measurement of the missing rate and correlation score across various medical signs and a large amount of patient hospital admission records, we discovered the comprehensive missing rate is extremely high, and a large number of useless signs could hurt the performance of prediction models. Then we concluded that only improving data-quality could improve the baseline accuracy of different prediction algorithms. We designed MEDLENS, with an automatic vital medical signs selection approach via statistics and a flexible interpolation approach for high missing rate time series. After augmenting the data-quality 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#30423;&#31363;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#22312;&#23454;&#38469;&#30423;&#31363;&#36807;&#31243;&#20013;&#30456;&#20114;&#19981;&#30830;&#23450;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#20316;&#32773;&#23581;&#35797;&#20351;&#29992;&#22810;&#20010;&#21487;&#33021;&#30340;&#32593;&#32476;&#24182;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#32452;&#21512;&#20197;&#25552;&#39640;&#36136;&#37327;&#65292;&#20294;&#32467;&#26524;&#34920;&#26126;&#21482;&#26377;&#24494;&#24369;&#30340;&#25913;&#21892;&#12290;&#20316;&#32773;&#21457;&#29616;&#32593;&#32476;&#22810;&#26679;&#24615;&#19981;&#36275;&#26159;&#23548;&#33268;&#36825;&#19968;&#32467;&#26524;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.05293</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#30340;&#27169;&#22411;&#30423;&#31363;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
On the Limitations of Model Stealing with Uncertainty Quantification Models. (arXiv:2305.05293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#30423;&#31363;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#22312;&#23454;&#38469;&#30423;&#31363;&#36807;&#31243;&#20013;&#30456;&#20114;&#19981;&#30830;&#23450;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#20316;&#32773;&#23581;&#35797;&#20351;&#29992;&#22810;&#20010;&#21487;&#33021;&#30340;&#32593;&#32476;&#24182;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#32452;&#21512;&#20197;&#25552;&#39640;&#36136;&#37327;&#65292;&#20294;&#32467;&#26524;&#34920;&#26126;&#21482;&#26377;&#24494;&#24369;&#30340;&#25913;&#21892;&#12290;&#20316;&#32773;&#21457;&#29616;&#32593;&#32476;&#22810;&#26679;&#24615;&#19981;&#36275;&#26159;&#23548;&#33268;&#36825;&#19968;&#32467;&#26524;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#30423;&#31363;&#26088;&#22312;&#20197;&#21407;&#22987;&#35757;&#32451;&#25104;&#26412;&#30340;&#19968;&#23567;&#37096;&#20998;&#25512;&#26029;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#26435;&#37325;&#23610;&#23544;&#21644;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#65292;&#23548;&#33268;&#22312;&#30423;&#31363;&#36807;&#31243;&#20013;&#30456;&#20114;&#19981;&#30830;&#23450;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#21487;&#33021;&#30340;&#32593;&#32476;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#32452;&#21512;&#36215;&#26469;&#26469;&#26174;&#24335;&#22320;&#22788;&#29702;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#30423;&#31363;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#20010;&#27969;&#34892;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#22312;&#27169;&#22411;&#30423;&#31363;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#32771;&#34385;&#30340;&#27169;&#22411;&#22312;&#26631;&#31614;&#19968;&#33268;&#24615;&#65288;&#21363;&#20445;&#30495;&#24230;&#65289;&#26041;&#38754;&#21482;&#33021;&#24102;&#26469;&#24494;&#23567;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#25214;&#21040;&#21407;&#22240;&#65292;&#25105;&#20204;&#36890;&#36807;&#26597;&#30475;&#39044;&#27979;&#26041;&#24046;&#20316;&#20026;&#35757;&#32451;&#36845;&#20195;&#20989;&#25968;&#30340;&#26041;&#24335;&#26469;&#26816;&#26597;&#27169;&#22411;&#39044;&#27979;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#24448;&#24448;&#20855;&#26377;&#30456;&#20284;&#30340;&#39044;&#27979;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#24819;&#35201;&#21033;&#29992;&#30340;&#32593;&#32476;&#22810;&#26679;&#24615;&#19981;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model stealing aims at inferring a victim model's functionality at a fraction of the original training cost. While the goal is clear, in practice the model's architecture, weight dimension, and original training data can not be determined exactly, leading to mutual uncertainty during stealing. In this work, we explicitly tackle this uncertainty by generating multiple possible networks and combining their predictions to improve the quality of the stolen model. For this, we compare five popular uncertainty quantification models in a model stealing task. Surprisingly, our results indicate that the considered models only lead to marginal improvements in terms of label agreement (i.e., fidelity) to the stolen model. To find the cause of this, we inspect the diversity of the model's prediction by looking at the prediction variance as a function of training iterations. We realize that during training, the models tend to have similar predictions, indicating that the network diversity we wanted
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26597;&#34920;&#27861;&#20195;&#26367;CNN&#20013;&#20056;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20445;&#30041;&#20102;&#20027;&#35201;CNN&#25805;&#20316;&#30340;&#35821;&#20041;&#65292;&#19988;&#21487;&#22312;FPGA&#23454;&#29616;&#20013;&#23454;&#29616;&#26174;&#33879;&#30340;&#33021;&#37327;&#38477;&#20302;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.05274</link><description>&lt;p&gt;
DietCNN:&#29992;&#20110;&#37327;&#21270;CNN&#30340;&#26080;&#20056;&#31215;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
DietCNN: Multiplication-free Inference for Quantized CNNs. (arXiv:2305.05274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26597;&#34920;&#27861;&#20195;&#26367;CNN&#20013;&#20056;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20445;&#30041;&#20102;&#20027;&#35201;CNN&#25805;&#20316;&#30340;&#35821;&#20041;&#65292;&#19988;&#21487;&#22312;FPGA&#23454;&#29616;&#20013;&#23454;&#29616;&#26174;&#33879;&#30340;&#33021;&#37327;&#38477;&#20302;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23884;&#20837;&#24335;&#31995;&#32479;&#19982;&#26426;&#22120;&#26234;&#33021;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#38656;&#27714;&#24050;&#32463;&#25104;&#20026;&#20419;&#36827;&#30740;&#31350;&#30028;&#22312;&#23884;&#20837;&#24335;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#25512;&#26029;&#30340;&#20652;&#21270;&#21058;&#12290;&#21024;&#38500;&#26114;&#36149;&#30340;&#20056;&#27861;&#25805;&#20316;&#26469;&#37325;&#26032;&#35774;&#35745;CNN&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20943;&#23569;&#25512;&#26029;&#33021;&#37327;&#20351;&#29992;&#26041;&#38754;&#20855;&#26377;&#26377;&#21069;&#36884;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26597;&#34920;&#27861;&#20195;&#26367;CNN&#20013;&#20056;&#27861;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#23436;&#20840;&#20462;&#25913;CNN&#25805;&#20316;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20445;&#30041;&#20102;&#20027;&#35201;CNN&#25805;&#20316;&#30340;&#35821;&#20041;&#12290;&#31526;&#21512;CNN&#23618;&#25805;&#20316;&#30340;&#29616;&#26377;&#26426;&#21046;&#30830;&#20445;&#20102;&#26631;&#20934;CNN&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#21333;&#20010;&#28608;&#27963;&#30721;&#26412;&#30340;&#26080;&#20056;&#31215;CNN&#22312;MNIST-LeNet-5&#12289;CIFAR10-VGG-11&#21644;Tiny ImageNet-ResNet&#30340;FPGA&#23454;&#29616;&#20013;&#65292;&#27599;&#27425;&#25512;&#29702;&#33021;&#22815;&#23454;&#29616;4.7&#20493;&#12289;5.6&#20493;&#21644;3.5&#20493;&#30340;&#33021;&#37327;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rising demand for networked embedded systems with machine intelligence has been a catalyst for sustained attempts by the research community to implement Convolutional Neural Networks (CNN) based inferencing on embedded resource-limited devices. Redesigning a CNN by removing costly multiplication operations has already shown promising results in terms of reducing inference energy usage. This paper proposes a new method for replacing multiplications in a CNN by table look-ups. Unlike existing methods that completely modify the CNN operations, the proposed methodology preserves the semantics of the major CNN operations. Conforming to the existing mechanism of the CNN layer operations ensures that the reliability of a standard CNN is preserved. It is shown that the proposed multiplication-free CNN, based on a single activation codebook, can achieve 4.7x, 5.6x, and 3.5x reduction in energy per inference in an FPGA implementation of MNIST-LeNet-5, CIFAR10-VGG-11, and Tiny ImageNet-ResNet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03515</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#24230;&#30340;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#26641;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#29983;&#38271;&#31639;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#19978;&#23616;&#37096;&#26368;&#23567;&#21270;&#19981;&#32431;&#24230;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#36138;&#24515;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#36724;&#23545;&#40784;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#30452;&#36890;&#31639;&#23376;&#22312;&#23494;&#38598;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#19978;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
&lt;/p&gt;</description></item><item><title>NNSplitter&#26159;&#19968;&#31181;&#20027;&#21160;&#20445;&#25252;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#28151;&#28102;&#27169;&#22411;&#21644;&#27169;&#22411;&#31192;&#23494;&#20004;&#37096;&#20998;&#65292;&#37319;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#26469;&#26368;&#22823;&#21270;&#31934;&#24230;&#19979;&#38477;&#21644;&#20943;&#23569;&#28151;&#28102;&#26435;&#37325;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.00097</link><description>&lt;p&gt;
NNSplitter&#65306;&#22522;&#20110;&#33258;&#21160;&#26435;&#37325;&#28151;&#28102;&#30340;DNN&#27169;&#22411;&#20027;&#21160;&#38450;&#24481;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation. (arXiv:2305.00097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00097
&lt;/p&gt;
&lt;p&gt;
NNSplitter&#26159;&#19968;&#31181;&#20027;&#21160;&#20445;&#25252;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#28151;&#28102;&#27169;&#22411;&#21644;&#27169;&#22411;&#31192;&#23494;&#20004;&#37096;&#20998;&#65292;&#37319;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#26469;&#26368;&#22823;&#21270;&#31934;&#24230;&#19979;&#38477;&#21644;&#20943;&#23569;&#28151;&#28102;&#26435;&#37325;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#24050;&#32463;&#36890;&#36807;&#25968;&#23383;&#27700;&#21360;&#31561;&#25216;&#26415;&#36827;&#34892;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#34987;&#21160;&#27169;&#22411;&#20445;&#25252;&#24182;&#19981;&#33021;&#23436;&#20840;&#38450;&#27490;&#27169;&#22411;&#28389;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26041;&#26696;&#65292;&#21363;NNSplitter&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#20004;&#37096;&#20998;&#26469;&#20027;&#21160;&#20445;&#25252;&#27169;&#22411;&#65306;&#19968;&#20010;&#34920;&#29616;&#36739;&#24046;&#30340;&#28151;&#28102;&#27169;&#22411;&#21644;&#30001;&#28151;&#28102;&#26435;&#37325;&#30340;&#32034;&#24341;&#21644;&#21407;&#22987;&#20540;&#32452;&#25104;&#30340;&#27169;&#22411;&#31192;&#23494;&#65292;&#21482;&#26377;&#25480;&#26435;&#29992;&#25143;&#25165;&#33021;&#35775;&#38382;&#12290;NNSplitter&#21033;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#26469;&#20445;&#25252;&#31192;&#23494;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#26469;&#20943;&#23569;&#28151;&#28102;&#26435;&#37325;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#31934;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#20462;&#25913;&#36229;&#36807;2800&#19975;&#20010;&#26435;&#37325;&#30340;313&#20010;&#65288;&#21363;0.001&#65285;&#65289;&#65292;&#28151;&#28102;VGG-11&#27169;&#22411;&#22312;Fashion-MNIST&#19978;&#30340;&#31934;&#24230;&#21487;&#20197;&#38477;&#20302;&#21040;10&#65285;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;NNSplitter&#20855;&#26377;&#38544;&#34109;&#24615;&#21644;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users. NNSplitter uses the trusted execution environment to secure the secrets and a reinforcement learning-based controller to reduce the number of obfuscated weights while maximizing accuracy drop. Our experiments show that by only modifying 313 out of over 28 million (i.e., 0.001%) weights, the accuracy of the obfuscated VGG-11 model on Fashion-MNIST can drop to 10%. We also demonstrate that NNSplitter is stealthy and resilient aga
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#21644;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00070</link><description>&lt;p&gt;
&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Platt Scaling with Calibeating. (arXiv:2305.00070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#21644;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#31216;&#20026;&#22312;&#32447;Platt&#32553;&#25918;(OPS)&#65292;&#23427;&#23558;Platt&#32553;&#25918;&#25216;&#26415;&#19982;&#22312;&#32447;&#36923;&#36753;&#22238;&#24402;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;OPS&#22914;&#20309;&#22312;&#20998;&#24067;&#28418;&#31227;&#30340;i.i.d.&#21644;&#38750;i.i.d.&#24773;&#20917;&#19979;&#24179;&#31283;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#24403;&#26368;&#20339;&#30340;Platt&#32553;&#25918;&#27169;&#22411;&#26412;&#36523;&#34987;&#38169;&#35823;&#26657;&#20934;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26368;&#36817;&#24320;&#21457;&#30340;&#31216;&#20026;calibeating&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;OPS&#65292;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;OPS+calibeating&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#26159;&#20445;&#35777;&#26657;&#20934;&#30340;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#23427;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;OPS&#24605;&#24819;&#25193;&#23637;&#21040;beta&#32553;&#25918;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an online post-hoc calibration method, called Online Platt Scaling (OPS), which combines the Platt scaling technique with online logistic regression. We demonstrate that OPS smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance OPS by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting OPS+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all OPS ideas to the beta scaling method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gromov-Wasserstein Discrepancy&#36873;&#25321;&#26368;&#20339;&#20107;&#20214;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#34920;&#31034;&#12289;&#32593;&#32476;&#39592;&#24178;&#21644;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#65292;&#26412;&#25991;&#23545;&#22823;&#22411;&#20107;&#20214;&#34920;&#31034;&#27861;&#23478;&#26063;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29289;&#20307;&#26816;&#27979;&#30340;&#34920;&#31034;&#27861;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13455</link><description>&lt;p&gt;
&#20174;&#28151;&#27788;&#20013;&#36856;&#21457;&#20986;&#31209;&#24207;&#65306;&#20026;&#29289;&#20307;&#26816;&#27979;&#25490;&#24207;&#20107;&#20214;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Chaos Comes Order: Ordering Event Representations for Object Detection. (arXiv:2304.13455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gromov-Wasserstein Discrepancy&#36873;&#25321;&#26368;&#20339;&#20107;&#20214;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#34920;&#31034;&#12289;&#32593;&#32476;&#39592;&#24178;&#21644;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#65292;&#26412;&#25991;&#23545;&#22823;&#22411;&#20107;&#20214;&#34920;&#31034;&#27861;&#23478;&#26063;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29289;&#20307;&#26816;&#27979;&#30340;&#34920;&#31034;&#27861;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22788;&#29702;&#20107;&#20214;&#30340;&#39030;&#23574;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;&#29616;&#25104;&#32593;&#32476;&#20043;&#21069;&#65292;&#39318;&#20808;&#23558;&#20854;&#36716;&#25442;&#20026;&#31264;&#23494;&#30340;&#32593;&#26684;&#29366;&#36755;&#20837;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#20026;&#20219;&#21153;&#36873;&#25321;&#36866;&#24403;&#30340;&#34920;&#31034;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#34920;&#31034;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26681;&#25454;&#39564;&#35777;&#20998;&#25968;&#36873;&#25321;&#26368;&#20339;&#34920;&#31034;&#65292;&#36825;&#38750;&#24120;&#32791;&#26102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#21407;&#22987;&#20107;&#20214;&#21450;&#20854;&#34920;&#31034;&#20043;&#38388;&#30340;Gromov-Wasserstein Discrepancy (GWD)&#36873;&#25321;&#26368;&#20339;&#34920;&#31034;&#26469;&#28040;&#38500;&#36825;&#20010;&#29942;&#39048;&#12290;&#23427;&#30340;&#35745;&#31639;&#36895;&#24230;&#22823;&#32422;&#27604;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24555;200&#20493;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#34920;&#31034;&#12289;&#32593;&#32476;&#39592;&#24178;&#21644;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20107;&#20214;&#34920;&#31034;&#27861;&#20219;&#21153;&#24615;&#33021;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#25214;&#21040;&#20855;&#26377;&#39640;&#20219;&#21153;&#20998;&#25968;&#30340;&#34920;&#31034;&#30456;&#24403;&#20110;&#25214;&#21040;&#20855;&#26377;&#20302;GWD&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#39318;&#27425;&#23545;&#22823;&#22411;&#20107;&#20214;&#34920;&#31034;&#27861;&#23478;&#26063;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29289;&#20307;&#26816;&#27979;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Moving MNIST&#21644;N-Caltech101&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#21518;&#32773;&#36798;&#21040;&#20102;83.0%&#30340;1%&#35823;&#25253;&#29575;&#19979;&#30340;mAP&#26032;&#30340;&#26368;&#39640;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. In this work, we eliminate this bottleneck by selecting the best representation based on the Gromov-Wasserstein Discrepancy (GWD) between the raw events and their representation. It is approximately 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, and datasets. This means that finding a representation with a high task score is equivalent to finding a representation with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#20462;&#22797;&#26550;&#26500; (E2ELR) &#29992;&#20110;&#32463;&#27982;&#20998;&#27966;&#38382;&#39064;&#30340;&#20248;&#21270;&#20195;&#29702;&#65292; &#32467;&#26524;&#34920;&#26126;&#65292;E2ELR&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#33267;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2304.11726</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#32463;&#27982;&#20998;&#27966;&#38382;&#39064;&#30340;&#31471;&#21040;&#31471;&#21487;&#34892;&#24615;&#20248;&#21270;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
End-to-End Feasible Optimization Proxies for Large-Scale Economic Dispatch. (arXiv:2304.11726v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#20462;&#22797;&#26550;&#26500; (E2ELR) &#29992;&#20110;&#32463;&#27982;&#20998;&#27966;&#38382;&#39064;&#30340;&#20248;&#21270;&#20195;&#29702;&#65292; &#32467;&#26524;&#34920;&#26126;&#65292;E2ELR&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#33267;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#20462;&#22797;&#65288;E2ELR&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#32463;&#27982;&#20998;&#27966;&#38382;&#39064;&#30340;&#20248;&#21270;&#20195;&#29702;&#12290;E2ELR&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#38381;&#24335;&#12289;&#21487;&#24494;&#20998;&#30340;&#20462;&#22797;&#23618;&#65292;&#20174;&#32780;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#38598;&#25104;&#20102;&#23398;&#20064;&#21644;&#21487;&#34892;&#24615;&#12290;E2ELR&#36824;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#21644;&#31163;&#32447;&#35299;&#20915;&#22823;&#37327;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20855;&#26377;&#25968;&#19975;&#20010;&#27597;&#32447;&#30340;&#34892;&#19994;&#35268;&#27169;&#30005;&#21147;&#32593;&#32476;&#19978;&#36827;&#34892;&#32463;&#27982;&#20998;&#27966;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#30417;&#30563;&#30340;E2ELR&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20854;&#20248;&#21270;&#38388;&#38553;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#33267;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a novel End-to-End Learning and Repair (E2ELR) architecture for training optimization proxies for economic dispatch problems. E2ELR combines deep neural networks with closed-form, differentiable repair layers, thereby integrating learning and feasibility in an end-to-end fashion. E2ELR is also trained with self-supervised learning, removing the need for labeled data and the solving of numerous optimization problems offline. E2ELR is evaluated on industry-size power grids with tens of thousands of buses using an economic dispatch that co-optimizes energy and reserves. The results demonstrate that the self-supervised E2ELR achieves state-of-the-art performance, with optimality gaps that outperform other baselines by at least an order of magnitude.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2304.01752</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#23569;&#26679;&#26412;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#36719;&#25552;&#31034;&#23398;&#20064;&#26159;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#29992;&#30340;&#26368;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26032;&#39046;&#22495;&#24341;&#21457;&#30340;&#20998;&#24067;&#20559;&#31227;&#26469;&#32553;&#23567;&#27169;&#24577;&#24046;&#36317;&#12290;&#34429;&#28982;&#35813;&#26041;&#27861;&#24615;&#33021;&#39640;&#25928;&#65292;&#20294;&#20173;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#27169;&#22411;&#19978;&#21487;&#33021;&#20250;&#23548;&#33268;&#35745;&#31639;&#19978;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340; V-L &#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65292;&#35757;&#32451;&#36895;&#24230;&#24555;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasible for large models with billions of parameters. To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Linear Feature Alignment (LFA), a simple line
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340; DeepAccident &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#20013;&#21457;&#29983;&#30340;&#20107;&#25925;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21487;&#29992;&#20110;&#30452;&#25509;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#20107;&#25925;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01168</link><description>&lt;p&gt;
DeepAccident&#65306;V2X&#33258;&#21160;&#39550;&#39542;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving. (arXiv:2304.01168v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340; DeepAccident &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#20013;&#21457;&#29983;&#30340;&#20107;&#25925;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21487;&#29992;&#20110;&#30452;&#25509;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#20107;&#25925;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#26159;&#33258;&#21160;&#39550;&#39542;&#30340;&#39318;&#35201;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#27809;&#26377;&#24050;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#25903;&#25345;&#33258;&#21160;&#39550;&#39542;&#30340;&#30452;&#25509;&#21644;&#21487;&#35299;&#37322;&#30340;&#23433;&#20840;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DeepAccident&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#29616;&#23454;&#27169;&#25311;&#22120;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#32463;&#24120;&#22312;&#29616;&#23454;&#39550;&#39542;&#20013;&#21457;&#29983;&#30340;&#21508;&#31181;&#20107;&#25925;&#22330;&#26223;&#12290;DeepAccident &#25968;&#25454;&#38598;&#21253;&#21547; 57k &#20010;&#24102;&#27880;&#37322;&#24103;&#21644; 285k &#20010;&#24102;&#27880;&#37322;&#30340;&#26679;&#26412;&#65292;&#36825;&#20960;&#20046;&#26159;&#22823;&#35268;&#27169; nuScenes &#25968;&#25454;&#38598;&#30340; 7 &#20493;&#65292;&#20854;&#26679;&#26412;&#25968;&#20026; 40k&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#65292;&#21487;&#29992;&#20110;&#30452;&#25509;&#35780;&#20272;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#20107;&#25925;&#39044;&#27979;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27599;&#31181;&#22330;&#26223;&#65292;&#25105;&#20204;&#35774;&#32622;&#20102;&#22235;&#36742;&#36710;&#21644;&#19968;&#20010;&#22522;&#30784;&#35774;&#26045;&#26469;&#35760;&#24405;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#20107;&#25925;&#22330;&#26223;&#25552;&#20379;&#20102;&#22810;&#31181;&#35270;&#35282;&#65292;&#24182;&#20351; V2X&#65288;&#36710;&#36742;&#23545;&#19968;&#20999;&#65289;&#24863;&#30693;&#21644;&#39044;&#27979;&#30740;&#31350;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety is the primary priority of autonomous driving. Nevertheless, no published dataset currently supports the direct and explainable safety evaluation for autonomous driving. In this work, we propose DeepAccident, a large-scale dataset generated via a realistic simulator containing diverse accident scenarios that frequently occur in real-world driving. The proposed DeepAccident dataset contains 57K annotated frames and 285K annotated samples, approximately 7 times more than the large-scale nuScenes dataset with 40k annotated samples. In addition, we propose a new task, end-to-end motion and accident prediction, based on the proposed dataset, which can be used to directly evaluate the accident prediction ability for different autonomous driving algorithms. Furthermore, for each scenario, we set four vehicles along with one infrastructure to record data, thus providing diverse viewpoints for accident scenarios and enabling V2X (vehicle-to-everything) research on perception and predicti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17573</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#23478;&#20013;&#27979;&#37327;&#24085;&#37329;&#26862;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#22312;&#32593;&#32476;&#25668;&#20687;&#22836;&#21069;&#23436;&#25104;&#20102;&#36816;&#21160;&#20219;&#21153;&#65288;&#21363;&#28857;&#20987;&#25163;&#25351;&#65289;&#65292;250&#21517;&#20840;&#29699;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#25353;&#29031;&#36816;&#21160;&#38556;&#30861;&#21327;&#20250;&#32479;&#19968;&#24085;&#37329;&#26862;&#30149;&#35780;&#20998;&#37327;&#34920; (MDS-UPDRS) &#30340;&#26631;&#20934;&#30001;&#19977;&#21517;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#38752;&#24615;&#65292;&#20869;&#37096;&#19968;&#33268;&#24615;&#31995;&#25968;&#65288;ICC&#65289;&#20026;0.88&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#35745;&#31639;&#26426;&#31639;&#27861;&#26469;&#33719;&#24471;&#19982;MDS-UPDRS&#25351;&#21335;&#19968;&#33268;&#19988;&#19982;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#30340;&#23458;&#35266;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#25351;&#26631;&#30340;&#35757;&#32451;&#19979;&#34920;&#29616;&#20248;&#20110;&#19968;&#20010;MDS-UPDRS&#35748;&#35777;&#30340;&#35780;&#20998;&#32773;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.59&#65292;&#32780;&#35780;&#20998;&#32773;&#30340;MAE&#20026;0.79&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#34920;&#29616;&#30053;&#36874;&#20110;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#65288;0.53 MAE&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GRAF&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#32593;&#32476;&#34701;&#21512;&#22312;&#22810;&#20010;&#32593;&#32476;&#19978;&#20351;&#29992;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;attention-based neighborhood aggregation&#65292;GRAF&#33021;&#22815;&#23398;&#20064;&#27599;&#20010;&#33410;&#28857;&#21644;&#20851;&#32852;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#22312;&#32593;&#32476;&#34701;&#21512;&#20013;&#36827;&#34892;&#36793;&#32536;&#21152;&#26435;&#12290;</title><link>http://arxiv.org/abs/2303.16781</link><description>&lt;p&gt;
GRAF&#65306;&#22270;&#24418;&#27880;&#24847;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GRAF: Graph Attention-aware Fusion Networks. (arXiv:2303.16781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GRAF&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#32593;&#32476;&#34701;&#21512;&#22312;&#22810;&#20010;&#32593;&#32476;&#19978;&#20351;&#29992;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;attention-based neighborhood aggregation&#65292;GRAF&#33021;&#22815;&#23398;&#20064;&#27599;&#20010;&#33410;&#28857;&#21644;&#20851;&#32852;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#22312;&#32593;&#32476;&#34701;&#21512;&#20013;&#36827;&#34892;&#36793;&#32536;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#21253;&#21547;&#22810;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#20316;&#20026;&#19968;&#31181;&#21033;&#29992;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#29305;&#24449;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24050;&#32463;&#35777;&#26126;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#27969;&#34892;&#30340;&#22522;&#20110;GNN&#30340;&#26550;&#26500;&#21482;&#33021;&#22788;&#29702;&#19968;&#20010;&#21516;&#26500;&#32593;&#32476;&#12290;&#20351;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#22810;&#20010;&#32593;&#32476;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#32593;&#32476;&#30340;&#24322;&#26500;&#24615;&#21644;&#29616;&#26377;&#20851;&#32852;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GRAF&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#21033;&#29992;GNN&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#32593;&#32476;&#19978;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#32593;&#32476;&#34701;&#21512;&#12290;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#37051;&#22495;&#32858;&#21512;&#65292;GRAF&#23398;&#20064;&#27599;&#20010;&#33410;&#28857;&#30340;&#37051;&#23621;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#65288;&#31216;&#20026;&#33410;&#28857;&#32423;&#27880;&#24847;&#21147;&#65289;&#20197;&#21450;&#20998;&#23618;&#26041;&#24335;&#19979;&#30340;&#20851;&#32852;&#30340;&#37325;&#35201;&#24615;&#65288;&#31216;&#20026;&#20851;&#32852;&#32423;&#27880;&#24847;&#21147;&#65289;&#12290;&#28982;&#21518;&#65292;GRAF&#22788;&#29702;&#19968;&#20010;&#32593;&#32476;&#34701;&#21512;&#27493;&#39588;&#65292;&#26681;&#25454;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#21644;&#20851;&#32852;&#32423;&#27880;&#24847;&#21147;&#21152;&#26435;&#27599;&#26465;&#36793;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large number of real-world networks include multiple types of nodes and edges. Graph Neural Network (GNN) emerged as a deep learning framework to utilize node features on graph-structured data showing superior performance. However, popular GNN-based architectures operate on one homogeneous network. Enabling them to work on multiple networks brings additional challenges due to the heterogeneity of the networks and the multiplicity of the existing associations. In this study, we present a computational approach named GRAF utilizing GNN-based approaches on multiple networks with the help of attention mechanisms and network fusion. Using attention-based neighborhood aggregation, GRAF learns the importance of each neighbor per node (called node-level attention) followed by the importance of association (called association-level attention) in a hierarchical way. Then, GRAF processes a network fusion step weighing each edge according to learned node- and association-level attention, which r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26816;&#27979;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#34987;&#38024;&#23545;&#24615;&#25915;&#20987;&#65292;&#22240;&#27492;&#20445;&#25252;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#32467;&#26524;&#23545;&#29616;&#20195;&#30005;&#21147;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2303.16633</link><description>&lt;p&gt;
&#38024;&#23545;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340;&#26377;&#30446;&#26631;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Targeted Adversarial Attacks on Wind Power Forecasts. (arXiv:2303.16633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#27979;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#34987;&#38024;&#23545;&#24615;&#25915;&#20987;&#65292;&#22240;&#27492;&#20445;&#25252;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#32467;&#26524;&#23545;&#29616;&#20195;&#30005;&#21147;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22810;&#31181;&#29992;&#20110;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25110;&#29289;&#29702;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#39118;&#30005;&#22330;&#25110;&#25972;&#20010;&#22320;&#21306;&#30340;&#39118;&#21147;&#21457;&#30005;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24120;&#24120;&#20250;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#23545;&#29616;&#20195;&#30005;&#21147;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#20445;&#25252;&#39044;&#27979;&#32467;&#26524;&#20813;&#21463;&#36825;&#31181;&#23041;&#32961;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#39044;&#27979;&#27169;&#22411;&#23545;&#26377;&#30446;&#26631;&#12289;&#21322;&#26377;&#30446;&#26631;&#21644;&#26080;&#30446;&#26631;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#39118;&#30005;&#22330;&#21151;&#29575;&#21457;&#30005;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#21644;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#24503;&#22269;&#25972;&#20010;&#22320;&#21306;&#30340;&#39118;&#21147;&#21457;&#30005;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24635;&#23545;&#25239;&#40065;&#26834;&#24615;&#35780;&#20998;&#65288;TARS&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#22238;&#24402;&#27169;&#22411;&#31283;&#20581;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, researchers proposed a variety of deep learning models for wind power forecasting. These models predict the wind power generation of wind farms or entire regions more accurately than traditional machine learning algorithms or physical models. However, latest research has shown that deep learning models can often be manipulated by adversarial attacks. Since wind power forecasts are essential for the stability of modern power systems, it is important to protect them from this threat. In this work, we investigate the vulnerability of two different forecasting models to targeted, semitargeted, and untargeted adversarial attacks. We consider a Long Short-Term Memory (LSTM) network for predicting the power generation of a wind farm and a Convolutional Neural Network (CNN) for forecasting the wind power generation throughout Germany. Moreover, we propose the Total Adversarial Robustness Score (TARS), an evaluation metric for quantifying the robustness of regression models to 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#36827;&#34892;&#20102;&#28145;&#20837;&#23457;&#26597;&#21644;&#24635;&#32467;&#65292;&#36825;&#26159;&#19968;&#31181;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#20581;&#22766;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.16004</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Malware Detection with Graph Representation Learning. (arXiv:2303.16004v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#36827;&#34892;&#20102;&#28145;&#20837;&#23457;&#26597;&#21644;&#24635;&#32467;&#65292;&#36825;&#26159;&#19968;&#31181;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#20581;&#22766;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#36719;&#20214;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#31614;&#21517;&#21644;&#21551;&#21457;&#24335;&#30340;&#26816;&#27979;&#26041;&#27861;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#22312;&#26410;&#30693;&#25915;&#20987;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#28151;&#28102;&#25216;&#26415;&#36731;&#26494;&#35268;&#36991;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#24182;&#19988;&#24050;&#25104;&#20026;&#20256;&#32479;&#26041;&#27861;&#30340;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#36817;&#65292;&#22312;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#25968;&#25454;&#19978;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#24050;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20174;&#24694;&#24847;&#36719;&#20214;&#20013;&#23398;&#20064;&#26356;&#20581;&#22766;&#34920;&#31034;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#20851;&#20110;&#22522;&#20110;&#22270;&#24418;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#28145;&#20837;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#27010;&#25324;&#21644;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#20316;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware detection has become a major concern due to the increasing number and complexity of malware. Traditional detection methods based on signatures and heuristics are used for malware detection, but unfortunately, they suffer from poor generalization to unknown attacks and can be easily circumvented using obfuscation techniques. In recent years, Machine Learning (ML) and notably Deep Learning (DL) achieved impressive results in malware detection by learning useful representations from data and have become a solution preferred over traditional methods. More recently, the application of such techniques on graph-structured data has achieved state-of-the-art performance in various domains and demonstrates promising results in learning more robust representations from malware. Yet, no literature review focusing on graph-based deep learning for malware detection exists. In this survey, we provide an in-depth literature review to summarize and unify existing works under the common approach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25429;&#33719;&#20102;&#39318;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#21830;&#29992;&#35774;&#22791;&#30495;&#23454;&#23556;&#39057;&#25351;&#32441;&#25968;&#25454;&#38598;&#65292;&#36825;&#23545;&#20110;&#35782;&#21035;&#38750;&#27861;&#25110;&#26410;&#25480;&#26435;&#21457;&#23556;&#22120;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.13538</link><description>&lt;p&gt;
&#29992;&#20110;&#21830;&#29992;&#35774;&#22791;&#30495;&#23454;&#23556;&#39057;&#25351;&#32441;&#30340;&#34013;&#29273;&#21644;WiFi&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Bluetooth and WiFi Dataset for Real World RF Fingerprinting of Commercial Devices. (arXiv:2303.13538v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13538
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25429;&#33719;&#20102;&#39318;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#21830;&#29992;&#35774;&#22791;&#30495;&#23454;&#23556;&#39057;&#25351;&#32441;&#25968;&#25454;&#38598;&#65292;&#36825;&#23545;&#20110;&#35782;&#21035;&#38750;&#27861;&#25110;&#26410;&#25480;&#26435;&#21457;&#23556;&#22120;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#39057;&#25351;&#32441;&#20316;&#20026;&#19968;&#31181;&#29289;&#29702;&#23618;&#23433;&#20840;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#22312;&#20849;&#20139;RF&#39057;&#35889;&#30340;&#38750;&#27861;&#25110;&#26410;&#25480;&#26435;&#21457;&#23556;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#36719;&#20214;&#23450;&#20041;&#26080;&#32447;&#30005;&#65288;SDR&#65289;&#29983;&#25104;&#21512;&#25104;&#27874;&#24418;&#65292;&#36825;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#20165;&#20851;&#27880;&#29983;&#25104;&#19968;&#31181;&#27874;&#24418;&#30340;&#33455;&#29255;&#32452;&#12290;&#21830;&#29992;&#29616;&#25104;&#65288;COTS&#65289;&#32452;&#21512;&#33455;&#29255;&#32452;&#25903;&#25345;&#20351;&#29992;&#20849;&#20139;&#21452;&#39057;&#27573;&#22825;&#32447;&#30340;&#20004;&#31181;&#26080;&#32447;&#26631;&#20934;&#65288;&#20363;&#22914;WiFi&#21644;&#34013;&#29273;&#65289;&#65292;&#22914;&#22312;&#31508;&#35760;&#26412;&#30005;&#33041;&#12289;&#36866;&#37197;&#22120;&#12289;&#26080;&#32447;&#20805;&#30005;&#22120;&#12289;&#26641;&#33683;&#27966;&#31561;IoT&#35774;&#22791;&#20013;&#24191;&#27867;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#20026;&#36319;&#19978;&#29616;&#20195;IoT&#29615;&#22659;&#30340;&#27493;&#20240;&#65292;&#36843;&#20999;&#38656;&#35201;&#25429;&#33719;&#36825;&#20123;&#32452;&#21512;&#33455;&#29255;&#32452;&#21457;&#23556;&#24322;&#26500;&#36890;&#20449;&#21327;&#35758;&#30340;&#30495;&#23454;&#19990;&#30028;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25429;&#33719;&#20102;&#31532;&#19968;&#20010;&#24050;&#30693;&#30340;&#27492;&#31867;&#20844;&#24320;&#21487;&#35775;&#38382;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#21830;&#19994;&#35774;&#22791;&#30340;COTS Wi-Fi&#21644;&#34013;&#29273;&#32452;&#21512;&#33455;&#29255;&#32452;&#30340;RF&#21457;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
RF fingerprinting is emerging as a physical layer security scheme to identify illegitimate and/or unauthorized emitters sharing the RF spectrum. However, due to the lack of publicly accessible real-world datasets, most research focuses on generating synthetic waveforms with software-defined radios (SDRs) which are not suited for practical deployment settings. On other hand, the limited datasets that are available focus only on chipsets that generate only one kind of waveform. Commercial off-the-shelf (COTS) combo chipsets that support two wireless standards (for example WiFi and Bluetooth) over a shared dual-band antenna such as those found in laptops, adapters, wireless chargers, Raspberry Pis, among others are becoming ubiquitous in the IoT realm. Hence, to keep up with the modern IoT environment, there is a pressing need for real-world open datasets capturing emissions from these combo chipsets transmitting heterogeneous communication protocols. To this end, we capture the first kno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#22522;&#20110;&#25193;&#25955;&#24335;&#20928;&#21270;&#26041;&#27861;&#30340;&#35780;&#20272;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#20197;&#34913;&#37327;&#20928;&#21270;&#26041;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20928;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.09051</link><description>&lt;p&gt;
&#25193;&#25955;&#24335;&#23545;&#25239;&#20928;&#21270;&#30340;&#40065;&#26834;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robust Evaluation of Diffusion-Based Adversarial Purification. (arXiv:2303.09051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#22522;&#20110;&#25193;&#25955;&#24335;&#20928;&#21270;&#26041;&#27861;&#30340;&#35780;&#20272;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#20197;&#34913;&#37327;&#20928;&#21270;&#26041;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20928;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36136;&#30097;&#24403;&#21069;&#23545;&#22522;&#20110;&#25193;&#25955;&#24335;&#20928;&#21270;&#26041;&#27861;&#30340;&#35780;&#20272;&#26041;&#24335;&#12290;&#25193;&#25955;&#24335;&#20928;&#21270;&#26041;&#27861;&#26088;&#22312;&#28040;&#38500;&#27979;&#35797;&#25968;&#25454;&#28857;&#20013;&#30340;&#23545;&#25239;&#24615;&#24433;&#21709;&#12290;&#30001;&#20110;&#22522;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#35299;&#32806;&#65292;&#35813;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20316;&#20026;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#20102;&#27979;&#37327;&#20928;&#21270;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#24120;&#37319;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340;&#30333;&#30418;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#25915;&#20987;&#36890;&#24120;&#26159;&#20026;&#23545;&#25239;&#24615;&#35757;&#32451;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#65292;&#22240;&#27492;&#19981;&#30693;&#36947;&#36825;&#20123;&#25915;&#20987;&#26159;&#21542;&#23545;&#25193;&#25955;&#24335;&#20928;&#21270;&#26368;&#26377;&#25928;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#21069;&#30340;&#23454;&#36341;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#20197;&#34913;&#37327;&#20928;&#21270;&#26041;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20928;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We question the current evaluation practice on diffusion-based purification methods. Diffusion-based purification methods aim to remove adversarial effects from an input data point at test time. The approach gains increasing attention as an alternative to adversarial training due to the disentangling between training and testing. Well-known white-box attacks are often employed to measure the robustness of the purification. However, it is unknown whether these attacks are the most effective for the diffusion-based purification since the attacks are often tailored for adversarial training. We analyze the current practices and provide a new guideline for measuring the robustness of purification methods against adversarial attacks. Based on our analysis, we further propose a new purification strategy showing competitive results against the state-of-the-art adversarial training approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08010</link><description>&lt;p&gt;
&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#32423;&#32852;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65306;&#24403;&#28145;&#24230;&#38598;&#25104;&#27604;&#21333;&#19968;&#27169;&#22411;&#26356;&#26377;&#25928;&#26102;
&lt;/p&gt;
&lt;p&gt;
Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models. (arXiv:2303.08010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#31616;&#21333;&#12289;&#21487;&#38752;&#21644;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#37096;&#32626;&#22810;&#20010;&#29420;&#31435;&#27169;&#22411;&#65292;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#35745;&#31639;&#24320;&#38144;&#22823;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25361;&#25112;&#20102;&#36825;&#31181;&#35266;&#28857;&#65292;&#34920;&#26126;&#23545;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#38598;&#25104;&#21487;&#20197;&#27604;&#22312;&#21516;&#19968;&#26550;&#26500;&#26063;&#20013;&#32553;&#25918;&#21333;&#19968;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#36890;&#36807;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#32423;&#32852;&#38598;&#25104;&#25104;&#21592;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#23558;&#36825;&#20123;&#25928;&#29575;&#25552;&#39640;&#25193;&#23637;&#21040;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#35768;&#22810;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#37117;&#26159;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#26032;&#39062;&#35265;&#35299;&#26159;&#20165;&#23558;&#25509;&#36817;&#20108;&#20998;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#20256;&#36882;&#21040;&#21518;&#32493;&#32423;&#32852;&#38454;&#27573;&#12290;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#22312;&#20351;&#29992;&#27604;&#22522;&#32447;&#26356;&#23569;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#19982;&#23436;&#25972;&#38598;&#25104;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles are a simple, reliable, and effective method of improving both the predictive performance and uncertainty estimates of deep learning approaches. However, they are widely criticised as being computationally expensive, due to the need to deploy multiple independent models. Recent work has challenged this view, showing that for predictive accuracy, ensembles can be more computationally efficient (at inference) than scaling single models within an architecture family. This is achieved by cascading ensemble members via an early-exit approach. In this work, we investigate extending these efficiency gains to tasks related to uncertainty estimation. As many such tasks, e.g. selective classification, are binary classification, our key novel insight is to only pass samples within a window close to the binary decision boundary to later cascade stages. Experiments on ImageNet-scale data across a number of network architectures and uncertainty tasks show that the proposed window-base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DGAI&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22909;&#25163;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#24335;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#32473;&#23450;&#38408;&#20540;&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07154</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#22909;&#25163;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Differential Good Arm Identification. (arXiv:2303.07154v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DGAI&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22909;&#25163;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#24335;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#32473;&#23450;&#38408;&#20540;&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#31181;&#21464;&#20307;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#31216;&#20043;&#20026;&#22909;&#25163;&#33218;&#35782;&#21035;&#65288;GAI&#65289;&#12290; GAI&#26159;&#19968;&#20010;&#32431;&#25506;&#32034;&#30340;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#23613;&#21487;&#33021;&#23569;&#30340;&#26679;&#26412;&#25968;&#19979;&#36755;&#20986;&#23613;&#21487;&#33021;&#22810;&#30340;&#22909;&#25163;&#33218;&#65292;&#20854;&#20013;&#22909;&#25163;&#33218;&#34987;&#23450;&#20041;&#20026;&#20854;&#26399;&#26395;&#22870;&#21169;&#22823;&#20110;&#32473;&#23450;&#38408;&#20540;&#30340;&#25163;&#33218;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;DGAI-&#19968;&#31181;&#21487;&#24494;&#30340;&#22909;&#25163;&#33218;&#35782;&#21035;&#31639;&#27861;&#65292;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;HDoC&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290; &#25105;&#20204;&#36824;&#23637;&#31034;&#20102;DGAI&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;&#36890;&#29992;&#22810;&#33218;&#36172;&#21338;&#65288;MAB&#65289;&#38382;&#39064;&#30340;&#24615;&#33021;&#65292;&#32473;&#23450;&#19968;&#20010;&#38408;&#20540;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#24212;&#29992;&#20110;&#25163;&#33218;&#38598;&#12290; &#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;GAI&#21644;MAB&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper targets a variant of the stochastic multi-armed bandit problem called good arm identification (GAI). GAI is a pure-exploration bandit problem with the goal to output as many good arms using as few samples as possible, where a good arm is defined as an arm whose expected reward is greater than a given threshold. In this work, we propose DGAI - a differentiable good arm identification algorithm to improve the sample complexity of the state-of-the-art HDoC algorithm in a data-driven fashion. We also showed that the DGAI can further boost the performance of a general multi-arm bandit (MAB) problem given a threshold as a prior knowledge to the arm set. Extensive experiments confirm that our algorithm outperform the baseline algorithms significantly in both synthetic and real world datasets for both GAI and MAB tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65306;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TARGET&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#26469;&#20943;&#36731;FCCL&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#20256;&#36882;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#22120;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#27169;&#25311;&#25968;&#25454;&#30340;&#20840;&#23616;&#20998;&#24067;&#12290;&#19982;&#20808;&#21069;&#30340;FCCL&#26041;&#27861;&#30456;&#27604;&#65292;TARGET&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.06937</link><description>&lt;p&gt;
TARGET: &#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#23454;&#29616;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation. (arXiv:2303.06937v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65306;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TARGET&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#26469;&#20943;&#36731;FCCL&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#20256;&#36882;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#22120;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#27169;&#25311;&#25968;&#25454;&#30340;&#20840;&#23616;&#20998;&#24067;&#12290;&#19982;&#20808;&#21069;&#30340;FCCL&#26041;&#27861;&#30456;&#27604;&#65292;TARGET&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#40092;&#20026;&#20154;&#30693;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65306;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65288;FCCL&#65289;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#12290;&#24050;&#26377;&#30340;FCCL&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#22914;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21152;&#21095;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;TARGET&#65288;&#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#23454;&#29616;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#36731;FCCL&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#21516;&#26102;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#23558;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#24403;&#21069;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#27169;&#25311;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#25968;&#25454;&#30340;&#20840;&#23616;&#20998;&#24067;&#12290;&#19982;&#20808;&#21069;&#30340;FCCL&#26041;&#27861;&#30456;&#27604;&#65292;TARGET&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on an under-explored yet important problem: Federated Class-Continual Learning (FCCL), where new classes are dynamically added in federated learning. Existing FCCL works suffer from various limitations, such as requiring additional datasets or storing the private data from previous tasks. In response, we first demonstrate that non-IID data exacerbates catastrophic forgetting issue in FL. Then we propose a novel method called TARGET (federat\textbf{T}ed cl\textbf{A}ss-continual lea\textbf{R}nin\textbf{G} via \textbf{E}xemplar-free dis\textbf{T}illation), which alleviates catastrophic forgetting in FCCL while preserving client data privacy. Our proposed method leverages the previously trained global model to transfer knowledge of old tasks to the current task at the model level. Moreover, a generator is trained to produce synthetic data to simulate the global distribution of data on each client at the data level. Compared to previous FCCL methods, TARGET does not requi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#21457;&#29616;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#20294;&#36890;&#36807;&#23545;&#24494;&#35843;&#25968;&#25454;&#38598;&#36827;&#34892;&#24178;&#39044;&#21487;&#20197;&#32416;&#27491;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;&#36825;&#34920;&#26126;&#20180;&#32454;&#31574;&#21010;&#24494;&#35843;&#25968;&#25454;&#38598;&#23545;&#20110;&#20943;&#23569;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20559;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#26679;&#20570;&#29978;&#33267;&#21487;&#20197;&#24357;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2303.06167</link><description>&lt;p&gt;
&#36890;&#36807;&#25805;&#20316;&#24494;&#35843;&#25968;&#25454;&#38598;&#26469;&#20811;&#26381;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Overcoming Bias in Pretrained Models by Manipulating the Finetuning Dataset. (arXiv:2303.06167v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#21457;&#29616;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#20294;&#36890;&#36807;&#23545;&#24494;&#35843;&#25968;&#25454;&#38598;&#36827;&#34892;&#24178;&#39044;&#21487;&#20197;&#32416;&#27491;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;&#36825;&#34920;&#26126;&#20180;&#32454;&#31574;&#21010;&#24494;&#35843;&#25968;&#25454;&#38598;&#23545;&#20110;&#20943;&#23569;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20559;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#26679;&#20570;&#29978;&#33267;&#21487;&#20197;&#24357;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the bias problem in pretrained models and finds that finetuned models can inherit the biases of pretrained models, but these biases can be corrected by manipulating the finetuning dataset with little impact on performance. This implies that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.
&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#36890;&#36807;&#20801;&#35768;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#34920;&#36798;&#29305;&#24449;&#34987;&#24494;&#35843;&#21040;&#26356;&#23567;&#12289;&#26356;&#20855;&#39046;&#22495;&#29305;&#23450;&#24615;&#30340;&#25968;&#25454;&#38598;&#30340;&#30446;&#26631;&#20219;&#21153;&#20013;&#32780;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#26377;&#20154;&#25285;&#24515;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#24102;&#26377;&#33258;&#24049;&#30340;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#20250;&#20256;&#25773;&#21040;&#24494;&#35843;&#27169;&#22411;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20559;&#35265;&#65292;&#24403;&#20559;&#35265;&#34987;&#27010;&#24565;&#21270;&#20026;&#30446;&#26631;&#20219;&#21153;&#21644;&#25935;&#24863;&#23646;&#24615;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#25454;&#38598;&#20013;&#29305;&#23450;&#32676;&#20307;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#26102;&#12290;&#22312;&#20559;&#35265;&#30340;&#20004;&#31181;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;(1)&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#30830;&#23454;&#21487;&#20197;&#32487;&#25215;&#23427;&#20204;&#30340;&#20559;&#35265;&#65292;&#20294;(2)&#36890;&#36807;&#23545;&#24494;&#35843;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#23545;&#36739;&#23567;&#30340;&#24178;&#39044;&#65292;&#36825;&#31181;&#20559;&#35265;&#21487;&#20197;&#24471;&#21040;&#32416;&#27491;&#65292;&#32780;&#19988;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24448;&#24448;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24847;&#21619;&#30528;&#65292;&#20180;&#32454;&#31574;&#21010;&#24494;&#35843;&#25968;&#25454;&#38598;&#23545;&#20110;&#20943;&#23569;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20559;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#26679;&#20570;&#29978;&#33267;&#21487;&#20197;&#24357;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is beneficial by allowing the expressive features of models pretrained on large-scale datasets to be finetuned for the target task of smaller, more domain-specific datasets. However, there is a concern that these pretrained models may come with their own biases which would propagate into the finetuned model. In this work, we investigate bias when conceptualized as both spurious correlations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. Under both notions of bias, we find that (1) models finetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the finetuning dataset, and often with a negligible impact to performance. Our findings imply that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;&#12290;</title><link>http://arxiv.org/abs/2303.00396</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning. (arXiv:2303.00396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20219;&#21153;&#65292;&#23398;&#20064;&#29305;&#23450;&#20110;&#24207;&#25968;&#20998;&#31867;&#30340;&#33391;&#22909;&#32467;&#26500;&#21270;&#29305;&#24449;&#31354;&#38388;&#26377;&#21161;&#20110;&#24688;&#24403;&#22320;&#25429;&#25417;&#31867;&#20043;&#38388;&#30340;&#24207;&#25968;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20026;&#27599;&#20010;&#24207;&#25968;&#31867;&#23398;&#20064;&#19968;&#20010;&#20195;&#29702;&#65292;&#28982;&#21518;&#36890;&#36807;&#38480;&#21046;&#36825;&#20123;&#20195;&#29702;&#26469;&#35843;&#25972;&#31867;&#30340;&#20840;&#23616;&#24067;&#23616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#30828;&#24067;&#23616;&#32422;&#26463;&#21644;&#36719;&#24067;&#23616;&#32422;&#26463;&#12290;&#30828;&#24067;&#23616;&#32422;&#26463;&#36890;&#36807;&#30452;&#25509;&#25511;&#21046;&#20195;&#29702;&#30340;&#29983;&#25104;&#26469;&#23454;&#29616;&#65292;&#20197;&#24378;&#21046;&#23558;&#20854;&#25918;&#32622;&#22312;&#20005;&#26684;&#30340;&#32447;&#24615;&#24067;&#23616;&#25110;&#21322;&#22278;&#24418;&#24067;&#23616;&#65288;&#21363;&#20005;&#26684;&#24207;&#25968;&#24067;&#23616;&#30340;&#20004;&#31181;&#23454;&#20363;&#65289;&#20013;&#12290;&#36719;&#24067;&#23616;&#32422;&#26463;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#26469;&#23454;&#29616;&#65292;&#35813;&#39033;&#24809;&#32602;&#20559;&#31163;&#29702;&#24819;&#24207;&#25968;&#24067;&#23616;&#30340;&#24773;&#20917;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;CPL&#26041;&#27861;&#22312;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For deep ordinal classification, learning a well-structured feature space specific to ordinal classification is helpful to properly capture the ordinal nature among classes. Intuitively, when Euclidean distance metric is used, an ideal ordinal layout in feature space would be that the sample clusters are arranged in class order along a straight line in space. However, enforcing samples to conform to a specific layout in the feature space is a challenging problem. To address this problem, in this paper, we propose a novel Constrained Proxies Learning (CPL) method, which can learn a proxy for each ordinal class and then adjusts the global layout of classes by constraining these proxies. Specifically, we propose two kinds of strategies: hard layout constraint and soft layout constraint. The hard layout constraint is realized by directly controlling the generation of proxies to force them to be placed in a strict linear layout or semicircular layout (i.e., two instantiations of strict ordi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;Transformer&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19978;&#22788;&#29702;&#22810;&#27169;&#24577;&#21333;&#32454;&#32990;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#19979;&#28216;&#20219;&#21153;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;scMoFormer&#30340;&#26694;&#26550;</title><link>http://arxiv.org/abs/2303.00233</link><description>&lt;p&gt;
&#21333;&#32454;&#32990;&#22810;&#27169;&#24577;&#39044;&#27979;&#30340;Transformer&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Single-Cell Multimodal Prediction via Transformers. (arXiv:2303.00233v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;Transformer&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19978;&#22788;&#29702;&#22810;&#27169;&#24577;&#21333;&#32454;&#32990;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#19979;&#28216;&#20219;&#21153;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;scMoFormer&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#21333;&#32454;&#32990;&#25216;&#26415;&#30340;&#21457;&#23637;&#20351;&#24471;&#20174;&#21333;&#20010;&#32454;&#32990;&#20013;&#33719;&#21462;&#22810;&#20010;&#32452;&#23398;&#25968;&#25454;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#32454;&#32990;&#29366;&#24577;&#21644;&#21160;&#24577;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#21333;&#32454;&#32990;&#25968;&#25454;&#30340;&#28608;&#22686;&#20063;&#24102;&#26469;&#20102;&#24314;&#27169;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#26041;&#27861;&#20391;&#37325;&#20110;&#26500;&#24314;&#38745;&#24577;&#20132;&#20114;&#22270;&#65292;&#24182;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#38745;&#24577;&#22270;&#21487;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#19979;&#28216;&#20219;&#21153;&#20449;&#24687;&#65307;&#32780;&#19988;&#65292;&#24403;&#28145;&#24230;&#22534;&#21472;GNN&#23618;&#26102;&#65292;GNNs&#20063;&#26377;&#19968;&#20123;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;Transformer&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19978;&#22788;&#29702;&#22810;&#27169;&#24577;&#21333;&#32454;&#32990;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#19979;&#28216;&#20219;&#21153;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;scMoFormer&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#25972;&#21512;&#22806;&#37096;&#30340;d
&lt;/p&gt;
&lt;p&gt;
The recent development of multimodal single-cell technology has made the possibility of acquiring multiple omics data from individual cells, thereby enabling a deeper understanding of cellular states and dynamics. Nevertheless, the proliferation of multimodal single-cell data also introduces tremendous challenges in modeling the complex interactions among different modalities. The recently advanced methods focus on constructing static interaction graphs and applying graph neural networks (GNNs) to learn from multimodal data. However, such static graphs can be suboptimal as they do not take advantage of the downstream task information; meanwhile GNNs also have some inherent limitations when deeply stacking GNN layers. To tackle these issues, in this work, we investigate how to leverage transformers for multimodal single-cell data in an end-to-end manner while exploiting downstream task information. In particular, we propose a scMoFormer framework which can readily incorporate external d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#22312;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035;&#26102;&#65292;&#26368;&#20248;&#26399;&#26395;&#38169;&#35823;&#36793;&#30028;&#31561;&#20110;&#20854;&#38543;&#26426;&#21270;&#30340;Littlestone&#32500;&#24230;&#12290;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#19982;&#26368;&#20339;&#20989;&#25968;&#30340;&#38169;&#35823;&#27425;&#25968;&#20043;&#38388;&#23384;&#22312;&#29305;&#23450;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35299;&#20915;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13849</link><description>&lt;p&gt;
&#20351;&#29992;&#19987;&#23478;&#24314;&#35758;&#21644;&#38543;&#26426;&#21270;&#30340;Littlestone&#32500;&#24230;&#36827;&#34892;&#26368;&#20248;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Optimal Prediction Using Expert Advice and Randomized Littlestone Dimension. (arXiv:2302.13849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#22312;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035;&#26102;&#65292;&#26368;&#20248;&#26399;&#26395;&#38169;&#35823;&#36793;&#30028;&#31561;&#20110;&#20854;&#38543;&#26426;&#21270;&#30340;Littlestone&#32500;&#24230;&#12290;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#19982;&#26368;&#20339;&#20989;&#25968;&#30340;&#38169;&#35823;&#27425;&#25968;&#20043;&#38388;&#23384;&#22312;&#29305;&#23450;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35299;&#20915;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#32463;&#20856;&#30340;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#30830;&#23450;&#24615;&#23398;&#20064;&#22120;&#30340;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#21487;&#20197;&#36890;&#36807;Littlestone&#32500;&#24230;&#26469;&#23454;&#29616;&#65288;Littlestone '88&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#26426;&#23398;&#20064;&#22120;&#30340;&#31867;&#20284;&#32467;&#26524;&#65306;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035; $\mathcal{H}$&#26102;&#65292;&#26368;&#20248;&#26399;&#26395;&#38169;&#35823;&#36793;&#30028;&#31561;&#20110;&#20854;&#38543;&#26426;&#21270;&#30340;Littlestone&#32500;&#24230;&#65292;&#21363;&#23384;&#22312;&#19968;&#20010;&#30001; $\mathcal{H}$ &#25171;&#30862;&#30340;&#26641;&#65292;&#20854;&#24179;&#22343;&#28145;&#24230;&#20026; $2d$&#65292;&#32780; $d$ &#26159;&#26368;&#22823;&#30340;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#19982; $\mathcal{H}$ &#20013;&#26368;&#20339;&#20989;&#25968;&#30340;&#38169;&#35823;&#27425;&#25968; $k$ &#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;Littlestone&#32500;&#24230; $d$ &#30340;&#31867;&#21035;&#23398;&#20064;&#30340;&#26368;&#20248;&#38543;&#26426;&#21270;&#38169;&#35823;&#36793;&#30028;&#26159; $k + \Theta (\sqrt{k d} + d )$&#12290;&#36825;&#20063;&#24847;&#21619;&#30528;&#30830;&#23450;&#24615;&#23398;&#20064;&#30340;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#26159; $2k + O (\sqrt{k d} + d )$&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;Auer&#21644;Long ['99]&#30740;&#31350;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#20316;&#20026;&#25105;&#20204;&#29702;&#35770;&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#38382;&#39064;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A classical result in online learning characterizes the optimal mistake bound achievable by deterministic learners using the Littlestone dimension (Littlestone '88). We prove an analogous result for randomized learners: we show that the optimal expected mistake bound in learning a class $\mathcal{H}$ equals its randomized Littlestone dimension, which is the largest $d$ for which there exists a tree shattered by $\mathcal{H}$ whose average depth is $2d$. We further study optimal mistake bounds in the agnostic case, as a function of the number of mistakes made by the best function in $\mathcal{H}$, denoted by $k$. We show that the optimal randomized mistake bound for learning a class with Littlestone dimension $d$ is $k + \Theta (\sqrt{k d} + d )$. This also implies an optimal deterministic mistake bound of $2k + O (\sqrt{k d} + d )$, thus resolving an open question which was studied by Auer and Long ['99].  As an application of our theory, we revisit the classical problem of prediction 
&lt;/p&gt;</description></item><item><title>&#21435;&#22122;&#25193;&#25955;&#37319;&#26679;&#22120; (DDS) &#36817;&#20284;&#22320;&#20174;&#38750;&#26631;&#20934;&#21270;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#37319;&#26679;&#65292;&#24182;&#20272;&#35745;&#20854;&#26631;&#20934;&#21270;&#24120;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.13834</link><description>&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Samplers. (arXiv:2302.13834v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13834
&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#37319;&#26679;&#22120; (DDS) &#36817;&#20284;&#22320;&#20174;&#38750;&#26631;&#20934;&#21270;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#37319;&#26679;&#65292;&#24182;&#20272;&#35745;&#20854;&#26631;&#20934;&#21270;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#36880;&#28176;&#21521;&#25968;&#25454;&#28155;&#21152;&#22122;&#22768;&#65292;&#23558;&#25968;&#25454;&#20998;&#24067;&#36716;&#21270;&#20026;&#39640;&#26031;&#20998;&#24067;&#12290;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#36890;&#36807;&#27169;&#25311;&#35813;&#25193;&#25955;&#30340;&#26102;&#38388;&#21453;&#28436;&#30340;&#36817;&#20284;&#65292;&#24182;&#21021;&#22987;&#21270;&#20026;&#39640;&#26031;&#26679;&#26412;&#26469;&#33719;&#24471;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#35780;&#20998;&#21305;&#37197;&#25216;&#26415;&#23545;&#26102;&#38388;&#21453;&#28436;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#26840;&#25163;&#30340;&#35780;&#20998;&#39033;&#36827;&#34892;&#36817;&#20284;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25506;&#32034;&#20102;&#19968;&#31181;&#31867;&#20284;&#30340;&#24819;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#37319;&#26679;&#38750;&#26631;&#20934;&#21270;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#24182;&#20272;&#35745;&#20854;&#26631;&#20934;&#21270;&#24120;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#30446;&#26631;&#23494;&#24230;&#21521;&#39640;&#26031;&#25193;&#25955;&#30340;&#36807;&#31243;&#12290;&#21435;&#22122;&#25193;&#25955;&#37319;&#26679;&#22120; (DDS) &#26159;&#36890;&#36807;&#36817;&#20284;&#30456;&#24212;&#30340;&#26102;&#38388;&#21453;&#28436;&#32780;&#33719;&#24471;&#30340;&#12290;&#23613;&#31649;&#35780;&#20998;&#21305;&#37197;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#65292;&#20294;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#22312; Monte Carlo &#37319;&#26679;&#20013;&#24341;&#20837;&#30340;&#35768;&#22810;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models are a popular class of generative models providing state-of-the-art results in many domains. One adds gradually noise to data using a diffusion to transform the data distribution into a Gaussian distribution. Samples from the generative model are then obtained by simulating an approximation of the time-reversal of this diffusion initialized by Gaussian samples. Practically, the intractable score terms appearing in the time-reversed process are approximated using score matching techniques. We explore here a similar idea to sample approximately from unnormalized probability density functions and estimate their normalizing constants. We consider a process where the target density diffuses towards a Gaussian. Denoising Diffusion Samplers (DDS) are obtained by approximating the corresponding time-reversal. While score matching is not applicable in this context, we can leverage many of the ideas introduced in generative modeling for Monte Carlo sampling. Existing t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#21508;&#31181;&#26426;&#21046;&#65292;&#23545;20&#20010;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#22312;12&#20010;&#39046;&#22495;&#20869;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#31639;&#27861;&#21482;&#33021;&#24212;&#23545;&#26576;&#20123;&#36716;&#21464;&#65292;&#36827;&#19968;&#27493;&#22320;&#65292;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#36873;&#25321;&#26631;&#20934;&#26469;&#25913;&#21892;&#29616;&#26377;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12254</link><description>&lt;p&gt;
&#25913;&#21464;&#24456;&#38590;&#65306;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#28145;&#20837;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Change is Hard: A Closer Look at Subpopulation Shift. (arXiv:2302.12254v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#21508;&#31181;&#26426;&#21046;&#65292;&#23545;20&#20010;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#22312;12&#20010;&#39046;&#22495;&#20869;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#31639;&#27861;&#21482;&#33021;&#24212;&#23545;&#26576;&#20123;&#36716;&#21464;&#65292;&#36827;&#19968;&#27493;&#22320;&#65292;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#36873;&#25321;&#26631;&#20934;&#26469;&#25913;&#21892;&#29616;&#26377;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#23376;&#32676;&#20307;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23548;&#33268;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#26426;&#21046;&#20197;&#21450;&#31639;&#27861;&#22312;&#22914;&#27492;&#19981;&#21516;&#30340;&#36716;&#21464;&#20013;&#22914;&#20309;&#36827;&#34892;&#26222;&#36941;&#21270;&#65292;&#25105;&#20204;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#23376;&#32676;&#20307;&#36716;&#21464;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#21078;&#26512;&#21644;&#35299;&#37322;&#23376;&#32676;&#20307;&#20013;&#30340;&#24120;&#35265;&#36716;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#21307;&#30103;&#39046;&#22495;&#30340;12&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23545;20&#20010;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#35757;&#32451;10,000&#22810;&#20010;&#27169;&#22411;&#24471;&#21040;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#36827;&#23637;&#30340;&#26377;&#36259;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#31639;&#27861;&#20165;&#33021;&#22312;&#26576;&#20123;&#31867;&#22411;&#30340;&#36716;&#21464;&#19978;&#25552;&#39640;&#23376;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#22312;&#20854;&#20182;&#31867;&#22411;&#30340;&#36716;&#21464;&#19978;&#21017;&#19981;&#33021;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#24403;&#21069;&#31639;&#27861;&#20381;&#36182;&#20110;&#32676;&#20307;&#26631;&#27880;&#30340;&#39564;&#35777;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#26368;&#24046;&#31867;&#21035;&#20934;&#30830;&#24230;&#30340;&#31616;&#21333;&#36873;&#25321;&#26631;&#20934;&#20854;&#23454;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often perform poorly on subgroups that are underrepresented in the training data. Yet, little is understood on the variation in mechanisms that cause subpopulation shifts, and how algorithms generalize across such diverse shifts at scale. In this work, we provide a fine-grained analysis of subpopulation shift. We first propose a unified framework that dissects and explains common shifts in subgroups. We then establish a comprehensive benchmark of 20 state-of-the-art algorithms evaluated on 12 real-world datasets in vision, language, and healthcare domains. With results obtained from training over 10,000 models, we reveal intriguing observations for future progress in this space. First, existing algorithms only improve subgroup robustness over certain types of shifts but not others. Moreover, while current algorithms rely on group-annotated validation data for model selection, we find that a simple selection criterion based on worst-class accuracy is surprisingly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#30340;&#22495;&#22806;&#26816;&#27979;&#26041;&#27861;LMD&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#22270;&#20687;&#20174;&#21407;&#22987;&#27969;&#24418;&#25260;&#21319;&#24182;&#26144;&#23556;&#21040;&#22495;&#20869;&#27969;&#24418;&#65292;&#20174;&#32780;&#35782;&#21035;&#20986;&#22495;&#22806;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2302.10326</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22495;&#22806;&#26816;&#27979;&#19982;&#25193;&#25955;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Out-of-Distribution Detection with Diffusion Inpainting. (arXiv:2302.10326v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#30340;&#22495;&#22806;&#26816;&#27979;&#26041;&#27861;LMD&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#22270;&#20687;&#20174;&#21407;&#22987;&#27969;&#24418;&#25260;&#21319;&#24182;&#26144;&#23556;&#21040;&#22495;&#20869;&#27969;&#24418;&#65292;&#20174;&#32780;&#35782;&#21035;&#20986;&#22495;&#22806;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22495;&#22806;&#26816;&#27979;(OOD)&#36890;&#36807;&#20165;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#22495;&#20869;&#25968;&#25454;&#26469;&#35782;&#21035;&#22495;&#22806;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Lift, Map, Detect (LMD)&#65292;&#23427;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#31181;&#31867;&#22411;&#12290;&#22312;&#26680;&#24515;&#19978;&#65292;&#23427;&#20204;&#23398;&#20064;&#20102;&#19968;&#31181;&#36880;&#27493;&#21435;&#22122;&#30340;&#36807;&#31243;&#65292;&#36880;&#28176;&#23558;&#22122;&#22768;&#22270;&#20687;&#26144;&#23556;&#21040;&#23427;&#20204;&#30340;&#35757;&#32451;&#27969;&#24418;&#19978;&#12290;LMD&#21033;&#29992;&#36825;&#31181;&#30452;&#35273;&#36827;&#34892;&#22495;&#22806;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LMD&#36890;&#36807;&#30772;&#22351;&#22270;&#20687;&#26469;&#23558;&#20854;&#20174;&#21407;&#22987;&#27969;&#24418;&#19978;&#25260;&#21319;&#65292;&#24182;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23558;&#20854;&#26144;&#23556;&#21040;&#22495;&#20869;&#27969;&#24418;&#19978;&#12290;&#23545;&#20110;&#22495;&#22806;&#22270;&#20687;&#65292;&#26144;&#23556;&#21518;&#30340;&#22270;&#20687;&#23558;&#31163;&#20854;&#21407;&#22987;&#27969;&#24418;&#24456;&#36828;&#65292;LMD&#20250;&#30456;&#24212;&#22320;&#23558;&#20854;&#35782;&#21035;&#20026;&#22495;&#22806;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;LMD&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/zhenzhel/lift_map_detect&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised out-of-distribution detection (OOD) seeks to identify out-of-domain data by learning only from unlabeled in-domain data. We present a novel approach for this task - Lift, Map, Detect (LMD) - that leverages recent advancement in diffusion models. Diffusion models are one type of generative models. At their core, they learn an iterative denoising process that gradually maps a noisy image closer to their training manifolds. LMD leverages this intuition for OOD detection. Specifically, LMD lifts an image off its original manifold by corrupting it, and maps it towards the in-domain manifold with a diffusion model. For an out-of-domain image, the mapped image would have a large distance away from its original manifold, and LMD would identify it as OOD accordingly. We show through extensive experiments that LMD achieves competitive performance across a broad variety of datasets. Code can be found at https://github.com/zhenzhel/lift_map_detect.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26102;&#38388;&#23610;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#39034;&#24207;&#23398;&#20064;&#30340;&#26041;&#24335;&#26356;&#26032;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#21487;&#20197;&#20445;&#35777;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02792</link><description>&lt;p&gt;
&#22810;&#26102;&#38388;&#23610;&#24230;&#23398;&#20064;&#22312;&#22788;&#29702;&#20998;&#24067;&#24335;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dealing With Non-stationarity in Decentralized Cooperative Multi-Agent Deep Reinforcement Learning via Multi-Timescale Learning. (arXiv:2302.02792v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26102;&#38388;&#23610;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#39034;&#24207;&#23398;&#20064;&#30340;&#26041;&#24335;&#26356;&#26032;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#21487;&#20197;&#20445;&#35777;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26080;&#27861;&#36827;&#34892;&#38598;&#20013;&#24335;&#35757;&#32451;&#25110;&#19981;&#23454;&#38469;&#30340;&#22330;&#26223;&#12290;&#22312;&#20998;&#24067;&#24335;&#28145;&#24230;MARL&#20013;&#65292;&#24403;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#23398;&#20064;&#26102;&#65292;&#23398;&#20064;&#29615;&#22659;&#30340;&#38750;&#24179;&#31283;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#19968;&#20010;&#24120;&#29992;&#19988;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;MARL&#26041;&#26696;&#26159;&#29420;&#31435;&#23398;&#20064;&#65292;&#22312;&#36825;&#31181;&#26041;&#26696;&#20013;&#65292;&#26234;&#33021;&#20307;&#29420;&#31435;&#26356;&#26032;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#29420;&#31435;&#23398;&#20064;&#24182;&#38750;&#24635;&#33021;&#25910;&#25947;&#65292;&#32780;&#39034;&#24207;&#23398;&#20064;&#65292;&#21363;&#26234;&#33021;&#20307;&#20381;&#27425;&#26356;&#26032;&#31574;&#30053;&#65292;&#33021;&#22815;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#26234;&#33021;&#20307;&#26368;&#20248;&#35299;&#12290;&#22312;&#39034;&#24207;&#23398;&#20064;&#20013;&#65292;&#24403;&#19968;&#20010;&#26234;&#33021;&#20307;&#26356;&#26032;&#31574;&#30053;&#26102;&#65292;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#20445;&#25345;&#19981;&#21464;&#65292;&#32531;&#35299;&#20102;&#30001;&#20110;&#20854;&#20182;&#26234;&#33021;&#20307;&#31574;&#30053;&#30340;&#21516;&#26102;&#26356;&#26032;&#32780;&#23548;&#33268;&#30340;&#38750;&#24179;&#31283;&#24615;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#39034;&#24207;&#23398;&#20064;&#36895;&#24230;&#36739;&#24930;&#65292;&#22240;&#20026;&#21482;&#26377;&#19968;&#20010;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized cooperative multi-agent deep reinforcement learning (MARL) can be a versatile learning framework, particularly in scenarios where centralized training is either not possible or not practical. One of the critical challenges in decentralized deep MARL is the non-stationarity of the learning environment when multiple agents are learning concurrently. A commonly used and efficient scheme for decentralized MARL is independent learning in which agents concurrently update their policies independently of each other. We first show that independent learning does not always converge, while sequential learning where agents update their policies one after another in a sequence is guaranteed to converge to an agent-by-agent optimal solution. In sequential learning, when one agent updates its policy, all other agent's policies are kept fixed, alleviating the challenge of non-stationarity due to simultaneous updates in other agents' policies. However, it can be slow because only one agen
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20102;762&#20010;&#24050;&#30830;&#35748;&#30340;&#22806;&#34892;&#26143;&#21644;&#20843;&#20010;&#22826;&#38451;&#31995;&#34892;&#26143;&#30340;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#24040;&#22823;&#34892;&#26143;&#20855;&#26377;&#36739;&#20302;&#30340;&#23494;&#24230;&#65292;&#20027;&#35201;&#30001;&#27682;&#21644;&#27686;&#26500;&#25104;&#65292;&#32780;&#23567;&#34892;&#26143;&#26356;&#23494;&#38598;&#65292;&#20027;&#35201;&#30001;&#26356;&#37325;&#30340;&#20803;&#32032;&#26500;&#25104;&#12290;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#36136;&#37327;&#12289;&#36712;&#36947;&#21608;&#26399;&#21644;&#24658;&#26143;&#37329;&#23646;&#20016;&#24230;&#23545;&#22806;&#34892;&#26143;&#21322;&#24452;&#30340;&#39044;&#27979;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.07143</link><description>&lt;p&gt;
&#37325;&#28201;&#22806;&#34892;&#26143;&#31181;&#32676;&#30340;&#36136;&#37327;-&#21322;&#24452;&#20851;&#31995;&#65306;&#26426;&#22120;&#23398;&#20064;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revisiting mass-radius relationships for exoplanet populations: a machine learning insight. (arXiv:2301.07143v2 [astro-ph.EP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20102;762&#20010;&#24050;&#30830;&#35748;&#30340;&#22806;&#34892;&#26143;&#21644;&#20843;&#20010;&#22826;&#38451;&#31995;&#34892;&#26143;&#30340;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#24040;&#22823;&#34892;&#26143;&#20855;&#26377;&#36739;&#20302;&#30340;&#23494;&#24230;&#65292;&#20027;&#35201;&#30001;&#27682;&#21644;&#27686;&#26500;&#25104;&#65292;&#32780;&#23567;&#34892;&#26143;&#26356;&#23494;&#38598;&#65292;&#20027;&#35201;&#30001;&#26356;&#37325;&#30340;&#20803;&#32032;&#26500;&#25104;&#12290;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#36136;&#37327;&#12289;&#36712;&#36947;&#21608;&#26399;&#21644;&#24658;&#26143;&#37329;&#23646;&#20016;&#24230;&#23545;&#22806;&#34892;&#26143;&#21322;&#24452;&#30340;&#39044;&#27979;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#34892;&#26143;&#30340;&#21457;&#29616;&#25968;&#37327;&#27491;&#22312;&#22686;&#38271;&#65292;&#24182;&#19988;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#20026;&#25506;&#32034;&#21644;&#29702;&#35299;&#25105;&#20204;&#22826;&#38451;&#31995;&#20197;&#22806;&#30340;&#19990;&#30028;&#30340;&#29305;&#24615;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#20998;&#26512;&#21253;&#25324;762&#20010;&#24050;&#30830;&#35748;&#30340;&#22806;&#34892;&#26143;&#21644;&#20843;&#20010;&#22826;&#38451;&#31995;&#34892;&#26143;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#34920;&#24449;&#23427;&#20204;&#30340;&#22522;&#26412;&#24615;&#36136;&#12290;&#36890;&#36807;&#24212;&#29992;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#20998;&#25104;&#20102;&#20004;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#8216;&#23567;&#8217;&#34892;&#26143;&#21644;&#8216;&#24040;&#22823;&#8217;&#34892;&#26143;&#65292;&#20999;&#21106;&#20540;&#20998;&#21035;&#20026;$R_{p}=8.13R_{\oplus}$&#21644;$M_{p}=52.48M_{\oplus}$&#12290;&#36825;&#31181;&#20998;&#31867;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21306;&#21035;&#65306;&#24040;&#22823;&#34892;&#26143;&#30340;&#23494;&#24230;&#36739;&#20302;&#65292;&#26263;&#31034;&#23427;&#20204;&#20855;&#26377;&#26356;&#39640;&#30340;&#27682;-&#27686;&#36136;&#37327;&#20998;&#25968;&#65292;&#32780;&#23567;&#34892;&#26143;&#26356;&#23494;&#38598;&#65292;&#20027;&#35201;&#30001;&#26356;&#37325;&#30340;&#20803;&#32032;&#32452;&#25104;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#21508;&#31181;&#22238;&#24402;&#27169;&#22411;&#26469;&#25581;&#31034;&#29289;&#29702;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#34892;&#26143;&#21322;&#24452;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24378;&#35843;&#20102;&#34892;&#26143;&#36136;&#37327;&#12289;&#36712;&#36947; p&#233;riode &#21644;&#24658;&#26143;&#37329;&#23646;&#20016;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing number of exoplanet discoveries and advances in machine learning techniques have opened new avenues for exploring and understanding the characteristics of worlds beyond our Solar System. In this study, we employ efficient machine learning approaches to analyze a dataset comprising 762 confirmed exoplanets and eight Solar System planets, aiming to characterize their fundamental quantities. By applying different unsupervised clustering algorithms, we classify the data into two main classes: 'small' and 'giant' planets, with cut-off values at $R_{p}=8.13R_{\oplus}$ and $M_{p}=52.48M_{\oplus}$. This classification reveals an intriguing distinction: giant planets have lower densities, suggesting higher H-He mass fractions, while small planets are denser, composed mainly of heavier elements. We apply various regression models to uncover correlations between physical parameters and their predictive power for exoplanet radius. Our analysis highlights that planetary mass, orbital pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#22686;&#24378;&#25915;&#20987;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#33021;&#22815;&#36890;&#36807;&#26368;&#23567;&#30340;&#29305;&#24449;&#25913;&#21464;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#21644;&#22686;&#24378;&#25968;&#25454;&#20043;&#38388;&#30340;&#39640;&#29305;&#24449;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.01885</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22686;&#24378;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Enhancement attacks in biomedical machine learning. (arXiv:2301.01885v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#22686;&#24378;&#25915;&#20987;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#33021;&#22815;&#36890;&#36807;&#26368;&#23567;&#30340;&#29305;&#24449;&#25913;&#21464;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#21644;&#22686;&#24378;&#25968;&#25454;&#20043;&#38388;&#30340;&#39640;&#29305;&#24449;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#28982;&#32780;&#23545;&#20110;&#36825;&#20123;&#30740;&#31350;&#21487;&#20449;&#24230;&#30340;&#20851;&#27880;&#21364;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#23613;&#31649;&#19968;&#20123;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#21307;&#23398;&#22270;&#20687;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#30772;&#22351;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#20294;&#26368;&#36817;&#20986;&#29616;&#30340;&#8220;&#22686;&#24378;&#25915;&#20987;&#8221;&#36890;&#36807;&#35823;&#23548;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#20250;&#23545;&#29983;&#29289;&#21307;&#23398;&#26426;&#22120;&#23398;&#20064;&#26500;&#25104;&#26356;&#22823;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#21487;&#20449;&#24230;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#26497;&#23567;&#30340;&#29305;&#24449;&#25913;&#21464;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#24615;&#33021;&#65306;1&#65289;&#26222;&#36941;&#24615;&#33021;&#22686;&#24378;&#21644;2&#65289;&#26576;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#22686;&#24378;&#26694;&#26550;&#21487;&#20197;&#23558;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#20174;50&#65285;&#34394;&#20551;&#25552;&#39640;&#21040;&#25509;&#36817;100&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#21644;&#22686;&#24378;&#25968;&#25454;&#20043;&#38388;&#30340;&#39640;&#29305;&#24449;&#30456;&#20284;&#24615;&#65288;Pearson's r&gt;0.99&#65289;&#12290;&#31867;&#20284;&#22320;&#65292;&#22522;&#20110;&#26041;&#27861;&#30340;&#22686;&#24378;&#26694;&#26550;&#26377;&#25928;&#22320;&#34394;&#20551;&#25552;&#39640;&#20102;&#26576;&#31181;&#26041;&#27861;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of machine learning in biomedical research is rapidly growing, yet the trustworthiness of such research is often overlooked. While some previous works have investigated the ability of adversarial attacks to degrade model performance in medical imaging, the ability to falsely improve performance via recently-developed "enhancement attacks" may be a greater threat to biomedical machine learning. In the spirit of developing attacks to better understand trustworthiness, we developed two techniques to drastically enhance prediction performance of classifiers with minimal changes to features: 1) general enhancement of prediction performance, and 2) enhancement of a particular method over another. Our enhancement framework falsely improved classifiers' accuracy from 50% to almost 100% while maintaining high feature similarities between original and enhanced data (Pearson's r's&gt;0.99). Similarly, the method-specific enhancement framework was effective in falsely improving the per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;CLIP&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#23884;&#20837;&#23398;&#20064;&#35299;&#21078;&#23398;&#20851;&#31995;&#65292;&#33021;&#22815;&#20998;&#21106;25&#31181;&#22120;&#23448;&#21644;6&#31181;&#32959;&#30244;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.00785</link><description>&lt;p&gt;
&#22522;&#20110;CLIP&#30340;&#36890;&#29992;&#27169;&#22411;&#29992;&#20110;&#22120;&#23448;&#20998;&#21106;&#21644;&#32959;&#30244;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection. (arXiv:2301.00785v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;CLIP&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#23884;&#20837;&#23398;&#20064;&#35299;&#21078;&#23398;&#20851;&#31995;&#65292;&#33021;&#22815;&#20998;&#21106;25&#31181;&#22120;&#23448;&#21644;6&#31181;&#32959;&#30244;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#22312;&#33258;&#21160;&#21270;&#22120;&#23448;&#20998;&#21106;&#21644;&#32959;&#30244;&#26816;&#27979;&#26041;&#38754;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#36739;&#23567;&#19988;&#37096;&#20998;&#26631;&#27880;&#38382;&#39064;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#31867;&#22411;&#32959;&#30244;&#30340;&#26377;&#38480;&#25506;&#31350;&#65292;&#23548;&#33268;&#24471;&#21040;&#30340;&#27169;&#22411;&#36890;&#24120;&#38480;&#20110;&#20998;&#21106;&#29305;&#23450;&#30340;&#22120;&#23448;/&#32959;&#30244;&#65292;&#24182;&#24573;&#30053;&#35299;&#21078;&#32467;&#26500;&#30340;&#35821;&#20041;&#65292;&#20063;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CLIP&#39537;&#21160;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#23558;&#20174;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451; &#65288;CLIP&#65289;&#20013;&#23398;&#20064;&#21040;&#30340;&#25991;&#26412;&#23884;&#20837;&#32467;&#21512;&#21040;&#20998;&#21106;&#27169;&#22411;&#20013;&#12290;&#36825;&#31181;&#22522;&#20110;CLIP&#30340;&#26631;&#31614;&#32534;&#30721;&#25429;&#25417;&#20102;&#35299;&#21078;&#23398;&#20851;&#31995;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#21040;&#32467;&#26500;&#21270;&#29305;&#24449;&#23884;&#20837;&#65292;&#24182;&#20998;&#21106;25&#20010;&#22120;&#23448;&#21644;6&#31181;&#31867;&#22411;&#30340;&#32959;&#30244;&#12290;&#35813;&#27169;&#22411;&#30001;14&#20010;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#20351;&#29992;3410&#20010;CT&#25195;&#25551;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26469;&#33258;3&#20010;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;6162&#20010;&#22806;&#37096;CT&#25195;&#25551;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#30340;&#22269;&#38469;&#20934;&#30830;&#24615;&#22522;&#20934;&#27979;&#35797;&#65288;MIoU&#65289;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasing number of public datasets have shown a marked impact on automated organ segmentation and tumor detection. However, due to the small size and partially labeled problem of each dataset, as well as a limited investigation of diverse types of tumors, the resulting models are often limited to segmenting specific organs/tumors and ignore the semantics of anatomical structures, nor can they be extended to novel domains. To address these issues, we propose the CLIP-Driven Universal Model, which incorporates text embedding learned from Contrastive Language-Image Pre-training (CLIP) to segmentation models. This CLIP-based label encoding captures anatomical relationships, enabling the model to learn a structured feature embedding and segment 25 organs and 6 types of tumors. The proposed model is developed from an assembly of 14 datasets, using a total of 3,410 CT scans for training and then evaluated on 6,162 external CT scans from 3 additional datasets. We rank first on the Medical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REAP&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#23545;&#25239;&#36148;&#32440;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#20801;&#35768;&#29992;&#25143;&#22312;&#30495;&#23454;&#22270;&#20687;&#21644;&#30495;&#23454;&#29615;&#22659;&#26465;&#20214;&#19979;&#35780;&#20272;&#23545;&#25239;&#36148;&#32440;&#25915;&#20987;&#65292;&#20026;&#35299;&#20915;&#20381;&#36182;&#25668;&#20687;&#22836;&#30340;&#29289;&#29702;&#31995;&#32479;&#38754;&#20020;&#30340;&#20005;&#37325;&#23041;&#32961;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2212.05680</link><description>&lt;p&gt;
REAP&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#23545;&#25239;&#36148;&#32440;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
REAP: A Large-Scale Realistic Adversarial Patch Benchmark. (arXiv:2212.05680v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REAP&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#23545;&#25239;&#36148;&#32440;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#20801;&#35768;&#29992;&#25143;&#22312;&#30495;&#23454;&#22270;&#20687;&#21644;&#30495;&#23454;&#29615;&#22659;&#26465;&#20214;&#19979;&#35780;&#20272;&#23545;&#25239;&#36148;&#32440;&#25915;&#20987;&#65292;&#20026;&#35299;&#20915;&#20381;&#36182;&#25668;&#20687;&#22836;&#30340;&#29289;&#29702;&#31995;&#32479;&#38754;&#20020;&#30340;&#20005;&#37325;&#23041;&#32961;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#31181;&#33879;&#21517;&#30340;&#25915;&#20987;&#26041;&#24335;&#26159;&#23545;&#25239;&#36148;&#32440;&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26377;&#29305;&#23450;&#22270;&#26696;&#30340;&#36148;&#32440;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#36148;&#32440;&#25152;&#36148;&#29289;&#20307;&#19978;&#30340;&#39044;&#27979;&#38169;&#35823;&#12290;&#36825;&#31181;&#25915;&#20987;&#23545;&#20110;&#20381;&#36182;&#20110;&#25668;&#20687;&#22836;&#30340;&#29289;&#29702;&#31995;&#32479;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#23613;&#31649;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#30740;&#31350;&#26159;&#22256;&#38590;&#30340;&#65307;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35780;&#20272;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#25104;&#26412;&#39640;&#26114;&#65292;&#32780;&#21512;&#25104;&#25968;&#25454;&#21017;&#19981;&#22815;&#30495;&#23454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REAP&#65288;&#30495;&#23454;&#23545;&#25239;&#36148;&#32440;&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#23383;&#22522;&#20934;&#27979;&#35797;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#30495;&#23454;&#22270;&#20687;&#21644;&#30495;&#23454;&#29615;&#22659;&#26465;&#20214;&#19979;&#35780;&#20272;&#23545;&#25239;&#36148;&#32440;&#25915;&#20987;&#12290;&#22522;&#20110;Mapillary Vistas&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#36229;&#36807;14,000&#20010;&#20132;&#36890;&#26631;&#24535;&#12290;&#27599;&#20010;&#26631;&#24535;&#37117;&#32463;&#36807;&#20960;&#20309;&#21644;&#20809;&#29031;&#21464;&#25442;&#30340;&#25913;&#21464;&#65292;&#36825;&#21487;&#20197;&#29992;&#26469;&#23558;&#25968;&#23383;&#29983;&#25104;&#30340;&#36148;&#32440;&#30495;&#23454;&#22320;&#24212;&#29992;&#21040;&#22270;&#20687;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are known to be susceptible to adversarial perturbation. One famous attack is the adversarial patch, a sticker with a particularly crafted pattern that makes the model incorrectly predict the object it is placed on. This attack presents a critical threat to cyber-physical systems that rely on cameras such as autonomous cars. Despite the significance of the problem, conducting research in this setting has been difficult; evaluating attacks and defenses in the real world is exceptionally costly while synthetic data are unrealistic. In this work, we propose the REAP (REalistic Adversarial Patch) benchmark, a digital benchmark that allows the user to evaluate patch attacks on real images, and under real-world conditions. Built on top of the Mapillary Vistas dataset, our benchmark contains over 14,000 traffic signs. Each sign is augmented with a pair of geometric and lighting transformations, which can be used to apply a digitally generated patch realistically onto t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#36895;&#29575;VAE&#65288;MR-VAE&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#21333;&#27425;&#35757;&#32451;&#20013;&#23398;&#20064;&#19982;&#19981;&#21516;&#946;&#23545;&#24212;&#30340;&#26368;&#20248;&#21442;&#25968;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#32593;&#32476;&#23558;&#946;&#26144;&#23556;&#21040;&#26368;&#20248;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#29575;&#22833;&#30495;&#26354;&#32447;&#30340;&#23436;&#25972;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2212.03905</link><description>&lt;p&gt;
&#22810;&#36895;&#29575;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65306;&#19968;&#27425;&#35757;&#32451;&#65292;&#24471;&#21040;&#23436;&#25972;&#30340;&#29575;&#22833;&#30495;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve. (arXiv:2212.03905v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#36895;&#29575;VAE&#65288;MR-VAE&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#21333;&#27425;&#35757;&#32451;&#20013;&#23398;&#20064;&#19982;&#19981;&#21516;&#946;&#23545;&#24212;&#30340;&#26368;&#20248;&#21442;&#25968;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#32593;&#32476;&#23558;&#946;&#26144;&#23556;&#21040;&#26368;&#20248;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#29575;&#22833;&#30495;&#26354;&#32447;&#30340;&#23436;&#25972;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;VAEs&#36890;&#24120;&#38656;&#35201;&#22810;&#27425;&#35757;&#32451;&#26469;&#36873;&#25321;&#28508;&#22312;&#21464;&#37327;&#24212;&#35813;&#20445;&#30041;&#30340;&#20449;&#24687;&#37327;&#12290;&#37325;&#26500;&#35823;&#24046;&#65288;&#22833;&#30495;&#65289;&#21644;KL&#25955;&#24230;&#65288;&#29575;&#65289;&#20043;&#38388;&#30340;&#26435;&#34913;&#36890;&#24120;&#30001;&#36229;&#21442;&#25968;&#946;&#21442;&#25968;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#36895;&#29575;VAE&#65288;MR-VAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#35757;&#32451;&#20013;&#23398;&#20064;&#19982;&#19981;&#21516;&#946;&#23545;&#24212;&#30340;&#26368;&#20248;&#21442;&#25968;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#36229;&#32593;&#32476;&#26126;&#30830;&#22320;&#21046;&#23450;&#19968;&#20010;&#21709;&#24212;&#20989;&#25968;&#65292;&#23558;&#946;&#26144;&#23556;&#21040;&#26368;&#20248;&#21442;&#25968;&#12290;MR-VAEs&#26500;&#24314;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#21709;&#24212;&#36229;&#32593;&#32476;&#65292;&#20854;&#20013;&#30340;&#39044;&#28608;&#27963;&#26681;&#25454;&#946;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#38376;&#25511;&#12290;&#36890;&#36807;&#20998;&#26512;&#32447;&#24615;VAEs&#24182;&#23637;&#31034;&#23427;&#33021;&#22815;&#20934;&#30830;&#34920;&#31034;&#32447;&#24615;VAEs&#30340;&#21709;&#24212;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are powerful tools for learning latent representations of data used in a wide range of applications. In practice, VAEs usually require multiple training rounds to choose the amount of information the latent variable should retain. This trade-off between the reconstruction error (distortion) and the KL divergence (rate) is typically parameterized by a hyperparameter $\beta$. In this paper, we introduce Multi-Rate VAE (MR-VAE), a computationally efficient framework for learning optimal parameters corresponding to various $\beta$ in a single training run. The key idea is to explicitly formulate a response function that maps $\beta$ to the optimal parameters using hypernetworks. MR-VAEs construct a compact response hypernetwork where the pre-activations are conditionally gated based on $\beta$. We justify the proposed architecture by analyzing linear VAEs and showing that it can represent response functions exactly for linear VAEs. With the learned hypernetw
&lt;/p&gt;</description></item><item><title>uSplit&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#39640;&#25928;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#27178;&#21521;&#19978;&#19979;&#25991;&#21270;&#65292;&#24110;&#21161;&#35757;&#32451;&#26356;&#28145;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#24179;&#38138;&#20266;&#24433;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.12872</link><description>&lt;p&gt;
{\mu}Split: &#26174;&#24494;&#38236;&#25968;&#25454;&#30340;&#39640;&#25928;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
{\mu}Split: efficient image decomposition for microscopy data. (arXiv:2211.12872v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12872
&lt;/p&gt;
&lt;p&gt;
uSplit&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#39640;&#25928;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#27178;&#21521;&#19978;&#19979;&#25991;&#21270;&#65292;&#24110;&#21161;&#35757;&#32451;&#26356;&#28145;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#24179;&#38138;&#20266;&#24433;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; uSplit&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#35757;&#32451;&#22270;&#20687;&#20998;&#35299;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#24120;&#35268;&#30340;&#28145;&#24230;&#32467;&#26500;&#20307;&#31995;&#32467;&#26500;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#22823;&#22270;&#20687;&#22359;&#20250;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#20351;&#20869;&#23384;&#28040;&#32791;&#25104;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#30340;&#38480;&#21046;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27178;&#21521;&#19978;&#19979;&#25991;&#21270;&#65288;LC&#65289;&#65292;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#24378;&#22823;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;LC&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#22987;&#32456;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;LC&#19982;U-Nets&#12289;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;VAEs&#38598;&#25104;&#65292;&#20026;&#27492;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;ELBO loss&#12290;&#27492;&#22806;&#65292;LC&#20351;&#24471;&#35757;&#32451;&#27604;&#21407;&#26412;&#26356;&#28145;&#30340;&#20998;&#23618;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#20943;&#23569;&#20351;&#29992;&#20998;&#21106;VAE&#39044;&#27979;&#26102;&#19981;&#21487;&#36991;&#20813;&#30340;&#24179;&#38138;&#20266;&#24433;&#12290;&#25105;&#20204;&#23558;uSplit&#24212;&#29992;&#20110;&#20116;&#20010;&#20998;&#35299;&#20219;&#21153;&#65292;&#19968;&#20010;&#26159;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21478;&#22806;&#22235;&#20010;&#26469;&#33258;&#23454;&#38469;&#26174;&#24494;&#38236;&#25968;&#25454;&#12290;LC&#23454;&#29616;&#20102;SOTA&#30340;&#32467;&#26524;&#65288;&#24179;&#22343;im&#65289;
&lt;/p&gt;
&lt;p&gt;
We present uSplit, a dedicated approach for trained image decomposition in the context of fluorescence microscopy images. We find that best results using regular deep architectures are achieved when large image patches are used during training, making memory consumption the limiting factor to further improving performance. We therefore introduce lateral contextualization (LC), a memory efficient way to train powerful networks and show that LC leads to consistent and significant improvements on the task at hand. We integrate LC with U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a modified ELBO loss. Additionally, LC enables training deeper hierarchical models than otherwise possible and, interestingly, helps to reduce tiling artefacts that are inherently impossible to avoid when using tiled VAE predictions. We apply uSplit to five decomposition tasks, one on a synthetic dataset, four others derived from real microscopy data. LC achieves SOTA results (average im
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#21270;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21463;&#30410;&#20110;&#29109;&#27491;&#21017;&#21270;&#65292;&#22312;&#22810;&#20010;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#26410;&#30693;&#31867;&#21035;&#25968;&#37327;&#20855;&#26377;&#24378;&#22823;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11727</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#21270;&#20998;&#31867;&#30340;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;:&#19968;&#20010;&#22522;&#32447;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Parametric Classification for Generalized Category Discovery: A Baseline Study. (arXiv:2211.11727v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11727
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#21270;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21463;&#30410;&#20110;&#29109;&#27491;&#21017;&#21270;&#65292;&#22312;&#22810;&#20010;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#26410;&#30693;&#31867;&#21035;&#25968;&#37327;&#20855;&#26377;&#24378;&#22823;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#26088;&#22312;&#21033;&#29992;&#20174;&#26631;&#27880;&#26679;&#26412;&#20013;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#65292;&#22312;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#26032;&#30340;&#31867;&#21035;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#65292;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#23481;&#26131;&#23545;&#24050;&#30693;&#31867;&#21035;&#36807;&#24230;&#25311;&#21512;&#65292;&#24182;&#25903;&#25345;&#20351;&#29992;&#21322;&#30417;&#30563;k&#22343;&#20540;&#24418;&#25104;&#30340;&#38750;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#30340;&#22833;&#36133;&#24773;&#20917;&#65292;&#39564;&#35777;&#20102;&#24403;&#26377;&#39640;&#36136;&#37327;&#30340;&#30417;&#30563;&#21487;&#29992;&#26102;&#20808;&#21069;&#30340;&#35774;&#35745;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#19981;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#20004;&#31181;&#39044;&#27979;&#20559;&#24046;&#65306;&#20998;&#31867;&#22120;&#26356;&#20542;&#21521;&#20110;&#26356;&#39057;&#32321;&#22320;&#39044;&#27979;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#22312;&#24050;&#30693;&#21644;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#20135;&#29983;&#19968;&#20010;&#19981;&#24179;&#34913;&#30340;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#21270;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#21463;&#30410;&#20110;&#29109;&#27491;&#21017;&#21270;&#65292;&#22312;&#22810;&#20010;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#23545;&#26410;&#30693;&#31867;&#21035;&#25968;&#37327;&#20855;&#26377;&#24378;&#22823;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#25104;&#26524;&#33021;&#20026;&#26410;&#26469;&#26356;&#26377;&#25928;&#30340;GCD&#26041;&#27861;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Category Discovery (GCD) aims to discover novel categories in unlabelled datasets using knowledge learned from labelled samples. Previous studies argued that parametric classifiers are prone to overfitting to seen categories, and endorsed using a non-parametric classifier formed with semi-supervised k-means. However, in this study, we investigate the failure of parametric classifiers, verify the effectiveness of previous design choices when high-quality supervision is available, and identify unreliable pseudo-labels as a key problem. We demonstrate that two prediction biases exist: the classifier tends to predict seen classes more often, and produces an imbalanced distribution across seen and novel categories. Based on these findings, we propose a simple yet effective parametric classification method that benefits from entropy regularisation, achieves state-of-the-art performance on multiple GCD benchmarks and shows strong robustness to unknown class numbers. We hope the in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRONOS&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#22522;&#20110;Wi-Fi CSI&#23454;&#29616;&#26080;&#20154;&#35774;&#22791;NLoS&#20154;&#20307;&#26816;&#27979;&#65292;&#21487;&#20197;&#21306;&#20998;&#25151;&#38388;&#20013;&#30340;&#31227;&#21160;&#20154;&#21592;&#21644;&#31354;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;NLoS&#26465;&#20214;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#25151;&#38388;&#20013;&#30340;&#20154;&#29289;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2211.10354</link><description>&lt;p&gt;
CRONOS&#65306;&#22522;&#20110;Wi-Fi CSI&#30340;&#26080;&#20154;&#35774;&#22791;NLoS&#20154;&#20307;&#26816;&#27979;&#30340;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CRONOS: Colorization and Contrastive Learning for Device-Free NLoS Human Presence Detection using Wi-Fi CSI. (arXiv:2211.10354v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRONOS&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#22522;&#20110;Wi-Fi CSI&#23454;&#29616;&#26080;&#20154;&#35774;&#22791;NLoS&#20154;&#20307;&#26816;&#27979;&#65292;&#21487;&#20197;&#21306;&#20998;&#25151;&#38388;&#20013;&#30340;&#31227;&#21160;&#20154;&#21592;&#21644;&#31354;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;NLoS&#26465;&#20214;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#25151;&#38388;&#20013;&#30340;&#20154;&#29289;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#20840;&#38754;&#26234;&#33021;&#21270;&#26381;&#21153;&#21644;&#24212;&#29992;&#30340;&#38656;&#27714;&#36805;&#36895;&#22686;&#38271;&#12290;&#36890;&#36807;&#20256;&#24863;&#22120;&#25110;&#25668;&#20687;&#22836;&#36827;&#34892;&#26080;&#20154;&#26816;&#27979;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#20197;&#21450;&#23545;&#38745;&#27490;&#20154;&#21592;&#30340;&#38169;&#35823;&#26816;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#21830;&#29992;Wi-Fi&#35774;&#22791;&#25429;&#33719;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;(CSI)&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20449;&#21495;&#29305;&#24449;&#65292;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#38750;&#30452;&#35270;(NLoS)&#21644;&#38745;&#24577;&#22330;&#26223;&#19979;&#23384;&#22312;&#20998;&#31867;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#24403;&#19968;&#20010;&#20154;&#38745;&#27490;&#31449;&#22312;&#25151;&#38388;&#35282;&#33853;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRONOS(&#22522;&#20110;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;NLoS&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;)&#30340;&#31995;&#32479;&#65292;&#23427;&#29983;&#25104;&#21160;&#24577;&#30340;&#22797;&#21457;&#22270;(RPs)&#21644;&#39068;&#33394;&#32534;&#30721;&#30340;CSI&#27604;&#29575;&#20197;&#21306;&#20998;&#25151;&#38388;&#20013;&#30340;&#31227;&#21160;&#20154;&#21592;&#21644;&#31354;&#32622;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#26816;&#32034;&#23454;&#36136;&#24615;&#30340;&#34920;&#24449;&#65292;&#20854;&#20013;&#21672;&#35810;&#25439;&#22833;&#34987;&#21046;&#23450;&#20026;&#21306;&#20998;&#21516;&#31867;&#21644;&#24322;&#31867;&#30340;&#23884;&#20837;&#28857;&#20043;&#38388;&#36317;&#31163;&#24230;&#37327;&#30340;&#25439;&#22833;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;NLoS&#26465;&#20214;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#25151;&#38388;&#20013;&#30340;&#20154;&#29289;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the demand for pervasive smart services and applications has increased rapidly. Device-free human detection through sensors or cameras has been widely adopted, but it comes with privacy issues as well as misdetection for motionless people. To address these drawbacks, channel state information (CSI) captured from commercialized Wi-Fi devices provides rich signal features for accurate detection. However, existing systems suffer from inaccurate classification under a non-line-of-sight (NLoS) and stationary scenario, such as when a person is standing still in a room corner. In this work, we propose a system called CRONOS (Colorization and Contrastive Learning Enhanced NLoS Human Presence Detection), which generates dynamic recurrence plots (RPs) and color-coded CSI ratios to distinguish mobile people from vacancy in a room, respectively. We also incorporate supervised contrastive learning to retrieve substantial representations, where consultation loss is formulated to dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#29289;&#32852;&#32593;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#21361;&#38505;&#22240;&#32032;&#39044;&#27979;&#31958;&#23615;&#30149;&#65292;&#20197;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#19981;&#21516;&#21307;&#30103;&#20256;&#24863;&#22120;&#12289;&#35774;&#22791;&#21644;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.07643</link><description>&lt;p&gt;
&#23558;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#25805;&#20316;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#30340;&#32508;&#21512;&#29289;&#32852;&#32593;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#21306;&#22359;&#38142;&#30417;&#25511;&#31995;&#32479;&#20013;&#36827;&#34892;&#31958;&#23615;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Secure and Privacy-Preserving Automated Machine Learning Operations into End-to-End Integrated IoT-Edge-Artificial Intelligence-Blockchain Monitoring System for Diabetes Mellitus Prediction. (arXiv:2211.07643v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#29289;&#32852;&#32593;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#21361;&#38505;&#22240;&#32032;&#39044;&#27979;&#31958;&#23615;&#30149;&#65292;&#20197;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#19981;&#21516;&#21307;&#30103;&#20256;&#24863;&#22120;&#12289;&#35774;&#22791;&#21644;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#20840;&#29699;&#27515;&#20129;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#30446;&#21069;&#26080;&#27861;&#27835;&#24840;&#65292;&#22914;&#26524;&#19981;&#27835;&#30103;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#20581;&#24247;&#24182;&#21457;&#30151;&#65292;&#22914;&#35270;&#32593;&#33180;&#30149;&#21464;&#12289;&#32930;&#20307;&#25130;&#32930;&#12289;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#31070;&#32463;&#30142;&#30149;&#12290;&#22240;&#27492;&#65292;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#20197;&#36991;&#20813;/&#39044;&#27979;&#31958;&#23615;&#30149;&#30340;&#21457;&#29983;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#29992;&#20110;&#31958;&#23615;&#30149;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21361;&#38505;&#22240;&#32032;&#30340;&#29289;&#32852;&#32593;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#21306;&#22359;&#38142;&#31995;&#32479;&#36827;&#34892;&#31958;&#23615;&#30149;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#24314;&#31435;&#22312;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#22522;&#30784;&#19978;&#65292;&#20174;&#19981;&#21516;&#21307;&#38498;&#30340;&#24739;&#32773;&#20013;&#33719;&#21462;&#21361;&#38505;&#22240;&#32032;&#25968;&#25454;&#30340;&#25972;&#20307;&#35270;&#22270;&#65292;&#24182;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#31995;&#32479;&#20013;&#19981;&#21516;&#30340;&#21307;&#30103;&#20256;&#24863;&#22120;&#12289;&#35774;&#22791;&#21644;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#27979;&#37327;&#21644;&#25910;&#38598;&#21361;&#38505;&#22240;&#32032;&#30340;&#20540;&#12290;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#21644;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetes Mellitus, one of the leading causes of death worldwide, has no cure to date and can lead to severe health complications, such as retinopathy, limb amputation, cardiovascular diseases, and neuronal disease, if left untreated. Consequently, it becomes crucial to take precautionary measures to avoid/predict the occurrence of diabetes. Machine learning approaches have been proposed and evaluated in the literature for diabetes prediction. This paper proposes an IoT-edge-Artificial Intelligence (AI)-blockchain system for diabetes prediction based on risk factors. The proposed system is underpinned by the blockchain to obtain a cohesive view of the risk factors data from patients across different hospitals and to ensure security and privacy of the user's data. Furthermore, we provide a comparative analysis of different medical sensors, devices, and methods to measure and collect the risk factors values in the system. Numerical experiments and comparative analysis were carried out bet
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#20013;&#22914;&#20309;&#39640;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#26415;&#21644;&#25913;&#36827;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.15445</link><description>&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#39640;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Utilization of Large Pre-Trained Models for Low Resource ASR. (arXiv:2210.15445v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#20013;&#22914;&#20309;&#39640;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#26415;&#21644;&#25913;&#36827;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26080;&#30417;&#30563;&#30340;&#34920;&#31034;&#23398;&#20064;&#24110;&#21161;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#35299;&#20915;&#20102;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#30828;&#20214;&#38480;&#21046;&#21644;&#24212;&#29992;&#31243;&#24207;&#32473;&#20986;&#20102;&#22914;&#20309;&#39640;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#38477;&#20302;&#20854;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36234;&#21335;&#35821;&#21644;&#24503;&#35821;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20302;&#36164;&#28304;&#30005;&#35805;&#20250;&#35805;&#35821;&#38899;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#26080;&#30417;&#30563;&#25216;&#26415;&#36229;&#36234;&#31616;&#21333;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22909;&#22788;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#23427;&#20204;&#36866;&#24212;&#21040;&#23454;&#38469;&#30340;&#30005;&#35805;&#20219;&#21153;&#65292;&#21253;&#25324;&#24102;&#23485;&#20256;&#36755;&#65292;&#24182;&#35843;&#26597;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#26465;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#25216;&#26415;&#30456;&#23545;&#20110;&#39033;&#30446;&#22522;&#32447;&#25552;&#39640;&#20102;22%&#12290;&#36890;&#36807;&#26550;&#26500;&#21644;&#35757;&#32451;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;29%&#65292;&#36890;&#36807;&#28155;&#21152;0.8&#23567;&#26102;&#30340;&#39046;&#22495;&#20869;&#33258;&#36866;&#24212;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning has recently helped automatic speech recognition (ASR) to tackle tasks with limited labeled data. Following this, hardware limitations and applications give rise to the question how to take advantage of large pre-trained models efficiently and reduce their complexity. In this work, we study a challenging low resource conversational telephony speech corpus from the medical domain in Vietnamese and German. We show the benefits of using unsupervised techniques beyond simple fine-tuning of large pre-trained models, discuss how to adapt them to a practical telephony task including bandwidth transfer and investigate different data conditions for pre-training and fine-tuning. We outperform the project baselines by 22% relative using pretraining techniques. Further gains of 29% can be achieved by refinements of architecture and training and 6% by adding 0.8 h of in-domain adaptation data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#22270;&#27169;&#22411;&#65288;NGMs&#65289;&#65292;&#23427;&#21487;&#20197;&#20197;&#21512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#34920;&#31034;&#22797;&#26434;&#30340;&#29305;&#24449;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#24212;&#22810;&#31181;&#22270;&#32467;&#26500;&#21644;&#28151;&#21512;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#23398;&#20064;&#12289;&#25512;&#26029;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.00453</link><description>&lt;p&gt;
&#31070;&#32463;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Graphical Models. (arXiv:2210.00453v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#22270;&#27169;&#22411;&#65288;NGMs&#65289;&#65292;&#23427;&#21487;&#20197;&#20197;&#21512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#34920;&#31034;&#22797;&#26434;&#30340;&#29305;&#24449;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#24212;&#22810;&#31181;&#22270;&#32467;&#26500;&#21644;&#28151;&#21512;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#23398;&#20064;&#12289;&#25512;&#26029;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#22270;&#27169;&#22411;&#32463;&#24120;&#34987;&#29992;&#26469;&#29702;&#35299;&#31995;&#32479;&#30340;&#21160;&#24577;&#12290;&#23427;&#20204;&#21487;&#20197;&#24314;&#27169;&#29305;&#24449;&#65288;&#33410;&#28857;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#24213;&#23618;&#20998;&#24067;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#34920;&#31034;&#38750;&#24120;&#22797;&#26434;&#30340;&#20381;&#36182;&#20989;&#25968;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#19982;&#22270;&#25805;&#20316;&#30456;&#20851;&#30340;&#35745;&#31639;&#38480;&#21046;&#65292;&#36890;&#24120;&#20250;&#20570;&#31616;&#21270;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#22270;&#27169;&#22411;&#65288;NGMs&#65289;&#65292;&#35797;&#22270;&#20197;&#21512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#34920;&#31034;&#22797;&#26434;&#30340;&#29305;&#24449;&#20381;&#36182;&#20851;&#31995;&#12290;&#32473;&#23450;&#29305;&#24449;&#20851;&#31995;&#22270;&#21644;&#30456;&#24212;&#26679;&#26412;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#26469;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#32467;&#26500;&#20197;&#21450;&#23427;&#20204;&#30340;&#22797;&#26434;&#20989;&#25968;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#23398;&#20064;&#12289;&#25512;&#26029;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;NGMs&#21487;&#20197;&#36866;&#24212;&#36890;&#29992;&#30340;&#22270;&#32467;&#26500;&#65292;&#21253;&#25324;&#26377;&#21521;&#22270;&#12289;&#26080;&#21521;&#22270;&#21644;&#28151;&#21512;&#36793;&#22270;&#65292;&#21516;&#26102;&#25903;&#25345;&#28151;&#21512;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#39564;&#30740;&#31350;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;NGMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Graphical Models are often used to understand dynamics of a system. They can model relationships between features (nodes) and the underlying distribution. Theoretically these models can represent very complex dependency functions, but in practice often simplifying assumptions are made due to computational limitations associated with graph operations. In this work we introduce Neural Graphical Models (NGMs) which attempt to represent complex feature dependencies with reasonable computational costs. Given a graph of feature relationships and corresponding samples, we capture the dependency structure between the features along with their complex function representations by using a neural network as a multi-task learning framework. We provide efficient learning, inference and sampling algorithms. NGMs can fit generic graph structures including directed, undirected and mixed-edge graphs as well as support mixed input data types. We present empirical studies that show NGMs' cap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#25972;&#20307;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#24050;&#30693;&#30340;&#29289;&#20307;&#21644;&#26410;&#30693;&#30340;&#29289;&#20307;&#36827;&#34892;&#20934;&#30830;&#20998;&#21106;&#65292;&#24182;&#22312;&#22788;&#29702;&#24050;&#30693;&#31867;&#21035;&#26102;&#23545;&#26410;&#30693;&#31867;&#21035;&#36827;&#34892;&#40065;&#26834;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2209.05407</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#20998;&#21106;&#24050;&#30693;&#29289;&#20307;&#21644;&#26410;&#30693;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
Segmenting Known Objects and Unseen Unknowns without Prior Knowledge. (arXiv:2209.05407v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#25972;&#20307;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#24050;&#30693;&#30340;&#29289;&#20307;&#21644;&#26410;&#30693;&#30340;&#29289;&#20307;&#36827;&#34892;&#20934;&#30830;&#20998;&#21106;&#65292;&#24182;&#22312;&#22788;&#29702;&#24050;&#30693;&#31867;&#21035;&#26102;&#23545;&#26410;&#30693;&#31867;&#21035;&#36827;&#34892;&#40065;&#26834;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#35270;&#22495;&#20998;&#21106;&#26041;&#27861;&#20250;&#26681;&#25454;&#36755;&#20837;&#23558;&#24050;&#30693;&#31867;&#21035;&#20998;&#37197;&#32473;&#27599;&#20010;&#20687;&#32032;&#12290;&#21363;&#20351;&#22312;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#24517;&#28982;&#20250;&#23548;&#33268;&#22312;&#35757;&#32451;&#31867;&#21035;&#20043;&#22806;&#30340;&#29289;&#20307;&#19978;&#20135;&#29983;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#23545;&#20110;&#26410;&#30693;&#26679;&#26412;&#21644;&#26497;&#31471;&#24773;&#20917;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#36991;&#20813;&#21361;&#38505;&#21518;&#26524;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#26080;&#27861;&#21253;&#21547;&#36275;&#22815;&#30340;&#25968;&#25454;&#28857;&#26469;&#20805;&#20998;&#37319;&#26679;&#24213;&#23618;&#20998;&#24067;&#30340;&#38271;&#23614;&#37096;&#20998;&#65292;&#27169;&#22411;&#24517;&#39035;&#33021;&#22815;&#22788;&#29702;&#26410;&#35265;&#36807;&#21644;&#26410;&#30693;&#30340;&#24773;&#20917;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#35782;&#21035;&#24050;&#30475;&#21040;&#30340;&#26410;&#26631;&#35760;&#23545;&#35937;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#23637;&#20998;&#21106;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#25972;&#20307;&#20998;&#21106;&#12290;&#25972;&#20307;&#20998;&#21106;&#26088;&#22312;&#20174;&#26410;&#35265;&#36807;&#30340;&#26410;&#30693;&#31867;&#21035;&#20013;&#35782;&#21035;&#21644;&#20998;&#31163;&#23545;&#35937;&#23454;&#20363;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#65292;&#21516;&#26102;&#25191;&#34892;&#24050;&#30693;&#31867;&#21035;&#30340;&#27867;&#35270;&#22495;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic segmentation methods assign a known class to each pixel given in input. Even for state-of-the-art approaches, this inevitably enforces decisions that systematically lead to wrong predictions for objects outside the training categories. However, robustness against out-of-distribution samples and corner cases is crucial in safety-critical settings to avoid dangerous consequences. Since real-world datasets cannot contain enough data points to adequately sample the long tail of the underlying distribution, models must be able to deal with unseen and unknown scenarios as well. Previous methods targeted this by re-identifying already-seen unlabeled objects. In this work, we propose the necessary step to extend segmentation with a new setting which we term holistic segmentation. Holistic segmentation aims to identify and separate objects of unseen unknown categories into instances, without any prior knowledge about them, while performing panoptic segmentation of known classes. We tac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20984;&#38598;&#30340;&#26032;&#35282;&#24230;&#31995;&#32479;&#22320;&#21457;&#23637;&#20102;&#25439;&#22833;&#20989;&#25968;&#29702;&#35770;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#21512;&#36866;&#30340;&#25439;&#22833;&#20989;&#25968;&#23450;&#20041;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#25439;&#22833;&#21644;&#33539;&#25968;&#20043;&#38388;&#20851;&#31995;&#30340;&#26032;&#26426;&#20250;&#65292;&#20197;&#21450;&#20984;&#38598;&#24494;&#31215;&#20998;&#19979;&#30340;&#25439;&#22833;&#25554;&#20540;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.00238</link><description>&lt;p&gt;
&#25439;&#22833;&#30340;&#20960;&#20309;&#21644;&#24494;&#31215;&#20998;
&lt;/p&gt;
&lt;p&gt;
The Geometry and Calculus of Losses. (arXiv:2209.00238v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20984;&#38598;&#30340;&#26032;&#35282;&#24230;&#31995;&#32479;&#22320;&#21457;&#23637;&#20102;&#25439;&#22833;&#20989;&#25968;&#29702;&#35770;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#21512;&#36866;&#30340;&#25439;&#22833;&#20989;&#25968;&#23450;&#20041;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#25439;&#22833;&#21644;&#33539;&#25968;&#20043;&#38388;&#20851;&#31995;&#30340;&#26032;&#26426;&#20250;&#65292;&#20197;&#21450;&#20984;&#38598;&#24494;&#31215;&#20998;&#19979;&#30340;&#25439;&#22833;&#25554;&#20540;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#20915;&#31574;&#38382;&#39064;&#26159;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#30340;&#26680;&#24515;&#12290;&#26368;&#31616;&#21333;&#30340;&#38382;&#39064;&#26159;&#20108;&#20803;&#21644;&#22810;&#31867;&#20998;&#31867;&#20197;&#21450;&#31867;&#27010;&#29575;&#20272;&#35745;&#12290;&#23427;&#20204;&#30340;&#23450;&#20041;&#30340;&#26680;&#24515;&#26159;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#26159;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#30340;&#25163;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#31181;&#26032;&#39062;&#30340;&#35282;&#24230;&#31995;&#32479;&#22320;&#21457;&#23637;&#20102;&#36825;&#31867;&#38382;&#39064;&#30340;&#25439;&#22833;&#20989;&#25968;&#29702;&#35770;&#65292;&#20854;&#22522;&#26412;&#35201;&#32032;&#26159;&#20855;&#26377;&#29305;&#23450;&#32467;&#26500;&#30340;&#20984;&#38598;&#12290;&#25439;&#22833;&#20989;&#25968;&#34987;&#23450;&#20041;&#20026;&#20984;&#38598;&#30340;&#25903;&#25745;&#20989;&#25968;&#30340;&#27425;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#23427;&#33258;&#21160;&#26159;&#21512;&#36866;&#30340;&#65288;&#29992;&#20110;&#27010;&#29575;&#20272;&#35745;&#65289;&#12290;&#36825;&#31181;&#35270;&#35282;&#25552;&#20379;&#20102;&#19977;&#20010;&#26032;&#39062;&#30340;&#26426;&#20250;&#12290;&#23427;&#20351;&#24471;&#25439;&#22833;&#21644;(&#21453;)&#33539;&#25968;&#20043;&#38388;&#30340;&#22522;&#26412;&#20851;&#31995;&#30340;&#21457;&#23637;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#20284;&#20046;&#20197;&#21069;&#27809;&#26377;&#34987;&#27880;&#24847;&#21040;&#12290;&#20854;&#27425;&#65292;&#23427;&#36890;&#36807;&#20984;&#38598;&#30340;&#24494;&#31215;&#20998;&#20351;&#24471;&#25439;&#22833;&#30340;&#24494;&#31215;&#20998;&#30340;&#21457;&#23637;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#19981;&#21516;&#30340;&#25439;&#22833;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical decision problems lie at the heart of statistical machine learning. The simplest problems are binary and multiclass classification and class probability estimation. Central to their definition is the choice of loss function, which is the means by which the quality of a solution is evaluated. In this paper we systematically develop the theory of loss functions for such problems from a novel perspective whose basic ingredients are convex sets with a particular structure. The loss function is defined as the subgradient of the support function of the convex set. It is consequently automatically proper (calibrated for probability estimation). This perspective provides three novel opportunities. It enables the development of a fundamental relationship between losses and (anti)-norms that appears to have not been noticed before. Second, it enables the development of a calculus of losses induced by the calculus of convex sets which allows the interpolation between different losses,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GHN-Q&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#24418;&#36229;&#32593;&#32476;&#26469;&#39044;&#27979;&#26410;&#35265;&#37327;&#21270;&#21367;&#31215;&#26550;&#26500;&#30340;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#37327;&#21270;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.12489</link><description>&lt;p&gt;
GHN-Q&#65306;&#36890;&#36807;&#22270;&#24418;&#36229;&#32593;&#32476;&#39044;&#27979;&#26410;&#35265;&#37327;&#21270;&#21367;&#31215;&#26550;&#26500;&#30340;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
GHN-Q: Parameter Prediction for Unseen Quantized Convolutional Architectures via Graph Hypernetworks. (arXiv:2208.12489v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GHN-Q&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#24418;&#36229;&#32593;&#32476;&#26469;&#39044;&#27979;&#26410;&#35265;&#37327;&#21270;&#21367;&#31215;&#26550;&#26500;&#30340;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#37327;&#21270;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#35757;&#32451;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#25104;&#21151;&#65292;&#25214;&#21040;&#20102;&#26368;&#20339;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;CNN&#26550;&#26500;&#36890;&#24120;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#21333;&#20010;&#26550;&#26500;&#30340;&#20219;&#20309;&#32473;&#23450;&#27169;&#22411;&#37117;&#23384;&#22312;&#19968;&#20010;&#24222;&#22823;&#30340;&#21442;&#25968;&#31354;&#38388;&#12290;&#20855;&#26377;&#30456;&#20284;&#25439;&#22833;&#30340;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#25130;&#28982;&#19981;&#21516;&#30340;&#29305;&#24615;&#65292;&#22914;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#37327;&#21270;&#40065;&#26834;&#24615;&#12290;&#23545;&#20110;&#36793;&#32536;&#19978;&#30340;&#28145;&#24230;&#23398;&#20064;&#65292;&#37327;&#21270;&#40065;&#26834;&#24615;&#36890;&#24120;&#33267;&#20851;&#37325;&#35201;&#12290;&#25214;&#21040;&#19968;&#20010;&#37327;&#21270;&#40065;&#26834;&#30340;&#27169;&#22411;&#26377;&#26102;&#21487;&#33021;&#38656;&#35201;&#24456;&#22823;&#30340;&#21162;&#21147;&#12290;&#26368;&#36817;&#20351;&#29992;&#22270;&#24418;&#36229;&#32593;&#32476;&#65288;GHN&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#22312;&#39044;&#27979;&#19981;&#21516;CNN&#26550;&#26500;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;GHN-2&#30340;&#22270;&#24418;&#34920;&#31034;&#26159;&#21542;&#20063;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#37327;&#21270;&#40065;&#26834;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;GHN-Q&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#26377;&#21490;&#20197;&#26469;&#31532;&#19968;&#27425;&#25506;&#32034;&#20351;&#29992;&#22270;&#24418;&#36229;&#32593;&#32476;&#26469;&#39044;&#27979;&#37327;&#21270;&#40065;&#26834;&#21442;&#25968;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep convolutional neural network (CNN) training via iterative optimization has had incredible success in finding optimal parameters. However, modern CNN architectures often contain millions of parameters. Thus, any given model for a single architecture resides in a massive parameter space. Models with similar loss could have drastically different characteristics such as adversarial robustness, generalizability, and quantization robustness. For deep learning on the edge, quantization robustness is often crucial. Finding a model that is quantization-robust can sometimes require significant efforts. Recent works using Graph Hypernetworks (GHN) have shown remarkable performance predicting high-performant parameters of varying CNN architectures. Inspired by these successes, we wonder if the graph representations of GHN-2 can be leveraged to predict quantization-robust parameters as well, which we call GHN-Q. We conduct the first-ever study exploring the use of graph hypernetworks for predi
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#25233;&#21046;&#24615;/&#36127;&#21521;&#36830;&#25509;&#26159;&#20026;&#20102;&#23398;&#20064;&#26356;&#22810;&#30340;&#21151;&#33021;&#65292;&#36127;&#26435;&#37325;&#22312;&#34920;&#24449;&#33021;&#21147;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#38750;&#36127;&#28145;&#24230;&#32593;&#32476;&#26080;&#27861;&#34920;&#31034;&#26576;&#20123;&#34920;&#24449;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.03211</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#32593;&#32476;&#20855;&#26377;&#25233;&#21046;&#24615;/&#36127;&#21521;&#36830;&#25509;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why do networks have inhibitory/negative connections?. (arXiv:2208.03211v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03211
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#25233;&#21046;&#24615;/&#36127;&#21521;&#36830;&#25509;&#26159;&#20026;&#20102;&#23398;&#20064;&#26356;&#22810;&#30340;&#21151;&#33021;&#65292;&#36127;&#26435;&#37325;&#22312;&#34920;&#24449;&#33021;&#21147;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#38750;&#36127;&#28145;&#24230;&#32593;&#32476;&#26080;&#27861;&#34920;&#31034;&#26576;&#20123;&#34920;&#24449;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#20026;&#20160;&#20040;&#20855;&#26377;&#25233;&#21046;&#24615;&#36830;&#25509;&#65311;&#28145;&#24230;&#32593;&#32476;&#20026;&#20160;&#20040;&#20855;&#26377;&#36127;&#26435;&#37325;&#65311;&#25105;&#20204;&#20174;&#34920;&#24449;&#33021;&#21147;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#31572;&#26696;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#33258;&#28982;&#26234;&#33021;&#20013;&#65292;&#22823;&#33041;&#30340;&#20027;&#35201;&#20316;&#29992;&#26159;&#34920;&#24449;&#21151;&#33021;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#20027;&#35201;&#20316;&#29992;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#20026;&#20160;&#20040;&#23384;&#22312;&#25233;&#21046;&#24615;/&#36127;&#21521;&#26435;&#37325;&#65306;&#20026;&#20102;&#23398;&#20064;&#26356;&#22810;&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#27809;&#26377;&#36127;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#38750;&#36882;&#22686;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#25104;&#20026;&#26222;&#36866;&#36817;&#20284;&#22120;&#12290;&#23613;&#31649;&#36825;&#21487;&#33021;&#23545;&#19968;&#20123;&#20154;&#26469;&#35828;&#26159;&#19968;&#31181;&#30452;&#35266;&#30340;&#32467;&#26524;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26080;&#35770;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#36824;&#26159;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#65292;&#37117;&#27809;&#26377;&#25552;&#20379;&#27491;&#24335;&#29702;&#35770;&#26469;&#35777;&#26126;&#20026;&#20160;&#20040;&#22312;&#34920;&#24449;&#33021;&#21147;&#30340;&#32972;&#26223;&#19979;&#65292;&#36127;&#26435;&#37325;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#38750;&#36127;&#28145;&#24230;&#32593;&#32476;&#26080;&#27861;&#34920;&#31034;&#30340;&#34920;&#24449;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#35265;&#35299;&#33021;&#22815;&#24102;&#26469;&#23545;&#26356;&#22797;&#26434;&#30340;&#24402;&#32435;&#36807;&#31243;&#30340;&#26356;&#28145;&#20837;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why do brains have inhibitory connections? Why do deep networks have negative weights? We propose an answer from the perspective of representation capacity. We believe representing functions is the primary role of both (i) the brain in natural intelligence, and (ii) deep networks in artificial intelligence. Our answer to why there are inhibitory/negative weights is: to learn more functions. We prove that, in the absence of negative weights, neural networks with non-decreasing activation functions are not universal approximators. While this may be an intuitive result to some, to the best of our knowledge, there is no formal theory, in either machine learning or neuroscience, that demonstrates why negative weights are crucial in the context of representation capacity. Further, we provide insights on the geometric properties of the representation space that non-negative deep networks cannot represent. We expect these insights will yield a deeper understanding of more sophisticated inducti
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#20013;&#30340;&#25200;&#21160;&#23481;&#24525;&#24230;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#23545;&#31216;&#24352;&#37327;&#31209;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.12529</link><description>&lt;p&gt;
&#36817;&#20284;&#23454;&#23545;&#31216;&#24352;&#37327;&#31209;
&lt;/p&gt;
&lt;p&gt;
Approximate Real Symmetric Tensor Rank. (arXiv:2207.12529v4 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12529
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#20013;&#30340;&#25200;&#21160;&#23481;&#24525;&#24230;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#23545;&#31216;&#24352;&#37327;&#31209;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#20013;&#30340;&#25200;&#21160;&#23481;&#24525;&#24230;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#23454;&#23545;&#31216;d-&#24352;&#37327;f&#65292;&#19968;&#20010;&#24352;&#37327;&#31354;&#38388;&#19978;&#30340;&#33539;&#25968;||.||&#65292;&#20197;&#21450;&#19968;&#20010;&#27491;&#25968;&#949;&gt;0&#12290;&#22312;f&#30340;&#949;-&#37051;&#22495;&#20869;&#65292;&#26368;&#23567;&#30340;&#23545;&#31216;&#24352;&#37327;&#31209;&#26159;&#22810;&#23569;&#65311;&#25442;&#21477;&#35805;&#35828;&#65292;&#32463;&#36807;&#19968;&#20010;&#24039;&#22937;&#30340;&#949;-&#25200;&#21160;&#21518;&#65292;f&#30340;&#23545;&#31216;&#24352;&#37327;&#31209;&#26159;&#22810;&#23569;&#65311;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#23450;&#29702;&#24182;&#24320;&#21457;&#20102;&#19977;&#20010;&#23545;&#24212;&#30340;&#31639;&#27861;&#26469;&#32473;&#20986;&#36825;&#20010;&#38382;&#39064;&#30340;&#19978;&#30028;&#26500;&#36896;&#12290;&#32771;&#34385;&#21040;&#25105;&#20204;&#30340;&#35748;&#35782;&#30446;&#26631;&#65307;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#32467;&#26524;&#32972;&#21518;&#30340;&#27010;&#29575;&#21644;&#20984;&#20960;&#20309;&#24605;&#24819;&#65292;&#20877;&#29616;&#20102;&#19968;&#20123;&#24050;&#30693;&#32467;&#26524;&#65292;&#24182;&#25351;&#20986;&#20102;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the effect of an $\varepsilon$-room of perturbation tolerance on symmetric tensor decomposition. To be more precise, suppose a real symmetric $d$-tensor $f$, a norm $||.||$ on the space of symmetric $d$-tensors, and $\varepsilon &gt;0$ are given. What is the smallest symmetric tensor rank in the $\varepsilon$-neighborhood of $f$? In other words, what is the symmetric tensor rank of $f$ after a clever $\varepsilon$-perturbation? We prove two theorems and develop three corresponding algorithms that give constructive upper bounds for this question. With expository goals in mind; we present probabilistic and convex geometric ideas behind our results, reproduce some known results, and point out open problems.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#31283;&#20581;&#30340;&#25968;&#25454;&#39537;&#21160;&#20844;&#24335;&#65292;&#33021;&#22815;&#21516;&#26102;&#20445;&#25252;&#19977;&#20010;&#36807;&#25311;&#21512;&#30340;&#28304;&#22836;&#65306;&#26377;&#38480;&#26679;&#26412;&#25968;&#25454;&#30340;&#32479;&#35745;&#35823;&#24046;&#12289;&#25968;&#25454;&#28857;&#30340;&#26377;&#38480;&#31934;&#24230;&#27979;&#37327;&#24341;&#36215;&#30340;&#25968;&#25454;&#22122;&#22768;&#65292;&#20197;&#21450;&#34987;&#30772;&#22351;&#30340;&#37096;&#20998;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2207.09560</link><description>&lt;p&gt;
&#20840;&#38754;&#31283;&#20581;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Holistic Robust Data-Driven Decisions. (arXiv:2207.09560v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09560
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#31283;&#20581;&#30340;&#25968;&#25454;&#39537;&#21160;&#20844;&#24335;&#65292;&#33021;&#22815;&#21516;&#26102;&#20445;&#25252;&#19977;&#20010;&#36807;&#25311;&#21512;&#30340;&#28304;&#22836;&#65306;&#26377;&#38480;&#26679;&#26412;&#25968;&#25454;&#30340;&#32479;&#35745;&#35823;&#24046;&#12289;&#25968;&#25454;&#28857;&#30340;&#26377;&#38480;&#31934;&#24230;&#27979;&#37327;&#24341;&#36215;&#30340;&#25968;&#25454;&#22122;&#22768;&#65292;&#20197;&#21450;&#34987;&#30772;&#22351;&#30340;&#37096;&#20998;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#33391;&#22909;&#26679;&#26412;&#22806;&#24615;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20915;&#31574;&#30340;&#25968;&#25454;&#39537;&#21160;&#20844;&#24335;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#12290;&#22909;&#30340;&#26679;&#26412;&#20869;&#24615;&#33021;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#22909;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#65292;&#36825;&#34987;&#26222;&#36941;&#35748;&#20026;&#26159;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#23454;&#38469;&#30340;&#36807;&#25311;&#21512;&#36890;&#24120;&#19981;&#33021;&#24402;&#22240;&#20110;&#21333;&#19968;&#21407;&#22240;&#65292;&#32780;&#26159;&#30001;&#22810;&#20010;&#22240;&#32032;&#21516;&#26102;&#24341;&#36215;&#30340;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#32771;&#34385;&#20102;&#19977;&#20010;&#36807;&#25311;&#21512;&#30340;&#28304;&#22836;&#65306;&#65288;&#19968;&#65289;&#32479;&#35745;&#35823;&#24046;&#65292;&#30001;&#20110;&#20351;&#29992;&#26377;&#38480;&#30340;&#26679;&#26412;&#25968;&#25454;&#32780;&#20135;&#29983;&#30340;&#35823;&#24046;&#65292;&#65288;&#20108;&#65289;&#25968;&#25454;&#22122;&#22768;&#65292;&#24403;&#25968;&#25454;&#28857;&#21482;&#29992;&#26377;&#38480;&#31934;&#24230;&#27979;&#37327;&#26102;&#20135;&#29983;&#30340;&#22122;&#22768;&#65292;&#65288;&#19977;&#65289;&#25968;&#25454;&#38169;&#35823;&#65292;&#21363;&#20840;&#37096;&#25968;&#25454;&#20013;&#26377;&#19968;&#23567;&#37096;&#20998;&#25968;&#25454;&#34987;&#23436;&#20840;&#30772;&#22351;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#20844;&#24335;&#22312;&#21333;&#29420;&#22788;&#29702;&#36825;&#19977;&#20010;&#28304;&#22836;&#26102;&#21487;&#33021;&#26159;&#31283;&#20581;&#30340;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#21516;&#26102;&#25552;&#20379;&#23545;&#25152;&#26377;&#36807;&#25311;&#21512;&#28304;&#22836;&#30340;&#20840;&#38754;&#20445;&#25252;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#20844;&#24335;&#65292;&#21487;&#20197;&#20445;&#35777;&#36825;&#31181;&#20840;&#38754;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design of data-driven formulations for machine learning and decision-making with good out-of-sample performance is a key challenge. The observation that good in-sample performance does not guarantee good out-of-sample performance is generally known as overfitting. Practical overfitting can typically not be attributed to a single cause but instead is caused by several factors all at once. We consider here three overfitting sources: (i) statistical error as a result of working with finite sample data, (ii) data noise which occurs when the data points are measured only with finite precision, and finally (iii) data misspecification in which a small fraction of all data may be wholly corrupted. We argue that although existing data-driven formulations may be robust against one of these three sources in isolation they do not provide holistic protection against all overfitting sources simultaneously. We design a novel data-driven formulation which does guarantee such holistic protection an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39532;&#23572;&#31185;&#22827;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;MGPVAE&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39532;&#23572;&#31185;&#22827;&#39640;&#26031;&#36807;&#31243;&#30340;&#31561;&#25928;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#21644;&#24179;&#28369;&#25216;&#26415;&#23454;&#29616;&#20102;&#32447;&#24615;&#26102;&#38388;&#30340;GPVAE&#35757;&#32451;&#12290;&#22312;&#21508;&#31181;&#39640;&#32500;&#26102;&#38388;&#21644;&#26102;&#31354;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2207.05543</link><description>&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Markovian Gaussian Process Variational Autoencoders. (arXiv:2207.05543v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39532;&#23572;&#31185;&#22827;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;MGPVAE&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39532;&#23572;&#31185;&#22827;&#39640;&#26031;&#36807;&#31243;&#30340;&#31561;&#25928;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#21644;&#24179;&#28369;&#25216;&#26415;&#23454;&#29616;&#20102;&#32447;&#24615;&#26102;&#38388;&#30340;GPVAE&#35757;&#32451;&#12290;&#22312;&#21508;&#31181;&#39640;&#32500;&#26102;&#38388;&#21644;&#26102;&#31354;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#20013;&#65292;&#24207;&#21015;VAE&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#20013;&#35768;&#22810;&#21464;&#31181;&#27169;&#22411;&#20381;&#36182;&#20110;&#31163;&#25955;&#26102;&#38388;&#26426;&#21046;&#65292;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36830;&#32493;&#26102;&#38388;&#26041;&#27861;&#26368;&#36817;&#22312;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#32972;&#26223;&#19979;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#25968;&#25454;&#12290;&#20854;&#20013;&#19968;&#31181;&#26159;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;GPVAEs&#65289;&#65292;&#20854;&#20013;VAE&#20808;&#39564;&#34987;&#35774;&#32622;&#20026;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#12290;&#28982;&#32780;&#65292;GPVAEs&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#23427;&#32487;&#25215;&#20102;&#39640;&#26031;&#36807;&#31243;&#30340;&#31435;&#26041;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#20854;&#23545;&#23454;&#38469;&#24212;&#29992;&#32773;&#19981;&#22826;&#21560;&#24341;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39532;&#23572;&#31185;&#22827;&#39640;&#26031;&#36807;&#31243;&#30340;&#31561;&#25928;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#21345;&#23572;&#26364;&#28388;&#27874;&#21644;&#24179;&#28369;&#26469;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;GPVAE&#35757;&#32451;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;MGPVAE&#65289;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#39640;&#32500;&#26102;&#38388;&#21644;&#26102;&#31354;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#21033;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential VAEs have been successfully considered for many high-dimensional time series modelling problems, with many variant models relying on discrete-time mechanisms such as recurrent neural networks (RNNs). On the other hand, continuous-time methods have recently gained attraction, especially in the context of irregularly-sampled time series, where they can better handle the data than discrete-time methods. One such class are Gaussian process variational autoencoders (GPVAEs), where the VAE prior is set as a Gaussian process (GP). However, a major limitation of GPVAEs is that it inherits the cubic computational cost as GPs, making it unattractive to practioners. In this work, we leverage the equivalent discrete state space representation of Markovian GPs to enable linear time GPVAE training via Kalman filtering and smoothing. For our model, Markovian GPVAE (MGPVAE), we show on a variety of high-dimensional temporal and spatiotemporal tasks that our method performs favourably compar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35889;&#22270;&#21367;&#31215;&#65292;&#35774;&#35745;&#20102;&#29992;&#20110;&#20004;&#31181;&#19981;&#21516;&#30340;&#26102;&#38388;&#29420;&#31435;PDE&#30340;&#36890;&#29992;&#35299;&#31639;&#22120;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#24418;&#29366;&#21644;&#19981;&#22343;&#21248;&#24615;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#32593;&#32476;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#38598;&#20013;&#21253;&#21547;&#26377;&#38480;&#20803;&#32593;&#26684;&#30340;&#21508;&#31181;&#21464;&#21270;&#30340;&#25968;&#25454;&#26159;&#23454;&#29616;&#33391;&#22909;&#27867;&#21270;&#32467;&#26524;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36825;&#34920;&#26126;GNNs&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;PDE&#38382;&#39064;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.14092</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36793;&#30028;&#20540;&#38382;&#39064;&#30340;&#35299;&#31639;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning the Solution Operator of Boundary Value Problems using Graph Neural Networks. (arXiv:2206.14092v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35889;&#22270;&#21367;&#31215;&#65292;&#35774;&#35745;&#20102;&#29992;&#20110;&#20004;&#31181;&#19981;&#21516;&#30340;&#26102;&#38388;&#29420;&#31435;PDE&#30340;&#36890;&#29992;&#35299;&#31639;&#22120;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#24418;&#29366;&#21644;&#19981;&#22343;&#21248;&#24615;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#32593;&#32476;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#38598;&#20013;&#21253;&#21547;&#26377;&#38480;&#20803;&#32593;&#26684;&#30340;&#21508;&#31181;&#21464;&#21270;&#30340;&#25968;&#25454;&#26159;&#23454;&#29616;&#33391;&#22909;&#27867;&#21270;&#32467;&#26524;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36825;&#34920;&#26126;GNNs&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;PDE&#38382;&#39064;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20256;&#32479;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#36793;&#30028;&#20540;&#32422;&#26463;&#38382;&#39064;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36817;&#24180;&#26469;&#30740;&#31350;&#20154;&#21592;&#23545;&#33021;&#22815;&#39640;&#25928;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#35889;&#22270;&#21367;&#31215;&#35774;&#35745;&#20102;&#29992;&#20110;&#20004;&#31181;&#19981;&#21516;&#30340;&#26102;&#38388;&#29420;&#31435;PDE&#30340;&#36890;&#29992;&#35299;&#31639;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#30340;&#27169;&#25311;&#25968;&#25454;&#65292;&#22312;&#21508;&#31181;&#24418;&#29366;&#21644;&#19981;&#22343;&#21248;&#24615;&#30340;&#24773;&#20917;&#19979;&#23545;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#35757;&#32451;&#21518;&#30340;&#35299;&#31639;&#22120;&#22312;&#26410;&#35265;&#36807;&#30340;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#24418;&#29366;&#30340;&#32593;&#26684;&#21644;&#19981;&#21516;&#20010;&#25968;&#30340;&#19981;&#22343;&#21248;&#24615;&#21472;&#21152;&#35299;&#30340;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#38598;&#20013;&#21253;&#21547;&#26377;&#38480;&#20803;&#32593;&#26684;&#30340;&#21508;&#31181;&#21464;&#21270;&#30340;&#25968;&#25454;&#26159;&#23454;&#29616;&#33391;&#22909;&#27867;&#21270;&#32467;&#26524;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30456;&#20449;GNNs&#21487;&#29992;&#20110;&#35299;&#20915;PDE&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an alternative to classical numerical solvers for partial differential equations (PDEs) subject to boundary value constraints, there has been a surge of interest in investigating neural networks that can solve such problems efficiently. In this work, we design a general solution operator for two different time-independent PDEs using graph neural networks (GNNs) and spectral graph convolutions. We train the networks on simulated data from a finite elements solver on a variety of shapes and inhomogeneities. In contrast to previous works, we focus on the ability of the trained operator to generalize to previously unseen scenarios. Specifically, we test generalization to meshes with different shapes and superposition of solutions for a different number of inhomogeneities. We find that training on a diverse dataset with lots of variation in the finite element meshes is a key ingredient for achieving good generalization results in all cases. With this, we believe that GNNs can be used to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20027;&#35201;&#26435;&#34913;&#20998;&#26512;"&#65288;PTA&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#28216;&#25103;&#23884;&#20837;&#20302;&#32500;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.07520</link><description>&lt;p&gt;
&#20027;&#35201;&#26435;&#34913;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Principal Trade-off Analysis. (arXiv:2206.07520v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20027;&#35201;&#26435;&#34913;&#20998;&#26512;"&#65288;PTA&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#28216;&#25103;&#23884;&#20837;&#20302;&#32500;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;"&#20027;&#35201;&#26435;&#34913;&#20998;&#26512;"&#65288;PTA&#65289;&#65292;&#19968;&#31181;&#23558;&#28216;&#25103;&#23884;&#20837;&#20302;&#32500;&#29305;&#24449;&#31354;&#38388;&#30340;&#20998;&#35299;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#21457;&#23637;&#19982;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#31867;&#27604;&#26469;&#35770;&#35777;&#23884;&#20837;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;PTA&#23558;&#20219;&#24847;&#30340;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#27491;&#20132;&#30340;&#20108;&#32500;&#29305;&#24449;&#24179;&#38754;&#30340;&#21152;&#26435;&#21644;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#29305;&#24449;&#24179;&#38754;&#20195;&#34920;&#20102;&#29420;&#29305;&#30340;&#25112;&#30053;&#26435;&#34913;&#65292;&#32780;&#24207;&#21015;&#30340;&#25130;&#26029;&#25552;&#20379;&#20102;&#26377;&#27934;&#23519;&#21147;&#30340;&#27169;&#22411;&#31616;&#21270;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#28216;&#25103;&#65288;&#24211;&#24681;&#25169;&#20811;&#12289;RPS+2&#12289;&#24067;&#27931;&#25176;&#21644;&#23453;&#21487;&#26790;&#65289;&#19978;&#39564;&#35777;&#20102;PTA&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#24211;&#24681;&#25169;&#20811;&#20013;&#65292;PTA&#28165;&#26224;&#22320;&#35782;&#21035;&#20986;&#20102;&#34394;&#24352;&#22768;&#21183;&#21644;&#36319;&#27880;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#24067;&#27931;&#25176;&#20013;&#65292;PTA&#35782;&#21035;&#20986;&#20102;&#28216;&#25103;&#30340;&#23545;&#31216;&#24615;&#65292;&#24182;&#25351;&#23450;&#20102;&#19982;&#19981;&#21516;&#32988;&#21033;&#26465;&#20214;&#30456;&#20851;&#30340;&#25112;&#30053;&#26435;&#34913;&#12290;&#36825;&#20123;&#23545;&#31216;&#24615;&#25581;&#31034;&#20102;&#28216;&#25103;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
How are the advantage relations between a set of agents playing a game organized and how do they reflect the structure of the game? In this paper, we illustrate "Principal Trade-off Analysis" (PTA), a decomposition method that embeds games into a low-dimensional feature space. We argue that the embeddings are more revealing than previously demonstrated by developing an analogy to Principal Component Analysis (PCA). PTA represents an arbitrary two-player zero-sum game as the weighted sum of pairs of orthogonal 2D feature planes. We show that the feature planes represent unique strategic trade-offs and truncation of the sequence provides insightful model reduction. We demonstrate the validity of PTA on a quartet of games (Kuhn poker, RPS+2, Blotto, and Pokemon). In Kuhn poker, PTA clearly identifies the trade-off between bluffing and calling. In Blotto, PTA identifies game symmetries, and specifies strategic trade-offs associated with distinct win conditions. These symmetries reveal limi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#26497;&#38480;&#19979;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65288;SGD&#65289;&#30340;&#21487;&#25193;&#23637;&#26497;&#38480;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#32500;&#24230;&#36235;&#20110;&#26080;&#31351;&#26102;&#65292;SGD&#30340;&#36712;&#36857;&#30340;&#26497;&#38480;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#36873;&#25321;&#35201;&#36319;&#36394;&#30340;&#24635;&#32467;&#32479;&#35745;&#37327;&#12289;&#21021;&#22987;&#21270;&#21644;&#27493;&#38271;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#29699;&#24418;&#65288;ODE&#65289;&#21644;&#25193;&#25955;&#65288;SDE&#65289;&#26497;&#38480;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#27493;&#38271;&#30340;&#20020;&#30028;&#23610;&#24230;&#65292;&#36825;&#20010;&#23610;&#24230;&#19979;&#65292;&#26377;&#25928;&#30340;&#29699;&#24418;&#21160;&#21147;&#23398;&#19982;&#26799;&#24230;&#27969;&#30456;&#21305;&#37197;&#65292;&#20294;&#26159;&#20986;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#20462;&#27491;&#39033;&#65292;&#25913;&#21464;&#20102;&#30456;&#22270;&#12290;&#36825;&#20010;&#26377;&#25928;&#21160;&#21147;&#23398;&#30340;&#19981;&#21160;&#28857;&#23545;&#24212;&#30340;&#25193;&#25955;&#26497;&#38480;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#12290;</title><link>http://arxiv.org/abs/2206.04030</link><description>&lt;p&gt;
SGD&#30340;&#39640;&#32500;&#26497;&#38480;&#23450;&#29702;&#65306;&#26377;&#25928;&#21160;&#21147;&#23398;&#21644;&#20020;&#30028;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
High-dimensional limit theorems for SGD: Effective dynamics and critical scaling. (arXiv:2206.04030v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#26497;&#38480;&#19979;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65288;SGD&#65289;&#30340;&#21487;&#25193;&#23637;&#26497;&#38480;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#32500;&#24230;&#36235;&#20110;&#26080;&#31351;&#26102;&#65292;SGD&#30340;&#36712;&#36857;&#30340;&#26497;&#38480;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#36873;&#25321;&#35201;&#36319;&#36394;&#30340;&#24635;&#32467;&#32479;&#35745;&#37327;&#12289;&#21021;&#22987;&#21270;&#21644;&#27493;&#38271;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#29699;&#24418;&#65288;ODE&#65289;&#21644;&#25193;&#25955;&#65288;SDE&#65289;&#26497;&#38480;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#27493;&#38271;&#30340;&#20020;&#30028;&#23610;&#24230;&#65292;&#36825;&#20010;&#23610;&#24230;&#19979;&#65292;&#26377;&#25928;&#30340;&#29699;&#24418;&#21160;&#21147;&#23398;&#19982;&#26799;&#24230;&#27969;&#30456;&#21305;&#37197;&#65292;&#20294;&#26159;&#20986;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#20462;&#27491;&#39033;&#65292;&#25913;&#21464;&#20102;&#30456;&#22270;&#12290;&#36825;&#20010;&#26377;&#25928;&#21160;&#21147;&#23398;&#30340;&#19981;&#21160;&#28857;&#23545;&#24212;&#30340;&#25193;&#25955;&#26497;&#38480;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#65292;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#21487;&#25193;&#23637;&#26497;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SGD&#30340;&#24635;&#32467;&#32479;&#35745;&#36712;&#36857;&#65288;&#21363;&#26377;&#38480;&#32500;&#20989;&#25968;&#65289;&#22312;&#32500;&#24230;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#30340;&#26497;&#38480;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#36873;&#25321;&#35201;&#36319;&#36394;&#30340;&#24635;&#32467;&#32479;&#35745;&#37327;&#12289;&#21021;&#22987;&#21270;&#21644;&#27493;&#38271;&#12290;&#23427;&#20135;&#29983;&#20102;&#19968;&#20010;&#22312;&#21069;&#36848;&#36873;&#25321;&#19978;&#26497;&#20854;&#20381;&#36182;&#30340;&#29699;&#24418;&#65288;ODE&#65289;&#21644;&#25193;&#25955;&#65288;SDE&#65289;&#26497;&#38480;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27493;&#38271;&#30340;&#20020;&#30028;&#23610;&#24230;&#65292;&#20302;&#20110;&#36825;&#20010;&#23610;&#24230;&#65292;&#26377;&#25928;&#30340;&#29699;&#24418;&#21160;&#21147;&#23398;&#19982;&#20154;&#21475;&#25439;&#22833;&#30340;&#26799;&#24230;&#27969;&#30456;&#21305;&#37197;&#65292;&#20294;&#22312;&#36825;&#20010;&#23610;&#24230;&#19978;&#65292;&#20986;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#20462;&#27491;&#39033;&#65292;&#25913;&#21464;&#20102;&#30456;&#22270;&#12290;&#20851;&#20110;&#36825;&#20010;&#26377;&#25928;&#21160;&#21147;&#23398;&#30340;&#19981;&#21160;&#28857;&#65292;&#30456;&#24212;&#30340;&#25193;&#25955;&#26497;&#38480;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#29978;&#33267;&#36864;&#21270;&#12290;&#25105;&#20204;&#22312;&#19968;&#20123;&#27969;&#34892;&#30340;&#20363;&#23376;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23574;&#23792;&#30697;&#38453;&#21644;&#24352;&#37327;&#27169;&#22411;&#30340;&#20272;&#35745;&#20197;&#21450;&#36890;&#36807;&#20004;&#23618;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the scaling limits of stochastic gradient descent (SGD) with constant step-size in the high-dimensional regime. We prove limit theorems for the trajectories of summary statistics (i.e., finite-dimensional functions) of SGD as the dimension goes to infinity. Our approach allows one to choose the summary statistics that are tracked, the initialization, and the step-size. It yields both ballistic (ODE) and diffusive (SDE) limits, with the limit depending dramatically on the former choices. We show a critical scaling regime for the step-size, below which the effective ballistic dynamics matches gradient flow for the population loss, but at which, a new correction term appears which changes the phase diagram. About the fixed points of this effective dynamics, the corresponding diffusive limits can be quite complex and even degenerate. We demonstrate our approach on popular examples including estimation for spiked matrix and tensor models and classification via two-layer networks fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#23567;&#22411;&#38750;&#21487;&#20998;&#31163;&#31243;&#24207;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#26102;&#38388;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.01614</link><description>&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#31243;&#24207;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning logic programs by combining programs. (arXiv:2206.01614v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#23567;&#22411;&#38750;&#21487;&#20998;&#31163;&#31243;&#24207;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#26102;&#38388;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#30446;&#26631;&#26159;&#24402;&#32435;&#20986;&#19968;&#20010;&#36923;&#36753;&#31243;&#24207;&#65288;&#19968;&#32452;&#36923;&#36753;&#35268;&#21017;&#65289;&#65292;&#20197;&#27010;&#25324;&#35757;&#32451;&#26679;&#20363;&#12290;&#24402;&#32435;&#20855;&#26377;&#22810;&#20010;&#35268;&#21017;&#21644;&#25991;&#23383;&#30340;&#31243;&#24207;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23398;&#20064;&#23567;&#22411;&#30340;&#19981;&#21487;&#20998;&#31163;&#30340;&#31243;&#24207;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22522;&#20110;&#32422;&#26463;&#30340;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#26368;&#20248;&#21644;&#36882;&#24402;&#31243;&#24207;&#65292;&#24182;&#36827;&#34892;&#35859;&#35789;&#21457;&#26126;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#65288;&#21253;&#25324;&#28216;&#25103;&#29609;&#27861;&#21644;&#31243;&#24207;&#21512;&#25104;&#65289;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#26102;&#38388;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#26377;&#26102;&#23558;&#23398;&#20064;&#26102;&#38388;&#20174;&#19968;&#20010;&#23567;&#26102;&#38477;&#20302;&#21040;&#20960;&#31186;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of inductive logic programming is to induce a logic program (a set of logical rules) that generalises training examples. Inducing programs with many rules and literals is a major challenge. To tackle this challenge, we introduce an approach where we learn small non-separable programs and combine them. We implement our approach in a constraint-driven ILP system. Our approach can learn optimal and recursive programs and perform predicate invention. Our experiments on multiple domains, including game playing and program synthesis, show that our approach can drastically outperform existing approaches in terms of predictive accuracies and learning times, sometimes reducing learning times from over an hour to a few seconds.
&lt;/p&gt;</description></item><item><title>RoCourseNet&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#40065;&#26834;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#20248;&#21270;&#39044;&#27979;&#21644;&#22238;&#24212;&#65292;&#20197;&#35299;&#20915;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#36896;&#25104;&#30340;&#22238;&#24212;&#26080;&#25928;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.00700</link><description>&lt;p&gt;
RoCourseNet&#65306;&#39044;&#27979;&#24863;&#30693;&#22238;&#24212;&#27169;&#22411;&#30340;&#20998;&#24067;&#40065;&#26834;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
RoCourseNet: Distributionally Robust Training of a Prediction Aware Recourse Model. (arXiv:2206.00700v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00700
&lt;/p&gt;
&lt;p&gt;
RoCourseNet&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#40065;&#26834;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#20248;&#21270;&#39044;&#27979;&#21644;&#22238;&#24212;&#65292;&#20197;&#35299;&#20915;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#36896;&#25104;&#30340;&#22238;&#24212;&#26080;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22240;&#26524;&#21453;&#20107;&#23454;&#65288;CF&#65289;&#35299;&#37322;&#26159;&#26368;&#32456;&#29992;&#25143;&#20559;&#22909;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#20026;&#21463;&#21040;&#39044;&#27979;&#32467;&#26524;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#34917;&#25937;&#65288;&#25110;&#23545;&#27604;&#65289;&#26696;&#20363;&#26469;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#29616;&#26377;&#30340;CF&#35299;&#37322;&#26041;&#27861;&#22312;&#20551;&#35774;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#19978;&#20445;&#25345;&#31283;&#23450;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#23454;&#38469;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#26029;&#26356;&#26032;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#20808;&#21069;&#29983;&#25104;&#30340;&#22238;&#24212;&#26080;&#25928;&#65292;&#24182;&#38477;&#20302;&#26368;&#32456;&#29992;&#25143;&#23545;&#25105;&#20204;&#31639;&#27861;&#26694;&#26550;&#30340;&#20449;&#20219;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RoCourseNet&#65292;&#36825;&#26159;&#19968;&#20010;&#21516;&#26102;&#20248;&#21270;&#23545;&#26410;&#26469;&#25968;&#25454;&#20559;&#31227;&#40065;&#26834;&#30340;&#39044;&#27979;&#21644;&#22238;&#24212;&#30340;&#35757;&#32451;&#26694;&#26550;&#12290;&#36825;&#39033;&#24037;&#20316;&#21253;&#21547;&#22235;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#65288;1&#65289;&#25105;&#20204;&#23558;&#40065;&#26834;&#30340;&#22238;&#24212;&#29983;&#25104;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#19977;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#23376;&#38382;&#39064;&#65306;&#65288;i&#65289;&#19968;&#20010;&#21452;&#23618;&#38382;&#39064;&#65292;&#25214;&#21040;&#26368;&#22351;&#30340;
&lt;/p&gt;
&lt;p&gt;
Counterfactual (CF) explanations for machine learning (ML) models are preferred by end-users, as they explain the predictions of ML models by providing a recourse (or contrastive) case to individuals who are adversely impacted by predicted outcomes. Existing CF explanation methods generate recourses under the assumption that the underlying target ML model remains stationary over time. However, due to commonly occurring distributional shifts in training data, ML models constantly get updated in practice, which might render previously generated recourses invalid and diminish end-users trust in our algorithmic framework. To address this problem, we propose RoCourseNet, a training framework that jointly optimizes predictions and recourses that are robust to future data shifts. This work contains four key contributions: (1) We formulate the robust recourse generation problem as a tri-level optimization problem which consists of two sub-problems: (i) a bi-level problem that finds the worst-c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21892;&#24847;&#30340;&#20030;&#25514;&#65292;&#36890;&#36807;&#24847;&#22270;&#20449;&#21495;&#26426;&#21046;&#23454;&#29616;&#33258;&#36866;&#24212;&#21442;&#25968;&#31649;&#29702;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#25163;&#21160;&#38598;&#25104;&#21644;&#26114;&#36149;&#30340;&#35843;&#35797;&#36807;&#31243;&#65292;&#25552;&#39640;&#20998;&#24067;&#24335;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.00470</link><description>&lt;p&gt;
&#21892;&#24847;&#30340;&#20030;&#25514;&#65306;&#36890;&#36807;&#24847;&#22270;&#20449;&#21495;&#36827;&#34892;&#33258;&#36866;&#24212;&#21442;&#25968;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Good Intentions: Adaptive Parameter Management via Intent Signaling. (arXiv:2206.00470v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21892;&#24847;&#30340;&#20030;&#25514;&#65292;&#36890;&#36807;&#24847;&#22270;&#20449;&#21495;&#26426;&#21046;&#23454;&#29616;&#33258;&#36866;&#24212;&#21442;&#25968;&#31649;&#29702;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#25163;&#21160;&#38598;&#25104;&#21644;&#26114;&#36149;&#30340;&#35843;&#35797;&#36807;&#31243;&#65292;&#25552;&#39640;&#20998;&#24067;&#24335;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#31649;&#29702;&#23545;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#24120;&#35265;&#30340;&#21442;&#25968;&#31649;&#29702;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#65292;&#19968;&#20123;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#38590;&#20197;&#20998;&#24067;&#24335;&#22788;&#29702;&#12290;&#20808;&#36827;&#30340;&#21442;&#25968;&#31649;&#29702;&#26041;&#27861;&#65292;&#20363;&#22914;&#36873;&#25321;&#24615;&#22797;&#21046;&#25110;&#21160;&#24577;&#21442;&#25968;&#20998;&#37197;&#65292;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#38598;&#25104;&#21040;&#27599;&#20010;&#20219;&#21153;&#30340;&#23454;&#29616;&#20013;&#65292;&#24182;&#19988;&#38656;&#35201;&#26114;&#36149;&#30340;&#21069;&#26399;&#23454;&#39564;&#26469;&#27491;&#30830;&#35843;&#25972;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#36991;&#20813;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24847;&#22270;&#20449;&#21495;&#26426;&#21046;&#65292;&#23427;&#33258;&#28982;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#22534;&#26632;&#20013;&#65292;&#24182;&#20026;&#21442;&#25968;&#31649;&#29702;&#22120;&#25552;&#20379;&#20851;&#38190;&#30340;&#21442;&#25968;&#35775;&#38382;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;AdaPM&#65292;&#19968;&#31181;&#22522;&#20110;&#36825;&#31181;&#26426;&#21046;&#30340;&#20840;&#33258;&#36866;&#24212;&#12289;&#38646;&#35843;&#35797;&#30340;&#21442;&#25968;&#31649;&#29702;&#22120;&#12290;&#19982;&#20043;&#21069;&#30340;&#31995;&#32479;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#23558;&#25552;&#20379;&#20449;&#24687;&#65288;&#20219;&#21153;&#31616;&#21333;&#23436;&#25104;&#65289;&#19982;&#26377;&#25928;&#21033;&#29992;&#20449;&#24687;&#65288;&#22256;&#38590;&#37096;&#20998;&#65289;&#20998;&#31163;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter management is essential for distributed training of large machine learning (ML) tasks. Some ML tasks are hard to distribute because common approaches to parameter management can be highly inefficient. Advanced parameter management approaches -- such as selective replication or dynamic parameter allocation -- can improve efficiency, but to do so, they typically need to be integrated manually into each task's implementation and they require expensive upfront experimentation to tune correctly. In this work, we explore whether these two problems can be avoided. We first propose a novel intent signaling mechanism that integrates naturally into existing ML stacks and provides the parameter manager with crucial information about parameter accesses. We then describe AdaPM, a fully adaptive, zero-tuning parameter manager based on this mechanism. In contrast to prior systems, this approach separates providing information (simple, done by the task) from exploiting it effectively (hard, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35774;&#35745;&#20102;&#24182;&#34892;&#24615;&#20998;&#31867;&#26041;&#27861;&#21518;&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;GNN&#27169;&#22411;&#12289;&#20219;&#21153;&#12289;&#36719;&#20214;&#26694;&#26550;&#21644;&#30828;&#20214;&#21152;&#36895;&#22120;&#20013;&#30340;&#24182;&#34892;&#24615;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#30456;&#20851;&#24352;&#37327;&#30340;&#31232;&#30095;&#24615;/&#23494;&#24230;&#12290;</title><link>http://arxiv.org/abs/2205.09702</link><description>&lt;p&gt;
&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#28145;&#20837;&#24182;&#21457;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. (arXiv:2205.09702v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09702
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35774;&#35745;&#20102;&#24182;&#34892;&#24615;&#20998;&#31867;&#26041;&#27861;&#21518;&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;GNN&#27169;&#22411;&#12289;&#20219;&#21153;&#12289;&#36719;&#20214;&#26694;&#26550;&#21644;&#30828;&#20214;&#21152;&#36895;&#22120;&#20013;&#30340;&#24182;&#34892;&#24615;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#30456;&#20851;&#24352;&#37327;&#30340;&#31232;&#30095;&#24615;/&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24378;&#22823;&#30340;&#24037;&#20855;&#20043;&#19968;&#12290;&#23427;&#20204;&#24120;&#24120;&#22312;&#26080;&#32467;&#26500;&#32593;&#32476;&#19978;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#12289;&#22270;&#20998;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#25512;&#29702;&#21644;&#35757;&#32451;&#37117;&#38750;&#24120;&#22797;&#26434;&#65292;&#23427;&#20204;&#29420;&#29305;&#22320;&#23558;&#19981;&#35268;&#21017;&#22270;&#22788;&#29702;&#30340;&#29305;&#24615;&#19982;&#23494;&#38598;&#21644;&#35268;&#21017;&#35745;&#31639;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#20351;&#24471;&#22312;&#29616;&#20195;&#22823;&#35268;&#27169;&#24182;&#34892;&#26550;&#26500;&#19978;&#39640;&#25928;&#25191;&#34892;GNNs&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;GNNs&#20013;&#30340;&#24182;&#34892;&#24615;&#20998;&#31867;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25968;&#25454;&#24182;&#34892;&#24615;&#21644;&#27169;&#22411;&#24182;&#34892;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#24418;&#24335;&#30340;&#27969;&#27700;&#32447;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20998;&#31867;&#26041;&#27861;&#26469;&#30740;&#31350;&#20247;&#22810;GNN&#27169;&#22411;&#12289;GNN&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12289;&#36719;&#20214;&#26694;&#26550;&#25110;&#30828;&#20214;&#21152;&#36895;&#22120;&#20013;&#30340;&#24182;&#34892;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#20316;&#28145;&#24230;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#36890;&#20449;&#37327;&#21644;&#21516;&#27493;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#30456;&#20851;&#24352;&#37327;&#30340;&#31232;&#30095;&#24615;/&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are among the most powerful tools in deep learning. They routinely solve complex problems on unstructured networks, such as node classification, graph classification, or link prediction, with high accuracy. However, both inference and training of GNNs are complex, and they uniquely combine the features of irregular graph processing with dense and regular computations. This complexity makes it very challenging to execute GNNs efficiently on modern massively parallel architectures. To alleviate this, we first design a taxonomy of parallelism in GNNs, considering data and model parallelism, and different forms of pipelining. Then, we use this taxonomy to investigate the amount of parallelism in numerous GNN models, GNN-driven machine learning tasks, software frameworks, or hardware accelerators. We use the work-depth model, and we also assess communication volume and synchronization. We specifically focus on the sparsity/density of the associated tensors, in o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#26368;&#23567;&#25104;&#26412;&#30340;&#24178;&#39044;&#38598;&#21512;&#26469;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#38382;&#39064;&#30340;NP-hard&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#35299;&#25110;&#36817;&#20284;&#35299;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#38543;&#26426;&#22270;&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#39564;&#35777;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.02232</link><description>&lt;p&gt;
&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#30340;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Experimental Design for Causal Effect Identification. (arXiv:2205.02232v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#26368;&#23567;&#25104;&#26412;&#30340;&#24178;&#39044;&#38598;&#21512;&#26469;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#38382;&#39064;&#30340;NP-hard&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#35299;&#25110;&#36817;&#20284;&#35299;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#38543;&#26426;&#22270;&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#39564;&#35777;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pearl&#30340;&#20570;&#27861;&#26159;&#19968;&#31181;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#23436;&#25972;&#20844;&#29702;&#26041;&#27861;&#12290;&#24403;&#36825;&#31181;&#25928;&#24212;&#19981;&#21487;&#35782;&#21035;&#26102;&#65292;&#38656;&#35201;&#25191;&#34892;&#19968;&#31995;&#21015;&#36890;&#24120;&#26114;&#36149;&#30340;&#24178;&#39044;&#26469;&#23398;&#20064;&#22240;&#26524;&#25928;&#24212;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#35774;&#35745;&#26368;&#23567;&#25104;&#26412;&#30340;&#24178;&#39044;&#38598;&#21512;&#26469;&#35782;&#21035;&#25152;&#38656;&#25928;&#24212;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#26159;NP-hard&#30340;&#65292;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#35299;&#25110;&#20854;&#23545;&#25968;&#22240;&#23376;&#30340;&#36817;&#20284;&#35299;&#12290;&#36825;&#26159;&#36890;&#36807;&#24314;&#31435;&#25105;&#20204;&#30340;&#38382;&#39064;&#19982;&#26368;&#23567;&#21629;&#20013;&#38598;&#38382;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#23454;&#29616;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22810;&#39033;&#24335;&#26102;&#38388;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#21487;&#33021;&#25214;&#21040;&#27425;&#20248;&#35299;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#22312;&#38543;&#26426;&#22270;&#19978;&#21462;&#24471;&#20102;&#36739;&#23567;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pearl's do calculus is a complete axiomatic approach to learn the identifiable causal effects from observational data. When such an effect is not identifiable, it is necessary to perform a collection of often costly interventions in the system to learn the causal effect. In this work, we consider the problem of designing the collection of interventions with the minimum cost to identify the desired effect. First, we prove that this problem is NP-hard, and subsequently propose an algorithm that can either find the optimal solution or a logarithmic-factor approximation of it. This is done by establishing a connection between our problem and the minimum hitting set problem. Additionally, we propose several polynomial-time heuristic algorithms to tackle the computational complexity of the problem. Although these algorithms could potentially stumble on sub-optimal solutions, our simulations show that they achieve small regrets on random graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#32416;&#27491;&#30340;&#36816;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#36816;&#21160;&#21644;&#32416;&#27491;&#19981;&#30495;&#23454;&#20266;&#24433;&#26469;&#25552;&#39640;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.09116</link><description>&lt;p&gt;
MotionAug: &#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#30340;&#29289;&#29702;&#32416;&#27491;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MotionAug: Augmentation with Physical Correction for Human Motion Prediction. (arXiv:2203.09116v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#32416;&#27491;&#30340;&#36816;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#36816;&#21160;&#21644;&#32416;&#27491;&#19981;&#30495;&#23454;&#20266;&#24433;&#26469;&#25552;&#39640;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#65292;&#20854;&#20013;&#21253;&#25324;&#40723;&#21169;&#22810;&#26679;&#24615;&#30340;&#36816;&#21160;&#21512;&#25104;&#21644;&#24378;&#21046;&#29289;&#29702;&#21512;&#29702;&#24615;&#30340;&#36816;&#21160;&#32416;&#27491;&#12290;&#36825;&#31181;&#36816;&#21160;&#21512;&#25104;&#21253;&#25324;&#25105;&#20204;&#20462;&#25913;&#21518;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#21644;&#36870;&#36816;&#21160;&#23398;&#65288;IK&#65289;&#12290;&#22312;&#36825;&#20010;VAE&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#38468;&#36817;&#26679;&#26412;&#37319;&#26679;&#26041;&#27861;&#21487;&#20197;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#21508;&#31181;&#26377;&#25928;&#30340;&#36816;&#21160;&#12290;&#25105;&#20204;&#22522;&#20110;IK&#30340;&#36816;&#21160;&#21512;&#25104;&#26041;&#27861;&#21487;&#20197;&#21322;&#33258;&#21160;&#22320;&#29983;&#25104;&#21508;&#31181;&#36816;&#21160;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#26041;&#26696;&#22312;&#21512;&#25104;&#30340;&#36816;&#21160;&#20013;&#20250;&#20135;&#29983;&#19981;&#30495;&#23454;&#30340;&#20266;&#24433;&#65292;&#25105;&#20204;&#30340;&#36816;&#21160;&#32416;&#27491;&#26041;&#27861;&#20250;&#23545;&#20854;&#36827;&#34892;&#26657;&#27491;&#12290;&#36825;&#31181;&#36816;&#21160;&#32416;&#27491;&#26041;&#26696;&#21253;&#25324;&#20351;&#29992;&#29289;&#29702;&#27169;&#25311;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#21518;&#32493;&#30340;&#36816;&#21160;&#21435;&#20559;&#20506;&#12290;&#23545;&#20110;&#36825;&#20010;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#30340;PD&#27531;&#20313;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#36816;&#21160;&#21435;&#20559;&#20506;&#25104;&#21151;&#22320;&#25269;&#28040;&#20102;&#27169;&#20223;&#23398;&#20064;&#24341;&#36215;&#30340;&#36816;&#21160;&#20559;&#20506;&#65292;&#20197;&#26368;&#22823;&#21270;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a motion data augmentation scheme incorporating motion synthesis encouraging diversity and motion correction imposing physical plausibility. This motion synthesis consists of our modified Variational AutoEncoder (VAE) and Inverse Kinematics (IK). In this VAE, our proposed sampling-near-samples method generates various valid motions even with insufficient training motion data. Our IK-based motion synthesis method allows us to generate a variety of motions semi-automatically. Since these two schemes generate unrealistic artifacts in the synthesized motions, our motion correction rectifies them. This motion correction scheme consists of imitation learning with physics simulation and subsequent motion debiasing. For this imitation learning, we propose the PD-residual force that significantly accelerates the training process. Furthermore, our motion debiasing successfully offsets the motion bias induced by imitation learning to maximize the effect of augmentation. As a r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#40654;&#26364;&#27969;&#24418;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#65292;&#36890;&#36807;&#23398;&#20064;&#40654;&#26364;&#24230;&#37327;&#21644;&#20351;&#29992;&#27979;&#22320;&#32447;&#29983;&#25104;&#30340;&#36816;&#21160;&#33021;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#23454;&#29616;&#36991;&#38556;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.07761</link><description>&lt;p&gt;
&#22312;&#23398;&#20064;&#30340;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#21453;&#24212;&#24335;&#36816;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reactive Motion Generation on Learned Riemannian Manifolds. (arXiv:2203.07761v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#40654;&#26364;&#27969;&#24418;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#65292;&#36890;&#36807;&#23398;&#20064;&#40654;&#26364;&#24230;&#37327;&#21644;&#20351;&#29992;&#27979;&#22320;&#32447;&#29983;&#25104;&#30340;&#36816;&#21160;&#33021;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#23454;&#29616;&#36991;&#38556;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#21313;&#24180;&#20013;&#65292;&#36816;&#21160;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#24182;&#36866;&#24212;&#26410;&#30693;&#26465;&#20214;&#12290;&#23454;&#36341;&#20013;&#65292;&#36816;&#21160;&#23398;&#20064;&#26041;&#27861;&#25429;&#25417;&#30456;&#20851;&#27169;&#24335;&#24182;&#35843;&#25972;&#23427;&#20204;&#20197;&#36866;&#24212;&#21160;&#24577;&#36991;&#38556;&#25110;&#21487;&#21464;&#30446;&#26631;&#31561;&#26032;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#40654;&#26364;&#27969;&#24418;&#30340;&#35282;&#24230;&#30740;&#31350;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#20154;&#31867;&#31034;&#25945;&#21487;&#20197;&#23398;&#20064;&#21040;&#40654;&#26364;&#27969;&#24418;&#65292;&#20854;&#20013;&#27979;&#22320;&#32447;&#26159;&#33258;&#28982;&#30340;&#36816;&#21160;&#25216;&#33021;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#22411;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#29983;&#25104;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#40654;&#26364;&#24230;&#37327;&#20135;&#29983;&#30340;&#27979;&#22320;&#32447;&#65292;&#35813;&#33258;&#32534;&#30721;&#22120;&#29305;&#21035;&#29992;&#20110;&#24674;&#22797;&#20840;&#20301;&#23039;&#26411;&#31471;&#25191;&#34892;&#22120;&#29366;&#24577;&#21644;&#20851;&#33410;&#31354;&#38388;&#37197;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22609;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#26469;&#20419;&#36827;&#26411;&#31471;&#25191;&#34892;&#22120;/&#22810;&#32930;&#20307;&#36991;&#38556;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20351;&#29992;&#20102;&#19968;&#20010;&#33021;&#22815;&#24863;&#30693;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#24230;&#37327;&#12290;&#20351;&#29992;&#36825;&#20123;&#27979;&#22320;&#32447;&#29983;&#25104;&#30340;&#36816;&#21160;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
In recent decades, advancements in motion learning have enabled robots to acquire new skills and adapt to unseen conditions in both structured and unstructured environments. In practice, motion learning methods capture relevant patterns and adjust them to new conditions such as dynamic obstacle avoidance or variable targets. In this paper, we investigate the robot motion learning paradigm from a Riemannian manifold perspective. We argue that Riemannian manifolds may be learned via human demonstrations in which geodesics are natural motion skills. The geodesics are generated using a learned Riemannian metric produced by our novel variational autoencoder (VAE), which is especially intended to recover full-pose end-effector states and joint space configurations. In addition, we propose a technique for facilitating on-the-fly end-effector/multiple-limb obstacle avoidance by reshaping the learned manifold using an obstacle-aware ambient metric. The motion generated using these geodesics may
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#22312;&#31038;&#20250;&#32972;&#26223;&#19979;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#39044;&#27979;&#22120;&#36827;&#34892;&#27169;&#22411;&#22810;&#26679;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#20010;&#20154;&#21463;&#20260;&#23475;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2203.07139</link><description>&lt;p&gt;
&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#65306;&#22810;&#27169;&#22411;&#24773;&#20917;&#19979;&#30340;&#20844;&#24179;&#24615;&#19982;&#20262;&#29702;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cross-model Fairness: Empirical Study of Fairness and Ethics Under Model Multiplicity. (arXiv:2203.07139v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#22312;&#31038;&#20250;&#32972;&#26223;&#19979;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#39044;&#27979;&#22120;&#36827;&#34892;&#27169;&#22411;&#22810;&#26679;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#20010;&#20154;&#21463;&#20260;&#23475;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#27979;&#27169;&#22411;&#26159;&#19968;&#20010;&#20005;&#26684;&#30340;&#25216;&#26415;&#26500;&#36896;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#22312;&#31038;&#20250;&#32972;&#26223;&#19979;&#36816;&#20316;&#65292;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#21892;&#24847;&#30340;&#24037;&#31243;&#36873;&#25321;&#21487;&#33021;&#24102;&#26469;&#38544;&#21547;&#30340;&#12289;&#38388;&#25509;&#30340;&#21644;&#24847;&#24819;&#19981;&#21040;&#30340;&#29616;&#23454;&#21518;&#26524;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#65292;&#28041;&#21450;&#21040;&#20010;&#20154;&#21644;&#32676;&#20307;&#65292;&#26159;&#19968;&#20010;&#30456;&#20851;&#30340;&#32771;&#34385;&#22240;&#32032;&#65307;&#23427;&#22312;&#25968;&#25454;&#25429;&#25417;&#21487;&#23548;&#33268;&#20154;&#20204;&#21463;&#21040;&#27495;&#35270;&#30340;&#21463;&#20445;&#25252;&#29305;&#24449;&#26102;&#20986;&#29616;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#27010;&#24565;&#20027;&#35201;&#38024;&#23545;&#22266;&#23450;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#20998;&#31867;&#38408;&#20540;&#19979;&#36827;&#34892;&#30740;&#31350;&#65292;&#21147;&#22270;&#35782;&#21035;&#21644;&#28040;&#38500;&#20854;&#36816;&#20316;&#20013;&#19981;&#24076;&#26395;&#30340;&#12289;&#20855;&#26377;&#27495;&#35270;&#24615;&#21644;&#21487;&#33021;&#36829;&#27861;&#30340;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#28335;&#20102;&#36825;&#20010;&#22266;&#23450;&#27169;&#22411;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#24182;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#21363;&#22312;&#20174;&#19968;&#32452;&#34920;&#29616;&#21516;&#26679;&#20986;&#33394;&#30340;&#27169;&#22411;&#20013;&#29305;&#23450;&#36873;&#25321;&#39044;&#27979;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20010;&#20154;&#21487;&#33021;&#21463;&#21040;&#20260;&#23475;&#65292;&#21363;&#22312;&#22522;&#20110;&#25928;&#29992;&#30340;&#27169;&#22411;&#22810;&#26679;&#24615;&#30340;&#35270;&#22270;&#19979;&#12290;&#30001;&#20110;&#19968;&#20010;&#20154;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#19979;&#21487;&#33021;&#34987;&#20998;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
While data-driven predictive models are a strictly technological construct, they may operate within a social context in which benign engineering choices entail implicit, indirect and unexpected real-life consequences. Fairness of such systems -- pertaining both to individuals and groups -- is one relevant consideration in this space; it arises when data capture protected characteristics upon which people may be discriminated. To date, this notion has predominantly been studied for a fixed model, often under different classification thresholds, striving to identify and eradicate undesirable, discriminative and possibly unlawful aspects of its operation. Here, we backtrack on this fixed model assumption to propose and explore a novel definition of cross-model fairness where individuals can be harmed when one predictor is chosen ad hoc from a group of equally-well performing models, i.e., in view of utility-based model multiplicity. Since a person may be classified differently across mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#24191;&#20041;&#23637;&#24320;&#24418;&#24335;&#21338;&#24328;&#20013;&#23398;&#20064;&#36817;&#20284;&#32435;&#20160;&#24179;&#34913;&#35299;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;SIX-OMD&#26469;&#26368;&#23567;&#21270;&#36951;&#25022;&#24182;&#25913;&#21892;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2203.05920</link><description>&lt;p&gt;
&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#24191;&#20041;&#23637;&#24320;&#24418;&#24335;&#21338;&#24328;&#20013;&#30340;&#24191;&#20041;&#36172;&#21338;&#36951;&#25022;&#26368;&#23567;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Generalized Bandit Regret Minimizer Framework in Imperfect Information Extensive-Form Game. (arXiv:2203.05920v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#24191;&#20041;&#23637;&#24320;&#24418;&#24335;&#21338;&#24328;&#20013;&#23398;&#20064;&#36817;&#20284;&#32435;&#20160;&#24179;&#34913;&#35299;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;SIX-OMD&#26469;&#26368;&#23567;&#21270;&#36951;&#25022;&#24182;&#25913;&#21892;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#27861;&#26159;&#23398;&#20064;&#36817;&#20284;&#32435;&#20160;&#24179;&#34913;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#29992;&#20110;&#20004;&#20154;&#38646;&#21644;&#19981;&#23436;&#20840;&#20449;&#24687;&#24191;&#20041;&#23637;&#24320;&#24418;&#24335;&#21338;&#24328;&#12290;&#25105;&#20204;&#32771;&#34385;&#20132;&#20114;&#36172;&#21338;&#21453;&#39304;&#35774;&#32622;&#19979;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#19981;&#30693;&#36947;&#19981;&#23436;&#20840;&#20449;&#24687;&#24191;&#20041;&#23637;&#24320;&#24418;&#24335;&#21338;&#24328;&#30340;&#21160;&#24577;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#21482;&#26377;&#20132;&#20114;&#36712;&#36857;&#21644;&#21040;&#36798;&#32456;&#27490;&#33410;&#28857;&#30340;&#20215;&#20540;$v(z^t)$&#34987;&#25581;&#31034;&#12290;&#20026;&#20102;&#23398;&#20064;&#32435;&#20160;&#24179;&#34913;&#35299;&#65292;&#36951;&#25022;&#26368;&#23567;&#21270;&#22120;&#38656;&#35201;&#36890;&#36807;$v(z^t)$&#20272;&#35745;&#23436;&#20840;&#21453;&#39304;&#25439;&#22833;&#26799;&#24230;$\ell^t$&#24182;&#26368;&#23567;&#21270;&#36951;&#25022;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#36825;&#31181;&#23398;&#20064;&#35774;&#32622;&#30340;&#24191;&#20041;&#26694;&#26550;&#12290;&#23427;&#20026;&#36172;&#21338;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#27861;&#30340;&#35774;&#35745;&#21644;&#27169;&#22359;&#21270;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#26032;&#30340;&#36172;&#21338;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#29305;&#20363;&#36827;&#34892;&#20998;&#26512;&#12290;&#22312;&#25353;&#29031;&#35813;&#26694;&#26550;&#36827;&#34892;&#25805;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;SIX-OMD&#30340;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#36817;&#20284;&#32435;&#20160;&#24179;&#34913;&#35299;&#12290;&#23427;&#26159;&#26080;&#27169;&#22411;&#30340;&#65292;&#24182;&#19988;&#26497;&#22823;&#22320;&#25913;&#21892;&#20102;&#26368;&#22909;&#30340;&#29616;&#26377;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regret minimization methods are a powerful tool for learning approximate Nash equilibrium (NE) in two-player zero-sum imperfect information extensive-form games (IIEGs). We consider the problem in the interactive bandit-feedback setting where we don't know the dynamics of the IIEG. In general, only the interactive trajectory and the reached terminal node value $v(z^t)$ are revealed. To learn NE, the regret minimizer is required to estimate the full-feedback loss gradient $\ell^t$ by $v(z^t)$ and minimize the regret. In this paper, we propose a generalized framework for this learning setting. It presents a theoretical framework for the design and the modular analysis of the bandit regret minimization methods. We demonstrate that the most recent bandit regret minimization methods can be analyzed as a particular case of our framework. Following this framework, we describe a novel method SIX-OMD to learn approximate NE. It is model-free and extremely improves the best existing convergence 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#24046;&#24322;&#30340;&#38750;&#21512;&#20316;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#30340;&#22810;&#23610;&#24230;&#12289;&#22810;&#20219;&#21153;CNN&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#35757;&#32451;&#20849;&#20139;&#32534;&#30721;&#22120;&#20197;&#23398;&#20064;&#36890;&#29992;&#29305;&#24449;&#12290;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#22495;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#27169;&#22411;&#30340;&#26631;&#20934;&#21270;&#23618;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2203.04275</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#24046;&#24322;&#30340;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22312;&#32447;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap. (arXiv:2203.04275v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#24046;&#24322;&#30340;&#38750;&#21512;&#20316;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#30340;&#22810;&#23610;&#24230;&#12289;&#22810;&#20219;&#21153;CNN&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#35757;&#32451;&#20849;&#20139;&#32534;&#30721;&#22120;&#20197;&#23398;&#20064;&#36890;&#29992;&#29305;&#24449;&#12290;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32447;&#22495;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#27169;&#22411;&#30340;&#26631;&#20934;&#21270;&#23618;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Spacecraft Pose Network v2&#65288;SPNv2&#65289;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#24046;&#24322;&#30340;&#38750;&#21512;&#20316;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#12290;SPNv2&#26159;&#19968;&#20010;&#22810;&#23610;&#24230;&#12289;&#22810;&#20219;&#21153;&#30340;CNN&#65292;&#30001;&#20849;&#20139;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#32534;&#30721;&#22120;&#21644;&#22810;&#20010;&#39044;&#27979;&#22836;&#32452;&#25104;&#65292;&#36825;&#20123;&#39044;&#27979;&#22836;&#22312;&#20849;&#20139;&#29305;&#24449;&#36755;&#20986;&#19978;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#37117;&#19982;&#20174;&#22270;&#20687;&#20013;&#26816;&#27979;&#21644;&#20272;&#35745;&#30446;&#26631;&#33322;&#22825;&#22120;&#30340;&#23039;&#24577;&#26377;&#20851;&#65292;&#20363;&#22914;&#39044;&#27979;&#39044;&#23450;&#20041;&#30340;&#21355;&#26143;&#20851;&#38190;&#28857;&#12289;&#30452;&#25509;&#23039;&#24577;&#22238;&#24402;&#21644;&#21355;&#26143;&#21069;&#26223;&#30340;&#20108;&#20803;&#20998;&#21106;&#31561;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#25968;&#25454;&#22686;&#24378;&#26469;&#20849;&#21516;&#35757;&#32451;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#20849;&#20139;&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#23545;&#20855;&#26377;&#22522;&#26412;&#19981;&#21516;&#35270;&#35273;&#29305;&#24615;&#30340;&#22270;&#20687;&#22495;&#26159;&#36890;&#29992;&#30340;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Online Domain Refinement&#65288;ODR&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30446;&#26631;&#22495;&#19978;&#35843;&#25972;SPNv2&#30340;&#26631;&#20934;&#21270;&#23618;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural Network (CNN) for pose estimation of noncooperative spacecraft across domain gap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared multi-scale feature encoder and multiple prediction heads that perform different tasks on a shared feature output. These tasks are all related to detection and pose estimation of a target spacecraft from an image, such as prediction of pre-defined satellite keypoints, direct pose regression, and binary segmentation of the satellite foreground. It is shown that by jointly training on different yet related tasks with extensive data augmentations on synthetic images only, the shared encoder learns features that are common across image domains that have fundamentally different visual characteristics compared to synthetic images. This work also introduces Online Domain Refinement (ODR) which refines the parameters of the normalization layers of SPNv2 on the target doma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#24230;&#25968;&#19981;&#21464;&#30340;&#38543;&#26426;&#21270;&#21709;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#23646;&#24615;&#22270;&#20013;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#30340;&#36793;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;&#36890;&#36807;&#20351;&#29992;Warner&#30340;&#38543;&#26426;&#21270;&#21709;&#24212;&#21644;&#31574;&#30053;&#24615;&#36793;&#37319;&#26679;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#21516;&#26102;&#20445;&#30041;&#22270;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2202.10209</link><description>&lt;p&gt;
&#20445;&#25345;&#24230;&#25968;&#19981;&#21464;&#30340;&#38543;&#26426;&#21270;&#21709;&#24212;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Degree-Preserving Randomized Response for Graph Neural Networks under Local Differential Privacy. (arXiv:2202.10209v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#24230;&#25968;&#19981;&#21464;&#30340;&#38543;&#26426;&#21270;&#21709;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#23646;&#24615;&#22270;&#20013;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#30340;&#36793;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;&#36890;&#36807;&#20351;&#29992;Warner&#30340;&#38543;&#26426;&#21270;&#21709;&#24212;&#21644;&#31574;&#30053;&#24615;&#36793;&#37319;&#26679;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#21516;&#26102;&#20445;&#30041;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#22270;&#31070;&#32463;&#32593;&#32476;(Differentially private GNNs)&#26469;&#22312;&#22270;&#25968;&#25454;&#19978;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#21516;&#26102;&#24378;&#21147;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#29305;&#21035;&#22320;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;(LDP)&#20445;&#25252;&#26377;&#23646;&#24615;&#30340;&#22270;&#20013;&#27599;&#20010;&#29992;&#25143;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#32780;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#24378;&#38544;&#31169;&#27010;&#24565;&#65292;&#19981;&#38656;&#35201;&#21487;&#20449;&#31532;&#19977;&#26041;&#12290;&#28982;&#32780;&#65292;&#35813;&#31639;&#27861;&#19981;&#20445;&#25252;&#31038;&#20132;&#22270;&#20013;&#30340;&#36793;&#65288;&#21451;&#35850;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#20445;&#25252;&#26080;&#23646;&#24615;&#22270;&#20013;&#30340;&#29992;&#25143;&#38544;&#31169;&#12290;&#22914;&#20309;&#22312;&#26080;&#23646;&#24615;&#22270;&#20013;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#30340;&#24378;&#38544;&#31169;&#20445;&#25252;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LDP&#31639;&#27861;&#65292;&#21517;&#20026;DPRR&#65288;Degree-Preserving Randomized Response&#65289;&#65292;&#29992;&#20110;&#22312;GNN&#20013;&#25552;&#20379;&#36793;&#30340;LDP&#12290;&#25105;&#20204;&#30340;DPRR&#22312;&#25552;&#20379;&#36793;&#30340;LDP&#30340;&#21516;&#26102;&#20445;&#30041;&#20102;&#27599;&#20010;&#29992;&#25143;&#30340;&#24230;&#25968;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#22270;&#32467;&#26500;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;DPRR&#20351;&#29992;&#20102;Warner&#30340;RR&#65288;Randomized Response&#65289;&#21644;&#31574;&#30053;&#24615;&#36793;&#37319;&#26679;&#65292;&#20854;&#20013;&#27599;&#20010;&#29992;&#25143;&#30340;&#37319;&#26679;&#27010;&#29575;&#26159;&#36890;&#36807;Lapla&#36827;&#34892;&#33258;&#21160;&#35843;&#25972;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private GNNs (Graph Neural Networks) have been recently studied to provide high accuracy in various tasks on graph data while strongly protecting user privacy. In particular, a recent study proposes an algorithm to protect each user's feature vector in an attributed graph with LDP (Local Differential Privacy), a strong privacy notion without a trusted third party. However, this algorithm does not protect edges (friendships) in a social graph, hence cannot protect user privacy in unattributed graphs. How to provide strong privacy with high accuracy in unattributed graphs remains open.  In this paper, we propose a novel LDP algorithm called the DPRR (Degree-Preserving Randomized Response) to provide LDP for edges in GNNs. Our DPRR preserves each user's degree hence a graph structure while providing edge LDP. Technically, our DPRR uses Warner's RR (Randomized Response) and strategic edge sampling, where each user's sampling probability is automatically tuned using the Lapla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#38543;&#26426;&#22359;&#27169;&#22411;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;$(\epsilon, \delta)$-&#36793;&#24046;&#20998;&#38544;&#31169;&#30340;&#27010;&#24565;&#65292;&#25506;&#35752;&#20102;$(p, q)$&#12289;DP&#39044;&#31639;$(\epsilon, \delta)$&#21644;&#31038;&#21306;&#26631;&#31614;&#30340;&#20934;&#30830;&#24674;&#22797;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2202.00636</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#19979;&#22522;&#20110;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#31038;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Community Detection for Stochastic Block Models. (arXiv:2202.00636v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#38543;&#26426;&#22359;&#27169;&#22411;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;$(\epsilon, \delta)$-&#36793;&#24046;&#20998;&#38544;&#31169;&#30340;&#27010;&#24565;&#65292;&#25506;&#35752;&#20102;$(p, q)$&#12289;DP&#39044;&#31639;$(\epsilon, \delta)$&#21644;&#31038;&#21306;&#26631;&#31614;&#30340;&#20934;&#30830;&#24674;&#22797;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#22270;&#20013;&#29992;&#25143;&#20043;&#38388;&#30340;&#36830;&#25509;&#24615;&#65288;&#30001;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#34920;&#31034;&#65289;&#30340;&#22522;&#30784;&#19978;&#24674;&#22797;&#29992;&#25143;&#30340;&#26631;&#31614;/&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#25919;&#27835;&#20542;&#21521;&#65289;&#12290;&#26368;&#36817;&#22312;&#29702;&#35299;&#20174;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#29983;&#25104;&#30340;&#22270;&#30340;&#31038;&#21306;&#26816;&#27979;&#30340;&#22522;&#26412;&#38480;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24050;&#32463;&#33719;&#24471;&#20102;&#38024;&#23545;$p$&#21644;$q$&#30340;SBM&#30340;&#23574;&#38160;&#20449;&#24687;&#35770;&#38480;&#21046;&#21644;&#39640;&#25928;&#31639;&#27861;&#65292;&#23427;&#20204;&#20998;&#21035;&#34920;&#31034;&#31038;&#21306;&#20869;&#37096;&#21644;&#31038;&#21306;&#38388;&#30340;&#36830;&#25509;&#27010;&#29575;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312;&#20445;&#25252;&#39030;&#28857;&#20043;&#38388;&#30340;&#20010;&#20307;&#36830;&#25509;&#65288;&#36793;&#65289;&#30340;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#20851;&#27880;$(\epsilon, \delta)$-&#36793;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#29702;&#35299;$(p, q)$&#12289;DP&#39044;&#31639;$(\epsilon, \delta)$&#21644;&#20855;&#20307;&#24674;&#22797;&#31038;&#21306;&#26631;&#31614;&#30340;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of community detection over graphs is to recover underlying labels/attributes of users (e.g., political affiliation) given the connectivity between users (represented by adjacency matrix of a graph). There has been significant recent progress on understanding the fundamental limits of community detection when the graph is generated from a stochastic block model (SBM). Specifically, sharp information theoretic limits and efficient algorithms have been obtained for SBMs as a function of $p$ and $q$, which represent the intra-community and inter-community connection probabilities. In this paper, we study the community detection problem while preserving the privacy of the individual connections (edges) between the vertices. Focusing on the notion of $(\epsilon, \delta)$-edge differential privacy (DP), we seek to understand the fundamental tradeoffs between $(p, q)$, DP budget $(\epsilon, \delta)$, and computational efficiency for exact recovery of the community labels.  To this en
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align&#65292;&#19968;&#31181;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#19968;&#33268;&#24615;&#33410;&#28857;&#23545;&#36880;&#27493;&#21457;&#29616;&#33410;&#28857;&#23545;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#65292;&#28982;&#21518;&#35745;&#31639;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#36880;&#27493;&#23545;&#40784;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2201.10945</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#36827;&#34892;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#30340;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Power of Gradual Network Alignment Using Dual-Perception Similarities. (arXiv:2201.10945v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.10945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align&#65292;&#19968;&#31181;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#19968;&#33268;&#24615;&#33410;&#28857;&#23545;&#36880;&#27493;&#21457;&#29616;&#33410;&#28857;&#23545;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#65292;&#28982;&#21518;&#35745;&#31639;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#36880;&#27493;&#23545;&#40784;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#65288;NA&#65289;&#26159;&#22522;&#20110;&#32593;&#32476;&#32467;&#26500;&#21644;&#33410;&#28857;&#23646;&#24615;&#26597;&#25214;&#20004;&#20010;&#32593;&#32476;&#20043;&#38388;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#22312;&#20110;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;NA&#26041;&#27861;&#37117;&#35797;&#22270;&#19968;&#27425;&#24615;&#21457;&#29616;&#25152;&#26377;&#33410;&#28857;&#23545;&#65292;&#22240;&#27492;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#36890;&#36807;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20013;&#38388;&#21457;&#29616;&#26469;&#26356;&#20934;&#30830;&#22320;&#25214;&#21040;&#33410;&#28857;&#21305;&#37197;&#36807;&#31243;&#20013;&#30340;&#19979;&#19968;&#20010;&#23545;&#24212;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Grad-Align&#65292;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22312;&#28176;&#36827;&#21305;&#37197;&#30340;&#26089;&#26399;&#38454;&#27573;&#23481;&#26131;&#21457;&#29616;&#30340;&#33410;&#28857;&#23545;&#26469;&#36880;&#27493;&#21457;&#29616;&#33410;&#28857;&#23545;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Grad-Align&#39318;&#20808;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25105;&#20204;&#30340;&#36880;&#23618;&#37325;&#26500;&#25439;&#22833;&#29983;&#25104;&#20004;&#20010;&#32593;&#32476;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#35745;&#31639;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#36880;&#27493;&#23545;&#40784;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment (NA) is the task of finding the correspondence of nodes between two networks based on the network structure and node attributes. Our study is motivated by the fact that, since most of existing NA methods have attempted to discover all node pairs at once, they do not harness information enriched through interim discovery of node correspondences to more accurately find the next correspondences during the node matching. To tackle this challenge, we propose Grad-Align, a new NA method that gradually discovers node pairs by making full use of node pairs exhibiting strong consistency, which are easy to be discovered in the early stage of gradual matching. Specifically, Grad-Align first generates node embeddings of the two networks based on graph neural networks along with our layer-wise reconstruction loss, a loss built upon capturing the first-order and higher-order neighborhood structures. Then, nodes are gradually aligned by computing dual-perception similarity measures 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#26368;&#23567;&#21270;&#26368;&#22823;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#65288;MRCs&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#21487;&#33021;&#21253;&#21547;&#22522;&#30784;&#20998;&#24067;&#30340;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30456;&#23545;&#24212;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;0-1&#25439;&#22833;&#26469;&#25552;&#20379;&#20005;&#26684;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#21644;&#29305;&#24449;&#26680;&#65292;MRCs&#22312;&#23398;&#20064;&#26102;&#20855;&#26377;&#24378;&#24230;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20248;&#21270;&#25216;&#26415;&#21644;&#20934;&#30830;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2201.06487</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#26368;&#22823;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#19982;0-1&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Minimax risk classifiers with 0-1 loss. (arXiv:2201.06487v6 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#26368;&#23567;&#21270;&#26368;&#22823;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#65288;MRCs&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#21487;&#33021;&#21253;&#21547;&#22522;&#30784;&#20998;&#24067;&#30340;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30456;&#23545;&#24212;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;0-1&#25439;&#22833;&#26469;&#25552;&#20379;&#20005;&#26684;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#21644;&#29305;&#24449;&#26680;&#65292;MRCs&#22312;&#23398;&#20064;&#26102;&#20855;&#26377;&#24378;&#24230;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20248;&#21270;&#25216;&#26415;&#21644;&#20934;&#30830;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#20998;&#31867;&#25216;&#26415;&#20351;&#29992;&#35757;&#32451;&#26679;&#26412;&#26469;&#23398;&#20064;&#19968;&#31181;&#20855;&#26377;&#23567;&#26399;&#26395;0-1&#25439;&#22833;&#65288;&#38169;&#35823;&#27010;&#29575;&#65289;&#30340;&#20998;&#31867;&#35268;&#21017;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#25439;&#22833;&#32780;&#19981;&#26159;0-1&#25439;&#22833;&#65292;&#24182;&#32771;&#34385;&#29305;&#23450;&#30340;&#35268;&#21017;&#26063;&#65288;&#20551;&#35774;&#31867;&#65289;&#26469;&#23454;&#29616;&#21487;&#35745;&#31639;&#30340;&#23398;&#20064;&#21644;&#26679;&#26412;&#22806;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#21270;&#26368;&#22823;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#65288;MRCs&#65289;&#65292;&#23427;&#20204;&#26368;&#23567;&#21270;&#19982;&#21487;&#20197;&#21253;&#21547;&#22522;&#30784;&#20998;&#24067;&#30340;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30456;&#23545;&#24212;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;0-1&#25439;&#22833;&#65292;&#20855;&#26377;&#21487;&#35843;&#30340;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;MRCs&#21487;&#20197;&#22312;&#23398;&#20064;&#26102;&#25552;&#20379;&#20005;&#26684;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#19988;&#20351;&#29992;&#30001;&#29305;&#24449;&#26144;&#23556;&#32473;&#20986;&#30340;&#29305;&#24449;&#26680;&#26159;&#24378;&#24230;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;MRC&#23398;&#20064;&#30340;&#39640;&#25928;&#20248;&#21270;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#20998;&#31867;&#21644;&#20005;&#26684;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised classification techniques use training samples to learn a classification rule with small expected 0-1 loss (error probability). Conventional methods enable tractable learning and provide out-of-sample generalization by using surrogate losses instead of the 0-1 loss and considering specific families of rules (hypothesis classes). This paper presents minimax risk classifiers (MRCs) that minize the worst-case 0-1 loss with respect to uncertainty sets of distributions that can include the underlying distribution, with a tunable confidence. We show that MRCs can provide tight performance guarantees at learning and are strongly universally consistent using feature mappings given by characteristic kernels. The paper also proposes efficient optimization techniques for MRC learning and shows that the methods presented can provide accurate classification together with tight performance guarantees in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#19981;&#38656;&#35201;&#35843;&#25972;&#25351;&#25968;&#21442;&#25968;&#30340;MNL-Contextual Bandit&#38382;&#39064;&#30340;&#31616;&#20415;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#31639;&#27861;&#20855;&#26377;&#19982;&#35813;&#38382;&#39064;&#30340;&#26368;&#20339;&#29702;&#35770;&#30028;&#38480;&#21305;&#37197;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2011.14033</link><description>&lt;p&gt;
MNL&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#30340;&#31616;&#20415;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Tractable Online Learning Algorithm for the Multinomial Logit Contextual Bandit. (arXiv:2011.14033v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.14033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#19981;&#38656;&#35201;&#35843;&#25972;&#25351;&#25968;&#21442;&#25968;&#30340;MNL-Contextual Bandit&#38382;&#39064;&#30340;&#31616;&#20415;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#31639;&#27861;&#20855;&#26377;&#19982;&#35813;&#38382;&#39064;&#30340;&#26368;&#20339;&#29702;&#35770;&#30028;&#38480;&#21305;&#37197;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;MNL-Bandit&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#21464;&#20307;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21160;&#24577;&#38598;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#32773;&#21521;&#28040;&#36153;&#32773;&#25552;&#20379;&#19968;&#32452;&#20135;&#21697;&#65288;&#36141;&#29289;&#28165;&#21333;&#65289;&#65292;&#24182;&#22312;&#27599;&#20010;&#22238;&#21512;&#35266;&#23519;&#21709;&#24212;&#12290;&#28040;&#36153;&#32773;&#36141;&#20080;&#20135;&#21697;&#20197;&#26368;&#22823;&#21270;&#20182;&#20204;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#20551;&#35774;&#19968;&#32452;&#23646;&#24615;&#25551;&#36848;&#20102;&#20135;&#21697;&#65292;&#20135;&#21697;&#30340;&#24179;&#22343;&#25928;&#29992;&#19982;&#36825;&#20123;&#23646;&#24615;&#30340;&#20540;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#20351;&#29992;&#30340;Multinomial Logit&#65288;MNL&#65289;&#27169;&#22411;&#24314;&#27169;&#28040;&#36153;&#32773;&#36873;&#25321;&#34892;&#20026;&#65292;&#24182;&#32771;&#34385;&#22312;&#20248;&#21270;&#38144;&#21806;&#21608;&#26399;$T$&#20869;&#32047;&#31215;&#25910;&#30410;&#30340;&#21516;&#26102;&#21160;&#24577;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#30340;&#20915;&#31574;&#32773;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20010;&#38382;&#39064;&#36817;&#26469;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#20182;&#20204;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#21462;&#20915;&#20110;&#19968;&#20010;&#21487;&#33021;&#38750;&#24120;&#22823;&#30340;&#38382;&#39064;&#30456;&#20851;&#21442;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#35843;&#25972;&#38543;&#30528;&#23646;&#24615;&#38598;&#35268;&#27169;&#25351;&#25968;&#22686;&#38271;&#30340;&#35843;&#25972;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MNL-Contextual Bandit&#38382;&#39064;&#30340;&#31616;&#20415;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#35843;&#25972;&#27492;&#31867;&#25351;&#25968;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#19982;&#35813;&#38382;&#39064;&#30340;&#26368;&#20339;&#29702;&#35770;&#30028;&#38480;&#21305;&#37197;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the contextual variant of the MNL-Bandit problem. More specifically, we consider a dynamic set optimization problem, where a decision-maker offers a subset (assortment) of products to a consumer and observes the response in every round. Consumers purchase products to maximize their utility. We assume that a set of attributes describe the products, and the mean utility of a product is linear in the values of these attributes. We model consumer choice behavior using the widely used Multinomial Logit (MNL) model and consider the decision maker problem of dynamically learning the model parameters while optimizing cumulative revenue over the selling horizon $T$. Though this problem has attracted considerable attention in recent times, many existing methods often involve solving an intractable non-convex optimization problem. Their theoretical performance guarantees depend on a problem-dependent parameter which could be prohibitively large. In particular, existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#20989;&#25968;&#22797;&#26434;&#24615;&#65288;LFC&#65289;&#30340;&#20272;&#35745;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#23616;&#37096;&#32467;&#26500;&#22797;&#26434;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21457;&#23637;&#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#30340;&#23616;&#37096;&#22810;&#39033;&#24335;&#24179;&#28369;&#65288;LPS&#65289;&#27169;&#22411;&#30340;&#31867;&#27604;&#65292;&#20351;&#24471;&#35813;&#26694;&#26550;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>http://arxiv.org/abs/1902.10664</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#28151;&#21512;&#23454;&#29616;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#23616;&#37096;&#20989;&#25968;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Local Function Complexity for Active Learning via Mixture of Gaussian Processes. (arXiv:1902.10664v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.10664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#20989;&#25968;&#22797;&#26434;&#24615;&#65288;LFC&#65289;&#30340;&#20272;&#35745;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#23616;&#37096;&#32467;&#26500;&#22797;&#26434;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21457;&#23637;&#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#30340;&#23616;&#37096;&#22810;&#39033;&#24335;&#24179;&#28369;&#65288;LPS&#65289;&#27169;&#22411;&#30340;&#31867;&#27604;&#65292;&#20351;&#24471;&#35813;&#26694;&#26550;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#30340;&#19981;&#22343;&#21248;&#24615;&#65292;&#20363;&#22914;&#35266;&#27979;&#22122;&#22768;&#27700;&#24179;&#30340;&#21464;&#21270;&#25110;&#28304;&#20989;&#25968;&#32467;&#26500;&#22797;&#26434;&#24615;&#30340;&#21464;&#21270;&#65292;&#32473;&#32479;&#35745;&#25512;&#26029;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#22240;&#32032;&#21487;&#20197;&#22312;&#29289;&#29702;&#36164;&#28304;&#25110;&#35745;&#31639;&#26102;&#38388;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#25991;&#20511;&#37492;&#20102;&#26368;&#36817;&#20851;&#20110;&#23616;&#37096;&#22810;&#39033;&#24335;&#24179;&#28369;&#65288;LPS&#65289;&#39046;&#22495;&#20013;&#23616;&#37096;&#20989;&#25968;&#22797;&#26434;&#24615;&#65288;LFC&#65289;&#30340;&#20272;&#35745;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#23616;&#37096;&#32467;&#26500;&#22797;&#26434;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#29992;&#23427;&#26469;&#24320;&#21457;&#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26694;&#26550;&#12290;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#28857;&#20272;&#35745;&#65292;LPS&#27169;&#22411;&#31867;&#22312;&#22788;&#29702;&#36890;&#24120;&#20276;&#38543;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#22823;&#36755;&#20837;&#31354;&#38388;&#32500;&#24230;&#26102;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#21644;&#20272;&#35745;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#30340;LPS-based LFC&#30340;&#31867;&#27604;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20197;&#19978;&#26694;&#26550;&#30340;&#26367;&#20195;&#65292;&#20351;&#20043;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inhomogeneities in real-world data, e.g., due to changes in the observation noise level or variations in the structural complexity of the source function, pose a unique set of challenges for statistical inference. Accounting for them can greatly improve predictive power when physical resources or computation time is limited. In this paper, we draw on recent theoretical results on the estimation of local function complexity (LFC), derived from the domain of local polynomial smoothing (LPS), to establish a notion of local structural complexity, which is used to develop a model-agnostic active learning (AL) framework. Due to its reliance on pointwise estimates, the LPS model class is not robust and scalable concerning large input space dimensions that typically come along with real-world problems. Here, we derive and estimate the Gaussian process regression (GPR)-based analog of the LPS-based LFC and use it as a substitute in the above framework to make it robust and scalable. We assess t
&lt;/p&gt;</description></item></channel></rss>