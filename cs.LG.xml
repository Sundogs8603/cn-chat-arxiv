<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20351;&#29992;&#25193;&#25955;&#24341;&#23548;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.01102</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#38646;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#29992;&#20110;&#36328;&#27169;&#24577;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01102
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#24341;&#23548;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#27169;&#24577;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#20351;&#29992;&#22312;&#28304;&#27169;&#24577;&#20013;&#35774;&#35745;&#30340;&#26041;&#27861;&#23545;&#30446;&#26631;&#27169;&#24577;&#36827;&#34892;&#20998;&#21106;&#12290;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#23558;&#30446;&#26631;&#27169;&#24577;&#22270;&#20687;&#36716;&#25442;&#20026;&#28304;&#27169;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#27169;&#24577;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#37327;&#20132;&#21449;&#27169;&#24577;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#20381;&#36182;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#26412;&#24037;&#20316;&#26088;&#22312;&#35299;&#20915;&#22522;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#22270;&#20687;&#32763;&#35793;&#20219;&#21153;&#30340;&#25361;&#25112;&#65288;&#26497;&#31471;&#24773;&#20917;&#19979;&#30446;&#26631;&#27169;&#24577;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#30693;&#65289;&#12290;&#20026;&#20102;&#21033;&#29992;&#29983;&#25104;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#22270;&#20687;&#20998;&#21106;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#22266;&#26377;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#36827;&#34892;&#25193;&#25955;&#24341;&#23548;&#65292;&#23398;&#20064;&#23558;&#26410;&#30693;&#28304;&#22270;&#20687;&#36716;&#25442;&#20026;&#30446;&#26631;&#27169;&#24577;&#20197;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25429;&#25417;&#20102;&#30456;&#21516;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01102v1 Announce Type: cross  Abstract: Cross-modality image segmentation aims to segment the target modalities using a method designed in the source modality. Deep generative models can translate the target modality images into the source modality, thus enabling cross-modality segmentation. However, a vast body of existing cross-modality image translation methods relies on supervised learning. In this work, we aim to address the challenge of zero-shot learning-based image translation tasks (extreme scenarios in the target modality is unseen in the training phase). To leverage generative learning for zero-shot cross-modality image segmentation, we propose a novel unsupervised image translation method. The framework learns to translate the unseen source image to the target modality for image segmentation by leveraging the inherent statistical consistency between different modalities for diffusion guidance. Our framework captures identical cross-modality features in the statis
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25299;&#25169;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#23545;Transformer&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20855;&#26377;&#39640;&#38454;&#25512;&#29702;&#30340;&#29305;&#28857;&#65292;&#24182;&#19982;&#20854;&#20182;&#24120;&#35265;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;</title><link>https://arxiv.org/abs/2403.18415</link><description>&lt;p&gt;
Transformer&#32593;&#32476;&#30340;&#25299;&#25169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Topos of Transformer Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18415
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#23545;Transformer&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20855;&#26377;&#39640;&#38454;&#25512;&#29702;&#30340;&#29305;&#28857;&#65292;&#24182;&#19982;&#20854;&#20182;&#24120;&#35265;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#36828;&#36828;&#36229;&#36234;&#25152;&#26377;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32972;&#21518;&#30340;&#24341;&#25806;&#12290;&#25105;&#20204;&#36890;&#36807;&#25299;&#25169;&#29702;&#35770;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#23545;Transformer&#26550;&#26500;&#34920;&#36798;&#33021;&#21147;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#20174;&#36825;&#20010;&#35266;&#28857;&#20986;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#24120;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22914;&#21367;&#31215;&#32593;&#32476;&#12289;&#24490;&#29615;&#32593;&#32476;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21487;&#20197;&#23884;&#20837;&#22312;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#39044;&#25299;&#25169;&#20013;&#65292;&#20294;Transformer&#24517;&#28982;&#23384;&#22312;&#20110;&#20854;&#25299;&#25169;&#23436;&#22791;&#24615;&#20013;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#34920;&#26126;&#36825;&#20004;&#20010;&#32593;&#32476;&#23478;&#26063;&#23454;&#20363;&#21270;&#20102;&#19981;&#21516;&#30340;&#36923;&#36753;&#29255;&#27573;&#65306;&#21069;&#32773;&#26159;&#19968;&#38454;&#30340;&#65292;&#32780;Transformers&#26159;&#39640;&#38454;&#25512;&#29702;&#26426;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25299;&#25169;&#29702;&#35770;&#19982;&#26550;&#26500;&#25628;&#32034;&#21644;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20102;&#31867;&#27604;&#65292;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#32435;&#20837;&#20102;&#25511;&#21046;&#35770;&#20195;&#29702;&#30340;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18415v1 Announce Type: new  Abstract: The transformer neural network has significantly out-shined all other neural network architectures as the engine behind large language models. We provide a theoretical analysis of the expressivity of the transformer architecture through the lens of topos theory. From this viewpoint, we show that many common neural network architectures, such as the convolutional, recurrent and graph convolutional networks, can be embedded in a pretopos of piecewise-linear functions, but that the transformer necessarily lives in its topos completion. In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas transformers are higher-order reasoners. Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20998;&#23618;&#36755;&#20986;&#21453;&#39304;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#20056;&#23376;&#35825;&#23548;&#30340;&#25439;&#22833;&#26223;&#35266;&#35843;&#24230;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#22797;&#26434;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13728</link><description>&lt;p&gt;
M-HOF-Opt: &#22810;&#30446;&#26631;&#20998;&#23618;&#36755;&#20986;&#21453;&#39304;&#20248;&#21270;&#65306;&#22522;&#20110;&#20056;&#23376;&#35825;&#23548;&#25439;&#22833;&#26223;&#35266;&#35843;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via Multiplier Induced Loss Landscape Scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20998;&#23618;&#36755;&#20986;&#21453;&#39304;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#20056;&#23376;&#35825;&#23548;&#30340;&#25439;&#22833;&#26223;&#35266;&#35843;&#24230;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#22797;&#26434;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#30001;&#35768;&#22810;&#39033;&#32452;&#25104;&#26102;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23545;&#26435;&#37325;&#20056;&#23376;&#30340;&#32452;&#21512;&#36873;&#25321;&#24418;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#65292;&#29992;&#20110;&#32852;&#21512;&#27169;&#22411;&#21442;&#25968;&#21644;&#20056;&#23376;&#28436;&#21270;&#36807;&#31243;&#65292;&#20855;&#26377;&#22522;&#20110;&#36229;&#20307;&#31215;&#30340;&#20284;&#28982;&#65292;&#20419;&#36827;&#27599;&#20010;&#25439;&#22833;&#39033;&#30340;&#22810;&#30446;&#26631;&#19979;&#38477;&#12290;&#30456;&#24212;&#30340;&#21442;&#25968;&#21644;&#20056;&#23376;&#20272;&#35745;&#20316;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#34987;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#30446;&#26631;&#19979;&#38477;&#30446;&#26631;&#34987;&#20998;&#23618;&#22320;&#20998;&#27966;&#21040;&#19968;&#31995;&#21015;&#32422;&#26463;&#20248;&#21270;&#23376;&#38382;&#39064;&#20013;&#12290;&#23376;&#38382;&#39064;&#32422;&#26463;&#26681;&#25454;&#24085;&#32047;&#25176;&#25903;&#37197;&#33258;&#21160;&#36866;&#24212;&#24182;&#20316;&#20026;&#20302;&#23618;&#20056;&#23376;&#25511;&#21046;&#22120;&#35843;&#24230;&#25439;&#22833;&#26223;&#35266;&#30340;&#35774;&#23450;&#28857;&#65292;&#36890;&#36807;&#27599;&#20010;&#25439;&#22833;&#39033;&#30340;&#36755;&#20986;&#21453;&#39304;&#26469;&#36816;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#20056;&#23376;&#30340;&#65292;&#24182;&#19988;&#22312;&#26102;&#20195;&#23610;&#24230;&#19978;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13728v1 Announce Type: new  Abstract: When a neural network parameterized loss function consists of many terms, the combinatorial choice of weight multipliers during the optimization process forms a challenging problem. To address this, we proposed a probabilistic graphical model (PGM) for the joint model parameter and multiplier evolution process, with a hypervolume based likelihood that promotes multi-objective descent of each loss term. The corresponding parameter and multiplier estimation as a sequential decision process is then cast into an optimal control problem, where the multi-objective descent goal is dispatched hierarchically into a series of constraint optimization sub-problems. The sub-problem constraint automatically adapts itself according to Pareto dominance and serves as the setpoint for the low level multiplier controller to schedule loss landscapes via output feedback of each loss term. Our method is multiplier-free and operates at the timescale of epochs,
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#29702;&#24615;&#21407;&#21017;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11782</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#20174;&#20559;&#22909;&#21644;&#36873;&#25321;&#20013;&#23398;&#20064;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A tutorial on learning from preferences and choices with Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#29702;&#24615;&#21407;&#21017;&#34701;&#20837;&#23398;&#20064;&#36807;&#31243;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24314;&#27169;&#20301;&#20110;&#32463;&#27982;&#23398;&#12289;&#20915;&#31574;&#29702;&#35770;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#29702;&#35299;&#20010;&#20307;&#30340;&#20559;&#22909;&#21450;&#20854;&#36873;&#25321;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#26356;&#25509;&#36817;&#20182;&#20204;&#26399;&#26395;&#30340;&#20135;&#21697;&#65292;&#20026;&#36328;&#39046;&#22495;&#30340;&#26356;&#39640;&#25928;&#12289;&#20010;&#24615;&#21270;&#24212;&#29992;&#38138;&#24179;&#36947;&#36335;&#12290;&#27492;&#25945;&#31243;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#36830;&#36143;&#12289;&#20840;&#38754;&#30340;&#20559;&#22909;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#28436;&#31034;&#22914;&#20309;&#23558;&#29702;&#24615;&#21407;&#21017;&#65288;&#26469;&#33258;&#32463;&#27982;&#23398;&#21644;&#20915;&#31574;&#29702;&#35770;&#65289;&#26080;&#32541;&#22320;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#21512;&#36866;&#22320;&#23450;&#21046;&#20284;&#28982;&#20989;&#25968;&#65292;&#36825;&#19968;&#26694;&#26550;&#20351;&#24471;&#33021;&#22815;&#26500;&#24314;&#28085;&#30422;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#12289;&#36776;&#35782;&#38480;&#21046;&#21644;&#23545;&#35937;&#21644;&#26631;&#31614;&#20559;&#22909;&#30340;&#22810;&#37325;&#20914;&#31361;&#25928;&#29992;&#24773;&#26223;&#30340;&#20559;&#22909;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11782v1 Announce Type: new  Abstract: Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals' preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30005;&#27744;&#20379;&#30005;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#25193;&#23637;&#24182;&#36129;&#29486;&#20110;TinyML&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.05106</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30340;&#30005;&#27744;&#20379;&#30005;TinyML&#31995;&#32479;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Simulating Battery-Powered TinyML Systems Optimised using Reinforcement Learning in Image-Based Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30005;&#27744;&#20379;&#30005;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#25193;&#23637;&#24182;&#36129;&#29486;&#20110;TinyML&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tiny&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#30340;&#36827;&#23637;&#20419;&#36827;&#20102;&#26234;&#33021;&#34892;&#19994;&#35299;&#20915;&#26041;&#26696;&#30340;&#21019;&#24314;&#65292;&#21253;&#25324;&#26234;&#33021;&#20892;&#19994;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#26234;&#33021;&#22478;&#24066;&#12290;&#26412;&#25991;&#36890;&#36807;&#20248;&#21270;&#30005;&#27744;&#20379;&#30005;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#24322;&#24120;&#26816;&#27979;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#31995;&#32479;&#65292;&#25193;&#23637;&#24182;&#20026;TinyML&#30740;&#31350;&#20570;&#20986;&#36129;&#29486;&#12290;&#21033;&#29992;&#27169;&#25311;&#24314;&#27169;&#65292;&#23545;RL&#31639;&#27861;&#23545;&#30005;&#27744;&#23551;&#21629;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05106v1 Announce Type: new  Abstract: Advances in Tiny Machine Learning (TinyML) have bolstered the creation of smart industry solutions, including smart agriculture, healthcare and smart cities. Whilst related research contributes to enabling TinyML solutions on constrained hardware, there is a need to amplify real-world applications by optimising energy consumption in battery-powered systems. The work presented extends and contributes to TinyML research by optimising battery-powered image-based anomaly detection Internet of Things (IoT) systems. Whilst previous work in this area has yielded the capabilities of on-device inferencing and training, there has yet to be an investigation into optimising the management of such capabilities using machine learning approaches, such as Reinforcement Learning (RL), to improve the deployment battery life of such systems. Using modelled simulations, the battery life effects of an RL algorithm are benchmarked against static and dynamic o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#25511;&#21046;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20248;&#21270;&#31574;&#30053;&#26469;&#20943;&#23569;&#27969;&#20307;&#27169;&#25311;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.16543</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21152;&#36895;&#27969;&#20307;&#27169;&#25311;&#20013;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-based deep reinforcement learning for accelerated learning from flow simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#25511;&#21046;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20248;&#21270;&#31574;&#30053;&#26469;&#20943;&#23569;&#27969;&#20307;&#27169;&#25311;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#38381;&#29615;&#27969;&#25511;&#38382;&#39064;&#30340;&#25216;&#26415;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;&#29615;&#22659;&#21487;&#20197;&#20107;&#20808;&#31471;&#21040;&#31471;&#22320;&#20248;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#20026;&#23433;&#20840;&#20851;&#38190;&#30340;&#25511;&#21046;&#24212;&#29992;&#25552;&#20379;&#34394;&#25311;&#35797;&#39564;&#24179;&#21488;&#65292;&#24182;&#19988;&#21487;&#20197;&#28145;&#20837;&#29702;&#35299;&#25511;&#21046;&#26426;&#21046;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#25511;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#22312;&#20174;&#27969;&#27169;&#25311;&#20013;&#37319;&#26679;&#30340;&#36712;&#36857;&#21644;&#20174;&#29615;&#22659;&#27169;&#22411;&#38598;&#21512;&#20013;&#37319;&#26679;&#30340;&#36712;&#36857;&#20043;&#38388;&#20132;&#26367;&#20248;&#21270;&#31574;&#30053;&#12290;&#27169;&#22411;&#20026;&#22522;&#30784;&#23398;&#20064;&#38477;&#20302;&#20102;&#25972;&#20307;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16543v1 Announce Type: cross  Abstract: In recent years, deep reinforcement learning has emerged as a technique to solve closed-loop flow control problems. Employing simulation-based environments in reinforcement learning enables a priori end-to-end optimization of the control system, provides a virtual testbed for safety-critical control applications, and allows to gain a deep understanding of the control mechanisms. While reinforcement learning has been applied successfully in a number of rather simple flow control benchmarks, a major bottleneck toward real-world applications is the high computational cost and turnaround time of flow simulations. In this contribution, we demonstrate the benefits of model-based reinforcement learning for flow control applications. Specifically, we optimize the policy by alternating between trajectories sampled from flow simulations and trajectories sampled from an ensemble of environment models. The model-based learning reduces the overall 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#23436;&#20840;&#38750;&#21442;&#25968;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#35774;&#32622;&#65292;&#26088;&#22312;&#22312;&#22810;&#20010;&#20998;&#24067;&#20043;&#38388;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#26080;&#38656;&#20551;&#35774;&#30828;&#24178;&#39044;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#24674;&#22797;&#20986;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.05052</link><description>&lt;p&gt;
&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#36827;&#34892;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#20010;&#36890;&#29992;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Causal Representation Learning from Multiple Distributions: A General Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#23436;&#20840;&#38750;&#21442;&#25968;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#35774;&#32622;&#65292;&#26088;&#22312;&#22312;&#22810;&#20010;&#20998;&#24067;&#20043;&#38388;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#26080;&#38656;&#20551;&#35774;&#30828;&#24178;&#39044;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#24674;&#22797;&#20986;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#65292;&#27979;&#37327;&#21464;&#37327;&#65288;&#20363;&#22914;&#22270;&#20687;&#20687;&#32032;&#65289;&#21482;&#26159;&#38544;&#34255;&#30340;&#22240;&#26524;&#21464;&#37327;&#65288;&#20363;&#22914;&#28508;&#22312;&#30340;&#27010;&#24565;&#25110;&#23545;&#35937;&#65289;&#30340;&#25968;&#23398;&#20989;&#25968;&#12290;&#20026;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#39044;&#27979;&#25110;&#23545;&#31995;&#32479;&#36827;&#34892;&#36866;&#24403;&#30340;&#26356;&#25913;&#65292;&#24674;&#22797;&#38544;&#34255;&#30340;&#22240;&#26524;&#21464;&#37327;$Z_i$&#20197;&#21450;&#30001;&#22270;$\mathcal{G}_Z$&#34920;&#31034;&#30340;&#23427;&#20204;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#36825;&#20010;&#38382;&#39064;&#26368;&#36817;&#34987;&#31216;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#12290;&#26412;&#25991;&#20851;&#27880;&#26469;&#33258;&#22810;&#20010;&#20998;&#24067;&#65288;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#25110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65289;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#12289;&#23436;&#20840;&#38750;&#21442;&#25968;&#30340;&#35774;&#32622;&#65292;&#19981;&#38656;&#35201;&#20551;&#35774;&#20998;&#24067;&#25913;&#21464;&#32972;&#21518;&#23384;&#22312;&#30828;&#24178;&#39044;&#12290;&#25105;&#20204;&#26088;&#22312;&#22312;&#36825;&#20010;&#22522;&#26412;&#24773;&#20917;&#19979;&#24320;&#21457;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65307;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#36825;&#26377;&#21161;&#20110;&#30475;&#21040;&#20854;&#20182;&#20551;&#35774;&#65288;&#22914;&#21442;&#25968;&#22240;&#26524;&#27169;&#22411;&#25110;&#30828;&#24178;&#39044;&#65289;&#25552;&#20379;&#30340;&#29420;&#29305;&#22909;&#22788;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#24674;&#22797;&#36807;&#31243;&#20013;&#23545;&#22270;&#30340;&#31232;&#30095;&#24615;&#32422;&#26463;&#19979;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#23398;&#20064;&#20986;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many problems, the measured variables (e.g., image pixels) are just mathematical functions of the hidden causal variables (e.g., the underlying concepts or objects). For the purpose of making predictions in changing environments or making proper changes to the system, it is helpful to recover the hidden causal variables $Z_i$ and their causal relations represented by graph $\mathcal{G}_Z$. This problem has recently been known as causal representation learning. This paper is concerned with a general, completely nonparametric setting of causal representation learning from multiple distributions (arising from heterogeneous data or nonstationary time series), without assuming hard interventions behind distribution changes. We aim to develop general solutions in this fundamental case; as a by product, this helps see the unique benefit offered by other assumptions such as parametric causal models or hard interventions. We show that under the sparsity constraint on the recovered graph over
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21040;&#32039;&#25903;&#25345;&#22522;&#30340;&#26680;&#20998;&#32452;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#38477;&#20302;&#39640;&#26031;&#36807;&#31243;&#30340;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#36866;&#24403;&#30340;&#32447;&#24615;&#32452;&#21512;&#20135;&#29983;&#20102;$m$&#20010;&#32039;&#25903;&#25345;&#30340;&#26680;&#20998;&#32452;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.04022</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21040;&#32039;&#25903;&#25345;&#22522;&#30340;&#26680;&#20998;&#32452;&#30340;&#36890;&#29992;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A General Theory for Kernel Packets: from state space model to compactly supported basis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21040;&#32039;&#25903;&#25345;&#22522;&#30340;&#26680;&#20998;&#32452;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#38477;&#20302;&#39640;&#26031;&#36807;&#31243;&#30340;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#36866;&#24403;&#30340;&#32447;&#24615;&#32452;&#21512;&#20135;&#29983;&#20102;$m$&#20010;&#32039;&#25903;&#25345;&#30340;&#26680;&#20998;&#32452;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#30340;&#29366;&#24577;&#31354;&#38388;&#65288;SS&#65289;&#27169;&#22411;&#20844;&#24335;&#21487;&#20197;&#23558;&#20854;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#38477;&#20302;&#21040;O&#65288;n&#65289;&#65288;n&#20026;&#25968;&#25454;&#28857;&#20010;&#25968;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;m&#32500;&#30340;GP&#30340;SS&#27169;&#22411;&#20844;&#24335;&#31561;&#20215;&#20110;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#20010;&#27010;&#24565;&#65292;&#31216;&#20026;&#36890;&#29992;&#21491;&#26680;&#20998;&#32452;&#65288;KP&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;GP&#21327;&#26041;&#24046;&#20989;&#25968;K&#30340;&#21464;&#25442;&#65292;&#20351;&#24471;&#23545;&#20110;&#20219;&#24847;$t \leq t_1$&#65292;$0 \leq j \leq m-1$&#21644;$m+1$&#20010;&#36830;&#32493;&#28857;$t_i$&#65292;&#37117;&#28385;&#36275;$\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$&#65292;&#20854;&#20013;${D}_t^{(j)}f(t)$&#34920;&#31034;&#22312;$t$&#19978;&#20316;&#29992;&#30340;&#31532;j&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24605;&#24819;&#25193;&#23637;&#21040;&#20102;GP&#30340;&#21521;&#21518;SS&#27169;&#22411;&#20844;&#24335;&#65292;&#24471;&#21040;&#20102;&#19979;&#19968;&#20010;$m$&#20010;&#36830;&#32493;&#28857;&#30340;&#24038;&#26680;&#20998;&#32452;&#30340;&#27010;&#24565;&#65306;$\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$&#65292;&#23545;&#20110;&#20219;&#24847;$t\geq t_{2m}$&#12290;&#36890;&#36807;&#32467;&#21512;&#24038;&#21491;&#26680;&#20998;&#32452;&#65292;&#21487;&#20197;&#35777;&#26126;&#36825;&#20123;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#36866;&#24403;&#32447;&#24615;&#32452;&#21512;&#20135;&#29983;&#20102;$m$&#20010;&#32039;&#25903;&#25345;&#30340;&#26680;&#20998;&#32452;&#20989;&#25968;&#65306;&#23545;&#20110;&#20219;&#24847;$t\not\in(t_0,t_{2m})$&#21644;$j=0,\cdots,m-1$&#65292;$\phi^{(j)}(t)=0$&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that the state space (SS) model formulation of a Gaussian process (GP) can lower its training and prediction time both to O(n) for n data points. We prove that an $m$-dimensional SS model formulation of GP is equivalent to a concept we introduce as the general right Kernel Packet (KP): a transformation for the GP covariance function $K$ such that $\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \leq t_1$, 0 $\leq j \leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}_t^{(j)}f(t) $ denotes $j$-th order derivative acting on $t$. We extend this idea to the backward SS model formulation of the GP, leading to the concept of the left KP for next $m$ consecutive points: $\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$ for any $t\geq t_{2m}$. By combining both left and right KPs, we can prove that a suitable linear combination of these covariance functions yields $m$ compactly supported KP functions: $\phi^{(j)}(t)=0$ for any $t\not\in(t_0,t_{2m})$ and $j=0,\cdots,m-1$
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#39044;&#27979;&#26102;&#22495;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#26681;&#25454;&#20854;&#29305;&#23450;&#38656;&#27714;&#26469;&#30830;&#23450;&#26368;&#20339;&#39044;&#27979;&#26102;&#22495;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.03893</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#30340;&#39044;&#27979;&#26102;&#22495;&#38656;&#27714;&#65306;&#20248;&#21270;&#23433;&#20840;&#12289;&#33298;&#36866;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Prediction Horizon Requirements for Automated Driving: Optimizing Safety, Comfort, and Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03893
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#39044;&#27979;&#26102;&#22495;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#26681;&#25454;&#20854;&#29305;&#23450;&#38656;&#27714;&#26469;&#30830;&#23450;&#26368;&#20339;&#39044;&#27979;&#26102;&#22495;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20854;&#20182;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#31227;&#21160;&#23545;&#20110;&#25913;&#21892;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;(AV)&#30340;&#24615;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#19982;AV&#24615;&#33021;&#30456;&#20851;&#30340;&#26102;&#38388;&#33539;&#22260;&#30340;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#23613;&#31649;&#23384;&#22312;&#22823;&#37327;&#30340;&#36712;&#36857;&#39044;&#27979;&#31639;&#27861;&#65292;&#20294;&#36824;&#27809;&#26377;&#30740;&#31350;&#25506;&#35752;&#19981;&#21516;&#39044;&#27979;&#38271;&#24230;&#22914;&#20309;&#24433;&#21709;AV&#23433;&#20840;&#21644;&#20854;&#20182;&#36710;&#36742;&#24615;&#33021;&#25351;&#26631;&#65292;&#23548;&#33268;&#39044;&#27979;&#26041;&#27861;&#30340;&#39044;&#27979;&#26102;&#22495;&#38656;&#27714;&#26410;&#23450;&#20041;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#22522;&#20110;&#39118;&#38505;&#30340;&#39044;&#27979;&#36712;&#36857;&#35268;&#21010;&#22120;&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#65292;&#27169;&#25311;&#20102;&#38271;&#36798;20&#31186;&#30340;&#39044;&#27979;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#29305;&#23450;AV&#24615;&#33021;&#26631;&#20934;&#21644;&#24212;&#29992;&#38656;&#27714;&#25351;&#23450;&#26368;&#20302;&#35201;&#27714;&#21644;&#26368;&#20339;&#39044;&#27979;&#26102;&#22495;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Predicting the movement of other road users is beneficial for improving automated vehicle (AV) performance. However, the relationship between the time horizon associated with these predictions and AV performance remains unclear. Despite the existence of numerous trajectory prediction algorithms, no studies have been conducted on how varying prediction lengths affect AV safety and other vehicle performance metrics, resulting in undefined horizon requirements for prediction methods. Our study addresses this gap by examining the effects of different prediction horizons on AV performance, focusing on safety, comfort, and efficiency. Through multiple experiments using a state-of-the-art, risk-based predictive trajectory planner, we simulated predictions with horizons up to 20 seconds. Based on our simulations, we propose a framework for specifying the minimum required and optimal prediction horizons based on specific AV performance criteria and application needs. Our results indicate that a
&lt;/p&gt;</description></item><item><title>MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.02263</link><description>&lt;p&gt;
MixedNUTS: &#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02263
&lt;/p&gt;
&lt;p&gt;
MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24448;&#24448;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#65292;&#38459;&#30861;&#20102;&#40065;&#26834;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#24050;&#35757;&#32451;&#30340;&#22823;&#22411;&#39640;&#24615;&#33021;&#27169;&#22411;&#20860;&#23481;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#26080;&#38656;&#35757;&#32451;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#40065;&#26834;&#27169;&#22411;&#22312;&#24178;&#20928;&#25968;&#25454;&#21644;&#23545;&#25239;&#25968;&#25454;&#19978;&#30340;&#27491;&#30830;&#39044;&#27979;&#27604;&#38169;&#35823;&#39044;&#27979;&#26356;&#33258;&#20449;&#65292;&#25105;&#20204;&#25512;&#27979;&#36890;&#36807;&#22686;&#24378;&#36825;&#31181;&#8220;&#33391;&#24615;&#32622;&#20449;&#24230;&#29305;&#24615;&#8221;&#21487;&#20197;&#22312;&#38598;&#25104;&#29615;&#22659;&#20013;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MixedNUTS&#8221;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#26377;&#19977;&#20010;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#26469;&#22788;&#29702;&#40065;&#26834;&#20998;&#31867;&#22120;&#21644;&#26631;&#20934;&#38750;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;Logits&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;MixedNUTS&#23558;&#36716;&#25442;&#21518;&#30340;Logits&#36716;&#25442;&#20026;&#27010;&#29575;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20316;&#20026;&#26368;&#32456;&#30340;&#36755;&#20986;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#23454;&#29616;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2311.15487</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Global $\mathcal{L}^2$ minimization at uniform exponential rate via geometrically adapted gradient descent in Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15487
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#23454;&#29616;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#29992;&#20110;&#26368;&#23567;&#21270;$\mathcal{L}^2$&#20195;&#20215;&#20989;&#25968;&#30340;&#26799;&#24230;&#19979;&#38477;&#27969;&#65292;&#24182;&#24341;&#20837;&#20004;&#20010;&#25913;&#36827;&#29256;&#26412;&#65307;&#19968;&#20010;&#36866;&#29992;&#20110;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#65292;&#21478;&#19968;&#20010;&#36866;&#29992;&#20110;&#27424;&#21442;&#25968;&#21270;&#35774;&#32622;&#12290;&#36825;&#20004;&#20010;&#29256;&#26412;&#37117;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#65292;&#32771;&#34385;&#21040;&#22312;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#30340;&#25289;&#22238;&#21521;&#37327;&#19995;&#32467;&#26500;&#21644;&#22312;&#27424;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#30340;&#25512;&#21069;&#21521;&#37327;&#19995;&#32467;&#26500;&#12290;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#28385;&#36275;&#31209;&#26465;&#20214;&#65292;&#25913;&#36827;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#25152;&#26377;&#36712;&#36947;&#23558;&#20197;&#22343;&#21248;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#23558;$\mathcal{L}^2$&#20195;&#20215;&#39537;&#21160;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#65307;&#22240;&#27492;&#65292;&#23545;&#20110;&#20219;&#20309;&#39044;&#20808;&#25351;&#23450;&#30340;&#25509;&#36817;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#36817;&#20284;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#20808;&#39564;&#20572;&#27490;&#26102;&#38388;&#12290;&#25105;&#20204;&#25351;&#20986;&#21518;&#32773;&#19982;&#27425;Riemann&#20960;&#20309;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15487v3 Announce Type: replace-cross  Abstract: We consider the gradient descent flow widely used for the minimization of the $\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two modified versions; one adapted for the overparametrized setting, and the other for the underparametrized setting. Both have a clear and natural invariant geometric meaning, taking into account the pullback vector bundle structure in the overparametrized, and the pushforward vector bundle structure in the underparametrized setting. In the overparametrized case, we prove that, provided that a rank condition holds, all orbits of the modified gradient descent drive the $\mathcal{L}^2$ cost to its global minimum at a uniform exponential convergence rate; one thereby obtains an a priori stopping time for any prescribed proximity to the global minimum. We point out relations of the latter to sub-Riemannian geometry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#20010;&#23398;&#20064;&#26426;&#20250;&#32463;&#36807;&#22810;&#27425;&#23616;&#37096;&#26356;&#26032;&#21518;&#36817;&#20284;&#36827;&#34892;&#26799;&#24230;&#27493;&#39588;&#12290;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21487;&#33021;&#22312;&#20248;&#21270;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#36215;&#21040;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2309.05102</link><description>&lt;p&gt;
&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26159;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#21527;&#65311;&#20351;&#29992;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Is Learning in Biological Neural Networks based on Stochastic Gradient Descent? An analysis using stochastic processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#20010;&#23398;&#20064;&#26426;&#20250;&#32463;&#36807;&#22810;&#27425;&#23616;&#37096;&#26356;&#26032;&#21518;&#36817;&#20284;&#36827;&#34892;&#26799;&#24230;&#27493;&#39588;&#12290;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21487;&#33021;&#22312;&#20248;&#21270;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#36215;&#21040;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20013;&#23398;&#20064;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#21306;&#21035;&#19968;&#30452;&#23384;&#22312;&#28608;&#28872;&#30340;&#20105;&#35770;&#12290;&#36890;&#24120;&#35748;&#20026;&#65292;&#22823;&#33041;&#20013;&#36830;&#25509;&#30340;&#26356;&#26032;&#20165;&#20381;&#36182;&#20110;&#23616;&#37096;&#20449;&#24687;&#65292;&#22240;&#27492;&#19981;&#33021;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31867;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;BNNs&#20013;&#30417;&#30563;&#23398;&#20064;&#30340;&#38543;&#26426;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#27599;&#20010;&#23398;&#20064;&#26426;&#20250;&#36890;&#36807;&#35768;&#22810;&#23616;&#37096;&#26356;&#26032;&#36827;&#34892;&#22788;&#29702;&#26102;&#65292;&#65288;&#36830;&#32493;&#30340;&#65289;&#26799;&#24230;&#27493;&#39588;&#36817;&#20284;&#21457;&#29983;&#12290;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21487;&#33021;&#22312;&#20248;&#21270;BNNs&#20013;&#36215;&#21040;&#19968;&#23450;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an intense debate about how learning in biological neural networks (BNNs) differs from learning in artificial neural networks. It is often argued that the updating of connections in the brain relies only on local information, and therefore a stochastic gradient-descent type optimization method cannot be used. In this paper, we study a stochastic model for supervised learning in BNNs. We show that a (continuous) gradient step occurs approximately when each learning opportunity is processed by many local updates. This result suggests that stochastic gradient descent may indeed play a role in optimizing BNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20174;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#35282;&#24230;&#20998;&#26512;&#19981;&#21516;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;DNN&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2302.13425</link><description>&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36827;&#34892;&#35843;&#26597;&#65306;&#20174;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#35282;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Survey on Uncertainty Quantification for Deep Learning: An Uncertainty Source Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.13425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20174;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#35282;&#24230;&#20998;&#26512;&#19981;&#21516;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;DNN&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20197;&#21450;&#31185;&#23398;&#19982;&#24037;&#31243;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#20063;&#35748;&#35782;&#21040;DNNs&#26377;&#26102;&#20250;&#20570;&#20986;&#24847;&#22806;&#12289;&#38169;&#35823;&#20294;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#33258;&#21160;&#39550;&#39542;&#12289;&#21307;&#23398;&#35786;&#26029;&#21644;&#28798;&#38590;&#21709;&#24212;&#31561;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#20986;&#29616;&#20005;&#37325;&#21518;&#26524;&#12290;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26088;&#22312;&#20272;&#35745;DNN&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#36229;&#36234;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#38024;&#23545;DNNs&#30340;UQ&#26041;&#27861;&#12290;&#31995;&#32479;&#22320;&#23545;&#36825;&#20123;UQ&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#20855;&#26377;&#26497;&#22823;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35843;&#26597;&#22823;&#22810;&#38598;&#20013;&#22312;&#20174;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35282;&#24230;&#25110;&#36125;&#21494;&#26031;&#35282;&#24230;&#23545;UQ&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#27599;&#31181;&#26041;&#27861;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.13425v3 Announce Type: replace  Abstract: Deep neural networks (DNNs) have achieved tremendous success in making accurate predictions for computer vision, natural language processing, as well as science and engineering domains. However, it is also well-recognized that DNNs sometimes make unexpected, incorrect, but overconfident predictions. This can cause serious consequences in high-stake applications, such as autonomous driving, medical diagnosis, and disaster response. Uncertainty quantification (UQ) aims to estimate the confidence of DNN predictions beyond prediction accuracy. In recent years, many UQ methods have been developed for DNNs. It is of great practical value to systematically categorize these UQ methods and compare their advantages and disadvantages. However, existing surveys mostly focus on categorizing UQ methodologies from a neural network architecture perspective or a Bayesian perspective and ignore the source of uncertainty that each methodology can incor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#39318;&#27425;&#35843;&#30740;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2302.06670</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Explainable Anomaly Detection in Images and Videos: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.06670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#39318;&#27425;&#35843;&#30740;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#35270;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#24212;&#29992;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21487;&#35270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#30340;&#35299;&#37322;&#20197;&#21450;&#20026;&#20309;&#21487;&#20197;&#21306;&#20998;&#24322;&#24120;&#30340;&#21512;&#29702;&#35299;&#37322;&#21364;&#21313;&#20998;&#31232;&#32570;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#19968;&#39033;&#38598;&#20013;&#20110;&#21487;&#35299;&#37322;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#35843;&#30740;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22270;&#20687;&#32423;&#21644;&#35270;&#39057;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#26412;&#32972;&#26223;&#12290;&#28982;&#21518;&#65292;&#20316;&#20026;&#26412;&#35843;&#30740;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20840;&#38754;&#21644;&#35814;&#23613;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#21482;&#33021;&#24212;&#29992;&#20110;&#19968;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.06670v2 Announce Type: replace-cross  Abstract: Anomaly detection and localization of visual data, including images and videos, are of great significance in both machine learning academia and applied real-world scenarios. Despite the rapid development of visual anomaly detection techniques in recent years, the interpretations of these black-box models and reasonable explanations of why anomalies can be distinguished out are scarce. This paper provides the first survey concentrated on explainable visual anomaly detection methods. We first introduce the basic background of image-level and video-level anomaly detection. Then, as the main content of this survey, a comprehensive and exhaustive literature review of explainable anomaly detection methods for both images and videos is presented. Next, we analyze why some explainable anomaly detection methods can be applied to both images and videos and why others can be only applied to one modality. Additionally, we provide summaries
&lt;/p&gt;</description></item><item><title>Centaur&#25552;&#20986;&#20102;&#38754;&#21521;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#36873;&#25321;&#26041;&#26696;&#21644;&#22522;&#20110;&#20998;&#21306;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#38480;&#21046;&#35774;&#22791;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#21442;&#19982;&#65292;&#30456;&#27604;&#26412;&#22320;&#35757;&#32451;&#33021;&#33719;&#24471;&#26356;&#39640;&#20934;&#30830;&#24615;&#21644;&#33410;&#32422;&#33021;&#37327;&#12290;</title><link>https://arxiv.org/abs/2211.04175</link><description>&lt;p&gt;
Centaur: &#38754;&#21521;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Centaur: Federated Learning for Constrained Edge Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.04175
&lt;/p&gt;
&lt;p&gt;
Centaur&#25552;&#20986;&#20102;&#38754;&#21521;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#36873;&#25321;&#26041;&#26696;&#21644;&#22522;&#20110;&#20998;&#21306;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#38480;&#21046;&#35774;&#22791;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#21442;&#19982;&#65292;&#30456;&#27604;&#26412;&#22320;&#35757;&#32451;&#33021;&#33719;&#24471;&#26356;&#39640;&#20934;&#30830;&#24615;&#21644;&#33410;&#32422;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20419;&#36827;&#20102;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#26032;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#21487;&#31359;&#25140;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#36825;&#20123;&#35774;&#22791;&#25429;&#33719;&#22823;&#37327;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#20869;&#23384;&#12289;&#35745;&#31639;&#12289;&#21151;&#32791;&#21644;&#36830;&#25509;&#24615;&#32422;&#26463;&#65292;&#36825;&#20123;&#32422;&#26463;&#38459;&#30861;&#20102;&#23427;&#20204;&#21442;&#19982;FL&#12290;&#25105;&#20204;&#25552;&#20986;Centaur&#65292;&#19968;&#20010;&#22810;&#23618;FL&#26694;&#26550;&#65292;&#20351;&#36229;&#38480;&#21046;&#30340;&#35774;&#22791;&#33021;&#22815;&#39640;&#25928;&#22320;&#21442;&#19982;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;FL&#12290;Centaur&#32467;&#21512;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#24819;&#27861;&#65306;&#65288;i&#65289;&#25968;&#25454;&#36873;&#25321;&#26041;&#26696;&#36873;&#25321;&#19968;&#37096;&#20998;&#26679;&#26412;&#21152;&#36895;&#23398;&#20064;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#19968;&#20010;&#22522;&#20110;&#20998;&#21306;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#25972;&#21512;&#21516;&#19968;&#29992;&#25143;&#25317;&#26377;&#30340;&#21463;&#38480;&#21644;&#24378;&#22823;&#35774;&#22791;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#31070;&#32463;&#32593;&#32476;&#21644;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;Centaur&#30456;&#27604;&#20110;&#22312;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#26412;&#22320;&#35757;&#32451;&#65292;&#33021;&#22815;&#33719;&#24471;&#32422;10\%&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24179;&#22343;&#33021;&#33410;&#32422;&#32422;58\%&#30340;&#33021;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#20063;&#34920;&#26126;&#20102;Centaur&#22312;&#22788;&#29702;&#26102;&#30340;&#21331;&#36234;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.04175v3 Announce Type: replace  Abstract: Federated learning (FL) facilitates new applications at the edge, especially for wearable and Internet-of-Thing devices. Such devices capture a large and diverse amount of data, but they have memory, compute, power, and connectivity constraints which hinder their participation in FL. We propose Centaur, a multitier FL framework, enabling ultra-constrained devices to efficiently participate in FL on large neural nets. Centaur combines two major ideas: (i) a data selection scheme to choose a portion of samples that accelerates the learning, and (ii) a partition-based training algorithm that integrates both constrained and powerful devices owned by the same user. Evaluations, on four benchmark neural nets and three datasets, show that Centaur gains ~10\% higher accuracy than local training on constrained devices with ~58\% energy saving on average. Our experimental results also demonstrate the superior efficiency of Centaur when dealing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10831</link><description>&lt;p&gt;
&#36890;&#36807;&#36890;&#29992;&#27010;&#24565;&#21457;&#29616;&#29702;&#35299;&#35270;&#39057;Transformer
&lt;/p&gt;
&lt;p&gt;
Understanding Video Transformers via Universal Concept Discovery. (arXiv:2401.10831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#39057;Transformer&#34920;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35797;&#22270;&#35299;&#37322;&#22522;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#39640;&#23618;&#26102;&#31354;&#27010;&#24565;&#30340;&#35270;&#39057;Transformer&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20197;&#24448;&#20851;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#20165;&#38598;&#20013;&#22312;&#22270;&#20687;&#32423;&#20219;&#21153;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35270;&#39057;&#27169;&#22411;&#22788;&#29702;&#20102;&#39069;&#22806;&#30340;&#26102;&#38388;&#32500;&#24230;&#65292;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#21160;&#24577;&#27010;&#24565;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;(VTCD)&#31639;&#27861;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#35270;&#39057;Transformer&#34920;&#31034;&#30340;&#21333;&#20803;&#65288;&#27010;&#24565;&#65289;&#24182;&#23545;&#20854;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#21517;&#12290;&#24471;&#21040;&#30340;&#27010;&#24565;&#20855;&#26377;&#24456;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25581;&#31034;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;HVAC&#25511;&#21046;&#30340;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#21457;&#29616;SAC&#21644;TD3&#31561;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#27867;&#21270;&#21644;&#22686;&#37327;&#23398;&#20064;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05737</link><description>&lt;p&gt;
HVAC&#25511;&#21046;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An experimental evaluation of Deep Reinforcement Learning algorithms for HVAC control. (arXiv:2401.05737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;HVAC&#25511;&#21046;&#30340;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#21457;&#29616;SAC&#21644;TD3&#31561;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#27867;&#21270;&#21644;&#22686;&#37327;&#23398;&#20064;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26262;&#36890;&#31354;&#35843;&#31995;&#32479;&#26159;&#21830;&#19994;&#21644;&#23621;&#20303;&#24314;&#31569;&#33021;&#28304;&#28040;&#32791;&#30340;&#37325;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#32988;&#36807;&#20256;&#32479;&#30340;&#21453;&#24212;&#24335;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#26159;&#20026;&#29305;&#23450;&#35774;&#32622;&#32780;&#35774;&#35745;&#30340;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#27604;&#24615;&#30340;&#26631;&#20934;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#37319;&#29992;Sinergym&#26694;&#26550;&#65292;&#20197;&#33298;&#36866;&#24230;&#21644;&#33021;&#28304;&#28040;&#32791;&#20026;&#35780;&#21028;&#26631;&#20934;&#65292;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;HVAC&#25511;&#21046;&#26041;&#38754;&#36827;&#34892;&#20102;&#20851;&#38190;&#21644;&#21487;&#37325;&#29616;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#36890;&#36807;&#26816;&#26597;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#30830;&#35748;&#20102;SAC&#21644;TD3&#31561;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#27867;&#21270;&#21644;&#22686;&#37327;&#23398;&#20064;&#30456;&#20851;&#30340;&#20960;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heating, Ventilation, and Air Conditioning (HVAC) systems are a major driver of energy consumption in commercial and residential buildings. Recent studies have shown that Deep Reinforcement Learning (DRL) algorithms can outperform traditional reactive controllers. However, DRL-based solutions are generally designed for ad hoc setups and lack standardization for comparison. To fill this gap, this paper provides a critical and reproducible evaluation, in terms of comfort and energy consumption, of several state-of-the-art DRL algorithms for HVAC control. The study examines the controllers' robustness, adaptability, and trade-off between optimization goals by using the Sinergym framework. The results obtained confirm the potential of DRL algorithms, such as SAC and TD3, in complex scenarios and reveal several challenges related to generalization and incremental learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#23548;&#21521;&#30340;&#31232;&#30095;&#22870;&#21169;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#31034;&#33539;&#36712;&#36857;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#23454;&#29616;&#26356;&#24555;&#36895;&#12289;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.02225</link><description>&lt;p&gt;
&#22522;&#20110;&#36712;&#36857;&#23548;&#21521;&#30340;&#31232;&#30095;&#22870;&#21169;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Trajectory-Oriented Policy Optimization with Sparse Rewards. (arXiv:2401.02225v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02225
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#23548;&#21521;&#30340;&#31232;&#30095;&#22870;&#21169;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#31034;&#33539;&#36712;&#36857;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#23454;&#29616;&#26356;&#24555;&#36895;&#12289;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#31232;&#30095;&#22870;&#21169;&#36890;&#24120;&#21482;&#34920;&#31034;&#20219;&#21153;&#26159;&#21542;&#37096;&#20998;&#25110;&#23436;&#20840;&#23436;&#25104;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#20195;&#29702;&#33719;&#24471;&#26377;&#29992;&#21453;&#39304;&#20043;&#21069;&#24517;&#39035;&#25191;&#34892;&#35768;&#22810;&#25506;&#32034;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;DRL&#31639;&#27861;&#26080;&#27861;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#23398;&#20064;&#21487;&#34892;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#31034;&#33539;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#24555;&#36895;&#21644;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#36890;&#36807;&#23558;&#31163;&#32447;&#31034;&#33539;&#36712;&#36857;&#35270;&#20026;&#25351;&#23548;&#32780;&#19981;&#26159;&#27169;&#20223;&#23427;&#20204;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#20351;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20998;&#24067;&#19982;&#31163;&#32447;&#31034;&#33539;&#30456;&#21305;&#37197;&#30340;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;(MMD)&#30340;&#36712;&#36857;&#36317;&#31163;&#65292;&#24182;&#23558;&#31574;&#30053;&#20248;&#21270;&#24314;&#27169;&#20026;&#19968;&#20010;&#21463;&#36317;&#31163;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) remains challenging in tasks with sparse rewards. These sparse rewards often only indicate whether the task is partially or fully completed, meaning that many exploration actions must be performed before the agent obtains useful feedback. Hence, most existing DRL algorithms fail to learn feasible policies within a reasonable time frame. To overcome this problem, we develop an approach that exploits offline demonstration trajectories for faster and more efficient online RL in sparse reward settings. Our key insight is that by regarding offline demonstration trajectories as guidance, instead of imitating them, our method learns a policy whose state-action visitation marginal distribution matches that of offline demonstrations. Specifically, we introduce a novel trajectory distance based on maximum mean discrepancy (MMD) and formulate policy optimization as a distance-constrained optimization problem. Then, we show that this distance-constrained optimizat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#19968;GPU&#19978;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2312.10144</link><description>&lt;p&gt;
&#21333;&#19968;GPU&#19978;&#30340;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#19968;GPU&#19978;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#20849;&#20139;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#30340;&#21333;&#19968;&#28508;&#22312;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#29616;&#26377;&#30340;&#22312;&#22823;&#37327;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#24212;&#35813;&#33021;&#22815;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#20174;&#21333;&#27169;&#24577;&#27169;&#22411;&#20013;&#21019;&#24314;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuseMix&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#22686;&#24378;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#20219;&#24847;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25805;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;FuseMix&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21644;&#38899;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#22312;Flickr30K&#30340;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#20013;&#27604;CLIP&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#32422;600&#20493;&#65292;&#32780;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of multimodal alignment is to learn a single latent space that is shared between multimodal inputs. The most powerful models in this space have been trained using massive datasets of paired inputs and large-scale computational resources, making them prohibitively expensive to train in many practical scenarios. We surmise that existing unimodal encoders pre-trained on large amounts of unimodal data should provide an effective bootstrap to create multimodal models from unimodal ones at much lower costs. We therefore propose FuseMix, a multimodal augmentation scheme that operates on the latent spaces of arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal alignment, we achieve competitive performance -- and in certain cases outperform state-of-the art methods -- in both image-text and audio-text retrieval, with orders of magnitude less compute and data: for example, we outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \! 600\times$ fewer GP
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#29992;&#20110;&#31232;&#30095;&#32534;&#30721;&#21442;&#25968;&#30340;&#23398;&#20064;&#65292;&#36890;&#36807;&#38750;&#24179;&#20961;&#30340;&#21518;&#39564;&#36924;&#36817;&#21644;&#35299;&#26512;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#26631;&#20934;&#31232;&#30095;&#32534;&#30721;&#30340;&#23398;&#20064;&#65292;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01888</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#29109;&#30340;ELBO&#23398;&#20064;&#31232;&#30095;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Learning Sparse Codes with Entropy-Based ELBOs. (arXiv:2311.01888v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#29992;&#20110;&#31232;&#30095;&#32534;&#30721;&#21442;&#25968;&#30340;&#23398;&#20064;&#65292;&#36890;&#36807;&#38750;&#24179;&#20961;&#30340;&#21518;&#39564;&#36924;&#36817;&#21644;&#35299;&#26512;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#26631;&#20934;&#31232;&#30095;&#32534;&#30721;&#30340;&#23398;&#20064;&#65292;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#27010;&#29575;&#31232;&#30095;&#32534;&#30721;&#20551;&#35774;&#25289;&#26222;&#25289;&#26031;&#20808;&#39564;&#12289;&#20174;&#28508;&#22312;&#21040;&#21487;&#35266;&#27979;&#30340;&#32447;&#24615;&#26144;&#23556;&#20197;&#21450;&#39640;&#26031;&#21487;&#35266;&#27979;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23548;&#20986;&#20102;&#19968;&#20010;&#20165;&#22522;&#20110;&#29109;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#29992;&#20110;&#26631;&#20934;&#31232;&#30095;&#32534;&#30721;&#30340;&#21442;&#25968;&#12290;&#36825;&#20010;&#26032;&#30340;&#21464;&#20998;&#30446;&#26631;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;A&#65289;&#19982;MAP&#36924;&#36817;&#19981;&#21516;&#65292;&#23427;&#20351;&#29992;&#20102;&#27010;&#29575;&#25512;&#29702;&#30340;&#38750;&#24179;&#20961;&#21518;&#39564;&#36924;&#36817;&#65307;&#65288;B&#65289;&#19982;&#20197;&#21069;&#30340;&#38750;&#24179;&#20961;&#36924;&#36817;&#19981;&#21516;&#65292;&#36825;&#20010;&#26032;&#30340;&#30446;&#26631;&#26159;&#23436;&#20840;&#35299;&#26512;&#30340;&#65307;&#65288;C&#65289;&#35813;&#30446;&#26631;&#20801;&#35768;&#19968;&#31181;&#26032;&#30340;&#21407;&#21017;&#24615;&#30340;&#36864;&#28779;&#24418;&#24335;&#12290;&#30446;&#26631;&#30340;&#23548;&#20986;&#39318;&#20808;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;ELBO&#30446;&#26631;&#25910;&#25947;&#21040;&#29109;&#30340;&#21644;&#65292;&#36825;&#19982;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#36817;&#31867;&#20284;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ELBO&#31561;&#20110;&#29109;&#30340;&#26465;&#20214;&#20855;&#26377;&#35299;&#26512;&#35299;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#23436;&#20840;&#35299;&#26512;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#23398;&#20064;&#36924;&#30495;&#24615;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard probabilistic sparse coding assumes a Laplace prior, a linear mapping from latents to observables, and Gaussian observable distributions. We here derive a solely entropy-based learning objective for the parameters of standard sparse coding. The novel variational objective has the following features: (A) unlike MAP approximations, it uses non-trivial posterior approximations for probabilistic inference; (B) unlike for previous non-trivial approximations, the novel objective is fully analytical; and (C) the objective allows for a novel principled form of annealing. The objective is derived by first showing that the standard ELBO objective converges to a sum of entropies, which matches similar recent results for generative models with Gaussian priors. The conditions under which the ELBO becomes equal to entropies are then shown to have analytical solutions, which leads to the fully analytical objective. Numerical experiments are used to demonstrate the feasibility of learning wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;EqDrive&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;EqMotion&#31561;&#21464;&#31890;&#23376;&#21644;&#20154;&#31867;&#39044;&#27979;&#27169;&#22411;&#20197;&#21450;&#22810;&#27169;&#24335;&#39044;&#27979;&#26426;&#21046;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36710;&#36742;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#22312;&#27169;&#22411;&#23481;&#37327;&#36739;&#20302;&#12289;&#21442;&#25968;&#26356;&#23569;&#12289;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#32553;&#30701;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#19994;&#30028;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17540</link><description>&lt;p&gt;
EqDrive: &#33258;&#21160;&#39550;&#39542;&#30340;&#39640;&#25928;&#31561;&#21464;&#36816;&#21160;&#39044;&#27979;&#19982;&#22810;&#27169;&#24335;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
EqDrive: Efficient Equivariant Motion Forecasting with Multi-Modality for Autonomous Driving. (arXiv:2310.17540v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;EqDrive&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;EqMotion&#31561;&#21464;&#31890;&#23376;&#21644;&#20154;&#31867;&#39044;&#27979;&#27169;&#22411;&#20197;&#21450;&#22810;&#27169;&#24335;&#39044;&#27979;&#26426;&#21046;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36710;&#36742;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#22312;&#27169;&#22411;&#23481;&#37327;&#36739;&#20302;&#12289;&#21442;&#25968;&#26356;&#23569;&#12289;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#32553;&#30701;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#19994;&#30028;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#39044;&#27979;&#36710;&#36742;&#36816;&#21160;&#38656;&#35201;&#23545;&#36710;&#36742;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#24182;&#20445;&#25345;&#22312;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#21464;&#25442;&#19979;&#30340;&#36816;&#21160;&#31561;&#21464;&#24615;&#12290;&#20256;&#32479;&#27169;&#22411;&#24448;&#24448;&#32570;&#20047;&#22788;&#29702;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#22797;&#26434;&#21160;&#21147;&#23398;&#21644;&#22330;&#26223;&#20013;&#21508;&#20027;&#20307;&#20043;&#38388;&#20132;&#20114;&#20851;&#31995;&#25152;&#38656;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#23481;&#37327;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#36739;&#20302;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;EqMotion&#65292;&#19968;&#20010;&#39046;&#20808;&#30340;&#31561;&#21464;&#31890;&#23376;&#21644;&#20154;&#31867;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36824;&#32771;&#34385;&#21040;&#19981;&#21464;&#30340;&#20027;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#29992;&#20110;&#22810;&#20195;&#29702;&#36710;&#36742;&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#27169;&#24335;&#39044;&#27979;&#26426;&#21046;&#20197;&#27010;&#29575;&#21270;&#26041;&#24335;&#32771;&#34385;&#22810;&#20010;&#21487;&#33021;&#30340;&#26410;&#26469;&#36335;&#24452;&#12290;&#36890;&#36807;&#21033;&#29992;EqMotion&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21442;&#25968;&#26356;&#23569;&#65288;120&#19975;&#65289;&#21644;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#32553;&#30701;&#65288;&#23569;&#20110;..&#65289;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19994;&#30028;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting vehicular motions in autonomous driving requires a deep understanding of agent interactions and the preservation of motion equivariance under Euclidean geometric transformations. Traditional models often lack the sophistication needed to handle the intricate dynamics inherent to autonomous vehicles and the interaction relationships among agents in the scene. As a result, these models have a lower model capacity, which then leads to higher prediction errors and lower training efficiency. In our research, we employ EqMotion, a leading equivariant particle, and human prediction model that also accounts for invariant agent interactions, for the task of multi-agent vehicle motion forecasting. In addition, we use a multi-modal prediction mechanism to account for multiple possible future paths in a probabilistic manner. By leveraging EqMotion, our model achieves state-of-the-art (SOTA) performance with fewer parameters (1.2 million) and a significantly reduced training time (less 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;fairret&#30340;&#21487;&#24494;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#30446;&#26631;&#37327;&#21270;&#20559;&#35265;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#33258;&#21160;&#24494;&#20998;&#27969;&#31243;&#20013;&#12290;&#36890;&#36807;&#20174;&#32447;&#24615;&#20998;&#24335;&#32479;&#35745;&#35282;&#24230;&#23450;&#20041;&#20844;&#24179;&#24615;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#22810;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;fairret&#26694;&#26550;&#19982;&#22522;&#20934;&#30456;&#27604;&#22312;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#24615;&#26102;&#20960;&#20046;&#19981;&#25439;&#22833;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17256</link><description>&lt;p&gt;
fairret&#65306;&#19968;&#31181;&#21487;&#24494;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
fairret: a Framework for Differentiable Fairness Regularization Terms. (arXiv:2310.17256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;fairret&#30340;&#21487;&#24494;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#30446;&#26631;&#37327;&#21270;&#20559;&#35265;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#33258;&#21160;&#24494;&#20998;&#27969;&#31243;&#20013;&#12290;&#36890;&#36807;&#20174;&#32447;&#24615;&#20998;&#24335;&#32479;&#35745;&#35282;&#24230;&#23450;&#20041;&#20844;&#24179;&#24615;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#22810;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;fairret&#26694;&#26550;&#19982;&#22522;&#20934;&#30456;&#27604;&#22312;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#24615;&#26102;&#20960;&#20046;&#19981;&#25439;&#22833;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#24037;&#20855;&#20165;&#25509;&#21463;&#26377;&#38480;&#33539;&#22260;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#19988;&#19982;&#33258;&#21160;&#24494;&#20998;&#24211;&#30340;&#25972;&#21512;&#36739;&#23569;&#65292;&#23613;&#31649;&#36825;&#20123;&#24211;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#65288;fairret&#65289;&#30340;&#26694;&#26550;&#65292;&#20197;&#27169;&#22359;&#21270;&#30446;&#26631;&#30340;&#24418;&#24335;&#37327;&#21270;&#20559;&#35265;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#33258;&#21160;&#24494;&#20998;&#27969;&#31243;&#20013;&#12290;&#36890;&#36807;&#37319;&#29992;&#32447;&#24615;&#20998;&#24335;&#32479;&#35745;&#30340;&#24191;&#20041;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#20986;&#19968;&#31867;&#24191;&#27867;&#30340;fairret&#12290;&#23454;&#39564;&#26174;&#31034;&#20102;&#23427;&#20204;&#30340;&#26799;&#24230;&#34892;&#20026;&#20197;&#21450;&#19982;&#22522;&#20934;&#30456;&#27604;&#23558;&#20844;&#24179;&#24615;&#24378;&#21046;&#25191;&#34892;&#30340;&#23454;&#29992;&#24615;&#32780;&#26368;&#23567;&#21270;&#39044;&#27979;&#33021;&#21147;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;fairret&#26694;&#26550;&#30340;PyTorch&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current tools for machine learning fairness only admit a limited range of fairness definitions and have seen little integration with automatic differentiation libraries, despite the central role these libraries play in modern machine learning pipelines.  We introduce a framework of fairness regularization terms (fairrets) which quantify bias as modular objectives that are easily integrated in automatic differentiation pipelines. By employing a general definition of fairness in terms of linear-fractional statistics, a wide class of fairrets can be computed efficiently. Experiments show the behavior of their gradients and their utility in enforcing fairness with minimal loss of predictive power compared to baselines. Our contribution includes a PyTorch implementation of the fairret framework.
&lt;/p&gt;</description></item><item><title>Zipformer&#26159;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#33410;&#30465;&#20869;&#23384;&#12289;&#24615;&#33021;&#26356;&#22909;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;U-Net-like&#32534;&#30721;&#22120;&#32467;&#26500;&#12289;&#37325;&#26032;&#32452;&#32455;&#30340;&#22359;&#32467;&#26500;&#12289;&#25913;&#36827;&#30340;LayerNorm&#12289;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#20248;&#21270;&#22120;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;LibriSpeech&#12289;Aishell-1&#21644;Wenet&#31561;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11230</link><description>&lt;p&gt;
Zipformer&#65306;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#22909;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Zipformer: A faster and better encoder for automatic speech recognition. (arXiv:2310.11230v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11230
&lt;/p&gt;
&lt;p&gt;
Zipformer&#26159;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#33410;&#30465;&#20869;&#23384;&#12289;&#24615;&#33021;&#26356;&#22909;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;U-Net-like&#32534;&#30721;&#22120;&#32467;&#26500;&#12289;&#37325;&#26032;&#32452;&#32455;&#30340;&#22359;&#32467;&#26500;&#12289;&#25913;&#36827;&#30340;LayerNorm&#12289;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#20248;&#21270;&#22120;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;LibriSpeech&#12289;Aishell-1&#21644;Wenet&#31561;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Conformer&#24050;&#25104;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#26368;&#27969;&#34892;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#23427;&#22312;&#21464;&#25442;&#22120;&#20013;&#21152;&#20837;&#20102;&#21367;&#31215;&#27169;&#22359;&#20197;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#33410;&#30465;&#20869;&#23384;&#12289;&#24615;&#33021;&#26356;&#22909;&#30340;&#21464;&#25442;&#22120;&#8212;&#8212;Zipformer&#12290;&#24314;&#27169;&#25913;&#21464;&#21253;&#25324;&#65306;1&#65289;&#31867;&#20284;U-Net&#30340;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#20013;&#38388;&#22534;&#26632;&#22312;&#36739;&#20302;&#30340;&#24103;&#29575;&#19979;&#36816;&#34892;&#65307;2&#65289;&#37325;&#26032;&#32452;&#32455;&#30340;&#22359;&#32467;&#26500;&#65292;&#22686;&#21152;&#20102;&#26356;&#22810;&#30340;&#27169;&#22359;&#65292;&#20854;&#20013;&#25105;&#20204;&#37325;&#22797;&#20351;&#29992;&#27880;&#24847;&#21147;&#26435;&#37325;&#20197;&#25552;&#39640;&#25928;&#29575;&#65307;3&#65289;&#19968;&#31181;&#25913;&#36827;&#30340;LayerNorm&#24418;&#24335;&#65292;&#31216;&#20026;BiasNorm&#65292;&#20801;&#35768;&#25105;&#20204;&#20445;&#30041;&#19968;&#20123;&#38271;&#24230;&#20449;&#24687;&#65307;4&#65289;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;SwooshR&#21644;SwooshL&#30340;&#24615;&#33021;&#20248;&#20110;Swish&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#22120;&#65292;&#31216;&#20026;ScaledAdam&#65292;&#23427;&#36890;&#36807;&#24403;&#21069;&#24352;&#37327;&#30340;&#35268;&#27169;&#26469;&#32553;&#25918;&#26356;&#26032;&#65292;&#20197;&#20445;&#25345;&#30456;&#23545;&#21464;&#21270;&#22823;&#33268;&#30456;&#21516;&#65292;&#24182;&#26126;&#30830;&#23398;&#20064;&#21442;&#25968;&#35268;&#27169;&#12290;&#19982;Adam&#30456;&#27604;&#65292;&#23427;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;LibriSpeech&#12289;Aishell-1&#21644;Wenet&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer. Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4) new activation functions SwooshR and SwooshL work better than Swish. We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster convergence and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and Wenet
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25511;&#21046;&#21464;&#37327;&#30340;&#26041;&#27861;&#31283;&#23450;Shapley&#20540;&#30340;&#20272;&#35745;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07672</link><description>&lt;p&gt;
&#29992;&#25511;&#21046;&#21464;&#37327;&#31283;&#23450;Shapley&#20540;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Estimates of Shapley Values with Control Variates. (arXiv:2310.07672v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07672
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25511;&#21046;&#21464;&#37327;&#30340;&#26041;&#27861;&#31283;&#23450;Shapley&#20540;&#30340;&#20272;&#35745;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#26159;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26368;&#27969;&#34892;&#30340;&#24037;&#20855;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#27492;&#37319;&#29992;&#25277;&#26679;&#36817;&#20284;&#26469;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#31283;&#23450;&#36825;&#20123;&#27169;&#22411;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#21464;&#37327;&#30340;&#33945;&#29305;&#21345;&#27931;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ControlSHAP&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25110;&#24314;&#27169;&#24037;&#20316;&#12290;&#22312;&#22810;&#20010;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;Shapley&#20272;&#35745;&#30340;&#33945;&#29305;&#21345;&#27931;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapley values are among the most popular tools for explaining predictions of blackbox machine learning models. However, their high computational cost motivates the use of sampling approximations, inducing a considerable degree of uncertainty. To stabilize these model explanations, we propose ControlSHAP, an approach based on the Monte Carlo technique of control variates. Our methodology is applicable to any machine learning model and requires virtually no extra computation or modeling effort. On several high-dimensional datasets, we find it can produce dramatic reductions in the Monte Carlo variability of Shapley estimates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#25552;&#31034;&#35843;&#20248;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03103</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#21452;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Dual Prompt Tuning for Domain-Aware Federated Learning. (arXiv:2310.03103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#25552;&#31034;&#35843;&#20248;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#23427;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#20043;&#38388;&#26222;&#36941;&#23384;&#22312;&#39046;&#22495;&#21464;&#21270;&#65292;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#24448;&#24448;&#38590;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#29616;&#23454;&#30340;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#39046;&#22495;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#21452;&#25552;&#31034;&#35843;&#20248;&#65288;Fed-DPT&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Fed-DPT&#37319;&#29992;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#24212;&#29992;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#25552;&#31034;&#35843;&#20248;&#26469;&#20419;&#36827;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#12290;&#22823;&#37327;&#30340;Fed-DPT&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed machine learning paradigm that allows multiple clients to collaboratively train a shared model with their local data. Nonetheless, conventional federated learning algorithms often struggle to generalize well due to the ubiquitous domain shift across clients. In this work, we consider a challenging yet realistic federated learning scenario where the training data of each client originates from different domains. We address the challenges of domain shift by leveraging the technique of prompt learning, and propose a novel method called Federated Dual Prompt Tuning (Fed-DPT). Specifically, Fed-DPT employs a pre-trained vision-language model and then applies both visual and textual prompt tuning to facilitate domain adaptation over decentralized data. Extensive experiments of Fed-DPT demonstrate its significant effectiveness in domain-aware federated learning. With a pre-trained CLIP model (ViT-Base as image encoder), the proposed Fed-DPT attains 68.4% av
&lt;/p&gt;</description></item><item><title>L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.02003</link><description>&lt;p&gt;
L2MAC&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35745;&#31639;&#26426;&#29992;&#20110;&#26080;&#38480;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02003
&lt;/p&gt;
&lt;p&gt;
L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21463;&#21040;&#24213;&#23618;Transformer&#26550;&#26500;&#22266;&#23450;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#22686;&#24378;&#35760;&#24518;&#30340;LLM&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#35835;&#21462;&#20869;&#23384;&#24182;&#23558;&#20854;&#28436;&#21464;&#20026;&#26032;&#20869;&#23384;&#30340;&#36830;&#25509;&#65292;&#35201;&#20040;&#20351;&#29992;&#38750;&#24120;&#19987;&#38376;&#30340;&#20869;&#23384;&#65292;&#26080;&#27861;&#36866;&#24212;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;L2MAC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38271;&#19988;&#19968;&#33268;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#29992;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#12290;&#23427;&#30340;&#20869;&#23384;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#25351;&#20196;&#27880;&#20876;&#34920;&#65292;&#20854;&#20013;&#22635;&#20805;&#20102;&#19968;&#20010;&#35299;&#20915;&#29992;&#25143;&#32473;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#31243;&#24207;&#65292;&#20197;&#21450;&#25991;&#20214;&#23384;&#20648;&#65292;&#20854;&#20013;&#21253;&#21547;&#26368;&#32456;&#21644;&#20013;&#38388;&#36755;&#20986;&#12290;&#27599;&#20010;&#25351;&#20196;&#30001;&#21333;&#29420;&#30340;LLM&#23454;&#20363;&#25191;&#34892;&#65292;&#20854;&#19978;&#19979;&#25991;&#30001;&#25511;&#21046;&#21333;&#20803;&#31649;&#29702;&#65292;&#33021;&#22815;&#31934;&#30830;&#35835;&#21462;&#21644;&#20889;&#20837;&#20869;&#23384;&#65292;&#20197;&#30830;&#20445;&#26377;&#25928;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#27169;&#22411;&#26469;&#27169;&#25311;&#29616;&#23454;&#30340;&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#29615;&#22659;&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;"&#36719;&#24213;&#20215;"&#21487;&#20197;&#25552;&#39640;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65292;&#21363;&#20351;&#25237;&#26631;&#32773;&#26469;&#33258;&#30456;&#21516;&#30340;&#20154;&#32676;&#12290;</title><link>http://arxiv.org/abs/2307.11732</link><description>&lt;p&gt;
&#25512;&#36827;&#24191;&#21578;&#25293;&#21334;&#30340;&#29616;&#23454;&#24615;&#65306;&#23454;&#38469;&#35265;&#35299;&#19982;&#24314;&#27169;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Advancing Ad Auction Realism: Practical Insights &amp; Modeling Implications. (arXiv:2307.11732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#27169;&#22411;&#26469;&#27169;&#25311;&#29616;&#23454;&#30340;&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#29615;&#22659;&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;"&#36719;&#24213;&#20215;"&#21487;&#20197;&#25552;&#39640;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65292;&#21363;&#20351;&#25237;&#26631;&#32773;&#26469;&#33258;&#30456;&#21516;&#30340;&#20154;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#23398;&#20064;&#27169;&#22411;&#65292;&#20801;&#35768;&#32771;&#34385;&#24403;&#20195;&#22312;&#32447;&#25293;&#21334;&#30340;&#22235;&#20010;&#20851;&#38190;&#29616;&#23454;&#29305;&#24449;&#65306;&#65288;1&#65289;&#24191;&#21578;&#27133;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#25628;&#32034;&#26597;&#35810;&#20855;&#26377;&#19981;&#21516;&#30340;&#20215;&#20540;&#21644;&#28857;&#20987;&#29575;&#65292;&#65288;2&#65289;&#31454;&#20105;&#24191;&#21578;&#21830;&#30340;&#25968;&#37327;&#21644;&#36523;&#20221;&#26159;&#19981;&#21487;&#35266;&#23519;&#30340;&#65292;&#24182;&#19988;&#22312;&#27599;&#27425;&#31454;&#25293;&#20013;&#20250;&#21457;&#29983;&#26356;&#25913;&#65292;&#65288;3&#65289;&#24191;&#21578;&#21830;&#20165;&#25509;&#25910;&#21040;&#37096;&#20998;&#30340;&#27719;&#24635;&#21453;&#39304;&#65292;&#65288;4&#65289;&#20184;&#27454;&#35268;&#21017;&#21482;&#37096;&#20998;&#30830;&#23450;&#12290;&#25105;&#20204;&#23558;&#24191;&#21578;&#21830;&#24314;&#27169;&#20026;&#21463;&#23545;&#25239;&#24615;&#36172;&#21338;&#31639;&#27861;&#39537;&#21160;&#30340;&#20195;&#29702;&#65292;&#29420;&#31435;&#20110;&#25293;&#21334;&#26426;&#21046;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#20102;&#27169;&#25311;&#24191;&#21578;&#21830;&#30340;&#34892;&#20026;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#20998;&#26512;&#12289;&#39044;&#27979;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#26356;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#21363;&#20351;&#25237;&#26631;&#32773;&#26469;&#33258;&#30456;&#21516;&#30340;&#20154;&#32676;&#65292;"&#36719;&#24213;&#20215;"&#20063;&#21487;&#20197;&#25552;&#39640;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#35266;&#23519;&#21040;&#30340;&#31454;&#26631;&#20013;&#25512;&#26029;&#24191;&#21578;&#21830;&#20215;&#20540;&#20998;&#24067;&#65292;&#20174;&#32780;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#38469;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a learning model of online ad auctions that allows for the following four key realistic characteristics of contemporary online auctions: (1) ad slots can have different values and click-through rates depending on users' search queries, (2) the number and identity of competing advertisers are unobserved and change with each auction, (3) advertisers only receive partial, aggregated feedback, and (4) payment rules are only partially specified. We model advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. Our objective is to simulate the behavior of advertisers for counterfactual analysis, prediction, and inference purposes. Our findings reveal that, in such richer environments, "soft floors" can enhance key performance metrics even when bidders are drawn from the same population. We further demonstrate how to infer advertiser value distributions from observed bids, thereby affirming the practical efficacy of o
&lt;/p&gt;</description></item><item><title>&#25345;&#32493;&#24615;&#23398;&#20064;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#20250;&#22833;&#21435;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13812</link><description>&lt;p&gt;
&#25345;&#32493;&#24615;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#32500;&#25252;
&lt;/p&gt;
&lt;p&gt;
Maintaining Plasticity in Deep Continual Learning. (arXiv:2306.13812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13812
&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#24615;&#23398;&#20064;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#20250;&#22833;&#21435;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#19987;&#38376;&#29992;&#20110;&#19968;&#27425;&#24615;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#25345;&#32493;&#24615;&#23398;&#20064;&#65292;&#22914;&#26524;&#23558;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#24212;&#29992;&#20110;&#25345;&#32493;&#24615;&#23398;&#20064;&#20013;&#65292;&#21017;&#20247;&#25152;&#21608;&#30693;&#23427;&#20204;&#21487;&#33021;&#22312;&#35760;&#20303;&#26089;&#26399;&#30340;&#20363;&#23376;&#26041;&#38754;&#36973;&#36935;&#22833;&#36133;&#12290;&#26356;&#20026;&#22522;&#26412;&#20294;&#19981;&#20026;&#20154;&#30693;&#30340;&#26159;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#22833;&#21435;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#21487;&#22609;&#24615;&#20007;&#22833;&#8221;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;MNIST&#21644;ImageNet&#25968;&#25454;&#38598;&#37325;&#26500;&#20026;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#12290;&#22312;ImageNet&#20013;&#65292;&#20108;&#20803;&#20998;&#31867;&#30340;&#24615;&#33021;&#20174;&#19968;&#20010;&#26089;&#26399;&#20219;&#21153;&#30340;89&#65285;&#27491;&#30830;&#19979;&#38477;&#21040;77&#65285;&#65292;&#25110;&#32773;&#22823;&#32422;&#31561;&#20110;&#32447;&#24615;&#32593;&#32476;&#30340;&#27700;&#24179;&#12290;&#36825;&#31181;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#21457;&#29983;&#22312;&#21508;&#31181;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#20248;&#21270;&#22120;&#21644;&#28608;&#27963;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#19981;&#20250;&#22240;&#25209;&#37327;&#24402;&#19968;&#21270;&#25110;&#25918;&#24323;&#32780;&#24471;&#21040;&#32531;&#35299;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Contrastive Plasticity&#65292;&#21487;&#20197;&#32531;&#35299;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#36866;&#24212;&#26032;&#30340;&#25968;&#25454;&#21516;&#26102;&#20445;&#30041;&#35760;&#20303;&#26087;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;Contrastive Plasticity&#21487;&#20197;&#28155;&#21152;&#21040;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#24182;&#24102;&#26469;&#38750;&#24120;&#23569;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep-learning systems are specialized to problem settings in which training occurs once and then never again, as opposed to continual-learning settings in which training occurs continually. If deep-learning systems are applied in a continual learning setting, then it is well known that they may fail catastrophically to remember earlier examples. More fundamental, but less well known, is that they may also lose their ability to adapt to new data, a phenomenon called \textit{loss of plasticity}. We show loss of plasticity using the MNIST and ImageNet datasets repurposed for continual learning as sequences of tasks. In ImageNet, binary classification performance dropped from 89% correct on an early task down to 77%, or to about the level of a linear network, on the 2000th task. Such loss of plasticity occurred with a wide range of deep network architectures, optimizers, and activation functions, and was not eased by batch normalization or dropout. In our experiments, loss of plasti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;Transformer&#22312;&#28857;&#20113;&#39046;&#22495;&#20013;&#30340;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#26041;&#26696;&#33021;&#22815;&#24110;&#21161;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#30784;&#20960;&#20309;&#65292;&#25552;&#20986;&#19968;&#31181;&#35299;&#20923;&#31574;&#30053;&#65292;&#22312;&#19981;&#24341;&#20837;&#20854;&#20182;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#65292;&#24182;&#22312;Transformer&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10798</link><description>&lt;p&gt;
ExpPoint-MAE&#65306;&#33258;&#30417;&#30563;&#28857;&#20113;Transformer&#30340;&#26356;&#22909;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
ExpPoint-MAE: Better interpretability and performance for self-supervised point cloud transformers. (arXiv:2306.10798v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;Transformer&#22312;&#28857;&#20113;&#39046;&#22495;&#20013;&#30340;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#26041;&#26696;&#33021;&#22815;&#24110;&#21161;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#30784;&#20960;&#20309;&#65292;&#25552;&#20986;&#19968;&#31181;&#35299;&#20923;&#31574;&#30053;&#65292;&#22312;&#19981;&#24341;&#20837;&#20854;&#20182;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#65292;&#24182;&#22312;Transformer&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;Transformer&#22312;&#28857;&#20113;&#39046;&#22495;&#20013;&#30340;&#29305;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Masked Autoencoding&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#32034;&#20102;Momentum Contrast&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;Transformer&#23398;&#20064;&#20851;&#27880;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#21306;&#22495;&#65292;&#34920;&#26126;&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#30784;&#20960;&#20309;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#24494;&#35843;&#36807;&#31243;&#21450;&#20854;&#23545;&#25152;&#23398;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35299;&#20923;&#31574;&#30053;&#65292;&#23427;&#22312;&#19981;&#24341;&#20837;&#20219;&#20309;&#20854;&#20182;&#20462;&#25913;&#27169;&#22411;&#25110;&#35757;&#32451;&#27969;&#31243;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#20248;&#20110;&#25105;&#20204;&#30340;&#22522;&#32447;&#65292;&#24182;&#22312;Transformer&#27169;&#22411;&#20013;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we delve into the properties of transformers, attained through self-supervision, in the point cloud domain. Specifically, we evaluate the effectiveness of Masked Autoencoding as a pretraining scheme, and explore Momentum Contrast as an alternative. In our study we investigate the impact of data quantity on the learned features, and uncover similarities in the transformer's behavior across domains. Through comprehensive visualiations, we observe that the transformer learns to attend to semantically meaningful regions, indicating that pretraining leads to a better understanding of the underlying geometry. Moreover, we examine the finetuning process and its effect on the learned representations. Based on that, we devise an unfreezing strategy which consistently outperforms our baseline without introducing any other modifications to the model or the training pipeline, and achieve state-of-the-art results in the classification task among transformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#38544;&#31169;&#38480;&#21046;&#36866;&#24212;&#22122;&#22768;&#8221;&#65288;PLAN&#65289;&#65292;&#26159;&#19968;&#32452;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#32467;&#26500;&#20013;&#36827;&#34892;&#26356;&#22909;&#30340;&#22343;&#20540;&#20272;&#35745;&#12290;PLAN&#23558;&#22122;&#22768;&#30340;&#24418;&#29366;&#37327;&#36523;&#23450;&#21046;&#20026;&#25968;&#25454;&#30340;&#24418;&#29366;&#65292;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#32780;&#19988;&#21487;&#20197;&#22312;&#19968;&#20123;&#38598;&#20013;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#20934;&#24046;&#30340;&#20559;&#26012;&#26469;&#33719;&#24471;&#25509;&#36817;&#38646;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.08745</link><description>&lt;p&gt;
PLAN: &#26041;&#24046;&#24863;&#30693;&#30340;&#24046;&#20998;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PLAN: Variance-Aware Private Mean Estimation. (arXiv:2306.08745v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#38544;&#31169;&#38480;&#21046;&#36866;&#24212;&#22122;&#22768;&#8221;&#65288;PLAN&#65289;&#65292;&#26159;&#19968;&#32452;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#32467;&#26500;&#20013;&#36827;&#34892;&#26356;&#22909;&#30340;&#22343;&#20540;&#20272;&#35745;&#12290;PLAN&#23558;&#22122;&#22768;&#30340;&#24418;&#29366;&#37327;&#36523;&#23450;&#21046;&#20026;&#25968;&#25454;&#30340;&#24418;&#29366;&#65292;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#32780;&#19988;&#21487;&#20197;&#22312;&#19968;&#20123;&#38598;&#20013;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#20934;&#24046;&#30340;&#20559;&#26012;&#26469;&#33719;&#24471;&#25509;&#36817;&#38646;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#26159;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#20445;&#25252;&#38544;&#31169;&#30340;&#31639;&#27861;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#24050;&#32463;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#65292;&#20294;&#35768;&#22810;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#21487;&#33021;&#34987;&#21033;&#29992;&#20197;&#20135;&#29983;&#26356;&#22909;&#31639;&#27861;&#30340;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#38544;&#31169;&#38480;&#21046;&#36866;&#24212;&#22122;&#22768;&#8221;&#65288;PLAN&#65289;&#12290;PLAN&#26159;&#19968;&#32452;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#29420;&#31435;&#37319;&#26679;&#20110;&#20998;&#24067;$\mathcal{D}$&#30340;&#36755;&#20837;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#22343;&#20540;&#20272;&#35745;&#65292;&#20854;&#20013;&#20998;&#24067;&#30340;&#22352;&#26631;&#26631;&#20934;&#24046;$\boldsymbol{\sigma}\in \mathbf{R}^d$&#12290;&#19982;Mahalanobis&#36317;&#31163;&#19979;&#30340;&#22343;&#20540;&#20272;&#35745;&#31867;&#20284;&#65292;PLAN&#23558;&#22122;&#22768;&#30340;&#24418;&#29366;&#37327;&#36523;&#23450;&#21046;&#20026;&#25968;&#25454;&#30340;&#24418;&#29366;&#65292;&#20294;&#19982;&#20197;&#21069;&#30340;&#31639;&#27861;&#19981;&#21516;&#65292;&#38544;&#31169;&#39044;&#31639;&#19981;&#26159;&#22343;&#21248;&#22320;&#33457;&#36153;&#22312;&#21508;&#20010;&#22352;&#26631;&#19978;&#12290;&#22312;&#23545;$\mathcal{D}$&#30340;&#38598;&#20013;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#21521;&#37327;$\boldsymbol{\sigma}$&#20013;&#30340;&#20559;&#26012;&#65292;&#20174;&#32780;&#33719;&#24471;&#25509;&#36817;&#38646;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private mean estimation is an important building block in privacy-preserving algorithms for data analysis and machine learning. Though the trade-off between privacy and utility is well understood in the worst case, many datasets exhibit structure that could potentially be exploited to yield better algorithms. In this paper we present $\textit{Private Limit Adapted Noise}$ (PLAN), a family of differentially private algorithms for mean estimation in the setting where inputs are independently sampled from a distribution $\mathcal{D}$ over $\mathbf{R}^d$, with coordinate-wise standard deviations $\boldsymbol{\sigma} \in \mathbf{R}^d$. Similar to mean estimation under Mahalanobis distance, PLAN tailors the shape of the noise to the shape of the data, but unlike previous algorithms the privacy budget is spent non-uniformly over the coordinates. Under a concentration assumption on $\mathcal{D}$, we show how to exploit skew in the vector $\boldsymbol{\sigma}$, obtaining a (zero-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WeiAvg&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16351</link><description>&lt;p&gt;
WeiAvg&#65306;&#20419;&#36827;&#25968;&#25454;&#22810;&#26679;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WeiAvg: Federated Learning Model Aggregation Promoting Data Diversity. (arXiv:2305.16351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WeiAvg&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20026;&#21033;&#29992;&#22823;&#35268;&#27169;&#31169;&#26377;&#36793;&#32536;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#24335;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36890;&#20449;&#24320;&#38144;&#31561;&#26041;&#38754;&#65292;&#24573;&#30053;&#20102;&#21442;&#19982;&#32773;&#23545;&#32852;&#37030;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#65288;WeiAvg&#65289;&#30340;&#26694;&#26550;&#65292;&#30528;&#37325;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#65292;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25237;&#24433;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#26469;&#35780;&#20272;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning provides a promising privacy-preserving way for utilizing large-scale private edge data from massive Internet-of-Things (IoT) devices. While existing research extensively studied optimizing the learning process, computing efficiency, and communication overhead, one important and often overlooked aspect is that participants contribute predictive knowledge from their data, impacting the quality of the federated models learned. While FedAvg treats each client equally and assigns weight solely based on the number of samples, the diversity of samples on each client could greatly affect the local update performance and the final aggregated model. In this paper, we propose a novel approach to address this issue by introducing a Weighted Averaging (WeiAvg) framework that emphasizes updates from high-diversity clients and diminishes the influence of those from low-diversity clients. Specifically, we introduced a projection-based approximation method to estimate the diversity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;RKGCN&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#20998;&#26512;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01147</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ripple Knowledge Graph Convolutional Networks For Recommendation Systems. (arXiv:2305.01147v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;RKGCN&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#20998;&#26512;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#65292;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#36741;&#21161;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#20915;&#31574;&#33021;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;RKGCN&#65292;&#23427;&#21160;&#24577;&#20998;&#26512;&#27599;&#20010;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#23427;&#22312;&#29289;&#21697;&#21644;&#29992;&#25143;&#21452;&#26041;&#38754;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#20016;&#23500;&#23427;&#20204;&#30340;&#34920;&#31034;&#65292;&#26368;&#22823;&#21270;&#30693;&#35782;&#22270;&#35889;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#30340;&#21033;&#29992;&#12290; RKGCN&#33021;&#22815;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#19979;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using knowledge graphs to assist deep learning models in making recommendation decisions has recently been proven to effectively improve the model's interpretability and accuracy. This paper introduces an end-to-end deep learning model, named RKGCN, which dynamically analyses each user's preferences and makes a recommendation of suitable items. It combines knowledge graphs on both the item side and user side to enrich their representations to maximize the utilization of the abundant information in knowledge graphs. RKGCN is able to offer more personalized and relevant recommendations in three different scenarios. The experimental results show the superior effectiveness of our model over 5 baseline models on three real-world datasets including movies, books, and music.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#20998;&#26512;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;&#20998;&#32423;&#27169;&#24577;&#36923;&#36753;&#20013;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#20174;&#32780;&#35774;&#35745;&#20986;&#26356;&#22909;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12306</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Logical Expressiveness of Graph Neural Network for Knowledge Graph Reasoning. (arXiv:2303.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#20998;&#26512;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;&#20998;&#32423;&#27169;&#24577;&#36923;&#36753;&#20013;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#20174;&#32780;&#35774;&#35745;&#20986;&#26356;&#22909;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#34987;&#24341;&#20837;&#29992;&#20110;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23427;&#20204;&#33391;&#22909;&#30340;&#32463;&#39564;&#24615;&#33021;&#32570;&#20047;&#29702;&#35770;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#23545;&#20110;&#24402;&#32435;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21482;&#26159;&#20026;&#20102;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#19988;&#23545;&#23427;&#20204;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#19978;&#36848;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#36923;&#36753;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;GNN&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25214;&#20986;&#30693;&#35782;&#22270;&#35889;&#20013;&#21487;&#20197;&#25429;&#33719;&#21738;&#20123;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#20808;&#34920;&#26126;&#65292;GNN&#21487;&#20197;&#20174;&#20998;&#32423;&#27169;&#24577;&#36923;&#36753;&#20013;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#20026;&#20998;&#26512;GNN&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#24037;&#20855;&#65307;&#32780;&#19968;&#20010;&#26597;&#35810;&#26631;&#35760;&#25216;&#24039;&#20351;&#24471;GNN&#26356;&#23481;&#26131;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#26631;&#35760;&#25216;&#24039;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#30340;&#35265;&#35299;&#20419;&#36827;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;GNN&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#30340;&#35774;&#35745;&#65292;&#23427;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been recently introduced to learn from knowledge graph (KG) and achieved state-of-the-art performance in KG reasoning. However, a theoretical certification for their good empirical performance is still absent. Besides, while logic in KG is important for inductive and interpretable inference, existing GNN-based methods are just designed to fit data distributions with limited knowledge of their logical expressiveness. We propose to fill the above gap in this paper. Specifically, we theoretically analyze GNN from logical expressiveness and find out what kind of logical rules can be captured from KG. Our results first show that GNN can capture logical rules from graded modal logic, providing a new theoretical tool for analyzing the expressiveness of GNN for KG reasoning; and a query labeling trick makes it easier for GNN to capture logical rules, explaining why SOTA methods are mainly based on labeling trick. Finally, insights from our theory motivate the 
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;UTSP&#33021;&#22815;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#36335;&#24452;&#20026;&#21704;&#23494;&#39039;&#24490;&#29615;&#30340;&#21069;&#25552;&#19979;&#65292;&#33021;&#22815;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;UTSP&#22312;&#35757;&#32451;&#26679;&#26412;&#19982;&#21442;&#25968;&#25968;&#37327;&#19978;&#21344;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#19988;&#24615;&#33021;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.10538</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#27714;&#35299;&#26053;&#34892;&#21830;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning for Solving the Travelling Salesman Problem. (arXiv:2303.10538v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10538
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;UTSP&#33021;&#22815;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#36335;&#24452;&#20026;&#21704;&#23494;&#39039;&#24490;&#29615;&#30340;&#21069;&#25552;&#19979;&#65292;&#33021;&#22815;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;UTSP&#22312;&#35757;&#32451;&#26679;&#26412;&#19982;&#21442;&#25968;&#25968;&#37327;&#19978;&#21344;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#19988;&#24615;&#33021;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;UTSP&#65292;&#19968;&#31181;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#27714;&#35299;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26367;&#20195;&#25439;&#22833;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;GNN&#36755;&#20986;&#19968;&#20010;&#28909;&#21147;&#22270;&#34920;&#31034;&#27599;&#20010;&#36793;&#25104;&#20026;&#26368;&#20248;&#36335;&#24452;&#30340;&#27010;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#23616;&#37096;&#25628;&#32034;&#26681;&#25454;&#28909;&#21147;&#22270;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#19968;&#37096;&#20998;&#25512;&#21160;&#27169;&#22411;&#25214;&#21040;&#26368;&#30701;&#30340;&#36335;&#24452;&#65292;&#21478;&#19968;&#37096;&#20998;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#65292;&#30830;&#20445;&#36335;&#24452;&#24418;&#25104;&#21704;&#23494;&#39039;&#24490;&#29615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UTSP&#20248;&#20110;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;TSP&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21442;&#25968;&#25928;&#29575;&#21644;&#25968;&#25454;&#25928;&#29575;&#22343;&#36739;&#39640;&#65306;&#19982;&#24378;&#21270;&#23398;&#20064;&#25110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#20165;&#21344;&#29992;&#32422;10&#65285;&#30340;&#21442;&#25968;&#21644;&#32422;0.2&#65285;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose UTSP, an unsupervised learning (UL) framework for solving the Travelling Salesman Problem (TSP). We train a Graph Neural Network (GNN) using a surrogate loss. The GNN outputs a heat map representing the probability for each edge to be part of the optimal path. We then apply local search to generate our final prediction based on the heat map. Our loss function consists of two parts: one pushes the model to find the shortest path and the other serves as a surrogate for the constraint that the route should form a Hamiltonian Cycle. Experimental results show that UTSP outperforms the existing data-driven TSP heuristics. Our approach is parameter efficient as well as data efficient: the model takes $\sim$ 10\% of the number of parameters and $\sim$ 0.2\% of training samples compared with reinforcement learning or supervised learning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26412;&#22320;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#31639;&#27861; LDECC&#65292;&#21487;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#23454;&#29616;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2302.08070</link><description>&lt;p&gt;
&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#26412;&#22320;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local Causal Discovery for Estimating Causal Effects. (arXiv:2302.08070v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26412;&#22320;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#31639;&#27861; LDECC&#65292;&#21487;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#23454;&#29616;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#25105;&#20204;&#30340;&#25968;&#25454;&#32972;&#21518;&#30340;&#22240;&#26524;&#22270;&#24418;&#26159;&#26410;&#30693;&#30340;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#26469;&#32553;&#23567;&#21487;&#33021;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21462;&#20540;&#33539;&#22260;&#65292;&#26041;&#24335;&#26159;&#65288;1&#65289;&#35782;&#21035;&#21040;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;; &#21644;&#65288;2&#65289;&#20272;&#35745;&#35813;&#31867;&#20013;&#27599;&#20010;&#22270;&#30340;ATE&#12290;&#23613;&#31649;PC&#31639;&#27861;&#22312;&#24378;&#20445;&#30495;&#24230;&#20551;&#35774;&#19979;&#21487;&#20197;&#35782;&#21035;&#35813;&#31867;&#21035;&#65292;&#20294;&#35745;&#31639;&#19978;&#30340;&#38480;&#21046;&#35753;&#20154;&#26395;&#32780;&#21364;&#27493;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20165;&#38656;&#35201;&#20851;&#20110;&#22788;&#29702;&#30340;&#23616;&#37096;&#22270;&#32467;&#26500;&#21363;&#21487;&#35782;&#21035;&#20986;&#21487;&#33021;&#30340;ATE&#20540;&#38598;&#65292;&#36825;&#26159;&#30001;&#26412;&#22320;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#20107;&#23454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26412;&#22320;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#31639;&#27861;&#8212;&#8212;&#20351;&#29992;&#24613;&#20999;&#30896;&#25758;&#26816;&#26597;&#30340;&#26412;&#22320;&#21457;&#29616;&#65288;LDECC&#65289;&#65292;&#20351;&#29992;&#26410;&#23631;&#34109;&#30340;&#30896;&#25758;&#22120;&#26469;&#20351;&#22788;&#29702;&#30340;&#29238;&#39033;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23384;&#22312;&#22270;&#24418;&#65292;&#22312;&#36825;&#20123;&#22270;&#24418;&#20013;&#65292;LDECC&#21576;&#25351;&#25968;&#32423;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#26412;&#22320;&#21457;&#29616;&#31639;&#27861;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;LDECC&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#20272;&#35745;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#39640;&#26031;&#35823;&#24046;&#19979;&#38480;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even when the causal graph underlying our data is unknown, we can use observational data to narrow down the possible values that an average treatment effect (ATE) can take by (1) identifying the graph up to a Markov equivalence class; and (2) estimating that ATE for each graph in the class. While the PC algorithm can identify this class under strong faithfulness assumptions, it can be computationally prohibitive. Fortunately, only the local graph structure around the treatment is required to identify the set of possible ATE values, a fact exploited by local discovery algorithms to improve computational efficiency. In this paper, we introduce Local Discovery using Eager Collider Checks (LDECC), a new local causal discovery algorithm that leverages unshielded colliders to orient the treatment's parents differently from existing methods. We show that there exist graphs where LDECC exponentially outperforms existing local discovery algorithms and vice versa. Moreover, we show that LDECC an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#35757;&#32451;&#26041;&#26696;&#65292;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#26368;&#23567;&#26368;&#22823;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#29983;&#25104;&#22120;&#20998;&#24067;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SAN&#30456;&#23545;&#20110;&#26222;&#36890;GAN&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;StyleGAN-XL&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2301.12811</link><description>&lt;p&gt;
SAN: &#21033;&#29992;&#21028;&#21035;&#24335;&#24402;&#19968;&#21270;&#32447;&#24615;&#23618;&#35825;&#23548;GAN&#30340;&#21487;&#27979;&#24615;
&lt;/p&gt;
&lt;p&gt;
SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer. (arXiv:2301.12811v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#35757;&#32451;&#26041;&#26696;&#65292;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#26368;&#23567;&#26368;&#22823;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#29983;&#25104;&#22120;&#20998;&#24067;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SAN&#30456;&#23545;&#20110;&#26222;&#36890;GAN&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;StyleGAN-XL&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#26368;&#23567;&#26368;&#22823;&#30446;&#26631;&#20989;&#25968;&#26469;&#23398;&#20064;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65306;&#20248;&#21270;&#26159;&#21542;&#30495;&#27491;&#25552;&#20379;&#20102;&#20351;&#29983;&#25104;&#22120;&#20998;&#24067;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;GAN&#30340;&#24418;&#24335;&#19982;&#20999;&#29255;&#26368;&#20248;&#36755;&#36816;&#30340;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#65292;&#25512;&#23548;&#20102;&#21487;&#27979;&#24615;&#26465;&#20214;&#65292;&#21363;&#21028;&#21035;&#22120;&#20316;&#20026;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#35757;&#32451;&#26041;&#26696;&#65292;&#31216;&#20026;&#20999;&#29255;&#23545;&#25239;&#32593;&#32476;&#65288;SAN&#65289;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#21487;&#20197;&#23558;&#24191;&#27867;&#31867;&#21035;&#30340;&#29616;&#26377;GAN&#36716;&#21270;&#20026;SAN&#12290;&#22312;&#21512;&#25104;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#21644;SAN&#30456;&#23545;&#20110;&#26222;&#36890;GAN&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;SAN&#24212;&#29992;&#20110;StyleGAN-XL&#65292;&#22312;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;GAN&#20013;&#26368;&#20808;&#36827;&#30340;FID&#65288;Frechet Inception Distance&#65289;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) learn a target probability distribution by optimizing a generator and a discriminator with minimax objectives. This paper addresses the question of whether such optimization actually provides the generator with gradients that make its distribution close to the target distribution. We derive metrizable conditions, sufficient conditions for the discriminator to serve as the distance between the distributions by connecting the GAN formulation with the concept of sliced optimal transport. Furthermore, by leveraging these theoretical results, we propose a novel GAN training scheme, called slicing adversarial network (SAN). With only simple modifications, a broad class of existing GANs can be converted to SANs. Experiments on synthetic and image datasets support our theoretical results and the SAN's effectiveness as compared to usual GANs. Furthermore, we also apply SAN to StyleGAN-XL, which leads to state-of-the-art FID score amongst GANs for class con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#25216;&#26415;&#25552;&#39640;&#22270;&#20687;&#20445;&#30495;&#24230;&#30340;&#20154;&#33080;&#21464;&#24418;&#25915;&#20987;&#65292;&#36890;&#36807;&#23558;&#20004;&#31181;&#29305;&#24449;&#32467;&#21512;&#25552;&#39640;&#20102;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#21644;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#31995;&#32479;&#30340;&#26131;&#21463;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.04218</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#25216;&#26415;&#36827;&#34892;&#39640;&#20445;&#30495;&#24230;&#20154;&#33080;&#21464;&#24418;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Leveraging Diffusion For Strong and High Quality Face Morphing Attacks. (arXiv:2301.04218v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#25216;&#26415;&#25552;&#39640;&#22270;&#20687;&#20445;&#30495;&#24230;&#30340;&#20154;&#33080;&#21464;&#24418;&#25915;&#20987;&#65292;&#36890;&#36807;&#23558;&#20004;&#31181;&#29305;&#24449;&#32467;&#21512;&#25552;&#39640;&#20102;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#21644;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#31995;&#32479;&#30340;&#26131;&#21463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#21464;&#24418;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21576;&#29616;&#30001;&#20004;&#20010;&#19981;&#21516;&#36523;&#20221;&#30340;&#29983;&#29289;&#29305;&#24449;&#32452;&#25104;&#30340;&#21464;&#24418;&#22270;&#20687;&#26469;&#27450;&#39575;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#65292;&#20197;&#26399;&#26395;&#35302;&#21457;&#19968;&#20010;&#38169;&#35823;&#30340;&#25509;&#21463;&#65292;&#20174;&#32780;&#23545;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#26550;&#26500;&#26469;&#25913;&#36827;&#22270;&#20687;&#35270;&#35273;&#20445;&#30495;&#24230;&#30340;&#21464;&#24418;&#25915;&#20987;&#65292;&#24182;&#25552;&#39640;&#21464;&#24418;&#25915;&#20987;&#34920;&#31034;&#20004;&#31181;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;Frechet Inception Distance (FID)&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#30340;&#35270;&#35273;&#20445;&#30495;&#24230;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#27979;&#37327;FR&#31995;&#32479;&#23545;&#25152;&#25552;&#20986;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#12290;&#36824;&#27979;&#35797;&#20102;&#19968;&#31181;&#21464;&#24418;&#25915;&#20987;&#26816;&#27979;&#22120;&#26469;&#26816;&#27979;&#25152;&#25552;&#20986;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face morphing attacks seek to deceive a Face Recognition (FR) system by presenting a morphed image consisting of the biometric qualities from two different identities with the aim of triggering a false acceptance with one of the two identities, thereby presenting a significant threat to biometric systems. The success of a morphing attack is dependent on the ability of the morphed image to represent the biometric characteristics of both identities that were used to create the image. We present a novel morphing attack that uses a Diffusion-based architecture to improve the visual fidelity of the image and the ability of the morphing attack to represent characteristics from both identities. We demonstrate the effectiveness of the proposed attack by evaluating its visual fidelity via the Frechet Inception Distance (FID). Also, extensive experiments are conducted to measure the vulnerability of FR systems to the proposed attack. The ability of a morphing attack detector to detect the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2211.05985</link><description>&lt;p&gt;
&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#26469;&#35299;&#37322;&#21644;&#26816;&#27979;&#20581;&#24247;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#26159;&#24403;&#20170;&#31038;&#20250;&#30340;&#19968;&#22823;&#38382;&#39064;&#65292;&#35768;&#22810;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#21162;&#21147;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#27599;&#22825;&#21019;&#36896;&#30340;&#34394;&#20551;&#20449;&#24687;&#25968;&#37327;&#24040;&#22823;&#65292;&#23558;&#27492;&#20219;&#21153;&#30041;&#32473;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#22810;&#24180;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65292;&#20294;&#20170;&#22825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#20026;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#28155;&#21152;&#19968;&#20010;&#26032;&#23618;&#27425;&#65307;&#20351;&#29992;&#20855;&#26377;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36825;&#31687;&#25991;&#31456;&#21487;&#20197;&#26631;&#35760;&#20026;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#35768;&#22810;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#26032;&#27880;&#37322;&#26041;&#26696;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.15240</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Prompt&#35843;&#25972;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#24341;&#36215;&#20102;&#30740;&#31350;&#28909;&#28526;&#12290;&#19982;&#35821;&#35328;&#39046;&#22495;&#37319;&#29992;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#31574;&#30053;&#19981;&#21516;&#65292;&#22270;&#24418;&#39046;&#22495;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;&#22522;&#20110;Prompt&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35843;&#25972;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature (GPF) &#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#19979;&#30340;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;GPF&#22312;&#36755;&#20837;&#22270;&#24418;&#30340;&#29305;&#24449;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#29702;&#35770;&#19978;&#21487;&#23454;&#29616;&#19982;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#31561;&#25928;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#20877;&#38656;&#35201;&#26126;&#30830;&#35828;&#26126;&#27599;&#20010;&#39044;&#35757;&#32451;&#31574;&#30053;&#23545;&#24212;&#30340;Prompt&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#37319;&#29992;GPF&#26469;&#23454;&#29616;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#32422;&#26463;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38480;&#21046;&#28508;&#22312;&#34920;&#31034;&#36981;&#24490;&#29305;&#23450;&#21160;&#24577;&#35268;&#24459;&#20197;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#24847;&#20041;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;DNA&#33639;&#20809;&#30005;&#24433;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#65292;&#34920;&#26126;&#20854;&#21487;&#20197;&#20934;&#30830;&#22320;&#23398;&#20064;&#21160;&#24577;&#35268;&#24459;&#65292;&#24182;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2209.00905</link><description>&lt;p&gt;
&#20174;&#28508;&#22312;&#21160;&#21147;&#23398;&#21040;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
From latent dynamics to meaningful representations. (arXiv:2209.00905v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#32422;&#26463;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38480;&#21046;&#28508;&#22312;&#34920;&#31034;&#36981;&#24490;&#29305;&#23450;&#21160;&#24577;&#35268;&#24459;&#20197;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#24847;&#20041;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;DNA&#33639;&#20809;&#30005;&#24433;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#65292;&#34920;&#26126;&#20854;&#21487;&#20197;&#20934;&#30830;&#22320;&#23398;&#20064;&#21160;&#24577;&#35268;&#24459;&#65292;&#24182;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#34920;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#23835;&#36215;&#30340;&#26680;&#24515;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#26159;&#20351;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#24847;&#20041;&#12290;&#20026;&#27492;&#65292;&#20856;&#22411;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20808;&#39564;&#27010;&#29575;&#20998;&#24067;&#26469;&#35268;&#33539;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20808;&#39564;&#36890;&#24120;&#26159;&#19981;&#21487;&#29992;&#25110;&#20020;&#26102;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#32422;&#26463;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#19981;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#27010;&#29575;&#65292;&#32780;&#26159;&#38480;&#21046;&#28508;&#22312;&#34920;&#31034;&#36981;&#24490;&#29305;&#23450;&#30340;&#21160;&#24577;&#35268;&#24459;&#65292;&#36825;&#26159;&#21160;&#24577;&#31995;&#32479;&#34920;&#31034;&#23398;&#20064;&#26356;&#33258;&#28982;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#20449;&#20208;&#28304;&#20110;&#29289;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#35266;&#23519;&#65292;&#21363;&#34429;&#28982;&#19981;&#21516;&#30340;&#31995;&#32479;&#21487;&#20197;&#26377;&#19981;&#21516;&#30340;&#36793;&#38469;&#27010;&#29575;&#20998;&#24067;&#65292;&#20294;&#36890;&#24120;&#36981;&#24490;&#30456;&#21516;&#30340;&#21160;&#24577;&#35268;&#24459;&#65292;&#20363;&#22914;&#29275;&#39039;&#21644;&#34203;&#23450;&#35860;&#26041;&#31243;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#31995;&#32479;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#33639;&#20809;DNA&#30005;&#24433;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#23398;&#20064;&#21160;&#24577;&#35268;&#24459;&#65292;&#24182;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
While representation learning has been central to the rise of machine learning and artificial intelligence, a key problem remains in making the learnt representations meaningful. For this the typical approach is to regularize the learned representation through prior probability distributions. However such priors are usually unavailable or ad hoc. To deal with this, we propose a dynamics-constrained representation learning framework. Instead of using predefined probabilities, we restrict the latent representation to follow specific dynamics, which is a more natural constraint for representation learning in dynamical systems. Our belief stems from a fundamental observation in physics that though different systems can have different marginalized probability distributions, they typically obey the same dynamics, such as Newton's and Schrodinger's equations. We validate our framework for different systems including a real-world fluorescent DNA movie dataset. We show that our algorithm can un
&lt;/p&gt;</description></item></channel></rss>