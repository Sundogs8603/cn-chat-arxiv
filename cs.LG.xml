<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#22312;&#30701;&#26102;&#38388;&#20869;&#26631;&#35760;8000&#20010;&#33145;&#37096;CT&#25195;&#25551;&#20013;&#30340;8&#20010;&#22120;&#23448;&#65292;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#22810;&#22120;&#23448;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.09666</link><description>&lt;p&gt;
&#22312;&#19977;&#21608;&#20869;&#20026;8,000&#20010;&#33145;&#37096;CT&#25195;&#25551;&#26631;&#27880;&#22810;&#22120;&#23448;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Annotating 8,000 Abdominal CT Volumes for Multi-Organ Segmentation in Three Weeks. (arXiv:2305.09666v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#22312;&#30701;&#26102;&#38388;&#20869;&#26631;&#35760;8000&#20010;&#33145;&#37096;CT&#25195;&#25551;&#20013;&#30340;8&#20010;&#22120;&#23448;&#65292;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#22810;&#22120;&#23448;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#26631;&#27880;&#65292;&#29305;&#21035;&#26159;&#22120;&#23448;&#20998;&#21106;&#65292;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#22120;&#23448;&#20998;&#21106;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;&#25105;&#20204;&#26631;&#27880;&#20102;8,448&#20010;&#33145;&#37096;CT&#25195;&#25551;&#65292;&#26631;&#35760;&#20102;&#33086;&#33039;&#12289;&#32925;&#33039;&#12289;&#32958;&#33039;&#12289;&#32963;&#12289;&#32966;&#22218;&#12289;&#33008;&#33146;&#12289;&#20027;&#21160;&#33033;&#21644;&#19979;&#33108;&#38745;&#33033;&#12290;&#20256;&#32479;&#30340;&#26631;&#27880;&#26041;&#27861;&#38656;&#35201;&#19968;&#20301;&#32463;&#39564;&#20016;&#23500;&#30340;&#26631;&#27880;&#21592;1600&#21608;&#65292;&#32780;&#25105;&#20204;&#30340;&#26631;&#27880;&#26041;&#27861;&#20165;&#29992;&#20102;&#19977;&#21608;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating medical images, particularly for organ segmentation, is laborious and time-consuming. For example, annotating an abdominal organ requires an estimated rate of 30-60 minutes per CT volume based on the expertise of an annotator and the size, visibility, and complexity of the organ. Therefore, publicly available datasets for multi-organ segmentation are often limited in data size and organ diversity. This paper proposes a systematic and efficient method to expedite the annotation process for organ segmentation. We have created the largest multi-organ dataset (by far) with the spleen, liver, kidneys, stomach, gallbladder, pancreas, aorta, and IVC annotated in 8,448 CT volumes, equating to 3.2 million slices. The conventional annotation methods would take an experienced annotator up to 1,600 weeks (or roughly 30.8 years) to complete this task. In contrast, our annotation method has accomplished this task in three weeks (based on an 8-hour workday, five days a week) while maintain
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;P2MPO&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#40065;&#26834;&#31163;&#32447;RL&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#21644;&#21452;&#37325;&#24754;&#35266;&#30340;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#65292;&#37319;&#29992;&#21452;&#37325;&#24754;&#35266;&#24615;&#21407;&#21017;&#20197;&#20811;&#26381;&#27169;&#22411;&#20559;&#31227;&#31561;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#35813;&#26694;&#26550;&#22312;&#25317;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#20855;&#22791;&#39640;&#25928;&#24615;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.09659</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40065;&#26834;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#21452;&#37325;&#24754;&#35266;&#24615;&#30340;&#36890;&#29992;&#31639;&#27861;&#21644;&#24378;&#20581;&#37096;&#20998;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage. (arXiv:2305.09659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;P2MPO&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#40065;&#26834;&#31163;&#32447;RL&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#21644;&#21452;&#37325;&#24754;&#35266;&#30340;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#65292;&#37319;&#29992;&#21452;&#37325;&#24754;&#35266;&#24615;&#21407;&#21017;&#20197;&#20811;&#26381;&#27169;&#22411;&#20559;&#31227;&#31561;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#35813;&#26694;&#26550;&#22312;&#25317;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#20855;&#22791;&#39640;&#25928;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#40065;&#26834;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;&#40065;&#26834;&#31163;&#32447;RL&#65289;&#65292;&#20854;&#26088;&#22312;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#32431;&#31929;&#22320;&#25214;&#21040;&#19968;&#20010;&#33021;&#22815;&#22312;&#25200;&#21160;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#26368;&#20248;&#24378;&#40065;&#26834;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;P2MPO&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#21644;&#21452;&#37325;&#24754;&#35266;&#30340;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#12290;&#21452;&#37325;&#24754;&#35266;&#24615;&#21407;&#21017;&#23545;&#20110;&#20811;&#26381;&#30001;&#34892;&#20026;&#31574;&#30053;&#21644;&#30446;&#26631;&#31574;&#30053;&#23478;&#26063;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#20197;&#21450;&#21517;&#20041;&#27169;&#22411;&#30340;&#25200;&#21160;&#25152;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23545;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#36827;&#34892;&#19968;&#23450;&#20934;&#30830;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;P2MPO&#31639;&#27861;&#22312;&#25317;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distributionally robust offline reinforcement learning (robust offline RL), which seeks to find an optimal robust policy purely from an offline dataset that can perform well in perturbed environments. We propose a generic algorithm framework \underline{D}oubly \underline{P}essimistic \underline{M}odel-based \underline{P}olicy \underline{O}ptimization ($\texttt{P}^2\texttt{MPO}$) for robust offline RL, which features a novel combination of a flexible model estimation subroutine and a doubly pessimistic policy optimization step. The \emph{double pessimism} principle is crucial to overcome the distributional shift incurred by i) the mismatch between behavior policy and the family of target policies; and ii) the perturbation of the nominal model. Under certain accuracy assumptions on the model estimation subroutine, we show that $\texttt{P}^2\texttt{MPO}$ is provably efficient with \emph{robust partial coverage data}, which means that the offline dataset has good coverage of the d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25351;&#23548;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#31216;&#20026;LGTM&#65292;&#20854;&#21033;&#29992;&#33976;&#39311;&#25928;&#24212;&#36873;&#25321;&#26679;&#26412;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09651</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25351;&#23548;&#26377;&#21161;&#20110;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation. (arXiv:2305.09651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25351;&#23548;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#31216;&#20026;LGTM&#65292;&#20854;&#21033;&#29992;&#33976;&#39311;&#25928;&#24212;&#36873;&#25321;&#26679;&#26412;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#33021;&#21147;&#36229;&#32676;&#30340;&#25945;&#24072;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#35753;&#23398;&#29983;&#27700;&#24179;&#24471;&#21040;&#25552;&#21319;&#65292;&#36825;&#20984;&#26174;&#20102;&#24403;&#21069;&#25945;&#24072;&#22521;&#35757;&#23454;&#36341;&#21644;&#26377;&#25928;&#30693;&#35782;&#20256;&#25480;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#25945;&#24072;&#22521;&#35757;&#36807;&#31243;&#30340;&#25351;&#23548;&#25928;&#26524;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#33976;&#39311;&#25928;&#24212;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#23545;&#23398;&#29983;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#22909;&#25945;&#24072;&#24456;&#37325;&#35201;&#65288;LGTM&#65289;&#30340;&#26377;&#25928;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#23558;&#33976;&#39311;&#25928;&#24212;&#32435;&#20837;&#25945;&#24072;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#21487;&#33021;&#25552;&#21319;&#23398;&#29983;&#27867;&#21270;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#30340;LGTM&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student's generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher's learning process. By prioritizing samples that are likely to enhance the student's generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Prompt-Tuning DT&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36712;&#36857;&#27573;&#20316;&#20026;prompt&#26469;&#25351;&#23548;RL agent&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09648</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#25490;&#24207;&#30340;Prompt-Tuning&#20915;&#31574;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Prompt-Tuning Decision Transformer with Preference Ranking. (arXiv:2305.09648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Prompt-Tuning DT&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36712;&#36857;&#27573;&#20316;&#20026;prompt&#26469;&#25351;&#23548;RL agent&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-tuning&#24050;&#25104;&#20026;&#19968;&#31181;&#24456;&#26377;&#21069;&#36884;&#30340;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#25110;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;Prompt learning &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#30001;&#20110;RL prompts&#20013;&#21253;&#21547;&#30340;&#22797;&#26434;&#29289;&#29702;&#21547;&#20041;&#21644;&#29615;&#22659;&#29305;&#23450;&#20449;&#24687;&#65292;&#20854;&#36866;&#29992;&#24615;&#26377;&#38480;&#12290;&#36825;&#20123;&#22240;&#32032;&#38656;&#35201;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#26469;&#27169;&#20223;&#28436;&#31034;&#65292;&#24182;&#22312;&#23398;&#20064;&#21518;&#21487;&#33021;&#23548;&#33268;&#24847;&#20041;&#30340;&#20007;&#22833;&#12290;&#27492;&#22806;&#65292;&#23558;prompt-tuning&#26041;&#27861;&#30452;&#25509;&#25193;&#23637;&#21040;RL&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;RL prompts&#26159;&#26681;&#25454;&#29615;&#22659;&#24314;&#27169;&#21644;&#20998;&#26512;&#26469;&#25351;&#23548;agent&#34892;&#20026;&#30340;&#65292;&#32780;&#19981;&#26159;&#22635;&#34917;&#32570;&#22833;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#35843;&#25972;prompt&#26684;&#24335;&#65292;&#22914;&#22312;NLP&#20013;&#65292;&#21487;&#33021;&#19981;&#20250;&#20135;&#29983;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Prompt-Tuning DT&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;&#36712;&#36857;&#27573;&#29992;&#20316;prompt&#26469;&#25351;&#23548;RL agent&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#40657;&#30418;&#35843;&#25972;&#26469;&#20248;&#21270;prompt&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-tuning has emerged as a promising method for adapting pre-trained models to downstream tasks or aligning with human preferences. Prompt learning is widely used in NLP but has limited applicability to RL due to the complex physical meaning and environment-specific information contained within RL prompts. These factors require supervised learning to imitate the demonstrations and may result in a loss of meaning after learning. Additionally, directly extending prompt-tuning approaches to RL is challenging because RL prompts guide agent behavior based on environmental modeling and analysis, rather than filling in missing information, making it unlikely that adjustments to the prompt format for downstream tasks, as in NLP, can yield significant improvements. In this work, we propose the Prompt-Tuning DT algorithm to address these challenges by using trajectory segments as prompts to guide RL agents in acquiring environmental information and optimizing prompts via black-box tuning to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;torchosr--&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;Open Set Recognition&#35774;&#35745;&#30340;Python&#25193;&#23637;&#21253;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#26368;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31995;&#21015;&#22788;&#29702;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#30340;&#24037;&#20855;&#65292;&#30446;&#30340;&#26159;&#20026;&#20102;&#31616;&#21270;&#21644;&#25512;&#24191;&#27491;&#30830;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.09646</link><description>&lt;p&gt;
torchosr--&#19968;&#20010;PyTorch&#25193;&#23637;&#21253;&#29992;&#20110;Python&#20013;Open Set Recognition&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
torchosr -- a PyTorch extension package for Open Set Recognition models evaluation in Python. (arXiv:2305.09646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;torchosr--&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;Open Set Recognition&#35774;&#35745;&#30340;Python&#25193;&#23637;&#21253;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#26368;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31995;&#21015;&#22788;&#29702;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#30340;&#24037;&#20855;&#65292;&#30446;&#30340;&#26159;&#20026;&#20102;&#31616;&#21270;&#21644;&#25512;&#24191;&#27491;&#30830;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;torchosr&#21253;--&#19968;&#20010;&#19982;PyTorch&#24211;&#20860;&#23481;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#19987;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Open Set Recognition&#35774;&#35745;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;&#35813;&#36719;&#20214;&#21253;&#25552;&#20379;&#20102;&#20004;&#31181;&#26368;&#26032;&#30340;Open Set Recognition&#26041;&#27861;&#65292;&#19968;&#32452;&#22788;&#29702;&#22522;&#30784;&#38598;&#24182;&#29983;&#25104;&#29992;&#20110;Open Set Recognition&#20219;&#21153;&#30340;&#27966;&#29983;&#38598;&#30340;&#20989;&#25968;&#65288;&#20854;&#20013;&#19968;&#20123;&#31867;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#34987;&#35748;&#20026;&#26159;&#26410;&#30693;&#30340;&#24182;&#20165;&#20351;&#29992;&#65289;&#65292;&#20197;&#21450;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#30340;&#20854;&#20182;&#24037;&#20855;&#12290;&#35813;&#36719;&#20214;&#21253;&#25552;&#35758;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#31616;&#21270;&#21644;&#25512;&#24191;&#27491;&#30830;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#20854;&#20013;&#23454;&#39564;&#22312;&#22823;&#37327;&#20855;&#26377;&#19981;&#21516;Openness&#21644;&#31867;&#21035;&#20998;&#37197;&#30340;&#27966;&#29983;&#38598;&#19978;&#36827;&#34892;&#12290;&#20316;&#32773;&#24076;&#26395;&#65292;&#22312;&#36719;&#20214;&#21253;&#20013;&#25552;&#20379;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23558;&#25104;&#20026;&#25152;&#36848;&#39046;&#22495;&#30456;&#20851;&#35299;&#20915;&#26041;&#26696;&#30340;&#27491;&#30830;&#21644;&#24320;&#28304;&#23454;&#29616;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article presents the torchosr package - a Python package compatible with PyTorch library - offering tools and methods dedicated to Open Set Recognition in Deep Neural Networks. The package offers two state-of-the-art methods in the field, a set of functions for handling base sets and generation of derived sets for the Open Set Recognition task (where some classes are considered unknown and used only in the testing process) and additional tools to handle datasets and methods. The main goal of the package proposal is to simplify and promote the correct experimental evaluation, where experiments are carried out on a large number of derivative sets with various Openness and class-to-category assignments. The authors hope that state-of-the-art methods available in the package will become a source of a correct and open-source implementation of the relevant solutions in the domain.
&lt;/p&gt;</description></item><item><title>FitMe&#26159;&#19968;&#20010;&#21487;&#29992;&#20110;&#21333;&#20010;&#25110;&#22810;&#20010;&#22270;&#20687;&#30340;&#28145;&#23618;&#36924;&#30495;&#30340;&#19977;&#32500;&#21487;&#22609;&#27169;&#22411;&#21270;&#36523;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#27169;&#24335;&#29983;&#25104;&#22120;&#21644;PCA&#24418;&#29366;&#27169;&#22411;&#25429;&#25417;&#20102;&#20154;&#33080;&#22806;&#35266;&#30340;&#28459;&#21453;&#23556;&#21644;&#38236;&#38754;&#21453;&#23556;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#24555;&#36895;&#21487;&#24494;&#20998;&#28210;&#26579;&#36807;&#31243;&#20197;&#23454;&#29616;&#39640;&#20445;&#30495;&#21487;&#28210;&#26579;&#30340;&#20154;&#31867;&#21270;&#36523;&#12290;</title><link>http://arxiv.org/abs/2305.09641</link><description>&lt;p&gt;
FitMe&#65306;&#28145;&#23618;&#36924;&#30495;&#30340;&#19977;&#32500;&#21487;&#22609;&#27169;&#22411;&#21270;&#36523;
&lt;/p&gt;
&lt;p&gt;
FitMe: Deep Photorealistic 3D Morphable Model Avatars. (arXiv:2305.09641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09641
&lt;/p&gt;
&lt;p&gt;
FitMe&#26159;&#19968;&#20010;&#21487;&#29992;&#20110;&#21333;&#20010;&#25110;&#22810;&#20010;&#22270;&#20687;&#30340;&#28145;&#23618;&#36924;&#30495;&#30340;&#19977;&#32500;&#21487;&#22609;&#27169;&#22411;&#21270;&#36523;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#27169;&#24335;&#29983;&#25104;&#22120;&#21644;PCA&#24418;&#29366;&#27169;&#22411;&#25429;&#25417;&#20102;&#20154;&#33080;&#22806;&#35266;&#30340;&#28459;&#21453;&#23556;&#21644;&#38236;&#38754;&#21453;&#23556;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#24555;&#36895;&#21487;&#24494;&#20998;&#28210;&#26579;&#36807;&#31243;&#20197;&#23454;&#29616;&#39640;&#20445;&#30495;&#21487;&#28210;&#26579;&#30340;&#20154;&#31867;&#21270;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FitMe&#65292;&#19968;&#20010;&#20154;&#33080;&#21453;&#23556;&#27169;&#22411;&#21644;&#21487;&#24494;&#20998;&#28210;&#26579;&#20248;&#21270;&#31649;&#36947;&#65292;&#21487;&#20174;&#21333;&#20010;&#25110;&#22810;&#20010;&#22270;&#20687;&#20013;&#33719;&#21462;&#39640;&#20445;&#30495;&#21487;&#28210;&#26579;&#30340;&#20154;&#31867;&#21270;&#36523;&#12290;&#35813;&#27169;&#22411;&#30001;&#22810;&#27169;&#24335;&#22522;&#20110;&#39118;&#26684;&#30340;&#29983;&#25104;&#22120;&#21644;&#22522;&#20110;PCA&#30340;&#24418;&#29366;&#27169;&#22411;&#32452;&#25104;&#65292;&#25429;&#25417;&#20154;&#33080;&#22806;&#35266;&#30340;&#28459;&#21453;&#23556;&#21644;&#38236;&#38754;&#21453;&#23556;&#12290;&#25105;&#20204;&#37319;&#29992;&#24555;&#36895;&#21487;&#24494;&#20998;&#28210;&#26579;&#36807;&#31243;&#65292;&#22312;&#20248;&#21270;&#31649;&#36947;&#20013;&#20351;&#29992;&#65292;&#21516;&#26102;&#23454;&#29616;&#36924;&#30495;&#30340;&#38754;&#37096;&#30528;&#33394;&#12290;FitMe&#22312;&#21333;&#20010;&#8220;&#37326;&#22806;&#8221;&#20154;&#33080;&#22270;&#20687;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21453;&#23556;&#33719;&#21462;&#21644;&#36523;&#20221;&#20445;&#25252;&#65292;&#24182;&#22312;&#32473;&#23450;&#21516;&#19968;&#36523;&#20221;&#30340;&#22810;&#20010;&#19981;&#21463;&#32422;&#26463;&#30340;&#20154;&#33080;&#22270;&#20687;&#26102;&#20135;&#29983;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25195;&#25551;&#25928;&#26524;&#12290;&#19982;&#26368;&#36817;&#25913;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;FitMe&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#20248;&#21270;&#36807;&#31243;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#21487;&#28210;&#26579;&#30340;&#20154;&#31867;&#21270;&#36523;&#65292;&#20934;&#30830;&#25429;&#25417;&#20154;&#33080;&#21453;&#23556;&#21644;&#24418;&#29366;&#65292;&#21516;&#26102;&#23454;&#29616;&#36924;&#30495;&#30340;&#38754;&#37096;&#30528;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce FitMe, a facial reflectance model and a differentiable rendering optimization pipeline, that can be used to acquire high-fidelity renderable human avatars from single or multiple images. The model consists of a multi-modal style-based generator, that captures facial appearance in terms of diffuse and specular reflectance, and a PCA-based shape model. We employ a fast differentiable rendering process that can be used in an optimization pipeline, while also achieving photorealistic facial shading. Our optimization process accurately captures both the facial reflectance and shape in high-detail, by exploiting the expressivity of the style-based latent representation and of our shape model. FitMe achieves state-of-the-art reflectance acquisition and identity preservation on single "in-the-wild" facial images, while it produces impressive scan-like results, when given multiple unconstrained facial images pertaining to the same identity. In contrast with recent im
&lt;/p&gt;</description></item><item><title>SoundStorm &#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#33258;&#22238;&#24402;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#29983;&#25104;&#38899;&#36136;&#30456;&#21516;&#20294;&#26356;&#21152;&#19968;&#33268;&#30340;&#38899;&#39057;&#65292;&#24182;&#19988;&#33021;&#22815;&#25193;&#23637;&#24207;&#21015;&#38271;&#24230;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#23545;&#35805;&#29255;&#27573;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.09636</link><description>&lt;p&gt;
SoundStorm&#65306;&#39640;&#25928;&#24182;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SoundStorm: Efficient Parallel Audio Generation. (arXiv:2305.09636v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09636
&lt;/p&gt;
&lt;p&gt;
SoundStorm &#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#33258;&#22238;&#24402;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#29983;&#25104;&#38899;&#36136;&#30456;&#21516;&#20294;&#26356;&#21152;&#19968;&#33268;&#30340;&#38899;&#39057;&#65292;&#24182;&#19988;&#33021;&#22815;&#25193;&#23637;&#24207;&#21015;&#38271;&#24230;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#23545;&#35805;&#29255;&#27573;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; SoundStorm&#65292;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#30340;&#38750;&#33258;&#22238;&#24402;&#38899;&#39057;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;SoundStorm &#25509;&#25910; AudioLM &#30340;&#35821;&#20041;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20381;&#38752;&#21452;&#21521;&#27880;&#24847;&#21147;&#21644;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#24182;&#34892;&#35299;&#30721;&#26469;&#29983;&#25104;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#30340;&#26631;&#35760;&#12290;&#19982; AudioLM &#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38899;&#33394;&#21644;&#22768;&#23398;&#26465;&#20214;&#19979;&#20135;&#29983;&#30456;&#21516;&#36136;&#37327;&#19988;&#26356;&#39640;&#19968;&#33268;&#24615;&#30340;&#38899;&#39057;&#65292;&#36895;&#24230;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;SoundStorm &#22312; TPU-v4 &#19978;&#33021;&#22312;0.5&#31186;&#20869;&#29983;&#25104;30&#31186;&#30340;&#38899;&#39057;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#24102;&#26377;&#35828;&#35805;&#32773;&#36716;&#25442;&#27880;&#37322;&#21644;&#30701;&#25552;&#31034;&#30340;&#39640;&#36136;&#37327;&#33258;&#28982;&#23545;&#35805;&#29255;&#27573;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#25193;&#22823;&#38899;&#39057;&#29983;&#25104;&#30340;&#24207;&#21015;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#34928;&#20943;&#23616;&#37096;SGD&#27493;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#32456;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#35757;&#32451;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.09628</link><description>&lt;p&gt;
&#20855;&#26377;&#34928;&#20943;&#23616;&#37096;SGD&#27493;&#25968;&#30340;&#24555;&#36895;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Faster Federated Learning with Decaying Number of Local SGD Steps. (arXiv:2305.09628v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#34928;&#20943;&#23616;&#37096;SGD&#27493;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#32456;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#35757;&#32451;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36830;&#25509;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#23458;&#25143;&#31471;&#35774;&#22791;&#21487;&#20197;&#22312;&#19981;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#25110;&#20854;&#20182;&#23458;&#25143;&#31471;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;FedAvg&#31639;&#27861;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#23616;&#37096;&#35757;&#32451;&#24182;&#36827;&#34892;&#27169;&#22411;&#24179;&#22343;&#26469;&#35757;&#32451;&#21333;&#20010;&#20840;&#29699;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#30528;&#35757;&#32451;&#30340;&#36827;&#34892;&#32780;&#34928;&#20943;&#30340;K&#20540;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;FedAvg&#22312;&#27492;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#21516;&#26102;&#65292;&#22312;&#21508;&#31181;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Federated Learning (FL) client devices connected over the internet collaboratively train a machine learning model without sharing their private data with a central server or with other clients. The seminal Federated Averaging (FedAvg) algorithm trains a single global model by performing rounds of local training on clients followed by model averaging. FedAvg can improve the communication-efficiency of training by performing more steps of Stochastic Gradient Descent (SGD) on clients in each round. However, client data in real-world FL is highly heterogeneous, which has been extensively shown to slow model convergence and harm final performance when $K &gt; 1$ steps of SGD are performed on clients per round. In this work we propose decaying $K$ as training progresses, which can jointly improve the final performance of the FL model whilst reducing the wall-clock time and the total computational cost of training compared to using a fixed $K$. We analyse the convergence of FedAvg with decayi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#26377;&#30417;&#30563;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#20934;&#30830;&#30340;&#12289;&#31867;&#20284;&#20110;&#27169;&#25311;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#29289;&#29702;&#31995;&#32479;&#27169;&#25311;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.09627</link><description>&lt;p&gt;
&#36816;&#29992;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#29289;&#29702;&#31995;&#32479;&#27169;&#25311;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Addressing computational challenges in physical system simulations with machine learning. (arXiv:2305.09627v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#26377;&#30417;&#30563;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#20934;&#30830;&#30340;&#12289;&#31867;&#20284;&#20110;&#27169;&#25311;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#29289;&#29702;&#31995;&#32479;&#27169;&#25311;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#21033;&#29992;&#27169;&#25311;&#30740;&#31350;&#21508;&#31181;&#29289;&#29702;&#31995;&#32479;&#25110;&#36807;&#31243;&#30340;&#30740;&#31350;&#20154;&#21592;&#12290;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#30001;&#27492;&#23548;&#33268;&#30340;&#26377;&#38480;&#25968;&#25454;&#32463;&#24120;&#20250;&#23545;&#33719;&#24471;&#36825;&#20123;&#31995;&#32479;&#25110;&#36807;&#31243;&#30340;&#28145;&#20837;&#35265;&#35299;&#20135;&#29983;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26377;&#38480;&#30340;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#26377;&#30417;&#30563;&#30340;&#39044;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;&#27169;&#25311;&#32467;&#26524;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#12289;&#31867;&#20284;&#20110;&#27169;&#25311;&#30340;&#25968;&#25454;&#12290;&#20511;&#21161;&#36825;&#20010;&#26694;&#26550;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#25968;&#25454;&#24182;&#30693;&#36947;&#32467;&#26524;&#65292;&#32780;&#19981;&#24517;&#36816;&#34892;&#39640;&#35745;&#31639;&#20195;&#20215;&#30340;&#27169;&#25311;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#21442;&#25968;&#31354;&#38388;&#24182;&#28145;&#20837;&#27934;&#23519;&#29289;&#29702;&#31995;&#32479;&#25110;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#23637;&#31034;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#19968;&#20010;&#26159;&#22320;&#38663;&#39044;&#27979;&#65292;&#19968;&#20010;&#26159;&#22885;&#25176;&#24490;&#29615;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a machine learning-based data generator framework tailored to aid researchers who utilize simulations to examine various physical systems or processes. High computational costs and the resulting limited data often pose significant challenges to gaining insights into these systems or processes. Our approach involves a two-step process: initially, we train a supervised predictive model using a limited simulated dataset to predict simulation outcomes. Subsequently, a reinforcement learning agent is trained to generate accurate, simulation-like data by leveraging the supervised model. With this framework, researchers can generate more accurate data and know the outcomes without running high computational simulations, which enables them to explore the parameter space more efficiently and gain deeper insights into physical systems or processes. We demonstrate the effectiveness of the proposed framework by applying it to two case studies, one focusing on earthquake r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#20998;&#38454;&#27573;&#21457;&#24067;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#25511;&#21046;&#39118;&#38505;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#21551;&#21160;&#36895;&#24230;&#65292;&#36890;&#36807;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#20998;&#25209;&#36172;&#21338;&#26426;&#38382;&#39064;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09626</link><description>&lt;p&gt;
&#24179;&#34913;&#39118;&#38505;&#19982;&#25910;&#30410;&#65306;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#20998;&#38454;&#27573;&#21457;&#24067;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Balancing Risk and Reward: An Automated Phased Release Strategy. (arXiv:2305.09626v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#20998;&#38454;&#27573;&#21457;&#24067;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#25511;&#21046;&#39118;&#38505;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#21551;&#21160;&#36895;&#24230;&#65292;&#36890;&#36807;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#20998;&#25209;&#36172;&#21338;&#26426;&#38382;&#39064;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#38454;&#27573;&#21457;&#24067;&#26159;&#31185;&#25216;&#34892;&#19994;&#20013;&#36880;&#27493;&#21457;&#24067;&#26032;&#20135;&#21697;&#25110;&#26356;&#26032;&#30340;&#24120;&#35265;&#31574;&#30053;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;A/B&#27979;&#35797;&#65292;&#36880;&#27493;&#22686;&#21152;&#22788;&#29702;&#21333;&#20803;&#30340;&#25968;&#37327;&#65292;&#30452;&#21040;&#23436;&#20840;&#37096;&#32626;&#25110;&#24223;&#24323;&#12290;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#25191;&#34892;&#20998;&#38454;&#27573;&#21457;&#24067;&#38656;&#35201;&#20197;&#24179;&#34913;&#19981;&#33391;&#24433;&#21709;&#30340;&#39118;&#38505;&#21644;&#36845;&#20195;&#21644;&#24555;&#36895;&#23398;&#20064;&#30340;&#38656;&#27714;&#26469;&#36873;&#25321;&#20998;&#37197;&#32473;&#26032;&#21457;&#24067;&#30340;&#21333;&#20301;&#27604;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#22320;&#38416;&#36848;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#35843;&#24230;&#30340;&#27599;&#20010;&#38454;&#27573;&#33258;&#21160;&#30830;&#23450;&#21457;&#24067;&#30334;&#20998;&#27604;&#65292;&#24179;&#34913;&#25511;&#21046;&#39118;&#38505;&#21644;&#26368;&#22823;&#21270;&#21551;&#21160;&#36895;&#24230;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#36825;&#19968;&#25361;&#25112;&#24314;&#27169;&#20026;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#20998;&#25209;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#25105;&#20204;&#39044;&#20808;&#25351;&#23450;&#30340;&#23454;&#39564;&#39044;&#31639;&#19981;&#20250;&#34987;&#39640;&#27010;&#29575;&#32791;&#23613;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#33258;&#36866;&#24212;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20854;&#20013;&#23558;&#20998;&#37197;&#32473;&#22788;&#29702;&#30340;&#26368;&#22823;&#21333;&#20803;&#25968;&#30001;
&lt;/p&gt;
&lt;p&gt;
Phased releases are a common strategy in the technology industry for gradually releasing new products or updates through a sequence of A/B tests in which the number of treated units gradually grows until full deployment or deprecation. Performing phased releases in a principled way requires selecting the proportion of units assigned to the new release in a way that balances the risk of an adverse effect with the need to iterate and learn from the experiment rapidly. In this paper, we formalize this problem and propose an algorithm that automatically determines the release percentage at each stage in the schedule, balancing the need to control risk while maximizing ramp-up speed. Our framework models the challenge as a constrained batched bandit problem that ensures that our pre-specified experimental budget is not depleted with high probability. Our proposed algorithm leverages an adaptive Bayesian approach in which the maximal number of units assigned to the treatment is determined by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#35782;&#21035;&#30340;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#21442;&#25968;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#20302;&#32500;&#29305;&#24449;&#21644;&#23398;&#20064;&#21442;&#25968;&#21040;&#28508;&#21464;&#37327;&#30340;&#26144;&#23556;&#26469;&#32531;&#35299;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09625</link><description>&lt;p&gt;
&#24102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#35782;&#21035;&#30340;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#21442;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditional variational autoencoder with Gaussian process regression recognition for parametric models. (arXiv:2305.09625v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#35782;&#21035;&#30340;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#21442;&#25968;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#20302;&#32500;&#29305;&#24449;&#21644;&#23398;&#20064;&#21442;&#25968;&#21040;&#28508;&#21464;&#37327;&#30340;&#26144;&#23556;&#26469;&#32531;&#35299;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26377;&#22122;&#22768;&#35266;&#23519;&#25968;&#25454;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#22522;&#20110;&#38477;&#38454;&#24314;&#27169;&#65288;GPR-based ROM&#65289;&#21487;&#20197;&#23454;&#29616;&#24555;&#36895;&#30340;&#22312;&#32447;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#31163;&#32447;&#38454;&#27573;&#20351;&#29992;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;GPR-based ROM&#23545;&#20110;&#22797;&#26434;&#31995;&#32479;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;POD&#25237;&#24433;&#22825;&#28982;&#26159;&#32447;&#24615;&#30340;&#12290;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#21487;&#20197;&#36890;&#36807;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20855;&#26377;&#26356;&#39640;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#36825;&#32473;&#35757;&#32451;&#21644;&#35843;&#25972;&#36229;&#21442;&#25968;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#35782;&#21035;&#30340;CVAE&#26694;&#26550;&#65288;CVAE-GPRR&#65289;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#35782;&#21035;&#27169;&#22411;&#21644;&#21487;&#33021;&#24615;&#27169;&#22411;&#12290;&#22312;&#35782;&#21035;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;POD&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20302;&#32500;&#29305;&#24449;&#26469;&#36807;&#28388;&#39640;&#39057;&#20887;&#20313;&#20449;&#24687;&#12290;&#28982;&#21518;&#20351;&#29992;&#38750;&#21442;&#25968;&#27169;&#22411;GPR&#26469;&#23398;&#20064;&#20174;&#21442;&#25968;&#21040;POD&#28508;&#21464;&#37327;&#30340;&#26144;&#23556;&#65292;&#36825;&#20063;&#21487;&#20197;&#32531;&#35299;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present a data-driven method for parametric models with noisy observation data. Gaussian process regression based reduced order modeling (GPR-based ROM) can realize fast online predictions without using equations in the offline stage. However, GPR-based ROM does not perform well for complex systems since POD projection are naturally linear. Conditional variational autoencoder (CVAE) can address this issue via nonlinear neural networks but it has more model complexity, which poses challenges for training and tuning hyperparameters. To this end, we propose a framework of CVAE with Gaussian process regression recognition (CVAE-GPRR). The proposed method consists of a recognition model and a likelihood model. In the recognition model, we first extract low-dimensional features from data by POD to filter the redundant information with high frequency. And then a non-parametric model GPR is used to learn the map from parameters to POD latent variables, which can also allevi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.09620</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#35843;&#26597;&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#35266;&#28857;&#39044;&#27979;&#20013;&#65292;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#65292;&#22238;&#28335;&#25512;&#29702;&#21644;&#38646;&#27425;&#39044;&#27979;&#19977;&#20010;&#19981;&#21516;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#23558;&#35843;&#26597;&#38382;&#39064;&#12289;&#20010;&#20154;&#20449;&#24565;&#21644;&#26102;&#38388;&#32972;&#26223;&#30340;&#31070;&#32463;&#23884;&#20837;&#24341;&#20837;&#21040;&#35266;&#28857;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;LLMs&#20013;&#12290;&#22312;1972&#24180;&#21040;2021&#24180;&#30340;&#8220;&#24120;&#35268;&#31038;&#20250;&#35843;&#26597;&#8221;&#20013;&#65292;&#25105;&#20204;&#20174;68,846&#21517;&#32654;&#22269;&#20154;&#20013;&#33719;&#24471;&#20102;3,110&#20010;&#20108;&#36827;&#21046;&#35266;&#28857;&#65292;&#22312;Alpaca-7b&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#26524;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#25554;&#20540;&#65288;AUC=0.87&#65292;&#20844;&#24320;&#35266;&#28857;&#39044;&#27979;&#20026;$\rho$=0.99&#65289;&#21644;&#22238;&#28335;&#25512;&#29702;&#65288;AUC=0.86&#65292;$\rho$=0.98&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26174;&#33879;&#30340;&#39044;&#27979;&#33021;&#21147;&#33021;&#22815;&#20197;&#39640;&#32622;&#20449;&#24230;&#22635;&#34917;&#32570;&#22833;&#30340;&#36235;&#21183;&#65292;&#24182;&#26631;&#26126;&#20844;&#20247;&#24577;&#24230;&#20309;&#26102;&#21457;&#29983;&#21464;&#21270;&#65292;&#22914;&#21516;&#24615;&#23130;&#23035;&#30340;&#33719;&#21462;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#21463;&#21040;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#38750;&#32447;&#24615;&#31995;&#32479;&#21160;&#24577;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20272;&#35745;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#21644;&#25191;&#34892;&#31867;&#20284;&#20110;$\mathtt{iLQR}$&#30340;&#31574;&#30053;&#26356;&#26032;&#20043;&#38388;&#30340;&#36845;&#20195;&#26469;&#23454;&#29616;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#24182;&#20811;&#26381;&#20102;&#25351;&#25968;&#21306;&#38388;&#19978;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09619</link><description>&lt;p&gt;
&#23398;&#20064;&#30340;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#22312;&#38750;&#32447;&#24615;&#31574;&#30053;&#20248;&#21270;&#20013;&#30340;&#23041;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Power of Learned Locally Linear Models for Nonlinear Policy Optimization. (arXiv:2305.09619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#38750;&#32447;&#24615;&#31995;&#32479;&#21160;&#24577;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20272;&#35745;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#21644;&#25191;&#34892;&#31867;&#20284;&#20110;$\mathtt{iLQR}$&#30340;&#31574;&#30053;&#26356;&#26032;&#20043;&#38388;&#30340;&#36845;&#20195;&#26469;&#23454;&#29616;&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#24182;&#20811;&#26381;&#20102;&#25351;&#25968;&#21306;&#38388;&#19978;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#20013;&#65292;&#24120;&#35265;&#30340;&#27969;&#31243;&#26159;&#36880;&#27493;&#20272;&#35745;&#31995;&#32479;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65288;&#20363;&#22914;$\mathtt{iLQR}$&#65289;&#22312;&#23398;&#20064;&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#26368;&#23567;&#21270;&#30446;&#26631;&#25104;&#26412;&#12290;&#26412;&#25991;&#23545;&#19968;&#31181;&#31616;&#21270;&#29256;&#30340;&#27492;&#31574;&#30053;&#24212;&#29992;&#20110;&#19968;&#33324;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20272;&#35745;&#38750;&#32447;&#24615;&#31995;&#32479;&#21160;&#24577;&#30340;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#21644;&#25191;&#34892;&#31867;&#20284;&#20110;$\mathtt{iLQR}$&#30340;&#31574;&#30053;&#26356;&#26032;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#12290;&#25105;&#20204;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#30456;&#20851;&#38382;&#39064;&#21442;&#25968;&#20013;&#36798;&#21040;&#20102;&#22810;&#39033;&#24335;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#23616;&#37096;&#31283;&#23450;&#22686;&#30410;&#65292;&#20811;&#26381;&#20102;&#22312;&#38382;&#39064;&#21306;&#38388;&#19978;&#30340;&#25351;&#25968;&#20381;&#36182;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#33258;&#28982;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common pipeline in learning-based control is to iteratively estimate a model of system dynamics, and apply a trajectory optimization algorithm e.g.~$\mathtt{iLQR}$ - on the learned model to minimize a target cost. This paper conducts a rigorous analysis of a simplified variant of this strategy for general nonlinear systems. We analyze an algorithm which iterates between estimating local linear models of nonlinear system dynamics and performing $\mathtt{iLQR}$-like policy updates. We demonstrate that this algorithm attains sample complexity polynomial in relevant problem parameters, and, by synthesizing locally stabilizing gains, overcomes exponential dependence in problem horizon. Experimental results validate the performance of our algorithm, and compare to natural deep-learning baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Med-PaLM2&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#30784;LLM&#25913;&#36827;&#12289;&#21307;&#23398;&#39046;&#22495;&#24494;&#35843;&#21644;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#29992;&#26032;&#39062;&#30340;&#38598;&#25104;&#31934;&#28860;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;MedQA&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;86.5%&#30340;&#21307;&#23398;&#38382;&#31572;&#20934;&#30830;&#29575;&#65292;&#36808;&#21521;&#21307;&#23398;&#19987;&#23478;&#32423;&#21035;&#30340;&#38382;&#31572;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09617</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;&#65306;&#36808;&#21521;&#21307;&#23398;&#19987;&#23478;&#32423;&#21035;&#30340;&#38382;&#31572;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Expert-Level Medical Question Answering with Large Language Models. (arXiv:2305.09617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Med-PaLM2&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#30784;LLM&#25913;&#36827;&#12289;&#21307;&#23398;&#39046;&#22495;&#24494;&#35843;&#21644;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#29992;&#26032;&#39062;&#30340;&#38598;&#25104;&#31934;&#28860;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;MedQA&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;86.5%&#30340;&#21307;&#23398;&#38382;&#31572;&#20934;&#30830;&#29575;&#65292;&#36808;&#21521;&#21307;&#23398;&#19987;&#23478;&#32423;&#21035;&#30340;&#38382;&#31572;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#35832;&#22914;&#22260;&#26827;&#21644;&#34507;&#30333;&#36136;&#25240;&#21472;&#31561;&#8220;&#23439;&#20255;&#25361;&#25112;&#8221;&#26041;&#38754;&#21462;&#24471;&#20102;&#37324;&#31243;&#30865;&#24335;&#30340;&#36827;&#23637;&#12290;&#20294;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#24182;&#20687;&#21307;&#29983;&#19968;&#26679;&#36827;&#34892;&#25512;&#29702;&#34987;&#35748;&#20026;&#20063;&#26159;&#19968;&#31181;&#23439;&#20255;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65307;Med-PaLM&#26159;&#31532;&#19968;&#20010;&#22312;MedQA&#25968;&#25454;&#38598;&#19978;&#20197;67.2&#65285;&#30340;&#20998;&#25968;&#36229;&#36807;&#32654;&#22269;&#21307;&#30103;&#25191;&#19994;&#32771;&#35797;&#65288;USMLE&#65289;&#26679;&#24335;&#38382;&#39064;&#30340;&#8220;&#21450;&#26684;&#8221;&#20998;&#25968;&#30340;&#27169;&#22411;&#12290; &#28982;&#32780;&#65292;&#23545;&#27604;&#27169;&#22411;&#31572;&#26696;&#21644;&#21307;&#29983;&#31572;&#26696;&#65292;&#36825;&#39033;&#21644;&#20854;&#20182;&#20808;&#21069;&#24037;&#20316;&#34920;&#26126;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Med-PaLM2&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;LLM&#25913;&#36827;&#65288;PaLM2&#65289;&#12289;&#21307;&#23398;&#39046;&#22495;&#24494;&#35843;&#21644;&#25552;&#31034;&#31574;&#30053;&#65288;&#21253;&#25324;&#26032;&#39062;&#30340;&#38598;&#25104;&#31934;&#28860;&#26041;&#27861;&#65289;&#26469;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#12290;&#22312;MedQA&#25968;&#25454;&#38598;&#19978;&#65292;Med-PaLM2&#30340;&#24471;&#20998;&#21487;&#36798;86.5&#65285;&#65292;&#27604;Med-PaLM&#25552;&#39640;&#20102;&#36229;&#36807;11&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent artificial intelligence (AI) systems have reached milestones in "grand challenges" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge.  Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a "passing" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach.  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#24402;&#19968;&#21270;&#27969;&#26694;&#26550;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#24182;&#21457;&#30340;&#22312;&#20998;&#24067;&#20869;&#35823;&#20998;&#31867;&#65288;IDM&#65289;&#21644;&#36234;&#30028;&#26816;&#27979;&#65292;&#21487;&#20197;&#25193;&#23637;&#20808;&#21069;&#37096;&#32626;&#30340;&#20998;&#21106;&#27169;&#22411;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23454;&#29616;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09610</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#24402;&#19968;&#21270;&#27969;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#24182;&#21457;&#35823;&#20998;&#31867;&#21644;&#36234;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Concurrent Misclassification and Out-of-Distribution Detection for Semantic Segmentation via Energy-Based Normalizing Flow. (arXiv:2305.09610v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09610
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#24402;&#19968;&#21270;&#27969;&#26694;&#26550;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#24182;&#21457;&#30340;&#22312;&#20998;&#24067;&#20869;&#35823;&#20998;&#31867;&#65288;IDM&#65289;&#21644;&#36234;&#30028;&#26816;&#27979;&#65292;&#21487;&#20197;&#25193;&#23637;&#20808;&#21069;&#37096;&#32626;&#30340;&#20998;&#21106;&#27169;&#22411;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23454;&#29616;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#23545;&#20110;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#20998;&#24067;&#30456;&#20284;&#30340;&#27979;&#35797;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#30340;&#21028;&#21035;&#24335;&#38381;&#21512;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#35774;&#32622;&#20013;&#23545;&#20110;&#20998;&#24067;&#20559;&#31227;&#21644;&#36234;&#30028;&#65288;OOD&#65289;&#31867;&#19981;&#22815;&#31283;&#20581;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26102;&#38388;&#20351;&#29992;&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#32622;&#20449;&#24230;&#20998;&#25968;&#26102;&#65292;&#39044;&#27979;&#30340;&#27010;&#29575;&#21487;&#33021;&#38750;&#24120;&#19981;&#31934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#24120;&#21270;&#27969;&#26694;&#26550;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#24182;&#21457;&#30340;&#22312;&#20998;&#24067;&#20869;&#35823;&#20998;&#31867;&#65288;IDM&#65289;&#21644;&#36234;&#30028;&#26816;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#27969;&#30340;&#24102;&#26377;&#33021;&#37327;&#36755;&#20837;&#30340;&#26816;&#27979;&#22120;&#65288;FlowEneDet&#65289;&#21487;&#20197;&#25193;&#23637;&#20808;&#21069;&#37096;&#32626;&#30340;&#20998;&#21106;&#27169;&#22411;&#32780;&#26080;&#38656;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;FlowEneDet&#22312;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#20855;&#26377;&#26497;&#23567;&#30340;&#22686;&#21152;&#65292;&#22240;&#27492;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#26550;&#26500;&#12290;&#22312;Cityscapes&#12289;Cityscapes-C&#12289;FishyScapes&#21644; SegmentMeIfYouCan&#35780;&#20272;&#20013;&#65292;FlowEneDet&#22312;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;DeepLabV3+&#21644;Translated title:&#22522;&#20110;&#33021;&#37327;&#24402;&#19968;&#21270;&#27969;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#24182;&#21457;&#35823;&#20998;&#31867;&#21644;&#36234;&#30028;&#26816;&#27979;&#30340;&#27979;&#35797;&#20013;&#23454;&#29616;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent semantic segmentation models accurately classify test-time examples that are similar to a training dataset distribution. However, their discriminative closed-set approach is not robust in practical data setups with distributional shifts and out-of-distribution (OOD) classes. As a result, the predicted probabilities can be very imprecise when used as confidence scores at test time. To address this, we propose a generative model for concurrent in-distribution misclassification (IDM) and OOD detection that relies on a normalizing flow framework. The proposed flow-based detector with an energy-based inputs (FlowEneDet) can extend previously deployed segmentation models without their time-consuming retraining. Our FlowEneDet results in a low-complexity architecture with marginal increase in the memory footprint. FlowEneDet achieves promising results on Cityscapes, Cityscapes-C, FishyScapes and SegmentMeIfYouCan benchmarks in IDM/OOD detection when applied to pretrained DeepLabV3+ and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#22686;&#24378;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#30340;&#20914;&#31361;&#21644;&#37325;&#22797;&#26816;&#27979;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#20840;&#38754;&#23454;&#35777;&#20998;&#26512;&#20845;&#20010;&#36719;&#20214;&#25991;&#26412;&#25968;&#25454;&#38598;&#26469;&#35782;&#21035;&#21477;&#23545;&#20914;&#31361;&#21644;&#37325;&#22797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#23545;&#36719;&#20214;&#21477;&#23545;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#37117;&#26377;&#26174;&#30528;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09608</link><description>&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#21477;&#23545;&#20013;&#30340;&#20914;&#31361;&#21644;&#37325;&#22797;&#26816;&#27979;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Conflict and Duplicate Detection in Software Engineering Sentence Pairs. (arXiv:2305.09608v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#22686;&#24378;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#30340;&#20914;&#31361;&#21644;&#37325;&#22797;&#26816;&#27979;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#20840;&#38754;&#23454;&#35777;&#20998;&#26512;&#20845;&#20010;&#36719;&#20214;&#25991;&#26412;&#25968;&#25454;&#38598;&#26469;&#35782;&#21035;&#21477;&#23545;&#20914;&#31361;&#21644;&#37325;&#22797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#23545;&#36719;&#20214;&#21477;&#23545;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#37117;&#26377;&#26174;&#30528;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#22686;&#24378;&#36890;&#36807;&#21477;&#23545;&#20998;&#31867;&#36827;&#34892;&#30340;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#30340;&#20914;&#31361;&#21644;&#37325;&#22797;&#26816;&#27979;&#12290;&#35813;&#30740;&#31350;&#37319;&#29992;&#20102;&#24120;&#35265;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#22914;&#25171;&#20081;&#39034;&#24207;&#65292;&#22238;&#35793;&#21644;&#37322;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22914;&#21517;&#35789;-&#21160;&#35789;&#26367;&#25442;&#65292;&#30446;&#26631;-&#24341;&#29702;&#26367;&#25442;&#21644;&#28436;&#21592;-&#21160;&#20316;&#26367;&#25442;&#29992;&#20110;&#36719;&#20214;&#38656;&#27714;&#25991;&#26412;&#12290;&#36890;&#36807;&#23545;&#20845;&#20010;&#36719;&#20214;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#21477;&#23545;&#20043;&#38388;&#30340;&#20914;&#31361;&#21644;&#37325;&#22797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#23545;&#25152;&#26377;&#36719;&#20214;&#21477;&#23545;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#37117;&#26377;&#26174;&#30528;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#25968;&#25454;&#38598;&#30456;&#23545;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22686;&#24378;&#25216;&#26415;&#21487;&#33021;&#20250;&#23545;&#20998;&#31867;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of text data augmentation techniques to enhance conflict and duplicate detection in software engineering tasks through sentence pair classification. The study adapts generic augmentation techniques such as shuffling, back translation, and paraphrasing and proposes new data augmentation techniques such as Noun-Verb Substitution, target-lemma replacement and Actor-Action Substitution for software requirement texts. A comprehensive empirical analysis is conducted on six software text datasets to identify conflicts and duplicates among sentence pairs. The results demonstrate that data augmentation techniques have a significant impact on the performance of all software pair text datasets. On the other hand, in cases where the datasets are relatively balanced, the use of augmentation techniques may result in a negative effect on the classification performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#26041;&#38754;&#36827;&#34892;&#20102;&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#23558;&#24050;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#32467;&#26524;&#25193;&#23637;&#21040;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09605</link><description>&lt;p&gt;
&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Expressiveness Remarks for Denoising Diffusion Models and Samplers. (arXiv:2305.09605v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#26041;&#38754;&#36827;&#34892;&#20102;&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#23558;&#24050;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#32467;&#26524;&#25193;&#23637;&#21040;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#26368;&#36817;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#28459;&#25193;&#36807;&#31243;&#36880;&#28176;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#22122;&#22768;&#65292;&#23558;&#25968;&#25454;&#20998;&#24067;&#36716;&#21270;&#20026;&#39640;&#26031;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#27169;&#25311;&#35813;&#28459;&#25193;&#30340;&#26102;&#38388;&#21453;&#28436;&#30340;&#36924;&#36817;&#26469;&#33719;&#21462;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21018;&#24320;&#22987;&#36825;&#20010;&#28459;&#25193;&#27169;&#25311;&#30340;&#21021;&#22987;&#20540;&#26159;&#39640;&#26031;&#26679;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#28459;&#25193;&#27169;&#22411;&#36866;&#24212;&#20110;&#37319;&#26679;&#21644;&#25512;&#26029;&#20219;&#21153;&#12290;&#26412;&#25991;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#19982;F\"ollmer&#28418;&#31227;&#31867;&#20284;&#30340;&#38543;&#26426;&#25511;&#21046;&#32852;&#31995;&#65292;&#23558;&#38024;&#23545;F\"ollmer&#28418;&#31227;&#30340;&#24050;&#30693;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#32467;&#26524;&#25193;&#23637;&#21040;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models are a class of generative models which have recently achieved state-of-the-art results across many domains. Gradual noise is added to the data using a diffusion process, which transforms the data distribution into a Gaussian. Samples from the generative model are then obtained by simulating an approximation of the time reversal of this diffusion initialized by Gaussian samples. Recent research has explored adapting diffusion models for sampling and inference tasks. In this paper, we leverage known connections to stochastic control akin to the F\"ollmer drift to extend established neural network approximation results for the F\"ollmer drift to denoising diffusion models and samplers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#32321;&#24537;&#22810;&#36710;&#36947;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#25317;&#22581;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#36866;&#24212;&#24615;&#32469;&#34892;&#31574;&#30053;&#65292;&#22312;&#20943;&#23569;&#25317;&#22581;&#21644;&#25552;&#39640;&#36710;&#36895;&#30340;&#21516;&#26102;&#65292;&#20248;&#21270;&#39640;&#36895;&#20844;&#36335;&#36710;&#36947;&#21644;&#21608;&#36793;&#23616;&#37096;&#25910;&#21457;&#36335;&#32593;&#30340;&#20351;&#29992;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#26174;&#33879;&#20248;&#21270;&#36947;&#36335;&#25910;&#21457;&#25928;&#29575;&#24182;&#20943;&#23569;&#24635;&#36890;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.09600</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26368;&#22823;&#21270;&#32321;&#24537;&#36335;&#27573;&#36890;&#34892;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning to Maximize Arterial Usage during Extreme Congestion. (arXiv:2305.09600v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#32321;&#24537;&#22810;&#36710;&#36947;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#25317;&#22581;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#36866;&#24212;&#24615;&#32469;&#34892;&#31574;&#30053;&#65292;&#22312;&#20943;&#23569;&#25317;&#22581;&#21644;&#25552;&#39640;&#36710;&#36895;&#30340;&#21516;&#26102;&#65292;&#20248;&#21270;&#39640;&#36895;&#20844;&#36335;&#36710;&#36947;&#21644;&#21608;&#36793;&#23616;&#37096;&#25910;&#21457;&#36335;&#32593;&#30340;&#20351;&#29992;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#26174;&#33879;&#20248;&#21270;&#36947;&#36335;&#25910;&#21457;&#25928;&#29575;&#24182;&#20943;&#23569;&#24635;&#36890;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36947;&#36335;&#32593;&#32476;&#20013;&#65292;&#20132;&#36890;&#20107;&#25925;&#21644;&#20854;&#20182;&#20107;&#20214;&#65292;&#22914;&#26524;&#19981;&#21152;&#20197;&#32531;&#35299;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#32423;&#32852;&#25925;&#38556;&#65292;&#24433;&#21709;&#31995;&#32479;&#30340;&#22823;&#37096;&#20998;&#21151;&#33021;&#12290;&#21450;&#26102;&#22788;&#29702;&#36825;&#31181;&#26497;&#31471;&#25317;&#22581;&#24773;&#20917;&#26159;&#38477;&#20302;&#25490;&#25918;&#37327;&#12289;&#25552;&#39640;&#29983;&#20135;&#29575;&#21644;&#25913;&#21892;&#22478;&#24066;&#29983;&#27963;&#36136;&#37327;&#30340;&#24517;&#35201;&#25163;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#32321;&#24537;&#30340;&#22810;&#36710;&#36947;&#39640;&#36895;&#20844;&#36335;&#19978;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12290;&#35813;&#26234;&#33021;&#20307;&#34987;&#35757;&#32451;&#23398;&#20064;&#36866;&#24212;&#24615;&#32469;&#34892;&#31574;&#30053;&#65292;&#20197;&#20415;&#22312;&#25317;&#22581;&#30340;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#20013;&#26368;&#20248;&#22320;&#21033;&#29992;&#39640;&#36895;&#20844;&#36335;&#36710;&#36947;&#21644;&#21608;&#36793;&#23616;&#37096;&#25910;&#21457;&#36335;&#32593;&#65292;&#22870;&#21169;&#26159;&#20943;&#23569;&#25317;&#22581;&#21450;&#25552;&#39640;&#36710;&#36895;&#12290;&#23454;&#39564;&#35774;&#32622;&#22312;&#32654;&#22269;&#21326;&#30427;&#39039;&#24030;Shoreline&#24066;&#30340;&#19968;&#27573;&#38271;2.6&#33521;&#37324;&#12289;4&#26465;&#36710;&#36947;&#30340;&#39640;&#36895;&#20844;&#36335;&#20013;&#36827;&#34892;&#65292;&#22312;&#24494;&#35266;&#21644;&#36830;&#32493;&#30340;&#32508;&#21512;&#20132;&#36890;&#27169;&#25311;&#22120;SUMO&#65288;&#22478;&#24066;&#31227;&#21160;&#20223;&#30495;&#65289;&#20013;&#27169;&#25311;&#20102;&#20004;&#20010;&#20986;&#21475;&#21644;&#30456;&#20851;&#30340;&#25910;&#21457;&#36335;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#21442;&#25968;&#21270;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26497;&#31471;&#25317;&#22581;&#26102;&#30340;&#36947;&#36335;&#25910;&#21457;&#25928;&#29575;&#24182;&#20943;&#23569;&#24635;&#36890;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collisions, crashes, and other incidents on road networks, if left unmitigated, can potentially cause cascading failures that can affect large parts of the system. Timely handling such extreme congestion scenarios is imperative to reduce emissions, enhance productivity, and improve the quality of urban living. In this work, we propose a Deep Reinforcement Learning (DRL) approach to reduce traffic congestion on multi-lane freeways during extreme congestion. The agent is trained to learn adaptive detouring strategies for congested freeway traffic such that the freeway lanes along with the local arterial network in proximity are utilized optimally, with rewards being congestion reduction and traffic speed improvement. The experimental setup is a 2.6-mile-long 4-lane freeway stretch in Shoreline, Washington, USA with two exits and associated arterial roads simulated on a microscopic and continuous multi-modal traffic simulator SUMO (Simulation of Urban MObility) while using parameterized t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23545;Kepler&#22826;&#31354;&#26395;&#36828;&#38236;&#21644;&#25193;&#23637;&#20219;&#21153;K2&#30340;&#25968;&#25454;&#36827;&#34892;&#31995;&#22806;&#34892;&#26143;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#20351;&#29992;Siamese&#26550;&#26500;&#21487;&#26377;&#25928;&#35299;&#20915;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.09596</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35782;&#21035;&#21644;&#20998;&#31867;&#31995;&#22806;&#34892;&#26143;
&lt;/p&gt;
&lt;p&gt;
Identification and Classification of Exoplanets Using Machine Learning Techniques. (arXiv:2305.09596v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23545;Kepler&#22826;&#31354;&#26395;&#36828;&#38236;&#21644;&#25193;&#23637;&#20219;&#21153;K2&#30340;&#25968;&#25454;&#36827;&#34892;&#31995;&#22806;&#34892;&#26143;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#20351;&#29992;Siamese&#26550;&#26500;&#21487;&#26377;&#25928;&#35299;&#20915;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#23431;&#33322;&#23616;&#30340;Kepler&#22826;&#31354;&#26395;&#36828;&#38236;&#22312;&#21457;&#29616;&#31995;&#22806;&#34892;&#26143;&#30340;&#20219;&#21153;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36890;&#36807;&#23545;Kepler&#26395;&#36828;&#38236;&#25509;&#25910;&#21040;&#30340;&#20449;&#21495;&#36827;&#34892;&#35745;&#31639;&#25968;&#25454;&#20998;&#26512;&#65292;&#25903;&#25345;&#20102;&#36825;&#39033;&#25628;&#32034;&#24037;&#20316;&#12290;&#26412;&#25991;&#32771;&#34385;&#21033;&#29992;&#27531;&#24046;&#32593;&#32476;&#23545;Kepler&#22826;&#31354;&#26395;&#36828;&#38236;&#21450;&#20854;&#25193;&#23637;&#20219;&#21153;K2&#30340;&#25968;&#25454;&#36827;&#34892;&#31995;&#22806;&#34892;&#26143;&#35782;&#21035;&#26041;&#38754;&#30340;&#29616;&#26377;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#24314;&#35774;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22914;&#20309;&#24110;&#21161;&#22312;&#19968;&#20010;&#20302;&#25968;&#25454;&#24773;&#20917;&#21644;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#31995;&#22806;&#34892;&#26143;&#23384;&#22312;&#30340;&#20998;&#31867;&#12290;&#38500;&#20102;&#26631;&#20934;CNN&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Siamese&#26550;&#26500;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#20998;&#31867;&#12290;CNN&#21644;ResNet&#31639;&#27861;&#23545;&#20110;&#19977;&#31867;&#20998;&#31867;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;68&#65285;&#65292;&#23545;&#20110;&#20004;&#31867;&#20998;&#31867;&#30340;&#20934;&#30830;&#29575;&#20026;86&#65285;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19977;&#31867;&#21644;&#20004;&#31867;&#20998;&#31867;&#65292;Siamese&#31639;&#27861;&#22343;&#36798;&#21040;&#20102;99&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
NASA's Kepler Space Telescope has been instrumental in the task of finding the presence of exoplanets in our galaxy. This search has been supported by computational data analysis to identify exoplanets from the signals received by the Kepler telescope. In this paper, we consider building upon some existing work on exoplanet identification using residual networks for the data of the Kepler space telescope and its extended mission K2. This paper aims to explore how deep learning algorithms can help in classifying the presence of exoplanets with less amount of data in one case and a more extensive variety of data in another. In addition to the standard CNN-based method, we propose a Siamese architecture that is particularly useful in addressing classification in a low-data scenario. The CNN and ResNet algorithms achieved an average accuracy of 68% for three classes and 86% for two-class classification. However, for both the three and two classes, the Siamese algorithm achieved 99% accurac
&lt;/p&gt;</description></item><item><title>HiNoVa&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#23556;&#39057;&#35774;&#22791;&#35748;&#35777;&#30340;&#26032;&#22411;&#24320;&#25918;&#38598;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;&#20013;&#38544;&#34255;&#29366;&#24577;&#20540;&#27169;&#24335;&#65292;&#21487;&#20197;&#25104;&#21151;&#29992;&#20110;&#30417;&#25511;&#21644;&#25511;&#21046;&#26080;&#32447;&#35774;&#22791;&#30340;&#26410;&#32463;&#25480;&#26435;&#30340;&#32593;&#32476;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2305.09594</link><description>&lt;p&gt;
HiNoVa&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#23556;&#39057;&#35774;&#22791;&#35748;&#35777;&#30340;&#26032;&#22411;&#24320;&#25918;&#38598;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HiNoVa: A Novel Open-Set Detection Method for Automating RF Device Authentication. (arXiv:2305.09594v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09594
&lt;/p&gt;
&lt;p&gt;
HiNoVa&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#23556;&#39057;&#35774;&#22791;&#35748;&#35777;&#30340;&#26032;&#22411;&#24320;&#25918;&#38598;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;&#20013;&#38544;&#34255;&#29366;&#24577;&#20540;&#27169;&#24335;&#65292;&#21487;&#20197;&#25104;&#21151;&#29992;&#20110;&#30417;&#25511;&#21644;&#25511;&#21046;&#26080;&#32447;&#35774;&#22791;&#30340;&#26410;&#32463;&#25480;&#26435;&#30340;&#32593;&#32476;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21033;&#29992;&#23556;&#39057;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35748;&#35777;&#35774;&#22791;&#65292;&#20026;&#26080;&#32447;&#32593;&#32476;&#23433;&#20840;&#25552;&#20379;&#20102;&#26032;&#30340;&#21151;&#33021;&#12290;&#24320;&#25918;&#38598;&#26816;&#27979;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#39046;&#22495;&#65292;&#23427;&#21487;&#20197;&#22312;&#37096;&#32626;&#26399;&#38388;&#35782;&#21035;&#26469;&#33258;&#26032;&#35774;&#22791;&#30340;&#26679;&#26412;&#65292;&#32780;&#36825;&#20123;&#35774;&#22791;&#19981;&#22312;&#35757;&#32451;&#38598;&#20013;&#12290;&#36807;&#21435;&#22312;&#24320;&#25918;&#38598;&#26816;&#27979;&#26041;&#38754;&#30340;&#24037;&#20316;&#20027;&#35201;&#24212;&#29992;&#20110;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#22270;&#20687;&#12290;&#30456;&#21453;&#65292;RF&#20449;&#21495;&#25968;&#25454;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#24418;&#25104;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#19988;&#26679;&#26412;&#20043;&#38388;&#23384;&#22312;&#38750;&#32447;&#24615;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;&#20013;&#38544;&#34255;&#29366;&#24577;&#20540;&#27169;&#24335;&#30340;&#26032;&#22411;&#24320;&#25918;&#38598;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#22312;LoRa&#12289;&#26080;&#32447;WiFi&#21644;&#26377;&#32447;WiFi&#25968;&#25454;&#38598;&#19978;&#30340;&#31934;&#24230;-&#21484;&#22238;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65292;&#22240;&#27492;&#65292;&#21487;&#20197;&#25104;&#21151;&#29992;&#20110;&#30417;&#25511;&#21644;&#25511;&#21046;&#26080;&#32447;&#35774;&#22791;&#30340;&#26410;&#32463;&#25480;&#26435;&#30340;&#32593;&#32476;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
New capabilities in wireless network security have been enabled by deep learning, which leverages patterns in radio frequency (RF) data to identify and authenticate devices. Open-set detection is an area of deep learning that identifies samples captured from new devices during deployment that were not part of the training set. Past work in open-set detection has mostly been applied to independent and identically distributed data such as images. In contrast, RF signal data present a unique set of challenges as the data forms a time series with non-linear time dependencies among the samples. We introduce a novel open-set detection approach based on the patterns of the hidden state values within a Convolutional Neural Network (CNN) Long Short-Term Memory (LSTM) model. Our approach greatly improves the Area Under the Precision-Recall Curve on LoRa, Wireless-WiFi, and Wired-WiFi datasets, and hence, can be used successfully to monitor and control unauthorized network access of wireless devi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#22270;&#24402;&#32435;&#31227;&#21160;&#29289;&#20307;&#20998;&#21106;&#31639;&#27861;(GraphIMOS)&#65292;&#21487;&#20197;&#23545;&#26032;&#22686;&#25968;&#25454;&#24103;&#36827;&#34892;&#39044;&#27979;&#65292;&#23558;&#22522;&#20110;&#22270;&#30340;MOS&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#37096;&#32626;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09585</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31227;&#21160;&#29289;&#20307;&#20998;&#21106;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Inductive Graph Neural Networks for Moving Object Segmentation. (arXiv:2305.09585v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#22270;&#24402;&#32435;&#31227;&#21160;&#29289;&#20307;&#20998;&#21106;&#31639;&#27861;(GraphIMOS)&#65292;&#21487;&#20197;&#23545;&#26032;&#22686;&#25968;&#25454;&#24103;&#36827;&#34892;&#39044;&#27979;&#65292;&#23558;&#22522;&#20110;&#22270;&#30340;MOS&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#37096;&#32626;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#29289;&#20307;&#20998;&#21106;(MOS)&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#21160;&#24577;&#32972;&#26223;&#12289;&#31361;&#21464;&#20809;&#29031;&#12289;&#38452;&#24433;&#12289;&#20266;&#35013;&#21644;&#31227;&#21160;&#30456;&#26426;&#30340;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#32467;&#26500;&#30340;&#26032;&#22411;&#22270;&#24402;&#32435;&#31227;&#21160;&#29289;&#20307;&#20998;&#21106;(GraphIMOS)&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#33021;&#22815;&#23545;&#26032;&#22686;&#25968;&#25454;&#24103;&#36827;&#34892;&#39044;&#27979;&#65292;&#20351;&#29992;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#12290;GraphIMOS &#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#24402;&#32435;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#27604;&#20197;&#21069;&#30340;&#20256;&#23548;&#25216;&#26415;&#26356;&#21152;&#36890;&#29992;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#20351;&#22522;&#20110;&#22270;&#30340; MOS &#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moving Object Segmentation (MOS) is a challenging problem in computer vision, particularly in scenarios with dynamic backgrounds, abrupt lighting changes, shadows, camouflage, and moving cameras. While graph-based methods have shown promising results in MOS, they have mainly relied on transductive learning which assumes access to the entire training and testing data for evaluation. However, this assumption is not realistic in real-world applications where the system needs to handle new data during deployment. In this paper, we propose a novel Graph Inductive Moving Object Segmentation (GraphIMOS) algorithm based on a Graph Neural Network (GNN) architecture. Our approach builds a generic model capable of performing prediction on newly added data frames using the already trained model. GraphIMOS outperforms previous inductive learning methods and is more generic than previous transductive techniques. Our proposed algorithm enables the deployment of graph-based MOS models in real-world ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31169;&#26377;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#20445;&#25252;&#35757;&#32451;&#38598;&#30340;&#38544;&#31169;&#65292;&#25506;&#35752;&#20102;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#39044;&#27979;&#27169;&#24335;&#30340;&#31169;&#26377;&#39044;&#27979;&#22120;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#38544;&#31169;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09579</link><description>&lt;p&gt;
&#31169;&#26377;&#30340;&#27704;&#20037;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Private Everlasting Prediction. (arXiv:2305.09579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31169;&#26377;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#20445;&#25252;&#35757;&#32451;&#38598;&#30340;&#38544;&#31169;&#65292;&#25506;&#35752;&#20102;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#39044;&#27979;&#27169;&#24335;&#30340;&#31169;&#26377;&#39044;&#27979;&#22120;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#31169;&#26377;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#31572;&#19968;&#31995;&#21015;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20379;&#26032;&#26679;&#26412;&#30340;&#26631;&#31614;&#39044;&#27979;&#65292;&#26088;&#22312;&#20445;&#25252;&#35757;&#32451;&#38598;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23454;&#29616;&#20004;&#31181;&#19981;&#21516;&#30340;&#39044;&#27979;&#27169;&#24335;&#30340;&#31169;&#26377;&#39044;&#27979;&#22120;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#38544;&#31169;&#24615;&#65292;&#24182;&#22312;&#20855;&#20307;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A private learner is trained on a sample of labeled points and generates a hypothesis that can be used for predicting the labels of newly sampled points while protecting the privacy of the training set [Kasiviswannathan et al., FOCS 2008]. Research uncovered that private learners may need to exhibit significantly higher sample complexity than non-private learners as is the case with, e.g., learning of one-dimensional threshold functions [Bun et al., FOCS 2015, Alon et al., STOC 2019].  We explore prediction as an alternative to learning. Instead of putting forward a hypothesis, a predictor answers a stream of classification queries. Earlier work has considered a private prediction model with just a single classification query [Dwork and Feldman, COLT 2018]. We observe that when answering a stream of queries, a predictor must modify the hypothesis it uses over time, and, furthermore, that it must use the queries for this modification, hence introducing potential privacy risks with respe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26500;&#24314;&#33410;&#28857;&#32622;&#25442;&#22522;&#32447;&#30340;&#26032;&#22411;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22240;&#26524;&#22270;&#30340;&#27491;&#30830;&#24615;&#24182;&#25351;&#23548;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.09565</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#25442;&#26816;&#39564;&#30340;&#22240;&#26524;&#22270;&#20551;&#35774;&#39564;&#35777;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Toward Falsifying Causal Graphs Using a Permutation-Based Test. (arXiv:2305.09565v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26500;&#24314;&#33410;&#28857;&#32622;&#25442;&#22522;&#32447;&#30340;&#26032;&#22411;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22240;&#26524;&#22270;&#30340;&#27491;&#30830;&#24615;&#24182;&#25351;&#23548;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31995;&#32479;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#23545;&#20110;&#35299;&#37322;&#21644;&#25511;&#21046;&#20854;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#22270;&#38656;&#35201;&#24456;&#22810;&#19981;&#24635;&#26159;&#29616;&#23454;&#30340;&#24378;&#20551;&#35774;&#12290;&#23545;&#20110;&#39046;&#22495;&#19987;&#23478;&#26469;&#35828;&#65292;&#24456;&#38590;&#34920;&#36798;&#22240;&#26524;&#22270;&#12290;&#22240;&#27492;&#65292;&#22312;&#23558;&#22240;&#26524;&#22270;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#20043;&#21069;&#65292;&#23450;&#37327;&#35780;&#20272;&#22240;&#26524;&#22270;&#30340;&#20248;&#21155;&#30340;&#24230;&#37327;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#26816;&#26597;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#25552;&#20379;&#20102;&#19968;&#20010;&#32477;&#23545;&#25968;&#37327;&#30340;&#22240;&#26524;&#22270;&#19982;&#35266;&#23519;&#25968;&#25454;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#32780;&#27809;&#26377;&#22522;&#30784;&#32447;&#65292;&#20174;&#19994;&#20154;&#21592;&#38656;&#35201;&#22238;&#31572;&#26377;&#22810;&#23569;&#36825;&#26679;&#30340;&#19981;&#19968;&#33268;&#24615;&#26159;&#21487;&#25509;&#21463;&#25110;&#39044;&#26399;&#30340;&#36825;&#19968;&#38590;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#33410;&#28857;&#32622;&#25442;&#30340;&#26367;&#20195;&#22522;&#32447;&#12290;&#36890;&#36807;&#23558;&#19981;&#19968;&#33268;&#24615;&#30340;&#25968;&#37327;&#19982;&#26367;&#20195;&#22522;&#32447;&#19978;&#30340;&#25968;&#37327;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#35299;&#37322;&#30340;&#24230;&#37327;&#65292;&#25429;&#25417;&#26377;&#21521;&#26080;&#29615;&#22270;&#26159;&#21542;&#26174;&#33879;&#36866;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the causal relationships among the variables of a system is paramount to explain and control its behaviour. Inferring the causal graph from observational data without interventions, however, requires a lot of strong assumptions that are not always realistic. Even for domain experts it can be challenging to express the causal graph. Therefore, metrics that quantitatively assess the goodness of a causal graph provide helpful checks before using it in downstream tasks. Existing metrics provide an absolute number of inconsistencies between the graph and the observed data, and without a baseline, practitioners are left to answer the hard question of how many such inconsistencies are acceptable or expected. Here, we propose a novel consistency metric by constructing a surrogate baseline through node permutations. By comparing the number of inconsistencies with those on the surrogate baseline, we derive an interpretable metric that captures whether the DAG fits significantly bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#32858;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#20849;&#21516;&#29305;&#24449;&#23558;&#25968;&#25454;&#28857;&#20998;&#32452;&#30340;&#31934;&#36873;&#21253;&#21644;&#23558;&#25968;&#25454;&#28857;&#38543;&#26426;&#20998;&#32452;&#30340;&#38543;&#26426;&#21253;&#65292;&#23545;&#20110;&#31934;&#36873;&#21253;&#35774;&#32622;&#21644;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32780;&#19981;&#20250;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.09557</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#23398;&#20064;&#65306;&#31934;&#36873;&#21253;&#19982;&#38543;&#26426;&#21253;&#30340;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning from Aggregated Data: Curated Bags versus Random Bags. (arXiv:2305.09557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#32858;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#20849;&#21516;&#29305;&#24449;&#23558;&#25968;&#25454;&#28857;&#20998;&#32452;&#30340;&#31934;&#36873;&#21253;&#21644;&#23558;&#25968;&#25454;&#28857;&#38543;&#26426;&#20998;&#32452;&#30340;&#38543;&#26426;&#21253;&#65292;&#23545;&#20110;&#31934;&#36873;&#21253;&#35774;&#32622;&#21644;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32780;&#19981;&#20250;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#37096;&#32626;&#30340;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#36825;&#20123;&#31995;&#32479;&#25910;&#38598;&#26469;&#33258;&#21508;&#31181;&#32676;&#20307;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#38382;&#39064;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#20197;&#32858;&#21512;&#30340;&#24418;&#24335;&#25910;&#38598;&#21644;&#21457;&#24067;&#25968;&#25454;&#26631;&#31614;&#65292;&#20174;&#32780;&#21487;&#20197;&#23558;&#21333;&#20010;&#29992;&#25143;&#30340;&#20449;&#24687;&#19982;&#20854;&#20182;&#29992;&#25143;&#30340;&#20449;&#24687;&#32452;&#21512;&#36215;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#32858;&#21512;&#25968;&#25454;&#26631;&#31614;&#32780;&#38750;&#21333;&#20010;&#26631;&#31614;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#32858;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#20849;&#21516;&#29305;&#24449;&#23558;&#25968;&#25454;&#28857;&#20998;&#32452;&#30340;&#31934;&#36873;&#21253;&#21644;&#23558;&#25968;&#25454;&#28857;&#38543;&#26426;&#20998;&#32452;&#30340;&#38543;&#26426;&#21253;&#12290;&#23545;&#20110;&#31934;&#36873;&#21253;&#35774;&#32622;&#21644;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32780;&#19981;&#20250;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20197;&#19979;&#35266;&#23519;&#65306;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#20043;&#21644;&#21487;&#20197;&#34920;&#31034;&#20026;&#27599;&#20010;&#21253;&#30340;&#26799;&#24230;&#30340;&#21152;&#26435;&#21644;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#21253;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting user privacy is a major concern for many machine learning systems that are deployed at scale and collect from a diverse set of population. One way to address this concern is by collecting and releasing data labels in an aggregated manner so that the information about a single user is potentially combined with others. In this paper, we explore the possibility of training machine learning models with aggregated data labels, rather than individual labels. Specifically, we consider two natural aggregation procedures suggested by practitioners: curated bags where the data points are grouped based on common features and random bags where the data points are grouped randomly in bag of similar sizes. For the curated bag setting and for a broad range of loss functions, we show that we can perform gradient-based learning without any degradation in performance that may result from aggregating data. Our method is based on the observation that the sum of the gradients of the loss functio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#27880;&#24847;&#21147;EEG&#30561;&#30496;&#20998;&#26399;&#65288;HASS&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#26102;&#31354;&#27880;&#24847;&#21147;&#26426;&#21046;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#32473;&#20104;&#36890;&#36947;&#38388;&#21644;&#36890;&#36947;&#20869;&#30340;EEG&#29255;&#27573;&#26435;&#37325;&#65292;&#26174;&#33879;&#25552;&#39640;&#20856;&#22411;&#30561;&#30496;&#20998;&#26399;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09543</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#27880;&#24847;&#21147;&#30340;EEG&#30561;&#30496;&#20998;&#26399;
&lt;/p&gt;
&lt;p&gt;
EEG-based Sleep Staging with Hybrid Attention. (arXiv:2305.09543v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#27880;&#24847;&#21147;EEG&#30561;&#30496;&#20998;&#26399;&#65288;HASS&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#26102;&#31354;&#27880;&#24847;&#21147;&#26426;&#21046;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#32473;&#20104;&#36890;&#36947;&#38388;&#21644;&#36890;&#36947;&#20869;&#30340;EEG&#29255;&#27573;&#26435;&#37325;&#65292;&#26174;&#33879;&#25552;&#39640;&#20856;&#22411;&#30561;&#30496;&#20998;&#26399;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#20998;&#26399;&#23545;&#20110;&#35780;&#20272;&#30561;&#30496;&#36136;&#37327;&#21644;&#35786;&#26029;&#30561;&#30496;&#38556;&#30861;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#30561;&#30496;&#38454;&#27573;&#26399;&#38388;&#65292;&#25429;&#25417;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#20869;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#28151;&#21512;&#27880;&#24847;&#21147;EEG&#30561;&#30496;&#20998;&#26399;&#65288;HASS&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26102;&#31354;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26681;&#25454;&#19981;&#21516;&#30561;&#30496;&#38454;&#27573;&#26399;&#38388;&#30340;&#22823;&#33041;&#31354;&#38388;-&#26102;&#38388;&#20851;&#31995;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#32473;&#20104;&#36890;&#36947;&#38388;&#21644;&#36890;&#36947;&#20869;&#30340;EEG&#29255;&#27573;&#26435;&#37325;&#12290;&#22312;MASS&#21644;ISRUC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HASS&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20856;&#22411;&#30561;&#30496;&#20998;&#26399;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#32531;&#35299;&#20102;&#22312;&#30561;&#30496;&#20998;&#26399;&#26399;&#38388;&#25429;&#25417;EEG&#20449;&#21495;&#26102;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20851;&#31995;&#30340;&#22256;&#38590;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#20020;&#24202;&#21644;&#30740;&#31350;&#29615;&#22659;&#20013;&#30561;&#30496;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep staging is critical for assessing sleep quality and diagnosing sleep disorders. However, capturing both the spatial and temporal relationships within electroencephalogram (EEG) signals during different sleep stages remains challenging. In this paper, we propose a novel framework called the Hybrid Attention EEG Sleep Staging (HASS) Framework. Specifically, we propose a well-designed spatio-temporal attention mechanism to adaptively assign weights to inter-channels and intra-channel EEG segments based on the spatio-temporal relationship of the brain during different sleep stages. Experiment results on the MASS and ISRUC datasets demonstrate that HASS can significantly improve typical sleep staging networks. Our proposed framework alleviates the difficulties of capturing the spatial-temporal relationship of EEG signals during sleep staging and holds promise for improving the accuracy and reliability of sleep assessment in both clinical and research settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20272;&#35745;&#26465;&#20214;Shapley&#20540;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#26041;&#27861;&#20998;&#31867;&#65292;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31867;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09536</link><description>&lt;p&gt;
&#20272;&#35745;&#26465;&#20214;Shapley&#20540;&#30340;&#26041;&#27861;&#27604;&#36739;&#21450;&#20854;&#24212;&#29992;&#22330;&#26223;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Methods for Estimating Conditional Shapley Values and When to Use Them. (arXiv:2305.09536v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20272;&#35745;&#26465;&#20214;Shapley&#20540;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#26041;&#27861;&#20998;&#31867;&#65292;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31867;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#26368;&#26089;&#36215;&#28304;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#65292;&#20294;&#29616;&#22312;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#26694;&#26550;&#20013;&#65292;&#29992;&#26469;&#35299;&#37322;&#22797;&#26434;&#27169;&#22411;&#25152;&#20570;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#39044;&#27979;&#27169;&#22411;&#30340;&#26465;&#20214;Shapley&#20540;&#30340;&#35745;&#31639;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#31639;&#27861;&#36884;&#24452;&#19982;&#24212;&#29992;&#22330;&#26223;&#65292;&#36825;&#20123;&#35745;&#31639;&#38656;&#35201;&#20272;&#35745;&#22797;&#26434;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#20043;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#26041;&#27861;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#35780;&#20272;&#12290;&#20998;&#31867;&#26041;&#24335;&#37319;&#29992;&#33945;&#29305;&#21345;&#32599;&#31215;&#20998;&#25110;&#22238;&#24402;&#23545;&#26465;&#20214;&#26399;&#26395;&#36827;&#34892;&#24314;&#27169;&#12290;&#20316;&#32773;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#26469;&#34913;&#37327;&#19981;&#21516;&#26041;&#27861;&#20998;&#31867;&#20272;&#35745;&#26465;&#20214;&#26399;&#26395;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapley values originated in cooperative game theory but are extensively used today as a model-agnostic explanation framework to explain predictions made by complex machine learning models in the industry and academia. There are several algorithmic approaches for computing different versions of Shapley value explanations. Here, we focus on conditional Shapley values for predictive models fitted to tabular data. Estimating precise conditional Shapley values is difficult as they require the estimation of non-trivial conditional expectations. In this article, we develop new methods, extend earlier proposed approaches, and systematize the new refined and existing methods into different method classes for comparison and evaluation. The method classes use either Monte Carlo integration or regression to model the conditional expectations. We conduct extensive simulation studies to evaluate how precisely the different method classes estimate the conditional expectations, and thereby the condit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#23454;&#26102;&#21516;&#26102;&#22810;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#37325;&#24314;&#12289;6DoF&#23039;&#24577;&#20272;&#35745;&#21644;&#23494;&#38598;&#25235;&#21462;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#39034;&#24207;&#24863;&#30693;&#21644;&#25235;&#21462;&#35268;&#21010;&#27493;&#39588;&#65292;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09510</link><description>&lt;p&gt;
&#23454;&#26102;&#21516;&#26102;&#22810;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#37325;&#24314;&#65292;6DoF&#23039;&#24577;&#20272;&#35745;&#21644;&#23494;&#38598;&#25235;&#21462;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction. (arXiv:2305.09510v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09510
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#23454;&#26102;&#21516;&#26102;&#22810;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#37325;&#24314;&#12289;6DoF&#23039;&#24577;&#20272;&#35745;&#21644;&#23494;&#38598;&#25235;&#21462;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#39034;&#24207;&#24863;&#30693;&#21644;&#25235;&#21462;&#35268;&#21010;&#27493;&#39588;&#65292;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#20381;&#36182;&#20110;&#24863;&#30693;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#25552;&#20379;&#26377;&#20851;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#20960;&#20309;&#65288;&#23039;&#24577;&#21644;&#19977;&#32500;&#24418;&#29366;&#65289;&#20197;&#21450;&#20854;&#20182;&#35821;&#20041;&#20449;&#24687;&#65288;&#20363;&#22914;&#23545;&#35937;&#26631;&#31614;&#65289;&#30340;&#20449;&#24687;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#36873;&#25321;&#30456;&#20851;&#23545;&#35937;&#19978;&#30340;&#21487;&#34892;&#25235;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#25552;&#20379;&#22330;&#26223;&#20013;&#25152;&#26377;&#23545;&#35937;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#20449;&#24687;&#20197;&#21450;&#36825;&#20123;&#23545;&#35937;&#19978;&#30340;&#21487;&#34892;&#25235;&#21462;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#28857;&#22312;&#20110;&#20854;&#36895;&#24230;&#65292;&#22240;&#20026;&#23427;&#36991;&#20813;&#20102;&#39034;&#24207;&#24863;&#30693;&#21644;&#25235;&#21462;&#35268;&#21010;&#27493;&#39588;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#19982;&#38754;&#21521;&#23545;&#35937;&#24418;&#29366;&#12289;&#23039;&#24577;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26368;&#26032;&#19987;&#29992;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#27599;&#31186;30&#24103;&#30340;&#24555;&#36895;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulation systems operating in complex environments rely on perception systems that provide information about the geometry (pose and 3D shape) of the objects in the scene along with other semantic information such as object labels. This information is then used for choosing the feasible grasps on relevant objects. In this paper, we present a novel method to provide this geometric and semantic information of all objects in the scene as well as feasible grasps on those objects simultaneously. The main advantage of our method is its speed as it avoids sequential perception and grasp planning steps. With detailed quantitative analysis, we show that our method delivers competitive performance compared to the state-of-the-art dedicated methods for object shape, pose, and grasp predictions while providing fast inference at 30 frames per second speed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23481;&#33258;&#36866;&#24212;&#19979;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#22522;&#20110;&#24863;&#30693;&#37325;&#35201;&#24615;&#20197;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#22788;&#29702;&#19981;&#21516;&#36755;&#20837;&#22270;&#20687;&#30340;&#21306;&#22495;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#35745;&#31639;&#25104;&#26412;&#21644;&#32593;&#32476;&#22823;&#23567;&#24320;&#38144;&#26368;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.09504</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#19979;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Content-Adaptive Downsampling in Convolutional Neural Networks. (arXiv:2305.09504v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23481;&#33258;&#36866;&#24212;&#19979;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#22522;&#20110;&#24863;&#30693;&#37325;&#35201;&#24615;&#20197;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#22788;&#29702;&#19981;&#21516;&#36755;&#20837;&#22270;&#20687;&#30340;&#21306;&#22495;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#35745;&#31639;&#25104;&#26412;&#21644;&#32593;&#32476;&#22823;&#23567;&#24320;&#38144;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20381;&#36182;&#20110;&#36880;&#27493;&#38477;&#37319;&#26679;&#20854;&#29305;&#24449;&#26144;&#23556;&#26469;&#22686;&#21152;&#32593;&#32476;&#30340;&#24863;&#21463;&#37326;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#22312;&#29305;&#24449;&#26144;&#23556;&#20013;&#22833;&#21435;&#31890;&#24230;&#30340;&#20195;&#20215;&#20026;&#20195;&#20215;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#27491;&#30830;&#29702;&#35299;&#22270;&#20687;&#25110;&#22312;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#20013;&#24674;&#22797;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#29992;&#33192;&#32960;&#21367;&#31215;&#26367;&#25442;CNN&#20013;&#30340;&#26368;&#21518;&#20960;&#20010;&#19979;&#37319;&#26679;&#25805;&#20316;&#65292;&#20174;&#32780;&#22312;&#19981;&#20943;&#23569;&#25509;&#21463;&#22495;&#30340;&#24773;&#20917;&#19979;&#20445;&#30041;&#29305;&#24449;&#22320;&#22270;&#30340;&#20998;&#36776;&#29575;&#65292;&#23613;&#31649;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#20801;&#35768;&#26681;&#25454;&#36755;&#20986;&#29305;&#24449;&#20998;&#36776;&#29575;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#25104;&#26412;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#23450;&#26399;&#19979;&#37319;&#26679;&#25110;&#19981;&#19979;&#37319;&#26679;&#25972;&#20010;&#29305;&#24449;&#26144;&#23556;&#65292;&#38544;&#21547;&#22320;&#23558;&#36755;&#20837;&#22270;&#20687;&#21644;&#38543;&#21518;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#25152;&#26377;&#21306;&#22495;&#35270;&#20026;&#21516;&#31561;&#37325;&#35201;&#65292;&#36825;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#24182;&#19981;&#25104;&#31435;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#19979;&#37319;&#26679;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#22522;&#20110;&#20854;&#24863;&#30693;&#37325;&#35201;&#24615;&#65292;&#20197;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#22788;&#29702;&#36755;&#20837;&#22270;&#20687;&#30340;&#19981;&#21516;&#21306;&#22495;&#26469;&#27010;&#25324;&#19978;&#36848;&#24605;&#24819;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#27599;&#20010;&#21306;&#22495;&#26159;&#21542;&#19979;&#37319;&#26679;&#20197;&#21450;&#19979;&#37319;&#26679;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#20013;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#19988;&#35745;&#31639;&#25104;&#26412;&#21644;&#32593;&#32476;&#22823;&#23567;&#24320;&#38144;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many convolutional neural networks (CNNs) rely on progressive downsampling of their feature maps to increase the network's receptive field and decrease computational cost. However, this comes at the price of losing granularity in the feature maps, limiting the ability to correctly understand images or recover fine detail in dense prediction tasks. To address this, common practice is to replace the last few downsampling operations in a CNN with dilated convolutions, allowing to retain the feature map resolution without reducing the receptive field, albeit increasing the computational cost. This allows to trade off predictive performance against cost, depending on the output feature resolution. By either regularly downsampling or not downsampling the entire feature map, existing work implicitly treats all regions of the input image and subsequent feature maps as equally important, which generally does not hold. We propose an adaptive downsampling scheme that generalizes the above idea by
&lt;/p&gt;</description></item><item><title>Contrastive Label Enhancement&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#29305;&#24449;&#21644;&#36923;&#36753;&#26631;&#31614;&#38598;&#25104;&#21040;&#32479;&#19968;&#30340;&#25237;&#24433;&#31354;&#38388;&#20013;&#65292;&#20197;&#29983;&#25104;&#39640;&#32423;&#29305;&#24449;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#30456;&#21516;&#26679;&#26412;&#30340;&#29305;&#24449;&#21644;&#36923;&#36753;&#26631;&#31614;&#34987;&#25289;&#36817;&#65292;&#19981;&#21516;&#26679;&#26412;&#30340;&#34987;&#25237;&#24433;&#21040;&#26356;&#36828;&#12290;</title><link>http://arxiv.org/abs/2305.09500</link><description>&lt;p&gt;
&#23545;&#27604;&#26631;&#31614;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Contrastive Label Enhancement. (arXiv:2305.09500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09500
&lt;/p&gt;
&lt;p&gt;
Contrastive Label Enhancement&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#29305;&#24449;&#21644;&#36923;&#36753;&#26631;&#31614;&#38598;&#25104;&#21040;&#32479;&#19968;&#30340;&#25237;&#24433;&#31354;&#38388;&#20013;&#65292;&#20197;&#29983;&#25104;&#39640;&#32423;&#29305;&#24449;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#30456;&#21516;&#26679;&#26412;&#30340;&#29305;&#24449;&#21644;&#36923;&#36753;&#26631;&#31614;&#34987;&#25289;&#36817;&#65292;&#19981;&#21516;&#26679;&#26412;&#30340;&#34987;&#25237;&#24433;&#21040;&#26356;&#36828;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;(Label distribution learning, LDL)&#26159;&#35299;&#20915;&#26631;&#31614;&#27169;&#31946;&#24615;&#30340;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#12290;&#30001;&#20110;&#30452;&#25509;&#33719;&#21462;&#26631;&#31614;&#20998;&#24067;&#24456;&#22256;&#38590;&#65292;&#22240;&#27492;&#35768;&#22810;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#22914;&#20309;&#20174;&#36923;&#36753;&#26631;&#31614;&#20013;&#24674;&#22797;&#26631;&#31614;&#20998;&#24067;&#65292;&#34987;&#31216;&#20026;&#26631;&#31614;&#22686;&#24378;(LE)&#12290;&#29616;&#26377;&#30340;LE&#26041;&#27861;&#36890;&#36807;&#22312;&#36923;&#36753;&#26631;&#31614;&#30340;&#30417;&#30563;&#19979;&#26500;&#24314;&#29305;&#24449;&#19982;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#26469;&#20272;&#35745;&#26631;&#31614;&#20998;&#24067;&#12290;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20102;&#29305;&#24449;&#21644;&#36923;&#36753;&#26631;&#31614;&#37117;&#26159;&#20174;&#19981;&#21516;&#35270;&#35282;&#25551;&#36848;&#23454;&#20363;&#30340;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#27604;&#26631;&#31614;&#22686;&#24378;(ConLE)&#65292;&#23427;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#23558;&#29305;&#24449;&#21644;&#36923;&#36753;&#26631;&#31614;&#38598;&#25104;&#21040;&#32479;&#19968;&#30340;&#25237;&#24433;&#31354;&#38388;&#20013;&#65292;&#20197;&#29983;&#25104;&#39640;&#32423;&#29305;&#24449;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#23646;&#20110;&#21516;&#19968;&#26679;&#26412;&#30340;&#29305;&#24449;&#21644;&#36923;&#36753;&#26631;&#31614;&#34987;&#25289;&#36817;&#65292;&#32780;&#19981;&#21516;&#26679;&#26412;&#30340;&#34987;&#25237;&#24433;&#21040;&#25237;&#24433;&#31354;&#38388;&#20013;&#26356;&#36828;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label distribution learning (LDL) is a new machine learning paradigm for solving label ambiguity. Since it is difficult to directly obtain label distributions, many studies are focusing on how to recover label distributions from logical labels, dubbed label enhancement (LE). Existing LE methods estimate label distributions by simply building a mapping relationship between features and label distributions under the supervision of logical labels. They typically overlook the fact that both features and logical labels are descriptions of the instance from different views. Therefore, we propose a novel method called Contrastive Label Enhancement (ConLE) which integrates features and logical labels into the unified projection space to generate high-level features by contrastive learning strategy. In this approach, features and logical labels belonging to the same sample are pulled closer, while those of different samples are projected farther away from each other in the projection space. Sub
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#22522;&#20110;&#30828;&#20214;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20809;&#23398;&#22343;&#34913;&#22120;&#26102;&#65292;&#20351;&#29992;&#36817;&#20284;&#28608;&#27963;&#20989;&#25968;&#30340; biLSTM &#22343;&#34913;&#22120;&#33021;&#22815;&#21462;&#24471;&#25509;&#36817;&#20110;&#21407;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09495</link><description>&lt;p&gt;
&#22522;&#20110;&#30828;&#20214;&#23454;&#29616;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#20809;&#23398;&#22343;&#34913;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hardware Realization of Nonlinear Activation Functions for NN-based Optical Equalizers. (arXiv:2305.09495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#22522;&#20110;&#30828;&#20214;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20809;&#23398;&#22343;&#34913;&#22120;&#26102;&#65292;&#20351;&#29992;&#36817;&#20284;&#28608;&#27963;&#20989;&#25968;&#30340; biLSTM &#22343;&#34913;&#22120;&#33021;&#22815;&#21462;&#24471;&#25509;&#36817;&#20110;&#21407;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#38477;&#20302;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20809;&#23398;&#20449;&#36947;&#22343;&#34913;&#22120;&#30340;&#30828;&#20214;&#23454;&#29616;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#36817;&#20284;&#28608;&#27963;&#20989;&#25968;&#30340; biLSTM &#22343;&#34913;&#22120;&#30340;&#24615;&#33021;&#25509;&#36817;&#20110;&#21407;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To reduce the complexity of the hardware implementation of neural network-based optical channel equalizers, we demonstrate that the performance of the biLSTM equalizer with approximated activation functions is close to that of the original model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22826;&#38451;&#27963;&#21160;&#21306;&#30913;&#22270;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#22826;&#38451;&#32768;&#26001;&#26631;&#31614;&#12290;&#23427;&#21487;&#29992;&#20110;&#30740;&#31350;&#30913;&#32467;&#26500;&#12289;&#20854;&#28436;&#21270;&#20197;&#21450;&#22826;&#38451;&#32768;&#26001;&#30340;&#20851;&#31995;&#65292;&#24182;&#23545;&#20110;&#33258;&#21160;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.09492</link><description>&lt;p&gt;
&#31354;&#38388;&#22825;&#27668;&#30740;&#31350;&#20013;&#30340;&#22826;&#38451;&#27963;&#21160;&#21306;&#30913;&#22330;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Solar Active Region Magnetogram Image Dataset for Studies of Space Weather. (arXiv:2305.09492v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22826;&#38451;&#27963;&#21160;&#21306;&#30913;&#22270;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#22826;&#38451;&#32768;&#26001;&#26631;&#31614;&#12290;&#23427;&#21487;&#29992;&#20110;&#30740;&#31350;&#30913;&#32467;&#26500;&#12289;&#20854;&#28436;&#21270;&#20197;&#21450;&#22826;&#38451;&#32768;&#26001;&#30340;&#20851;&#31995;&#65292;&#24182;&#23545;&#20110;&#33258;&#21160;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#32654;&#22269;&#22269;&#23478;&#33322;&#31354;&#33322;&#22825;&#23616;&#65288;NASA&#65289;&#22826;&#38451;&#21160;&#21147;&#23398;&#35266;&#27979;&#21355;&#26143;&#65288;SDO&#65289;&#30340;&#30913;&#22270;&#65288;&#34913;&#37327;&#30913;&#22330;&#24378;&#24230;&#30340;&#22270;&#20687;&#65289;&#30340;&#20840;&#38754;&#25910;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;&#19977;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;SDO&#22320;&#38663;&#23398;&#21644;&#30913;&#23398;&#20202;&#65288;HMI&#65289;&#22826;&#38451;&#27963;&#36291;&#21306;&#65288;&#22823;&#30913;&#36890;&#21306;&#22495;&#65292;&#36890;&#24120;&#26159;&#29190;&#21457;&#20107;&#20214;&#30340;&#28304;&#22836;&#65289;&#30340;&#30913;&#22270;&#20197;&#21450;&#30456;&#24212;&#32768;&#26001;&#27963;&#21160;&#30340;&#26631;&#31614;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#30740;&#31350;&#30913;&#32467;&#26500;&#12289;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21270;&#20197;&#21450;&#19982;&#22826;&#38451;&#32768;&#26001;&#30340;&#20851;&#31995;&#30340;&#22270;&#20687;&#20998;&#26512;&#25110;&#22826;&#38451;&#29289;&#29702;&#23398;&#30740;&#31350;&#23558;&#38750;&#24120;&#26377;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#23545;&#37027;&#20123;&#30740;&#31350;&#33258;&#21160;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20154;&#21592;&#20135;&#29983;&#20852;&#36259;&#65292;&#21253;&#25324;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;&#32463;&#20856;&#21644;&#28145;&#24230;&#65289;&#12289;&#20108;&#20803;&#21644;&#22810;&#31867;&#20998;&#31867;&#20197;&#21450;&#22238;&#24402;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26368;&#23567;&#22788;&#29702;&#19988;&#29992;&#25143;&#21487;&#37197;&#32622;&#30340;&#19968;&#33268;&#22823;&#23567;&#22826;&#38451;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this dataset we provide a comprehensive collection of magnetograms (images quantifying the strength of the magnetic field) from the National Aeronautics and Space Administration's (NASA's) Solar Dynamics Observatory (SDO). The dataset incorporates data from three sources and provides SDO Helioseismic and Magnetic Imager (HMI) magnetograms of solar active regions (regions of large magnetic flux, generally the source of eruptive events) as well as labels of corresponding flaring activity. This dataset will be useful for image analysis or solar physics research related to magnetic structure, its evolution over time, and its relation to solar flares. The dataset will be of interest to those researchers investigating automated solar flare prediction methods, including supervised and unsupervised machine learning (classical and deep), binary and multi-class classification, and regression. This dataset is a minimally processed, user configurable dataset of consistently sized images of sola
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#34892;&#25919;&#27807;&#36890;&#20013;&#30340;&#21457;&#22768;&#31505;&#22768;&#23545;&#20110;&#31038;&#20250;&#35748;&#21487;&#30340;&#31215;&#26497;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24403;&#21457;&#29983;&#21452;&#21521;&#31505;&#22768;&#26102;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#24433;&#21709;&#38543;&#30528;&#32452;&#32455;&#19994;&#32489;&#30340;&#19979;&#38477;&#32780;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2305.09485</link><description>&lt;p&gt;
&#34892;&#25919;&#20154;&#21592;&#30340;&#21457;&#22768;&#31505;&#22768;&#19982;&#31038;&#20250;&#35748;&#21487;&#65306;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#25506;&#31350;&#12290; (arXiv:2305.09485v1 [&#32463;&#27982;&#23398;.GN])
&lt;/p&gt;
&lt;p&gt;
Executive Voiced Laughter and Social Approval: An Explorative Machine Learning Study. (arXiv:2305.09485v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#34892;&#25919;&#27807;&#36890;&#20013;&#30340;&#21457;&#22768;&#31505;&#22768;&#23545;&#20110;&#31038;&#20250;&#35748;&#21487;&#30340;&#31215;&#26497;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24403;&#21457;&#29983;&#21452;&#21521;&#31505;&#22768;&#26102;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#24433;&#21709;&#38543;&#30528;&#32452;&#32455;&#19994;&#32489;&#30340;&#19979;&#38477;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#34892;&#25919;&#20154;&#21592;&#27807;&#36890;&#20013;&#30340;&#21457;&#22768;&#31505;&#22768;&#20197;&#21450;&#23427;&#23545;&#31038;&#20250;&#35748;&#21487;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#31505;&#22768;&#65292;&#24773;&#24863;&#20316;&#20026;&#20449;&#24687;&#21644;&#20449;&#24687;&#23186;&#20171;&#23545;&#20844;&#21496;&#30340;&#31038;&#20250;&#35780;&#20215;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#20551;&#35774;&#34892;&#25919;&#20154;&#21592;&#27807;&#36890;&#20013;&#30340;&#21457;&#22768;&#31505;&#22768;&#23545;&#31038;&#20250;&#35748;&#21487;&#26377;&#31215;&#26497;&#24433;&#21709;&#65292;&#31038;&#20250;&#35748;&#21487;&#26159;&#25351;&#21463;&#20247;&#23545;&#19968;&#20010;&#32452;&#32455;&#30340;&#20146;&#21644;&#21147;&#30340;&#24863;&#30693;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19982;&#20247;&#31505;&#30340;&#25928;&#26524;&#23588;&#20854;&#24378;&#65292;&#21363;&#22312;&#32473;&#23450;&#30340;&#27807;&#36890;&#22330;&#21512;&#20013;&#65292;&#32858;&#28966;&#30340;&#34892;&#25919;&#20154;&#21592;&#21644;&#35266;&#20247;&#21516;&#26102;&#21457;&#31505;&#30340;&#27425;&#25968;&#12290;&#26368;&#21518;&#65292;&#32467;&#21512;&#24773;&#24863;&#20316;&#20026;&#20449;&#24687;&#21644;&#20154;&#31867;&#35748;&#30693;&#30340;&#36127;&#38754;&#20559;&#35265;&#65292;&#25105;&#20204;&#20551;&#35774;&#31505;&#22768;&#23545;&#31038;&#20250;&#35748;&#21487;&#30340;&#31215;&#26497;&#24433;&#21709;&#38543;&#30528;&#32452;&#32455;&#19994;&#32489;&#30340;&#19979;&#38477;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#22312;902&#20010;&#24503;&#22269;&#24052;&#26519;&#24503;&#26031;&#21033;&#21152;&#36275;&#29699;&#26032;&#38395;&#21457;&#24067;&#20250;&#21644;&#23186;&#20307;&#21313;&#22823;&#25968;&#25454;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#31505;&#22768;&#26816;&#27979;&#65292;&#24182;&#25214;&#21040;&#20102;&#37096;&#20998;&#25903;&#25345;&#25105;&#20204;&#24819;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study voiced laughter in executive communication and its effect on social approval. Integrating research on laughter, affect-as-information, and infomediaries' social evaluations of firms, we hypothesize that voiced laughter in executive communication positively affects social approval, defined as audience perceptions of affinity towards an organization. We surmise that the effect of laughter is especially strong for joint laughter, i.e., the number of instances in a given communication venue for which the focal executive and the audience laugh simultaneously. Finally, combining the notions of affect-as-information and negativity bias in human cognition, we hypothesize that the positive effect of laughter on social approval increases with bad organizational performance. We find partial support for our ideas when testing them on panel data comprising 902 German Bundesliga soccer press conferences and media tenor, applying state-of-the-art machine learning approaches for laughter dete
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#31227;&#21160;&#35302;&#25720;&#21160;&#24577;&#36827;&#34892;&#36830;&#32493;&#35748;&#35777;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#12289;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#19977;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#37319;&#38598;&#20102;40&#20010;&#21463;&#35797;&#32773;&#30340;&#35302;&#25720;&#21160;&#24577;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25345;&#32493;&#39564;&#35777;&#29992;&#25143;&#36523;&#20221;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09482</link><description>&lt;p&gt;
&#8220;&#36523;&#34892;&#30456;&#38543;&#8221;&#8212;&#8212;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#35302;&#25720;&#21160;&#24577;&#30340;&#36830;&#32493;&#29992;&#25143;&#35748;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Your Identity is Your Behavior -- Continuous User Authentication based on Machine Learning and Touch Dynamics. (arXiv:2305.09482v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#31227;&#21160;&#35302;&#25720;&#21160;&#24577;&#36827;&#34892;&#36830;&#32493;&#35748;&#35777;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#12289;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#19977;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#37319;&#38598;&#20102;40&#20010;&#21463;&#35797;&#32773;&#30340;&#35302;&#25720;&#21160;&#24577;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25345;&#32493;&#39564;&#35777;&#29992;&#25143;&#36523;&#20221;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#20351;&#29992;&#31227;&#21160;&#35302;&#25720;&#21160;&#24577;&#36827;&#34892;&#36830;&#32493;&#35748;&#35777;&#30340;&#21487;&#34892;&#24615;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#12289;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#19977;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#36827;&#34892;&#23454;&#39564;&#12290;&#31227;&#21160;&#35774;&#22791;&#22312;&#19990;&#30028;&#33539;&#22260;&#20869;&#30340;&#26222;&#21450;&#29575;&#19981;&#26029;&#25552;&#21319;&#65292;&#30446;&#21069;&#26234;&#33021;&#25163;&#26426;&#30340;&#35746;&#38405;&#37327;&#24050;&#32463;&#36229;&#36807;&#20102;60&#20159;&#12290;&#31227;&#21160;&#35302;&#25720;&#21160;&#24577;&#26159;&#25351;&#29992;&#25143;&#19982;&#31227;&#21160;&#35774;&#22791;&#20132;&#20114;&#30340;&#29305;&#23450;&#27169;&#24335;&#65292;&#21253;&#25324;&#35302;&#25720;&#30340;&#21387;&#21147;&#12289;&#28369;&#21160;&#36895;&#24230;&#21644;&#35302;&#25720;&#30340;&#25345;&#32493;&#26102;&#38388;&#31561;&#22240;&#32032;&#12290;&#36830;&#32493;&#35748;&#35777;&#26159;&#25351;&#22312;&#29992;&#25143;&#20351;&#29992;&#35774;&#22791;&#26102;&#25345;&#32493;&#39564;&#35777;&#20854;&#36523;&#20221;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#21021;&#22987;&#30331;&#24405;&#26102;&#36827;&#34892;&#36523;&#20221;&#39564;&#35777;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;LG V30+&#25910;&#38598;&#20102;40&#20010;&#21463;&#35797;&#32773;&#30340;&#35302;&#25720;&#21160;&#24577;&#25968;&#25454;&#38598;&#12290;&#21442;&#19982;&#32773;&#21508;&#29609;&#20102;PUBG&#12289;Diep.io&#12289;Slither&#21644;Minecraft&#31561;&#22235;&#27454;&#25163;&#26426;&#28216;&#25103;&#65292;&#27599;&#20010;&#28216;&#25103;&#29609;10&#20998;&#38047;&#12290;&#20351;&#29992;&#25552;&#21462;&#30340;&#25968;&#25454;&#38598;&#23545;&#19977;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this research paper is to look into the use of continuous authentication with mobile touch dynamics, using three different algorithms: Neural Network, Extreme Gradient Boosting, and Support Vector Machine. Mobile devices are constantly increasing in popularity in the world, today smartphone subscriptions have surpassed 6 billion. Mobile touch dynamics refer to the distinct patterns of how a user interacts with their mobile device, this includes factors such as touch pressure, swipe speed, and touch duration. Continuous authentication refers to the process of continuously verifying a user's identity while they are using a device, rather than just at the initial login. This research used a dataset of touch dynamics collected from 40 subjects using the LG V30+. The participants played four mobile games, PUBG, Diep.io, Slither, and Minecraft, for 10 minutes each game. The three algorithms were trained and tested on the extracted dataset, and their performance was evaluated based
&lt;/p&gt;</description></item><item><title>&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#25928;&#29575;</title><link>http://arxiv.org/abs/2305.09481</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#20998;&#23376;&#34920;&#31034;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#33647;&#29289;&#21457;&#29616;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Context-enriched molecule representations improve few-shot drug discovery. (arXiv:2305.09481v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09481
&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#36741;&#21161;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#26159;&#37319;&#29992;&#24050;&#30693;&#30340;&#27963;&#24615;&#20998;&#23376;&#26500;&#24314;&#27169;&#22411;&#65292;&#20197;&#21457;&#29616;&#38656;&#35201;&#36827;&#19968;&#27493;&#31579;&#36873;&#30340;&#26377;&#28508;&#21147;&#20998;&#23376;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#21482;&#26377;&#38750;&#24120;&#23569;&#25968;&#30340;&#27963;&#24615;&#20998;&#23376;&#26159;&#24050;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#26377;&#21487;&#33021;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20013;&#36825;&#20010;&#20851;&#38190;&#38454;&#27573;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23569;&#26679;&#26412;&#33647;&#29289;&#21457;&#29616;&#26041;&#27861;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#24050;&#30693;&#30340;&#19978;&#19979;&#25991;&#25110;&#21442;&#32771;&#20998;&#23376;&#26469;&#20016;&#23500;&#20998;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#20998;&#23376;&#34920;&#31034;&#20016;&#23500;&#26032;&#27010;&#24565;&#26159;&#36890;&#36807;&#29616;&#20195; Hopfield &#32593;&#32476;&#23558;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#20013;&#30340;&#20998;&#23376;&#19982;&#22823;&#37327;&#30340;&#21442;&#32771;&#65288;&#19978;&#19979;&#25991;&#65289;&#20998;&#23376;&#30456;&#20851;&#32852;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#36825;&#20010;&#20016;&#23500;&#27493;&#39588;&#31867;&#20284;&#20110;&#20154;&#31867;&#19987;&#23478;&#23558;&#19968;&#20010;&#32473;&#23450;&#30340;&#20998;&#23376;&#19982;&#29087;&#24713;&#30340;&#20854;&#23646;&#24615;&#24050;&#30693;&#30340;&#20998;&#23376;&#30456;&#20851;&#32852;&#12290;&#36825;&#20010;&#20016;&#23500;&#27493;&#39588;&#22686;&#24378;&#24182;&#25918;&#22823;&#20102;&#25968;&#25454;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#23569;&#26679;&#26412;&#33647;&#29289;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#23427;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central task in computational drug discovery is to construct models from known active molecules to find further promising molecules for subsequent screening. However, typically only very few active molecules are known. Therefore, few-shot learning methods have the potential to improve the effectiveness of this critical phase of the drug discovery process. We introduce a new method for few-shot drug discovery. Its main idea is to enrich a molecule representation by knowledge about known context or reference molecules. Our novel concept for molecule representation enrichment is to associate molecules from both the support set and the query set with a large set of reference (context) molecules through a Modern Hopfield Network. Intuitively, this enrichment step is analogous to a human expert who would associate a given molecule with familiar molecules whose properties are known. The enrichment step reinforces and amplifies the covariance structure of the data, while simultaneously remov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09480</link><description>&lt;p&gt;
&#20132;&#21449;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#19979;&#30340;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#26159;&#19968;&#31181;&#19968;&#27425;&#24615;&#25239;&#20307;&#35774;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#30001;&#20813;&#30123;&#31995;&#32479;&#20135;&#29983;&#30340;&#38024;&#23545;&#22806;&#26469;&#29289;&#36136;&#25110;&#25239;&#21407;&#30340;&#37325;&#35201;&#34507;&#30333;&#36136;&#12290;&#25239;&#20307;&#30340;&#29305;&#24322;&#24615;&#30001;&#20854;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#20915;&#23450;&#65292;CDR&#20301;&#20110;&#25239;&#20307;&#38142;&#30340;&#21487;&#21464;&#21306;&#22495;&#20013;&#65292;&#24418;&#25104;&#19982;&#25239;&#21407;&#32467;&#21512;&#30340;&#20301;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;&#25216;&#26415;&#29983;&#25104;CDR&#65292;&#20294;&#23427;&#20204;&#36973;&#21463;&#20102;&#20960;&#20309;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;&#36845;&#20195;&#31934;&#21270;&#31574;&#30053;&#23548;&#33268;&#20102;&#20302;&#25928;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25239;&#20307;CDR&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#65288;ii&#65289;&#24207;&#21015;&#32467;&#26500;&#20849;&#23398;&#20064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#65292;&#21487;&#25429;&#25417;&#34507;&#30333;&#36136;&#39592;&#26550;&#21407;&#23376;&#65288;&#21253;&#25324;C&#945;&#12289;N&#12289;C&#21644;O&#21407;&#23376;&#65289;&#20043;&#38388;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#32452;&#20998;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#20840;&#38754;&#30340;&#20960;&#20309;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#33041;&#30005;&#20449;&#21495;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20351;&#29992;&#22810;&#29305;&#24449;&#30456;&#20851;&#20998;&#26512;&#26041;&#27861;&#26469;&#33258;&#21160;&#20998;&#35299;&#21644;&#25552;&#21462;&#22810;&#31181;&#31867;&#22411;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#12290;&#20854;&#20013;&#65292;PCA&#38477;&#32500;&#25216;&#26415;&#29992;&#20110;&#25214;&#21040;&#21344;&#20027;&#23548;&#30340;&#20381;&#36182;&#20851;&#31995;&#26041;&#21521;&#65292;&#20174;&#32780;&#25552;&#21462;&#33041;&#30005;&#20449;&#21495;&#20013;&#24494;&#23567;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09478</link><description>&lt;p&gt;
&#26102;&#38388;&#24310;&#36831;&#22810;&#29305;&#24449;&#30456;&#20851;&#20998;&#26512;&#20197;&#25552;&#21462;&#33041;&#30005;&#20449;&#21495;&#20013;&#24494;&#23567;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
Time delay multi-feature correlation analysis to extract subtle dependencies from EEG signals. (arXiv:2305.09478v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#33041;&#30005;&#20449;&#21495;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20351;&#29992;&#22810;&#29305;&#24449;&#30456;&#20851;&#20998;&#26512;&#26041;&#27861;&#26469;&#33258;&#21160;&#20998;&#35299;&#21644;&#25552;&#21462;&#22810;&#31181;&#31867;&#22411;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#12290;&#20854;&#20013;&#65292;PCA&#38477;&#32500;&#25216;&#26415;&#29992;&#20110;&#25214;&#21040;&#21344;&#20027;&#23548;&#30340;&#20381;&#36182;&#20851;&#31995;&#26041;&#21521;&#65292;&#20174;&#32780;&#25552;&#21462;&#33041;&#30005;&#20449;&#21495;&#20013;&#24494;&#23567;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#26159;&#26497;&#20854;&#22797;&#26434;&#30340;&#33041;&#27963;&#21160;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20363;&#22914;&#19968;&#23545;&#30005;&#26497;&#20449;&#21495;&#22312;&#19981;&#21516;&#26102;&#38388;&#24310;&#36831;&#65288;&#28382;&#21518;$\Delta$t&#65289;&#19979;&#30340;&#32852;&#21512;&#20998;&#24067;&#65288;$\rho_{\Delta t}$&#65289;&#21487;&#20197;&#35775;&#38382;&#36825;&#31181;&#38544;&#34255;&#21160;&#24577;&#30340;&#26576;&#20123;&#32454;&#33410;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#30417;&#35270;&#36825;&#26679;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#21333;&#19968;&#35780;&#20272;&#65292;&#20363;&#22914;Pearson&#30456;&#20851;&#65288;&#25110;&#20114;&#20449;&#24687;&#65289;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#32467;&#26524;&#36890;&#24120;&#30456;&#23545;&#19981;&#22826;&#26377;&#36259;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36825;&#26679;&#30340;&#22797;&#26434;&#20449;&#21495;&#21487;&#33021;&#30001;&#22810;&#31181;&#31867;&#22411;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#26500;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20998;&#35299;&#21644;&#25552;&#21462;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#32852;&#21512;&#20998;&#24067;&#24314;&#27169;&#20026;&#25152;&#26377;&#32771;&#34385;&#30340;&#28382;&#21518;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#39033;&#24335;&#20272;&#35745;&#65292;&#28982;&#21518;&#36890;&#36807;PCA&#38477;&#32500;&#25214;&#21040;&#21344;&#20027;&#23548;&#30340;&#20381;&#36182;&#20851;&#31995;&#26041;&#21521;$f_v$&#12290;&#36825;&#26679;&#25105;&#20204;&#24471;&#21040;&#19968;&#20123;&#28382;&#21518;&#20381;&#36182;&#29305;&#24449;$a_i(\Delta t)$&#65292;&#29992;&#20110;&#25551;&#36848;&#21508;&#20010;&#28382;&#21518;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#21450;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) signals are resultants of extremely complex brain activity. Some details of this hidden dynamics might be accessible through e.g. joint distributions $\rho_{\Delta t}$ of signals of pairs of electrodes shifted by various time delays (lag $\Delta t$). A standard approach is monitoring a single evaluation of such joint distributions, like Pearson correlation (or mutual information), which turns out relatively uninteresting as expected, there is usually a small peak for zero delay and nearly symmetric drop with delay. In contrast, such a complex signal might be composed of multiple types of statistical dependencies - this article proposes approach to automatically decompose and extract them. Specifically, we model such joint distributions as polynomials estimated for all considered lag dependencies, then with PCA dimensionality reduction find dominant dependency directions $f_v$. This way we get a few lag dependent features $a_i(\Delta t)$ describing separat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36719;&#20214;&#22871;&#20214;ANALYSE&#65292;&#35753;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#29289;&#29702;&#32593;&#32476;&#33021;&#28304;&#31995;&#32479;&#20013;&#21457;&#29616;&#25915;&#20987;&#12290;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25214;&#21040;&#26410;&#30693;&#25915;&#20987;&#31867;&#22411;&#30340;&#33258;&#25105;&#35760;&#24405;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.09476</link><description>&lt;p&gt;
ANALYSE -- &#23398;&#20064;&#20351;&#29992;&#26234;&#33021;&#20195;&#29702;&#25915;&#20987;&#29289;&#29702;&#32593;&#32476;&#33021;&#28304;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ANALYSE -- Learning to Attack Cyber-Physical Energy Systems With Intelligent Agents. (arXiv:2305.09476v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36719;&#20214;&#22871;&#20214;ANALYSE&#65292;&#35753;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#29289;&#29702;&#32593;&#32476;&#33021;&#28304;&#31995;&#32479;&#20013;&#21457;&#29616;&#25915;&#20987;&#12290;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25214;&#21040;&#26410;&#30693;&#25915;&#20987;&#31867;&#22411;&#30340;&#33258;&#25105;&#35760;&#24405;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#36890;&#20449;&#25216;&#26415;&#30340;&#26222;&#21450;&#21644;&#24066;&#22330;&#30340;&#24341;&#20837;&#20351;&#24471;&#24694;&#24847;&#25915;&#20987;&#21644;&#20197;&#21033;&#30410;&#20026;&#39537;&#21160;&#30340;&#25915;&#20987;&#23545;&#29289;&#29702;&#32593;&#32476;&#33021;&#28304;&#31995;&#32479;&#36896;&#25104;&#20102;&#23041;&#32961;&#12290;&#20026;&#30830;&#20445;&#20379;&#24212;&#23433;&#20840;&#65292;&#26377;&#24517;&#35201;&#20998;&#26512;&#36825;&#20123;&#25915;&#20987;&#21450;&#20854;&#28508;&#22312;&#28431;&#27934;&#65292;&#21046;&#23450;&#23545;&#31574;&#24182;&#25913;&#36827;&#31995;&#32479;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36719;&#20214;&#22871;&#20214;ANALYSE&#65292;&#35753;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#21253;&#25324;&#30005;&#21147;&#31995;&#32479;&#12289;&#20449;&#24687;&#36890;&#20449;&#25216;&#26415;&#21644;&#33021;&#28304;&#24066;&#22330;&#22312;&#20869;&#30340;&#29289;&#29702;&#32593;&#32476;&#33021;&#28304;&#31995;&#32479;&#20013;&#21457;&#29616;&#25915;&#20987;&#12290;ANALYSE &#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#12289;&#21487;&#37197;&#32622;&#21644;&#33258;&#25105;&#35760;&#24405;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25214;&#21040;&#23578;&#26410;&#30693;&#36947;&#30340;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#22312;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#22797;&#29616;&#35768;&#22810;&#24050;&#30693;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ongoing penetration of energy systems with information and communications technology (ICT) and the introduction of new markets increase the potential for malicious or profit-driven attacks that endanger system stability. To ensure security-of-supply, it is necessary to analyze such attacks and their underlying vulnerabilities, to develop countermeasures and improve system design. We propose ANALYSE, a machine-learning-based software suite to let learning agents autonomously find attacks in cyber-physical energy systems, consisting of the power system, ICT, and energy markets. ANALYSE is a modular, configurable, and self-documenting framework designed to find yet unknown attack types and to reproduce many known attack strategies in cyber-physical energy systems from the scientific literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#37325;&#26500;&#30340;LSTM-Autoencoder&#65288;LSTM-AE&#65289;&#27169;&#22411;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;DDoS&#25915;&#20987;&#24322;&#24120;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#26368;&#20339;&#38408;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.09475</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26500;&#30340;LSTM&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;DDoS&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Reconstruction-based LSTM-Autoencoder for Anomaly-based DDoS Attack Detection over Multivariate Time-Series Data. (arXiv:2305.09475v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09475
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#37325;&#26500;&#30340;LSTM-Autoencoder&#65288;LSTM-AE&#65289;&#27169;&#22411;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;DDoS&#25915;&#20987;&#24322;&#24120;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#26368;&#20339;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#26159;&#19968;&#31181;&#24694;&#24847;&#20225;&#22270;&#36890;&#36807;&#21457;&#36865;&#28023;&#37327;&#27969;&#37327;&#26469;&#28153;&#27809;&#30446;&#26631;&#25110;&#20854;&#21608;&#22260;&#22522;&#30784;&#26550;&#26500;&#30340;&#26381;&#21153;&#22120;&#12289;&#26381;&#21153;&#25110;&#32593;&#32476;&#30340;&#24120;&#35268;&#27969;&#37327;&#30340;&#25915;&#20987;&#12290;&#20256;&#32479;&#30340;&#32479;&#35745;&#21644;&#27973;&#23618;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#22522;&#20110;&#27973;&#23618;&#25968;&#25454;&#21644;&#29305;&#24449;&#36873;&#25321;&#26816;&#27979;&#34920;&#38754;&#24322;&#24120;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#26816;&#27979;&#21040;&#26410;&#35265;&#36807;&#30340;DDoS&#25915;&#20987;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LSTM-Autoencoder&#65288;LSTM-AE&#65289;&#30340;&#22522;&#20110;&#37325;&#26500;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#20004;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;DDoS&#25915;&#20987;&#24322;&#24120;&#12290;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#25552;&#20986;&#32467;&#26500;&#25552;&#20379;&#20102;&#21333;&#20803;&#65292;&#36825;&#20123;&#21333;&#20803;&#24444;&#27492;&#21327;&#20316;&#26469;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#24207;&#21015;&#20869;&#25968;&#25454;&#30340;&#38271;&#26399;&#30701;&#26399;&#30456;&#20851;&#24615;&#12290;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#22522;&#20110;&#22312;&#27599;&#20010;&#26679;&#26412;&#19978;&#35780;&#20272;&#30340;&#37325;&#26500;&#35823;&#24046;&#29575;&#26469;&#35782;&#21035;&#26368;&#20339;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Distributed Denial-of-service (DDoS) attack is a malicious attempt to disrupt the regular traffic of a targeted server, service, or network by sending a flood of traffic to overwhelm the target or its surrounding infrastructure. As technology improves, new attacks have been developed by hackers. Traditional statistical and shallow machine learning techniques can detect superficial anomalies based on shallow data and feature selection, however, these approaches cannot detect unseen DDoS attacks. In this context, we propose a reconstruction-based anomaly detection model named LSTM-Autoencoder (LSTM-AE) which combines two deep learning-based models for detecting DDoS attack anomalies. The proposed structure of long short-term memory (LSTM) networks provides units that work with each other to learn the long short-term correlation of data within a time series sequence. Autoencoders are used to identify the optimal threshold based on the reconstruction error rates evaluated on each sample 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22270;&#24418;&#37325;&#26032;&#37319;&#26679;&#21644;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20840;&#29699;&#28023;&#34920;&#28201;&#24230;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#25345;&#20037;&#24615;&#27169;&#22411;&#65292;GNN &#22312;&#22823;&#22810;&#25968;&#28023;&#27915;&#20013;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#25552;&#21069;&#19968;&#20010;&#26376;&#30340; SST &#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09468</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#28023;&#34920;&#28201;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Deep Learning for Sea Surface Temperature Forecasts. (arXiv:2305.09468v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09468
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22270;&#24418;&#37325;&#26032;&#37319;&#26679;&#21644;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20840;&#29699;&#28023;&#34920;&#28201;&#24230;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#25345;&#20037;&#24615;&#27169;&#22411;&#65292;GNN &#22312;&#22823;&#22810;&#25968;&#28023;&#27915;&#20013;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#25552;&#21069;&#19968;&#20010;&#26376;&#30340; SST &#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#34920;&#28201;&#24230; (SST) &#39044;&#27979;&#26377;&#21161;&#20110;&#31649;&#29702;&#21463;&#20154;&#31867;&#27668;&#20505;&#21464;&#21270;&#24433;&#21709;&#30340;&#28023;&#27915;&#29983;&#24577;&#31995;&#32479;&#21644;&#27700;&#20135;&#20859;&#27542;&#12290;&#25968;&#20540;&#21160;&#21147;&#27169;&#22411;&#23545;&#20110; SST &#39044;&#27979;&#20855;&#26377;&#36164;&#28304;&#23494;&#38598;&#22411;&#65307;&#26426;&#22120;&#23398;&#20064; (ML) &#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#39640;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#19988;&#26368;&#36817;&#19968;&#30452;&#26159;&#30740;&#31350;&#30028;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;ML &#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#29615;&#22659;&#25968;&#25454;&#26159;&#22312;&#23450;&#26399;&#38388;&#38548;&#30340;&#32593;&#26684;&#19978;&#25910;&#38598;&#30340;&#65292;&#22240;&#27492;&#26089;&#26399;&#30340;&#24037;&#20316;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#32593;&#26684;&#30340;&#28145;&#24230;&#23398;&#20064; (DL) &#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#32593;&#26684;&#25968;&#25454;&#21644;&#30456;&#24212;&#30340; DL &#26041;&#27861;&#37117;&#23384;&#22312;&#22266;&#26377;&#38382;&#39064;&#12290;&#38543;&#30528;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#23558;&#22270;&#24418;&#20316;&#20026;&#26356;&#24191;&#20041;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476; (GNNs) &#24341;&#20837;&#21040;&#26102;&#31354;&#22495;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21021;&#27493;&#25506;&#35752;&#20102;&#22270;&#24418;&#37325;&#26032;&#37319;&#26679;&#21644; GNN &#29992;&#20110;&#20840;&#29699; SST &#39044;&#27979;&#65292;&#32780; GNN &#22312;&#22823;&#22810;&#25968;&#28023;&#27915;&#20013;&#30456;&#23545;&#20110;&#25345;&#20037;&#24615;&#27169;&#22411;&#22312; RMSE &#21644; MAE &#26041;&#38754;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#25552;&#21069;&#19968;&#20010;&#26376;&#30340; SST &#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sea surface temperature (SST) forecasts help with managing the marine ecosystem and the aquaculture impacted by anthropogenic climate change. Numerical dynamical models are resource intensive for SST forecasts; machine learning (ML) models could reduce high computational requirements and have been in the focus of the research community recently. ML models normally require a large amount of data for training. Environmental data are collected on regularly-spaced grids, so early work mainly used grid-based deep learning (DL) for prediction. However, both grid data and the corresponding DL approaches have inherent problems. As geometric DL has emerged, graphs as a more generalized data structure and graph neural networks (GNNs) have been introduced to the spatiotemporal domains. In this work, we preliminarily explored graph re-sampling and GNNs for global SST forecasts, and GNNs show better one month ahead SST prediction than the persistence model in most oceans in terms of root mean squar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#21475;&#30340;MARL&#35757;&#32451;&#31649;&#32447;&#20197;&#21450;&#36229;&#21442;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#26234;&#33021;&#20307;&#36275;&#29699;&#22330;&#26223;&#65292;&#20174;&#38646;&#24320;&#22987;&#22312;200&#19975;&#27493;&#20869;&#25171;&#36133;&#20102;&#38590;&#24230;&#20026;1.0&#30340;&#26426;&#22120;&#20154;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#35757;&#32451;&#26694;&#26550;Light-MALib&#12290;</title><link>http://arxiv.org/abs/2305.09458</link><description>&lt;p&gt;
&#35895;&#27468;&#30740;&#31350;&#36275;&#29699;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Google Research Football Multi-agent Scenarios. (arXiv:2305.09458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#21475;&#30340;MARL&#35757;&#32451;&#31649;&#32447;&#20197;&#21450;&#36229;&#21442;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#26234;&#33021;&#20307;&#36275;&#29699;&#22330;&#26223;&#65292;&#20174;&#38646;&#24320;&#22987;&#22312;200&#19975;&#27493;&#20869;&#25171;&#36133;&#20102;&#38590;&#24230;&#20026;1.0&#30340;&#26426;&#22120;&#20154;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#35757;&#32451;&#26694;&#26550;Light-MALib&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#30740;&#31350;&#35895;&#27468;&#30740;&#31350;&#36275;&#29699;&#65288;GRF&#65289;&#19978;&#30340;11v11&#22810;&#26234;&#33021;&#20307;&#65288;MARL&#65289;&#22330;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#36739;&#23569;&#34987;&#20851;&#27880;&#30340;&#35838;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#21475;&#30340;MARL&#35757;&#32451;&#31649;&#32447;&#20197;&#21450;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#26234;&#33021;&#20307;&#36275;&#29699;&#22330;&#26223;&#65292;&#20174;&#38646;&#24320;&#22987;&#22312;200&#19975;&#27493;&#20869;&#23601;&#33021;&#20248;&#20110;&#38590;&#24230;&#20026;1.0&#30340;&#26426;&#22120;&#20154;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#24320;&#28304;&#20102;&#35757;&#32451;&#26694;&#26550;Light-MALib&#65292;&#25193;&#23637;&#20102;MALib&#20195;&#30721;&#24211;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#21644;&#24322;&#27493;&#23454;&#29616;&#20197;&#21450;&#36275;&#29699;&#28216;&#25103;&#30340;&#38468;&#21152;&#20998;&#26512;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#25351;&#23548;&#26426;&#22120;&#20154;&#36739;&#22909;&#34920;&#29616;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few multi-agent reinforcement learning (MARL) research on Google Research Football (GRF) focus on the 11v11 multi-agent full-game scenario and to the best of our knowledge, no open benchmark on this scenario has been released to the public. In this work, we fill the gap by providing a population-based MARL training pipeline and hyperparameter settings on multi-agent football scenario that outperforms the bot with difficulty 1.0 from scratch within 2 million steps. Our experiments serve as a reference for the expected performance of Independent Proximal Policy Optimization (IPPO), a state-of-the-art multi-agent reinforcement learning algorithm where each agent tries to maximize its own policy independently across various training configurations. Meanwhile, we open-source our training framework Light-MALib which extends the MALib codebase by distributed and asynchronized implementation with additional analytical tools for football games. Finally, we provide guidance for building strong f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38477;&#32500;&#30340;&#32534;&#36753;&#21521;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#12289;&#39640;&#31934;&#24230;&#30340;&#35821;&#20041;&#22270;&#20687;&#32534;&#36753;&#65292;&#19981;&#38656;&#35201;&#35821;&#20041;&#20998;&#21106;&#25110;&#21487;&#24494;&#20998;&#30340;&#29305;&#24449;&#20272;&#35745;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2305.09454</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#32534;&#36753;: &#19968;&#31181;&#22522;&#20110;&#38477;&#32500;&#30340;&#32534;&#36753;&#21521;&#37327;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking the editing of generative adversarial networks: a method to estimate editing vectors based on dimension reduction. (arXiv:2305.09454v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38477;&#32500;&#30340;&#32534;&#36753;&#21521;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#12289;&#39640;&#31934;&#24230;&#30340;&#35821;&#20041;&#22270;&#20687;&#32534;&#36753;&#65292;&#19981;&#38656;&#35201;&#35821;&#20041;&#20998;&#21106;&#25110;&#21487;&#24494;&#20998;&#30340;&#29305;&#24449;&#20272;&#35745;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#22270;&#20687;&#32534;&#36753;&#26041;&#38754;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;GAN-based&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#22823;&#22810;&#38656;&#35201;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#20998;&#21106;&#27880;&#37322;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#21482;&#33021;&#25552;&#20379;&#39640;&#32423;&#21035;&#25511;&#21046;&#25110;&#20165;&#22312;&#19981;&#21516;&#22270;&#20687;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#32773;&#36890;&#36807;&#26597;&#25214;&#8220;&#32534;&#36753;&#21521;&#37327;&#8221;&#25552;&#20986;&#20102;EditGAN&#65292;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#12289;&#39640;&#31934;&#24230;&#30340;&#35821;&#20041;&#22270;&#20687;&#32534;&#36753;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#30340;&#35821;&#20041;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#35768;&#22810;&#19982;&#35821;&#20041;&#20851;&#32852;&#19981;&#39640;&#30340;&#29305;&#24449;&#65292;&#23548;&#33268;EditGAN&#21487;&#33021;&#22312;&#36825;&#20123;&#29305;&#24449;&#19978;&#22833;&#36133;&#12290;&#26412;&#25991;&#22522;&#20110;EditGAN&#25152;&#35266;&#23519;&#21040;&#30340;&#28508;&#31354;&#38388;&#27491;&#20132;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#32534;&#36753;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#35813;&#27861;&#19981;&#20381;&#36182;&#20110;&#35821;&#20041;&#20998;&#21106;&#25110;&#21487;&#24494;&#20998;&#30340;&#29305;&#24449;&#20272;&#35745;&#32593;&#32476;&#12290;&#26412;&#26041;&#27861;&#20551;&#35774;&#29305;&#24449;&#30340;&#24378;&#24230;&#20998;&#24067;&#19982;&#38544;&#34255;&#21521;&#37327;&#30340;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#26469;&#20272;&#35745;&#19978;&#36848;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Generative Adversarial Networks (GANs) have recently found applications in image editing, most previous GAN-based image editing methods require largescale datasets with semantic segmentation annotations for training, only provide high level control, or merely interpolate between different images. Previous researchers have proposed EditGAN for high-quality, high-precision semantic image editing with limited semantic annotations by finding `editing vectors'. However, it is noticed that there are many features that are not highly associated with semantics, and EditGAN may fail on them. Based on the orthogonality of latent space observed by EditGAN, we propose a method to estimate editing vectors that do not rely on semantic segmentation nor differentiable feature estimation network. Our method assumes that there is a correlation between the intensity distribution of features and the distribution of hidden vectors, and estimates the relationship between the above distributions by sam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#20998;&#25968;&#36716;&#21270;&#20026;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#20272;&#35745;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19982;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#36317;&#31163;&#24314;&#27169;&#36317;&#31163;&#27010;&#29575;&#20998;&#24067;&#65292;&#23558;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#20998;&#25968;&#36716;&#25442;&#20026;&#24322;&#24120;&#27010;&#29575;&#65292;&#25552;&#39640;&#20102;&#27491;&#24120;&#28857;&#21644;&#24322;&#24120;&#28857;&#20043;&#38388;&#30340;&#23545;&#27604;&#24230;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09446</link><description>&lt;p&gt;
&#27010;&#29575;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Distance-Based Outlier Detection. (arXiv:2305.09446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#20998;&#25968;&#36716;&#21270;&#20026;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#20272;&#35745;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19982;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#36317;&#31163;&#24314;&#27169;&#36317;&#31163;&#27010;&#29575;&#20998;&#24067;&#65292;&#23558;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#20998;&#25968;&#36716;&#25442;&#20026;&#24322;&#24120;&#27010;&#29575;&#65292;&#25552;&#39640;&#20102;&#27491;&#24120;&#28857;&#21644;&#24322;&#24120;&#28857;&#20043;&#38388;&#30340;&#23545;&#27604;&#24230;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20998;&#25968;&#38590;&#20197;&#35299;&#37322;&#65292;&#22240;&#27492;&#22312;&#27809;&#26377;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#30830;&#23450;&#27491;&#24120;&#28857;&#21644;&#24322;&#24120;&#28857;&#20043;&#38388;&#30340;&#25130;&#26029;&#38408;&#20540;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#20998;&#25968;&#36716;&#21270;&#20026;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#20272;&#35745;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#35813;&#36716;&#25442;&#26159;&#25490;&#21517;&#31283;&#23450;&#30340;&#65292;&#24182;&#22686;&#21152;&#20102;&#27491;&#24120;&#28857;&#21644;&#24322;&#24120;&#28857;&#20043;&#38388;&#30340;&#23545;&#27604;&#24230;&#12290;&#30830;&#23450;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#20851;&#31995;&#26159;&#35782;&#21035;&#25968;&#25454;&#20013;&#26368;&#36817;&#37051;&#20851;&#31995;&#25152;&#24517;&#38656;&#30340;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#35745;&#31639;&#20986;&#30340;&#36317;&#31163;&#36890;&#24120;&#34987;&#20002;&#24323;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#19982;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#36317;&#31163;&#26469;&#24314;&#27169;&#36317;&#31163;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#38543;&#21518;&#20351;&#29992;&#36825;&#20123;&#20998;&#24067;&#23558;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#20998;&#25968;&#36716;&#25442;&#20026;&#24322;&#24120;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#27010;&#29575;&#36716;&#25442;&#19981;&#20250;&#24433;&#21709;&#20247;&#22810;&#34920;&#26684;&#21644;&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#20294;&#20250;&#20135;&#29983;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scores of distance-based outlier detection methods are difficult to interpret, making it challenging to determine a cut-off threshold between normal and outlier data points without additional context. We describe a generic transformation of distance-based outlier scores into interpretable, probabilistic estimates. The transformation is ranking-stable and increases the contrast between normal and outlier data points. Determining distance relationships between data points is necessary to identify the nearest-neighbor relationships in the data, yet, most of the computed distances are typically discarded. We show that the distances to other data points can be used to model distance probability distributions and, subsequently, use the distributions to turn distance-based outlier scores into outlier probabilities. Our experiments show that the probabilistic transformation does not impact detection performance over numerous tabular and image benchmark datasets but results in interpretable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;MPI-rical&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#35757;&#32451;&#23454;&#29616;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#24182;&#34892;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09438</link><description>&lt;p&gt;
MPI-rical&#65306;&#22522;&#20110;Transformer&#30340;&#25968;&#25454;&#39537;&#21160;MPI&#20998;&#24067;&#24335;&#24182;&#34892;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;MPI-rical&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#35757;&#32451;&#23454;&#29616;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#24182;&#34892;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#65292;&#23558;&#20018;&#34892;&#20195;&#30721;&#33258;&#21160;&#24182;&#34892;&#21270;&#20197;&#25903;&#25345;&#20849;&#20139;&#20869;&#23384;&#21644;&#20998;&#24067;&#24335;&#20869;&#23384;&#31995;&#32479;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#35768;&#22810;&#23581;&#35797;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#20849;&#20139;&#20869;&#23384;&#29615;&#22659;&#30340;&#24182;&#34892;&#20195;&#30721;&#65288;&#36890;&#24120;&#20351;&#29992;OpenMP&#65289;&#65292;&#20294;&#27809;&#26377;&#20219;&#20309;&#19968;&#39033;&#23581;&#35797;&#25104;&#21151;&#23558;&#20854;&#36716;&#21270;&#20026;&#20998;&#24067;&#24335;&#20869;&#23384;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MPI-rical&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;Transformer&#27169;&#22411;&#23545;&#22823;&#32422;25,000&#20010;&#20018;&#34892;&#20195;&#30721;&#29255;&#27573;&#21450;&#20854;&#23545;&#24212;&#30340;&#24182;&#34892;MPI&#20195;&#30721;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#65288;MPICodeCorpus&#65289;&#30340;50,000&#22810;&#20010;&#20195;&#30721;&#29255;&#27573;&#20013;&#29983;&#25104;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#20195;&#30721;&#32763;&#35793;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#21046;&#23450;&#20004;&#20010;&#30740;&#31350;&#30446;&#26631;&#65306;&#20195;&#30721;&#34917;&#20840;&#65292;&#21363;&#22312;&#32473;&#23450;&#28304;&#20195;&#30721;&#20013;&#30340;&#26576;&#20010;&#20301;&#32622;&#65292;&#39044;&#27979;&#35813;&#20301;&#32622;&#30340;MPI&#20989;&#25968;&#65307;&#20195;&#30721;&#32763;&#35793;&#65292;&#21363;&#39044;&#27979;&#19968;&#20010;MPI&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic source-to-source parallelization of serial code for shared and distributed memory systems is a challenging task in high-performance computing. While many attempts were made to translate serial code into parallel code for a shared memory environment (usually using OpenMP), none has managed to do so for a distributed memory environment. In this paper, we propose a novel approach, called MPI-rical, for automated MPI code generation using a transformer-based model trained on approximately 25,000 serial code snippets and their corresponding parallelized MPI code out of more than 50,000 code snippets in our corpus (MPICodeCorpus). To evaluate the performance of the model, we first break down the serial code to MPI-based parallel code translation problem into two sub-problems and develop two research objectives: code completion defined as given a location in the source code, predict the MPI function for that location, and code translation defined as predicting an MPI function as wel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#39046;&#22495;&#20013;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26426;&#21046;&#65292;&#21253;&#25324;&#33258;&#28982;&#22810;&#20219;&#21153;&#12289;&#36755;&#20986;&#20316;&#20026;&#36755;&#20837;&#21644;&#20351;&#29992;&#39069;&#22806;&#25439;&#22833;&#20989;&#25968;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#20363;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.09425</link><description>&lt;p&gt;
&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#38382;&#39064;&#20309;&#26102;&#25104;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
When is an SHM problem a Multi-Task-Learning problem?. (arXiv:2305.09425v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#39046;&#22495;&#20013;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26426;&#21046;&#65292;&#21253;&#25324;&#33258;&#28982;&#22810;&#20219;&#21153;&#12289;&#36755;&#20986;&#20316;&#20026;&#36755;&#20837;&#21644;&#20351;&#29992;&#39069;&#22806;&#25439;&#22833;&#20989;&#25968;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#20197;&#25552;&#39640;&#21333;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65288;SHM&#65289;&#39046;&#22495;&#20013;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#19977;&#31181;&#26426;&#21046;: (i) &#33258;&#28982;&#23384;&#22312;&#30340;&#22810;&#20010;&#20219;&#21153;&#65307;(ii) &#23558;&#36755;&#20986;&#29992;&#20316;&#36755;&#20837;&#65288;&#19982;&#26368;&#26032;&#30340;&#22522;&#20110;&#32676;&#20307;&#30340;SHM&#65288;PBSHM&#65289;&#30340;&#30740;&#31350;&#30456;&#20851;&#65289;&#65307;&#20197;&#21450; (iii) &#20351;&#29992;&#39069;&#22806;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#19981;&#21516;&#30340;&#35265;&#35299;&#12290;&#35814;&#32454;&#20171;&#32461;&#20102;MTL&#30340;&#27599;&#20010;&#38382;&#39064;&#35774;&#32622;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task neural networks learn tasks simultaneously to improve individual task performance. There are three mechanisms of multi-task learning (MTL) which are explored here for the context of structural health monitoring (SHM): (i) the natural occurrence of multiple tasks; (ii) using outputs as inputs (both linked to the recent research in population-based SHM (PBSHM)); and, (iii) additional loss functions to provide different insights. Each of these problem settings for MTL is detailed and an example is given.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#24320;ReLU&#32593;&#32476;&#30340;&#31192;&#23494;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#20998;&#35299;&#25104;&#32447;&#24615;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#21367;&#31215;&#32593;&#32476;&#31561;&#32467;&#26500;&#30340;&#25193;&#23637;&#65292;&#24182;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#29702;&#35299;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;SHAP&#20540;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09424</link><description>&lt;p&gt;
&#35299;&#24320;&#25152;&#26377;ReLU&#32593;&#32476;&#30340;&#31192;&#23494;
&lt;/p&gt;
&lt;p&gt;
Unwrapping All ReLU Networks. (arXiv:2305.09424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#24320;ReLU&#32593;&#32476;&#30340;&#31192;&#23494;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#20998;&#35299;&#25104;&#32447;&#24615;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#21367;&#31215;&#32593;&#32476;&#31561;&#32467;&#26500;&#30340;&#25193;&#23637;&#65292;&#24182;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#29702;&#35299;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;SHAP&#20540;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;ReLU&#32593;&#32476;&#21487;&#20197;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#32447;&#24615;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#22312;&#36755;&#20837;&#31354;&#38388;&#30340;&#19968;&#20010;&#21306;&#22495;&#20869;&#23450;&#20041;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19977;&#20010;&#32467;&#26524;&#26469;&#25193;&#23637;&#36825;&#20010;&#29702;&#35770;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#32447;&#24615;&#20998;&#35299;&#25193;&#23637;&#21040;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#21367;&#31215;&#32593;&#32476;&#65292;&#20197;&#21450;&#20855;&#26377;&#20056;&#27861;&#20132;&#20114;&#30340;&#32593;&#32476;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#29702;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#22810;&#20803;&#20915;&#31574;&#26641;&#21644;&#36923;&#36753;&#29702;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#31181;&#27169;&#22411;&#26469;&#35745;&#31639;&#20415;&#23452;&#19988;&#20934;&#30830;&#30340;SHAP&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ReLU Networks can be decomposed into a collection of linear models, each defined in a region of a partition of the input space. This paper provides three results extending this theory. First, we extend this linear decompositions to Graph Neural networks and tensor convolutional networks, as well as networks with multiplicative interactions. Second, we provide proofs that neural networks can be understood as interpretable models such as Multivariate Decision trees and logical theories. Finally, we show how this model leads to computing cheap and exact SHAP values. We validate the theory through experiments with on Graph Neural Networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;SHAP&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#27169;&#31946;&#35748;&#30693;&#22320;&#22270;&#27169;&#22411;&#65292;&#23545;&#38544;&#24615;&#20559;&#35265;&#36827;&#34892;&#27979;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#29305;&#24449;&#37325;&#35201;&#24615;&#20316;&#20026;&#32477;&#23545;&#24037;&#20855;&#19981;&#36866;&#24212;&#20110;&#27979;&#37327;&#38544;&#24615;&#20559;&#35265;&#65292;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#20559;&#35265;&#25968;&#37327;&#21487;&#33021;&#22240;&#29305;&#24449;&#26159;&#25968;&#20540;&#32534;&#30721;&#36824;&#26159;&#20998;&#31867;&#32534;&#30721;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2305.09399</link><description>&lt;p&gt;
&#20351;&#29992;SHAP&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#27169;&#31946;&#35748;&#30693;&#22320;&#22270;&#27979;&#37327;&#38544;&#24615;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Measuring Implicit Bias Using SHAP Feature Importance and Fuzzy Cognitive Maps. (arXiv:2305.09399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;SHAP&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#27169;&#31946;&#35748;&#30693;&#22320;&#22270;&#27169;&#22411;&#65292;&#23545;&#38544;&#24615;&#20559;&#35265;&#36827;&#34892;&#27979;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#29305;&#24449;&#37325;&#35201;&#24615;&#20316;&#20026;&#32477;&#23545;&#24037;&#20855;&#19981;&#36866;&#24212;&#20110;&#27979;&#37327;&#38544;&#24615;&#20559;&#35265;&#65292;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#20559;&#35265;&#25968;&#37327;&#21487;&#33021;&#22240;&#29305;&#24449;&#26159;&#25968;&#20540;&#32534;&#30721;&#36824;&#26159;&#20998;&#31867;&#32534;&#30721;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#29305;&#24449;&#37325;&#35201;&#24615;&#27010;&#24565;&#19982;&#27169;&#24335;&#20998;&#31867;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#19977;&#27493;&#26041;&#27861;&#23454;&#29616;&#65306;&#65288;i&#65289;&#26500;&#24314;&#19968;&#20010;&#20998;&#31867;&#22120;&#21644;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#65292;&#65288;ii&#65289;&#26500;&#24314;&#19968;&#20010;&#33021;&#37327;&#21270;&#38544;&#24615;&#20559;&#35265;&#30340;&#27169;&#31946;&#35748;&#30693;&#22320;&#22270;&#27169;&#22411;&#65292;&#65288;iii&#65289;&#20351;&#29992;SHAP&#29305;&#24449;&#37325;&#35201;&#24615;&#22312;&#27169;&#25311;&#20013;&#28608;&#27963;&#31070;&#32463;&#20803;&#27010;&#24565;&#12290;&#20197;&#20851;&#20110;&#20844;&#24179;&#30740;&#31350;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#21452;&#37325;&#20551;&#35774;&#12290;&#19968;&#26041;&#38754;&#65292;&#38416;&#26126;&#20102;&#20351;&#29992;&#29305;&#24449;&#37325;&#35201;&#24615;&#20316;&#20026;&#32477;&#23545;&#24037;&#20855;&#26469;&#34913;&#37327;&#38544;&#24615;&#20559;&#35265;&#30340;&#39118;&#38505;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24471;&#20986;&#32467;&#35770;&#65306;&#23545;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#20559;&#35265;&#25968;&#37327;&#21487;&#33021;&#22240;&#29305;&#24449;&#26159;&#25968;&#20540;&#32534;&#30721;&#36824;&#26159;&#20998;&#31867;&#32534;&#30721;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we integrate the concepts of feature importance with implicit bias in the context of pattern classification. This is done by means of a three-step methodology that involves (i) building a classifier and tuning its hyperparameters, (ii) building a Fuzzy Cognitive Map model able to quantify implicit bias, and (iii) using the SHAP feature importance to active the neural concepts when performing simulations. The results using a real case study concerning fairness research support our two-fold hypothesis. On the one hand, it is illustrated the risks of using a feature importance method as an absolute tool to measure implicit bias. On the other hand, it is concluded that the amount of bias towards protected features might differ depending on whether the features are numerically or categorically encoded.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#23616;&#37096;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#19968;&#33268;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#38750;&#24120;&#24369;&#30340;&#26465;&#20214;&#19979;&#65292;&#23427;&#20204;&#20174;&#20840;&#23616;SVM&#32487;&#25215;&#20102;$L_p$&#21644;&#39118;&#38505;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#24213;&#23618;&#21306;&#22495;&#38543;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.09385</link><description>&lt;p&gt;
&#23616;&#37096;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;$L_p$&#21644;&#39118;&#38505;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Lp- and Risk Consistency of Localized SVMs. (arXiv:2305.09385v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#23616;&#37096;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#19968;&#33268;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#38750;&#24120;&#24369;&#30340;&#26465;&#20214;&#19979;&#65292;&#23427;&#20204;&#20174;&#20840;&#23616;SVM&#32487;&#25215;&#20102;$L_p$&#21644;&#39118;&#38505;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#24213;&#23618;&#21306;&#22495;&#38543;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26680;&#30340;&#27491;&#21017;&#21270;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#65292;&#21448;&#31216;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#24050;&#30693;&#20855;&#26377;&#35768;&#22810;&#29702;&#24819;&#30340;&#23646;&#24615;&#65292;&#20294;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#20855;&#26377;&#36229;&#32447;&#24615;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;SVM&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#36755;&#20837;&#31354;&#38388;&#21306;&#22495;&#24212;&#29992;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#39069;&#22806;&#20248;&#21183;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#23616;&#37096;SVM&#30340;&#19968;&#33268;&#24615;&#12290;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#38750;&#24120;&#24369;&#30340;&#24773;&#20917;&#19979;&#20174;&#20840;&#23616;SVM&#32487;&#25215;&#20102;$L_p$-&#20197;&#21450;&#39118;&#38505;-&#19968;&#33268;&#24615;&#65292;&#29978;&#33267;&#21487;&#20197;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#22686;&#21152;&#26102;&#65292;&#20801;&#35768;&#24213;&#23618;&#30340;&#21306;&#22495;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel-based regularized risk minimizers, also called support vector machines (SVMs), are known to possess many desirable properties but suffer from their super-linear computational requirements when dealing with large data sets. This problem can be tackled by using localized SVMs instead, which also offer the additional advantage of being able to apply different hyperparameters to different regions of the input space. In this paper, localized SVMs are analyzed with regards to their consistency. It is proven that they inherit $L_p$- as well as risk consistency from global SVMs under very weak conditions and even if the regions underlying the localized SVMs are allowed to change as the size of the training data set increases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#22270;&#20687;&#30340;&#24635;&#20307;&#32654;&#23398;&#35780;&#20998;&#21644;&#32654;&#23398;&#23646;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#25509;&#36817;&#20154;&#31867;&#34920;&#29616;&#30340;&#25972;&#20307;&#32654;&#23398;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.09373</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22270;&#20687;&#32654;&#23398;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multi-task convolutional neural network for image aesthetic assessment. (arXiv:2305.09373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#22270;&#20687;&#30340;&#24635;&#20307;&#32654;&#23398;&#35780;&#20998;&#21644;&#32654;&#23398;&#23646;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#25509;&#36817;&#20154;&#31867;&#34920;&#29616;&#30340;&#25972;&#20307;&#32654;&#23398;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#20204;&#23545;&#22270;&#20687;&#32654;&#23398;&#20559;&#22909;&#30340;&#29702;&#35299;&#36824;&#36828;&#36828;&#19981;&#22815;&#65292;&#22270;&#20687;&#32654;&#23398;&#35780;&#20272;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#32771;&#34385;&#20102;&#24433;&#21709;&#22270;&#20687;&#32654;&#23398;&#30340;&#22240;&#32032;&#12290;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#21516;&#26102;&#23398;&#20064;&#20102;&#22270;&#20687;&#30340;&#24635;&#20307;&#32654;&#23398;&#35780;&#20998;&#20197;&#21450;&#36825;&#20123;&#24050;&#30693;&#32654;&#23398;&#23646;&#24615;&#12290;&#36825;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#36890;&#36807;&#20849;&#20139;&#34920;&#31034;&#23454;&#29616;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#22270;&#20687;&#32654;&#23398;&#30340;&#25972;&#20307;&#35780;&#20998;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#22312;&#32771;&#34385;Spearman&#31561;&#32423;&#30456;&#20851;&#24615;&#26102;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#20154;&#31867;&#34920;&#29616;&#30340;&#25972;&#20307;&#32654;&#23398;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21478;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36824;&#36890;&#36807;&#39044;&#27979;&#22270;&#20687;&#30340;&#29305;&#23450;&#32654;&#23398;&#23646;&#24615;&#24320;&#21019;&#20102;&#22810;&#20219;&#21153;&#24212;&#29992;&#30340;&#20808;&#27827;&#12290;
&lt;/p&gt;
&lt;p&gt;
As people's aesthetic preferences for images are far from understood, image aesthetic assessment is a challenging artificial intelligence task. The range of factors underlying this task is almost unlimited, but we know that some aesthetic attributes affect those preferences. In this study, we present a multi-task convolutional neural network that takes into account these attributes. The proposed neural network jointly learns the attributes along with the overall aesthetic scores of images. This multi-task learning framework allows for effective generalization through the utilization of shared representations. Our experiments demonstrate that the proposed method outperforms the state-of-the-art approaches in predicting overall aesthetic scores for images in one benchmark of image aesthetics. We achieve near-human performance in terms of overall aesthetic scores when considering the Spearman's rank correlations. Moreover, our model pioneers the application of multi-tasking in another ben
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23545;&#23156;&#20799;&#36816;&#21160;&#20998;&#31867;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#25552;&#39640;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#33258;&#21160;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09366</link><description>&lt;p&gt;
&#20329;&#25140;&#36816;&#21160;&#20256;&#24863;&#22120;&#33258;&#21160;&#20998;&#31867;&#23156;&#20799;&#36816;&#21160;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of self-supervised pre-training for automatic infant movement classification using wearable movement sensors. (arXiv:2305.09366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23545;&#23156;&#20799;&#36816;&#21160;&#20998;&#31867;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#25552;&#39640;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#33258;&#21160;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;&#23156;&#24188;&#20799;&#21487;&#31359;&#25140;&#35774;&#22791;MAIJU&#20026;&#36275;&#19981;&#20986;&#25143;&#30340;&#23156;&#20799;&#36816;&#21160;&#35780;&#20272;&#25552;&#20379;&#20102;&#19968;&#31181;&#23458;&#35266;&#21644;&#21487;&#25193;&#23637;&#30340;&#25163;&#27573;&#12290;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#20110;&#21457;&#23637;&#24615;&#30740;&#31350;&#65292;&#24182;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#12290;MAIJU&#30340;&#20998;&#26512;&#23436;&#20840;&#20381;&#36182;&#20110;&#23156;&#20799;&#23039;&#21183;&#21644;&#36816;&#21160;&#30340;&#20998;&#31867;&#65307;&#22240;&#27492;&#65292;&#30740;&#31350;&#22686;&#21152;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#26041;&#24335;&#26159;&#24517;&#35201;&#30340;&#65292;&#26088;&#22312;&#22686;&#21152;&#33258;&#21160;&#20998;&#26512;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#25552;&#39640;&#23545;&#20998;&#26512;MAIJU&#35760;&#24405;&#25152;&#20351;&#29992;&#30340;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#20998;&#31867;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#36873;&#25321;&#24615;&#31579;&#36873;&#30340;&#24433;&#21709;&#65292;&#20197;&#25490;&#38500;&#23156;&#20799;&#36816;&#21160;&#36739;&#23567;&#25110;&#20256;&#24863;&#22120;&#32570;&#22833;&#30340;&#26102;&#38388;&#27573;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65306;i&#65289;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
The recently-developed infant wearable MAIJU provides a means to automatically evaluate infants' motor performance in an objective and scalable manner in out-of-hospital settings. This information could be used for developmental research and to support clinical decision-making, such as detection of developmental problems and guiding of their therapeutic interventions. MAIJU-based analyses rely fully on the classification of infant's posture and movement; it is hence essential to study ways to increase the accuracy of such classifications, aiming to increase the reliability and robustness of the automated analysis. Here, we investigated how self-supervised pre-training improves performance of the classifiers used for analyzing MAIJU recordings, and we studied whether performance of the classifier models is affected by context-selective quality-screening of pre-training data to exclude periods of little infant movement or with missing sensors. Our experiments show that i) pre-training th
&lt;/p&gt;</description></item><item><title>&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36234;&#26469;&#36234;&#24341;&#36215;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#29992;&#25143;&#20844;&#24179;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#36731;&#29992;&#25143;&#22312;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#36807;&#31243;&#20013;&#20307;&#39564;&#21040;&#30340;&#27495;&#35270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09330</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#20844;&#24179;&#24615;: &#26041;&#27861;&#21644;&#35780;&#20272;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Consumer-side Fairness in Recommender Systems: A Systematic Survey of Methods and Evaluation. (arXiv:2305.09330v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09330
&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#36234;&#26469;&#36234;&#24341;&#36215;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#29992;&#25143;&#20844;&#24179;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#36731;&#29992;&#25143;&#22312;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#36807;&#31243;&#20013;&#20307;&#39564;&#21040;&#30340;&#27495;&#35270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#27700;&#24179;&#19981;&#26029;&#25552;&#39640;&#30340;&#24403;&#21069;&#31038;&#20250;&#20013;&#65292;&#38754;&#20020;&#30528;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#24110;&#21161;&#29992;&#25143;&#23548;&#33322;&#26085;&#30410;&#22686;&#38271;&#30340;&#25968;&#25454;&#37327;&#65292;&#20197;&#21450;&#24110;&#21161;&#20379;&#24212;&#21830;&#21521;&#24863;&#20852;&#36259;&#30340;&#29992;&#25143;&#33829;&#38144;&#20135;&#21697;&#30340;&#24517;&#19981;&#21487;&#23569;&#30340;&#24037;&#20855;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#27495;&#35270;&#38382;&#39064;&#26085;&#30410;&#31361;&#20986;&#65292;&#36825;&#20419;&#20351;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30740;&#31350;&#22914;&#20309;&#30830;&#20445;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#32844;&#19994;&#25512;&#33616;&#20013;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#20307;&#29616;&#65292;&#21382;&#21490;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#25512;&#33616;&#31995;&#32479;&#23558;&#19968;&#20010;&#24615;&#21035;&#19982;&#36739;&#20302;&#30340;&#24037;&#36164;&#25110;&#21051;&#26495;&#21360;&#35937;&#32852;&#31995;&#36215;&#26469;&#12290;&#29305;&#21035;&#22320;&#65292;&#29992;&#25143;&#20844;&#24179;&#24615;&#20851;&#27880;&#22914;&#20309;&#20943;&#36731;&#29992;&#25143;&#22312;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#36807;&#31243;&#20013;&#20307;&#39564;&#21040;&#30340;&#27495;&#35270;&#38382;&#39064;&#65292;&#35813;&#39046;&#22495;&#24050;&#32463;&#20986;&#29616;&#20102;&#24456;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#27495;&#35270;&#12290;&#25152;&#36848;&#27495;&#35270;&#30340;&#24615;&#36136;&#21462;&#20915;&#20110;&#25152;&#22788;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current landscape of ever-increasing levels of digitalization, we are facing major challenges pertaining to scalability. Recommender systems have become irreplaceable both for helping users navigate the increasing amounts of data and, conversely, aiding providers in marketing products to interested users. The growing awareness of discrimination in machine learning methods has recently motivated both academia and industry to research how fairness can be ensured in recommender systems. For recommender systems, such issues are well exemplified by occupation recommendation, where biases in historical data may lead to recommender systems relating one gender to lower wages or to the propagation of stereotypes. In particular, consumer-side fairness, which focuses on mitigating discrimination experienced by users of recommender systems, has seen a vast number of diverse approaches for addressing different types of discrimination. The nature of said discrimination depends on the setting 
&lt;/p&gt;</description></item><item><title>OmniSafe&#26159;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#65292;&#29992;&#20110;&#21152;&#36895;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#65292;&#24110;&#21161;&#35299;&#20915;&#24403;&#20195;SafeRL&#30740;&#31350;&#29615;&#22659;&#20013;&#32570;&#20047;&#21327;&#35843;&#21644;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09304</link><description>&lt;p&gt;
OmniSafe&#65306;&#19968;&#31181;&#21152;&#36895;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#22522;&#30784;&#35774;&#26045;
&lt;/p&gt;
&lt;p&gt;
OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research. (arXiv:2305.09304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09304
&lt;/p&gt;
&lt;p&gt;
OmniSafe&#26159;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#65292;&#29992;&#20110;&#21152;&#36895;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#65292;&#24110;&#21161;&#35299;&#20915;&#24403;&#20195;SafeRL&#30740;&#31350;&#29615;&#22659;&#20013;&#32570;&#20047;&#21327;&#35843;&#21644;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36171;&#33021;&#30340;AI&#31995;&#32479;&#26377;&#30528;&#20419;&#36827;&#31038;&#20250;&#36827;&#27493;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#24120;&#24120;&#21463;&#21040;&#37325;&#22823;&#23433;&#20840;&#38544;&#24739;&#30340;&#38459;&#30861;&#12290;&#23588;&#20854;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#19981;&#21463;&#32422;&#26463;&#30340;RL&#20195;&#29702;&#30340;&#24847;&#22806;&#20260;&#23475;&#25110;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#25285;&#24551;&#12290;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;SafeRL&#65289;&#30340;&#29702;&#24565;&#26159;&#23558;RL&#20195;&#29702;&#19982;&#26080;&#23475;&#24847;&#22270;&#21644;&#23433;&#20840;&#34892;&#20026;&#27169;&#24335;&#30456;&#19968;&#33268;&#12290;&#22312;SafeRL&#20013;&#65292;&#20195;&#29702;&#36890;&#36807;&#20174;&#29615;&#22659;&#20013;&#25509;&#25910;&#21453;&#39304;&#26469;&#23398;&#20064;&#24320;&#21457;&#20248;&#21270;&#31574;&#30053;&#65292;&#21516;&#26102;&#20063;&#28385;&#36275;&#20102;&#23558;&#24847;&#22806;&#20260;&#23475;&#25110;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SafeRL&#31639;&#27861;&#23454;&#29616;&#30340;&#22797;&#26434;&#24615;&#65292;&#36328;&#36234;&#21508;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#30340;&#32467;&#21512;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#20102;&#24403;&#20195;SafeRL&#30740;&#31350;&#29615;&#22659;&#20013;&#32570;&#20047;&#19968;&#20010;&#21327;&#35843;&#21644;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;&#65306;OmniSafe&#65292;&#29992;&#20110;&#21152;&#36895;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI systems empowered by reinforcement learning (RL) algorithms harbor the immense potential to catalyze societal advancement, yet their deployment is often impeded by significant safety concerns. Particularly in safety-critical applications, researchers have raised concerns about unintended harms or unsafe behaviors of unaligned RL agents. The philosophy of safe reinforcement learning (SafeRL) is to align RL agents with harmless intentions and safe behavioral patterns. In SafeRL, agents learn to develop optimal policies by receiving feedback from the environment, while also fulfilling the requirement of minimizing the risk of unintended harm or unsafe behavior. However, due to the intricate nature of SafeRL algorithm implementation, combining methodologies across various domains presents a formidable challenge. This had led to an absence of a cohesive and efficacious learning framework within the contemporary SafeRL research milieu. In this work, we introduce a foundational framework d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35745;&#31639;&#26426;&#35270;&#35273;&#22330;&#26223;&#30340;&#36234;&#30028;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#36234;&#30028;&#26816;&#27979;&#22120;&#33258;&#36866;&#24212;&#30456;&#26426;&#21442;&#25968;&#65292;&#33021;&#20351;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#25351;&#26631; mAP&#12289;mAR &#21644; F1 &#24179;&#22343;&#25552;&#39640; 3-4 &#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.09293</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#35745;&#31639;&#26426;&#35270;&#35273;&#22330;&#26223;&#30340;&#36234;&#30028;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection for Adaptive Computer Vision. (arXiv:2305.09293v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09293
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35745;&#31639;&#26426;&#35270;&#35273;&#22330;&#26223;&#30340;&#36234;&#30028;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#36234;&#30028;&#26816;&#27979;&#22120;&#33258;&#36866;&#24212;&#30456;&#26426;&#21442;&#25968;&#65292;&#33021;&#20351;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#25351;&#26631; mAP&#12289;mAR &#21644; F1 &#24179;&#22343;&#25552;&#39640; 3-4 &#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#24403;&#36935;&#21040;&#20197;&#21069;&#26410;&#35265;&#30340;&#22270;&#20687;&#26465;&#20214;&#26102;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#21487;&#33021;&#20250;&#19981;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#36234;&#30028;&#26816;&#27979;&#22120;&#33258;&#36866;&#24212;&#30456;&#26426;&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;&#36827;&#34892;&#20102;&#19968;&#39033;&#23567;&#35268;&#27169;&#30340;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#26681;&#25454;&#36825;&#20010;&#36234;&#30028;&#26816;&#27979;&#22120;&#36866;&#24212;&#30456;&#26426;&#21442;&#25968;&#21487;&#20197;&#20351; YOLOv4 &#30446;&#26631;&#26816;&#27979;&#22120;&#30340; mAP&#12289;mAR &#21644; F1 &#24615;&#33021;&#25351;&#26631;&#24179;&#22343;&#25552;&#39640; 3 &#21040; 4 &#20010;&#30334;&#20998;&#28857;&#12290;&#20316;&#20026;&#27425;&#35201;&#32467;&#26524;&#65292;&#26412;&#25991;&#36824;&#34920;&#26126;&#21487;&#20197;&#22312; COCO &#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27491;&#21017;&#21270;&#27969;&#27169;&#22411;&#26469;&#36827;&#34892;&#36234;&#30028;&#26816;&#27979;&#65292;&#35813;&#25968;&#25454;&#38598;&#27604;&#22823;&#22810;&#25968;&#36234;&#30028;&#26816;&#27979;&#22120;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#26356;&#22823;&#12289;&#26356;&#22810;&#26679;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that computer vision can be unreliable when faced with previously unseen imaging conditions. This paper proposes a method to adapt camera parameters according to a normalizing flow-based out-of-distibution detector. A small-scale study is conducted which shows that adapting camera parameters according to this out-of-distibution detector leads to an average increase of 3 to 4 percentage points in mAP, mAR and F1 performance metrics of a YOLOv4 object detector. As a secondary result, this paper also shows that it is possible to train a normalizing flow model for out-of-distribution detection on the COCO dataset, which is larger and more diverse than most benchmarks for out-of-distibution detectors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26102;&#38388;&#24207;&#25968;&#20998;&#31867;&#26041;&#27861;(O-TDE)&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;18&#20010;TSOC&#38382;&#39064;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#22235;&#31181;&#29616;&#26377;&#30340;&#21517;&#20041;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09288</link><description>&lt;p&gt;
&#22522;&#20110;&#35789;&#20856;&#30340;&#26102;&#38388;&#24207;&#21015;&#24207;&#25968;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Dictionary-based approach to Time Series Ordinal Classification. (arXiv:2305.09288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26102;&#38388;&#24207;&#25968;&#20998;&#31867;&#26041;&#27861;(O-TDE)&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;18&#20010;TSOC&#38382;&#39064;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#22235;&#31181;&#29616;&#26377;&#30340;&#21517;&#20041;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;(TSC)&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#39046;&#22495;&#65292;&#21487;&#20197;&#35299;&#20915;&#22810;&#31181;&#23454;&#38469;&#38382;&#39064;&#24182;&#21462;&#24471;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#20854;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#19968;&#31867;&#26041;&#27861;&#26159;&#25152;&#35859;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#25216;&#26415;&#12290;&#26102;&#38388;&#35789;&#20856;&#38598;&#21512;(TDE)&#26159;&#30446;&#21069;&#22522;&#20110;&#35789;&#20856;&#30340;TSC&#26041;&#27861;&#20013;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#22312;&#35768;&#22810;TSC&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#26631;&#31614;&#20855;&#26377;&#33258;&#28982;&#30340;&#25490;&#24207;&#29305;&#24615;&#65292;&#36825;&#31181;&#29305;&#24615;&#34987;&#31216;&#20316;&#24207;&#25968;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#23427;&#26469;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#22788;&#29702;&#24207;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#39046;&#22495;&#31216;&#20026;&#26102;&#38388;&#24207;&#25968;&#20998;&#31867;(TSOC)&#65292;&#30446;&#21069;&#23578;&#26410;&#34987;&#20805;&#20998;&#24320;&#21457;&#21033;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TDE&#31639;&#27861;&#30340;&#24207;&#25968;&#36866;&#24212;&#29256;&#65292;&#31216;&#20026;&#24207;&#25968;TDE(O-TDE)&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;18&#20010;TSOC&#38382;&#39064;&#30340;&#27979;&#35797;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#22235;&#31181;&#29616;&#26377;&#30340;&#21517;&#20041;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;&#24207;&#25968;&#30340;&#35789;&#20856;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time Series Classification (TSC) is an extensively researched field from which a broad range of real-world problems can be addressed obtaining excellent results. One sort of the approaches performing well are the so-called dictionary-based techniques. The Temporal Dictionary Ensemble (TDE) is the current state-of-the-art dictionary-based TSC approach. In many TSC problems we find a natural ordering in the labels associated with the time series. This characteristic is referred to as ordinality, and can be exploited to improve the methods performance. The area dealing with ordinal time series is the Time Series Ordinal Classification (TSOC) field, which is yet unexplored. In this work, we present an ordinal adaptation of the TDE algorithm, known as ordinal TDE (O-TDE). For this, a comprehensive comparison using a set of 18 TSOC problems is performed. Experiments conducted show the improvement achieved by the ordinal dictionary-based approach in comparison to four other existing nominal d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Dune Neural Network&#8221;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#30340;&#27599;&#20010;&#33258;&#30001;&#21442;&#25968;&#34920;&#31034;&#20026;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#65292;&#24182;&#23545;&#27599;&#20010;&#36755;&#20837;&#20803;&#32032;&#24212;&#29992;&#32447;&#24615;&#21464;&#25442;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#30333;&#22122;&#22768;&#25968;&#25454;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#33021;&#65292;&#21363;&#20351;&#20026;&#38750;&#24120;&#22024;&#26434;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#27492;&#26041;&#27861;&#20063;&#27604;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#20154;&#31867;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.09276</link><description>&lt;p&gt;
&#22122;&#22768;&#40065;&#26834;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Noise robust neural network architecture. (arXiv:2305.09276v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Dune Neural Network&#8221;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#30340;&#27599;&#20010;&#33258;&#30001;&#21442;&#25968;&#34920;&#31034;&#20026;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#65292;&#24182;&#23545;&#27599;&#20010;&#36755;&#20837;&#20803;&#32032;&#24212;&#29992;&#32447;&#24615;&#21464;&#25442;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#30333;&#22122;&#22768;&#25968;&#25454;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#33021;&#65292;&#21363;&#20351;&#20026;&#38750;&#24120;&#22024;&#26434;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#27492;&#26041;&#27861;&#20063;&#27604;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#20154;&#31867;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Dune Neural Network&#8221;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35782;&#21035;&#19968;&#33324;&#22122;&#22768;&#22270;&#20687;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#20219;&#20309;&#20154;&#24037;&#22122;&#22768;&#12290;&#36890;&#36807;&#23558;&#32593;&#32476;&#30340;&#27599;&#20010;&#33258;&#30001;&#21442;&#25968;&#34920;&#31034;&#20026;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#65292;&#24182;&#23545;&#27599;&#20010;&#36755;&#20837;&#20803;&#32032;&#24212;&#29992;&#32447;&#24615;&#21464;&#25442;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#24471;&#21040;&#30340;&#26550;&#26500;&#22312;&#38754;&#23545;&#26377;&#30333;&#22122;&#22768;&#30340;&#36755;&#20837;&#25968;&#25454;&#26102;&#20855;&#26377;&#30456;&#24403;&#22909;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#31616;&#21333;&#30340;Dune&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;MNIST&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#23545;&#20110;&#20154;&#31867;&#38590;&#20197;&#35782;&#21035;&#30340;&#38750;&#24120;&#22024;&#26434;&#30340;&#36755;&#20837;&#22270;&#20687;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#27604;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#20154;&#31867;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#35768;&#22810;&#20854;&#20182;&#28155;&#21152;&#20102;&#21508;&#31181;&#32972;&#26223;&#27169;&#24335;&#30340;&#31034;&#20363;&#37117;&#24456;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
In which we propose neural network architecture (dune neural network) for recognizing general noisy image without adding any artificial noise in the training data. By representing each free parameter of the network as an uncertainty interval, and applying a linear transformation to each input element, we show that the resulting architecture achieves decent noise robustness when faced with input data with white noise. We apply simple dune neural networks for MNIST dataset and demonstrate that even for very noisy input images which are hard for human to recognize, our approach achieved better test set accuracy than human without dataset augmentation. We also find that our method is robust for many other examples with various background patterns added.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#36866;&#24212;&#24615;&#30340;&#24120;&#35265;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#22312;&#32447;&#20934;&#30830;&#29575;&#24230;&#37327;&#12290;&#35813;&#24230;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#29616;&#26377;&#31639;&#27861;&#20250;&#23398;&#20064;&#34920;&#38754;&#30340;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26469;&#34913;&#37327;&#36866;&#24212;&#24615;&#24182;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.09275</link><description>&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#24555;&#36895;&#36866;&#24212;&#24615;&#65306;&#25105;&#20204;&#35780;&#20272;&#24471;&#23545;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?. (arXiv:2305.09275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#36866;&#24212;&#24615;&#30340;&#24120;&#35265;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#22312;&#32447;&#20934;&#30830;&#29575;&#24230;&#37327;&#12290;&#35813;&#24230;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#29616;&#26377;&#31639;&#27861;&#20250;&#23398;&#20064;&#34920;&#38754;&#30340;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26469;&#34913;&#37327;&#36866;&#24212;&#24615;&#24182;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#36866;&#24212;&#24615;&#30340;&#24120;&#35265;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#22312;&#32447;&#20934;&#30830;&#29575;&#24230;&#37327;&#12290;&#35813;&#24230;&#37327;&#34913;&#37327;&#27169;&#22411;&#22312;&#25509;&#19979;&#26469;&#30340;&#20960;&#20010;&#26679;&#26412;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#24230;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#21363;&#20351;&#26159;&#31354;&#27867;&#30340;&#30450;&#20998;&#31867;&#22120;&#20063;&#33021;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#27969;&#20013;&#34920;&#38754;&#30340;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#38750;&#24120;&#39640;&#30340;&#22312;&#32447;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#20063;&#33021;&#22815;&#23454;&#29616;&#39640;&#22312;&#32447;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#65292;&#34920;&#26126;&#23427;&#20204;&#24847;&#22806;&#22320;&#23398;&#20064;&#20102;&#34920;&#38754;&#30340;&#26631;&#31614;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#38500;&#34920;&#38754;&#30340;&#30456;&#20851;&#24615;&#30340;&#36817;&#26399;&#26679;&#26412;&#20934;&#30830;&#24230;&#30340;&#26032;&#24230;&#37327;&#26469;&#34913;&#37327;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#22312;&#21508;&#31181;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#29616;&#26377;&#30340;OCL&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#36798;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the common practice of evaluating adaptation of Online Continual Learning (OCL) algorithms through the metric of online accuracy, which measures the accuracy of the model on the immediate next few samples. However, we show that this metric is unreliable, as even vacuous blind classifiers, which do not use input images for prediction, can achieve unrealistically high online accuracy by exploiting spurious label correlations in the data stream. Our study reveals that existing OCL algorithms can also achieve high online accuracy, but perform poorly in retaining useful information, suggesting that they unintentionally learn spurious label correlations. To address this issue, we propose a novel metric for measuring adaptation based on the accuracy on the near-future samples, where spurious correlations are removed. We benchmark existing OCL approaches using our proposed metric on large-scale datasets under various computational budgets and find that better generalization can be a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21033;&#29992;&#19981;&#21487;&#20449;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#25490;&#24207;&#21644;&#36229;&#22270;&#23450;&#21521;&#30340;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#20110;&#27491;&#30830;&#39044;&#27979;&#26102;$1+1/\gamma$&#30340;&#31454;&#20105;&#27604;&#21644;&#23545;&#20110;&#20219;&#24847;&#38169;&#35823;&#39044;&#27979;&#26102;$\gamma$&#30340;&#31454;&#20105;&#27604;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09245</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#25490;&#24207;&#21644;&#36229;&#22270;&#23450;&#21521;
&lt;/p&gt;
&lt;p&gt;
Sorting and Hypergraph Orientation under Uncertainty with Predictions. (arXiv:2305.09245v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21033;&#29992;&#19981;&#21487;&#20449;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#25490;&#24207;&#21644;&#36229;&#22270;&#23450;&#21521;&#30340;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#20110;&#27491;&#30830;&#39044;&#27979;&#26102;$1+1/\gamma$&#30340;&#31454;&#20105;&#27604;&#21644;&#23545;&#20110;&#20219;&#24847;&#38169;&#35823;&#39044;&#27979;&#26102;$\gamma$&#30340;&#31454;&#20105;&#27604;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#30340;&#31639;&#27861;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#26368;&#36817;&#25165;&#22312;&#21487;&#25506;&#32034;&#19981;&#30830;&#23450;&#24615;&#30340;&#29615;&#22659;&#20013;&#34987;&#32771;&#34385;&#65292;&#20854;&#20013;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#33719;&#24471;&#19981;&#30830;&#23450;&#36755;&#20837;&#20803;&#32032;&#30340;&#31934;&#30830;&#20540;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#35299;&#20915;&#38382;&#39064;&#25152;&#38656;&#30340;&#26597;&#35810;&#25968;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21033;&#29992;&#19981;&#21487;&#20449;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#25490;&#24207;&#21644;&#36229;&#22270;&#23450;&#21521;&#30340;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#25552;&#39640;&#20102;&#20934;&#30830;&#39044;&#27979;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22312;&#27809;&#26377;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#26368;&#22909;&#30340;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#12290;&#23545;&#20110;&#36229;&#22270;&#23450;&#21521;&#65292;&#23545;&#20110;&#20219;&#20309;$\gamma\geq 2$&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#27491;&#30830;&#39044;&#27979;&#26102;&#36798;&#21040;$1+1/\gamma$&#30340;&#31454;&#20105;&#27604;&#65292;&#23545;&#20110;&#20219;&#24847;&#38169;&#35823;&#39044;&#27979;&#21017;&#20026;$\gamma$&#12290;&#23545;&#20110;&#25490;&#24207;&#65292;&#25105;&#20204;&#22312;&#20934;&#30830;&#39044;&#27979;&#26102;&#23454;&#29616;&#20102;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#22312;&#20219;&#24847;&#38169;&#35823;&#39044;&#27979;&#26102;&#20173;&#26159;$2$&#31454;&#20105;&#30340;&#12290;&#36825;&#20123;&#26435;&#34913;&#26159;&#22312;&#23454;&#29616;&#38750;&#24179;&#20961;&#20445;&#35777;&#30340;&#21516;&#26102;&#23454;&#29616;&#30340;&#26368;&#20339;&#21487;&#33021;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-augmented algorithms have been attracting increasing interest, but have only recently been considered in the setting of explorable uncertainty where precise values of uncertain input elements can be obtained by a query and the goal is to minimize the number of queries needed to solve a problem. We study learning-augmented algorithms for sorting and hypergraph orientation under uncertainty, assuming access to untrusted predictions for the uncertain values. Our algorithms provide improved performance guarantees for accurate predictions while maintaining worst-case guarantees that are best possible without predictions. For hypergraph orientation, for any $\gamma \geq 2$, we give an algorithm that achieves a competitive ratio of $1+1/\gamma$ for correct predictions and $\gamma$ for arbitrarily wrong predictions. For sorting, we achieve an optimal solution for accurate predictions while still being $2$-competitive for arbitrarily wrong predictions. These tradeoffs are the best poss
&lt;/p&gt;</description></item><item><title>&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#25552;&#20986;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#23545;&#20445;&#25252;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#21033;&#29992;&#12290;&#36890;&#36807;&#25552;&#20986;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#21644;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#22909;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.09241</link><description>&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#32473;&#20986;&#20102;&#19968;&#31181;&#34394;&#20551;&#30340;&#23433;&#20840;&#24863;&#65306;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20363;&#23376;&#31359;&#36879;&#37027;&#20123;&#26080;&#27861;&#21033;&#29992;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. (arXiv:2305.09241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09241
&lt;/p&gt;
&lt;p&gt;
&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#25552;&#20986;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#23545;&#20445;&#25252;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#21033;&#29992;&#12290;&#36890;&#36807;&#25552;&#20986;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#21644;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#22909;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#19979;&#38543;&#22788;&#21487;&#35265;&#30340;&#23433;&#20840;&#28431;&#27934;&#20013;&#65292;&#20445;&#25252;&#25968;&#25454;&#20813;&#20110;&#26410;&#32463;&#25480;&#26435;&#30340;&#21033;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21483;&#20570;&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#65288;UEs&#65289;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#24494;&#23567;&#30340;&#25200;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#22312;&#21407;&#22987;&#30340;&#24178;&#20928;&#20998;&#24067;&#19978;&#20934;&#30830;&#22320;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#20445;&#25252;&#25514;&#26045;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616; UEs &#24102;&#26469;&#30340;&#23433;&#20840;&#23041;&#32961;&#26159;&#34394;&#20551;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#21033;&#29992;&#20854;&#20182;&#26410;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#26469;&#21435;&#38500;&#20445;&#25252;&#65292;&#23558;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#37325;&#36716;&#20026;&#21487;&#23398;&#20064;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#31181;&#23041;&#32961;&#65292;&#24341;&#20837;&#20102;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#65288;LEs&#65289;&#65292;&#36825;&#20123;&#26159;&#24050;&#32463;&#21435;&#38500;&#20445;&#25252;&#30340;UEs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#23558;UEs&#25237;&#23556;&#21040;LEs&#30340;&#27969;&#24418;&#19978;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#27169;&#22411;&#23545;UEs&#36827;&#34892;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safeguarding data from unauthorized exploitation is vital for privacy and security, especially in recent rampant research in security breach such as adversarial/membership attacks. To this end, \textit{unlearnable examples} (UEs) have been recently proposed as a compelling protection, by adding imperceptible perturbation to data so that models trained on them cannot classify them accurately on original clean distribution. Unfortunately, we find UEs provide a false sense of security, because they cannot stop unauthorized users from utilizing other unprotected data to remove the protection, by turning unlearnable data into learnable again. Motivated by this observation, we formally define a new threat by introducing \textit{learnable unauthorized examples} (LEs) which are UEs with their protection removed. The core of this approach is a novel purification process that projects UEs onto the manifold of LEs. This is realized by a new joint-conditional diffusion model which denoises UEs con
&lt;/p&gt;</description></item><item><title>&#21512;&#25104;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#26159;&#19981;&#23436;&#32654;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#20250;&#23548;&#33268;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#38169;&#35823;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#31181;&#24773;&#20917;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#28145;&#24230;&#29983;&#25104;&#38598;&#25104;&#65288;DGE&#65289;&#26694;&#26550;&#26469;&#36817;&#20284;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09235</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#65292;&#30495;&#23454;&#35823;&#24046;&#65306;&#22914;&#20309;&#65288;&#19981;&#65289;&#21457;&#24067;&#21644;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Synthetic data, real errors: how (not) to publish and use synthetic data. (arXiv:2305.09235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09235
&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#26159;&#19981;&#23436;&#32654;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#20250;&#23548;&#33268;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#38169;&#35823;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#31181;&#24773;&#20917;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#28145;&#24230;&#29983;&#25104;&#38598;&#25104;&#65288;DGE&#65289;&#26694;&#26550;&#26469;&#36817;&#20284;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#20854;&#20182;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#36825;&#31181;&#26041;&#27861;&#25215;&#35834;&#23558;&#26469;&#21487;&#20197;&#26681;&#25454;&#20010;&#20307;&#38656;&#27714;&#23450;&#21046;&#25968;&#25454;&#38598;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21512;&#25104;&#25968;&#25454;&#36890;&#24120;&#24182;&#19981;&#23436;&#32654;&#65292;&#21487;&#33021;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#29983;&#25104;&#36807;&#31243;&#23545;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21333;&#32431;&#30340;&#21512;&#25104;&#25968;&#25454;&#26041;&#27861;&#8212;&#8212;&#23558;&#21512;&#25104;&#25968;&#25454;&#35270;&#20026;&#30495;&#23454;&#25968;&#25454;&#20351;&#29992;&#8212;&#8212;&#20250;&#23548;&#33268;&#19979;&#28216;&#27169;&#22411;&#21644;&#20998;&#26512;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#30495;&#23454;&#25968;&#25454;&#12290;&#20316;&#20026;&#21512;&#25104;&#25968;&#25454;&#29615;&#22659;&#19979;&#26356;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28145;&#24230;&#29983;&#25104;&#38598;&#25104;&#65288;DGE&#65289;&#8212;&#8212;&#21463;&#21040;&#28145;&#24230;&#38598;&#25104;&#21551;&#21457;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#38544;&#24335;&#22320;&#36817;&#20284;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;DGE&#25913;&#21892;&#20102;&#19979;&#28216;&#27169;&#22411;&#30340;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24179;&#22343;&#32780;&#35328;&#36828;&#36828;&#20248;&#20110;&#21333;&#32431;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#23569;&#25968;&#31867;&#21644;&#20302;&#23494;&#24230;&#21306;&#22495;&#65292;&#26368;&#22823;&#30340;&#25913;&#36827;&#25928;&#26524;&#24471;&#21040;&#20102;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating synthetic data through generative models is gaining interest in the ML community and beyond, promising a future where datasets can be tailored to individual needs. Unfortunately, synthetic data is usually not perfect, resulting in potential errors in downstream tasks. In this work we explore how the generative process affects the downstream ML task. We show that the naive synthetic data approach -- using synthetic data as if it is real -- leads to downstream models and analyses that do not generalize well to real data. As a first step towards better ML in the synthetic data regime, we introduce Deep Generative Ensemble (DGE) -- a framework inspired by Deep Ensembles that aims to implicitly approximate the posterior distribution over the generative process model parameters. DGE improves downstream model training, evaluation, and uncertainty quantification, vastly outperforming the naive approach on average. The largest improvements are achieved for minority classes and low-de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21322;&#24377;&#24615;&#32442;&#32455;&#21697;&#34920;&#38754;&#19978;&#36793;&#32536;&#30340;&#20256;&#24863;&#22120;&#36827;&#34892;&#25509;&#35302;&#24863;&#27979;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#22312;&#24863;&#27979;&#21306;&#22495;&#25918;&#32622;&#39069;&#22806;&#20256;&#24863;&#22120;&#12290;&#35813;&#26041;&#27861;&#21487;&#22312;&#21487;&#31359;&#25140;&#25216;&#26415;&#21644;&#26234;&#33021;&#32442;&#32455;&#21697;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#33021;&#22815;&#20197;82.85%&#30340;&#20934;&#30830;&#24230;&#20998;&#31867;&#35782;&#21035;&#19977;&#20010;&#21387;&#21147;&#27700;&#24179;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.09222</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#32536;&#20256;&#24863;&#22120;&#30340;&#21322;&#24377;&#24615;&#32442;&#32455;&#21697;&#35302;&#25720;&#24863;&#27979;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Touch Sensing on Semi-Elastic Textiles with Border-Based Sensors. (arXiv:2305.09222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21322;&#24377;&#24615;&#32442;&#32455;&#21697;&#34920;&#38754;&#19978;&#36793;&#32536;&#30340;&#20256;&#24863;&#22120;&#36827;&#34892;&#25509;&#35302;&#24863;&#27979;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#22312;&#24863;&#27979;&#21306;&#22495;&#25918;&#32622;&#39069;&#22806;&#20256;&#24863;&#22120;&#12290;&#35813;&#26041;&#27861;&#21487;&#22312;&#21487;&#31359;&#25140;&#25216;&#26415;&#21644;&#26234;&#33021;&#32442;&#32455;&#21697;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#33021;&#22815;&#20197;82.85%&#30340;&#20934;&#30830;&#24230;&#20998;&#31867;&#35782;&#21035;&#19977;&#20010;&#21387;&#21147;&#27700;&#24179;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25509;&#35302;&#24863;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#21322;&#24377;&#24615;&#32442;&#32455;&#21697;&#34920;&#38754;&#19978;&#36793;&#32536;&#30340;&#20256;&#24863;&#22120;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#25509;&#35302;&#21306;&#22495;&#25918;&#32622;&#39069;&#22806;&#30340;&#20256;&#24863;&#22120;&#12290;&#36890;&#36807;&#22312;&#24377;&#24615;&#36816;&#21160;&#32455;&#29289;&#19978;&#36827;&#34892;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#30340;&#20256;&#24863;&#22120;&#35774;&#35745;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#26631;&#35760;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;&#35270;&#35273;&#20256;&#24863;&#22120;&#39044;&#27979;125mm&#215;125mm&#21306;&#22495;&#19978;&#19968;&#20010;&#28857;&#30340;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#20026;1.36mm&#12290;&#25105;&#20204;&#21046;&#20316;&#20102;&#19968;&#31181;&#20165;&#29992;&#32442;&#32455;&#21697;&#23454;&#29616;&#30340;&#21407;&#22411;&#65292;&#33021;&#22815;&#20197;82.85%&#30340;&#20934;&#30830;&#24230;&#20998;&#31867;&#35782;&#21035;&#19977;&#20010;&#21387;&#21147;&#27700;&#24179;&#65288;0&#12289;15&#21644;20mm&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#31359;&#25140;&#25216;&#26415;&#21644;&#26234;&#33021;&#32442;&#32455;&#21697;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#32034;&#36825;&#20123;&#39046;&#22495;&#65292;&#36825;&#23558;&#26159;&#19968;&#20010;&#20805;&#28385;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel approach for touch sensing using semi-elastic textile surfaces that does not require the placement of additional sensors in the sensing area, instead relying on sensors located on the border of the textile. The proposed approach is demonstrated through experiments involving an elastic Jersey fabric and a variety of machine-learning models. The performance of one particular border-based sensor design is evaluated in depth. By using visual markers, the best-performing visual sensor arrangement predicts a single touch point with a mean squared error of 1.36 mm on an area of 125mm by 125mm. We built a textile only prototype that is able to classify touch at three indent levels (0, 15, and 20 mm) with an accuracy of 82.85%. Our results suggest that this approach has potential applications in wearable technology and smart textiles, making it a promising avenue for further exploration in these fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#23396;&#31435;&#35757;&#32451;&#30340;&#39640;&#26031;&#20808;&#39564;Turbo&#33258;&#32534;&#30721;&#22120;&#30340;&#32452;&#20214;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#24555;&#36895;&#12289;&#19968;&#33268;&#12289;&#27867;&#21270;&#24615;&#22909;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#20854;&#20013;&#20511;&#21161;EXIT&#22270;&#30340;&#35774;&#35745;&#21487;&#19987;&#27880;&#20110;&#22359;&#35823;&#30721;&#29575;&#32780;&#36798;&#21040;&#26399;&#26395;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.09216</link><description>&lt;p&gt;
Turbo&#33258;&#32534;&#30721;&#22120;&#30340;&#32452;&#20214;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Component Training of Turbo Autoencoders. (arXiv:2305.09216v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#23396;&#31435;&#35757;&#32451;&#30340;&#39640;&#26031;&#20808;&#39564;Turbo&#33258;&#32534;&#30721;&#22120;&#30340;&#32452;&#20214;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#24555;&#36895;&#12289;&#19968;&#33268;&#12289;&#27867;&#21270;&#24615;&#22909;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#20854;&#20013;&#20511;&#21161;EXIT&#22270;&#30340;&#35774;&#35745;&#21487;&#19987;&#27880;&#20110;&#22359;&#35823;&#30721;&#29575;&#32780;&#36798;&#21040;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;Turbo&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#32452;&#20214;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#23396;&#31435;&#39640;&#26031;&#20808;&#39564;&#35757;&#32451;(TGP)&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#19968;&#33268;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#23545;&#20219;&#24847;&#35299;&#30721;&#36845;&#20195;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22806;&#37096;&#20449;&#24687;&#20256;&#36882;EXIT&#22270;&#26469;&#36866;&#24212;&#32452;&#20214;&#30340;&#26399;&#26395;&#34892;&#20026;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#25968;&#25454;&#38271;&#24230;&#65288;k&#8776;1000&#65289;&#26102;&#30340;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#35813;&#21306;&#22495;&#20869;&#34920;&#29616;&#25509;&#36817;&#32463;&#20856;&#32534;&#30721;&#30340;&#33258;&#32534;&#30721;&#22120;&#12290;&#23613;&#31649;&#20108;&#36827;&#21046;&#20132;&#21449;&#29109;(BCE)&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#20102;&#32452;&#20214;&#30340;&#27604;&#29305;&#35823;&#30721;&#29575;(BER)&#65292;&#20294;&#26159;&#36890;&#36807;EXIT&#22270;&#30340;&#35774;&#35745;&#21487;&#20197;&#19987;&#27880;&#20110;&#22359;&#35823;&#30721;&#29575;(BLER)&#12290;&#22312;&#20018;&#32852;&#31995;&#32479;&#20013;&#65292;&#32452;&#20214;&#32423;&#21035;&#30340;TGP&#26041;&#27861;&#24050;&#20026;&#20869;&#37096;&#32452;&#20214;&#65288;&#20855;&#26377;&#22266;&#23450;&#22806;&#37096;&#20108;&#36827;&#21046;&#25509;&#21475;&#65292;&#20363;&#22914;&#23398;&#20064;&#30340;&#20869;&#37096;&#20195;&#30721;&#25110;&#22343;&#34913;&#22120;&#65289;&#21644;&#22806;&#37096;&#20108;&#36827;&#21046;&#32416;&#38169;&#30721;&#30340;&#31561;&#20540;&#22120;&#25152;&#29087;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#22987;&#32456;&#37319;&#29992;TGP&#26041;&#27861;&#25972;&#20010;Turbo&#33258;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Isolated training with Gaussian priors (TGP) of the component autoencoders of turbo-autoencoder architectures enables faster, more consistent training and better generalization to arbitrary decoding iterations than training based on deep unfolding. We propose fitting the components via extrinsic information transfer (EXIT) charts to a desired behavior which enables scaling to larger message lengths ($k \approx 1000$) while retaining competitive performance. To the best of our knowledge, this is the first autoencoder that performs close to classical codes in this regime. Although the binary cross-entropy (BCE) loss function optimizes the bit error rate (BER) of the components, the design via EXIT charts enables to focus on the block error rate (BLER). In serially concatenated systems the component-wise TGP approach is well known for inner components with a fixed outer binary interface, e.g., a learned inner code or equalizer, with an outer binary error correcting code. In this paper we 
&lt;/p&gt;</description></item><item><title>CB-HVTNet &#25552;&#20986;&#20102;&#19968;&#31181; Channel Boosted Hybrid Vision Transformer &#32593;&#32476;&#65292;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#29983;&#25104;&#22686;&#24378;&#36890;&#36947;&#65292;&#24182;&#32467;&#21512;&#20351;&#29992; Transformers &#21644; CNN&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#39640;&#25928;&#20934;&#30830;&#22320;&#35780;&#20272;&#28107;&#24052;&#32454;&#32990;&#12290;</title><link>http://arxiv.org/abs/2305.09211</link><description>&lt;p&gt;
CB-HVTNet&#65306;&#19968;&#31181;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#28107;&#24052;&#32454;&#32990;&#35780;&#20272;&#30340;&#36890;&#36947;&#22686;&#24378;&#28151;&#21512;&#35270;&#35273; Transformer &#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images. (arXiv:2305.09211v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09211
&lt;/p&gt;
&lt;p&gt;
CB-HVTNet &#25552;&#20986;&#20102;&#19968;&#31181; Channel Boosted Hybrid Vision Transformer &#32593;&#32476;&#65292;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#29983;&#25104;&#22686;&#24378;&#36890;&#36947;&#65292;&#24182;&#32467;&#21512;&#20351;&#29992; Transformers &#21644; CNN&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#39640;&#25928;&#20934;&#30830;&#22320;&#35780;&#20272;&#28107;&#24052;&#32454;&#32990;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#30001;&#20110;&#20854;&#23398;&#20064;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#24050;&#32463;&#20811;&#26381;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20840;&#23616;&#36879;&#35270;&#23398;&#20064;&#30340;&#32570;&#28857;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#20851;&#27880;&#30340;&#28966;&#28857;&#65292;&#29992;&#20110;&#22810;&#20010;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#21307;&#30103;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22810;&#22836;&#27880;&#24847;&#27169;&#22359;&#20165;&#25429;&#33719;&#20840;&#23616;&#32423;&#21035;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#36825;&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#26469;&#35828;&#26159;&#19981;&#36275;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181; Channel Boosted Hybrid Vision Transformer&#65288;CB HVT&#65289;&#65292;&#23427;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#29983;&#25104;&#22686;&#24378;&#36890;&#36947;&#65292;&#24182;&#20351;&#29992; Transformers &#21644; CNN &#26469;&#20998;&#26512;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#28107;&#24052;&#32454;&#32990;&#12290;&#25152;&#25552;&#20986;&#30340; CB HVT &#21253;&#25324;&#20116;&#20010;&#27169;&#22359;&#65292;&#21253;&#25324;&#36890;&#36947;&#29983;&#25104;&#27169;&#22359;&#12289;&#36890;&#36947;&#21033;&#29992;&#27169;&#22359;&#12289;&#36890;&#36947;&#21512;&#24182;&#27169;&#22359;&#12289;&#21306;&#22495;&#24863;&#30693;&#27169;&#22359;&#21644;&#26816;&#27979;&#21644;&#20998;&#27573;&#22836;&#65292;&#23427;&#20204;&#20849;&#21516;&#26377;&#25928;&#22320;&#35782;&#21035;&#28107;&#24052;&#32454;&#32990;&#12290;&#36890;&#36947;&#29983;&#25104;&#27169;&#22359;&#20351;&#29992;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#36890;&#36947;&#22686;&#24378;&#30340;&#24605;&#24819;&#21019;&#24314;&#22810;&#20010;&#24378;&#22823;&#30340;&#36890;&#36947;&#65292;&#28982;&#21518;&#19982; Transformers &#21644; CNN &#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#26356;&#22909;&#22320;&#20998;&#26512;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#28107;&#24052;&#32454;&#32990;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340; CB HVT &#26159;&#21307;&#23398;&#35786;&#26029;&#20013;&#20934;&#30830;&#12289;&#39640;&#25928;&#35780;&#20272;&#28107;&#24052;&#32454;&#32990;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers, due to their ability to learn long range dependencies, have overcome the shortcomings of convolutional neural networks (CNNs) for global perspective learning. Therefore, they have gained the focus of researchers for several vision related tasks including medical diagnosis. However, their multi-head attention module only captures global level feature representations, which is insufficient for medical images. To address this issue, we propose a Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning to generate boosted channels and employs both transformers and CNNs to analyse lymphocytes in histopathological images. The proposed CB HVT comprises five modules, including a channel generation module, channel exploitation module, channel merging module, region-aware module, and a detection and segmentation head, which work together to effectively identify lymphocytes. The channel generation module uses the idea of channel boosting through transfer learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#23545;&#32437;&#21521;&#25968;&#25454;&#20013;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;Treatment Effect Neural Controlled Differential Equation&#65292;S4Model&#26356;&#26377;&#25928;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12289;&#26356;&#26131;&#20110;&#35757;&#32451;&#12289;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#65292;&#19988;&#22312;&#24402;&#19968;&#21270;&#22343;&#26041;&#35823;&#24046;&#19978;&#25552;&#39640;&#20102;10&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.09207</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#29992;&#20110;&#21453;&#20107;&#23454;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Outcome Prediction using Structured State Space Model. (arXiv:2305.09207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#23545;&#32437;&#21521;&#25968;&#25454;&#20013;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;Treatment Effect Neural Controlled Differential Equation&#65292;S4Model&#26356;&#26377;&#25928;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12289;&#26356;&#26131;&#20110;&#35757;&#32451;&#12289;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#65292;&#19988;&#22312;&#24402;&#19968;&#21270;&#22343;&#26041;&#35823;&#24046;&#19978;&#25552;&#39640;&#20102;10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#32437;&#21521;&#25968;&#25454;&#20013;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#39044;&#27979;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;&#19968;&#31181;&#27969;&#34892;&#30340;&#24207;&#21015;&#27169;&#22411;&#65289;&#36827;&#34892;&#36825;&#39033;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#24615;&#33021;&#65306;Treatment Effect Neural Controlled Differential Equation&#65288;TE-CDE&#65289;&#21644;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;S4Model&#65289;&#12290;&#34429;&#28982;TE-CDE&#21033;&#29992;&#21487;&#25511;&#24494;&#20998;&#26041;&#31243;&#26469;&#35299;&#20915;&#26102;&#38388;&#20381;&#36182;&#24615;&#28151;&#28102;&#65292;&#20294;&#23427;&#23384;&#22312;&#20248;&#21270;&#38382;&#39064;&#21644;&#35757;&#32451;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;S4Model&#26356;&#26377;&#25928;&#22320;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#27169;&#25311;&#32954;&#37096;&#32959;&#30244;&#22686;&#38271;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#65292;&#21457;&#29616;S4Model&#22312;&#27599;&#20010;&#26102;&#26399;&#30340;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;1.63&#20493;&#65292;&#24182;&#19988;&#24402;&#19968;&#21270;&#22343;&#26041;&#35823;&#24046;&#20063;&#25552;&#39640;&#20102;10&#20493;&#12290;&#27492;&#22806;&#65292;&#30456;&#36739;TE-CDE&#65292;S4Model&#22312;&#35757;&#32451;&#26399;&#38388;&#26356;&#31283;&#23450;&#65292;&#23545;&#26435;&#37325;&#21021;&#22987;&#21270;&#20063;&#19981;&#22826;&#25935;&#24863;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;&#20854;&#39640;&#25928;&#21644;&#31283;&#23450;&#24615;&#65292;S4Model&#26159;&#32437;&#21521;&#25968;&#25454;&#20013;&#21453;&#20107;&#23454;&#32467;&#26524;&#39044;&#27979;&#26356;&#20026;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual outcome prediction in longitudinal data has recently gained attention due to its potential applications in healthcare and social sciences. In this paper, we explore the use of the state space model, a popular sequence model, for this task. Specifically, we compare the performance of two models: Treatment Effect Neural Controlled Differential Equation (TE-CDE) and structured state space model (S4Model). While TE-CDE uses controlled differential equations to address time-dependent confounding, it suffers from optimization issues and slow training. In contrast, S4Model is more efficient at modeling long-range dependencies and easier to train. We evaluate the models on a simulated lung tumor growth dataset and find that S4Model outperforms TE-CDE with 1.63x reduction in per epoch training time and 10x better normalized mean squared error. Additionally, S4Model is more stable during training and less sensitive to weight initialization than TE-CDE. Our results suggest that the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26435;&#37325;&#33707;&#27604;&#20044;&#26031;&#20998;&#25968;&#20316;&#20026;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#24402;&#22240;&#26694;&#26550;&#65292;&#21487;&#20197;&#28085;&#30422;&#24456;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#24449;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#26041;&#27861;&#32321;&#34893;&#21644;&#19981;&#21487;&#27604;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#26041;&#27861;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#21644;&#35299;&#37322;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20854;&#22810;&#21151;&#33021;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09204</link><description>&lt;p&gt;
&#26435;&#37325;&#33707;&#27604;&#20044;&#26031;&#20998;&#25968;&#65306;&#19968;&#20010;&#29305;&#24449;&#24402;&#22240;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Weighted M\"obius Score: A Unified Framework for Feature Attribution. (arXiv:2305.09204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26435;&#37325;&#33707;&#27604;&#20044;&#26031;&#20998;&#25968;&#20316;&#20026;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#24402;&#22240;&#26694;&#26550;&#65292;&#21487;&#20197;&#28085;&#30422;&#24456;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#24449;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#26041;&#27861;&#32321;&#34893;&#21644;&#19981;&#21487;&#27604;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#26041;&#27861;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#21644;&#35299;&#37322;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20854;&#22810;&#21151;&#33021;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#27599;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#39044;&#27979;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;&#29305;&#24449;&#24402;&#22240;&#25193;&#23637;&#21040;&#22810;&#20010;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#23548;&#33268;&#26041;&#27861;&#30340;&#22823;&#37327;&#32321;&#34893;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#19981;&#33021;&#30452;&#25509;&#27604;&#36739;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#30340;&#24402;&#22240;&#26694;&#26550;&#8212;&#8212;&#26435;&#37325;&#33707;&#27604;&#20044;&#26031;&#20998;&#25968;&#65292;&#24182;&#26174;&#31034;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#38024;&#23545;&#21333;&#20010;&#29305;&#24449;&#21644;&#29305;&#24449;&#20132;&#20114;&#30340;&#24402;&#22240;&#26041;&#27861;&#26159;&#29305;&#20363;&#65292;&#36824;&#39564;&#35777;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;&#24402;&#22240;&#26041;&#27861;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#26631;&#20934;&#32447;&#24615;&#20195;&#25968;&#24037;&#20855;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#25552;&#20379;&#35299;&#37322;&#65292;&#21253;&#25324;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#21644;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36825;&#20123;&#24402;&#22240;&#26041;&#27861;&#24212;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#29305;&#24449;&#20132;&#20114;&#26469;&#23454;&#35777;&#20102;&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution aims to explain the reasoning behind a black-box model's prediction by identifying the impact of each feature on the prediction. Recent work has extended feature attribution to interactions between multiple features. However, the lack of a unified framework has led to a proliferation of methods that are often not directly comparable. This paper introduces a parameterized attribution framework -- the Weighted M\"obius Score -- and (i) shows that many different attribution methods for both individual features and feature interactions are special cases and (ii) identifies some new methods. By studying the vector space of attribution methods, our framework utilizes standard linear algebra tools and provides interpretations in various fields, including cooperative game theory and causal mediation analysis. We empirically demonstrate the framework's versatility and effectiveness by applying these attribution methods to feature interactions in sentiment analysis and chain-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#22312;&#38754;&#23545;&#22122;&#22768;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#25152;&#34920;&#29616;&#20986;&#30340;&#33258;&#28982;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#25511;&#21046;ODE&#21160;&#21147;&#23398;&#30340;Lipschitz&#24120;&#25968;&#26469;&#26174;&#33879;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.09179</link><description>&lt;p&gt;
Ortho-ODE&#65306;&#22686;&#24378;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Ortho-ODE: Enhancing Robustness and of Neural ODEs against Adversarial Attacks. (arXiv:2305.09179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#22312;&#38754;&#23545;&#22122;&#22768;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#25152;&#34920;&#29616;&#20986;&#30340;&#33258;&#28982;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#25511;&#21046;ODE&#21160;&#21147;&#23398;&#30340;Lipschitz&#24120;&#25968;&#26469;&#26174;&#33879;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#36890;&#36807;&#20351;&#29992;&#25968;&#20540;&#27714;&#35299;&#22120;&#26469;&#27714;&#35299;&#30001;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#34920;&#31034;&#30340;&#24494;&#20998;&#26041;&#31243;&#65292;&#20174;&#32780;&#24341;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#26080;&#38480;&#28145;&#24230;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33539;&#24335;&#12290; NODE&#26088;&#22312;&#35299;&#20915;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;NODE&#23545;&#21508;&#31181;&#22122;&#22768;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#34920;&#29616;&#20986;&#20102;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;NODE&#30340;&#33258;&#28982;&#40065;&#26834;&#24615;&#24182;&#32771;&#23519;&#20854;&#20013;&#20196;&#20154;&#24778;&#35766;&#34892;&#20026;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#25511;&#21046;ODE&#21160;&#21147;&#23398;&#30340;Lipschitz&#24120;&#25968;&#65292;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20174;Grownwall&#19981;&#31561;&#24335;&#20013;&#25512;&#23548;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32472;&#21046;&#25910;&#32553;&#29702;&#35770;&#21644;Grownwall&#19981;&#31561;&#24335;&#20043;&#38388;&#30340;&#31867;&#27604;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#65288;MNIST&#12289;CIFAR-10&#21644;CIFAR 100&#65289;&#19978;&#35777;&#23454;&#20102;&#22686;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#23545;NODE&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Ordinary Differential Equations (NODEs) probed the usage of numerical solvers to solve the differential equation characterized by a Neural Network (NN), therefore initiating a new paradigm of deep learning models with infinite depth. NODEs were designed to tackle the irregular time series problem. However, NODEs have demonstrated robustness against various noises and adversarial attacks. This paper is about the natural robustness of NODEs and examines the cause behind such surprising behaviour. We show that by controlling the Lipschitz constant of the ODE dynamics the robustness can be significantly improved. We derive our approach from Grownwall's inequality. Further, we draw parallels between contractivity theory and Grownwall's inequality. Experimentally we corroborate the enhanced robustness on numerous datasets - MNIST, CIFAR-10, and CIFAR 100. We also present the impact of adaptive and non-adaptive solvers on the robustness of NODEs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#30452;&#25509;&#35745;&#31639;&#27599;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#24207;&#21015;&#39057;&#29575;&#65292;&#30740;&#31350;&#21457;&#29616;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#22312;&#36755;&#20986;&#24207;&#21015;&#39057;&#29575;&#26041;&#38754;&#20855;&#26377;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;LSTM&#26356;&#36866;&#21512;&#38656;&#35201;&#38271;&#26399;&#35760;&#24518;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.09178</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#20998;&#26512;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24402;&#32435;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis of the Inductive Bias of Recurrent Neural Networks by Discrete Fourier Transform of Output Sequences. (arXiv:2305.09178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09178
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#30452;&#25509;&#35745;&#31639;&#27599;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#24207;&#21015;&#39057;&#29575;&#65292;&#30740;&#31350;&#21457;&#29616;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#22312;&#36755;&#20986;&#24207;&#21015;&#39057;&#29575;&#26041;&#38754;&#20855;&#26377;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;LSTM&#26356;&#36866;&#21512;&#38656;&#35201;&#38271;&#26399;&#35760;&#24518;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNNs)&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#24615;&#26159;&#23427;&#36880;&#27493;&#22788;&#29702;&#36755;&#20837;&#24207;&#21015;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;RNNs&#20869;&#22312;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#21363;&#22312;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;RNNs&#22312;&#22810;&#20037;&#26102;&#38388;&#27493;&#39588;&#20013;&#36890;&#36807;&#36755;&#20986;&#36827;&#34892;&#20999;&#25442;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#36755;&#20986;&#24207;&#21015;&#39057;&#29575;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#35757;&#32451;&#19968;&#20123;&#21512;&#25104;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#19982;&#20505;&#36873;&#27867;&#21270;&#27169;&#24335;&#36827;&#34892;&#27604;&#36739;&#65292;&#20998;&#26512;&#20102;&#24402;&#32435;&#20559;&#32622;&#12290;&#28982;&#32780;&#65292;&#24403;&#26816;&#26597;&#36755;&#20986;&#24207;&#21015;&#39057;&#29575;&#26102;&#65292;&#30001;&#20110;&#26522;&#20030;&#20505;&#36873;&#27169;&#24335;&#22312;&#26356;&#38271;&#30340;&#24207;&#21015;&#19978;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#35270;&#20026;&#31163;&#25955;&#26102;&#38388;&#20449;&#21495;&#65292;&#24182;&#24212;&#29992;&#39057;&#29575;&#22495;&#20998;&#26512;&#26469;&#30452;&#25509;&#35745;&#31639;&#27599;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#24207;&#21015;&#39057;&#29575;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#22312;&#36755;&#20986;&#24207;&#21015;&#39057;&#29575;&#26041;&#38754;&#20855;&#26377;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;LSTM&#20542;&#21521;&#20110;&#22312;&#26102;&#38388;&#27493;&#39588;&#20043;&#38388;&#26356;&#23569;&#22320;&#20999;&#25442;&#36755;&#20986;&#65292;&#36825;&#34920;&#26126;LSTM&#26356;&#21916;&#27426;&#21508;&#20010;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#26356;&#36866;&#21512;&#38656;&#35201;&#38271;&#26399;&#35760;&#24518;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
A unique feature of Recurrent Neural Networks (RNNs) is that it incrementally processes input sequences. In this research, we aim to uncover the inherent generalization properties, i.e., inductive bias, of RNNs with respect to how frequently RNNs switch the outputs through time steps in the sequence classification task, which we call output sequence frequency. Previous work analyzed inductive bias by training models with a few synthetic data and comparing the model's generalization with candidate generalization patterns. However, when examining the output sequence frequency, previous methods cannot be directly applied since enumerating candidate patterns is computationally difficult for longer sequences. To this end, we propose to directly calculate the output sequence frequency for each model by regarding the outputs of the model as discrete-time signals and applying frequency domain analysis. Experimental results showed that Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35745;&#31639;&#21644;&#20998;&#26512;ReLU&#32593;&#32476;&#22810;&#38754;&#20307;&#30340;&#21333;&#32431;&#24418;&#30452;&#26041;&#22270;&#65292;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#26102;&#23427;&#20204;&#32467;&#26500;&#30456;&#23545;&#31616;&#21333;&#65292;&#36825;&#35828;&#26126;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.09145</link><description>&lt;p&gt;
&#28145;&#23618;ReLU&#32593;&#32476;&#30340;&#22810;&#38754;&#20307;&#24322;&#24120;&#31616;&#21333;
&lt;/p&gt;
&lt;p&gt;
Deep ReLU Networks Have Surprisingly Simple Polytopes. (arXiv:2305.09145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35745;&#31639;&#21644;&#20998;&#26512;ReLU&#32593;&#32476;&#22810;&#38754;&#20307;&#30340;&#21333;&#32431;&#24418;&#30452;&#26041;&#22270;&#65292;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#26102;&#23427;&#20204;&#32467;&#26500;&#30456;&#23545;&#31616;&#21333;&#65292;&#36825;&#35828;&#26126;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#32593;&#32476;&#26159;&#19968;&#31181;&#22810;&#38754;&#20307;&#19978;&#30340;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#30740;&#31350;&#36825;&#31181;&#22810;&#38754;&#20307;&#30340;&#24615;&#36136;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#23545;&#20110;&#22810;&#38754;&#20307;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#20165;&#20572;&#30041;&#22312;&#35745;&#31639;&#25968;&#37327;&#30340;&#27700;&#24179;&#65292;&#36825;&#36828;&#36828;&#19981;&#33021;&#23436;&#25972;&#22320;&#25551;&#36848;&#22810;&#38754;&#20307;&#12290;&#20026;&#20102;&#23558;&#29305;&#24449;&#25552;&#21319;&#21040;&#19968;&#20010;&#26032;&#30340;&#27700;&#24179;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#19977;&#35282;&#21078;&#20998;&#22810;&#38754;&#20307;&#24471;&#20986;&#22810;&#38754;&#20307;&#30340;&#24418;&#29366;&#12290;&#36890;&#36807;&#35745;&#31639;&#21644;&#20998;&#26512;&#19981;&#21516;&#22810;&#38754;&#20307;&#30340;&#21333;&#32431;&#24418;&#30452;&#26041;&#22270;&#65292;&#25105;&#20204;&#21457;&#29616;ReLU&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#26102;&#20855;&#26377;&#30456;&#23545;&#31616;&#21333;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#65292;&#23613;&#31649;&#36825;&#20123;&#22810;&#38754;&#20307;&#20174;&#29702;&#35770;&#19978;&#26469;&#35828;&#21487;&#20197;&#38750;&#24120;&#20016;&#23500;&#21644;&#22797;&#26434;&#12290;&#36825;&#19968;&#21457;&#29616;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#38750;&#24179;&#20961;&#30340;&#32452;&#21512;&#25512;&#23548;&#26469;&#29702;&#35770;&#19978;&#35299;&#37322;&#20026;&#20160;&#20040;&#22686;&#21152;&#28145;&#24230;&#19981;&#20250;&#21019;&#24314;&#26356;&#22797;&#26434;&#30340;&#22810;&#38754;&#20307;&#65292;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#32500;&#24230;&#30340;&#24179;&#22343;&#21333;&#32431;&#24418;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A ReLU network is a piecewise linear function over polytopes. Figuring out the properties of such polytopes is of fundamental importance for the research and development of neural networks. So far, either theoretical or empirical studies on polytopes only stay at the level of counting their number, which is far from a complete characterization of polytopes. To upgrade the characterization to a new level, here we propose to study the shapes of polytopes via the number of simplices obtained by triangulating the polytope. Then, by computing and analyzing the histogram of simplices across polytopes, we find that a ReLU network has relatively simple polytopes under both initialization and gradient descent, although these polytopes theoretically can be rather diverse and complicated. This finding can be appreciated as a novel implicit bias. Next, we use nontrivial combinatorial derivation to theoretically explain why adding depth does not create a more complicated polytope by bounding the av
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#31574;&#30053;&#25511;&#21046;&#65292;&#21487;&#20197;&#20445;&#38556;&#32852;&#21512;&#23398;&#20064;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#30830;&#20445;&#27599;&#20010;&#21442;&#19982;&#32773;&#37117;&#36981;&#23432;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#65292;&#21516;&#26102;&#21487;&#20197;&#23457;&#35745;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.09134</link><description>&lt;p&gt;
&#20445;&#38556;&#32852;&#21512;&#23398;&#20064;&#31649;&#29702;&#31995;&#32479;&#23433;&#20840;&#30340;&#26234;&#33021;&#31574;&#30053;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Smart Policy Control for Securing Federated Learning Management System. (arXiv:2305.09134v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#31574;&#30053;&#25511;&#21046;&#65292;&#21487;&#20197;&#20445;&#38556;&#32852;&#21512;&#23398;&#20064;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#30830;&#20445;&#27599;&#20010;&#21442;&#19982;&#32773;&#37117;&#36981;&#23432;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#65292;&#21516;&#26102;&#21487;&#20197;&#23457;&#35745;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#35774;&#22791;&#22312;&#26234;&#24935;&#22478;&#24066;&#12289;&#26234;&#33021;&#21307;&#30103;&#31995;&#32479;&#21644;&#20854;&#20182;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20102;&#22823;&#37327;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#24120;&#24120;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22810;&#20010;&#21442;&#19982;&#32773;&#21512;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#20010;FL&#21442;&#19982;&#32773;&#23454;&#26045;&#20102;&#21508;&#31181;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#65292;&#24403;&#21069;&#30340;FL&#26550;&#26500;&#19981;&#20801;&#35768;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#23457;&#35745;&#12290;&#21478;&#22806;&#65292;&#24403;&#21069;&#26550;&#26500;&#20013;&#27809;&#26377;&#20840;&#23616;&#27169;&#22411;&#21487;&#39564;&#35777;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#31574;&#30053;&#25511;&#21046;&#26469;&#20445;&#38556;FL&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;FL&#21442;&#19982;&#32773;&#20391;&#24320;&#21457;&#21644;&#37096;&#32626;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#25511;&#21046;&#12290;&#36825;&#20010;&#31574;&#30053;&#25511;&#21046;&#34987;&#29992;&#26469;&#39564;&#35777;&#35757;&#32451;&#36807;&#31243;&#65292;&#30830;&#20445;&#27599;&#20010;FL&#21442;&#19982;&#32773;&#37117;&#36981;&#23432;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#65292;&#20174;&#32780;&#23454;&#29616;FL&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of Internet of Things (IoT) devices in smart cities, intelligent healthcare systems, and various real-world applications have resulted in the generation of vast amounts of data, often analyzed using different Machine Learning (ML) models. Federated learning (FL) has been acknowledged as a privacy-preserving machine learning technology, where multiple parties cooperatively train ML models without exchanging raw data. However, the current FL architecture does not allow for an audit of the training process due to the various data-protection policies implemented by each FL participant. Furthermore, there is no global model verifiability available in the current architecture. This paper proposes a smart contract-based policy control for securing the Federated Learning (FL) management system. First, we develop and deploy a smart contract-based local training policy control on the FL participants' side. This policy control is used to verify the training process, ensuri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#36739;&#22909;&#30340;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#65292;&#24182;&#20855;&#22791;&#40065;&#26834;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09129</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#25511;&#21046;&#21452;&#23618;&#20248;&#21270;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Graph Reinforcement Learning for Network Control via Bi-Level Optimization. (arXiv:2305.09129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#36739;&#22909;&#30340;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#65292;&#24182;&#20855;&#22791;&#40065;&#26834;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#23545;&#21160;&#24577;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#24182;&#34987;&#24191;&#27867;&#29992;&#20110;&#35268;&#21010;&#26080;&#25968;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;(1)&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#19981;&#33021;&#25193;&#23637;&#21040;&#22823;&#22411;&#32593;&#32476;&#65292;(2)&#35774;&#35745;&#22909;&#30340;&#21551;&#21457;&#24335;&#25110;&#36817;&#20284;&#31639;&#27861;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#25163;&#21160;&#35797;&#39564;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#21487;&#20197;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#24182;&#23398;&#20064;&#26377;&#25928;&#30340;&#31639;&#27861;&#32780;&#19981;&#29306;&#29298;&#20248;&#21270;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;&#32593;&#32476;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#24418;&#32593;&#32476;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#24191;&#27867;&#30340;&#38382;&#39064;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#26041;&#26696;&#65292;&#32780;&#19981;&#26159;&#22825;&#30495;&#22320;&#23545;&#39640;&#32500;&#22270;&#24418;&#20803;&#32032;&#65288;&#20363;&#22914;&#36793;&#32536;&#65289;&#36827;&#34892;&#25805;&#20316;&#65292;(1)&#25105;&#20204;&#36890;&#36807;RL&#25351;&#23450;&#20102;&#19979;&#19968;&#20010;&#29366;&#24577;&#65292;(2)&#24182;&#35299;&#20915;&#19968;&#20010;&#20984;&#38382;&#39064;&#26469;&#26368;&#22909;&#22320;&#23454;&#29616;&#23427;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#30340;&#19968;&#31995;&#21015;&#26399;&#26395;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#40065;&#26834;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#32593;&#32476;&#25511;&#21046;&#30340;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#65292;&#21253;&#25324;&#22270;&#24418;&#26579;&#33394;&#21644;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization problems over dynamic networks have been extensively studied and widely used in the past decades to formulate numerous real-world problems. However, (1) traditional optimization-based approaches do not scale to large networks, and (2) the design of good heuristics or approximation algorithms often requires significant manual trial-and-error. In this work, we argue that data-driven strategies can automate this process and learn efficient algorithms without compromising optimality. To do so, we present network control problems through the lens of reinforcement learning and propose a graph network-based framework to handle a broad class of problems. Instead of naively computing actions over high-dimensional graph elements, e.g., edges, we propose a bi-level formulation where we (1) specify a desired next state via RL, and (2) solve a convex program to best achieve it, leading to drastically improved scalability and performance. We further highlight a collection of desirable f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\ell_1$-TCL&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#36801;&#31227;&#21644;Lasso&#22238;&#24402;&#26469;&#25552;&#39640;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.09126</link><description>&lt;p&gt;
&#30693;&#35782;&#36801;&#31227;&#19979;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;: &#36716;&#31227;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Causal Learning: Causal Effect Estimation with Knowledge Transfer. (arXiv:2305.09126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\ell_1$-TCL&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#36801;&#31227;&#21644;Lasso&#22238;&#24402;&#26469;&#25552;&#39640;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#30456;&#21516;&#30340;&#21327;&#21464;&#37327;&#65288;&#25110;&#29305;&#24449;&#65289;&#31354;&#38388;&#35774;&#32622;&#19979;&#36890;&#36807;&#30693;&#35782;&#36801;&#31227;&#26469;&#25552;&#39640;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#31934;&#24230;&#65292;&#21363;&#21516;&#31867;&#21035;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#65292;&#23558;&#20854;&#31216;&#20026;&#36716;&#31227;&#22240;&#26524;&#23398;&#20064;&#65288;TCL&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;$\ell_1$-TCL&#65292;&#20854;&#20013;&#21253;&#21547;$\ell_1$&#27491;&#21017;&#21270;TL&#26469;&#36827;&#34892;&#33510;&#20107;&#21442;&#25968;&#20272;&#35745;&#21644;&#19979;&#28216;&#25554;&#20214;ACE&#20272;&#35745;&#22120;&#65292;&#21253;&#25324;&#32467;&#26524;&#22238;&#24402;&#12289;&#36870;&#27010;&#29575;&#21152;&#26435;&#21644;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#20511;&#21161;&#20110;Lasso&#29992;&#20110;&#39640;&#32500;&#22238;&#24402;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#24674;&#22797;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel problem of improving causal effect estimation accuracy with the help of knowledge transfer under the same covariate (or feature) space setting, i.e., homogeneous transfer learning (TL), is studied, referred to as the Transfer Causal Learning (TCL) problem. While most recent efforts in adapting TL techniques to estimate average causal effect (ACE) have been focused on the heterogeneous covariate space setting, those methods are inadequate for tackling the TCL problem since their algorithm designs are based on the decomposition into shared and domain-specific covariate spaces. To address this issue, we propose a generic framework called \texttt{$\ell_1$-TCL}, which incorporates $\ell_1$ regularized TL for nuisance parameter estimation and downstream plug-in ACE estimators, including outcome regression, inverse probability weighted, and doubly robust estimators. Most importantly, with the help of Lasso for high-dimensional regression, we establish non-asymptotic recovery guarantee
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIC-DDPM&#30340;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#21487;&#35265;&#24230;&#25968;&#25454;&#21644;&#33039;&#22270;&#20687;&#30340;&#24110;&#21161;&#19979;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#32454;&#33410;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#28040;&#38500;&#22122;&#22768;&#21644;&#20266;&#24433;&#12290;&#30456;&#20851;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#31639;&#27861;&#22312;&#24674;&#22797;&#24494;&#24369;&#20449;&#21495;&#12289;&#20445;&#30041;&#32454;&#33410;&#32467;&#26500;&#21644;&#28040;&#38500;&#20266;&#24433;&#31561;&#26041;&#38754;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09121</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23556;&#30005;&#24178;&#28041;&#25104;&#20687;&#37325;&#24314;&#30340;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Conditional Denoising Diffusion Probabilistic Model for Radio Interferometric Image Reconstruction. (arXiv:2305.09121v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIC-DDPM&#30340;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#21487;&#35265;&#24230;&#25968;&#25454;&#21644;&#33039;&#22270;&#20687;&#30340;&#24110;&#21161;&#19979;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#32454;&#33410;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#28040;&#38500;&#22122;&#22768;&#21644;&#20266;&#24433;&#12290;&#30456;&#20851;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#31639;&#27861;&#22312;&#24674;&#22797;&#24494;&#24369;&#20449;&#21495;&#12289;&#20445;&#30041;&#32454;&#33410;&#32467;&#26500;&#21644;&#28040;&#38500;&#20266;&#24433;&#31561;&#26041;&#38754;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#65292;&#23556;&#30005;&#26395;&#36828;&#38236;&#25509;&#25910;&#21040;&#30340;&#20449;&#21495;&#20250;&#36716;&#25442;&#20026;&#22825;&#20307;&#23545;&#35937;&#30340;&#22270;&#20687;&#12290;&#20294;&#36825;&#20123;&#22270;&#20687;&#36890;&#24120;&#20250;&#21253;&#21547;&#20266;&#28304;&#21644;&#20854;&#20182;&#22240;&#32032;&#23548;&#33268;&#30340;&#20266;&#24433;&#65292;&#31216;&#20026;&#8220;&#33039;&#22270;&#20687;&#8221;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#23545;&#33039;&#22270;&#20687;&#36827;&#34892;&#37325;&#24314;&#20197;&#33719;&#21462;&#26356;&#24178;&#20928;&#30340;&#22270;&#20687;&#24182;&#28040;&#38500;&#20266;&#24433;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#24674;&#22797;&#24494;&#24369;&#20449;&#21495;&#65292;&#20445;&#30041;&#32454;&#33410;&#32467;&#26500;&#21644;&#28040;&#38500;&#20266;&#24433;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIC-DDPM&#30340;&#21487;&#35265;&#24230;&#21644;&#22270;&#20687;&#26465;&#20214;&#19979;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21516;&#26102;&#21033;&#29992;&#26102;&#22495;&#21644;&#31354;&#22495;&#30340;&#20449;&#24687;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#36798;&#21040;&#28040;&#38500;&#22122;&#22768;&#21644;&#29983;&#25104;&#26356;&#32454;&#33410;&#30340;&#22270;&#20687;&#30340;&#30446;&#30340;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23792;&#20540;&#20449;&#22122;&#27604;&#12289;&#35270;&#35273;&#36136;&#37327;&#21644;&#24674;&#22797;&#24494;&#24369;&#20449;&#21495;&#30340;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In radio astronomy, signals from radio telescopes are transformed into images of observed celestial objects, or sources. However, these images, called dirty images, contain real sources as well as artifacts due to signal sparsity and other factors. Therefore, radio interferometric image reconstruction is performed on dirty images, aiming to produce clean images in which artifacts are reduced and real sources are recovered. So far, existing methods have limited success on recovering faint sources, preserving detailed structures, and eliminating artifacts. In this paper, we present VIC-DDPM, a Visibility and Image Conditioned Denoising Diffusion Probabilistic Model. Our main idea is to use both the original visibility data in the spectral domain and dirty images in the spatial domain to guide the image generation process with DDPM. This way, we can leverage DDPM to generate fine details and eliminate noise, while utilizing visibility data to separate signals from noise and retaining spat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#33258;&#21160;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#25968;&#25454;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#26080;&#38656;&#35782;&#21035;&#20803;&#29305;&#24449;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#22343;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09101</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#33258;&#21160;&#23398;&#20064;&#31639;&#27861;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Automatic learning algorithm selection for classification via convolutional neural networks. (arXiv:2305.09101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#33258;&#21160;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#25968;&#25454;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#26080;&#38656;&#35782;&#21035;&#20803;&#29305;&#24449;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#22343;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#20182;&#20219;&#21153;&#19968;&#26679;&#65292;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36807;&#31243;&#21487;&#20197;&#21463;&#30410;&#20110;&#20808;&#21069;&#30340;&#32463;&#39564;&#12290;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#36873;&#25321;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#20197;&#25552;&#39640;&#24403;&#21069;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#26041;&#26696;&#65292;&#30452;&#25509;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#20026;&#20108;&#36827;&#21046;&#20998;&#31867;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#23398;&#20064;&#25968;&#25454;&#22266;&#26377;&#30340;&#32467;&#26500;&#32780;&#19981;&#35782;&#21035;&#20803;&#29305;&#24449;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#27169;&#24335;&#26041;&#38754;&#36798;&#21040;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#22522;&#20110;&#20803;&#29305;&#24449;&#30340;&#20256;&#32479;&#20004;&#27493;&#26041;&#27861;&#12290;&#25991;&#20013;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#38543;&#21518;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As in any other task, the process of building machine learning models can benefit from prior experience. Meta-learning for classifier selection gains knowledge from characteristics of different datasets and/or previous performance of machine learning techniques to make better decisions for the current modeling process. Meta-learning approaches first collect meta-data that describe this prior experience and then use it as input for an algorithm selection model. In this paper, however, we propose an automatic learning scheme in which we train convolutional networks directly with the information of tabular datasets for binary classification. The goal of this study is to learn the inherent structure of the data without identifying meta-features. Experiments with simulated datasets show that the proposed approach achieves nearly perfect performance in identifying linear and nonlinear patterns, outperforming the traditional two-step method based on meta-features. The proposed method is then 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#23601;&#21487;&#20197;&#35757;&#32451;&#20986;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;KD&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.09098</link><description>&lt;p&gt;
&#20219;&#21153;&#26080;&#20851;BERT&#21387;&#32553;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Weight-Inherited Distillation for Task-Agnostic BERT Compression. (arXiv:2305.09098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#23601;&#21487;&#20197;&#35757;&#32451;&#20986;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;KD&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#21387;&#32553;BERT&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;KD&#26041;&#27861;&#20391;&#37325;&#20110;&#20026;&#23398;&#29983;&#27169;&#22411;&#35774;&#35745;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#65292;&#20197;&#27169;&#20223;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#36825;&#20123;&#26041;&#27861;&#20197;&#38388;&#25509;&#30340;&#26041;&#24335;&#20256;&#36882;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#65288;WID&#65289;&#26041;&#27861;&#65292;&#30452;&#25509;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#12290;WID&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#65292;&#36890;&#36807;&#32487;&#25215;&#26435;&#37325;&#26469;&#35757;&#32451;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#35270;&#35282;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#34892;&#21387;&#32553;&#22120;&#21644;&#21015;&#21387;&#32553;&#22120;&#35774;&#35745;&#20026;&#26144;&#23556;&#65292;&#28982;&#21518;&#36890;&#36807;&#32467;&#26500;&#37325;&#21442;&#25968;&#21270;&#21387;&#32553;&#26435;&#37325;&#12290;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WID&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;KD&#30340;&#22522;&#32447;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;WID&#20063;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27880;&#24847;&#21147;&#20998;&#24067;&#23545;&#40784;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25945;&#24072;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) is a predominant approach for BERT compression. Previous KD-based methods focus on designing extra alignment losses for the student model to mimic the behavior of the teacher model. These methods transfer the knowledge in an indirect way. In this paper, we propose a novel Weight-Inherited Distillation (WID), which directly transfers knowledge from the teacher. WID does not require any additional alignment loss and trains a compact student by inheriting the weights, showing a new perspective of knowledge distillation. Specifically, we design the row compactors and column compactors as mappings and then compress the weights via structural re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms previous state-of-the-art KD-based baselines. Further analysis indicates that WID can also learn the attention patterns from the teacher model without any alignment loss on attention distributions.
&lt;/p&gt;</description></item><item><title>ProtoVAE&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26080;&#30417;&#30563;&#35299;&#32544;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#21487;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.09092</link><description>&lt;p&gt;
ProtoVAE&#65306;&#26080;&#30417;&#30563;&#35299;&#32544;&#30340;&#21407;&#22411;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ProtoVAE: Prototypical Networks for Unsupervised Disentanglement. (arXiv:2305.09092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09092
&lt;/p&gt;
&lt;p&gt;
ProtoVAE&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26080;&#30417;&#30563;&#35299;&#32544;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#21487;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#36817;&#24180;&#26469;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#25968;&#25454;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20173;&#38656;&#25506;&#32034;&#22914;&#20309;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#21487;&#35299;&#37322;&#25110;&#21487;&#35828;&#26126;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#26080;&#30417;&#30563;&#35299;&#32544;&#38382;&#39064;&#23588;&#20026;&#37325;&#35201;&#65292;&#23427;&#25552;&#20986;&#20102;&#21457;&#29616;&#25968;&#25454;&#20013;&#19981;&#21516;&#30340;&#28508;&#22312;&#21464;&#21270;&#22240;&#32032;&#25110;&#35821;&#20041;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#26631;&#35760;&#26679;&#26412;&#65292;&#24182;&#23558;&#23427;&#20204;&#32534;&#30721;&#20026;&#32467;&#26500;&#19978;&#19981;&#30456;&#20132;&#30340;&#28508;&#22312;&#34920;&#31034;&#24418;&#24335;&#12290;&#22312;&#32593;&#32476;&#20013;&#27809;&#26377;&#39069;&#22806;&#30340;&#32422;&#26463;&#25110;&#24402;&#32435;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#32534;&#30721;&#22240;&#32032;&#65292;&#20294;&#19981;&#19968;&#23450;&#20197;&#35299;&#32544;&#30340;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modeling and self-supervised learning have in recent years made great strides towards learning from data in a completely unsupervised way. There is still however an open area of investigation into guiding a neural network to encode the data into representations that are interpretable or explainable. The problem of unsupervised disentanglement is of particular importance as it proposes to discover the different latent factors of variation or semantic concepts from the data alone, without labeled examples, and encode them into structurally disjoint latent representations. Without additional constraints or inductive biases placed in the network, a generative model may learn the data distribution and encode the factors, but not necessarily in a disentangled way. Here, we introduce a novel deep generative VAE-based model, ProtoVAE, that leverages a deep metric learning Prototypical network trained using self-supervision to impose these constraints. The prototypical network constr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;Hessian&#26144;&#23556;&#65292;&#25581;&#31034;&#20102;CNN&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#26412;&#36136;&#65292;&#24182;&#35777;&#26126;&#20102;Hessian&#31209;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#38271;&#32780;&#21576;&#29616;&#20986;&#24179;&#26041;&#26681;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2305.09088</link><description>&lt;p&gt;
&#22522;&#20110;Hessian&#26144;&#23556;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26412;&#36136;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
The Hessian perspective into the Nature of Convolutional Neural Networks. (arXiv:2305.09088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;Hessian&#26144;&#23556;&#65292;&#25581;&#31034;&#20102;CNN&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#26412;&#36136;&#65292;&#24182;&#35777;&#26126;&#20102;Hessian&#31209;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#38271;&#32780;&#21576;&#29616;&#20986;&#24179;&#26041;&#26681;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#19968;&#30452;&#34987;&#30740;&#31350;&#12289;&#24212;&#29992;&#21644;&#29702;&#35770;&#21270;&#65292;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#20174;&#23427;&#20204;&#30340;Hessian&#26144;&#23556;&#30340;&#35282;&#24230;&#25552;&#20379;&#19968;&#20010;&#31245;&#24494;&#19981;&#21516;&#30340;&#35266;&#28857;&#65292;&#22240;&#20026;&#25439;&#22833;&#30340;Hessian&#25429;&#25417;&#20102;&#21442;&#25968;&#30340;&#25104;&#23545;&#20132;&#20114;&#65292;&#22240;&#27492;&#24418;&#25104;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#22522;&#30784;&#26469;&#25506;&#32034;CNN&#30340;&#26550;&#26500;&#26041;&#38754;&#22914;&#20309;&#34920;&#29616;&#20986;&#23427;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;CNN&#30340;Toeplitz&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#25581;&#31034;Hessian&#32467;&#26500;&#65292;&#29305;&#21035;&#26159;&#23427;&#30340;&#31209;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32039;&#23494;&#30340;&#19978;&#30028;&#65288;&#20351;&#29992;&#32447;&#24615;&#28608;&#27963;&#65289;&#65292;&#23427;&#20204;&#32039;&#23494;&#22320;&#36981;&#24490;&#20102;Hessian&#31209;&#30340;&#32463;&#39564;&#36235;&#21183;&#65292;&#24182;&#22312;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#20013;&#20445;&#25345;&#22312;&#23454;&#36341;&#20013;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#27010;&#25324;&#21644;&#30830;&#35748;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#27934;&#35265;&#65292;&#21363;&#21363;&#20351;&#22312;CNNs&#20013;&#65292;Hessian&#31209;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#38271;&#32780;&#21576;&#29616;&#20986;&#24179;&#26041;&#26681;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Convolutional Neural Networks (CNNs) have long been investigated and applied, as well as theorized, we aim to provide a slightly different perspective into their nature -- through the perspective of their Hessian maps. The reason is that the loss Hessian captures the pairwise interaction of parameters and therefore forms a natural ground to probe how the architectural aspects of CNN get manifested in its structure and properties. We develop a framework relying on Toeplitz representation of CNNs, and then utilize it to reveal the Hessian structure and, in particular, its rank. We prove tight upper bounds (with linear activations), which closely follow the empirical trend of the Hessian rank and hold in practice in more general settings. Overall, our work generalizes and establishes the key insight that, even in CNNs, the Hessian rank grows as the square root of the number of parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24694;&#24847;&#32593;&#31449;&#26816;&#27979;&#20013;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#12289;&#27169;&#22411;&#26500;&#24314;&#21644;&#25216;&#26415;&#25193;&#23637;&#31561;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#27492;&#39046;&#22495;&#12290;&#25991;&#20013;&#36824;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21450;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.09084</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24694;&#24847;&#32593;&#31449;&#26816;&#27979;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Data-driven Approaches for Malicious Website Detection. (arXiv:2305.09084v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24694;&#24847;&#32593;&#31449;&#26816;&#27979;&#20013;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#12289;&#27169;&#22411;&#26500;&#24314;&#21644;&#25216;&#26415;&#25193;&#23637;&#31561;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#27492;&#39046;&#22495;&#12290;&#25991;&#20013;&#36824;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21450;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#26816;&#27979;&#24694;&#24847;&#32593;&#31449;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;&#29992;&#20110;&#26816;&#27979;&#24694;&#24847;&#32593;&#31449;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#20256;&#32479;&#26041;&#27861;&#21450;&#20854;&#38480;&#21046;&#65292;&#28982;&#21518;&#27010;&#36848;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#25968;&#25454;-&#29305;&#24449;-&#27169;&#22411;-&#25193;&#23637;&#31649;&#36947;&#65292;&#24182;&#20171;&#32461;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#12289;&#27169;&#22411;&#26500;&#24314;&#21644;&#25216;&#26415;&#25193;&#23637;&#31561;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#27604;&#36739;&#20102;&#36817;&#24180;&#26469;&#25552;&#20986;&#30340;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#36981;&#24490;&#25968;&#25454;-&#29305;&#24449;-&#27169;&#22411;-&#25193;&#23637;&#31649;&#36947;&#65292;&#35752;&#35770;&#20102;&#24694;&#24847;&#32593;&#31449;&#26816;&#27979;&#20013;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#20197;&#21450;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of malicious websites has become a critical issue in cybersecurity. Therefore, this paper offers a comprehensive review of data-driven methods for detecting malicious websites. Traditional approaches and their limitations are discussed, followed by an overview of data-driven approaches. The paper establishes the data-feature-model-extension pipeline and the latest research developments of data-driven approaches, including data preprocessing, feature extraction, model construction and technology extension. Specifically, this paper compares methods using deep learning models proposed in recent years. Furthermore, the paper follows the data-feature-model-extension pipeline to discuss the challenges together with some future directions of data-driven methods in malicious website detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#21017;&#21270;&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#65292;&#21517;&#20026;Fini&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;Skew-t&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#8220;S-DoF&#29190;&#28856;&#8221;&#38382;&#39064;&#65292;&#25552;&#39640;&#28151;&#21512;&#27169;&#22411;&#30340;&#24314;&#27169;&#27867;&#21270;&#24615;&#21644;&#24377;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09071</link><description>&lt;p&gt;
FiMReSt: &#19968;&#31181;&#28789;&#27963;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#22810;&#38598;&#32676;&#25968;&#25454;&#30340;&#38750;&#23545;&#31216;&#20998;&#24067;&#38750;&#39640;&#26031;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
FiMReSt: Finite Mixture of Multivariate Regulated Skew-t Kernels -- A Flexible Probabilistic Model for Multi-Clustered Data with Asymmetrically-Scattered Non-Gaussian Kernels. (arXiv:2305.09071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#21017;&#21270;&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#65292;&#21517;&#20026;Fini&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;Skew-t&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#8220;S-DoF&#29190;&#28856;&#8221;&#38382;&#39064;&#65292;&#25552;&#39640;&#28151;&#21512;&#27169;&#22411;&#30340;&#24314;&#27169;&#27867;&#21270;&#24615;&#21644;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Skew-t&#28151;&#21512;&#27169;&#22411;&#34987;&#24341;&#20837;&#20316;&#20026;&#19968;&#31181;&#28789;&#27963;&#30340;&#27010;&#29575;&#24314;&#27169;&#25216;&#26415;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#31751;&#20013;&#30340;&#20559;&#26012;&#21644;&#32479;&#35745;&#33258;&#30001;&#24230;&#65288;S-DoF&#65289;&#65292;&#20197;&#25552;&#39640;&#24314;&#27169;&#30340;&#27867;&#21270;&#24615;&#21644;&#23545;&#37325;&#23614;&#21644;&#20559;&#26012;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;Skew-t&#28151;&#21512;&#27169;&#22411;&#22522;&#26412;&#19978;&#21463;&#21040;&#19968;&#31181;&#21517;&#20026;&#8220;S-DoF&#29190;&#28856;&#8221;&#30340;&#38544;&#34255;&#29616;&#35937;&#30340;&#22256;&#25200;&#65292;&#22312;&#26399;&#26395;&#26497;&#20540;&#21270;&#30340;&#38750;&#20984;&#36845;&#20195;&#36807;&#31243;&#20013;&#65292;&#23548;&#33268;&#27491;&#24577;&#26680;&#30340;&#24418;&#29366;&#20135;&#29983;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#20851;&#20110;S-DoF&#19981;&#31283;&#23450;&#24615;&#30340;&#35265;&#35299;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20869;&#26680;&#20174;t&#20998;&#24067;&#30340;&#28151;&#21512;&#20013;&#21457;&#25955;&#65292;&#22833;&#21435;&#24314;&#27169;&#31163;&#32676;&#20540;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24378;&#24230;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#21017;&#21270;&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#35757;&#32451;&#28151;&#21512;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#25216;&#26415;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24377;&#24615;&#12290;&#26368;&#32456;&#30340;&#28151;&#21512;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;Fini&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently skew-t mixture models have been introduced as a flexible probabilistic modeling technique taking into account both skewness in data clusters and the statistical degree of freedom (S-DoF) to improve modeling generalizability, and robustness to heavy tails and skewness. In this paper, we show that the state-of-the-art skew-t mixture models fundamentally suffer from a hidden phenomenon named here as "S-DoF explosion," which results in local minima in the shapes of normal kernels during the non-convex iterative process of expectation maximization. For the first time, this paper provides insights into the instability of the S-DoF, which can result in the divergence of the kernels from the mixture of t-distribution, losing generalizability and power for modeling the outliers. Thus, in this paper, we propose a regularized iterative optimization process to train the mixture model, enhancing the generalizability and resiliency of the technique. The resulting mixture model is named Fini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;THEMES&#23398;&#24466;&#24335;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#21307;&#30103;&#20445;&#20581;&#31561;&#20154;&#31867;&#20013;&#24515;&#20219;&#21153;&#20013;&#30340;&#36827;&#21270;&#22870;&#21169;&#20989;&#25968;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.09070</link><description>&lt;p&gt;
&#19968;&#31181;&#31163;&#32447;&#26102;&#38388;&#24863;&#30693;&#30340;&#23398;&#24466;&#24335;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#36827;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
An Offline Time-aware Apprenticeship Learning Framework for Evolving Reward Functions. (arXiv:2305.09070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;THEMES&#23398;&#24466;&#24335;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#21307;&#30103;&#20445;&#20581;&#31561;&#20154;&#31867;&#20013;&#24515;&#20219;&#21153;&#20013;&#30340;&#36827;&#21270;&#22870;&#21169;&#20989;&#25968;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#24466;&#24335;&#23398;&#20064;&#26159;&#25351;&#36890;&#36807;&#35266;&#23519;&#21644;&#27169;&#20223;&#19987;&#23478;&#30340;&#28436;&#31034;&#26469;&#35825;&#23548;&#26377;&#25928;&#30340;&#20915;&#31574;&#31574;&#30053;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23398;&#24466;&#24335;&#23398;&#20064;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;&#24212;&#23545;&#20154;&#31867;&#20013;&#24515;&#20219;&#21153;&#20013;&#24120;&#35265;&#30340;&#36827;&#21270;&#22870;&#21169;&#20989;&#25968;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#36825;&#31181;&#20219;&#21153;&#38656;&#35201;&#31163;&#32447;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#26102;&#38388;&#24863;&#30693;&#30340;&#20998;&#23618;EM&#33021;&#37327;&#23376;&#36712;&#36857; (THEMES) &#23398;&#24466;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#36827;&#21270;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#23545;&#25361;&#25112;&#24615;&#20219;&#21153;&#65288;&#36133;&#34880;&#30151;&#27835;&#30103;&#65289;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;THEMES&#26497;&#22823;&#22320;&#32988;&#36807;&#20102;&#31454;&#20105;&#23545;&#25163;&#30340;&#26368;&#26032;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Apprenticeship learning (AL) is a process of inducing effective decision-making policies via observing and imitating experts' demonstrations. Most existing AL approaches, however, are not designed to cope with the evolving reward functions commonly found in human-centric tasks such as healthcare, where offline learning is required. In this paper, we propose an offline Time-aware Hierarchical EM Energy-based Sub-trajectory (THEMES) AL framework to tackle the evolving reward functions in such tasks. The effectiveness of THEMES is evaluated via a challenging task -- sepsis treatment. The experimental results demonstrate that THEMES can significantly outperform competitive state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;IRT&#30340;&#27169;&#22411;&#26469;&#20998;&#26512;&#20154;&#31867;&#23545;&#20110;AI&#30340;&#24863;&#30693;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20154;&#20204;&#26222;&#36941;&#26399;&#26395;AI&#22242;&#38431;&#20249;&#20276;&#30340;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#24182;&#19988;&#20154;&#20204;&#23545;AI&#19982;&#20154;&#31867;&#25104;&#21592;&#30340;&#24515;&#29702;&#27169;&#22411;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2305.09064</link><description>&lt;p&gt;
&#25429;&#25417;&#20154;&#31867;&#23545;AI&#30340;&#24515;&#29702;&#27169;&#22411;&#65306;&#22522;&#20110;IRT&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Capturing Humans' Mental Models of AI: An Item Response Theory Approach. (arXiv:2305.09064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;IRT&#30340;&#27169;&#22411;&#26469;&#20998;&#26512;&#20154;&#31867;&#23545;&#20110;AI&#30340;&#24863;&#30693;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20154;&#20204;&#26222;&#36941;&#26399;&#26395;AI&#22242;&#38431;&#20249;&#20276;&#30340;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#24182;&#19988;&#20154;&#20204;&#23545;AI&#19982;&#20154;&#31867;&#25104;&#21592;&#30340;&#24515;&#29702;&#27169;&#22411;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#20837;&#20102;&#35299;&#20154;&#31867;&#22914;&#20309;&#24863;&#30693;&#19982;AI&#25645;&#26723;&#30340;&#20449;&#24687;&#65292;&#26159;&#25105;&#20204;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#21512;&#20316;&#30340;&#37325;&#35201;&#22522;&#30784;&#12290;&#25105;&#20204;&#20511;&#37492;&#35748;&#30693;&#31185;&#23398;&#30340;&#30456;&#20851;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;IRT&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#20154;&#20204;&#30340;&#24863;&#30693;&#65292;&#38024;&#23545;&#20154;&#19982;&#20154;&#12289;&#20154;&#19982;AI&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#37325;&#28857;&#30740;&#31350;&#20004;&#32773;&#24863;&#30693;&#20043;&#38388;&#30340;&#24322;&#21516;&#65292;&#20197;&#21450;&#20154;&#20204;&#23545;AI&#22242;&#38431;&#20249;&#20276;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#20204;&#26222;&#36941;&#26399;&#26395;&#20154;&#24037;&#26234;&#33021;&#30340;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#32780;&#19988;&#23545;AI&#25104;&#21592;&#30340;&#24515;&#29702;&#27169;&#22411;&#19982;&#20154;&#31867;&#25104;&#21592;&#30340;&#19981;&#21516;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#23398;&#20064;&#12289;&#21453;&#39304;&#21644;&#20010;&#20307;&#24046;&#24322;&#31561;&#22240;&#32032;&#21516;&#26679;&#26377;&#21487;&#33021;&#24433;&#21709;&#20154;&#20204;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25429;&#25417;&#21644;&#20998;&#26512;&#20154;&#31867;&#23545;&#19981;&#21516;&#24773;&#22659;&#20013;&#30340;AI&#24863;&#30693;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving our understanding of how humans perceive AI teammates is an important foundation for our general understanding of human-AI teams. Extending relevant work from cognitive science, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate's performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people's perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants' own self-perception. Our results indicate that people expect AI agents' performance to be significantly better on average than the performance of other hu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#26144;&#23556;B-KRnet&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25968;&#25454;&#25110;PDE&#30340;&#23494;&#24230;&#20272;&#35745;/&#36817;&#20284;&#65292;&#30001;&#20110;&#20854;&#23450;&#20041;&#22312;&#26377;&#30028;&#22495;&#19978;&#65292;&#22240;&#27492;&#27604;KRnet&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.09063</link><description>&lt;p&gt;
&#26377;&#30028;KRnet&#21450;&#20854;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#36817;&#20284;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bounded KRnet and its applications to density estimation and approximation. (arXiv:2305.09063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#26144;&#23556;B-KRnet&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25968;&#25454;&#25110;PDE&#30340;&#23494;&#24230;&#20272;&#35745;/&#36817;&#20284;&#65292;&#30001;&#20110;&#20854;&#23450;&#20041;&#22312;&#26377;&#30028;&#22495;&#19978;&#65292;&#22240;&#27492;&#27604;KRnet&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#26377;&#30028;&#22495;&#19978;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#36870;&#26144;&#23556;&#65292;&#31216;&#20026;B-KRnet&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25968;&#25454;&#25110;PDE&#65288;&#20363;&#22914;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#21644;Keller-Segel&#26041;&#31243;&#65289;&#30340;&#23494;&#24230;&#20272;&#35745;/&#36817;&#20284;&#12290;&#19982;KRnet&#31867;&#20284;&#65292;B-KRnet&#30340;&#32467;&#26500;&#23558;Knothe-Rosenblatt&#37325;&#25490;&#30340;&#19977;&#35282;&#24418;&#24418;&#24335;&#36716;&#21270;&#20026;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#12290;B-KRnet&#21644;KRnet&#20043;&#38388;&#30340;&#20027;&#35201;&#21306;&#21035;&#26159;B-KRnet&#23450;&#20041;&#22312;&#36229;&#31435;&#26041;&#20307;&#19978;&#65292;&#32780;KRnet&#23450;&#20041;&#22312;&#25972;&#20010;&#31354;&#38388;&#19978;&#65292;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#22312;B-KRnet&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#26469;&#20445;&#25345;&#31934;&#30830;&#30340;&#21487;&#36870;&#24615;&#12290;&#23558;B-KRnet&#29992;&#20316;&#20256;&#36755;&#26144;&#23556;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDF&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23545;&#24212;&#20110;&#20808;&#39564;&#65288;&#22343;&#21248;&#65289;&#20998;&#24067;&#22312;&#36229;&#31435;&#26041;&#20307;&#19978;&#30340;&#25512;&#31227;&#12290;&#20026;&#20102;&#36817;&#20284;&#35745;&#31639;&#22495;&#19978;&#23450;&#20041;&#30340;PDF&#65292;B-KRnet&#27604;KRnet&#26356;&#26377;&#25928;&#12290;&#36890;&#36807;&#32806;&#21512;KRnet&#21644;B-KRnet&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#22312;&#39640;&#32500;&#22495;&#19978;&#23450;&#20041;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop an invertible mapping, called B-KRnet, on a bounded domain and apply it to density estimation/approximation for data or the solutions of PDEs such as the Fokker-Planck equation and the Keller-Segel equation. Similar to KRnet, the structure of B-KRnet adapts the triangular form of the Knothe-Rosenblatt rearrangement into a normalizing flow model. The main difference between B-KRnet and KRnet is that B-KRnet is defined on a hypercube while KRnet is defined on the whole space, in other words, we introduce a new mechanism in B-KRnet to maintain the exact invertibility. Using B-KRnet as a transport map, we obtain an explicit probability density function (PDF) model that corresponds to the pushforward of a prior (uniform) distribution on the hypercube. To approximate PDFs defined on a bounded computational domain, B-KRnet is more effective than KRnet. By coupling KRnet and B-KRnet, we can also define a deep generative model on a high-dimensional domain where some di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#30340;transformer&#26041;&#27861;&#24182;&#24212;&#29992;&#20110;fMRI&#25968;&#25454;&#65292;&#39318;&#27425;&#34920;&#26126;&#22810;&#20219;&#21153;&#35757;&#32451;&#23545;fMRI&#25968;&#25454;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#12290;&#39044;&#35757;&#32451;&#20219;&#21153;&#20419;&#36827;&#36801;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2305.09057</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#24207;&#21015;&#25968;&#25454;&#29992;&#20110;&#33041;&#35299;&#30721;&#20219;&#21153;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Pretraining on Paired Sequences of fMRI Data for Transfer Learning to Brain Decoding Tasks. (arXiv:2305.09057v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#30340;transformer&#26041;&#27861;&#24182;&#24212;&#29992;&#20110;fMRI&#25968;&#25454;&#65292;&#39318;&#27425;&#34920;&#26126;&#22810;&#20219;&#21153;&#35757;&#32451;&#23545;fMRI&#25968;&#25454;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#12290;&#39044;&#35757;&#32451;&#20219;&#21153;&#20419;&#36827;&#36801;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#30340;transformer&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21516;&#26102;&#22312;&#20004;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#19978;&#23545;&#26550;&#26500;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25945;&#20250;&#27169;&#22411;&#23545;&#38899;&#20048;&#21548;&#35273;&#30382;&#23618;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#21160;&#24577;&#26377;&#19968;&#20010;&#26222;&#36941;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#32467;&#26524;&#39318;&#27425;&#34920;&#26126;&#65292;&#22810;&#20219;&#21153;&#35757;&#32451;&#23545;fMRI&#25968;&#25454;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21644;&#22312;&#19968;&#20010;&#21463;&#30417;&#30563;&#30340;fMRI&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#39069;&#22806;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#24494;&#35843;&#27169;&#22411;&#19978;&#65292;&#20445;&#30041;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#36825;&#35777;&#26126;&#20102;&#25105;&#20204;&#39044;&#35757;&#32451;&#20219;&#21153;&#20419;&#36827;&#36801;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#20026;&#39044;&#35757;&#32451;&#21644;fMRI&#25968;&#25454;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;transformer&#26550;&#26500;&#22686;&#21152;&#20102;&#25968;&#37327;&#19978;&#30340;&#35777;&#25454;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;fMRI&#25968;&#25454;&#19978;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we introduce a self-supervised pretraining framework for transformers on functional Magnetic Resonance Imaging (fMRI) data. First, we pretrain our architecture on two self-supervised tasks simultaneously to teach the model a general understanding of the temporal and spatial dynamics of human auditory cortex during music listening. Our pretraining results are the first to suggest a synergistic effect of multitask training on fMRI data. Second, we finetune the pretrained models and train additional fresh models on a supervised fMRI classification task. We observe significantly improved accuracy on held-out runs with the finetuned models, which demonstrates the ability of our pretraining tasks to facilitate transfer learning. This work contributes to the growing body of literature on transformer architectures for pretraining and transfer learning with fMRI data, and serves as a proof of concept for our pretraining tasks and multitask pretraining on fMRI data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#27169;&#25311;&#20117;&#25511;&#21046;&#19979;&#22320;&#19979;&#22810;&#23380;&#20171;&#36136;&#27969;&#20307;&#27969;&#21160;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#20986;&#26410;&#26469;&#30340;&#27833;&#34255;&#29366;&#24577;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.09056</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#21367;&#31215;&#24490;&#29615;&#26367;&#20195;&#27169;&#22411;&#22312;&#24102;&#20117;&#25511;&#21046;&#19979;&#30340;&#27833;&#34255;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Convolutional Recurrent Surrogate Model for Reservoir Simulation with Well Controls. (arXiv:2305.09056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#27169;&#25311;&#20117;&#25511;&#21046;&#19979;&#22320;&#19979;&#22810;&#23380;&#20171;&#36136;&#27969;&#20307;&#27969;&#21160;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#39044;&#27979;&#20986;&#26410;&#26469;&#30340;&#27833;&#34255;&#29366;&#24577;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(PICRNN)&#27169;&#25311;&#20117;&#25511;&#21046;&#19979;&#30340;&#22320;&#19979;&#27969;&#20307;&#27969;&#21160;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;(ConvLSTM)&#26469;&#25429;&#25417;&#22810;&#23380;&#27969;&#20307;&#29366;&#24577;&#28436;&#21464;&#21160;&#24577;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#12290;ConvLSTM&#19982;&#29366;&#24577;&#31354;&#38388;&#26041;&#31243;&#30456;&#36830;&#65292;&#20351;&#24471;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#20117;&#25511;&#20449;&#21495;&#33021;&#22815;&#34987;&#32435;&#20837;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#38656;&#35201;&#36755;&#20837;&#21021;&#22987;&#29366;&#24577;&#26465;&#20214;&#21644;&#20117;&#25511;&#24207;&#21015;&#65292;&#24182;&#20197;&#36755;&#20986;&#29366;&#24577;&#21464;&#37327;&#65288;&#22914;&#21387;&#21147;&#65289;&#20316;&#20026;&#36755;&#20986;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#27833;&#34255;&#27969;&#20307;&#29366;&#24577;-&#31354;&#38388;&#26041;&#31243;&#30340;&#27531;&#24046;&#65292;&#32593;&#32476;&#33021;&#22815;&#22312;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#30340;&#35774;&#35745;&#26088;&#22312;&#29992;&#20316;&#39044;&#27979;&#26410;&#26469;&#27833;&#34255;&#29366;&#24577;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22522;&#20110;&#21021;&#22987;&#27833;&#34255;&#29366;&#24577;&#21644;&#36755;&#20837;&#30340;&#25511;&#21046;&#24037;&#31243;&#12290;&#22312;&#29366;&#24577;&#31354;&#38388;&#26041;&#31243;&#20013;&#24378;&#21046;&#36793;&#30028;&#26465;&#20214;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25439;&#32791;&#39033;&#26469;&#30830;&#20445;&#27169;&#22411;&#30340;&#29289;&#29702;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel surrogate model for modeling subsurface fluid flow with well controls using a physics-informed convolutional recurrent neural network (PICRNN). The model uses a convolutional long-short term memory (ConvLSTM) to capture the spatiotemporal dependencies of the state evolution dynamics in the porous flow. The ConvLSTM is linked to the state space equations, enabling the incorporation of a discrete-time sequence of well control. The model requires initial state condition and a sequence of well controls as inputs, and predicts the state variables of the system, such as pressure, as output. By minimizing the residuals of reservoir flow state-space equations, the network is trained without the need for labeled data. The model is designed to serve as a surrogate model for predicting future reservoir states based on the initial reservoir state and input engineering controls. Boundary conditions are enforced into the state-space equations so no additional loss term is
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#27714;&#35299;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#36895;&#24230;&#24555;&#19988;&#31616;&#21333;&#26131;&#34892;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.09046</link><description>&lt;p&gt;
&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Convex optimization over a probability simplex. (arXiv:2305.09046v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09046
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#27714;&#35299;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#36895;&#24230;&#24555;&#19988;&#31616;&#21333;&#26131;&#34892;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26041;&#26696;&#8212;&#8212;&#26607;&#35199;&#21333;&#32431;&#24418;&#26469;&#20248;&#21270;&#20984;&#38382;&#39064;&#65292;&#20351;&#20854;&#28385;&#36275;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#38480;&#21046;&#26465;&#20214;&#65292;&#21363;$w\in\mathbb{R}^n$&#20013;$\sum_i w_i=1$&#65292;$w_i\geq0$&#12290;&#25105;&#20204;&#23558;&#21333;&#32431;&#24418;&#26144;&#23556;&#21040;&#21333;&#20301;&#29699;&#30340;&#27491;&#22235;&#38754;&#20307;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#33719;&#24471;&#38544;&#21464;&#37327;&#30340;&#35299;&#65292;&#24182;&#23558;&#32467;&#26524;&#26144;&#23556;&#22238;&#21407;&#22987;&#21464;&#37327;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#32500;&#38382;&#39064;&#65292;&#27599;&#27425;&#36845;&#20195;&#30001;&#31616;&#21333;&#30340;&#25805;&#20316;&#32452;&#25104;&#65292;&#19988;&#38024;&#23545;&#20984;&#20989;&#25968;&#35777;&#26126;&#20102;&#25910;&#25947;&#36895;&#24230;&#20026;${O}(1/T)$&#12290;&#21516;&#26102;&#26412;&#25991;&#20851;&#27880;&#20102;&#20449;&#24687;&#29702;&#35770;&#65288;&#22914;&#20132;&#21449;&#29109;&#21644;KL&#25955;&#24230;&#65289;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new iteration scheme, the Cauchy-Simplex, to optimize convex problems over the probability simplex $\{w\in\mathbb{R}^n\ |\ \sum_i w_i=1\ \textrm{and}\ w_i\geq0\}$. Other works have taken steps to enforce positivity or unit normalization automatically but never simultaneously within a unified setting. This paper presents a natural framework for manifestly requiring the probability condition. Specifically, we map the simplex to the positive quadrant of a unit sphere, envisage gradient descent in latent variables, and map the result back in a way that only depends on the simplex variable. Moreover, proving rigorous convergence results in this formulation leads inherently to tools from information theory (e.g. cross entropy and KL divergence). Each iteration of the Cauchy-Simplex consists of simple operations, making it well-suited for high-dimensional problems. We prove that it has a convergence rate of ${O}(1/T)$ for convex functions, and numerical experiments of projection 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#21098;&#26525;&#26041;&#27861;&#65292;&#26469;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#24182;&#35299;&#20915;&#30001;&#20110;&#35774;&#22791;&#22686;&#22810;&#23548;&#33268;&#30340;&#23398;&#20064;&#24310;&#36831;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09042</link><description>&lt;p&gt;
&#20998;&#23618;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Adaptive Federated Pruning in Hierarchical Wireless Networks. (arXiv:2305.09042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#21098;&#26525;&#26041;&#27861;&#65292;&#26469;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#24182;&#35299;&#20915;&#30001;&#20110;&#35774;&#22791;&#22686;&#22810;&#23548;&#33268;&#30340;&#23398;&#20064;&#24310;&#36831;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#27719;&#24635;&#20102;&#30001;&#22810;&#20010;&#35774;&#22791;&#26356;&#26032;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#35775;&#38382;&#23427;&#20204;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#12290;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#35774;&#22791;-&#36793;&#32536;-&#20113;&#32858;&#21512;&#23618;&#27425;&#32467;&#26500;&#65292;&#26082;&#21487;&#20197;&#20139;&#21463;&#20113;&#26381;&#21153;&#22120;&#23545;&#26356;&#22810;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#26435;&#65292;&#21448;&#21487;&#20197;&#20139;&#21463;&#36793;&#32536;&#26381;&#21153;&#22120;&#19982;&#35774;&#22791;&#20043;&#38388;&#30340;&#26377;&#25928;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36793;&#32536;&#26381;&#21153;&#22120;&#21644;&#35774;&#22791;&#25968;&#37327;&#22686;&#21152;&#65292;&#26412;&#22320;&#35745;&#31639;&#33021;&#21147;&#21644;&#36890;&#20449;&#24102;&#23485;&#26377;&#38480;&#65292;HFL&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#21152;&#20250;&#23548;&#33268;&#23398;&#20064;&#24310;&#36831;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#20351;&#29992;&#27169;&#22411;&#21098;&#26525;&#26469;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#27169;&#22411;&#21098;&#26525;&#30340;HFL&#26799;&#24230;l2&#33539;&#25968;&#19978;&#38480;&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21098;&#26525;&#26041;&#26696;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#22312;&#32473;&#23450;&#30340;&#24310;&#36831;&#38408;&#20540;&#19979;&#26368;&#22823;&#21270;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a promising privacy-preserving distributed learning framework where a server aggregates models updated by multiple devices without accessing their private datasets. Hierarchical FL (HFL), as a device-edge-cloud aggregation hierarchy, can enjoy both the cloud server's access to more datasets and the edge servers' efficient communications with devices. However, the learning latency increases with the HFL network scale due to the increasing number of edge servers and devices with limited local computation capability and communication bandwidth. To address this issue, in this paper, we introduce model pruning for HFL in wireless networks to reduce the neural network scale. We present the convergence analysis of an upper on the l2 norm of gradients for HFL with model pruning, analyze the computation and communication latency of the proposed model pruning scheme, and formulate an optimization problem to maximize the convergence rate under a given latency threshold 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;Tractography&#20013;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;RL&#31639;&#27861;&#36873;&#25321;&#12289;&#20195;&#29702;&#20154;&#36755;&#20837;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#25773;&#31181;&#31574;&#30053;&#30340;&#19968;&#31995;&#21015;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.09041</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;Tractography&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
What Matters in Reinforcement Learning for Tractography. (arXiv:2305.09041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;Tractography&#20013;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;RL&#31639;&#27861;&#36873;&#25321;&#12289;&#20195;&#29702;&#20154;&#36755;&#20837;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#25773;&#31181;&#31574;&#30053;&#30340;&#19968;&#31995;&#21015;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#23398;&#20064;Tractography&#36807;&#31243;&#24182;&#35757;&#32451;&#20195;&#29702;&#20154;&#22312;&#27809;&#26377;&#25163;&#21160;&#31579;&#36873;&#30340;&#21442;&#32771;&#27969;&#32447;&#30340;&#24773;&#20917;&#19979;&#37325;&#24314;&#30333;&#36136;&#32467;&#26500;&#12290;&#34429;&#28982;&#25253;&#21578;&#30340;&#34920;&#29616;&#39047;&#20855;&#31454;&#20105;&#21147;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22797;&#26434;&#65292;&#24182;&#19988;&#23545;&#20110;&#20854;&#22810;&#20010;&#37096;&#20998;&#30340;&#20316;&#29992;&#21644;&#24433;&#21709;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#65292;&#20363;&#22914;RL&#31639;&#27861;&#30340;&#36873;&#25321;&#65292;&#25773;&#31181;&#31574;&#30053;&#65292;&#36755;&#20837;&#20449;&#21495;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#20204;&#30340;&#24433;&#21709;&#12290;&#26412;&#27425;&#30740;&#31350;&#20849;&#35757;&#32451;&#20102;&#32422;7400&#20010;&#27169;&#22411;&#65292;&#20849;&#35745;&#36817;41000&#23567;&#26102;&#30340;GPU&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25351;&#23548;&#28909;&#34935;&#20110;&#25506;&#32034;&#28145;&#24230;RL&#22312;Tractography&#20013;&#21487;&#33021;&#24615;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26368;&#32456;&#25552;&#20986;&#20102;&#20851;&#20110;RL&#31639;&#27861;&#30340;&#36873;&#25321;&#12289;&#20195;&#29702;&#20154;&#36755;&#20837;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#25773;&#31181;&#31574;&#30053;&#30340;&#19968;&#31995;&#21015;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep reinforcement learning (RL) has been proposed to learn the tractography procedure and train agents to reconstruct the structure of the white matter without manually curated reference streamlines. While the performances reported were competitive, the proposed framework is complex, and little is still known about the role and impact of its multiple parts. In this work, we thoroughly explore the different components of the proposed framework, such as the choice of the RL algorithm, seeding strategy, the input signal and reward function, and shed light on their impact. Approximately 7,400 models were trained for this work, totalling nearly 41,000 hours of GPU time. Our goal is to guide researchers eager to explore the possibilities of deep RL for tractography by exposing what works and what does not work with the category of approach. As such, we ultimately propose a series of recommendations concerning the choice of RL algorithm, the input to the agents, the reward function
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21160;&#24577;&#23398;&#20064;&#31995;&#32479;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23457;&#26597;&#29616;&#35937;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#38450;&#33539;&#23457;&#26597;&#30340;&#25514;&#26045;&#20197;&#21450;&#38543;&#26426;&#25506;&#32034;&#65292;&#20174;&#32780;&#30830;&#20445;&#26469;&#33258;&#34987;&#23457;&#26597;&#32452;&#30340;&#26679;&#26412;&#36827;&#20837;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#32416;&#27491;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.09035</link><description>&lt;p&gt;
&#21160;&#24577;&#23398;&#20064;&#31995;&#32479;&#30340;&#31639;&#27861;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Censoring in Dynamic Learning Systems. (arXiv:2305.09035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21160;&#24577;&#23398;&#20064;&#31995;&#32479;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23457;&#26597;&#29616;&#35937;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#38450;&#33539;&#23457;&#26597;&#30340;&#25514;&#26045;&#20197;&#21450;&#38543;&#26426;&#25506;&#32034;&#65292;&#20174;&#32780;&#30830;&#20445;&#26469;&#33258;&#34987;&#23457;&#26597;&#32452;&#30340;&#26679;&#26412;&#36827;&#20837;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#32416;&#27491;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#36873;&#25321;&#26631;&#35760;&#24433;&#21709;&#30340;&#21160;&#24577;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#20250;&#20986;&#29616;&#23457;&#26597;&#29616;&#35937;&#65292;&#21363;&#38024;&#23545;&#19968;&#32452;&#25110;&#22810;&#32452;&#25968;&#25454;&#28857;&#20998;&#37197;&#25345;&#32493;&#30340;&#36127;&#38754;&#39044;&#27979;&#12290;&#22312;&#28040;&#36153;&#37329;&#34701;&#31561;&#24212;&#29992;&#20013;&#65292;&#36825;&#20250;&#23548;&#33268;&#19968;&#20123;&#30003;&#35831;&#20154;&#32452;&#34987;&#25345;&#32493;&#25298;&#32477;&#65292;&#24182;&#19988;&#20174;&#26410;&#36827;&#20837;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#35268;&#33539;&#21270;&#23457;&#26597;&#29616;&#35937;&#65292;&#23637;&#31034;&#20854;&#21487;&#33021;&#30340;&#20986;&#29616;&#26041;&#24335;&#65292;&#24182;&#24378;&#35843;&#26816;&#27979;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#37319;&#21462;&#38450;&#33539;&#23457;&#26597;&#30340;&#25514;&#26045;&#65292;&#24182;&#36827;&#34892;&#38543;&#26426;&#25506;&#32034;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#30830;&#20445;&#25105;&#20204;&#23545;&#21407;&#26412;&#26410;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#25216;&#26415;&#33021;&#22815;&#35753;&#26469;&#33258;&#34987;&#23457;&#26597;&#32452;&#30340;&#26679;&#26412;&#36827;&#20837;&#35757;&#32451;&#25968;&#25454;&#24182;&#32416;&#27491;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#23457;&#26597;&#30340;&#19981;&#21487;&#27979;&#37327;&#30340;&#21361;&#23475;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#32531;&#35299;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic learning systems subject to selective labeling exhibit censoring, i.e. persistent negative predictions assigned to one or more subgroups of points. In applications like consumer finance, this results in groups of applicants that are persistently denied and thus never enter into the training data. In this work, we formalize censoring, demonstrate how it can arise, and highlight difficulties in detection. We consider safeguards against censoring recourse and randomized-exploration - both of which ensure we collect labels for points that would otherwise go unobserved. The resulting techniques allow examples from censored groups to enter into the training data and correct the model. Our results highlight the otherwise unmeasured harms of censoring and demonstrate the effectiveness of mitigation strategies across a range of data generating processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#38750;&#23545;&#31216;&#26680;&#65288;asymmetric kernels&#65289;&#23454;&#29616;Toeplitz&#31070;&#32463;&#32593;&#32476;&#65288;TNNs&#65289;&#30340;&#21152;&#36895;&#65292;&#36890;&#36807;&#31232;&#30095;&#21152;&#20302;&#31209;Toeplitz&#30697;&#38453;&#20998;&#35299;&#12289;&#23567;&#22411;1D&#21367;&#31215;&#21644;&#26367;&#25442;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#22120;&#65288;RPE&#65289;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#23454;&#29616;O&#65288;n&#65289;&#22797;&#26434;&#24230;&#65292;&#38024;&#23545;&#22240;&#26524;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#8220;&#24555;&#36895;&#8221;&#22240;&#26524;&#23631;&#34109;&#26469;&#25269;&#28040;&#36825;&#31181;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.09028</link><description>&lt;p&gt;
SKI&#21152;&#36895;Toeplitz&#31070;&#32463;&#32593;&#32476;&#65306;&#36890;&#36807;&#38750;&#23545;&#31216;&#26680;&#23454;&#29616;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels. (arXiv:2305.09028v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#38750;&#23545;&#31216;&#26680;&#65288;asymmetric kernels&#65289;&#23454;&#29616;Toeplitz&#31070;&#32463;&#32593;&#32476;&#65288;TNNs&#65289;&#30340;&#21152;&#36895;&#65292;&#36890;&#36807;&#31232;&#30095;&#21152;&#20302;&#31209;Toeplitz&#30697;&#38453;&#20998;&#35299;&#12289;&#23567;&#22411;1D&#21367;&#31215;&#21644;&#26367;&#25442;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#22120;&#65288;RPE&#65289;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#23454;&#29616;O&#65288;n&#65289;&#22797;&#26434;&#24230;&#65292;&#38024;&#23545;&#22240;&#26524;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#8220;&#24555;&#36895;&#8221;&#22240;&#26524;&#23631;&#34109;&#26469;&#25269;&#28040;&#36825;&#31181;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Toeplitz&#31070;&#32463;&#32593;&#32476;&#65288;TNNs&#65289;&#26159;&#26368;&#36817;&#20986;&#29616;&#24182;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#32467;&#26524;&#30340;&#24207;&#21015;&#27169;&#22411;&#12290;&#23427;&#20204;&#38656;&#35201;O(n log n)&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;O(n)&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#22120;&#65288;RPE&#65289;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#34928;&#20943;&#20559;&#24046;&#35843;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20943;&#23569;&#23427;&#20204;&#12290;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#65292;RPE&#26159;&#19968;&#20010;&#38750;&#23545;&#31216;&#27491;&#23450;&#26680;&#65292;&#32780;Toeplitz&#30697;&#38453;&#26159;&#20266;&#26684;&#25289;&#22982;&#30697;&#38453;&#12290;&#27492;&#22806;&#65306;1&#65289;&#23398;&#20064;&#30340;&#26680;&#22312;&#20027;&#23545;&#35282;&#32447;&#38468;&#36817;&#26174;&#31034;&#20986;&#21050;&#29366;&#34892;&#20026;&#65292;&#32780;&#22312;&#20854;&#20182;&#20301;&#32622;&#21017;&#34920;&#29616;&#20986;&#24179;&#28369;&#34892;&#20026;&#65307;2&#65289;RPE MLP&#36739;&#24930;&#12290;&#23545;&#20110;&#21452;&#21521;&#27169;&#22411;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#36827;&#34892;&#31232;&#30095;&#21152;&#20302;&#31209;Toeplitz&#30697;&#38453;&#20998;&#35299;&#12290;&#23545;&#20110;&#31232;&#30095;&#32452;&#20214;&#30340;&#25805;&#20316;&#65292;&#25105;&#20204;&#36827;&#34892;&#23567;&#22411;1D&#21367;&#31215;&#12290;&#23545;&#20110;&#20302;&#31209;&#32452;&#20214;&#65292;&#25105;&#20204;&#23558;RPE MLP&#26367;&#25442;&#20026;&#32447;&#24615;&#25554;&#20540;&#65292;&#24182;&#20351;&#29992;&#38750;&#23545;&#31216;&#26377;&#32467;&#26500;&#30340;&#20869;&#26680;&#25554;&#20540;&#65288;SKI&#65289;&#65288;Wilson&#31561;&#65292;2015&#65289;&#20197;&#23454;&#29616;O&#65288;n&#65289;&#22797;&#26434;&#24230;&#65306;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#35823;&#24046;&#20998;&#26512;&#12290;&#23545;&#20110;&#22240;&#26524;&#27169;&#22411;&#65292;&#8220;&#24555;&#36895;&#8221;&#22240;&#26524;&#23631;&#34109;&#65288;Katharopoulos&#31561;&#65292;2020&#65289;&#25269;&#28040;&#20102;SKI&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toeplitz Neural Networks (TNNs) (Qin et. al. 2023) are a recent sequence model with impressive results. They require O(n log n) computational complexity and O(n) relative positional encoder (RPE) multi-layer perceptron (MLP) and decay bias calls. We aim to reduce both. We first note that the RPE is a non-SPD (symmetric positive definite) kernel and the Toeplitz matrices are pseudo-Gram matrices. Further 1) the learned kernels display spiky behavior near the main diagonals with otherwise smooth behavior; 2) the RPE MLP is slow. For bidirectional models, this motivates a sparse plus low-rank Toeplitz matrix decomposition. For the sparse component's action, we do a small 1D convolution. For the low rank component, we replace the RPE MLP with linear interpolation and use asymmetric Structured Kernel Interpolation (SKI) (Wilson et. al. 2015) for O(n) complexity: we provide rigorous error analysis. For causal models, "fast" causal masking (Katharopoulos et. al. 2020) negates SKI's benefits. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#22871;&#20851;&#20110;&#29983;&#25104;&#12289;&#27880;&#37322;&#21644;&#39564;&#35777;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#25351;&#21335;&#65292;&#38416;&#36848;&#20102;&#26435;&#34913;&#21644;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#21608;&#21040;&#30340;&#25277;&#26679;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09018</link><description>&lt;p&gt;
DATED&#65306;&#29992;&#20110;&#24037;&#31243;&#35774;&#35745;&#24212;&#29992;&#30340;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
DATED: Guidelines for Creating Synthetic Datasets for Engineering Design Applications. (arXiv:2305.09018v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#22871;&#20851;&#20110;&#29983;&#25104;&#12289;&#27880;&#37322;&#21644;&#39564;&#35777;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#25351;&#21335;&#65292;&#38416;&#36848;&#20102;&#26435;&#34913;&#21644;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#21608;&#21040;&#30340;&#25277;&#26679;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22914;ChatGPT&#21644;DALL-E&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#12289;&#39046;&#22495;&#29305;&#23450;&#19988;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#25968;&#25454;&#38598;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#23545;&#20110;&#24076;&#26395;&#23558;&#36825;&#20123;&#31361;&#30772;&#24212;&#29992;&#20110;&#24037;&#31243;&#35774;&#35745;&#30340;&#30740;&#31350;&#20154;&#21592;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#21512;&#25104;&#25968;&#25454;&#38598;&#21017;&#25104;&#20026;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20174;&#19994;&#32773;&#36890;&#24120;&#19981;&#30830;&#23450;&#22914;&#20309;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20934;&#30830;&#22320;&#20195;&#34920;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#65292;&#24182;&#36866;&#29992;&#20110;&#39044;&#26399;&#30340;&#19979;&#28216;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#29983;&#25104;&#12289;&#27880;&#37322;&#21644;&#39564;&#35777;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#25351;&#21335;&#26469;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#12290;&#38416;&#36848;&#20102;&#27599;&#20010;&#26041;&#38754;&#25152;&#28041;&#21450;&#30340;&#26435;&#34913;&#21644;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#28065;&#36718;&#22686;&#21387;&#22120;&#25968;&#25454;&#38598;&#65292;&#35828;&#26126;&#20102;&#36825;&#20123;&#25351;&#21335;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#21608;&#21040;&#30340;&#25277;&#26679;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting the recent advancements in artificial intelligence, showcased by ChatGPT and DALL-E, in real-world applications necessitates vast, domain-specific, and publicly accessible datasets. Unfortunately, the scarcity of such datasets poses a significant challenge for researchers aiming to apply these breakthroughs in engineering design. Synthetic datasets emerge as a viable alternative. However, practitioners are often uncertain about generating high-quality datasets that accurately represent real-world data and are suitable for the intended downstream applications. This study aims to fill this knowledge gap by proposing comprehensive guidelines for generating, annotating, and validating synthetic datasets. The trade-offs and methods associated with each of these aspects are elaborated upon. Further, the practical implications of these guidelines are illustrated through the creation of a turbo-compressors dataset. The study underscores the importance of thoughtful sampling methods 
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#36807;&#31243;Port-Hamiltonian&#31995;&#32479;&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#23545;&#25968;&#25454;&#24314;&#27169;&#29983;&#25104;&#20851;&#20110;&#25351;&#23450;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#34987;&#21160;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;Port-Hamiltonian&#31995;&#32479;&#30340;&#32452;&#21512;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.09017</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;Port-Hamiltonian&#31995;&#32479;&#65306;&#22522;&#20110;&#29289;&#29702;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Port-Hamiltonian Systems: Bayesian Learning with Physics Prior. (arXiv:2305.09017v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09017
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;Port-Hamiltonian&#31995;&#32479;&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#23545;&#25968;&#25454;&#24314;&#27169;&#29983;&#25104;&#20851;&#20110;&#25351;&#23450;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#34987;&#21160;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;Port-Hamiltonian&#31995;&#32479;&#30340;&#32452;&#21512;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#22312;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#22522;&#30784;&#19978;&#23545;&#22797;&#26434;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#24573;&#30053;&#20102;&#20915;&#23450;&#20219;&#20309;&#29616;&#23454;&#31995;&#32479;&#34892;&#20026;&#30340;&#22522;&#26412;&#29289;&#29702;&#21407;&#29702;&#65292;&#36825;&#31181;&#30465;&#30053;&#19981;&#21033;&#20110;&#20004;&#26041;&#38754;&#65306;&#20854;&#19968;&#65292;&#27169;&#22411;&#22312;&#34701;&#21512;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#26041;&#38754;&#32570;&#20047;&#25968;&#25454;&#25928;&#29575;&#65307;&#20854;&#20108;&#65292;&#27169;&#22411;&#26412;&#36523;&#21487;&#33021;&#19981;&#31526;&#21512;&#29289;&#29702;&#35268;&#24459;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;Port-Hamiltonian&#31995;&#32479;&#65288;GP-PHS&#65289;&#20316;&#20026;&#22522;&#20110;&#29289;&#29702;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#30001;&#20110;GP-PHS&#20855;&#26377;&#27010;&#29575;&#35770;&#26412;&#36136;&#65292;&#23427;&#20351;&#29992;&#25152;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#26469;&#24418;&#25104;&#25152;&#26377;&#21487;&#33021;&#30340;&#21704;&#23494;&#39039;&#37327;&#20998;&#24067;&#65292;&#32780;&#38750;&#21333;&#20010;&#28857;&#20272;&#35745;&#12290;&#30001;&#20110;&#24213;&#23618;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;GP-PHS&#29983;&#25104;&#20851;&#20110;&#25351;&#23450;&#36755;&#20837;&#21644;&#36755;&#20986;&#34987;&#21160;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20445;&#30041;&#20102;Port-Hamiltonian&#31995;&#32479;&#30340;&#32452;&#21512;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approaches achieve remarkable results for the modeling of complex dynamics based on collected data. However, these models often neglect basic physical principles which determine the behavior of any real-world system. This omission is unfavorable in two ways: The models are not as data-efficient as they could be by incorporating physical prior knowledge, and the model itself might not be physically correct. We propose Gaussian Process Port-Hamiltonian systems (GP-PHS) as a physics-informed Bayesian learning approach with uncertainty quantification. The Bayesian nature of GP-PHS uses collected data to form a distribution over all possible Hamiltonians instead of a single point estimate. Due to the underlying physics model, a GP-PHS generates passive systems with respect to designated inputs and outputs. Further, the proposed approach preserves the compositional nature of Port-Hamiltonian systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29289;&#29702;&#22686;&#24378;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#24182;&#23558;&#20854;&#21253;&#21547;&#22312;&#20869;&#26680;&#20989;&#25968;&#20013;&#65292;&#20197;&#23454;&#29616;&#29289;&#29702;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.09006</link><description>&lt;p&gt;
&#29289;&#29702;&#22686;&#24378;&#30340;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Physics-enhanced Gaussian Process Variational Autoencoder. (arXiv:2305.09006v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29289;&#29702;&#22686;&#24378;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#24182;&#23558;&#20854;&#21253;&#21547;&#22312;&#20869;&#26680;&#20989;&#25968;&#20013;&#65292;&#20197;&#23454;&#29616;&#29289;&#29702;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#22522;&#20110;&#39640;&#32500;&#30340;&#36755;&#20837;/&#36755;&#20986;&#25968;&#25454;&#23398;&#20064;&#19968;&#20010;&#20302;&#32500;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#20197;&#35270;&#39057;&#21098;&#36753;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#65292;&#32534;&#30721;&#22120;&#21487;&#20197;&#29992;&#20110;&#25551;&#36848;&#35270;&#39057;&#20013;&#29289;&#20307;&#30340;&#36816;&#21160;&#32780;&#26080;&#38656;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65288;&#26080;&#30417;&#30563;&#23398;&#20064;&#65289;&#12290;&#23613;&#31649;&#29289;&#20307;&#30340;&#21160;&#24577;&#36890;&#24120;&#22522;&#20110;&#22522;&#26412;&#21407;&#29702;&#65292;&#20294;&#29616;&#26377;&#25991;&#29486;&#22823;&#22810;&#24573;&#30053;&#20102;&#36825;&#20123;&#20808;&#21069;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#22686;&#24378;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29289;&#29702;&#22686;&#24378;&#30340;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#25918;&#32622;&#22312;&#28508;&#22312;&#21160;&#21147;&#23398;&#19978;&#65292;&#20197;&#25552;&#39640;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#25928;&#29575;&#24182;&#20801;&#35768;&#29289;&#29702;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#20316;&#20026;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#34920;&#36798;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#22312;&#39640;&#26031;&#36807;&#31243;&#30340;&#20869;&#26680;&#20989;&#25968;&#20013;&#21453;&#26144;&#20026;&#26684;&#26519;&#20989;&#25968;&#24182;&#34987;&#21253;&#21547;&#20854;&#20013;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#28857;&#22312;&#25391;&#33633;&#31890;&#23376;&#30340;&#20223;&#30495;&#20013;&#24471;&#21040;&#31361;&#20986;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders allow to learn a lower-dimensional latent space based on high-dimensional input/output data. Using video clips as input data, the encoder may be used to describe the movement of an object in the video without ground truth data (unsupervised learning). Even though the object's dynamics is typically based on first principles, this prior knowledge is mostly ignored in the existing literature. Thus, we propose a physics-enhanced variational autoencoder that places a physical-enhanced Gaussian process prior on the latent dynamics to improve the efficiency of the variational autoencoder and to allow physically correct predictions. The physical prior knowledge expressed as linear dynamical system is here reflected by the Green's function and included in the kernel function of the Gaussian process. The benefits of the proposed approach are highlighted in a simulation with an oscillating particle.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#26032;&#22522;&#20110;&#25511;&#21046;&#27969;&#22270;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20174;CFG&#20013;&#25552;&#21462;&#12289;&#34920;&#31034;&#12289;&#20998;&#31867;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08993</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25511;&#21046;&#27969;&#22270;&#24694;&#24847;&#36719;&#20214;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Survey of Malware Analysis through Control Flow Graph using Machine Learning. (arXiv:2305.08993v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#26032;&#22522;&#20110;&#25511;&#21046;&#27969;&#22270;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20174;CFG&#20013;&#25552;&#21462;&#12289;&#34920;&#31034;&#12289;&#20998;&#31867;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#36719;&#20214;&#23545;&#35745;&#31639;&#26426;&#31995;&#32479;&#21644;&#32593;&#32476;&#30340;&#23433;&#20840;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#65292;&#38656;&#35201;&#20808;&#36827;&#30340;&#25216;&#26415;&#23545;&#20854;&#36827;&#34892;&#34892;&#20026;&#21644;&#21151;&#33021;&#20998;&#26512;&#20197;&#36827;&#34892;&#26816;&#27979;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#31614;&#21517;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#24694;&#24847;&#36719;&#20214;&#30340;&#24555;&#36895;&#28436;&#21270;&#24050;&#32463;&#22833;&#25928;&#12290;&#20854;&#20013;&#26368;&#26377;&#24076;&#26395;&#20811;&#26381;&#22522;&#20110;&#31614;&#21517;&#26816;&#27979;&#26041;&#27861;&#23616;&#38480;&#30340;&#25216;&#26415;&#20043;&#19968;&#26159;&#20351;&#29992;&#25511;&#21046;&#27969;&#22270;&#65288;CFG&#65289;&#12290;CFG&#21033;&#29992;&#31243;&#24207;&#30340;&#32467;&#26500;&#20449;&#24687;&#23558;&#21487;&#25191;&#34892;&#36335;&#24452;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#25351;&#20196;&#65292;&#36793;&#34920;&#31034;&#25511;&#21046;&#27969;&#20381;&#36182;&#20851;&#31995;&#12290;&#30446;&#21069;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20174;CFG&#20013;&#25552;&#21462;&#36825;&#20123;&#29305;&#24449;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#24694;&#24847;&#25110;&#33391;&#24615;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22238;&#39038;&#19968;&#20123;&#22522;&#20110;CFG&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20174;CFG&#20013;&#25552;&#21462;&#65292;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware is a significant threat to the security of computer systems and networks which requires sophisticated techniques to analyze the behavior and functionality for detection. Traditional signature-based malware detection methods have become ineffective in detecting new and unknown malware due to their rapid evolution. One of the most promising techniques that can overcome the limitations of signature-based detection is to use control flow graphs (CFGs). CFGs leverage the structural information of a program to represent the possible paths of execution as a graph, where nodes represent instructions and edges represent control flow dependencies. Machine learning (ML) algorithms are being used to extract these features from CFGs and classify them as malicious or benign. In this survey, we aim to review some state-of-the-art methods for malware detection through CFGs using ML, focusing on the different ways of extracting, representing, and classifying. Specifically, we present a comprehe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;BraTS 2023&#20462;&#22797;&#25361;&#25112;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#20351;&#29992;&#20462;&#34917;&#25216;&#26415;&#20174;&#26377;&#30149;&#21464;&#30340;&#33041;&#37096;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#33041;&#25195;&#25551;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#31639;&#27861;&#26080;&#27861;&#20998;&#26512;&#30149;&#21464;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08992</link><description>&lt;p&gt;
&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;&#36187;2023&#65306;&#36890;&#36807;&#20462;&#22797;&#29983;&#25104;&#20581;&#24247;&#33041;&#32452;&#32455;&#30340;&#23616;&#37096;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Brain Tumor Segmentation (BraTS) Challenge 2023: Local Synthesis of Healthy Brain Tissue via Inpainting. (arXiv:2305.08992v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08992
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;BraTS 2023&#20462;&#22797;&#25361;&#25112;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#20351;&#29992;&#20462;&#34917;&#25216;&#26415;&#20174;&#26377;&#30149;&#21464;&#30340;&#33041;&#37096;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#33041;&#25195;&#25551;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#31639;&#27861;&#26080;&#27861;&#20998;&#26512;&#30149;&#21464;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#30340;&#20915;&#31574;&#65292;&#25552;&#20379;&#20102;&#35768;&#22810;&#33258;&#21160;&#20998;&#26512;&#33041;&#37096;MR&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#33041;&#32959;&#30244;&#24739;&#32773;&#65292;&#22270;&#20687;&#37319;&#38598;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#22987;&#20110;&#24050;&#32463;&#30149;&#29702;&#24615;&#30340;&#25195;&#25551;&#12290;&#36825;&#20250;&#24102;&#26469;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#31639;&#27861;&#26159;&#35774;&#35745;&#29992;&#20110;&#20998;&#26512;&#20581;&#24247;&#30340;&#22823;&#33041;&#22270;&#20687;&#65292;&#24182;&#19988;&#27809;&#26377;&#20026;&#21253;&#21547;&#30149;&#21464;&#30340;&#22270;&#20687;&#25552;&#20379;&#20445;&#35777;&#12290;&#20363;&#22914;&#65292;&#36827;&#34892;&#33041;&#37096;&#35299;&#21078;&#20998;&#21106;&#12289;&#32452;&#32455;&#20998;&#21106;&#21644;&#33041;&#37096;&#25552;&#21462;&#30340;&#31639;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BraTS 2023&#20462;&#22797;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#25506;&#32034;&#20462;&#22797;&#25216;&#26415;&#65292;&#20174;&#26377;&#30149;&#21464;&#30340;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#30340;&#33041;&#37096;&#25195;&#25551;&#12290;&#19979;&#38754;&#30340;&#25163;&#31295;&#21253;&#21547;&#20102;&#20219;&#21153;&#20844;&#24335;&#12289;&#25968;&#25454;&#38598;&#21644;&#25552;&#20132;&#31243;&#24207;&#12290;&#20043;&#21518;&#20250;&#26356;&#26032;&#20197;&#24635;&#32467;&#25361;&#25112;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#25361;&#25112;&#26159;&#20316;&#20026;BraTS 2023&#25361;&#25112;&#30340;&#19968;&#37096;&#20998;&#65292;&#30001;&#21152;&#25343;&#22823;&#28201;&#21733;&#21326;MICCAI 2023&#20250;&#35758;&#20027;&#21150;&#12290;
&lt;/p&gt;
&lt;p&gt;
A myriad of algorithms for the automatic analysis of brain MR images is available to support clinicians in their decision-making. For brain tumor patients, the image acquisition time series typically starts with a scan that is already pathological. This poses problems, as many algorithms are designed to analyze healthy brains and provide no guarantees for images featuring lesions. Examples include but are not limited to algorithms for brain anatomy parcellation, tissue segmentation, and brain extraction. To solve this dilemma, we introduce the BraTS 2023 inpainting challenge. Here, the participants' task is to explore inpainting techniques to synthesize healthy brain scans from lesioned ones. The following manuscript contains the task formulation, dataset, and submission procedure. Later it will be updated to summarize the findings of the challenge. The challenge is organized as part of the BraTS 2023 challenge hosted at the MICCAI 2023 conference in Vancouver, Canada.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#21327;&#35843;&#21644;&#25968;&#25454;&#22635;&#34917;&#31561;&#20851;&#38190;&#27493;&#39588;&#32435;&#20837;&#26550;&#26500;&#24895;&#26223;&#65292;&#26469;&#20419;&#36827;&#25968;&#25454;&#31649;&#29702;&#20449;&#24687;&#31995;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.08985</link><description>&lt;p&gt;
&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Learning over Harmonized Data Silos. (arXiv:2305.08985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#21327;&#35843;&#21644;&#25968;&#25454;&#22635;&#34917;&#31561;&#20851;&#38190;&#27493;&#39588;&#32435;&#20837;&#26550;&#26500;&#24895;&#26223;&#65292;&#26469;&#20419;&#36827;&#25968;&#25454;&#31649;&#29702;&#20449;&#24687;&#31995;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20351;&#22320;&#29702;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#20849;&#21516;&#23398;&#20064;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#26159;&#22522;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#25110;&#25991;&#26412;&#65289;&#65292;&#25110;&#32773;&#22522;&#20110;&#22312;&#19981;&#21516;&#30340;&#31449;&#28857;&#19978;&#20855;&#26377;&#19968;&#33268;&#32467;&#26500;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#26159;&#21508;&#31449;&#28857;&#20855;&#26377;&#19981;&#21516;&#30340;&#27169;&#24335;&#12289;&#25968;&#25454;&#26684;&#24335;&#12289;&#25968;&#25454;&#20540;&#21644;&#35775;&#38382;&#27169;&#24335;&#65292;&#22240;&#27492;&#38656;&#35201;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#20351;&#29992;&#22768;&#26126;&#24615;&#27169;&#24335;&#26144;&#23556;&#36827;&#34892;&#25968;&#25454;&#20132;&#25442;&#21644;&#26597;&#35810;&#37325;&#20889;&#65292;&#20197;&#21450;&#23454;&#20307;&#36830;&#25509;&#31561;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#25968;&#25454;&#21327;&#35843;&#21644;&#25968;&#25454;&#22635;&#34917;&#31561;&#20851;&#38190;&#27493;&#39588;&#30340;&#31471;&#21040;&#31471;&#32852;&#37030;&#23398;&#20064;&#21644;&#38598;&#25104;&#31995;&#32479;&#30340;&#26550;&#26500;&#24895;&#26223;&#65292;&#20197;&#20419;&#36827;&#25968;&#25454;&#31649;&#29702;&#20449;&#24687;&#31995;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning is a distributed machine learning approach that enables geographically distributed data silos to collaboratively learn a joint machine learning model without sharing data. Most of the existing work operates on unstructured data, such as images or text, or on structured data assumed to be consistent across the different sites. However, sites often have different schemata, data formats, data values, and access patterns. The field of data integration has developed many methods to address these challenges, including techniques for data exchange and query rewriting using declarative schema mappings, and for entity linkage. Therefore, we propose an architectural vision for an end-to-end Federated Learning and Integration system, incorporating the critical steps of data harmonization and data imputation, to spur further research on the intersection of data management information systems and machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#19982;&#28418;&#31227;&#26816;&#27979;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;strAEm++DD&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#26631;&#31614;&#30340;&#27969;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#19981;&#20439;&#30340;&#24615;&#33021;&#34920;&#29616;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#21644;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08977</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#27969;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65306;&#22686;&#37327;&#23398;&#20064;&#21644;&#27010;&#24565;&#28418;&#31227;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-based Anomaly Detection in Streaming Data with Incremental Learning and Concept Drift Adaptation. (arXiv:2305.08977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#19982;&#28418;&#31227;&#26816;&#27979;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;strAEm++DD&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#26631;&#31614;&#30340;&#27969;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#19981;&#20439;&#30340;&#24615;&#33021;&#34920;&#29616;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#21644;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#23431;&#23449;&#20013;&#65292;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#20197;&#21508;&#31181;&#26041;&#24335;&#36827;&#34892;&#27969;&#24335;&#29983;&#25104;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#24322;&#24120;&#31561;&#32597;&#35265;&#20107;&#20214;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#32780;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#23588;&#20854;&#22256;&#38590;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#19982;&#28418;&#31227;&#26816;&#27979;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;strAEm++DD&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#23545;strAEm++DD&#36827;&#34892;&#20102;&#32463;&#39564;&#20998;&#26512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#21644;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our digital universe nowadays, enormous amount of data are produced in a streaming manner in a variety of application areas. These data are often unlabelled. In this case, identifying infrequent events, such as anomalies, poses a great challenge. This problem becomes even more difficult in non-stationary environments, which can cause deterioration of the predictive performance of a model. To address the above challenges, the paper proposes an autoencoder-based incremental learning method with drift detection (strAEm++DD). Our proposed method strAEm++DD leverages on the advantages of both incremental learning and drift detection. We conduct an experimental study using real-world and synthetic datasets with severe or extreme class imbalance, and provide an empirical analysis of strAEm++DD. We further conduct a comparative study, showing that the proposed method significantly outperforms existing baseline and advanced methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20284;&#28982;&#27604;&#26041;&#27861;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#26080;&#38656;&#20351;&#29992;&#36882;&#24402;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#22312;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19978;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#27169;&#22411;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.08960</link><description>&lt;p&gt;
&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65306;&#28145;&#20837;&#25506;&#31350;&#20284;&#28982;&#27604;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks without Backpropagation: A Deeper Dive into the Likelihood Ratio Method. (arXiv:2305.08960v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20284;&#28982;&#27604;&#26041;&#27861;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#26080;&#38656;&#20351;&#29992;&#36882;&#24402;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#22312;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19978;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#27169;&#22411;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#37325;&#35201;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#34920;&#26126;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20284;&#28982;&#27604;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26080;&#38656;&#36882;&#24402;&#26799;&#24230;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#26377;&#25928;&#22320;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#26799;&#24230;&#20272;&#35745;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24471;&#21040;&#20102;&#25968;&#20540;&#32467;&#26524;&#12290;&#25152;&#26377;&#32467;&#26524;&#37117;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#65292;&#20284;&#28982;&#27604;&#26041;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#26377;&#25928;&#22320;&#35757;&#32451;&#20102;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation (BP) is the most important gradient estimation method for training neural networks in deep learning. However, the literature shows that neural networks trained by BP are vulnerable to adversarial attacks. We develop the likelihood ratio (LR) method, a new gradient estimation method, for training a broad range of neural network architectures, including convolutional neural networks, recurrent neural networks, graph neural networks, and spiking neural networks, without recursive gradient computation. We propose three methods to efficiently reduce the variance of the gradient estimation in the neural network training process. Our experiments yield numerical results for training different neural networks on several datasets. All results demonstrate that the LR method is effective for training various neural networks and significantly improves the robustness of the neural networks under adversarial attacks relative to the BP method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HumanMotionQA&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#19978;&#36827;&#34892;&#22797;&#26434;&#12289;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;NSPose&#26041;&#27861;&#65292;&#21033;&#29992;&#31526;&#21495;&#21270;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20013;&#65292;&#24182;&#36229;&#36807;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08953</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22359;&#21270;&#21160;&#20316;&#31243;&#24207;&#30340;&#21160;&#20316;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Motion Question Answering via Modular Motion Programs. (arXiv:2305.08953v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08953
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HumanMotionQA&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#19978;&#36827;&#34892;&#22797;&#26434;&#12289;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;NSPose&#26041;&#27861;&#65292;&#21033;&#29992;&#31526;&#21495;&#21270;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20013;&#65292;&#24182;&#36229;&#36807;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#24863;&#30693;&#21644;&#29702;&#35299;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#39318;&#20808;&#35774;&#35745;&#27169;&#22411;&#65292;&#23545;&#21160;&#20316;&#24207;&#21015;&#36827;&#34892;&#22797;&#26434;&#30340;&#26102;&#31354;&#25512;&#29702;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HumanMotionQA&#20219;&#21153;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#38271;&#26102;&#38388;&#20154;&#31867;&#36816;&#21160;&#24207;&#21015;&#19978;&#36827;&#34892;&#22797;&#26434;&#12289;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#38382;&#31572;&#23545;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#22312;&#21160;&#20316;&#24207;&#21015;&#30340;&#23567;&#37096;&#20998;&#20013;&#26816;&#27979;&#36816;&#21160;&#32447;&#32034;&#65292;&#23545;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#26597;&#35810;&#29305;&#23450;&#30340;&#36816;&#21160;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;NSPose&#65292;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35813;&#20219;&#21153;&#65292;&#23427;&#21033;&#29992;&#31526;&#21495;&#21270;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#27010;&#24565;&#12289;&#23646;&#24615;&#31070;&#32463;&#25805;&#20316;&#31526;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#26469;&#22788;&#29702;&#21160;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;NSPose&#22312;HumanMotionQA&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#32988;&#36807;&#20102;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to build artificial intelligence systems that can perceive and reason with human behavior in the real world, we must first design models that conduct complex spatio-temporal reasoning over motion sequences. Moving towards this goal, we propose the HumanMotionQA task to evaluate complex, multi-step reasoning abilities of models on long-form human motion sequences. We generate a dataset of question-answer pairs that require detecting motor cues in small portions of motion sequences, reasoning temporally about when events occur, and querying specific motion attributes. In addition, we propose NSPose, a neuro-symbolic method for this task that uses symbolic reasoning and a modular design to ground motion through learning motion concepts, attribute neural operators, and temporal relations. We demonstrate the suitability of NSPose for the HumanMotionQA task, outperforming all baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20998;&#26512;&#30340;&#40065;&#26834;&#24178;&#39044;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#65292;&#36991;&#20813;&#22122;&#38899;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2305.08950</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#22240;&#26524;&#20998;&#26512;&#26041;&#27861;&#29992;&#20110;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causal Analysis for Robust Interpretability of Neural Networks. (arXiv:2305.08950v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20998;&#26512;&#30340;&#40065;&#26834;&#24178;&#39044;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#65292;&#36991;&#20813;&#22122;&#38899;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#35299;&#37322;&#23545;&#20110;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#30340;&#21487;&#38752;&#24320;&#21457;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#35299;&#37322;&#26041;&#27861;&#38598;&#20013;&#22312;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#24230;&#37327;&#19978;&#65292;&#20197;&#23558;&#27169;&#22411;&#20915;&#31574;&#24402;&#22240;&#20110;&#20010;&#21035;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#35757;&#32451;&#38454;&#27573;&#20013;&#32534;&#30721;&#22312;&#27169;&#22411;&#20013;&#30340;&#22122;&#22768;&#21644;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#26377;&#20559;&#36755;&#20837;&#65292;&#27169;&#22411;&#36807;&#25311;&#21512;&#25110;&#38169;&#37197;&#65289;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#36807;&#31243;&#24050;&#32463;&#35777;&#26126;&#20250;&#20135;&#29983;&#22024;&#26434;&#21644;&#19981;&#31283;&#23450;&#30340;&#24402;&#22240;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#36879;&#26126;&#29702;&#35299;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20998;&#26512;&#30340;&#40065;&#26834;&#24178;&#39044;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22240;&#26524;&#26426;&#21046;&#21450;&#20854;&#19982;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#36335;&#24452;&#24178;&#39044;&#65292;&#20197;&#25512;&#26029;&#38544;&#34255;&#23618;&#20013;&#30340;&#22240;&#26524;&#26426;&#21046;&#24182;&#38548;&#31163;&#30456;&#20851;&#21644;&#24517;&#35201;&#30340;&#20449;&#24687;&#65288;&#20197;&#36827;&#34892;&#27169;&#22411;&#39044;&#27979;&#65289;&#65292;&#20174;&#32780;&#36991;&#20813;&#22122;&#38899;&#30340;&#24178;&#25200;&#12290;&#32467;&#26524;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#31283;&#20581;&#19988;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting the inner function of neural networks is crucial for the trustworthy development and deployment of these black-box models. Prior interpretability methods focus on correlation-based measures to attribute model decisions to individual examples. However, these measures are susceptible to noise and spurious correlations encoded in the model during the training phase (e.g., biased inputs, model overfitting, or misspecification). Moreover, this process has proven to result in noisy and unstable attributions that prevent any transparent understanding of the model's behavior. In this paper, we develop a robust interventional-based method grounded by causal analysis to capture cause-effect mechanisms in pre-trained neural networks and their relation to the prediction. Our novel approach relies on path interventions to infer the causal mechanisms within hidden layers and isolate relevant and necessary information (to model prediction), avoiding noisy ones. The result is task-specifi
&lt;/p&gt;</description></item><item><title>MIMEx&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#36974;&#30422;&#36755;&#20837;&#24314;&#27169;&#26469;&#25552;&#21462;&#20869;&#22312;&#22870;&#21169;&#65292;&#36890;&#36807;&#25511;&#21046;&#36974;&#30422;&#20998;&#24067;&#26469;&#25511;&#21046;&#38590;&#24230;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#21462;&#24471;&#20248;&#36234;&#30340;&#25506;&#32034;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08932</link><description>&lt;p&gt;
MIMEx: &#36974;&#30422;&#36755;&#20837;&#24314;&#27169;&#30340;&#20869;&#22312;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
MIMEx: Intrinsic Rewards from Masked Input Modeling. (arXiv:2305.08932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08932
&lt;/p&gt;
&lt;p&gt;
MIMEx&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#36974;&#30422;&#36755;&#20837;&#24314;&#27169;&#26469;&#25552;&#21462;&#20869;&#22312;&#22870;&#21169;&#65292;&#36890;&#36807;&#25511;&#21046;&#36974;&#30422;&#20998;&#24067;&#26469;&#25511;&#21046;&#38590;&#24230;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#21462;&#24471;&#20248;&#36234;&#30340;&#25506;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#35266;&#27979;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#24456;&#22256;&#38590;&#12290;&#20351;&#29992;&#20869;&#22312;&#22870;&#21169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#20272;&#35745;&#29366;&#24577;&#12289;&#36716;&#25442;&#25110;&#36712;&#36857;&#30340;&#8220;&#26032;&#39062;&#24615;&#8221;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26465;&#20214;&#39044;&#27979;&#30446;&#26631;&#65292;&#22914;&#36974;&#30422;&#33258;&#21160;&#32534;&#30721;&#21487;&#30475;&#20316;&#20266;&#20284;&#28982;&#30340;&#38543;&#26426;&#20272;&#35745;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#35266;&#28857;&#22914;&#20309;&#33258;&#28982;&#22320;&#23548;&#33268;&#23545;&#29616;&#26377;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#30340;&#32479;&#19968;&#30475;&#27861;:&#23427;&#20204;&#26159;&#26465;&#20214;&#39044;&#27979;&#30340;&#29305;&#20363;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26032;&#39062;&#24615;&#30340;&#20272;&#35745;&#21487;&#20197;&#30475;&#20316;&#26159;&#20351;&#29992;&#19981;&#21516;&#30340;&#36974;&#30422;&#20998;&#24067;&#36827;&#34892;&#20266;&#20284;&#28982;&#20272;&#35745;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#8212;&#8212;&#36974;&#30422;&#36755;&#20837;&#24314;&#27169;&#25506;&#32034;&#20869;&#22312;&#22870;&#21169;(MIMEx)&#65292;&#20854;&#20013;&#36974;&#30422;&#20998;&#24067;&#21487;&#20197;&#28789;&#27963;&#35843;&#25972;&#20197;&#25511;&#21046;&#24213;&#23618;&#26465;&#20214;&#39044;&#27979;&#20219;&#21153;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#24403;&#19982;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#26102;&#65292;MIMEx&#21487;&#20197;&#21462;&#24471;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring in environments with high-dimensional observations is hard. One promising approach for exploration is to use intrinsic rewards, which often boils down to estimating "novelty" of states, transitions, or trajectories with deep networks. Prior works have shown that conditional prediction objectives such as masked autoencoding can be seen as stochastic estimation of pseudo-likelihood. We show how this perspective naturally leads to a unified view on existing intrinsic reward approaches: they are special cases of conditional prediction, where the estimation of novelty can be seen as pseudo-likelihood estimation with different mask distributions. From this view, we propose a general framework for deriving intrinsic rewards -- Masked Input Modeling for Exploration (MIMEx) -- where the mask distribution can be flexibly tuned to control the difficulty of the underlying conditional prediction task. We demonstrate that MIMEx can achieve superior results when compared against competitive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AlphaFold2&#30340;&#23545;&#25239;&#24207;&#21015;&#31361;&#21464;&#65292;&#23454;&#27979;&#20165;&#20462;&#25913;&#19977;&#20010;&#27688;&#22522;&#37240;&#27531;&#22522;&#65292;&#23601;&#20351;&#20854;&#39044;&#27979;&#30340;&#34507;&#30333;&#36136;&#19977;&#32423;&#32467;&#26500;&#30340;&#25913;&#21464;&#36798;&#21040;&#20102;46.61&#65292;&#24182;&#33021;&#25104;&#21151;&#21457;&#29616;&#22312;&#29305;&#23450;&#34507;&#30333;&#36136;&#20013;&#23545;&#32467;&#26500;&#33267;&#20851;&#37325;&#35201;&#19988;&#26377;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#27688;&#22522;&#37240;&#27531;&#22522;&#65292;&#20026;&#34507;&#30333;&#36136;&#30340;&#20462;&#39280;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.08929</link><description>&lt;p&gt;
AF2-Mutation&#65306;&#38024;&#23545;AlphaFold2&#30340;&#34507;&#30333;&#19977;&#32423;&#32467;&#26500;&#39044;&#27979;&#30340;&#23545;&#25239;&#24207;&#21015;&#31361;&#21464;
&lt;/p&gt;
&lt;p&gt;
AF2-Mutation: Adversarial Sequence Mutations against AlphaFold2 on Protein Tertiary Structure Prediction. (arXiv:2305.08929v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AlphaFold2&#30340;&#23545;&#25239;&#24207;&#21015;&#31361;&#21464;&#65292;&#23454;&#27979;&#20165;&#20462;&#25913;&#19977;&#20010;&#27688;&#22522;&#37240;&#27531;&#22522;&#65292;&#23601;&#20351;&#20854;&#39044;&#27979;&#30340;&#34507;&#30333;&#36136;&#19977;&#32423;&#32467;&#26500;&#30340;&#25913;&#21464;&#36798;&#21040;&#20102;46.61&#65292;&#24182;&#33021;&#25104;&#21151;&#21457;&#29616;&#22312;&#29305;&#23450;&#34507;&#30333;&#36136;&#20013;&#23545;&#32467;&#26500;&#33267;&#20851;&#37325;&#35201;&#19988;&#26377;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#27688;&#22522;&#37240;&#27531;&#22522;&#65292;&#20026;&#34507;&#30333;&#36136;&#30340;&#20462;&#39280;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22914;AlphaFold2&#65288;AF2&#65289;&#65292;&#21462;&#24471;&#20102;&#19982;&#30495;&#23454;&#29983;&#29289;&#23454;&#39564;&#26041;&#27861;&#21487;&#27604;&#30340;&#34507;&#30333;&#36136;&#19977;&#32423;&#32467;&#26500;&#39044;&#27979;&#25928;&#26524;&#12290;&#34429;&#28982;AF2&#22312;&#39044;&#27979;&#21464;&#24322;&#25928;&#26524;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20294;&#20854;&#23545;&#24207;&#21015;&#21464;&#24322;&#30340;&#40065;&#26834;&#24615;&#23578;&#24453;&#30830;&#23450;&#12290;&#26412;&#25991;&#20174;&#37326;&#29983;&#22411;&#65288;WT&#65289;&#24207;&#21015;&#24320;&#22987;&#65292;&#36890;&#36807;&#36827;&#21270;&#26041;&#27861;&#29983;&#25104;&#23545;&#25239;&#24207;&#21015;&#65292;AF2&#39044;&#27979;&#19982;WT&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;CASP14&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#20165;&#20462;&#25913;3&#20010;&#27688;&#22522;&#37240;&#27531;&#22522;&#65292;&#37319;&#29992;&#26367;&#25442;&#12289;&#21024;&#38500;&#21644;&#25554;&#20837;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;AF2&#39044;&#27979;&#30340;&#26412;&#22320;&#36317;&#31163;&#24046;&#24322;&#27979;&#35797;&#65288;lDDT&#65289;&#30340;&#25913;&#21464;&#36798;&#21040;&#20102;46.61&#12290;&#27492;&#22806;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#34507;&#30333;&#36136;SPNS2&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30830;&#23450;&#33267;&#20851;&#37325;&#35201;&#19988;&#26377;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#27688;&#22522;&#37240;&#27531;&#22522;&#65292;&#21487;&#33021;&#34920;&#26126;&#21487;&#20462;&#25913;&#30340;&#34507;&#30333;&#36136;&#37096;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based approaches, such as AlphaFold2 (AF2), have significantly advanced protein tertiary structure prediction, achieving results comparable to real biological experimental methods. While AF2 has shown limitations in predicting the effects of mutations, its robustness against sequence mutations remains to be determined. Starting with the wild-type (WT) sequence, we investigate adversarial sequences generated via an evolutionary approach, which AF2 predicts to be substantially different from WT. Our experiments on CASP14 reveal that by modifying merely three residues in the protein sequence using a combination of replacement, deletion, and insertion strategies, the alteration in AF2's predictions, as measured by the Local Distance Difference Test (lDDT), reaches 46.61. Moreover, when applied to a specific protein, SPNS2, our proposed algorithm successfully identifies biologically meaningful residues critical to protein structure determination and potentially indicates alter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;DFCNN&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#21487;&#23398;&#20064;&#33021;&#21147;&#30340;FTSF&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.08890</link><description>&lt;p&gt;
&#24046;&#20998;&#21367;&#31215;&#27169;&#31946;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Differential Convolutional Fuzzy Time Series Forecasting. (arXiv:2305.08890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;DFCNN&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#21487;&#23398;&#20064;&#33021;&#21147;&#30340;FTSF&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;FTSF&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#20856;&#22411;&#39044;&#27979;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;FTSF&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#19987;&#23478;&#31995;&#32479;&#65292;&#23548;&#33268;&#22833;&#21435;&#20102;&#35782;&#21035;&#26410;&#23450;&#20041;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;FTSF&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24046;&#20998;&#27169;&#31946;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DFCNN&#65289;&#27169;&#22411;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20855;&#26377;&#21487;&#23398;&#20064;&#33021;&#21147;&#30340;FTSF&#12290;DFCNN&#33021;&#22815;&#35782;&#21035;&#28508;&#22312;&#20449;&#24687;&#24182;&#25913;&#21892;&#39044;&#27979;&#31934;&#24230;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#23398;&#20064;&#33021;&#21147;&#65292;FTSF&#24314;&#31435;&#30340;&#27169;&#31946;&#35268;&#21017;&#30340;&#38271;&#24230;&#21487;&#20197;&#20219;&#24847;&#25193;&#23637;&#65292;&#36825;&#26159;&#19987;&#23478;&#31995;&#32479;&#25152;&#26080;&#27861;&#22788;&#29702;&#30340;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#36235;&#21183;&#65292;FTSF&#36890;&#24120;&#26080;&#27861;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#36235;&#21183;&#20250;&#23548;&#33268;FTSF&#24314;&#31435;&#30340;&#27169;&#31946;&#38598;&#22833;&#25928;&#65292;&#24182;&#23548;&#33268;&#39044;&#27979;&#22833;&#36133;&#12290;DFCNN&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzy time series forecasting (FTSF) is a typical forecasting method with wide application. Traditional FTSF is regarded as an expert system which leads to lose the ability to recognize undefined feature. The mentioned is main reason of poor forecasting with FTSF. To solve the problem, the proposed model Differential Fuzzy Convolutional Neural Network (DFCNN) utilizes convolution neural network to re-implement FTSF with learnable ability. DFCNN is capable of recognizing the potential information and improve the forecasting accuracy. Thanks to learnable ability of neural network, length of fuzzy rules established in FTSF is expended to arbitrary length which expert is not able to be handle by expert system. At the same time, FTSF usually cannot achieve satisfactory performance of non-stationary time series due to trend of non-stationary time series. The trend of non-stationary time series causes the fuzzy set established by FTSF to invalid and cause the forecasting to fail. DFCNN utiliz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#8220;&#22823;&#25968;&#25454;&#8221;&#26102;&#20195;&#30340;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#30740;&#31350;&#26041;&#27861;&#35770;&#25361;&#25112;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65306;&#23450;&#37327;&#24402;&#32435;&#26041;&#27861;&#65292;&#20363;&#22914;&#22240;&#23376;&#20998;&#26512;&#12289;&#32858;&#31867;&#20998;&#26512;&#21644;&#28508;&#22312;&#31867;&#21035;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.08889</link><description>&lt;p&gt;
&#26032;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65311;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#30740;&#31350;&#20013;&#23450;&#37327;&#24402;&#32435;&#26041;&#27861;&#30340;&#27010;&#36848;&#19982;&#31034;&#33539;
&lt;/p&gt;
&lt;p&gt;
New methods for new data? An overview and illustration of quantitative inductive methods for HRM research. (arXiv:2305.08889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#8220;&#22823;&#25968;&#25454;&#8221;&#26102;&#20195;&#30340;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#30740;&#31350;&#26041;&#27861;&#35770;&#25361;&#25112;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65306;&#23450;&#37327;&#24402;&#32435;&#26041;&#27861;&#65292;&#20363;&#22914;&#22240;&#23376;&#20998;&#26512;&#12289;&#32858;&#31867;&#20998;&#26512;&#21644;&#28508;&#22312;&#31867;&#21035;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#25968;&#25454;&#26159;&#26032;&#30340;&#30707;&#27833;&#8221;&#65292;&#31616;&#35328;&#20043;&#65292;&#25968;&#25454;&#26159;&#25345;&#32493;&#36827;&#34892;&#30340;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#30340;&#37325;&#35201;&#26469;&#28304;&#65292;&#36825;&#23548;&#33268;&#19968;&#20123;&#35780;&#35770;&#21592;&#36807;&#20110;&#24555;&#36895;&#22320;&#23558;&#25968;&#25454;&#37327;&#35270;&#20026;&#36130;&#23500;&#26412;&#36523;&#65292;&#24182;&#35748;&#20026;&#22823;&#25968;&#25454;&#30340;&#21457;&#23637;&#26159;&#21033;&#28070;&#30340;&#20960;&#20046;&#30452;&#25509;&#21407;&#22240;&#12290;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#20063;&#27809;&#26377;&#36867;&#33073;&#36825;&#19968;&#36235;&#21183;&#65292;&#23545;&#21592;&#24037;&#22823;&#37327;&#25968;&#25454;&#30340;&#31215;&#32047;&#34987;&#26576;&#20123;&#20225;&#19994;&#23478;&#35270;&#20026;&#26500;&#24314;&#39044;&#27979;&#22797;&#26434;&#24037;&#20316;&#34892;&#20026;&#65288;&#22914;&#26103;&#24037;&#25110;&#24037;&#20316;&#32489;&#25928;&#65289;&#30340;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#31181;&#31867;&#27604;&#26377;&#28857;&#35823;&#23548;&#20154;&#65306;&#19982;&#30707;&#27833;&#19981;&#21516;&#65292;&#36825;&#37324;&#19981;&#23384;&#22312;&#26377;&#20851;&#25968;&#25454;&#20135;&#29983;&#30340;&#37325;&#22823;&#38382;&#39064;&#65288;&#20854;&#27969;&#37327;&#30001;&#21508;&#31181;&#20449;&#24687;&#31995;&#32479;&#25345;&#32493;&#19988;&#20302;&#25104;&#26412;&#22320;&#29983;&#25104;&#65289;&#65292;&#32780;&#26159;&#23427;&#20204;&#30340;&#8220;&#31934;&#28860;&#8221;&#65292;&#21363;&#23558;&#36825;&#20123;&#25968;&#25454;&#36716;&#21270;&#20026;&#26377;&#29992;&#30340;&#20135;&#21697;&#65292;&#21363;&#30693;&#35782;&#25152;&#38656;&#30340;&#25805;&#20316;&#12290;&#36825;&#31181;&#36716;&#21464;&#26159;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#30740;&#31350;&#26041;&#27861;&#35770;&#25361;&#25112;&#25152;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#37327;&#24402;&#32435;&#26041;&#27861;&#30340;&#27010;&#36848;&#19982;&#31034;&#33539;&#65292;&#20363;&#22914;&#22240;&#23376;&#20998;&#26512;&#12289;&#32858;&#31867;&#20998;&#26512;&#21644;&#28508;&#22312;&#31867;&#21035;&#20998;&#26512;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26041;&#27861;&#20026;&#8220;&#22823;&#25968;&#25454;&#8221;&#26102;&#20195;&#30340;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#21069;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
"Data is the new oil", in short, data would be the essential source of the ongoing fourth industrial revolution, which has led some commentators to assimilate too quickly the quantity of data to a source of wealth in itself, and consider the development of big data as an quasi direct cause of profit. Human resources management is not escaping this trend, and the accumulation of large amounts of data on employees is perceived by some entrepreneurs as a necessary and sufficient condition for the construction of predictive models of complex work behaviors such as absenteeism or job performance. In fact, the analogy is somewhat misleading: unlike oil, there are no major issues here concerning the production of data (whose flows are generated continuously and at low cost by various information systems), but rather their ''refining'', i.e. the operations necessary to transform this data into a useful product, namely into knowledge. This transformation is where the methodological challenges o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#37327;&#36317;&#31163;&#21152;&#26435;&#22238;&#24402;&#65288;CWR&#65289;&#26041;&#27861;&#65292;&#23558;&#22320;&#29702;&#36317;&#31163;&#21644;&#23646;&#24615;&#36317;&#31163;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#25151;&#20215;&#12290;CWR&#26041;&#27861;&#32771;&#34385;&#21040;&#22320;&#36136;&#21644;&#23646;&#24615;&#36317;&#31163;&#65292;&#21487;&#20197;&#20135;&#29983;&#20934;&#30830;&#30340;&#25151;&#20215;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.08887</link><description>&lt;p&gt;
&#21464;&#37327;&#36317;&#31163;&#21152;&#26435;&#22238;&#24402;&#65288;CWR&#65289;&#65306;&#25151;&#20215;&#20272;&#35745;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Covariate-distance Weighted Regression (CWR): A Case Study for Estimation of House Prices. (arXiv:2305.08887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#37327;&#36317;&#31163;&#21152;&#26435;&#22238;&#24402;&#65288;CWR&#65289;&#26041;&#27861;&#65292;&#23558;&#22320;&#29702;&#36317;&#31163;&#21644;&#23646;&#24615;&#36317;&#31163;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#25151;&#20215;&#12290;CWR&#26041;&#27861;&#32771;&#34385;&#21040;&#22320;&#36136;&#21644;&#23646;&#24615;&#36317;&#31163;&#65292;&#21487;&#20197;&#20135;&#29983;&#20934;&#30830;&#30340;&#25151;&#20215;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#21152;&#26435;&#22238;&#24402;&#65288;GWR&#65289;&#26159;&#24314;&#31435;&#22238;&#24402;&#27169;&#22411;&#20013;&#31354;&#38388;&#24322;&#36136;&#24615;&#30340;&#24120;&#29992;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;GWR&#20013;&#30340;&#26435;&#37325;&#20989;&#25968;&#20165;&#32771;&#34385;&#22320;&#29702;&#36317;&#31163;&#65292;&#32780;&#23646;&#24615;&#30456;&#20284;&#24615;&#21017;&#34987;&#23436;&#20840;&#24573;&#30053;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#37327;&#21152;&#26435;&#20989;&#25968;&#65292;&#23558;&#22320;&#29702;&#36317;&#31163;&#21644;&#23646;&#24615;&#36317;&#31163;&#32467;&#21512;&#36215;&#26469;&#12290;&#21464;&#37327;&#36317;&#31163;&#21152;&#26435;&#22238;&#24402;&#65288;CWR&#65289;&#26159;GWR&#30340;&#25193;&#23637;&#65292;&#21253;&#25324;&#22320;&#29702;&#36317;&#31163;&#21644;&#23646;&#24615;&#36317;&#31163;&#12290;&#25151;&#20215;&#21463;&#21040;&#35832;&#22810;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#22914;&#25151;&#40836;&#12289;&#24314;&#31569;&#38754;&#31215;&#21644;&#22303;&#22320;&#29992;&#36884;&#31561;&#12290;&#39044;&#27979;&#27169;&#22411;&#29992;&#20110;&#24110;&#21161;&#20102;&#35299;&#21306;&#22495;&#25151;&#20215;&#30340;&#29305;&#24449;&#12290;CWR&#34987;&#29992;&#26469;&#20102;&#35299;&#25151;&#20215;&#19982;&#25511;&#21046;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;CWR&#21487;&#20197;&#32771;&#34385;&#22320;&#36136;&#21644;&#23646;&#24615;&#36317;&#31163;&#65292;&#24182;&#20135;&#29983;&#20445;&#30041;&#22320;&#36136;&#21644;&#23646;&#24615;&#36317;&#31163;&#20989;&#25968;&#26435;&#37325;&#30697;&#38453;&#30340;&#25151;&#20215;&#20934;&#30830;&#20272;&#35745;&#12290;&#32467;&#26524;&#34920;&#26126;CWR&#22312;&#25151;&#20215;&#20272;&#35745;&#20013;&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geographically weighted regression (GWR) is a popular tool for modeling spatial heterogeneity in a regression model. However, the current weighting function used in GWR only considers the geographical distance, while the attribute similarity is totally ignored. In this study, we proposed a covariate weighting function that combines the geographical distance and attribute distance. The covariate-distance weighted regression (CWR) is the extension of GWR including geographical distance and attribute distance. House prices are affected by numerous factors, such as house age, floor area, and land use. Prediction model is used to help understand the characteristics of regional house prices. The CWR was used to understand the relationship between the house price and controlling factors. The CWR can consider the geological and attribute distances, and produce accurate estimates of house price that preserve the weight matrix for geological and attribute distance functions. Results show that th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#24314;&#31569;&#29289;&#25104;&#26412;&#38477;&#20302;&#21644;&#33021;&#32791;&#24433;&#21709;&#22240;&#32032;&#30340;&#35782;&#21035;&#65292;&#32467;&#26524;&#34920;&#26126;&#38548;&#28909;&#12289;&#24314;&#31569;&#29289;&#23610;&#23544;&#21644;&#21046;&#20919;&#31995;&#32479;&#31867;&#22411;&#26159;&#24433;&#21709;&#33021;&#28304;&#28040;&#32791;&#21644;&#25104;&#26412;&#38477;&#20302;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.08886</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#35782;&#21035;&#24433;&#21709;&#24314;&#31569;&#29289;&#33021;&#32791;&#21644;&#25104;&#26412;&#38477;&#20302;&#30340;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Identification of the Factors Affecting the Reduction of Energy Consumption and Cost in Buildings Using Data Mining Techniques. (arXiv:2305.08886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#24314;&#31569;&#29289;&#25104;&#26412;&#38477;&#20302;&#21644;&#33021;&#32791;&#24433;&#21709;&#22240;&#32032;&#30340;&#35782;&#21035;&#65292;&#32467;&#26524;&#34920;&#26126;&#38548;&#28909;&#12289;&#24314;&#31569;&#29289;&#23610;&#23544;&#21644;&#21046;&#20919;&#31995;&#32479;&#31867;&#22411;&#26159;&#24433;&#21709;&#33021;&#28304;&#28040;&#32791;&#21644;&#25104;&#26412;&#38477;&#20302;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#33021;&#32791;&#21644;&#21327;&#35843;&#20844;&#29992;&#20107;&#19994;&#31995;&#32479;&#19968;&#30452;&#26159;&#24314;&#31569;&#34892;&#19994;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#24314;&#31569;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#33021;&#28304;&#28040;&#32791;&#32773;&#20043;&#19968;&#65292;&#20854;&#33021;&#25928;&#23545;&#20110;&#38450;&#27490;&#28010;&#36153;&#21644;&#38477;&#20302;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#24314;&#31569;&#29289;&#20135;&#29983;&#22823;&#37327;&#30340;&#21407;&#22987;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#20110;&#20102;&#35299;&#33021;&#28304;&#28040;&#32791;&#27169;&#24335;&#24182;&#21327;&#21161;&#24320;&#21457;&#20248;&#21270;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35782;&#21035;&#24433;&#21709;&#24314;&#31569;&#29289;&#25104;&#26412;&#38477;&#20302;&#21644;&#33021;&#32791;&#30340;&#22240;&#32032;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#19977;&#31181;&#22238;&#24402;&#27169;&#22411;&#65288;Lasso&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#65289;&#26469;&#39044;&#27979;&#24314;&#31569;&#29289;&#30340;&#20027;&#35201;&#29123;&#26009;&#20351;&#29992;&#12289;&#30005;&#33021;&#28040;&#32791;&#21644;&#25104;&#26412;&#33410;&#30465;&#12290;&#36827;&#34892;&#20102;&#19968;&#39033;&#24433;&#21709;&#33021;&#32791;&#21644;&#25104;&#26412;&#38477;&#20302;&#22240;&#32032;&#30340;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#20248;&#21270;&#20915;&#31574;&#26641;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20803;&#21551;&#21457;&#25216;&#26415;&#65292;&#25105;&#20204;&#23545;&#20915;&#31574;&#26641;&#31639;&#27861;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24433;&#21709;&#24314;&#31569;&#29289;&#33021;&#32791;&#21644;&#25104;&#26412;&#38477;&#20302;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#38548;&#28909;&#12289;&#24314;&#31569;&#29289;&#23610;&#23544;&#21644;&#20351;&#29992;&#30340;&#21046;&#20919;&#31995;&#32479;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing energy consumption and coordination of utility systems have long been a concern of the building industry. Buildings are one of the largest energy consumers in the world, making their energy efficiency crucial for preventing waste and reducing costs. Additionally, buildings generate substantial amounts of raw data, which can be used to understand energy consumption patterns and assist in developing optimization strategies. Using a real-world dataset, this research aims to identify the factors that influence building cost reduction and energy consumption. To achieve this, we utilize three regression models (Lasso Regression, Decision Tree, and Random Forest) to predict primary fuel usage, electrical energy consumption, and cost savings in buildings. An analysis of the factors influencing energy consumption and cost reduction is conducted, and the decision tree algorithm is optimized using metaheuristics. By employing metaheuristic techniques, we fine-tune the decision tree alg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#29983;&#25104;&#26234;&#33021;&#23478;&#23621;&#33021;&#32791;&#25968;&#25454;&#30340;VAE-GAN&#25216;&#26415;&#65292;&#21516;&#26102;&#25506;&#32034;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;Q&#23398;&#20064;&#32467;&#21512;&#24212;&#29992;&#20110;&#22522;&#20110;Q&#23398;&#20064;&#30340;HEMS&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;VAE-GAN&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#22320;&#35757;&#32451;HEMS&#65292;&#20026;&#26234;&#33021;&#23478;&#23621;&#33410;&#33021;&#24102;&#26469;&#20102;&#26032;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.08885</link><description>&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#33021;&#28304;&#31649;&#29702;&#65306;VAE-GAN&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Smart Home Energy Management: VAE-GAN synthetic dataset generator and Q-learning. (arXiv:2305.08885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08885
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#29983;&#25104;&#26234;&#33021;&#23478;&#23621;&#33021;&#32791;&#25968;&#25454;&#30340;VAE-GAN&#25216;&#26415;&#65292;&#21516;&#26102;&#25506;&#32034;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;Q&#23398;&#20064;&#32467;&#21512;&#24212;&#29992;&#20110;&#22522;&#20110;Q&#23398;&#20064;&#30340;HEMS&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;VAE-GAN&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#22320;&#35757;&#32451;HEMS&#65292;&#20026;&#26234;&#33021;&#23478;&#23621;&#33410;&#33021;&#24102;&#26469;&#20102;&#26032;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#20998;&#26512;&#20303;&#23429;&#24314;&#31569;&#29289;&#30340;&#30005;&#21147;&#28040;&#32791;&#24182;&#37319;&#29992;&#26234;&#33021;&#23478;&#23621;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#65288;HEMS&#65289;&#20197;&#20943;&#23569;&#23478;&#24237;&#33021;&#28304;&#28040;&#32791;&#21644;&#25104;&#26412;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#12290; HEMS&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#27169;&#25311;&#23454;&#38469;&#26234;&#33021;&#30005;&#32593;&#30340;&#32479;&#35745;&#21644;&#21151;&#33021;&#23646;&#24615;&#12290;&#20844;&#20849;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#33719;&#21462;&#26159;&#36825;&#31867;&#30740;&#31350;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#21512;&#25104;&#31995;&#32479;&#30340;&#19981;&#21516;&#25805;&#20316;&#26465;&#20214;&#25152;&#20195;&#34920;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#24320;&#21457;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#20154;&#24037;HEMS&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;VAE-GAN&#25216;&#26415;&#65292;&#29992;&#20110;&#29983;&#25104;&#26234;&#33021;&#23478;&#23621;&#33021;&#32791;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290; &#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22312;&#22522;&#20110;Q&#23398;&#20064;&#30340;HEMS&#20013;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#20043;&#30456;&#32467;&#21512;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#30340;&#26234;&#33021;&#23478;&#23621;&#25968;&#25454;&#27979;&#35797;&#20102;&#22522;&#20110;Q&#23398;&#20064;&#30340;HEMS&#30340;&#22312;&#32447;&#34920;&#29616;&#12290;&#20026;&#20102;&#27979;&#35797;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#23558;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20316;&#20026;Q&#23398;&#20064;&#22522;&#30784;&#30340;HEMS&#30340;&#36755;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VAE-GAN&#25216;&#26415;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#21487;&#29992;&#20110;&#26377;&#25928;&#22320;&#35757;&#32451;HEMS&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have noticed an increasing interest among academia and industry towards analyzing the electrical consumption of residential buildings and employing smart home energy management systems (HEMS) to reduce household energy consumption and costs. HEMS has been developed to simulate the statistical and functional properties of actual smart grids. Access to publicly available datasets is a major challenge in this type of research. The potential of artificial HEMS applications will be further enhanced with the development of time series that represent different operating conditions of the synthetic systems. In this paper, we propose a novel variational auto-encoder-generative adversarial network (VAE-GAN) technique for generating time-series data on energy consumption in smart homes. We also explore how the generative model performs when combined with a Q-learning-based HEMS. We tested the online performance of Q-learning-based HEMS with real-world smart home data. To test the gen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#33041;&#32959;&#30244;&#20998;&#21106;&#30340;&#24494;&#35843;&#31639;&#27861;&#65292;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#23558;&#39640;&#32423;&#21035;&#33014;&#36136;&#30244;&#24212;&#29992;&#20110;&#33041;&#36716;&#31227;&#30244;&#65292;&#20960;&#27493;&#20043;&#20869;&#23454;&#29616;&#20102;&#33014;&#36136;&#30244;&#21644;&#33041;&#36716;&#31227;&#30244;&#39046;&#22495;&#30340;&#24179;&#34913;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.08878</link><description>&lt;p&gt;
&#23398;&#20064;&#26410;&#23398;&#29305;&#24449;&#29992;&#20110;&#33041;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn Unlearned Feature for Brain Tumor Segmentation. (arXiv:2305.08878v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08878
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#33041;&#32959;&#30244;&#20998;&#21106;&#30340;&#24494;&#35843;&#31639;&#27861;&#65292;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#23558;&#39640;&#32423;&#21035;&#33014;&#36136;&#30244;&#24212;&#29992;&#20110;&#33041;&#36716;&#31227;&#30244;&#65292;&#20960;&#27493;&#20043;&#20869;&#23454;&#29616;&#20102;&#33014;&#36136;&#30244;&#21644;&#33041;&#36716;&#31227;&#30244;&#39046;&#22495;&#30340;&#24179;&#34913;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32959;&#30244;&#20998;&#21106;&#30340;&#24494;&#35843;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#24182;&#24110;&#21161;&#32593;&#32476;&#19981;&#24536;&#35760;&#21407;&#22987;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#12290;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#38754;&#20020;&#30340;&#22256;&#38590;&#20043;&#19968;&#26159;&#32570;&#20047;&#36866;&#24403;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#22240;&#20026;&#38656;&#35201;&#21307;&#29983;&#23545;&#21487;&#38752;&#30340;&#27880;&#37322;&#36827;&#34892;&#26631;&#35760;&#65292;&#24182;&#19988;&#26377;&#35768;&#22810;&#30142;&#30149;&#30340;&#21464;&#20307;&#65292;&#20363;&#22914;&#33014;&#36136;&#30244;&#21644;&#33041;&#36716;&#31227;&#30244;&#65292;&#36825;&#20123;&#26159;&#19981;&#21516;&#31867;&#22411;&#30340;&#33041;&#32959;&#30244;&#65292;&#22312;MR&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#32467;&#26500;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#19981;&#21487;&#33021;&#20026;&#25152;&#26377;&#31867;&#22411;&#30340;&#30142;&#30149;&#20135;&#29983;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#39640;&#32423;&#21035;&#33014;&#36136;&#30244;&#21040;&#33041;&#36716;&#31227;&#30244;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20960;&#27493;&#20043;&#20869;&#23454;&#29616;&#20102;&#33014;&#36136;&#30244;&#21644;&#33041;&#36716;&#31227;&#30244;&#39046;&#22495;&#30340;&#24179;&#34913;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a fine-tuning algorithm for brain tumor segmentation that needs only a few data samples and helps networks not to forget the original tasks. Our approach is based on active learning and meta-learning. One of the difficulties in medical image segmentation is the lack of datasets with proper annotations, because it requires doctors to tag reliable annotation and there are many variants of a disease, such as glioma and brain metastasis, which are the different types of brain tumor and have different structural features in MR images. Therefore, it is impossible to produce the large-scale medical image datasets for all types of diseases. In this paper, we show a transfer learning method from high grade glioma to brain metastasis, and demonstrate that the proposed algorithm achieves balanced parameters for both glioma and brain metastasis domains within a few steps.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#25512;&#29702;&#20808;&#21069;&#21644;&#23398;&#20064;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.08876</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#21450;&#20854;&#20998;&#31867;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI and its Taxonomy: a survey. (arXiv:2305.08876v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#25512;&#29702;&#20808;&#21069;&#21644;&#23398;&#20064;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#32452;&#21512;&#31526;&#21495;&#22788;&#29702;&#65288;&#22914;&#32463;&#20856;&#20154;&#24037;&#26234;&#33021;&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#26159;&#19968;&#20010;&#38750;&#24120;&#25104;&#29087;&#30340;&#39046;&#22495;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#19968;&#31181;&#23581;&#35797;&#65292;&#22312;&#25506;&#32034;&#38500;&#20102;&#22686;&#21152;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#23610;&#23544;&#20197;&#22806;&#30340;&#26367;&#20195;&#26041;&#26696;&#20197;&#21450;&#23558;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#25512;&#29702;&#20808;&#21069;&#21644;&#23398;&#20064;&#30693;&#35782;&#30456;&#32467;&#21512;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#20316;&#29992;&#12290;&#26412;&#27425;&#35843;&#26597;&#30740;&#31350;&#20102;&#36825;&#19968;&#39046;&#22495;&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#24182;&#25552;&#20379;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#20998;&#31867;&#21644;&#27604;&#36739;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#24212;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI deals with models that combine symbolic processing, like classic AI, and neural networks, as it's a very established area. These models are emerging as an effort toward Artificial General Intelligence (AGI) by both exploring an alternative to just increasing datasets' and models' sizes and combining Learning over the data distribution, Reasoning on prior and learned knowledge, and by symbiotically using them. This survey investigates research papers in this area during recent years and brings classification and comparison between the presented models as well as applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#21333;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#38598;&#25104;&#20013;&#20272;&#35745;&#34920;&#31034;&#39044;&#27979;&#35823;&#24046;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29366;&#24577;&#20381;&#36182;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#28151;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08874</link><description>&lt;p&gt;
&#22312;&#20018;&#34892;&#25968;&#25454;&#21516;&#21270;&#20013;&#65292;&#22522;&#20110;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Online machine-learning forecast uncertainty estimation for sequential data assimilation. (arXiv:2305.08874v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#21333;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#38598;&#25104;&#20013;&#20272;&#35745;&#34920;&#31034;&#39044;&#27979;&#35823;&#24046;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29366;&#24577;&#20381;&#36182;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#28151;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#20013;&#24212;&#29992;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26159;&#29616;&#20195;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#21644;&#25968;&#25454;&#21516;&#21270;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#12290;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#21516;&#21270;&#31995;&#32479;&#37319;&#29992;&#22810;&#20010;&#27169;&#22411;&#38598;&#25104;&#26469;&#21253;&#21547;&#29366;&#24577;&#20381;&#36182;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;,&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#24320;&#21457;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#38598;&#25104;&#20272;&#35745;&#34920;&#31034;&#39044;&#27979;&#35823;&#24046;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29366;&#24577;&#20381;&#36182;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#30340;&#65292;&#35813;&#20989;&#25968;&#32771;&#34385;&#20102;&#39044;&#27979;&#35823;&#24046;&#30340;&#24322;&#26041;&#24046;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#22312;&#28151;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;Kalman&#26679;&#24335;&#20998;&#26512;&#26356;&#26032;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29366;&#24577;&#20381;&#36182;&#39044;&#27979;&#35823;&#24046;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20272;&#35745;&#12290;&#20351;&#29992;Lorensso&#31561;&#65288;2006&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#35266;&#27979;&#31995;&#32479;&#27169;&#25311;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24120;&#29992;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying forecast uncertainty is a key aspect of state-of-the-art numerical weather prediction and data assimilation systems. Ensemble-based data assimilation systems incorporate state-dependent uncertainty quantification based on multiple model integrations. However, this approach is demanding in terms of computations and development. In this work a machine learning method is presented based on convolutional neural networks that estimates the state-dependent forecast uncertainty represented by the forecast error covariance matrix using a single dynamical model integration. This is achieved by the use of a loss function that takes into account the fact that the forecast errors are heterodastic. The performance of this approach is examined within a hybrid data assimilation method that combines a Kalman-like analysis update and the machine learning based estimation of a state-dependent forecast error covariance matrix. Observing system simulation experiments are conducted using the Lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22312;&#32447;&#35270;&#35273;&#23548;&#33322;&#31995;&#32479; WVN&#65292;&#33021;&#22815;&#22312;&#19981;&#21040; 5 &#20998;&#38047;&#30340;&#23454;&#22320;&#22521;&#35757;&#26102;&#38388;&#20869;&#21551;&#21160;&#21487;&#36890;&#36807;&#22320;&#24418;&#20998;&#21106;&#65292;&#24182;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#25928;&#22320;&#22312;&#25361;&#25112;&#24615;&#29615;&#22659;&#19979;&#23436;&#25104;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2305.08510</link><description>&lt;p&gt;
&#37326;&#22806;&#35270;&#35273;&#23548;&#33322;&#20013;&#30340;&#24555;&#36895;&#21487;&#36890;&#34892;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fast Traversability Estimation for Wild Visual Navigation. (arXiv:2305.08510v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22312;&#32447;&#35270;&#35273;&#23548;&#33322;&#31995;&#32479; WVN&#65292;&#33021;&#22815;&#22312;&#19981;&#21040; 5 &#20998;&#38047;&#30340;&#23454;&#22320;&#22521;&#35757;&#26102;&#38388;&#20869;&#21551;&#21160;&#21487;&#36890;&#36807;&#22320;&#24418;&#20998;&#21106;&#65292;&#24182;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#25928;&#22320;&#22312;&#25361;&#25112;&#24615;&#29615;&#22659;&#19979;&#23436;&#25104;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#29615;&#22659;&#22914;&#26862;&#26519;&#21644;&#33609;&#22320;&#23545;&#20110;&#26426;&#22120;&#20154;&#23548;&#33322;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#39640;&#33609;&#12289;&#26641;&#26525;&#25110;&#28748;&#26408;&#20250;&#20135;&#29983;&#34394;&#20551;&#30340;&#38556;&#30861;&#29289;&#24863;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; WVN &#30340;&#22312;&#32447;&#33258;&#30417;&#30563;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#20165;&#20351;&#29992;&#35270;&#35273;&#36827;&#34892;&#30340;&#21487;&#36890;&#34892;&#24615;&#20272;&#35745;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#20174;&#23454;&#22320;&#30701;&#26102;&#38388;&#30340;&#20154;&#31867;&#28436;&#31034;&#20013;&#19981;&#26029;&#33258;&#36866;&#24212;&#12290;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#35273;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#39640;&#32500;&#29305;&#24449;&#65292;&#37319;&#29992;&#22312;&#32447;&#30417;&#30563;&#29983;&#25104;&#26041;&#26696;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#26102;&#36816;&#34892;&#12290;&#25105;&#20204;&#22312;&#26862;&#26519;&#12289;&#20844;&#22253;&#21644;&#33609;&#22320;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#19981;&#21040;5&#20998;&#38047;&#30340;&#23454;&#22320;&#22521;&#35757;&#26102;&#38388;&#20869;&#33021;&#22815;&#21551;&#21160;&#21487;&#36890;&#36807;&#22320;&#24418;&#20998;&#21106;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#25143;&#22806;&#22320;&#24418;&#20013;&#23548;&#33322;-&#20811;&#26381;&#39640;&#37326;&#33609;&#30340;&#38556;&#30861;&#65292;&#20197;&#21450;&#36319;&#38543;1.4&#20844;&#37324;&#30340;&#20154;&#34892;&#36947;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#23454;&#39564;&#25968;&#25454;&#22343;&#26469;&#33258;&#27431;&#27954;&#65292;&#20294;&#26159;&#25105;&#20204;&#35748;&#20026;&#32467;&#26524;&#20173;&#28982;&#20855;&#26377;&#19968;&#23450;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we propose Wild Visual Navigation (WVN), an online self-supervised learning system for traversability estimation which uses only vision. The system is able to continuously adapt from a short human demonstration in the field. It leverages high-dimensional features from self-supervised visual transformer models, with an online scheme for supervision generation that runs in real-time on the robot. We demonstrate the advantages of our approach with experiments and ablation studies in challenging environments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex outdoor terrains - negotiating obstacles in high grass as well as a 1.4 km footpath following. While our ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36890;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#65292;&#20171;&#32461;&#20102;&#21160;&#21147;&#23398;&#24179;&#22343;&#22330;&#29702;&#35770;&#21450;&#20854;&#25968;&#20540;&#23454;&#29616;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29289;&#29702;&#19978;&#36879;&#26126;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#32593;&#32476;&#30340;&#38598;&#20307;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2305.08459</link><description>&lt;p&gt;
&#36890;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#24179;&#22343;&#22330;&#29702;&#35770;&#20837;&#38376;
&lt;/p&gt;
&lt;p&gt;
Introduction to dynamical mean-field theory of generic random neural networks. (arXiv:2305.08459v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36890;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#65292;&#20171;&#32461;&#20102;&#21160;&#21147;&#23398;&#24179;&#22343;&#22330;&#29702;&#35770;&#21450;&#20854;&#25968;&#20540;&#23454;&#29616;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29289;&#29702;&#19978;&#36879;&#26126;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#32593;&#32476;&#30340;&#38598;&#20307;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#21147;&#23398;&#24179;&#22343;&#22330;&#29702;&#35770;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29289;&#29702;&#24037;&#20855;&#65292;&#29992;&#20110;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#20856;&#22411;&#34892;&#20026;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#21487;&#20197;&#24490;&#29615;&#36830;&#25509;&#65292;&#25110;&#32773;&#21487;&#20197;&#22534;&#21472;&#22810;&#23618;&#31070;&#32463;&#20803;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21021;&#23398;&#32773;&#26469;&#35828;&#65292;&#24456;&#38590;&#25509;&#35302;&#21040;&#27492;&#24037;&#20855;&#21644;&#22522;&#30784;&#29289;&#29702;&#30340;&#31934;&#39635;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20197;&#36890;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20026;&#20363;&#65292;&#32473;&#20986;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#25945;&#32946;&#24615;&#20171;&#32461;&#65292;&#22312;&#27492;&#31867;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#36890;&#36807;&#30456;&#20851;&#31361;&#35302;&#38543;&#26426;&#32780;&#23436;&#20840;&#36830;&#25509;&#65292;&#22240;&#27492;&#32593;&#32476;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#38598;&#20307;&#21160;&#24577;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#24212;&#29992;&#27492;&#24037;&#20855;&#30340;&#30456;&#20851;&#36807;&#21435;&#21644;&#26368;&#36817;&#37325;&#35201;&#20316;&#21697;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#29289;&#29702;&#19978;&#36879;&#26126;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#21160;&#24577;&#33108;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23548;&#20986;&#20102;&#23436;&#20840;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;&#35814;&#32454;&#20171;&#32461;&#20102;&#27714;&#35299;&#31215;&#20998;&#24494;&#20998;&#24179;&#22343;&#22330;&#26041;&#31243;&#30340;&#25968;&#20540;&#23454;&#29616;&#65292;&#20197;&#21450;&#25506;&#32034;&#27874;&#21160;&#32791;&#25955;&#23450;&#29702;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamical mean-field theory is a powerful physics tool used to analyze the typical behavior of neural networks, where neurons can be recurrently connected, or multiple layers of neurons can be stacked. However, it is not easy for beginners to access the essence of this tool and the underlying physics. Here, we give a pedagogical introduction of this method in a particular example of generic random neural networks, where neurons are randomly and fully connected by correlated synapses and therefore the network exhibits rich emergent collective dynamics. We also review related past and recent important works applying this tool. In addition, a physically transparent and alternative method, namely the dynamical cavity method, is also introduced to derive exactly the same results. The numerical implementation of solving the integro-differential mean-field equations is also detailed, with an illustration of exploring the fluctuation dissipation theorem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26412;&#26500;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#30340;&#25429;&#25417;&#19982;&#36895;&#29575;&#30456;&#20851;&#30340;&#24212;&#21147;-&#24212;&#21464;&#20851;&#31995;&#21644;&#19968;&#33268;&#30340;&#20999;&#32447;&#27169;&#37327;&#65292;&#20197;&#30740;&#31350;&#21547;&#28287;&#24230;&#32435;&#31859;&#39063;&#31890;/&#29615;&#27687;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#30340;&#24490;&#29615;&#31896;&#24377;-&#31896;&#22609;&#24615;-&#30772;&#22351;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.08102</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21547;&#28287;&#24230;&#29615;&#27687;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#31896;&#24377;-&#31896;&#22609;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A machine learning-based viscoelastic-viscoplastic model for epoxy nanocomposites with moisture content. (arXiv:2305.08102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26412;&#26500;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#30340;&#25429;&#25417;&#19982;&#36895;&#29575;&#30456;&#20851;&#30340;&#24212;&#21147;-&#24212;&#21464;&#20851;&#31995;&#21644;&#19968;&#33268;&#30340;&#20999;&#32447;&#27169;&#37327;&#65292;&#20197;&#30740;&#31350;&#21547;&#28287;&#24230;&#32435;&#31859;&#39063;&#31890;/&#29615;&#27687;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#30340;&#24490;&#29615;&#31896;&#24377;-&#31896;&#22609;&#24615;-&#30772;&#22351;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26412;&#26500;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#21547;&#28287;&#24230;&#32435;&#31859;&#39063;&#31890;/&#29615;&#27687;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#30340;&#24490;&#29615;&#31896;&#24377;-&#31896;&#22609;&#24615;-&#30772;&#22351;&#34892;&#20026;&#12290;&#37319;&#29992;&#19968;&#31181;&#37319;&#26679;&#25216;&#26415;&#21644;&#25200;&#21160;&#26041;&#27861;&#30340;&#32452;&#21512;&#26694;&#26550;&#35757;&#32451;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65292;&#23558;&#23454;&#39564;&#39564;&#35777;&#30340;&#31896;&#24377;-&#31896;&#22609;&#24615;&#27169;&#22411;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#21040;&#19982;&#21152;&#36733;&#36895;&#29575;&#30456;&#20851;&#30340;&#24212;&#21147;-&#24212;&#21464;&#20851;&#31995;&#21644;&#19968;&#33268;&#30340;&#20999;&#32447;&#27169;&#37327;&#12290;&#27492;&#22806;&#65292;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26412;&#26500;&#27169;&#22411;&#23454;&#29616;&#21040;&#26377;&#38480;&#20803;&#20998;&#26512;&#20013;&#12290;&#36890;&#36807;&#26377;&#38480;&#20803;&#27169;&#25311;&#30740;&#31350;&#20102;&#21152;&#36733;&#36895;&#29575;&#21644;&#28287;&#24230;&#21547;&#37327;&#23545;&#32435;&#31859;&#39063;&#31890;/&#29615;&#27687;&#26679;&#21697;&#30340;&#21147;-&#20301;&#31227;&#21709;&#24212;&#30340;&#24433;&#21709;&#12290;&#25968;&#20540;&#23454;&#20363;&#34920;&#26126;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#21462;&#20915;&#20110;&#21152;&#36733;&#26465;&#20214;&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26412;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a deep learning (DL)-based constitutive model for investigating the cyclic viscoelastic-viscoplastic-damage behavior of nanoparticle/epoxy nanocomposites with moisture content. For this, a long short-term memory network is trained using a combined framework of a sampling technique and a perturbation method. The training framework, along with the training data generated by an experimentally validated viscoelastic-viscoplastic model, enables the DL model to accurately capture the rate-dependent stress-strain relationship and consistent tangent moduli. In addition, the DL-based constitutive model is implemented into finite element analysis. Finite element simulations are performed to study the effect of load rate and moisture content on the force-displacement response of nanoparticle/ epoxy samples. Numerical examples show that the computational efficiency of the DL model depends on the loading condition and is significantly higher than the conventional constituti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08014</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#38754;&#32908;&#30005;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#30636;&#26102;&#39640;&#28165;&#32908;&#30005;&#22270;&#20687;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#21487;&#20197;&#24320;&#36767;&#21457;&#23637;&#26356;&#27969;&#30021;&#12289;&#26356;&#33258;&#28982;&#30340;&#32908;&#32905;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#26032;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#36328;&#22330;&#26223;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23384;&#22312;&#26497;&#22823;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#38750;&#24120;&#22823;&#19988;&#22797;&#26434;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25110;&#22522;&#20110;2SRNN&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#26469;&#36924;&#36817;&#30001;&#36825;&#20123;&#36328;&#22330;&#26223;&#25968;&#25454;&#21464;&#24322;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#38656;&#35201;&#22312;&#39044;&#35757;&#32451;&#21644;&#36866;&#24212;&#38454;&#27573;&#20013;&#22312;&#25968;&#30334;&#19975;&#20010;&#35757;&#32451;&#21442;&#25968;&#21644;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#36827;&#34892;&#39640;&#31471;&#36164;&#28304;&#32422;&#26463;&#21644;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;(TL)&#26469;&#22686;&#24378;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
&lt;/p&gt;</description></item><item><title>Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.07637</link><description>&lt;p&gt;
Text2Cohort: &#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#23545;&#30284;&#30151;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07637
&lt;/p&gt;
&lt;p&gt;
Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;(IDC)&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#25968;&#25454;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#24320;&#25918;&#33719;&#21462;&#30340;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#26088;&#22312;&#20419;&#36827;&#21307;&#23398;&#25104;&#20687;&#30740;&#31350;&#20013;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#25216;&#26415;&#24615;&#36136;&#65292;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#20197;&#36827;&#34892;&#38431;&#21015;&#21457;&#29616;&#21644;&#35775;&#38382;&#25104;&#20687;&#25968;&#25454;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#26174;&#33879;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Text2Cohort&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#23558;&#26597;&#35810;&#30340;&#21709;&#24212;&#36820;&#22238;&#32473;&#29992;&#25143;&#65292;&#20197;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26657;&#27491;&#20197;&#35299;&#20915;&#26597;&#35810;&#20013;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#65292;&#36890;&#36807;&#23558;&#38169;&#35823;&#20256;&#22238;&#27169;&#22411;&#36827;&#34892;&#35299;&#37322;&#21644;&#26657;&#27491;&#12290;&#25105;&#20204;&#23545;50&#20010;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#20102;Text2Cohort&#35780;&#20272;&#65292;&#33539;&#22260;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#38431;&#21015;&#21457;&#29616;&#12290;&#32467;&#26524;&#26597;&#35810;&#21644;&#36755;&#20986;&#30001;&#20004;&#20301;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Imaging Data Commons (IDC) is a cloud-based database that provides researchers with open access to cancer imaging data and tools for analysis, with the goal of facilitating collaboration in medical imaging research. However, querying the IDC database for cohort discovery and access to imaging data has a significant learning curve for researchers due to its complex and technical nature. We developed Text2Cohort, a large language model (LLM) based toolkit to facilitate natural language cohort discovery by translating user input into IDC database queries through prompt engineering and returning the query's response to the user. Furthermore, autocorrection is implemented to resolve syntax and semantic errors in queries by passing the errors back to the model for interpretation and correction. We evaluate Text2Cohort on 50 natural language user inputs ranging from information extraction to cohort discovery. The resulting queries and outputs were verified by two computer scientists to me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X&#65292;&#24182;&#35777;&#26126;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#23548;&#33268;&#20102;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2305.07437</link><description>&lt;p&gt;
&#24102;&#26377;&#38750;&#23545;&#35282;&#20449;&#24687;&#30340;&#35270;&#35273;-&#35821;&#35328;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Vision-Language Representaion Learning with Off-Diagonal Information. (arXiv:2305.07437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X&#65292;&#24182;&#35777;&#26126;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#23548;&#33268;&#20102;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#36861;&#36394;&#36830;&#32493;&#26356;&#26032;&#30340;CLIP&#27169;&#22411;&#20013;&#34920;&#31034;&#21521;&#37327;&#30340;&#26041;&#21521;&#21464;&#21270;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#24635;&#32467;&#20102;&#36825;&#20123;&#31354;&#38388;&#21464;&#21270;&#65292;&#31216;&#20026;&#31354;&#38388;&#28151;&#20081;&#65288;SD&#65289;&#65292;&#21487;&#20197;&#20998;&#20026;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#22914;&#20309;&#23548;&#33268;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#31354;&#38388;&#28151;&#20081;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X: &#32500;&#25252;&#38750;&#23545;&#35282;&#20449;&#24687;&#30697;&#38453;&#12290;&#22312;&#21508;&#31181;&#19981;&#21516;&#35268;&#27169;&#21644;&#33539;&#22260;&#30340;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the feasibility of continuously training the CLIP model through streaming data. Then, by tracking the directional changes of the representation vectors in the continuously updated CLIP model, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we demonstrate how intra-modal rotation and inter-modal deviation lead to a performance decline for CLIP on cross-modal retrieval tasks in both empirically and theoretically. To alleviate the spatial disorder, we propose a simple yet effective continual learning framework Mod-X: Maintain off-diagonal information-matriX. The experiments (in Section \ref{method}, \ref{experiments} and Appendix \ref{Appendix_to_experiments}) on commonly used datasets with different scales and scopes have illustrated the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;GS-VIN&#65289;&#26469;&#35299;&#20915;&#20540;&#36845;&#20195;&#32593;&#32476;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#36755;&#20837;&#22320;&#22270;&#21644;&#20943;&#36731;&#32047;&#31215;&#35823;&#24046;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#36845;&#20195;&#31574;&#30053;&#21644;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#65292;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#24182;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07039</link><description>&lt;p&gt;
&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Value Iteration Networks with Gated Summarization Module. (arXiv:2305.07039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;GS-VIN&#65289;&#26469;&#35299;&#20915;&#20540;&#36845;&#20195;&#32593;&#32476;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#36755;&#20837;&#22320;&#22270;&#21644;&#20943;&#36731;&#32047;&#31215;&#35823;&#24046;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#36845;&#20195;&#31574;&#30053;&#21644;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#65292;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#24182;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;VIN&#65289;&#22312;&#22788;&#29702;&#26356;&#22823;&#30340;&#36755;&#20837;&#22320;&#22270;&#65292;&#20943;&#36731;&#30001;&#22686;&#21152;&#36845;&#20195;&#27425;&#25968;&#24341;&#36215;&#30340;&#32047;&#31215;&#35823;&#24046;&#30340;&#25361;&#25112;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#20855;&#26377;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#30340;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;GS-VIN&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#33258;&#36866;&#24212;&#36845;&#20195;&#31574;&#30053;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#21367;&#31215;&#26680;&#20943;&#23569;&#36845;&#20195;&#27425;&#25968;&#65292;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#65292;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#21010;&#36807;&#31243;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#38376;&#25511;&#27719;&#24635;&#27169;&#22359;&#65292;&#20351;&#24471;&#32593;&#32476;&#21487;&#20197;&#24378;&#35843;&#25972;&#20010;&#35268;&#21010;&#36807;&#31243;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#26368;&#32456;&#30340;&#20840;&#23616;&#35268;&#21010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the challenges faced by Value Iteration Networks (VIN) in handling larger input maps and mitigating the impact of accumulated errors caused by increased iterations. We propose a novel approach, Value Iteration Networks with Gated Summarization Module (GS-VIN), which incorporates two main improvements: (1) employing an Adaptive Iteration Strategy in the Value Iteration module to reduce the number of iterations, and (2) introducing a Gated Summarization module to summarize the iterative process. The adaptive iteration strategy uses larger convolution kernels with fewer iteration times, reducing network depth and increasing training stability while maintaining the accuracy of the planning process. The gated summarization module enables the network to emphasize the entire planning process, rather than solely relying on the final global planning outcome, by temporally and spatially resampling the entire planning process within the VI module. We conduct experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#27491;&#21017;&#21270;Tucker&#20998;&#35299;&#30340;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#25913;&#21892;&#20102;Tucker&#26680;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#24341;&#20837;&#27969;&#24418;&#27491;&#21017;&#21270;&#21644;&#26102;&#38388;&#32422;&#26463;&#39033;&#26469;&#20248;&#21270;&#24352;&#37327;&#30340;&#22635;&#20805;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06563</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#27491;&#21017;&#21270; Tucker &#20998;&#35299;&#30340;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation. (arXiv:2305.06563v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#27491;&#21017;&#21270;Tucker&#20998;&#35299;&#30340;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#25913;&#21892;&#20102;Tucker&#26680;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#24341;&#20837;&#27969;&#24418;&#27491;&#21017;&#21270;&#21644;&#26102;&#38388;&#32422;&#26463;&#39033;&#26469;&#20248;&#21270;&#24352;&#37327;&#30340;&#22635;&#20805;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;(STDI)&#26159;&#25968;&#25454;&#39537;&#21160;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#19981;&#21487;&#36991;&#20813;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22312;&#37096;&#20998;&#35266;&#27979;&#21040;&#30340;&#20132;&#36890;&#25968;&#25454;&#20013;&#20272;&#35745;&#20002;&#22833;&#25968;&#25454;&#12290;&#30001;&#20110;&#20132;&#36890;&#25968;&#25454;&#20855;&#26377;&#22810;&#32500;&#21644;&#26102;&#31354;&#24615;&#36136;&#65292;&#25105;&#20204;&#23558;&#20002;&#22833;&#25968;&#25454;&#22635;&#20805;&#35270;&#20026;&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#12290;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#35768;&#22810;&#20851;&#20110;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340; STDI &#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#24320;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21033;&#29992;&#26102;&#31354;&#30456;&#20851;&#24615;&#21644;&#26680;&#24352;&#37327;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#22635;&#20805;&#24615;&#33021;&#20173;&#28982;&#38656;&#35201;&#35299;&#20915;&#12290;&#26412;&#25991;&#37325;&#26032;&#26500;&#36896;&#20102;3/4&#38454;&#27721;&#20811;&#23572;&#24352;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27969;&#24418;&#27491;&#21017;&#21270; Tucker &#20998;&#35299;(maniRTD)&#27169;&#22411;&#29992;&#20110;STDI&#12290;&#26126;&#30830;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22810;&#32500;&#24310;&#36831;&#23884;&#20837;&#21464;&#25442;&#23558;&#20256;&#24863;&#20132;&#36890;&#29366;&#24577;&#25968;&#25454;&#34920;&#31034;&#20026;3/4&#38454;&#24352;&#37327;&#12290;&#28982;&#21518;&#65292;ManiRTD&#20351;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#25913;&#21892;&#20102;Tucker&#26680;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#20351;&#29992;&#27969;&#24418;&#27491;&#21017;&#21270;&#21644;&#26102;&#38388;&#32422;&#26463;&#39033;&#26469;&#20248;&#21270;&#24352;&#37327;&#30340;&#22635;&#20805;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal traffic data imputation (STDI), estimating the missing data from partially observed traffic data, is an inevitable and challenging task in data-driven intelligent transportation systems (ITS). Due to traffic data's multidimensional and spatiotemporal properties, we treat the missing data imputation as a tensor completion problem. Many studies have been on STDI based on tensor decomposition in the past decade. However, how to use spatiotemporal correlations and core tensor sparsity to improve the imputation performance still needs to be solved. This paper reshapes a 3rd/4th order Hankel tensor and proposes an innovative manifold regularized Tucker decomposition (ManiRTD) model for STDI. Expressly, we represent the sensory traffic state data as the 3rd/4th tensors by introducing Multiway Delay Embedding Transforms. Then, ManiRTD improves the sparsity of the Tucker core using a sparse regularization term and employs manifold regularization and temporal constraint terms of f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21517;&#20026;FedDWA&#65292;&#37319;&#29992;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#20197;&#26356;&#23569;&#30340;&#36890;&#20449;&#24320;&#38144;&#25429;&#25417;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#33021;&#22815;&#35757;&#32451;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06124</link><description>&lt;p&gt;
FedDWA: &#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#19982;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
FedDWA: Personalized Federated Learning with Online Weight Adjustment. (arXiv:2305.06124v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21517;&#20026;FedDWA&#65292;&#37319;&#29992;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#20197;&#26356;&#23569;&#30340;&#36890;&#20449;&#24320;&#38144;&#25429;&#25417;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#33021;&#22815;&#35757;&#32451;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#19981;&#21516;&#65292;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#33021;&#22815;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#29420;&#29305;&#38656;&#27714;&#26469;&#35757;&#32451;&#23450;&#21046;&#21270;&#27169;&#22411;&#12290;&#20027;&#27969;&#26041;&#27861;&#26159;&#37319;&#29992;&#19968;&#31181;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#26469;&#29983;&#25104;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#30001;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25439;&#22833;&#20540;&#25110;&#27169;&#22411;&#21442;&#25968;&#30830;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#27714;&#23458;&#25143;&#31471;&#19979;&#36733;&#20854;&#20182;&#27169;&#22411;&#65292;&#19981;&#20165;&#22686;&#21152;&#20102;&#36890;&#20449;&#27969;&#37327;&#65292;&#32780;&#19988;&#21487;&#33021;&#20405;&#29359;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PFL&#31639;&#27861;&#65292;&#31216;&#20026;FedDWA&#65288;&#24102;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#30340;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#26681;&#25454;&#20174;&#23458;&#25143;&#31471;&#25910;&#38598;&#30340;&#27169;&#22411;&#35745;&#31639;&#20010;&#24615;&#21270;&#32858;&#21512;&#26435;&#37325;&#12290;&#36825;&#26679;&#65292;FedDWA&#21487;&#20197;&#20197;&#26356;&#23569;&#30340;&#36890;&#20449;&#24320;&#38144;&#25429;&#25417;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#23558;PFL&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#31181;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#26426;&#21046;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#12290;FedDWA&#33021;&#22815;&#23398;&#20064;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#22312;&#32508;&#21512;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;FedDWA&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different from conventional federated learning, personalized federated learning (PFL) is able to train a customized model for each individual client according to its unique requirement. The mainstream approach is to adopt a kind of weighted aggregation method to generate personalized models, in which weights are determined by the loss value or model parameters among different clients. However, such kinds of methods require clients to download others' models. It not only sheer increases communication traffic but also potentially infringes data privacy. In this paper, we propose a new PFL algorithm called \emph{FedDWA (Federated Learning with Dynamic Weight Adjustment)} to address the above problem, which leverages the parameter server (PS) to compute personalized aggregation weights based on collected models from clients. In this way, FedDWA can capture similarities between clients with much less communication overhead. More specifically, we formulate the PFL problem as an optimization 
&lt;/p&gt;</description></item><item><title>BARA&#26159;&#19968;&#31181;&#22312;&#32447;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#31639;&#27861;&#65292;&#29992;&#20110;&#28608;&#21169;&#36328;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25152;&#26377;&#32773;&#20026;&#27169;&#22411;&#35757;&#32451;&#20570;&#20986;&#36129;&#29486;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#34987;&#24573;&#30053;&#30340;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05221</link><description>&lt;p&gt;
BARA: &#39640;&#25928;&#30340;&#22312;&#32447;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#36328;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#28608;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
BARA: Efficient Incentive Mechanism with Online Reward Budget Allocation in Cross-Silo Federated Learning. (arXiv:2305.05221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05221
&lt;/p&gt;
&lt;p&gt;
BARA&#26159;&#19968;&#31181;&#22312;&#32447;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#31639;&#27861;&#65292;&#29992;&#20110;&#28608;&#21169;&#36328;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25152;&#26377;&#32773;&#20026;&#27169;&#22411;&#35757;&#32451;&#20570;&#20986;&#36129;&#29486;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#34987;&#24573;&#30053;&#30340;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#36328;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20132;&#25442;&#22810;&#20010;&#36890;&#20449;&#24490;&#29615;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20351;&#19981;&#21516;&#32452;&#32455;&#30340;&#23396;&#31435;&#25968;&#25454;&#23707;&#21327;&#20316;&#19968;&#20010;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#26469;&#23436;&#25104;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#36328;&#36793;&#32536;FL&#20013;&#65292;&#28608;&#21169;&#26426;&#21046;&#23545;&#20110;&#28608;&#21169;&#25968;&#25454;&#25152;&#26377;&#32773;&#20026;FL&#35757;&#32451;&#20570;&#20986;&#36129;&#29486;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#19981;&#21516;&#30340;&#24490;&#29615;&#20043;&#38388;&#20998;&#37197;&#22870;&#21169;&#39044;&#31639;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#34987;&#29616;&#26377;&#30740;&#31350;&#22823;&#37327;&#24573;&#30053;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25361;&#25112;&#22312;&#20110;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#21644;FL&#27169;&#22411;&#25928;&#29992;&#25913;&#36827;&#20043;&#38388;&#30340;&#19981;&#36879;&#26126;&#21453;&#39304;&#65292;&#20351;&#24471;&#26368;&#20248;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#21464;&#24471;&#22797;&#26434;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#22312;&#32447;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#31639;&#27861;&#65292;&#21517;&#20026;BARA&#65288;\underline{B}udget \underline{A}llocation for \underline{R}everse \underline{A}uction&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a prospective distributed machine learning framework that can preserve data privacy. In particular, cross-silo FL can complete model training by making isolated data islands of different organizations collaborate with a parameter server (PS) via exchanging model parameters for multiple communication rounds. In cross-silo FL, an incentive mechanism is indispensable for motivating data owners to contribute their models to FL training. However, how to allocate the reward budget among different rounds is an essential but complicated problem largely overlooked by existing works. The challenge of this problem lies in the opaque feedback between reward budget allocation and model utility improvement of FL, making the optimal reward budget allocation complicated. To address this problem, we design an online reward budget allocation algorithm using Bayesian optimization named BARA (\underline{B}udget \underline{A}llocation for \underline{R}everse \underline{A}uction).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Deep Energy Twin&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#30456;&#32467;&#21512;&#65292;&#20197;&#35782;&#21035;&#24314;&#31569;&#29289;&#33021;&#28304;&#20351;&#29992;&#27169;&#24335;&#24182;&#25552;&#20379;&#20248;&#21270;&#33021;&#28304;&#25928;&#29575;&#30340;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.04498</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#25552;&#39640;&#24314;&#31569;&#29289;&#33021;&#28304;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Leveraging Deep Learning and Digital Twins to Improve Energy Performance of Buildings. (arXiv:2305.04498v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Deep Energy Twin&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#30456;&#32467;&#21512;&#65292;&#20197;&#35782;&#21035;&#24314;&#31569;&#29289;&#33021;&#28304;&#20351;&#29992;&#27169;&#24335;&#24182;&#25552;&#20379;&#20248;&#21270;&#33021;&#28304;&#25928;&#29575;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#29289;&#25968;&#23383;&#21270;&#36716;&#22411;&#31215;&#32047;&#20102;&#22823;&#37327;&#36816;&#33829;&#25968;&#25454;&#65292;&#38656;&#35201;&#26234;&#33021;&#21270;&#35299;&#20915;&#26041;&#26696;&#26469;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#25552;&#39640;&#33021;&#28304;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#8220;Deep Energy Twin&#8221;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#30456;&#32467;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#24314;&#31569;&#29289;&#33021;&#28304;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#35782;&#21035;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;&#37319;&#29992;&#26412;&#20307;&#35770;&#21019;&#24314;&#21442;&#25968;&#25968;&#23383;&#23402;&#29983;&#65292;&#20197;&#25552;&#20379;&#24314;&#31569;&#29289;&#20013;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#25968;&#25454;&#26684;&#24335;&#30340;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#21019;&#24314;&#30340;&#25968;&#23383;&#23402;&#29983;&#21644;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#27169;&#24335;&#24182;&#20026;&#33021;&#28304;&#20248;&#21270;&#25552;&#20379;&#27934;&#35265;&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#26412;&#30740;&#31350;&#22312;&#29790;&#20856;&#35834;&#23572;&#32943;&#24179;&#30340;&#19968;&#24231;&#20844;&#20849;&#21382;&#21490;&#24314;&#31569;&#20013;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#24314;&#31569;&#29289;&#33021;&#28304;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital transformation in buildings accumulates massive operational data, which calls for smart solutions to utilize these data to improve energy performance. This study has proposed a solution, namely Deep Energy Twin, for integrating deep learning and digital twins to better understand building energy use and identify the potential for improving energy efficiency. Ontology was adopted to create parametric digital twins to provide consistency of data format across different systems in a building. Based on created digital twins and collected data, deep learning methods were used for performing data analytics to identify patterns and provide insights for energy optimization. As a demonstration, a case study was conducted in a public historic building in Norrk\"oping, Sweden, to compare the performance of state-of-the-art deep learning architectures in building energy forecasting.
&lt;/p&gt;</description></item><item><title>PiML&#24037;&#20855;&#31665;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;Python&#24037;&#20855;&#31665;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#21644;&#35786;&#26029;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#26085;&#30410;&#22686;&#38271;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#36824;&#25903;&#25345;&#19982;MLOps&#24179;&#21488;&#30340;&#38598;&#25104;&#21644;&#36136;&#37327;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.04214</link><description>&lt;p&gt;
PiML&#24037;&#20855;&#31665;&#65306;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
PiML Toolbox for Interpretable Machine Learning Model Development and Validation. (arXiv:2305.04214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04214
&lt;/p&gt;
&lt;p&gt;
PiML&#24037;&#20855;&#31665;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;Python&#24037;&#20855;&#31665;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#21644;&#35786;&#26029;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#26085;&#30410;&#22686;&#38271;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#36824;&#25903;&#25345;&#19982;MLOps&#24179;&#21488;&#30340;&#38598;&#25104;&#21644;&#36136;&#37327;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PiML&#26159;&#19968;&#20010;&#32508;&#21512;&#19988;&#24320;&#25918;&#30340;Python&#24037;&#20855;&#31665;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#27169;&#22411;&#35786;&#26029;&#12290;&#23427;&#35774;&#35745;&#20102;&#20302;&#20195;&#30721;&#21644;&#39640;&#20195;&#30721;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#65292;&#21253;&#25324;&#25968;&#25454;&#31649;&#36947;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#27169;&#22411;&#35299;&#37322;&#21644;&#35828;&#26126;&#20197;&#21450;&#27169;&#22411;&#35786;&#26029;&#21644;&#27604;&#36739;&#12290;&#35813;&#24037;&#20855;&#31665;&#25903;&#25345;&#26085;&#30410;&#22686;&#38271;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65288;&#20363;&#22914;GAM&#12289;GAMI-Net&#12289;XGB2&#65289;&#65292;&#20855;&#26377;&#26412;&#22320;&#21644;/&#25110;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;&#23427;&#36824;&#25903;&#25345;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65288;&#20363;&#22914;PFI&#12289;PDP&#12289;LIME&#12289;SHAP&#65289;&#21644;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#26080;&#20851;&#35786;&#26029;&#22871;&#20214;&#65288;&#20363;&#22914;&#24369;&#28857;&#12289;&#19981;&#30830;&#23450;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#65289;&#12290;&#36890;&#36807;&#28789;&#27963;&#30340;&#39640;&#20195;&#30721; API&#65292;&#23558; PiML &#27169;&#22411;&#21644;&#27979;&#35797;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340; MLOps &#24179;&#21488;&#20197;&#23454;&#29616;&#36136;&#37327;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;PiML &#24037;&#20855;&#31665;&#36824;&#24102;&#26377;&#32508;&#21512;&#30340;&#29992;&#25143;&#25351;&#21335;&#21644;&#23454;&#36341;&#20363;&#23376;&#65292;&#21253;&#25324;&#38134;&#34892;&#19994;&#20013;&#30340;&#27169;&#22411;&#24320;&#21457;&#21644;&#39564;&#35777;&#24212;&#29992;&#12290;&#35813;&#39033;&#30446;&#21487;&#36890;&#36807;arXiv:2305.04214v1[cs.LG]&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
PiML (read $\pi$-ML, /`pai.`em.`el/) is an integrated and open-access Python toolbox for interpretable machine learning model development and model diagnostics. It is designed with machine learning workflows in both low-code and high-code modes, including data pipeline, model training, model interpretation and explanation, and model diagnostics and comparison. The toolbox supports a growing list of interpretable models (e.g. GAM, GAMI-Net, XGB2) with inherent local and/or global interpretability. It also supports model-agnostic explainability tools (e.g. PFI, PDP, LIME, SHAP) and a powerful suite of model-agnostic diagnostics (e.g. weakness, uncertainty, robustness, fairness). Integration of PiML models and tests to existing MLOps platforms for quality assurance are enabled by flexible high-code APIs. Furthermore, PiML toolbox comes with a comprehensive user guide and hands-on examples, including the applications for model development and validation in banking. The project is available
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;FedNC&#65292;&#19968;&#20010;&#32852;&#21512;&#23398;&#20064;&#30340;&#36890;&#20449;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#32593;&#32476;&#32534;&#30721;&#25216;&#26415;&#65292;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#30340;&#38544;&#31169;&#12289;&#21534;&#21520;&#37327;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03292</link><description>&lt;p&gt;
FedNC&#65306;&#22522;&#20110;&#32593;&#32476;&#32534;&#30721;&#21551;&#21457;&#30340;&#23433;&#20840;&#39640;&#25928;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedNC: A Secure and Efficient Federated Learning Method Inspired by Network Coding. (arXiv:2305.03292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;FedNC&#65292;&#19968;&#20010;&#32852;&#21512;&#23398;&#20064;&#30340;&#36890;&#20449;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#32593;&#32476;&#32534;&#30721;&#25216;&#26415;&#65292;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#30340;&#38544;&#31169;&#12289;&#21534;&#21520;&#37327;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26426;&#21046;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#38544;&#31169;&#27844;&#28431;&#21644;&#31995;&#32479;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32593;&#32476;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#37325;&#26032;&#26500;&#24605;&#20102;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#21407;&#21019;&#30340;&#32852;&#21512;&#23398;&#20064;&#36890;&#20449;&#26694;&#26550;FedNC&#65292;&#35813;&#26694;&#26550;&#21463;&#21040;&#32593;&#32476;&#32534;&#30721;&#30340;&#21551;&#21457;&#12290; FedNC&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#23545;&#21407;&#22987;&#25968;&#25454;&#21253;&#36827;&#34892;&#38543;&#26426;&#32447;&#24615;&#32452;&#21512;&#65292;&#23558;&#26412;&#22320;&#27169;&#22411;&#30340;&#20449;&#24687;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#28982;&#21518;&#20877;&#19978;&#20256;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#20351;FL&#31995;&#32479;&#26356;&#21152;&#23433;&#20840;&#65292;&#21534;&#21520;&#37327;&#26356;&#39640;&#65292;&#40065;&#26834;&#24615;&#26356;&#22909;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;NC&#24341;&#20837;FL&#30340;&#26694;&#26550;&#12290;&#38543;&#30528;FL&#22312;&#23454;&#38469;&#32593;&#32476;&#26694;&#26550;&#20013;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#21487;&#20197;&#22522;&#20110;FedNC&#36827;&#19968;&#27493;&#35774;&#35745;&#26356;&#22810;&#30340;&#24212;&#29992;&#21644;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a promising distributed learning mechanism which still faces two major challenges, namely privacy breaches and system efficiency. In this work, we reconceptualize the FL system from the perspective of network information theory, and formulate an original FL communication framework, FedNC, which is inspired by Network Coding (NC). The main idea of FedNC is mixing the information of the local models by making random linear combinations of the original packets, before uploading for further aggregation. Due to the benefits of the coding scheme, both theoretical and experimental analysis indicate that FedNC improves the performance of traditional FL in several important ways, including security, throughput, and robustness. To the best of our knowledge, this is the first framework where NC is introduced in FL. As FL continues to evolve within practical network frameworks, more applications and variants can be further designed based on FedNC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#28216;&#25103;&#29702;&#35770;&#39537;&#21160;&#30340;&#24402;&#22240;&#21644;k&#38454;&#20132;&#20114;&#26041;&#27861;&#65292;&#36890;&#36807;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#36830;&#32493;&#36755;&#20837;&#35774;&#32622;&#20013;&#24471;&#21040;&#21807;&#19968;&#20840;&#38754;&#30340;&#29305;&#24449;&#20132;&#20114;&#35299;&#37322;&#65292;&#21363;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.03100</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21327;&#21516;&#21151;&#33021;&#65306;&#32479;&#19968;&#21338;&#24328;&#35770;&#20132;&#20114;&#26041;&#27861;&#26469;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributing Synergy Functions: Unifying Game-Theoretic Interaction Methods for Machine-Learning Explainability. (arXiv:2305.03100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#28216;&#25103;&#29702;&#35770;&#39537;&#21160;&#30340;&#24402;&#22240;&#21644;k&#38454;&#20132;&#20114;&#26041;&#27861;&#65292;&#36890;&#36807;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#36830;&#32493;&#36755;&#20837;&#35774;&#32622;&#20013;&#24471;&#21040;&#21807;&#19968;&#20840;&#38754;&#30340;&#29305;&#24449;&#20132;&#20114;&#35299;&#37322;&#65292;&#21363;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#39046;&#22495;&#65292;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#36825;&#20123;&#39640;&#24615;&#33021;&#27169;&#22411;&#36890;&#24120;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#12290;&#35299;&#37322;&#27492;&#31867;&#27169;&#22411;&#23558;&#25552;&#39640;AI&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#20449;&#20219;&#65292;&#24182;&#19988;&#23545;&#20110;&#29702;&#35299;&#20854;&#20182;&#23454;&#38469;&#38656;&#27714;&#65288;&#22914;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#65289;&#26159;&#24517;&#35201;&#30340;&#12290;&#22686;&#24378;&#27169;&#22411;&#36879;&#26126;&#24230;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#37327;&#21270;&#21333;&#20010;&#36755;&#20837;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#36129;&#29486;&#65288;&#31216;&#20026;&#24402;&#22240;&#65289;&#20197;&#21450;&#32676;&#32452;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#24378;&#24230;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#36825;&#20123;&#26041;&#27861;&#23548;&#20837;&#21338;&#24328;&#35770;&#30340;&#27010;&#24565;&#21644;&#32467;&#26524;&#26469;&#20135;&#29983;&#24402;&#22240;&#21644;&#20132;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21338;&#24328;&#35770;&#39537;&#21160;&#30340;&#24402;&#22240;&#21644;k&#38454;&#20132;&#20114;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36830;&#32493;&#36755;&#20837;&#35774;&#32622;&#20013;&#65292;&#20551;&#35774;&#36866;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#29305;&#24449;&#20043;&#38388;&#20132;&#20114;&#30340;&#21807;&#19968;&#20840;&#38754;&#35828;&#26126;&#65292;&#21363;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has revolutionized many areas of machine learning, from computer vision to natural language processing, but these high-performance models are generally "black box." Explaining such models would improve transparency and trust in AI-powered decision making and is necessary for understanding other practical needs such as robustness and fairness. A popular means of enhancing model transparency is to quantify how individual inputs contribute to model outputs (called attributions) and the magnitude of interactions between groups of inputs. A growing number of these methods import concepts and results from game theory to produce attributions and interactions. This work presents a unifying framework for game-theory-inspired attribution and $k^\text{th}$-order interaction methods. We show that, given modest assumptions, a unique full account of interactions between features, called synergies, is possible in the continuous input setting. We identify how various methods are characte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#24102;&#26377;&#38543;&#26426;&#32422;&#26463;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#21462;&#24471;&#20122;&#32447;&#24615;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01333</link><description>&lt;p&gt;
&#26080;&#25237;&#24433;&#22312;&#32447;&#20984;&#20248;&#21270;&#19982;&#38543;&#26426;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Projection-Free Online Convex Optimization with Stochastic Constraints. (arXiv:2305.01333v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01333
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#24102;&#26377;&#38543;&#26426;&#32422;&#26463;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#21462;&#24471;&#20122;&#32447;&#24615;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#38543;&#26426;&#32422;&#26463;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#32447;&#21407;&#22987;-&#23545;&#20598;&#26080;&#25237;&#24433;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#37319;&#29992;&#20219;&#20309;&#29992;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#26080;&#38271;&#26399;&#32422;&#26463;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#12290;&#20351;&#29992;&#35813;&#27169;&#26495;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#20122;&#32447;&#24615;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25439;&#22833;&#20989;&#25968;&#21644;&#32422;&#26463;&#20989;&#25968;&#37117;&#20809;&#28369;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#26465;&#20214;&#26799;&#24230;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#36951;&#25022;&#21644; $O(T^{3/4})$&#30340;&#32422;&#26463;&#36829;&#35268;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25439;&#22833;&#20989;&#25968;&#21644;&#32422;&#26463;&#20989;&#25968;&#37117;&#26159;&#38543;&#26426;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#19982;&#20851;&#32852;&#30340;&#31163;&#32447;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#23384;&#22312;&#24378;&#23545;&#20598;&#24615;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32422;&#26463;&#36829;&#35268;&#21487;&#20197;&#34987;&#20943;&#23569;&#21040;&#19982;&#36951;&#25022;&#26377;&#30456;&#21516;&#30340;&#28176;&#36817;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops projection-free algorithms for online convex optimization with stochastic constraints. We design an online primal-dual projection-free framework that can take any projection-free algorithms developed for online convex optimization with no long-term constraint. With this general template, we deduce sublinear regret and constraint violation bounds for various settings. Moreover, for the case where the loss and constraint functions are smooth, we develop a primal-dual conditional gradient method that achieves $O(\sqrt{T})$ regret and $O(T^{3/4})$ constraint violations. Furthermore, for the setting where the loss and constraint functions are stochastic and strong duality holds for the associated offline stochastic optimization problem, we prove that the constraint violation can be reduced to have the same asymptotic growth as the regret.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>LLT&#26159;&#19968;&#20010;R&#21253;&#65292;&#29992;&#20110;&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#21464;&#25442;&#65292;&#21487;&#20197;&#24110;&#21161;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.14211</link><description>&lt;p&gt;
LLT&#65306;&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#21464;&#25442;&#30340;R&#21253;
&lt;/p&gt;
&lt;p&gt;
LLT: An R package for Linear Law-based Feature Space Transformation. (arXiv:2304.14211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14211
&lt;/p&gt;
&lt;p&gt;
LLT&#26159;&#19968;&#20010;R&#21253;&#65292;&#29992;&#20110;&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#21464;&#25442;&#65292;&#21487;&#20197;&#24110;&#21161;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#36716;&#25442;(LLT )&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#24110;&#21161;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#12290;LLT R&#21253;&#20197;&#28789;&#27963;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#35813;&#31639;&#27861;&#12290;&#35813;&#21253;&#23558;&#23454;&#20363;&#20998;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#65292;&#24182;&#21033;&#29992;&#26102;&#24310;&#23884;&#20837;&#21644;&#35889;&#20998;&#35299;&#25216;&#26415;&#65292;&#35782;&#21035;&#35757;&#32451;&#38598;&#20013;&#27599;&#20010;&#36755;&#20837;&#24207;&#21015;(&#21021;&#22987;&#29305;&#24449;)&#30340;&#25511;&#21046;&#27169;&#24335;(&#31216;&#20026;&#32447;&#24615;&#23450;&#24459;)&#12290;&#26368;&#21518;&#65292;&#23427;&#24212;&#29992;&#35757;&#32451;&#38598;&#30340;&#32447;&#24615;&#23450;&#24459;&#26469;&#36716;&#25442;&#27979;&#35797;&#38598;&#30340;&#21021;&#22987;&#29305;&#24449;&#12290;trainTest&#12289;trainLaw&#21644;testTrans&#19977;&#20010;&#21333;&#29420;&#30340;&#20989;&#25968;&#26469;&#25191;&#34892;&#36825;&#20123;&#27493;&#39588;&#65292;&#23427;&#20204;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#25968;&#25454;&#32467;&#26500;;&#28982;&#32780;&#65292;&#20026;&#20102;&#24555;&#36895;&#35745;&#31639;&#65292;&#23427;&#20204;&#21482;&#20351;&#29992;&#20869;&#32622;&#20989;&#25968;&#12290;LLT R&#21253;&#21644;&#36866;&#24403;&#25968;&#25454;&#32467;&#26500;&#30340;&#31034;&#20363;&#25968;&#25454;&#38598;&#22312;GitHub&#19978;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of the linear law-based feature space transformation (LLT) algorithm is to assist with the classification of univariate and multivariate time series. The presented R package, called LLT, implements this algorithm in a flexible yet user-friendly way. This package first splits the instances into training and test sets. It then utilizes time-delay embedding and spectral decomposition techniques to identify the governing patterns (called linear laws) of each input sequence (initial feature) within the training set. Finally, it applies the linear laws of the training set to transform the initial features of the test set. These steps are performed by three separate functions called trainTest, trainLaw, and testTrans. Their application requires a predefined data structure; however, for fast calculation, they use only built-in functions. The LLT R package and a sample dataset with the appropriate data structure are publicly available on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#30340;&#20013;&#24230;&#20998;&#24067;&#25506;&#32034;&#65288;MODE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20849;&#20139;&#30456;&#21516;&#35821;&#20041;&#22240;&#32032;&#30340;&#19981;&#30830;&#23450;&#24615;&#23376;&#38598;&#20013;&#25506;&#32034;&#39046;&#22495;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13976</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#20013;&#24230;&#20998;&#24067;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Moderately Distributional Exploration for Domain Generalization. (arXiv:2304.13976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#30340;&#20013;&#24230;&#20998;&#24067;&#25506;&#32034;&#65288;MODE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20849;&#20139;&#30456;&#21516;&#35821;&#20041;&#22240;&#32032;&#30340;&#19981;&#30830;&#23450;&#24615;&#23376;&#38598;&#20013;&#25506;&#32034;&#39046;&#22495;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#26088;&#22312;&#35299;&#20915;&#35757;&#32451;&#39046;&#22495;&#19982;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#29983;&#25104;&#26032;&#30340;&#39046;&#22495;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#28982;&#32780;&#20854;&#24615;&#33021;&#22686;&#30410;&#21462;&#20915;&#20110;&#29983;&#25104;&#30340;&#39046;&#22495;&#19982;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26377;&#26395;&#36890;&#36807;&#22312;&#19981;&#30830;&#23450;&#24615;&#38598;&#20013;&#25506;&#32034;&#39046;&#22495;&#26469;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19981;&#30830;&#23450;&#24615;&#38598;&#21487;&#33021;&#38750;&#24120;&#24222;&#22823;&#65292;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#20250;&#23548;&#33268;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#65292;&#22240;&#20026;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21487;&#33021;&#20250;&#24341;&#20837;&#21253;&#21547;&#19982;&#35757;&#32451;&#39046;&#22495;&#35821;&#20041;&#19981;&#21516;&#30340;&#22240;&#32032;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#20013;&#24230;&#20998;&#24067;&#25506;&#32034;&#65288;MODE&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MODE&#22312;&#19968;&#20010;&#19982;&#35757;&#32451;&#39046;&#22495;&#20849;&#20139;&#30456;&#21516;&#35821;&#20041;&#22240;&#32032;&#30340;&#19981;&#30830;&#23450;&#24615;$\textit{&#23376;&#38598;}$&#20013;&#36827;&#34892;&#20998;&#24067;&#25506;&#32034;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;MODE&#21487;&#20197;&#20026;&#27169;&#22411;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#39046;&#22495;&#27867;&#21270;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) aims to tackle the distribution shift between training domains and unknown target domains. Generating new domains is one of the most effective approaches, yet its performance gain depends on the distribution discrepancy between the generated and target domains. Distributionally robust optimization is promising to tackle distribution discrepancy by exploring domains in an uncertainty set. However, the uncertainty set may be overwhelmingly large, leading to low-confidence prediction in DG. It is because a large uncertainty set could introduce domains containing semantically different factors from training domains. To address this issue, we propose to perform a $\textbf{mo}$derately $\textbf{d}$istributional $\textbf{e}$xploration (MODE) for domain generalization. Specifically, MODE performs distribution exploration in an uncertainty $\textit{subset}$ that shares the same semantic factors with the training domains. We show that MODE can endow models with provabl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#26469;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#22810;&#26679;&#24615;&#26435;&#37325;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#38656;&#35201;&#22810;&#26679;&#21270;&#36755;&#20986;&#30340;&#21019;&#24847;&#24212;&#29992;&#65292;&#24182;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11961</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#24615;&#26435;&#37325;&#23454;&#29616;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#24335;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Towards Mode Balancing of Generative Models via Diversity Weights. (arXiv:2304.11961v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#26469;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#22810;&#26679;&#24615;&#26435;&#37325;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#38656;&#35201;&#22810;&#26679;&#21270;&#36755;&#20986;&#30340;&#21019;&#24847;&#24212;&#29992;&#65292;&#24182;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25968;&#25454;&#39537;&#21160;&#30340;&#22270;&#20687;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#25903;&#25345;&#21019;&#24847;&#21644;&#33402;&#26415;&#20316;&#21697;&#12290;&#22312;&#24403;&#21069;&#20027;&#23548;&#30340;&#20998;&#24067;&#25311;&#21512;&#33539;&#24335;&#19979;&#65292;&#25968;&#25454;&#38598;&#34987;&#35270;&#20026;&#35201;&#23613;&#21487;&#33021;&#25509;&#36817;&#30340;&#30495;&#23454;&#20540;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21019;&#24847;&#24212;&#29992;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#21019;&#20316;&#32773;&#32463;&#24120;&#21162;&#21147;&#20174;&#32473;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#31215;&#26497;&#20998;&#31163;&#20986;&#26469;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20174;&#32431;&#27169;&#24335;&#35206;&#30422;&#36716;&#21521;&#27169;&#24335;&#24179;&#34913;&#30340;&#24314;&#27169;&#30446;&#26631;&#35843;&#25972;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#36866;&#24212;&#26356;&#39640;&#30340;&#36755;&#20986;&#22810;&#26679;&#24615;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26679;&#24615;&#26435;&#37325;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#26469;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#22810;&#26679;&#24615;&#12289;&#20844;&#24179;&#21644;&#21253;&#23481;&#22312;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#20197;&#21450;&#35745;&#31639;&#26426;&#21019;&#24847;&#20013;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#21487;&#20197;&#22312;https://github.com/&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large data-driven image models are extensively used to support creative and artistic work. Under the currently predominant distribution-fitting paradigm, a dataset is treated as ground truth to be approximated as closely as possible. Yet, many creative applications demand a diverse range of output, and creators often strive to actively diverge from a given data distribution. We argue that an adjustment of modelling objectives, from pure mode coverage towards mode balancing, is necessary to accommodate the goal of higher output diversity. We present diversity weights, a training scheme that increases a model's output diversity by balancing the modes in the training dataset. First experiments in a controlled setting demonstrate the potential of our method. We discuss connections of our approach to diversity, equity, and inclusion in generative machine learning more generally, and computational creativity specifically. An implementation of our algorithm is available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#24179;&#31561;&#21270;&#21463;&#20445;&#25252;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#39044;&#27979;&#20998;&#24067;&#26469;&#23454;&#29616;&#32452;&#20844;&#24179;&#21644;&#35270;&#30456;&#20284;&#20010;&#20307;&#21516;&#31561;&#23545;&#24453;&#23454;&#29616;&#20010;&#20154;&#20844;&#27491;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290; &#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#36830;&#32493;&#27010;&#29575;&#20989;&#25968;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#26469;&#23454;&#29616;&#32452;&#21644;&#20010;&#20154;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.09779</link><description>&lt;p&gt;
&#24179;&#31561;&#25912;&#20851;&#19981;&#31561;&#20110;&#24179;&#31561;&#20010;&#20154;&#20960;&#29575;: &#29992;&#20110;&#32452;&#21644;&#20010;&#20154;&#20844;&#24179;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness. (arXiv:2304.09779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09779
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#24179;&#31561;&#21270;&#21463;&#20445;&#25252;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#39044;&#27979;&#20998;&#24067;&#26469;&#23454;&#29616;&#32452;&#20844;&#24179;&#21644;&#35270;&#30456;&#20284;&#20010;&#20307;&#21516;&#31561;&#23545;&#24453;&#23454;&#29616;&#20010;&#20154;&#20844;&#27491;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290; &#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#36830;&#32493;&#27010;&#29575;&#20989;&#25968;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#26469;&#23454;&#29616;&#32452;&#21644;&#20010;&#20154;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#20844;&#24179;&#36890;&#36807;&#24179;&#34913;&#21463;&#20445;&#25252;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#39044;&#27979;&#20998;&#24067;&#26469;&#23454;&#29616;&#65307;&#20010;&#20154;&#20844;&#24179;&#35201;&#27714;&#23558;&#30456;&#20284;&#30340;&#20010;&#20307;&#35270;&#20026;&#21516;&#31561;&#23545;&#24453;&#12290;&#28982;&#32780;&#65292;&#24403;&#35780;&#20998;&#27169;&#22411;&#36890;&#36807;&#19981;&#36830;&#32493;&#30340;&#27010;&#29575;&#20989;&#25968;&#36827;&#34892;&#26657;&#20934;&#26102;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#26159;&#19981;&#20860;&#23481;&#30340;&#65292;&#20854;&#20013;&#20010;&#20307;&#21487;&#33021;&#20250;&#38543;&#26426;&#20998;&#37197;&#30001;&#22266;&#23450;&#27010;&#29575;&#30830;&#23450;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#20250;&#20351;&#26469;&#33258;&#21516;&#19968;&#21463;&#20445;&#25252;&#32452;&#30340;&#20004;&#20010;&#30456;&#20284;&#20010;&#20307;&#30340;&#20998;&#31867;&#20960;&#29575;&#24046;&#21035;&#26126;&#26174;&#19981;&#21516;&#65292;&#36825;&#26159;&#20010;&#20154;&#20844;&#24179;&#30340;&#26126;&#26174;&#36829;&#21453;&#12290;&#20026;&#27599;&#20010;&#21463;&#20445;&#25252;&#23376;&#32676;&#20307;&#20998;&#37197;&#21807;&#19968;&#30340;&#20960;&#29575;&#20063;&#21487;&#33021;&#20250;&#38459;&#27490;&#19968;&#20010;&#23376;&#32676;&#20307;&#30340;&#25104;&#21592;&#25509;&#21040;&#21478;&#19968;&#20010;&#23376;&#32676;&#20307;&#26377;&#27491;&#38754;&#32467;&#26524;&#30340;&#24179;&#31561;&#26426;&#20250;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#21478;&#19968;&#31181;&#31216;&#20026;&#20010;&#20154;&#20960;&#29575;&#30340;&#19981;&#20844;&#24179;&#31867;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#21463;&#32676;&#20307;&#38408;&#20540;&#32422;&#26463;&#30340;&#36830;&#32493;&#27010;&#29575;&#20989;&#25968;&#26469;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20445;&#30041;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group fairness is achieved by equalising prediction distributions between protected sub-populations; individual fairness requires treating similar individuals alike. These two objectives, however, are incompatible when a scoring model is calibrated through discontinuous probability functions, where individuals can be randomly assigned an outcome determined by a fixed probability. This procedure may provide two similar individuals from the same protected group with classification odds that are disparately different -- a clear violation of individual fairness. Assigning unique odds to each protected sub-population may also prevent members of one sub-population from ever receiving equal chances of a positive outcome to another, which we argue is another type of unfairness called individual odds. We reconcile all this by constructing continuous probability functions between group thresholds that are constrained by their Lipschitz constant. Our solution preserves the model's predictive powe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30244;&#20307;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23450;&#20301;&#12289;&#20998;&#21106;&#21644;&#29983;&#23384;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#35270;&#37326;&#30340;PET&#21644;CT&#25195;&#25551;&#19978;&#24110;&#21161;&#23454;&#29616;&#22836;&#39048;&#30284;&#24739;&#32773;&#30340;&#27835;&#30103;&#20915;&#31574;&#21644;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.08106</link><description>&lt;p&gt;
&#38754;&#21521;&#22836;&#39048;&#30284;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#30340;&#30244;&#20307;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Tumour Graph Learning for Survival Prediction in Head &amp; Neck Cancer Patients. (arXiv:2304.08106v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30244;&#20307;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23450;&#20301;&#12289;&#20998;&#21106;&#21644;&#29983;&#23384;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#35270;&#37326;&#30340;PET&#21644;CT&#25195;&#25551;&#19978;&#24110;&#21161;&#23454;&#29616;&#22836;&#39048;&#30284;&#24739;&#32773;&#30340;&#27835;&#30103;&#20915;&#31574;&#21644;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;2020&#24180;&#36817;100&#19975;&#20363;&#22836;&#39048;&#30284;&#30340;&#26032;&#21457;&#30149;&#20363;&#65292;&#22836;&#39048;&#30284;&#26159;&#19968;&#31181;&#33268;&#21629;&#19988;&#24120;&#35265;&#30340;&#24694;&#24615;&#30142;&#30149;&#12290;&#30001;&#20110;&#30149;&#21464;&#20986;&#29616;&#22312;&#22810;&#20010;&#20301;&#32622;&#20197;&#21450;&#24739;&#32773;&#20043;&#38388;&#30340;&#27835;&#30103;&#25928;&#26524;&#24046;&#24322;&#65292;&#27835;&#30103;&#20915;&#31574;&#21644;&#27835;&#30103;&#26412;&#36523;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#33258;&#21160;&#20998;&#21106;&#21644;&#39044;&#21518;&#20272;&#35745;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#30830;&#20445;&#27599;&#20301;&#24739;&#32773;&#24471;&#21040;&#26368;&#26377;&#25928;&#30340;&#27835;&#30103;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#20219;&#24847;&#35270;&#37326;(FOV) PET&#21644;CT&#27880;&#20876;&#25195;&#25551;&#19978;&#25191;&#34892;&#36825;&#20123;&#21151;&#33021;&#65292;&#20174;&#32780;&#20316;&#20026;VokCow&#22242;&#38431;&#21442;&#21152;&#20102;HECKTOR 2022&#25361;&#25112;&#30340;&#20219;&#21153;1&#21644;&#20219;&#21153;2&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#23450;&#20301;&#12289;&#20998;&#21106;&#21644;&#29983;&#23384;&#39044;&#27979;&#12290;&#39318;&#20808;&#65292;&#23558;&#20219;&#24847;FOV&#30340;&#25195;&#25551;&#35009;&#21098;&#21040;&#22836;&#39048;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;U&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#35757;&#32451;&#20998;&#21106;&#24863;&#20852;&#36259;&#21306;&#22495;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#21306;&#22495;&#65292;&#23558;&#21478;&#19968;&#20010;CNN&#19982;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#21462;&#35821;&#20041;&#20998;&#21106;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
With nearly one million new cases diagnosed worldwide in 2020, head \&amp; neck cancer is a deadly and common malignity. There are challenges to decision making and treatment of such cancer, due to lesions in multiple locations and outcome variability between patients. Therefore, automated segmentation and prognosis estimation approaches can help ensure each patient gets the most effective treatment. This paper presents a framework to perform these functions on arbitrary field of view (FoV) PET and CT registered scans, thus approaching tasks 1 and 2 of the HECKTOR 2022 challenge as team \texttt{VokCow}. The method consists of three stages: localization, segmentation and survival prediction. First, the scans with arbitrary FoV are cropped to the head and neck region and a u-shaped convolutional neural network (CNN) is trained to segment the region of interest. Then, using the obtained regions, another CNN is combined with a support vector machine classifier to obtain the semantic segmentati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#8212;&#8212;&#36335;&#24452;&#20462;&#34917;&#65292;&#29992;&#20110;&#34920;&#36798;&#21644;&#23450;&#37327;&#27979;&#35797;&#34920;&#26126;&#34892;&#20026;&#34987;&#23450;&#20301;&#21040;&#19968;&#32452;&#36335;&#24452;&#30340;&#19968;&#31867;&#33258;&#28982;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2304.05969</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#20462;&#34917;&#30340;&#27169;&#22411;&#34892;&#20026;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Localizing Model Behavior with Path Patching. (arXiv:2304.05969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#8212;&#8212;&#36335;&#24452;&#20462;&#34917;&#65292;&#29992;&#20110;&#34920;&#36798;&#21644;&#23450;&#37327;&#27979;&#35797;&#34920;&#26126;&#34892;&#20026;&#34987;&#23450;&#20301;&#21040;&#19968;&#32452;&#36335;&#24452;&#30340;&#19968;&#31867;&#33258;&#28982;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#23450;&#20301;&#21040;&#32593;&#32476;&#32452;&#20214;&#30340;&#26576;&#20010;&#23376;&#38598;&#25110;&#32452;&#20214;&#20043;&#38388;&#30340;&#26576;&#20010;&#20132;&#20114;&#30340;&#23376;&#38598;&#26159;&#20998;&#26512;&#32593;&#32476;&#26426;&#21046;&#21644;&#21487;&#33021;&#22833;&#25928;&#27169;&#24335;&#30340;&#33258;&#28982;&#31532;&#19968;&#27493;&#12290;&#29616;&#26377;&#24037;&#20316;&#24120;&#24120;&#26159;&#23450;&#24615;&#19988;&#20020;&#26102;&#30340;&#65292;&#23545;&#20110;&#35780;&#20272;&#23450;&#20301;&#22768;&#26126;&#30340;&#36866;&#24403;&#26041;&#24335;&#27809;&#26377;&#20849;&#35782;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36335;&#24452;&#20462;&#34917;&#25216;&#26415;&#65292;&#29992;&#20110;&#34920;&#36798;&#21644;&#23450;&#37327;&#27979;&#35797;&#34920;&#26126;&#34892;&#20026;&#34987;&#23450;&#20301;&#21040;&#19968;&#32452;&#36335;&#24452;&#30340;&#19968;&#31867;&#33258;&#28982;&#20551;&#35774;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#24863;&#24212;&#22836;&#30340;&#35299;&#37322;&#65292;&#34920;&#24449;&#20102;GPT-2&#30340;&#34892;&#20026;&#65292;&#24182;&#24320;&#28304;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#36816;&#34892;&#31867;&#20284;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes. Existing work is often qualitative and ad-hoc, and there is no consensus on the appropriate way to evaluate localization claims. We introduce path patching, a technique for expressing and quantitatively testing a natural class of hypotheses expressing that behaviors are localized to a set of paths. We refine an explanation of induction heads, characterize a behavior of GPT-2, and open source a framework for efficiently running similar experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#38750;&#23450;&#21521;&#23545;&#25239;&#28857;&#38750;&#24120;&#21487;&#33021;&#22312;&#37492;&#21035;&#24615;&#27169;&#22411;&#20013;&#38544;&#21547;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25317;&#26377;&#20302;&#33021;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#33021;&#37327;PGD&#30340;&#26032;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.04033</link><description>&lt;p&gt;
&#25506;&#31350;&#40065;&#26834;&#24615;&#27169;&#22411;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Connection between Robust and Generative Models. (arXiv:2304.04033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#19982;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#38750;&#23450;&#21521;&#23545;&#25239;&#28857;&#38750;&#24120;&#21487;&#33021;&#22312;&#37492;&#21035;&#24615;&#27169;&#22411;&#20013;&#38544;&#21547;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25317;&#26377;&#20302;&#33021;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#33021;&#37327;PGD&#30340;&#26032;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#36890;&#36807;&#20998;&#35299;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25506;&#31350;&#40065;&#26834;&#24615;&#21028;&#21035;&#20998;&#31867;&#22120;&#19982;&#33021;&#37327;&#22522;&#27169;&#22411;(EBM)&#24418;&#24335;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#24120;&#35265;&#30340;&#20551;&#35774;&#26159;&#23545;&#25239;&#28857;&#31163;&#24320;&#20102;&#36755;&#20837;&#25968;&#25454;&#30340;&#27969;&#24418;&#65292;&#20294;&#26159;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#38750;&#23450;&#21521;&#23545;&#25239;&#28857;&#38750;&#24120;&#21487;&#33021;&#22312;&#37492;&#21035;&#24615;&#27169;&#22411;&#20013;&#38544;&#21547;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25317;&#26377;&#20302;&#33021;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#35777;&#25454;:&#38750;&#23450;&#21521;&#25915;&#20987;&#30340;&#27010;&#29575;&#29978;&#33267;&#27604;&#33258;&#28982;&#25968;&#25454;&#36824;&#35201;&#39640;&#65292;&#24182;&#19988;&#38543;&#30528;&#25915;&#20987;&#24378;&#24230;&#30340;&#22686;&#21152;&#65292;&#20854;&#27010;&#29575;&#20063;&#20250;&#22686;&#21152;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#22320;&#26816;&#27979;&#23427;&#20204;&#24182;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;&#39640;&#33021;&#37327;PGD&#30340;&#26032;&#25915;&#20987;&#65292;&#33021;&#22815;&#27450;&#39575;&#20998;&#31867;&#22120;&#20294;&#20855;&#26377;&#19982;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We offer a study that connects robust discriminative classifiers trained with adversarial training (AT) with generative modeling in the form of Energy-based Models (EBM). We do so by decomposing the loss of a discriminative classifier and showing that the discriminative model is also aware of the input data density. Though a common assumption is that adversarial points leave the manifold of the input data, our study finds out that, surprisingly, untargeted adversarial points in the input space are very likely under the generative model hidden inside the discriminative classifier -- have low energy in the EBM. We present two evidence: untargeted attacks are even more likely than the natural data and their likelihood increases as the attack strength increases. This allows us to easily detect them and craft a novel attack called High-Energy PGD that fools the classifier yet has energy similar to the data set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#23545;&#39640;&#32500;&#36229;&#32479;&#35745;&#29305;&#24449;&#19979;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#27491;&#21017;&#21270;&#21644;&#20998;&#24067;&#23610;&#24230;&#21442;&#25968;&#23545;&#20998;&#31867;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.02912</link><description>&lt;p&gt;
&#39640;&#32500;&#36229;&#32479;&#35745;&#29305;&#24449;&#30340;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Classification of Superstatistical Features in High Dimensions. (arXiv:2304.02912v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#23545;&#39640;&#32500;&#36229;&#32479;&#35745;&#29305;&#24449;&#19979;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#27491;&#21017;&#21270;&#21644;&#20998;&#24067;&#23610;&#24230;&#21442;&#25968;&#23545;&#20998;&#31867;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#23545;&#20855;&#26377;&#19968;&#33324;&#20013;&#24515;&#28857;&#30340;&#20004;&#20010;&#25968;&#25454;&#20113;&#30340;&#28151;&#21512;&#36827;&#34892;&#20102;&#23398;&#20064;&#65292;&#20551;&#35774;&#20855;&#26377;&#36890;&#29992;&#30340;&#20984;&#25439;&#22833;&#21644;&#20984;&#27491;&#21017;&#21270;&#12290;&#27599;&#20010;&#25968;&#25454;&#20113;&#26159;&#36890;&#36807;&#20174;&#21487;&#33021;&#26159;&#19981;&#21487;&#25968;&#30340;&#39640;&#26031;&#20998;&#24067;&#21472;&#21152;&#20013;&#36827;&#34892;&#37319;&#26679;&#26469;&#33719;&#24471;&#30340;&#65292;&#20854;&#26041;&#24046;&#20855;&#26377;&#36890;&#29992;&#30340;&#27010;&#29575;&#23494;&#24230;$\varrho$&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#21253;&#25324;&#27809;&#26377;&#21327;&#26041;&#24046;&#30340;&#24130;&#24459;&#23614;&#37096;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25152;&#24471;&#20272;&#35745;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20998;&#26512;&#20102;&#27491;&#21017;&#21270;&#30340;&#20316;&#29992;&#20197;&#21450;&#20998;&#31163;&#36716;&#25442;&#19982;&#20998;&#24067;&#23610;&#24230;&#21442;&#25968;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterise the learning of a mixture of two clouds of data points with generic centroids via empirical risk minimisation in the high dimensional regime, under the assumptions of generic convex loss and convex regularisation. Each cloud of data points is obtained by sampling from a possibly uncountable superposition of Gaussian distributions, whose variance has a generic probability density $\varrho$. Our analysis covers therefore a large family of data distributions, including the case of power-law-tailed distributions with no covariance. We study the generalisation performance of the obtained estimator, we analyse the role of regularisation, and the dependence of the separability transition on the distribution scale parameters.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SemiMemes&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;Memes&#30340;&#20998;&#26512;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.00020</link><description>&lt;p&gt;
SemiMemes&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;Memes&#20998;&#26512;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SemiMemes&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;Memes&#30340;&#20998;&#26512;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;Memes&#30340;&#26222;&#21450;&#24615;&#24341;&#21457;&#20102;&#20998;&#26512;&#20854;&#38544;&#21547;&#21547;&#20041;&#12289;&#23457;&#26597;&#26377;&#23475;&#20869;&#23481;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;Meme&#23457;&#26597;&#31995;&#32479;&#38656;&#35201;&#21322;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21033;&#29992;&#20114;&#32852;&#32593;&#19978;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;Memes&#65292;&#24182;&#20351;&#27880;&#37322;&#36807;&#31243;&#21464;&#24471;&#26356;&#31616;&#21333;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#22240;&#20026;Memes&#30340;&#21547;&#20041;&#36890;&#24120;&#26469;&#33258;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#21363;&#22810;&#23186;&#20307;&#33258;&#21160;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;Memes&#25968;&#25454;&#38598;&#19978;&#65292;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#27169;&#22411;&#12290;&#20511;&#37492;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;SemiMemes&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#20998;&#31867;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The prevalence of memes on social media has created the need to sentiment analyze their underlying meanings for censoring harmful content. Meme censoring systems by machine learning raise the need for a semi-supervised learning solution to take advantage of the large number of unlabeled memes available on the internet and make the annotation process less challenging. Moreover, the approach needs to utilize multimodal data as memes' meanings usually come from both images and texts. This research proposes a multimodal semi-supervised learning approach that outperforms other multimodal semi-supervised learning and supervised learning state-of-the-art models on two datasets, the Multimedia Automatic Misogyny Identification and Hateful Memes dataset. Building on the insights gained from Contrastive Language-Image Pre-training, which is an effective multimodal learning technique, this research introduces SemiMemes, a novel training method that combines auto-encoder and classification task to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#29983;&#25104;4K&#25513;&#33180;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2303.10096</link><description>&lt;p&gt;
&#39640;&#25928;&#29983;&#25104;4K&#25513;&#33180;&#29992;&#20110;&#40784;&#27425;&#25193;&#25955;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Efficient Neural Generation of 4K Masks for Homogeneous Diffusion Inpainting. (arXiv:2303.10096v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10096
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#29983;&#25104;4K&#25513;&#33180;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20248;&#36873;&#25968;&#25454;&#65292;&#40784;&#27425;&#25193;&#25955;&#20462;&#22797;&#21487;&#20197;&#37325;&#24314;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#25968;&#25454;&#22270;&#20687;&#12290;&#34429;&#28982;&#22823;&#23567;&#20026;3840 x 2160&#30340;4K&#24425;&#33394;&#22270;&#20687;&#21487;&#20197;&#23454;&#26102;&#20462;&#22797;&#65292;&#20294;&#20248;&#21270;&#24050;&#30693;&#25968;&#25454;&#20197;&#29992;&#20110;&#22270;&#20687;&#21387;&#32553;&#31561;&#24212;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#31574;&#30053;&#21487;&#33021;&#38656;&#35201;&#25968;&#22825;&#25165;&#33021;&#22788;&#29702;&#21333;&#20010;4K&#22270;&#20687;&#12290;&#26368;&#36817;&#65292;&#31532;&#19968;&#20010;&#38024;&#23545;&#36825;&#20010;&#25152;&#35859;&#30340;&#25513;&#33180;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#20462;&#22797;&#20195;&#29702;&#24110;&#21161;&#35757;&#32451;&#25513;&#33180;&#29983;&#25104;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#39640;&#36895;&#24230;&#21644;&#33391;&#22909;&#36136;&#37327;&#30340;&#23567;&#22411;&#22270;&#20687;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25513;&#33180;&#32593;&#32476;&#21482;&#33021;&#22312;&#23427;&#20204;&#35757;&#32451;&#30340;&#20998;&#36776;&#29575;&#21644;&#25513;&#33180;&#23494;&#24230;&#19979;&#36755;&#20986;&#25513;&#33180;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#31070;&#32463;-&#26174;&#24335;&#30340;&#33258;&#31895;&#21040;&#32454;&#30340;&#31574;&#30053;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25513;&#33180;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#23558;&#25968;&#20540;&#20462;&#22797;&#27714;&#35299;&#22120;&#32435;&#20837;&#32593;&#32476;&#20013;&#26469;&#25913;&#36827;&#25513;&#33180;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20351;&#24471;&#33021;&#22815;&#22312;&#30701;&#30701;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;4K&#22270;&#20687;&#25513;&#33180;&#12290;
&lt;/p&gt;
&lt;p&gt;
With well-selected data, homogeneous diffusion inpainting can reconstruct images from sparse data with high quality. While 4K colour images of size 3840 x 2160 can already be inpainted in real time, optimising the known data for applications like image compression remains challenging: Widely used stochastic strategies can take days for a single 4K image. Recently, a first neural approach for this so-called mask optimisation problem offered high speed and good quality for small images. It trains a mask generation network with the help of a neural inpainting surrogate. However, these mask networks can only output masks for the resolution and mask density they were trained for. We solve these problems and enable mask optimisation for high-resolution images through a neuroexplicit coarse-to-fine strategy. Additionally, we improve the training and interpretability of mask networks by including a numerical inpainting solver directly into the network. This allows to generate masks for 4K imag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#26469;&#25193;&#20805;&#25968;&#25454;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2303.06614</link><description>&lt;p&gt;
&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#65306;&#26088;&#22312;&#29992;&#25193;&#20805;&#25968;&#25454;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Synthetic Experience Replay. (arXiv:2303.06614v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#26469;&#25193;&#20805;&#25968;&#25454;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#30340;&#19968;&#20010;&#20851;&#38190;&#20027;&#39064;&#26159;&#65292;&#24403;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#24778;&#24322;&#30340;&#32467;&#26524;&#12290;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#31181;&#33539;&#24335;&#36890;&#24120;&#36890;&#36807;&#32463;&#39564;&#22238;&#25918;&#23454;&#29616;&#65292;&#20854;&#20013;&#36807;&#21435;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#31574;&#30053;&#25110;&#20540;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#25110;&#33258;&#30417;&#30563;&#23398;&#20064;&#19981;&#21516;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24517;&#39035;&#25910;&#38598;&#33258;&#24049;&#30340;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#26159;&#26377;&#38480;&#30340;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#22909;&#22788;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21363;&#20351;&#26159;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#24320;&#22987;&#26102;&#20063;&#21487;&#33021;&#20986;&#29616;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#29983;&#25104;&#24314;&#27169;&#30340;&#24040;&#22823;&#36827;&#27493;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#65288;SynthER&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#26469;&#28789;&#27963;&#22320;&#19978;&#37319;&#26679;&#20195;&#29702;&#25910;&#38598;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SynthER&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#26080;&#35770;&#26159;&#22312;&#24863;&#30693;&#29615;&#22659;&#36824;&#26159;&#22312;&#20687;&#32032;&#29615;&#22659;&#20013;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#25955;&#36947;&#36335;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#24555;&#36895;&#22320;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#20197;&#22270;&#20026;&#27169;&#22411;&#30340;&#21452;&#23618;&#38382;&#39064;&#20915;&#31574;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.06024</link><description>&lt;p&gt;
&#19968;&#31181;&#31163;&#25955;&#36947;&#36335;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#30340;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;-&#20803;&#21551;&#21457;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A hybrid deep-learning-metaheuristic framework for discrete road network design problems. (arXiv:2303.06024v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#25955;&#36947;&#36335;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#24555;&#36895;&#22320;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#20197;&#22270;&#20026;&#27169;&#22411;&#30340;&#21452;&#23618;&#38382;&#39064;&#20915;&#31574;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#23618;&#26550;&#26500;&#30340;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#20803;&#21551;&#21457;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36947;&#36335;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#65288;NDPs&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#35757;&#32451;&#26469;&#36817;&#20284;&#29992;&#25143;&#22343;&#34913;&#65288;UE&#65289;&#20132;&#36890;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#30340;&#25512;&#29702;&#26469;&#35745;&#31639;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#35780;&#20272;&#65292;&#20197;&#36817;&#20284;&#35299;&#20915;NDPs&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;NDP&#21464;&#37327;&#21644;&#19968;&#20010;&#31934;&#30830;&#27714;&#35299;&#22120;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#23569;&#20110;1&#65285;&#30340;&#26102;&#38388;&#20869;&#32473;&#20986;&#20840;&#23616;&#26368;&#20248;&#32467;&#26524;&#30340;5&#65285;&#24038;&#21491;&#30340;&#38388;&#38553;&#20869;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#19987;&#23478;&#31995;&#32479;&#20013;&#20351;&#29992;&#65292;&#29992;&#20110;&#22522;&#30784;&#35774;&#26045;&#35268;&#21010;&#65292;&#20197;&#26234;&#33021;&#22320;&#30830;&#23450;&#26368;&#20339;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#20915;&#31574;&#12290;&#30001;&#20110;&#35813;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#35768;&#22810;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#22270;&#19978;&#21452;&#23618;&#38382;&#39064;&#30340;&#20854;&#20182;&#20915;&#31574;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#35768;&#22810;&#26377;&#36259;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a hybrid deep-learning-metaheuristic framework with a bi-level architecture for road network design problems (NDPs). We train a graph neural network (GNN) to approximate the solution of the user equilibrium (UE) traffic assignment problem, and use inferences made by the trained model to calculate fitness function evaluations of a genetic algorithm (GA) to approximate solutions for NDPs. Using two NDP variants and an exact solver as benchmark, we show that our proposed framework can provide solutions within 5% gap of the global optimum results given less than 1% of the time required for finding the optimal results. Our framework can be utilized within an expert system for infrastructure planning to intelligently determine the best infrastructure management decisions. Given the flexibility of the framework, it can easily be adapted to many other decision problems that can be modeled as bi-level problems on graphs. Moreover, we observe many interesting future direction
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#27973;&#23618;ReLU&#32593;&#32476;&#22312;&#34920;&#36798;&#20855;&#26377;&#38543;&#30528;&#36755;&#20837;&#32500;&#24230;&#22686;&#21152;&#30340;Lipschitz&#21442;&#25968;&#30340;&#20989;&#25968;&#26102;&#20250;&#36973;&#21463;&#32500;&#24230;&#28798;&#38590;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#26356;&#20381;&#36182;&#20110;&#23427;&#20204;&#30340;&#28145;&#24230;&#32780;&#19981;&#26159;&#24635;&#20307;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.03544</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#39033;&#24335;&#36924;&#36817;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expressivity of Shallow and Deep Neural Networks for Polynomial Approximation. (arXiv:2303.03544v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#27973;&#23618;ReLU&#32593;&#32476;&#22312;&#34920;&#36798;&#20855;&#26377;&#38543;&#30528;&#36755;&#20837;&#32500;&#24230;&#22686;&#21152;&#30340;Lipschitz&#21442;&#25968;&#30340;&#20989;&#25968;&#26102;&#20250;&#36973;&#21463;&#32500;&#24230;&#28798;&#38590;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#26356;&#20381;&#36182;&#20110;&#23427;&#20204;&#30340;&#28145;&#24230;&#32780;&#19981;&#26159;&#24635;&#20307;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35201;&#36817;&#20284;&#22810;&#20803;&#21333;&#39033;&#24335;&#25152;&#38656;&#30340;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#19968;&#33324;&#32039;&#33268;&#22495;&#19978;&#24314;&#31435;&#20102;&#20219;&#20309;&#27973;&#23618;&#32593;&#32476;&#36924;&#36817;&#20056;&#31215;&#20989;&#25968;&#30340;&#25351;&#25968;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#20010;&#19979;&#30028;&#19981;&#36866;&#29992;&#20110;&#22312;&#21333;&#20301;&#31435;&#26041;&#20307;&#19978;&#30340;&#35268;&#33539;&#21033;&#26222;&#24076;&#33576;&#21333;&#39033;&#24335;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#34920;&#36798;&#20855;&#26377;&#38543;&#30528;&#36755;&#20837;&#32500;&#24230;&#22686;&#21152;&#30340;Lipschitz&#21442;&#25968;&#30340;&#20989;&#25968;&#26102;&#65292;&#27973;&#23618;ReLU&#32593;&#32476;&#20250;&#36973;&#21463;&#32500;&#24230;&#28798;&#38590;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#26356;&#20381;&#36182;&#20110;&#23427;&#20204;&#30340;&#28145;&#24230;&#32780;&#19981;&#26159;&#24635;&#20307;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the number of neurons required for a Rectified Linear Unit (ReLU) neural network to approximate multivariate monomials. We establish an exponential lower bound on the complexity of any shallow network approximating the product function over a general compact domain. We also demonstrate this lower bound doesn't apply to normalized Lipschitz monomials over the unit cube. These findings suggest that shallow ReLU networks experience the curse of dimensionality when expressing functions with a Lipschitz parameter scaling with the dimension of the input, and that the expressive power of neural networks is more dependent on their depth rather than overall complexity.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#21046;&#36896;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#35828;&#26126;&#20854;&#23545;&#35299;&#20915;&#21046;&#36896;&#19994;&#20013;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#31561;&#38382;&#39064;&#30340;&#24847;&#20041;&#65292;&#32780;&#20854;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#21017;&#21487;&#20197;&#36890;&#36807;&#21327;&#20316;&#38598;&#25104;&#23567;&#22411;&#21378;&#21830;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#22791;&#38388;&#23436;&#25104;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20174;&#32780;&#20351;&#23567;&#22411;&#21046;&#36896;&#21830;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.13514</link><description>&lt;p&gt;
&#21046;&#36896;&#19994;&#20013;&#32852;&#37030;&#23398;&#20064;&#30340;&#24212;&#29992;&#65306;&#35782;&#21035;&#25361;&#25112;&#24182;&#25506;&#32034;&#19982;&#24037;&#19994;4.0&#21644;5.0&#24895;&#26223;&#30340;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Applications of Federated Learning in Manufacturing: Identifying the Challenges and Exploring the Future Directions with Industry 4.0 and 5.0 Visions. (arXiv:2302.13514v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13514
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#21046;&#36896;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#35828;&#26126;&#20854;&#23545;&#35299;&#20915;&#21046;&#36896;&#19994;&#20013;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#31561;&#38382;&#39064;&#30340;&#24847;&#20041;&#65292;&#32780;&#20854;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#21017;&#21487;&#20197;&#36890;&#36807;&#21327;&#20316;&#38598;&#25104;&#23567;&#22411;&#21378;&#21830;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#22791;&#38388;&#23436;&#25104;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20174;&#32780;&#20351;&#23567;&#22411;&#21046;&#36896;&#21830;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21046;&#36896;&#39046;&#22495;&#65292;&#25968;&#25454;&#30340;&#25910;&#38598;&#21644;&#20998;&#26512;&#36890;&#24120;&#26159;&#32791;&#26102;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#25104;&#26412;&#39640;&#26114;&#30340;&#36807;&#31243;&#65292;&#36825;&#20063;&#38459;&#30861;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#29983;&#25104;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#12290;&#23545;&#20110;&#37027;&#20123;&#27809;&#26377;&#22823;&#22411;&#20225;&#19994;&#36164;&#28304;&#30340;&#23567;&#22411;&#21046;&#36896;&#21830;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;&#29289;&#32852;&#32593;(IoT)&#30340;&#24341;&#20837;&#65292;&#25968;&#25454;&#21487;&#20197;&#22312;&#25972;&#20010;&#24037;&#21378;&#20013;&#20197;&#38598;&#25104;&#30340;&#26041;&#24335;&#23454;&#26102;&#25910;&#38598;&#65292;&#21457;&#36865;&#21040;&#20113;&#31471;&#36827;&#34892;&#39640;&#32423;&#20998;&#26512;&#65292;&#24182;&#29992;&#20110;&#25353;&#39034;&#24207;&#26356;&#26032;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23567;&#22411;&#21046;&#36896;&#21830;&#22312;&#21033;&#29992;IoT&#30340;&#22909;&#22788;&#26102;&#38754;&#20020;&#20004;&#20010;&#38556;&#30861;&#65306;&#20182;&#20204;&#21487;&#33021;&#26080;&#27861;&#36127;&#25285;&#25110;&#29983;&#25104;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#25805;&#20316;&#31169;&#26377;&#20113;&#65292;&#24182;&#19988;&#20182;&#20204;&#21487;&#33021;&#19981;&#24895;&#24847;&#23558;&#21407;&#22987;&#25968;&#25454;&#20849;&#20139;&#21040;&#20844;&#20849;&#20113;&#12290;&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#21327;&#20316;&#23398;&#20064;&#30340;&#26032;&#20852;&#27010;&#24565;&#65292;&#21487;&#20197;&#24110;&#21161;&#23567;&#35268;&#27169;&#24037;&#19994;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In manufacturing settings, data collection and analysis are often a time-consuming, challenging, and costly process. It also hinders the use of advanced machine learning and data-driven methods which require a substantial amount of offline training data to generate good results. It is particularly challenging for small manufacturers who do not share the resources of a large enterprise. Recently, with the introduction of the Internet of Things (IoT), data can be collected in an integrated manner across the factory in real-time, sent to the cloud for advanced analysis, and used to update the machine learning model sequentially. Nevertheless, small manufacturers face two obstacles in reaping the benefits of IoT: they may be unable to afford or generate enough data to operate a private cloud, and they may be hesitant to share their raw data with a public cloud. Federated learning (FL) is an emerging concept of collaborative learning that can help small-scale industries address these issues
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;QD&#31639;&#27861;&#8212;&#8212;&#24102;&#26377;&#31574;&#30053;&#26799;&#24230;&#36741;&#21161;&#21644;&#22522;&#20110;&#25317;&#25380;&#30340;&#25506;&#32034;&#30340;&#22810;&#30446;&#26631;MAP-Elites (MOME-PGX)&#26469;&#25193;&#23637;MOME&#20197;&#25552;&#39640;&#20854;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12668</link><description>&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#36741;&#21161;&#21644;&#23494;&#38598;&#25506;&#32034;&#25552;&#39640;&#22810;&#30446;&#26631;&#36136;&#37327;&#22810;&#26679;&#24615;&#30340;&#25968;&#25454;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improving the Data Efficiency of Multi-Objective Quality-Diversity through Gradient Assistance and Crowding Exploration. (arXiv:2302.12668v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;QD&#31639;&#27861;&#8212;&#8212;&#24102;&#26377;&#31574;&#30053;&#26799;&#24230;&#36741;&#21161;&#21644;&#22522;&#20110;&#25317;&#25380;&#30340;&#25506;&#32034;&#30340;&#22810;&#30446;&#26631;MAP-Elites (MOME-PGX)&#26469;&#25193;&#23637;MOME&#20197;&#25552;&#39640;&#20854;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#39640;&#25928;&#36867;&#31163;&#23616;&#37096;&#26368;&#20248;&#21644;&#29983;&#25104;&#24191;&#27867;&#39640;&#25928;&#35299;&#30340;&#33021;&#21147;&#65292;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#31639;&#27861;&#24050;&#25104;&#20026;&#20248;&#21270;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#22810;&#30446;&#26631;MAP-Elites (MOME)&#36890;&#36807;&#22312;Map Elites&#32593;&#26684;&#30340;&#27599;&#20010;&#21333;&#20803;&#26684;&#20013;&#32500;&#25252;&#24085;&#32047;&#25176;&#21069;&#27839;&#23558;QD&#33539;&#20363;&#25193;&#23637;&#21040;&#22810;&#30446;&#26631;&#35774;&#32622;&#12290;MOME&#22312;&#21516;&#26102;&#33719;&#24471;&#22810;&#26679;&#30340;&#35299;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;NSGA-II&#21644;SPEA2&#30456;&#31454;&#20105;&#30340;&#20840;&#23616;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;MOME&#21463;&#38750;&#23450;&#21521;&#36951;&#20256;&#25628;&#32034;&#26426;&#21046;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#22312;&#39640;&#32500;&#25628;&#32034;&#31354;&#38388;&#20013;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QD&#31639;&#27861;&#8212;&#8212;&#24102;&#26377;&#31574;&#30053;&#26799;&#24230;&#36741;&#21161;&#21644;&#22522;&#20110;&#25317;&#25380;&#30340;&#25506;&#32034;&#30340;&#22810;&#30446;&#26631;MAP-Elites (MOME-PGX)&#26469;&#25193;&#23637;MOME&#20197;&#25552;&#39640;&#20854;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;MOME-PGX&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26377;&#25928;&#22320;&#23558;&#35299;&#20915;&#26041;&#26696;&#39537;&#21521;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity (QD) algorithms have recently gained traction as optimisation methods due to their effectiveness at escaping local optima and capability of generating wide-ranging and high-performing solutions. Recently, Multi-Objective MAP-Elites (MOME) extended the QD paradigm to the multi-objective setting by maintaining a Pareto front in each cell of a map-elites grid. MOME achieved a global performance that competed with NSGA-II and SPEA2, two well-established Multi-Objective Evolutionary Algorithms (MOEA), while also acquiring a diverse repertoire of solutions. However, MOME is limited by non-directed genetic search mechanisms which struggle in high-dimensional search spaces. In this work, we present Multi-Objective MAP-Elites with Policy-Gradient Assistance and Crowding-based Exploration (MOME-PGX): a new QD algorithm that extends MOME to improve its data efficiency and performance. MOME-PGX uses gradient-based optimisation to efficiently drive solutions towards higher perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#26435;&#37325;&#24635;&#21644;&#21487;&#33021;&#23567;&#20110;1&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#20316;&#32773;&#25506;&#32034;&#20102;&#26368;&#20248;&#32553;&#23567;&#22240;&#23376;&#22914;&#20309;&#21463;&#21040;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26412;&#22320;&#21608;&#26399;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#23458;&#25143;&#31471;&#30456;&#24178;&#24615;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#23545;&#32858;&#21512;&#26435;&#37325;&#20197;&#25551;&#32472;&#23458;&#25143;&#31471;&#30340;&#37325;&#35201;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FLLAW&#65289;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#23398;&#20064;&#32858;&#21512;&#26435;&#37325;&#21644;&#20840;&#23616;&#26435;&#37325;&#32553;&#23567;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2302.10911</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revisiting Weighted Aggregation in Federated Learning with Neural Networks. (arXiv:2302.10911v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#26435;&#37325;&#24635;&#21644;&#21487;&#33021;&#23567;&#20110;1&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#20316;&#32773;&#25506;&#32034;&#20102;&#26368;&#20248;&#32553;&#23567;&#22240;&#23376;&#22914;&#20309;&#21463;&#21040;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26412;&#22320;&#21608;&#26399;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#23458;&#25143;&#31471;&#30456;&#24178;&#24615;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#23545;&#32858;&#21512;&#26435;&#37325;&#20197;&#25551;&#32472;&#23458;&#25143;&#31471;&#30340;&#37325;&#35201;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FLLAW&#65289;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#23398;&#20064;&#32858;&#21512;&#26435;&#37325;&#21644;&#20840;&#23616;&#26435;&#37325;&#32553;&#23567;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#23545;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#21152;&#26435;&#32858;&#21512;&#20197;&#29983;&#25104;&#20840;&#23616;&#27169;&#22411;&#65292;&#32858;&#21512;&#26435;&#37325;&#34987;&#26631;&#20934;&#21270;&#65288;&#26435;&#37325;&#21644;&#20026;1&#65289;&#24182;&#19982;&#26412;&#22320;&#25968;&#25454;&#22823;&#23567;&#25104;&#27604;&#20363;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21152;&#26435;&#32858;&#21512;&#36807;&#31243;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;FL&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#26435;&#37325;&#24635;&#21644;&#21487;&#33021;&#23567;&#20110;1&#65292;&#23548;&#33268;&#20840;&#23616;&#26435;&#37325;&#32553;&#23567;&#25928;&#24212;&#65288;&#31867;&#20284;&#20110;&#26435;&#37325;&#34928;&#20943;&#65289;&#24182;&#25913;&#21892;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#20248;&#32553;&#23567;&#22240;&#23376;&#22914;&#20309;&#21463;&#21040;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26412;&#22320;&#21608;&#26399;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#23545;&#32858;&#21512;&#26435;&#37325;&#20197;&#25551;&#32472;&#23458;&#25143;&#31471;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#23458;&#25143;&#31471;&#30456;&#24178;&#24615;&#26469;&#30740;&#31350;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#28857;&#12290;&#22312;&#36827;&#20837;&#20020;&#30028;&#28857;&#20043;&#21069;&#65292;&#30456;&#24178;&#24615;&#26356;&#39640;&#30340;&#23458;&#25143;&#31471;&#22312;&#27867;&#21270;&#20013;&#21457;&#25381;&#20102;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22522;&#20110;&#19978;&#36848;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20855;&#26377;&#21487;&#23398;&#20064;&#32858;&#21512;&#26435;&#37325;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FLLAW&#65289;&#65292;&#23427;&#20801;&#35768;&#20840;&#23616;&#26435;&#37325;&#32553;&#23567;&#25928;&#24212;&#21644;&#21487;&#23398;&#20064;&#32858;&#21512;&#26435;&#37325;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FLLAW&#22312;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#22909;&#30340;&#25239;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning (FL), weighted aggregation of local models is conducted to generate a global model, and the aggregation weights are normalized (the sum of weights is 1) and proportional to the local data sizes. In this paper, we revisit the weighted aggregation process and gain new insights into the training dynamics of FL. First, we find that the sum of weights can be smaller than 1, causing global weight shrinking effect (analogous to weight decay) and improving generalization. We explore how the optimal shrinking factor is affected by clients' data heterogeneity and local epochs. Second, we dive into the relative aggregation weights among clients to depict the clients' importance. We develop client coherence to study the learning dynamics and find a critical point that exists. Before entering the critical point, more coherent clients play more essential roles in generalization. Based on the above insights, we propose an effective method for Federated Learning with Learnable Ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#32570;&#22833;&#20540;&#22635;&#34917;&#26041;&#27861;&#65292;&#24182;&#22312;&#20116;&#20010;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#21457;&#29616;&#22635;&#34917;&#30340;&#25928;&#26524;&#20381;&#36182;&#20110;&#25968;&#25454;&#31867;&#22411;&#12289;&#21464;&#37327;&#32479;&#35745;&#12289;&#32570;&#22833;&#20540;&#29575;&#21644;&#31867;&#22411;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21516;&#26102;&#36827;&#34892;&#20132;&#21449;&#21078;&#38754;&#21644;&#32437;&#21521;&#32570;&#22833;&#20540;&#22635;&#34917;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2302.10902</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#32570;&#22833;&#20540;&#22635;&#34917;&#65306;&#22238;&#39038;&#19982;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking. (arXiv:2302.10902v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#32570;&#22833;&#20540;&#22635;&#34917;&#26041;&#27861;&#65292;&#24182;&#22312;&#20116;&#20010;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#21457;&#29616;&#22635;&#34917;&#30340;&#25928;&#26524;&#20381;&#36182;&#20110;&#25968;&#25454;&#31867;&#22411;&#12289;&#21464;&#37327;&#32479;&#35745;&#12289;&#32570;&#22833;&#20540;&#29575;&#21644;&#31867;&#22411;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21516;&#26102;&#36827;&#34892;&#20132;&#21449;&#21078;&#38754;&#21644;&#32437;&#21521;&#32570;&#22833;&#20540;&#22635;&#34917;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#25968;&#25454;&#20013;&#22635;&#34917;&#32570;&#22833;&#20540;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#21644;&#29983;&#25104;&#21487;&#38752;&#30340;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#38500;&#20102;&#35768;&#22810;&#32479;&#35745;&#26041;&#27861;&#22806;&#65292;&#36817;&#26399;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#22635;&#34917;MTS&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#26041;&#27861;&#30340;&#35780;&#20272;&#20165;&#23616;&#38480;&#20110;&#19968;&#20010;&#25110;&#20004;&#20010;&#25968;&#25454;&#38598;&#12289;&#36739;&#20302;&#30340;&#32570;&#22833;&#29575;&#21644;&#23436;&#20840;&#38543;&#26426;&#30340;&#32570;&#22833;&#20540;&#31867;&#22411;&#12290;&#26412;&#25991;&#23545;&#20116;&#20010;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20845;&#20010;&#25968;&#25454;&#20013;&#24515;&#23454;&#39564;&#65292;&#23545;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#22635;&#34917;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#22635;&#34917;&#26041;&#27861;&#22312;&#25152;&#26377;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#22909;&#12290;&#22635;&#34917;&#25928;&#26524;&#21462;&#20915;&#20110;&#25968;&#25454;&#31867;&#22411;&#12289;&#21333;&#20010;&#21464;&#37327;&#32479;&#35745;&#12289;&#32570;&#22833;&#20540;&#29575;&#21644;&#31867;&#22411;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21516;&#26102;&#36827;&#34892;&#20132;&#21449;&#21078;&#38754;&#65288;&#36328;&#21464;&#37327;&#65289;&#21644;&#32437;&#21521;&#65288;&#36328;&#26102;&#38388;&#65289;&#32570;&#22833;&#20540;&#22635;&#34917;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#20102;&#32479;&#35745;&#23398;&#19978;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The imputation of missing values in multivariate time series (MTS) data is critical in ensuring data quality and producing reliable data-driven predictive models. Apart from many statistical approaches, a few recent studies have proposed state-of-the-art deep learning methods to impute missing values in MTS data. However, the evaluation of these deep methods is limited to one or two data sets, low missing rates, and completely random missing value types. This survey performs six data-centric experiments to benchmark state-of-the-art deep imputation methods on five time series health data sets. Our extensive analysis reveals that no single imputation method outperforms the others on all five data sets. The imputation performance depends on data types, individual variable statistics, missing value rates, and types. Deep learning methods that jointly perform cross-sectional (across variables) and longitudinal (across time) imputations of missing values in time series data yield statistica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#20449;&#24565;&#24418;&#25104;&#31574;&#30053;&#65292;&#20351;&#24471;&#20195;&#29702;&#30340;&#21442;&#25968;&#21487;&#20197;&#19982;&#38598;&#20013;&#24335;&#22522;&#32447;&#26377;&#30028;&#30340;&#24046;&#24322;&#65292;&#24182;&#22312;&#22810;&#20256;&#24863;&#22120;&#30446;&#26631;&#36319;&#36394;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.04151</link><description>&lt;p&gt;
&#20855;&#26377;&#20449;&#24565;&#20849;&#20139;&#30340;&#21435;&#20013;&#24515;&#21270;POMDP&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Policy Evaluation in Decentralized POMDPs with Belief Sharing. (arXiv:2302.04151v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#20449;&#24565;&#24418;&#25104;&#31574;&#30053;&#65292;&#20351;&#24471;&#20195;&#29702;&#30340;&#21442;&#25968;&#21487;&#20197;&#19982;&#38598;&#20013;&#24335;&#22522;&#32447;&#26377;&#30028;&#30340;&#24046;&#24322;&#65292;&#24182;&#22312;&#22810;&#20256;&#24863;&#22120;&#30446;&#26631;&#36319;&#36394;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#29615;&#22659;&#29366;&#24577;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#32771;&#34385;&#19968;&#31181;&#21327;&#20316;&#31574;&#30053;&#35780;&#20272;&#20219;&#21153;&#65292;&#20854;&#20013;&#20195;&#29702;&#19981;&#33021;&#30452;&#25509;&#35266;&#23519;&#29615;&#22659;&#29366;&#24577;&#12290;&#30456;&#21453;&#65292;&#20195;&#29702;&#21482;&#33021;&#35775;&#38382;&#21547;&#22122;&#22768;&#35266;&#27979;&#21644;&#32622;&#20449;&#21521;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#30340;&#20449;&#24565;&#24418;&#25104;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20381;&#36182;&#20110;&#20010;&#20307;&#26356;&#26032;&#21644;&#36890;&#20449;&#32593;&#32476;&#19978;&#30340;&#26412;&#22320;&#21270;&#20132;&#20114;&#12290;&#38500;&#20102;&#20132;&#25442;&#20449;&#24565;&#22806;&#65292;&#20195;&#29702;&#36824;&#21033;&#29992;&#36890;&#20449;&#32593;&#32476;&#20132;&#25442;&#20215;&#20540;&#20989;&#25968;&#21442;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#20998;&#26512;&#22320;&#34920;&#26126;&#65292;&#35813;&#31574;&#30053;&#20801;&#35768;&#20449;&#24687;&#22312;&#32593;&#32476;&#20013;&#25193;&#25955;&#65292;&#20174;&#32780;&#20351;&#20195;&#29702;&#30340;&#21442;&#25968;&#19982;&#38598;&#20013;&#24335;&#22522;&#32447;&#20855;&#26377;&#26377;&#30028;&#24046;&#24322;&#12290;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#30446;&#26631;&#36319;&#36394;&#24212;&#29992;&#26159;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most works on multi-agent reinforcement learning focus on scenarios where the state of the environment is fully observable. In this work, we consider a cooperative policy evaluation task in which agents are not assumed to observe the environment state directly. Instead, agents can only have access to noisy observations and to belief vectors. It is well-known that finding global posterior distributions under multi-agent settings is generally NP-hard. As a remedy, we propose a fully decentralized belief forming strategy that relies on individual updates and on localized interactions over a communication network. In addition to the exchange of the beliefs, agents exploit the communication network by exchanging value function parameter estimates as well. We analytically show that the proposed strategy allows information to diffuse over the network, which in turn allows the agents' parameters to have a bounded difference with a centralized baseline. A multi-sensor target tracking applicatio
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#22914;&#20309;&#25913;&#36827;&#22312;&#32447;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#30340;TS&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20381;&#36182;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65307;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#22312;&#32447;&#24615;&#33021;&#65292;&#25913;&#36827;&#31243;&#24230;&#38543;&#19987;&#23478;&#33021;&#21147;&#27700;&#24179;&#30340;&#25552;&#39640;&#32780;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2302.03319</link><description>&lt;p&gt;
&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#25913;&#36827;&#22312;&#32447;&#23398;&#20064;:&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Leveraging Demonstrations to Improve Online Learning: Quality Matters. (arXiv:2302.03319v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#22914;&#20309;&#25913;&#36827;&#22312;&#32447;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#30340;TS&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20381;&#36182;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65307;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#22312;&#32447;&#24615;&#33021;&#65292;&#25913;&#36827;&#31243;&#24230;&#38543;&#19987;&#23478;&#33021;&#21147;&#27700;&#24179;&#30340;&#25552;&#39640;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#21487;&#20197;&#22914;&#20309;&#25913;&#36827;&#22312;&#32447;&#23398;&#20064;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#26399;&#26395;&#20250;&#26377;&#19968;&#23450;&#30340;&#25913;&#36827;&#65292;&#20294;&#38382;&#39064;&#22312;&#20110;&#22914;&#20309;&#25913;&#36827;&#20197;&#21450;&#21487;&#20197;&#25913;&#36827;&#22810;&#23569;&#65311;&#25105;&#20204;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#31243;&#24230;&#24517;&#39035;&#21462;&#20915;&#20110;&#28436;&#31034;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#29983;&#25104;&#21487;&#31227;&#26893;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#20316;&#20026;&#20856;&#22411;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#21644;&#27169;&#22411;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#19978;&#24212;&#29992;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;TS&#65289;&#12290;&#28436;&#31034;&#25968;&#25454;&#26159;&#30001;&#20855;&#26377;&#32473;&#23450;&#33021;&#21147;&#27700;&#24179;&#30340;&#19987;&#23478;&#29983;&#25104;&#30340;&#65292;&#36825;&#26159;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#20010;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#24773;TS&#31639;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#23450;&#29702;&#20197;&#19968;&#33268;&#30340;&#26041;&#24335;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#24182;&#23548;&#20986;&#20381;&#36182;&#20110;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;&#36825;&#25552;&#20379;&#20102;&#27934;&#35265;&#65292;&#21363;&#39044;&#35757;&#32451;&#22914;&#20309;&#26497;&#22823;&#22320;&#25552;&#39640;&#22312;&#32447;&#24615;&#33021;&#65292;&#20197;&#21450;&#25913;&#36827;&#31243;&#24230;&#38543;&#19987;&#23478;&#33021;&#21147;&#27700;&#24179;&#30340;&#25552;&#39640;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36125;&#21494;&#26031;&#24341;&#23548;&#23454;&#29616;&#20102;&#23454;&#29992;&#30340;&#12289;&#36817;&#20284;&#30340;&#30693;&#24773;TS&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23454;&#29616;&#20102;&#23454;&#36136;&#24615;&#30340;&#36951;&#25022;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the extent to which offline demonstration data can improve online learning. It is natural to expect some improvement, but the question is how, and by how much? We show that the degree of improvement must depend on the quality of the demonstration data. To generate portable insights, we focus on Thompson sampling (TS) applied to a multi-armed bandit as a prototypical online learning algorithm and model. The demonstration data is generated by an expert with a given competence level, a notion we introduce. We propose an informed TS algorithm that utilizes the demonstration data in a coherent way through Bayes' rule and derive a prior-dependent Bayesian regret bound. This offers insight into how pretraining can greatly improve online performance and how the degree of improvement increases with the expert's competence level. We also develop a practical, approximate informed TS algorithm through Bayesian bootstrapping and show substantial empirical regret reduction through exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROE&#30340;&#26032;&#22411;&#25968;&#25454;&#27745;&#26579;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22522;&#26412;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36816;&#34892;&#24335;&#36873;&#20030;&#65292;&#26377;&#25928;&#21033;&#29992;logits&#23618;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;MNIST&#25968;&#25454;&#38598;&#21644;CIFAR-10&#19978;&#24471;&#21040;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;&#38598;&#25104;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;</title><link>http://arxiv.org/abs/2302.02300</link><description>&lt;p&gt;
&#36816;&#34892;&#24335;&#36873;&#20030;&#65306;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#25913;&#36827;&#21487;&#35777;&#26126;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Run-Off Election: Improved Provable Defense against Data Poisoning Attacks. (arXiv:2302.02300v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROE&#30340;&#26032;&#22411;&#25968;&#25454;&#27745;&#26579;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22522;&#26412;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36816;&#34892;&#24335;&#36873;&#20030;&#65292;&#26377;&#25928;&#21033;&#29992;logits&#23618;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;MNIST&#25968;&#25454;&#38598;&#21644;CIFAR-10&#19978;&#24471;&#21040;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;&#38598;&#25104;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#32773;&#35797;&#22270;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#12289;&#20462;&#25913;&#25110;&#21024;&#38500;&#26679;&#26412;&#26469;&#25913;&#21464;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#38598;&#25104;&#26041;&#27861;&#30340;&#21487;&#35777;&#26126;&#25968;&#25454;&#27745;&#26579;&#38450;&#24481;&#26041;&#27861;&#65292;&#20854;&#20013;&#39044;&#27979;&#26159;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#26412;&#27169;&#22411;&#36827;&#34892;&#22810;&#25968;&#34920;&#20915;&#26469;&#23436;&#25104;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#32771;&#34385;&#38598;&#25104;&#38450;&#24481;&#20013;&#30340;&#22823;&#22810;&#25968;&#34920;&#20915;&#26159;&#28010;&#36153;&#30340;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#26377;&#25928;&#22320;&#21033;&#29992;&#22522;&#26412;&#27169;&#22411;&#20013;&#30340;logits&#23618;&#20013;&#21487;&#29992;&#30340;&#20449;&#24687;&#12290;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36816;&#34892;&#24335;&#36873;&#20030;&#65288;ROE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22522;&#26412;&#27169;&#22411;&#20043;&#38388;&#30340;&#20004;&#36718;&#36873;&#20030;&#30340;&#26032;&#22411;&#32858;&#21512;&#26041;&#27861;&#65306;&#22312;&#31532;&#19968;&#36718;&#20013;&#65292;&#27169;&#22411;&#20026;&#23427;&#20204;&#39318;&#36873;&#30340;&#31867;&#21035;&#25237;&#31080;&#65292;&#28982;&#21518;&#22312;&#31532;&#19968;&#36718;&#20013;&#25490;&#21517;&#21069;&#20004;&#30340;&#31867;&#21035;&#20043;&#38388;&#36827;&#34892;&#31532;&#20108;&#36718;&#8220;Run-Off&#8221;&#36873;&#20030;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;Deep Partition Aggregation&#65288;DPA&#65289;&#21644;Finite Aggregation&#65288;FA&#65289;&#26041;&#27861;&#30340;DPA+ROE&#21644;FA+ROE&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;MNIST&#25968;&#25454;&#38598;&#21644;CIFAR-10&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;ROE&#30340;&#38450;&#24481;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
In data poisoning attacks, an adversary tries to change a model's prediction by adding, modifying, or removing samples in the training data. Recently, ensemble-based approaches for obtaining provable defenses against data poisoning have been proposed where predictions are done by taking a majority vote across multiple base models. In this work, we show that merely considering the majority vote in ensemble defenses is wasteful as it does not effectively utilize available information in the logits layers of the base models. Instead, we propose Run-Off Election (ROE), a novel aggregation method based on a two-round election across the base models: In the first round, models vote for their preferred class and then a second, Run-Off election is held between the top two classes in the first round. Based on this approach, we propose DPA+ROE and FA+ROE defense methods based on Deep Partition Aggregation (DPA) and Finite Aggregation (FA) approaches from prior work. We evaluate our methods on MN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#25214;&#21040;&#26368;&#20248;&#21270;&#30340;&#30123;&#24773;&#35745;&#21010;&#36716;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#25628;&#32034;&#26368;&#23567;&#21270;&#30142;&#30149;&#21644;&#32463;&#27982;&#25104;&#26412;&#30340;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2301.12802</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35268;&#21010;&#22810;&#31181;&#20256;&#26579;&#30149;&#24178;&#39044;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Planning Multiple Epidemic Interventions with Reinforcement Learning. (arXiv:2301.12802v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25214;&#21040;&#26368;&#20248;&#21270;&#30340;&#30123;&#24773;&#35745;&#21010;&#36716;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#25628;&#32034;&#26368;&#23567;&#21270;&#30142;&#30149;&#21644;&#32463;&#27982;&#25104;&#26412;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#27969;&#34892;&#30149;&#38656;&#35201;&#21046;&#23450;&#35745;&#21010;&#65292;&#25551;&#36848;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#24212;&#29992;&#19981;&#21516;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#27604;&#22914;&#35201;&#27714;&#20329;&#25140;&#21475;&#32617;&#12289;&#25509;&#31181;&#30123;&#33495;&#12289;&#20851;&#38381;&#23398;&#26657;&#25110;&#24037;&#20316;&#22330;&#25152;&#31561;&#12290;&#26368;&#20248;&#30340;&#35745;&#21010;&#23558;&#20197;&#26368;&#23567;&#30340;&#29983;&#21629;&#25439;&#22833;&#12289;&#30142;&#30149;&#36127;&#25285;&#21644;&#32463;&#27982;&#25104;&#26412;&#36943;&#21046;&#30123;&#24773;&#12290;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#23547;&#25214;&#26368;&#20248;&#35745;&#21010;&#26159;&#19968;&#20010;&#38590;&#20197;&#35299;&#20915;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#34920;&#36848;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#34920;&#31034;&#23545;&#20110;&#20219;&#20309;&#30001;&#24120;&#24494;&#20998;&#26041;&#31243;&#23450;&#20041;&#30340;&#30142;&#30149;&#27169;&#22411;&#19978;&#30340;&#22810;&#20010;&#36830;&#32493;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO&#21644;SAC&#65289;&#65292;&#20197;&#22312;&#36830;&#32493;&#21644;&#22797;&#26434;&#30340;&#29366;&#24577;&#31354;&#38388;&#20013;&#25628;&#32034;&#26368;&#23567;&#21270;&#30142;&#30149;&#21644;&#32463;&#27982;&#25104;&#26412;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combating an epidemic entails finding a plan that describes when and how to apply different interventions, such as mask-wearing mandates, vaccinations, school or workplace closures. An optimal plan will curb an epidemic with minimal loss of life, disease burden, and economic cost. Finding an optimal plan is an intractable computational problem in realistic settings. Policy-makers, however, would greatly benefit from tools that can efficiently search for plans that minimize disease and economic costs especially when considering multiple possible interventions over a continuous and complex action space given a continuous and equally complex state space. We formulate this problem as a Markov decision process. Our formulation is unique in its ability to represent multiple continuous interventions over any disease model defined by ordinary differential equations. We illustrate how to effectively apply state-of-the-art actor-critic reinforcement learning algorithms (PPO and SAC) to search fo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31283;&#20581;&#20248;&#21270;&#35745;&#31639;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CE&#65289;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36873;&#25321;&#36866;&#24403;&#30340;&#25514;&#26045;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#32467;&#26524;&#65292;&#27492;&#26041;&#27861;&#22312;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#26368;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19978;&#35777;&#26126;&#20102;&#25910;&#25947;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.11113</link><description>&lt;p&gt;
&#36890;&#36807;&#31283;&#20581;&#20248;&#21270;&#25214;&#20986;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Finding Regions of Counterfactual Explanations via Robust Optimization. (arXiv:2301.11113v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11113
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31283;&#20581;&#20248;&#21270;&#35745;&#31639;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CE&#65289;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36873;&#25321;&#36866;&#24403;&#30340;&#25514;&#26045;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#32467;&#26524;&#65292;&#27492;&#26041;&#27861;&#22312;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#26368;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19978;&#35777;&#26126;&#20102;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#26816;&#27979;&#20559;&#35265;&#21644;&#25552;&#39640;&#25968;&#25454;&#39537;&#21160;&#20998;&#31867;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#19968;&#20010;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CE&#65289;&#26159;&#19968;&#20010;&#26368;&#23567;&#30340;&#25200;&#21160;&#25968;&#25454;&#28857;&#65292;&#20351;&#24471;&#27169;&#22411;&#30340;&#20915;&#31574;&#21457;&#29983;&#21464;&#21270;&#12290;&#29616;&#26377;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#33021;&#25552;&#20379;&#19968;&#20010;CE&#65292;&#21487;&#33021;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#26469;&#35745;&#31639;&#31283;&#20581;CE&#65292;&#21363;&#22312;&#29305;&#24449;&#36731;&#24494;&#25200;&#21160;&#21518;&#20173;&#28982;&#26377;&#25928;&#30340;CE&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#25972;&#20010;CE&#21306;&#22495;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36873;&#25321;&#36866;&#24403;&#30340;&#25514;&#26045;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#31283;&#20581;&#20248;&#21270;&#30340;&#31639;&#27861;&#24605;&#24819;&#65292;&#24182;&#35777;&#26126;&#20102;&#26368;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20026;&#21508;&#31181;&#24120;&#35265;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#27169;&#22411;&#29983;&#25104;&#20840;&#23616;&#26368;&#20339;&#30340;&#31283;&#20581;CE&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations play an important role in detecting bias and improving the explainability of data-driven classification models. A counterfactual explanation (CE) is a minimal perturbed data point for which the decision of the model changes. Most of the existing methods can only provide one CE, which may not be achievable for the user. In this work we derive an iterative method to calculate robust CEs, i.e. CEs that remain valid even after the features are slightly perturbed. To this end, our method provides a whole region of CEs allowing the user to choose a suitable recourse to obtain a desired outcome. We use algorithmic ideas from robust optimization and prove convergence results for the most common machine learning methods including logistic regression, decision trees, random forests, and neural networks. Our experiments show that our method can efficiently generate globally optimal robust CEs for a variety of common data sets and classification models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2301.10856</link><description>&lt;p&gt;
&#37096;&#20998;&#21160;&#21592;&#65306;&#36319;&#36394;&#20420;&#32599;&#26031;&#23186;&#20307;&#21644;&#30005;&#25253;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#21518;&#65292;&#38024;&#23545;&#20420;&#32599;&#26031;&#22312;&#32447;&#23186;&#20307;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#65292;&#21253;&#25324;&#20420;&#32599;&#26031;&#20043;&#22768;&#21644;&#21355;&#26143;&#26032;&#38395;&#22312;&#20869;&#30340;&#20420;&#32599;&#26031;&#23186;&#20307;&#22312;&#27431;&#27954;&#36973;&#21040;&#31105;&#27490;&#12290;&#20026;&#20102;&#20445;&#25345;&#35266;&#20247;&#25968;&#37327;&#65292;&#35768;&#22810;&#20420;&#32599;&#26031;&#23186;&#20307;&#24320;&#22987;&#22312;&#30005;&#25253;&#31561;&#28040;&#24687;&#26381;&#21153;&#19978;&#22823;&#21147;&#23459;&#20256;&#20854;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;2022&#24180;&#26399;&#38388;16&#23478;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#22914;&#20309;&#19982;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20114;&#21160;&#21644;&#21033;&#29992;&#12290;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;MPNet&#12289;DP-means&#32858;&#31867;&#21644;Hawkes&#36807;&#31243;&#65292;&#25105;&#20204;&#36319;&#36394;&#26032;&#38395;&#32593;&#31449;&#21644;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#21465;&#20107;&#20256;&#25773;&#24773;&#20917;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20182;&#20204;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#12290;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#32593;&#31449;&#20013;&#65292;2.3&#65285;&#65288;ura.news&#65289;&#33267;26.7&#65285;&#65288;ukraina.ru&#65289;&#30340;&#25991;&#31456;&#35752;&#35770;&#20102;&#28304;&#20110;/&#23548;&#33268;&#30005;&#25253;&#27963;&#21160;&#30340;&#20869;&#23481;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36319;&#36394;&#20010;&#21035;&#20027;&#39064;&#30340;&#25193;&#25955;&#65292;&#25105;&#20204;&#27979;&#37327;&#26032;&#38395;&#32593;&#31449;&#21457;&#34920;&#25991;&#31456;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#36947;&#24503;&#36873;&#25321;&#65292;&#35774;&#35745;&#20102;&#19968;&#22871;&#36947;&#24503;&#22870;&#21169;&#32467;&#26500;&#65292;&#26088;&#22312;&#20998;&#26512;&#21644;&#30740;&#31350;AI&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2301.08491</link><description>&lt;p&gt;
&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#36947;&#24503;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning. (arXiv:2301.08491v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#36947;&#24503;&#36873;&#25321;&#65292;&#35774;&#35745;&#20102;&#19968;&#22871;&#36947;&#24503;&#22870;&#21169;&#32467;&#26500;&#65292;&#26088;&#22312;&#20998;&#26512;&#21644;&#30740;&#31350;AI&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#26234;&#33021;&#20195;&#29702;&#20013;&#32435;&#20837;&#36947;&#24503;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#23637;&#29616;&#12290;&#21516;&#26102;&#20063;&#24378;&#35843;&#65292;&#25353;&#29031;&#20219;&#20309;&#19968;&#31181;&#36947;&#24503;&#35266;&#23450;&#20041;&#39030;&#23618;&#30340;AI&#20262;&#29702;&#32422;&#26463;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#65292;&#24182;&#19988;&#20250;&#24102;&#26469;&#39118;&#38505;&#12290;&#20174;&#24213;&#23618;&#23398;&#20064;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25110;&#35768;&#26356;&#36866;&#21512;&#30740;&#31350;&#21644;&#24320;&#21457;AI&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20998;&#26512;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#36947;&#24503;&#22870;&#21169;&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#23454;&#34892;&#34892;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26032;&#20852;&#34892;&#20026;&#26159;&#19968;&#20010;&#26377;&#36259;&#21644;&#23500;&#26377;&#27934;&#23519;&#21147;&#30340;&#36215;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26681;&#25454;&#36947;&#24503;&#29702;&#35770;&#30340;&#22870;&#21169;&#36827;&#34892;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31616;&#21270;&#20294;&#20195;&#34920;&#19968;&#32452;&#20851;&#38190;&#20262;&#29702;&#31995;&#32479;&#30340;&#22870;&#21169;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#21306;&#20998;&#21518;&#26524;&#21644;&#35268;&#33539;&#20262;&#29702;&#30340;&#36947;&#24503;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20197;&#21019;&#24314;&#26032;&#30340;&#22870;&#21169;&#26041;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#22312;&#31038;&#20250;&#22256;&#22659;&#19979;&#36827;&#34892;&#20869;&#22312;&#21160;&#26426;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#35780;&#20272;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22797;&#21046;&#24182;&#25193;&#23637;&#26377;&#20851;&#36947;&#24503;&#36873;&#25321;&#30340;&#25991;&#29486;&#30740;&#31350;&#20013;&#30340;&#35768;&#22810;&#21457;&#29616;&#65292;&#24182;&#33021;&#22815;&#20986;&#29616;&#20197;&#21069;&#26410;&#26366;&#25253;&#36947;&#30340;&#26032;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas.  In this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm
&lt;/p&gt;</description></item><item><title>D-Adaptation&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#32780;&#26080;&#38656;&#36229;&#21442;&#25968;&#65292;&#20063;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.07733</link><description>&lt;p&gt;
&#36890;&#36807;D&#36866;&#24212;&#23454;&#29616;&#23398;&#20064;&#29575;&#33258;&#30001;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07733
&lt;/p&gt;
&lt;p&gt;
D-Adaptation&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#32780;&#26080;&#38656;&#36229;&#21442;&#25968;&#65292;&#20063;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
D&#36866;&#24212;&#26159;&#19968;&#31181;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#28176;&#36817;&#22320;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#26080;&#38656;&#22238;&#28335;&#25110;&#32447;&#24615;&#25628;&#32034;&#65292;&#24182;&#19988;&#27599;&#27493;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#20989;&#25968;&#20540;&#25110;&#26799;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36825;&#19968;&#31867;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#26080;&#36229;&#21442;&#25968;&#19988;&#25910;&#25947;&#36895;&#29575;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;SGD&#21644;Adam&#21464;&#20307;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20854;&#20013;&#35813;&#26041;&#27861;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#65292;&#22312;&#21313;&#22810;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#24212;&#29992;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#38382;&#39064;&#12290;&#24320;&#28304;&#23454;&#29616;&#22312; \url{https://github.com/facebookresearch/dadaptation}.
&lt;/p&gt;
&lt;p&gt;
D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.  An open-source implementation is available at \url{https://github.com/facebookresearch/dadaptation}.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLAP&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26367;&#20195;&#25968;&#25454;&#22686;&#24378;&#65292;&#21152;&#24378;&#32463;&#39564;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#24182;&#20943;&#23569;&#26679;&#26412;&#22823;&#23567;&#12290; &#22312;Gomoku&#28216;&#25103;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#35777;&#26126;&#20102;SLAP&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#26679;&#26412;&#25968;&#37327;&#12290;&#36825;&#31181;&#31574;&#30053;&#33267;&#23569;&#36866;&#29992;&#20110;&#23545;&#31216;&#25110;&#29305;&#23450;&#21464;&#25442;&#19981;&#21464;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2301.04746</link><description>&lt;p&gt;
&#21487;&#20999;&#25442;&#36731;&#37327;&#32423;&#21453;&#23545;&#31216;&#22788;&#29702;&#65288;SLAP&#65289;&#22312; Gomoku &#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992; CNN &#27604;&#25968;&#25454;&#22686;&#24378;&#26356;&#24555;
&lt;/p&gt;
&lt;p&gt;
Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning. (arXiv:2301.04746v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLAP&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26367;&#20195;&#25968;&#25454;&#22686;&#24378;&#65292;&#21152;&#24378;&#32463;&#39564;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#24182;&#20943;&#23569;&#26679;&#26412;&#22823;&#23567;&#12290; &#22312;Gomoku&#28216;&#25103;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#35777;&#26126;&#20102;SLAP&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#26679;&#26412;&#25968;&#37327;&#12290;&#36825;&#31181;&#31574;&#30053;&#33267;&#23569;&#36866;&#29992;&#20110;&#23545;&#31216;&#25110;&#29305;&#23450;&#21464;&#25442;&#19981;&#21464;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SLAP &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#24378;&#32463;&#39564;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#24182;&#20943;&#23569;&#26679;&#26412;&#22823;&#23567;&#65292;&#20197;&#20195;&#26367;&#25968;&#25454;&#22686;&#24378;&#12290;SLAP&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#21327;&#35758;/&#20989;&#25968;&#65292;&#21487;&#20197;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#20294;&#32473;&#20104;&#19981;&#21516;&#30340;&#21464;&#25442;&#21464;&#37327;&#12290;&#22312;Gomoku&#28216;&#25103;&#29366;&#24577;&#30340;&#23454;&#39564;&#20013;&#65292;SLAP&#25552;&#39640;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#36798;83&#65285;&#65292;&#26679;&#26412;&#22823;&#23567;&#21482;&#26377;&#25968;&#25454;&#22686;&#24378;&#30340;1/8&#12290;&#22312;Gomoku&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;AlphaGo Zero / AlphaZero&#31639;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#20316;&#20026;&#22522;&#32447;&#65292;SLAP&#23558;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#20943;&#23569;&#20102;8&#20493;&#65292;&#24182;&#22312;&#19982;&#30456;&#21516;&#35780;&#20272;&#22120;&#23545;&#27604;&#26102;&#23454;&#29616;&#20102;&#31867;&#20284;&#30340;&#33719;&#32988;&#29575;&#65292;&#20294;&#23578;&#19981;&#33021;&#35777;&#26126;&#23427;&#21487;&#20197;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#20123;&#30410;&#22788;&#33267;&#23569;&#24212;&#36866;&#29992;&#20110;&#23545;&#31216;&#25110;&#29305;&#23450;&#21464;&#25442;&#19981;&#21464;&#30340;&#39046;&#22495;&#12290;&#20316;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#65292;SLAP&#21487;&#33021;&#26377;&#21161;&#20110;&#26356;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#20197;&#21450;&#19981;&#36866;&#29992;&#20110;&#23545;&#31216;&#30340;&#39046;&#22495;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
To replace data augmentation, this paper proposed a method called SLAP to intensify experience to speed up machine learning and reduce the sample size. SLAP is a model-independent protocol/function to produce the same output given different transformation variants. SLAP improved the convergence speed of convolutional neural network learning by 83% in the experiments with Gomoku game states, with only one eighth of the sample size compared with data augmentation. In reinforcement learning for Gomoku, using AlphaGo Zero/AlphaZero algorithm with data augmentation as baseline, SLAP reduced the number of training samples by a factor of 8 and achieved similar winning rate against the same evaluator, but it was not yet evident that it could speed up reinforcement learning. The benefits should at least apply to domains that are invariant to symmetry or certain transformations. As future work, SLAP may aid more explainable learning and transfer learning for domains that are not invariant to sym
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#30784;&#20915;&#31574;&#27169;&#22411;&#65288;FDM&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#36716;&#25442;&#22120;&#31070;&#32463;&#26550;&#26500;&#23558;&#21508;&#31181;&#20915;&#31574;&#20219;&#21153;&#21046;&#23450;&#20026;&#24207;&#21015;&#35299;&#30721;&#20219;&#21153;&#65292;&#20026;&#22312;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#23454;&#29616;&#26426;&#22120;&#39537;&#21160;&#26234;&#33021;&#20915;&#31574;(IDM)&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#26223;&#24191;&#38420;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2212.12669</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23454;&#29616;&#26234;&#33021;&#20915;&#31574;&#65306;&#22522;&#30784;&#20915;&#31574;&#27169;&#22411;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective. (arXiv:2212.12669v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#30784;&#20915;&#31574;&#27169;&#22411;&#65288;FDM&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#36716;&#25442;&#22120;&#31070;&#32463;&#26550;&#26500;&#23558;&#21508;&#31181;&#20915;&#31574;&#20219;&#21153;&#21046;&#23450;&#20026;&#24207;&#21015;&#35299;&#30721;&#20219;&#21153;&#65292;&#20026;&#22312;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#23454;&#29616;&#26426;&#22120;&#39537;&#21160;&#26234;&#33021;&#20915;&#31574;(IDM)&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#26223;&#24191;&#38420;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#26222;&#36941;&#19981;&#30830;&#23450;&#24615;&#21644;&#21160;&#24577;&#29305;&#24615;&#32473;&#26426;&#22120;&#39537;&#21160;&#26234;&#33021;&#20915;&#31574;(IDM)&#31995;&#32479;&#30340;&#24191;&#27867;&#23454;&#26045;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;IDM&#24212;&#20855;&#22791;&#25345;&#32493;&#33719;&#21462;&#26032;&#25216;&#33021;&#24182;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#26377;&#25928;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;&#36229;&#36234;&#20219;&#21153;&#21644;&#24212;&#29992;&#36793;&#30028;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#36827;&#27493;&#23545;&#20110;&#22686;&#24378;IDM&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24191;&#27867;&#35843;&#26597;&#20102;&#36716;&#25442;&#22120;&#31070;&#32463;&#26550;&#26500;&#20316;&#20026;&#21508;&#31181;&#20219;&#21153;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#22522;&#30784;&#20915;&#31574;&#27169;&#22411;(FDM)&#65292;&#36890;&#36807;&#20351;&#29992;&#36716;&#25442;&#22120;&#20307;&#31995;&#32467;&#26500;&#23558;&#21508;&#31181;&#20915;&#31574;&#20219;&#21153;&#21046;&#23450;&#20026;&#24207;&#21015;&#35299;&#30721;&#20219;&#21153;&#65292;&#20026;&#25193;&#23637;&#22797;&#26434;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;IDM&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#31070;&#32463;&#26550;&#26500;&#24320;&#21457;&#22522;&#30784;&#20915;&#31574;&#27169;&#22411;(FDM)&#30340;&#25928;&#29575;&#21644;&#21487;&#34892;&#24615;&#65292;&#29992;&#20110;&#26426;&#22120;&#39537;&#21160;&#26234;&#33021;&#20915;&#31574;(IDM)&#22312;&#22797;&#26434;&#23454;&#38469;&#24773;&#20917;&#20013;&#30340;&#24212;&#29992;&#12290;FDM&#21487;&#20197;&#20801;&#35768;&#25345;&#32493;&#30340;&#25216;&#33021;&#33719;&#21462;&#24182;&#22312;&#21508;&#20010;&#24212;&#29992;&#20043;&#38388;&#36827;&#34892;&#27010;&#25324;&#65292;&#20855;&#26377;&#22686;&#36827;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pervasive uncertainty and dynamic nature of real-world environments present significant challenges for the widespread implementation of machine-driven Intelligent Decision-Making (IDM) systems. Consequently, IDM should possess the ability to continuously acquire new skills and effectively generalize across a broad range of applications. The advancement of Artificial General Intelligence (AGI) that transcends task and application boundaries is critical for enhancing IDM. Recent studies have extensively investigated the Transformer neural architecture as a foundational model for various tasks, including computer vision, natural language processing, and reinforcement learning. We propose that a Foundation Decision Model (FDM) can be developed by formulating diverse decision-making tasks as sequence decoding tasks using the Transformer architecture, offering a promising solution for expanding IDM applications in complex real-world situations. In this paper, we discuss the efficiency an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;AI&#27169;&#22411;&#65292;&#20351;&#29992;&#32593;&#39029;&#25235;&#21462;&#30340;&#25968;&#25454;&#35757;&#32451;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#24615;&#29289;&#21270;&#20559;&#35265;&#65292;&#21363;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#19982;&#36523;&#20307;&#30340;&#21576;&#29616;&#30456;&#20851;&#65292;&#34920;&#29616;&#20986;&#23545;&#22899;&#24615;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2212.11261</link><description>&lt;p&gt;
&#20351;&#29992;&#32593;&#39029;&#25235;&#21462;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35821;&#35328;-&#35270;&#35273;AI&#27169;&#22411;&#23384;&#22312;&#24615;&#29289;&#21270;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias. (arXiv:2212.11261v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;AI&#27169;&#22411;&#65292;&#20351;&#29992;&#32593;&#39029;&#25235;&#21462;&#30340;&#25968;&#25454;&#35757;&#32451;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#24615;&#29289;&#21270;&#20559;&#35265;&#65292;&#21363;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#19982;&#36523;&#20307;&#30340;&#21576;&#29616;&#30456;&#20851;&#65292;&#34920;&#29616;&#20986;&#23545;&#22899;&#24615;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#30446;&#26631;&#36827;&#34892;&#32593;&#39029;&#25235;&#21462;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20061;&#31181;&#35821;&#35328;-&#35270;&#35273;AI&#27169;&#22411;&#65292;&#20197;&#23547;&#25214;&#24515;&#29702;&#23398;&#23478;&#30740;&#31350;&#30340;&#20559;&#35265;&#30340;&#35777;&#25454;&#65306;&#22899;&#23401;&#21644;&#22899;&#24615;&#30340;&#24615;&#29289;&#21270;&#29616;&#35937;&#12290;&#25105;&#20204;&#22797;&#21046;&#20102;&#19977;&#20010;&#24515;&#29702;&#23454;&#39564;&#65292;&#24182;&#26174;&#31034;&#36825;&#31181;&#20559;&#35265;&#22312;AI&#20013;&#20381;&#28982;&#23384;&#22312;&#12290;&#31532;&#19968;&#20010;&#23454;&#39564;&#20351;&#29992;Sexual OBjectification and EMotion Database&#20013;&#30340;&#26631;&#20934;&#22899;&#24615;&#22270;&#20687;&#65292;&#24182;&#21457;&#29616;&#24773;&#24863;&#29366;&#24577;&#30340;&#35782;&#21035;&#26159;&#30001;&#20027;&#20307;&#26159;&#21542;&#20840;&#36523;&#25110;&#37096;&#20998;&#31359;&#30528;&#36827;&#34892;&#20171;&#23548;&#30340;&#12290;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#36820;&#22238;&#20102;&#24868;&#24594;(d&gt;0.80)&#21644;&#24754;&#20260;(d&gt;0.50)&#30340;&#26174;&#30528;&#25928;&#24212;&#22823;&#23567;&#65292;&#23558;&#23436;&#20840;&#31359;&#30528;&#30340;&#20027;&#20307;&#30340;&#22270;&#20687;&#19982;&#24773;&#24863;&#30456;&#20851;&#32852;&#12290;GRAD-CAM&#26174;&#33879;&#24615;&#22270;&#31361;&#20986;&#26174;&#31034;&#65292;CLIP&#29983;&#25104;&#30340;&#27169;&#22411;&#23384;&#22312;&#24615;&#29289;&#21270;&#20559;&#35265;&#65292;&#21363;&#20351;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#23545;&#27604;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#32593;&#39029;&#25235;&#21462;&#30340;&#25968;&#25454;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body. We replicate three experiments in psychology quantifying sexual objectification and show that the phenomena persist in AI. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d &gt;0.80) and sadness (d &gt;0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#23454;&#38469;&#32422;&#26463;&#30340;&#28789;&#27963;&#29983;&#20135;&#35843;&#24230;&#38382;&#39064;&#65292;&#24182;&#24357;&#34917;&#20803;&#21551;&#21457;&#24335;&#30740;&#31350;&#20013;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2212.10936</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#29992;&#20110;&#31038;&#20250;&#25216;&#26415;&#29983;&#20135;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling. (arXiv:2212.10936v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#23454;&#38469;&#32422;&#26463;&#30340;&#28789;&#27963;&#29983;&#20135;&#35843;&#24230;&#38382;&#39064;&#65292;&#24182;&#24357;&#34917;&#20803;&#21551;&#21457;&#24335;&#30740;&#31350;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#21452;&#36164;&#28304;&#32422;&#26463;&#26580;&#24615;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;DRC-FJSSP&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;DRL&#25216;&#26415;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;&#27809;&#26377;&#32771;&#34385;&#21040;&#29616;&#23454;&#12289;&#28789;&#27963;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#36710;&#38388;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#20197;&#35746;&#21333;&#20026;&#23548;&#21521;&#30340;&#38388;&#27463;&#24615;&#21046;&#36896;&#20013;&#23384;&#22312;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#23427;&#32463;&#24120;&#22312;&#20855;&#26377;&#39640;&#26381;&#21153;&#27700;&#24179;&#30340;&#20013;&#23567;&#22411;&#20844;&#21496;&#20013;&#34920;&#31034;&#12290;&#20174;&#36825;&#19968;&#39046;&#22495;&#30340;&#23454;&#38469;&#24037;&#19994;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#38656;&#35201;&#25551;&#36848;&#28789;&#27963;&#30340;&#26426;&#22120;&#12289;&#20154;&#24037;&#24037;&#20316;&#32773;&#21644;&#33021;&#21147;&#12289;&#35774;&#32622;&#21644;&#22788;&#29702;&#25805;&#20316;&#12289;&#29289;&#26009;&#21040;&#36798;&#26102;&#38388;&#12289;&#20855;&#26377;&#24182;&#34892;&#20219;&#21153;&#30340;&#22797;&#26434;&#20316;&#19994;&#36335;&#24452;&#20197;&#36827;&#34892;&#29289;&#26009;&#28165;&#21333;&#65288;BOM&#65289;&#21046;&#36896;&#12289;&#39034;&#24207;&#30456;&#20851;&#35774;&#32622;&#26102;&#38388;&#21644;&#65288;&#37096;&#20998;&#65289;&#33258;&#21160;&#21270;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;DRC-FJSSP&#30340;&#32972;&#26223;&#19979;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#20803;&#21551;&#21457;&#24335;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#36866;&#24403;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#20135;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#26159;&#29616;&#23454;&#24037;&#19994;&#19990;&#30028;&#20013;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20197;&#24357;&#34917;&#30456;&#20851;&#39046;&#22495;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The following article presents a memetic algorithm with applying deep reinforcement learning (DRL) for solving practically oriented dual resource constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years, there has been extensive research on DRL techniques, but without considering realistic, flexible and human-centered shopfloors. A research gap can be identified in the context of make-to-order oriented discontinuous manufacturing as it is often represented in medium-size companies with high service levels. From practical industry projects in this domain, we recognize requirements to depict flexible machines, human workers and capabilities, setup and processing operations, material arrival times, complex job paths with parallel tasks for bill of material (BOM) manufacturing, sequence-depended setup times and (partially) automated tasks. On the other hand, intensive research has been done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of sui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36807;&#24230;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#20854;&#20013;&#21033;&#29992;&#32447;&#24615;&#23618;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#20197;&#21450;&#28608;&#27963;&#20989;&#25968;&#30340;&#24418;&#29366;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#35843;&#25972;&#22810;&#38754;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#20197;&#36866;&#24212;&#30495;&#23454;&#21487;&#36798;&#38598;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#25512;&#26029;&#20986;&#30340;&#27169;&#26495;&#35745;&#31639;&#20986;&#21487;&#36798;&#38598;&#30340;&#31934;&#30830;&#36807;&#24230;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2212.07553</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#22810;&#38754;&#20307;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#33258;&#21160;&#21487;&#36798;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Automated Reachability Analysis of Neural Network-Controlled Systems via Adaptive Polytopes. (arXiv:2212.07553v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36807;&#24230;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#20854;&#20013;&#21033;&#29992;&#32447;&#24615;&#23618;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#20197;&#21450;&#28608;&#27963;&#20989;&#25968;&#30340;&#24418;&#29366;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#35843;&#25972;&#22810;&#38754;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#20197;&#36866;&#24212;&#30495;&#23454;&#21487;&#36798;&#38598;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#25512;&#26029;&#20986;&#30340;&#27169;&#26495;&#35745;&#31639;&#20986;&#21487;&#36798;&#38598;&#30340;&#31934;&#30830;&#36807;&#24230;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#36924;&#36817;&#21160;&#24577;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#26159;&#23433;&#20840;&#39564;&#35777;&#21644;&#40065;&#26834;&#25511;&#21046;&#21512;&#25104;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#19988;&#36825;&#20123;&#38598;&#21512;&#30340;&#34920;&#31034;&#26159;&#24433;&#21709;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#36924;&#36817;&#35823;&#24046;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#27169;&#26495;&#22810;&#38754;&#20307;&#26469;&#36807;&#24230;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#32447;&#24615;&#23618;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#20197;&#21450;&#28608;&#27963;&#20989;&#25968;&#30340;&#24418;&#29366;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#35843;&#25972;&#22810;&#38754;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#20197;&#36866;&#24212;&#30495;&#23454;&#21487;&#36798;&#38598;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25903;&#23450;&#30028;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#26029;&#20986;&#30340;&#27169;&#26495;&#35745;&#31639;&#20986;&#21487;&#36798;&#38598;&#30340;&#31934;&#30830;&#36807;&#24230;&#36924;&#36817;&#12290;&#25105;&#20204;&#22312;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#19979;&#30340;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#20013;&#35828;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-approximating the reachable sets of dynamical systems is a fundamental problem in safety verification and robust control synthesis. The representation of these sets is a key factor that affects the computational complexity and the approximation error. In this paper, we develop a new approach for over-approximating the reachable sets of neural network dynamical systems using adaptive template polytopes. We use the singular value decomposition of linear layers along with the shape of the activation functions to adapt the geometry of the polytopes at each time step to the geometry of the true reachable sets. We then propose a branch-and-bound method to compute accurate over-approximations of the reachable sets by the inferred templates. We illustrate the utility of the proposed approach in the reachability analysis of linear systems driven by neural network controllers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.06751</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#21152;&#36895;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#30340;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23454;&#36341;&#32773;&#36890;&#24120;&#38754;&#20020;&#22810;&#20010;&#26041;&#38754;&#30340;&#26435;&#34913;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26102;&#38388;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#21644;&#23545;&#39640;&#25928;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#19979;&#65292;&#21152;&#36895;&#22810;&#30446;&#26631;&#20248;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23558;TPE&#30340;&#25910;&#36141;&#20989;&#25968;&#25193;&#23637;&#21040;&#20803;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#30001;&#20219;&#21153;&#20043;&#38388;&#39030;&#32423;&#22495;&#20043;&#38388;&#30340;&#37325;&#21472;&#24230;&#23450;&#20041;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20063;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#24182;&#35299;&#20915;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34920;&#26684;HPO&#22522;&#20934;&#19978;&#21152;&#36895;&#20102;MO-TPE&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#36194;&#24471;AutoML 2022&#26469;&#24471;&#21040;&#22806;&#37096;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#30456;&#20851;&#21442;&#25968;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#29702;&#35299;&#36825;&#20123;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2212.03130</link><description>&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#30456;&#20851;&#21442;&#25968;&#35782;&#21035;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Methods for Partial Differential Equations and Related Parameter Identification Problems. (arXiv:2212.03130v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#30456;&#20851;&#21442;&#25968;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#29702;&#35299;&#36825;&#20123;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#23398;&#39046;&#22495;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#26082;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#27010;&#24565;&#24182;&#25506;&#32034;&#22914;&#20309;&#20351;&#20854;&#26356;&#21152;&#31283;&#20581;&#65292;&#21448;&#23558;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#34987;&#31216;&#20026;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#34987;&#21457;&#23637;&#20986;&#26469;&#26469;&#35299;&#20915;&#29305;&#23450;&#31867;&#21035;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20102;&#20559;&#24494;&#20998;&#26041;&#31243;&#22266;&#26377;&#30340;&#24615;&#36136;&#65292;&#22240;&#27492;&#27604;&#26631;&#20934;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12289;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#36825;&#22312;&#25968;&#23398;&#24314;&#27169;&#39046;&#22495;&#20135;&#29983;&#20102;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#29702;&#35299;&#23427;&#20204;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20960;&#20010;&#19982;&#20559;&#24494;&#20998;&#26041;&#31243;&#30456;&#20851;&#30340;&#21442;&#25968;&#35782;&#21035;&#38382;&#39064;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a growth in mathematics for deep learning--which seeks a deeper understanding of the concepts of deep learning with mathematics and explores how to make it more robust--and deep learning for mathematics, where deep learning algorithms are used to solve problems in mathematics. The latter has popularised the field of scientific machine learning where deep learning is applied to problems in scientific computing. Specifically, more and more neural network architectures have been developed to solve specific classes of partial differential equations (PDEs). Such methods exploit properties that are inherent to PDEs and thus solve the PDEs better than standard feed-forward neural networks, recurrent neural networks, or convolutional neural networks. This has had a great impact in the area of mathematical modeling where parametric PDEs are widely used to model most natural and physical processes arising in science and engineering. In this work, we review such method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#26041;&#27861;&#65292;&#26159;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#38480;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2211.14411</link><description>&lt;p&gt;
c-TPE:&#22522;&#20110;&#26641;&#24418;&#32467;&#26500;&#30340;&#24102;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#24085;&#25463;&#26031;&#29305;&#20272;&#35745;&#22120;&#29992;&#20110;&#26114;&#36149;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
c-TPE: Tree-structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization. (arXiv:2211.14411v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#26041;&#27861;&#65292;&#26159;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#38480;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#24378;&#22823;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#20250;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#20869;&#23384;&#20351;&#29992;&#25110;&#24310;&#36831;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#65292;&#36825;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#22810;&#21151;&#33021;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#20197;&#22788;&#29702;&#36825;&#20123;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25193;&#23637;&#19981;&#20165;&#26159;&#31616;&#21333;&#22320;&#23558;&#29616;&#26377;&#25910;&#30410;&#20989;&#25968;&#21644;&#21407;&#22987;TPE&#32452;&#21512;&#36215;&#26469;&#65292;&#32780;&#26159;&#21253;&#25324;&#20462;&#25913;&#26469;&#35299;&#20915;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#28145;&#20837;&#20998;&#26512;&#36825;&#20123;&#20462;&#25913;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#23427;&#20204;&#22914;&#20309;&#26377;&#25928;&#22320;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#35265;&#35299;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;c-TPE&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#30340;&#24179;&#22343;&#25490;&#21517;&#24615;&#33021;&#65292;&#20855;&#26377;&#32479;&#35745;&#26174;&#30528;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is crucial for strong performance of deep learning algorithms and real-world applications often impose some constraints, such as memory usage, or latency on top of the performance requirement. In this work, we propose constrained TPE (c-TPE), an extension of the widely-used versatile Bayesian optimization method, tree-structured Parzen estimator (TPE), to handle these constraints. Our proposed extension goes beyond a simple combination of an existing acquisition function and the original TPE, and instead includes modifications that address issues that cause poor performance. We thoroughly analyze these modifications both empirically and theoretically, providing insights into how they effectively overcome these challenges. In the experiments, we demonstrate that c-TPE exhibits the best average rank performance among existing methods with statistical significance on 81 expensive HPO settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#22411;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22522;&#20110;&#30693;&#35782;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#28145;&#24230;&#38598;&#25104;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#21160;&#24577;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.13829</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#30693;&#35782;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#28145;&#24230;&#38598;&#25104;&#30340;&#23398;&#20064;&#22686;&#24378;&#22411;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning-enhanced Nonlinear Model Predictive Control using Knowledge-based Neural Ordinary Differential Equations and Deep Ensembles. (arXiv:2211.13829v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#22411;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22522;&#20110;&#30693;&#35782;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#28145;&#24230;&#38598;&#25104;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#21160;&#24577;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#25104;&#33021;&#22815;&#28385;&#36275;&#29366;&#24577;&#21644;&#25511;&#21046;&#36755;&#20837;&#32422;&#26463;&#30340;&#21453;&#39304;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#37117;&#20250;&#35299;&#20915;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21463;&#21040;&#30001;&#38750;&#32447;&#24615;&#21160;&#24577;&#27169;&#22411;&#34920;&#24449;&#30340;&#19968;&#32452;&#21160;&#24577;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#23613;&#31649;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#20294;&#38750;&#32447;&#24615;MPC&#30340;&#24615;&#33021;&#36890;&#24120;&#21462;&#20915;&#20110;&#21160;&#24577;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#65292;&#21363;&#22522;&#20110;&#30693;&#35782;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;KNODE&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;KNODE&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#31216;&#20026;KNODE&#38598;&#21512;&#65292;&#20197;&#33719;&#24471;&#30495;&#23454;&#31995;&#32479;&#21160;&#24577;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#28982;&#21518;&#23558;&#36825;&#20010;&#23398;&#20064;&#30340;&#27169;&#22411;&#25972;&#21512;&#21040;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#22686;&#24378;&#22411;&#38750;&#32447;&#24615;MPC&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#26465;&#20214;&#65292;&#20445;&#35777;&#20102;&#38381;&#29615;&#31995;&#32479;&#30340;&#28176;&#36827;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear model predictive control (MPC) is a flexible and increasingly popular framework used to synthesize feedback control strategies that can satisfy both state and control input constraints. In this framework, an optimization problem, subjected to a set of dynamics constraints characterized by a nonlinear dynamics model, is solved at each time step. Despite its versatility, the performance of nonlinear MPC often depends on the accuracy of the dynamics model. In this work, we leverage deep learning tools, namely knowledge-based neural ordinary differential equations (KNODE) and deep ensembles, to improve the prediction accuracy of this model. In particular, we learn an ensemble of KNODE models, which we refer to as the KNODE ensemble, to obtain an accurate prediction of the true system dynamics. This learned model is then integrated into a novel learning-enhanced nonlinear MPC framework. We provide sufficient conditions that guarantees asymptotic stability of the closed-loop system
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22235;&#39033;&#22686;&#24378;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#30340;&#31574;&#30053;&#65292;&#24182;&#39564;&#35777;&#20854;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;QNN&#22312;&#36817;&#20284;&#22797;&#26434;&#20989;&#25968;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.12670</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#22686;&#24378;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Expressibility-Enhancing Strategies for Quantum Neural Networks. (arXiv:2211.12670v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22235;&#39033;&#22686;&#24378;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#30340;&#31574;&#30053;&#65292;&#24182;&#39564;&#35777;&#20854;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;QNN&#22312;&#36817;&#20284;&#22797;&#26434;&#20989;&#25968;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#26469;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#20013;&#35757;&#32451;&#27169;&#22411;&#23558;&#36755;&#20837;&#25968;&#25454;&#26144;&#23556;&#21040;&#39044;&#27979;&#32467;&#26524;&#12290;&#20808;&#21069;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#38598;&#20013;&#20110;&#29702;&#35770;&#20998;&#26512;QNN&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#25991;&#29486;&#20013;&#65292;QNN&#30340;&#34920;&#36798;&#33021;&#21147;&#20165;&#22522;&#20110;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#20989;&#25968;&#36827;&#34892;&#25968;&#20540;&#39564;&#35777;&#12290;&#25105;&#20204;&#24778;&#22855;&#22320;&#21457;&#29616;&#65292;&#20855;&#26377;&#24378;&#34920;&#36798;&#33021;&#21147;&#30340;&#26368;&#20808;&#36827;QNN&#21363;&#20351;&#22312;&#36817;&#20284;&#31616;&#21333;&#30340;&#27491;&#24358;&#20989;&#25968;&#19978;&#65292;&#20063;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#22686;&#24378;QNN&#34920;&#36798;&#33021;&#21147;&#30340;&#31574;&#30053;&#65306;&#36866;&#29992;&#20110;&#27491;&#24358;&#20989;&#25968;&#30340;&#23884;&#20837;&#12289;&#20887;&#20313;&#27979;&#37327;&#12289;&#21518;&#27979;&#37327;&#20989;&#25968;&#21644;&#38543;&#26426;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#21644;/&#25110;&#25968;&#20540;&#30740;&#31350;&#65292;&#21253;&#25324;&#23398;&#20064;&#22797;&#26434;&#30340;&#22522;&#20110;&#27491;&#24358;&#20989;&#25968;&#30340;&#20989;&#25968;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#27604;&#36739;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#22235;&#20010;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;QNN&#22312;&#36817;&#20284;&#31616;&#21333;&#21333;&#21464;&#37327;&#20989;&#25968;&#20043;&#22806;&#30340;&#22797;&#26434;&#20989;&#25968;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum neural networks (QNNs), represented by parameterized quantum circuits, can be trained in the paradigm of supervised learning to map input data to predictions. Much work has focused on theoretically analyzing the expressive power of QNNs. However, in almost all literature, QNNs' expressive power is numerically validated using only simple univariate functions. We surprisingly discover that state-of-the-art QNNs with strong expressive power can have poor performance in approximating even just a simple sinusoidal function. To fill the gap, we propose four expressibility-enhancing strategies for QNNs: Sinusoidal-friendly embedding, redundant measurement, post-measurement function, and random training data. We analyze the effectiveness of these strategies via mathematical analysis and/or numerical studies including learning complex sinusoidal-based functions. Our results from comparative experiments validate that the four strategies can significantly increase the QNNs' performance in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;Diffusion Models&#26469;&#36827;&#34892;&#38899;&#39057;&#39537;&#21160;&#30340;&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#20855;&#22791;&#26497;&#39640;&#30340;&#36816;&#21160;&#36136;&#37327;&#65292;&#21487;&#20197;&#23454;&#29616;&#29420;&#29305;&#30340;&#39118;&#26684;&#34920;&#36798;&#25511;&#21046;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#22810;&#28304;&#38899;&#39057;&#19979;&#30340;&#36816;&#21160;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2211.09707</link><description>&lt;p&gt;
&#21548;&#12289;&#21435;&#22122;&#12289;&#34892;&#21160;&#65281;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38899;&#39057;&#39537;&#21160;&#21160;&#20316;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models. (arXiv:2211.09707v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09707
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;Diffusion Models&#26469;&#36827;&#34892;&#38899;&#39057;&#39537;&#21160;&#30340;&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#20855;&#22791;&#26497;&#39640;&#30340;&#36816;&#21160;&#36136;&#37327;&#65292;&#21487;&#20197;&#23454;&#29616;&#29420;&#29305;&#30340;&#39118;&#26684;&#34920;&#36798;&#25511;&#21046;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#22810;&#28304;&#38899;&#39057;&#19979;&#30340;&#36816;&#21160;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#39640;&#24230;&#34920;&#29616;&#21147;&#20294;&#35757;&#32451;&#39640;&#25928;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#21512;&#25104;&#19982;&#38899;&#39057;&#21516;&#26102;&#21457;&#29983;&#30340;&#20154;&#20307;&#36816;&#21160;&#65292;&#20363;&#22914;&#36339;&#33310;&#21644;&#20849;&#21516;&#35821;&#38899;&#25163;&#21183;&#12290;&#30001;&#20110;&#32473;&#23450;&#38899;&#39057;&#26102;&#36816;&#21160;&#22797;&#26434;&#19988;&#39640;&#24230;&#27169;&#31946;&#65292;&#38656;&#35201;&#23545;&#20854;&#36827;&#34892;&#27010;&#29575;&#25551;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;DiffWave&#32467;&#26500;&#29992;&#20110;&#24314;&#27169;3D&#23039;&#21183;&#24207;&#21015;&#65292;&#23558;Conformers&#29992;&#20110;&#26367;&#20195;&#33192;&#32960;&#21367;&#31215;&#20197;&#25552;&#39640;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23545;&#36816;&#21160;&#39118;&#26684;&#30340;&#25511;&#21046;&#65292;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#26469;&#35843;&#25972;&#39118;&#26684;&#34920;&#36798;&#30340;&#24378;&#24230;&#12290;&#25163;&#21183;&#21644;&#33310;&#36424;&#29983;&#25104;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#39640;&#27700;&#24179;&#30340;&#36816;&#21160;&#36136;&#37327;&#65292;&#20855;&#26377;&#29420;&#29305;&#30340;&#39118;&#26684;&#34920;&#36798;&#65292;&#20854;&#34920;&#36798;&#30340;&#24418;&#24335;&#21487;&#20197;&#26356;&#25110;&#32773;&#26356;&#23569;&#22320;&#31361;&#20986;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21512;&#25104;&#20102;&#36335;&#24452;&#39537;&#21160;&#30340;&#36816;&#21160;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#24341;&#23548;&#36807;&#31243;&#25512;&#24191;&#21040;&#22810;&#20010;&#38899;&#39057;&#28304;&#30340;&#26399;&#26395;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#32454;&#31890;&#24230;&#30340;&#36816;&#21160;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have experienced a surge of interest as highly expressive yet efficiently trainable probabilistic models. We show that these models are an excellent fit for synthesising human motion that co-occurs with audio, e.g., dancing and co-speech gesticulation, since motion is complex and highly ambiguous given audio, calling for a probabilistic description. Specifically, we adapt the DiffWave architecture to model 3D pose sequences, putting Conformers in place of dilated convolutions for improved modelling power. We also demonstrate control over motion style, using classifier-free guidance to adjust the strength of the stylistic expression. Experiments on gesture and dance generation confirm that the proposed method achieves top-of-the-line motion quality, with distinctive styles whose expression can be made more or less pronounced. We also synthesise path-driven locomotion using the same model architecture. Finally, we generalise the guidance procedure to obtain product-of-ex
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#38024;&#23545;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#38656;&#35201;&#19981;&#21516;&#30340;&#25915;&#20987;&#21644;&#26816;&#27979;&#26041;&#27861;&#12290;&#35770;&#25991;&#38024;&#23545;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#36827;&#34892;&#20102;&#32479;&#35745;&#27979;&#35797;&#65292;&#30830;&#23450;&#20102;&#19982;&#25968;&#25454;&#20860;&#23481;&#30340;&#27169;&#22411;&#38598;&#26159;&#21542;&#21253;&#25324;&#31995;&#32479;&#30340;&#30495;&#23454;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#34109;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2211.08804</link><description>&lt;p&gt;
&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#31163;&#32447;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20998;&#26512;&#21644;&#21487;&#26816;&#27979;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysis and Detectability of Offline Data Poisoning Attacks on Linear Dynamical Systems. (arXiv:2211.08804v4 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08804
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#38024;&#23545;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#38656;&#35201;&#19981;&#21516;&#30340;&#25915;&#20987;&#21644;&#26816;&#27979;&#26041;&#27861;&#12290;&#35770;&#25991;&#38024;&#23545;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#36827;&#34892;&#20102;&#32479;&#35745;&#27979;&#35797;&#65292;&#30830;&#23450;&#20102;&#19982;&#25968;&#25454;&#20860;&#23481;&#30340;&#27169;&#22411;&#38598;&#26159;&#21542;&#21253;&#25324;&#31995;&#32479;&#30340;&#30495;&#23454;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#34109;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#26041;&#27861;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#24433;&#21709;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#24050;&#32463;&#29087;&#30693;&#20102;&#27602;&#21270;&#25915;&#20987;&#65292;&#20294;&#36825;&#20123;&#25915;&#20987;&#36890;&#24120;&#20351;&#29992;&#20132;&#21449;&#26679;&#26412;&#29420;&#31435;&#31561;&#20551;&#35774;&#65292;&#32780;&#36825;&#20123;&#20551;&#35774;&#22312;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#36890;&#24120;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#31995;&#32479;&#38656;&#35201;&#19982;i.i.d.&#35774;&#32622;&#19979;&#38024;&#23545;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#24320;&#21457;&#30340;&#25915;&#20987;&#21644;&#26816;&#27979;&#26041;&#27861;&#19981;&#21516;&#30340;&#25915;&#20987;&#21644;&#26816;&#27979;&#26041;&#27861;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#31639;&#27861;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#27979;&#35797;&#26469;&#30740;&#31350;&#27745;&#26579;&#22914;&#20309;&#24433;&#21709;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#24182;&#36136;&#30097;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#21487;&#20197;&#20197;&#20160;&#20040;&#26041;&#24335;&#34987;&#26816;&#27979;&#21040;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#19982;&#25968;&#25454;&#20860;&#23481;&#30340;&#27169;&#22411;&#38598;&#21253;&#21547;&#31995;&#32479;&#30340;&#30495;&#23454;&#27169;&#22411;&#65292;&#24182;&#20998;&#26512;&#20102;&#25915;&#20987;&#32773;&#30340;&#19981;&#21516;&#27745;&#26579;&#31574;&#30053;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#34109;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a growing interest in the effects of data poisoning attacks on data-driven control methods. Poisoning attacks are well-known to the Machine Learning community, which, however, make use of assumptions, such as cross-sample independence, that in general do not hold for linear dynamical systems. Consequently, these systems require different attack and detection methods than those developed for supervised learning problems in the i.i.d.\ setting. Since most data-driven control algorithms make use of the least-squares estimator, we study how poisoning impacts the least-squares estimate through the lens of statistical testing, and question in what way data poisoning attacks can be detected. We establish under which conditions the set of models compatible with the data includes the true model of the system, and we analyze different poisoning strategies for the attacker. On the basis of the arguments hereby presented, we propose a stealthy data poisoning attack 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14609</link><description>&lt;p&gt;
&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation Using Parameter Pruning. (arXiv:2209.14609v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#33719;&#24471;&#20808;&#36827;&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#20915;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#23384;&#20648;&#21644;&#27169;&#22411;&#35757;&#32451;&#21464;&#24471;&#26114;&#36149;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#21512;&#25104;&#20445;&#30041;&#21407;&#22987;&#22823;&#22411;&#25968;&#25454;&#38598;&#22823;&#22810;&#25968;&#20449;&#24687;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#21305;&#37197;&#32593;&#32476;&#21442;&#25968;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#21442;&#25968;&#30340;&#32500;&#24230;&#36890;&#24120;&#24456;&#22823;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#21442;&#25968;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#38590;&#20197;&#21305;&#37197;&#65292;&#38477;&#20302;&#20102;&#33976;&#39311;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#20462;&#21098;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#21512;&#25104;&#26356;&#21152;&#31283;&#20581;&#30340;&#33976;&#39311;&#25968;&#25454;&#38598;&#24182;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many fields, the acquisition of advanced models depends on large datasets, making data storage and model training expensive. As a solution, dataset distillation can synthesize a small dataset that preserves most information of the original large dataset. The recently proposed dataset distillation method by matching network parameters has been proven effective for several datasets. However, the dimensions of network parameters are typically large. Furthermore, some parameters are difficult to match during the distillation process, degrading distillation performance. Based on this observation, this study proposes a novel dataset distillation method based on parameter pruning that solves the problem. The proposed method can synthesize more robust distilled datasets and improve distillation performance by pruning difficult-to-match parameters during the distillation process. Experimental results on three datasets show that the proposed method outperforms other state-of-the-art dataset d
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#8221;&#29702;&#24565;&#65292;&#21033;&#29992;&#27861;&#24459;&#20449;&#24687;&#23398;&#23558;&#27861;&#24459;&#30693;&#35782;&#21644;&#25512;&#29702;&#23884;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#20174;&#32780;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2209.13020</link><description>&lt;p&gt;
&#27861;&#24459;&#24341;&#23548;&#20195;&#30721;&#65306;&#19968;&#31181;&#27861;&#24459;&#20449;&#24687;&#23398;&#26041;&#27861;&#26469;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20445;&#25345;&#19968;&#33268;
&lt;/p&gt;
&lt;p&gt;
Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v13 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13020
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#8221;&#29702;&#24565;&#65292;&#21033;&#29992;&#27861;&#24459;&#20449;&#24687;&#23398;&#23558;&#27861;&#24459;&#30693;&#35782;&#21644;&#25512;&#29702;&#23884;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#20174;&#32780;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#25105;&#20204;&#26080;&#27861;&#21487;&#38752;&#22320;&#25351;&#23450;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#65292;&#20197;&#24341;&#23548;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#12290;&#21046;&#23450;&#27861;&#24459;&#21644;&#35299;&#37322;&#27861;&#24459;&#26500;&#25104;&#20102;&#19968;&#31181;&#35745;&#31639;&#24341;&#25806;&#65292;&#23558;&#19981;&#36879;&#26126;&#30340;&#20154;&#31867;&#20215;&#20540;&#36716;&#21270;&#20026;&#26131;&#35835;&#30340;&#25351;&#20196;&#12290; &#8220;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#8221;&#26159;&#23884;&#20837;&#20102;&#27861;&#24459;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#35758;&#31243;&#12290;&#31867;&#20284;&#20110;&#21512;&#21516;&#24403;&#20107;&#20154;&#26080;&#27861;&#39044;&#35265;&#20182;&#20204;&#26410;&#26469;&#20851;&#31995;&#30340;&#27599;&#20010;&#28508;&#22312;&#21464;&#25968;&#65292;&#31435;&#27861;&#32773;&#26080;&#27861;&#39044;&#27979;&#20854;&#25552;&#20986;&#30340;&#27861;&#26696;&#23558;&#36866;&#29992;&#30340;&#25152;&#26377;&#24773;&#20917;&#65292;&#25105;&#20204;&#26080;&#27861;&#25552;&#21069;&#26126;&#30830;&#35268;&#21017;&#65292;&#20197;&#21487;&#38752;&#22320;&#24341;&#23548;&#33391;&#22909;&#30340;&#20154;&#24037;&#26234;&#33021;&#34892;&#20026;&#12290;&#27861;&#24459;&#29702;&#35770;&#21644;&#23454;&#36341;&#24050;&#32463;&#24320;&#21457;&#20986;&#21508;&#31181;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20123;&#35268;&#23450;&#38382;&#39064;&#12290;&#19982;&#27861;&#24459;&#26356;&#20026;&#26222;&#36890;&#30340;&#29992;&#36884;&#65288;&#20363;&#22914;&#36890;&#36807;&#21046;&#35009;&#23041;&#32961;&#26469;&#38459;&#27490;&#19981;&#33391;&#34892;&#20026;&#65289;&#30456;&#21453;&#65292;&#27861;&#24459;&#20316;&#20026;&#19968;&#31181;&#34920;&#36798;&#20154;&#31867;&#27807;&#36890;&#30446;&#26631;&#21644;&#20215;&#20540;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#24341;&#23548;&#20154;&#24037;&#26234;&#33021;&#20195;&#30721;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27861;&#24459;&#20449;&#24687;&#23398;&#26159;&#35745;&#31639;&#35268;&#21017;&#21644;&#31995;&#32479;&#29992;&#20110;&#34920;&#31034;&#65292;&#20998;&#26512;&#21644;&#25805;&#20316;&#27861;&#24459;&#30693;&#35782;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#21033;&#29992;&#27861;&#24459;&#20449;&#24687;&#23398;&#26469;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#20445;&#25345;&#19968;&#33268;&#65292;&#20197;&#19968;&#31181;&#36879;&#26126;&#65292;&#36127;&#36131;&#65292;&#28789;&#27963;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are currently unable to specify human goals and societal values in a way that reliably directs AI behavior. Law-making and legal interpretation form a computational engine that converts opaque human values into legible directives. "Law Informs Code" is the research agenda embedding legal knowledge and reasoning in AI. Similar to how parties to a legal contract cannot foresee every potential contingency of their future relationship, and legislators cannot predict all the circumstances under which their proposed bills will be applied, we cannot ex ante specify rules that provably direct good AI behavior. Legal theory and practice have developed arrays of tools to address these specification problems. For instance, legal standards allow humans to develop shared understandings and adapt them to novel situations. In contrast to more prosaic uses of the law (e.g., as a deterrent of bad behavior through the threat of sanction), leveraged as an expression of how humans communicate their goa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#20026;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#35266;&#27979;&#39044;&#27979;&#20013;&#30340;&#23376;&#32676;&#20307;&#20844;&#24179;&#21644;&#30636;&#26102;&#20844;&#24179;&#24341;&#20837;&#20102;&#20840;&#29699;&#25910;&#25947;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.05274</link><description>&lt;p&gt;
&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#35266;&#27979;&#39044;&#27979;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness in Forecasting of Observations of Linear Dynamical Systems. (arXiv:2209.05274v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05274
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#20026;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#35266;&#27979;&#39044;&#27979;&#20013;&#30340;&#23376;&#32676;&#20307;&#20844;&#24179;&#21644;&#30636;&#26102;&#20844;&#24179;&#24341;&#20837;&#20102;&#20840;&#29699;&#25910;&#25947;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#32463;&#24120;&#20250;&#25429;&#25417;&#21040;&#26576;&#20010;&#28508;&#22312;&#20154;&#21475;&#30340;&#22810;&#20010;&#23376;&#32676;&#20307;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#34892;&#20026;&#32463;&#24120;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#20540;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;&#26410;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#23376;&#32676;&#20307;&#30340;&#35757;&#32451;&#25968;&#25454;&#27809;&#26377;&#24471;&#21040;&#31934;&#24515;&#25511;&#21046;&#65292;&#37027;&#20040;&#23601;&#20250;&#20986;&#29616;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#23545;&#25239;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#20559;&#35265;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#20013;&#20844;&#24179;&#30340;&#33258;&#28982;&#27010;&#24565;&#65306;&#23376;&#32676;&#20307;&#20844;&#24179;&#21644;&#30636;&#26102;&#20844;&#24179;&#12290;&#36825;&#20123;&#27010;&#24565;&#23558;&#39044;&#27979;&#20844;&#27491;&#24615;&#25193;&#23637;&#21040;&#20102;&#21160;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20013;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#38750;&#20132;&#25442;&#22810;&#39033;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#20984;&#21270;&#23618;&#27425;&#32467;&#26500;&#26469;&#20844;&#24179;&#22320;&#23398;&#20064;&#38382;&#39064;&#26102;&#30340;&#20840;&#29699;&#25910;&#25947;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#65292;&#36890;&#36807;&#21033;&#29992;&#20984;&#21270;&#20013;&#30340;&#31232;&#30095;&#24615;&#65292;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30001;&#20445;&#38505;&#24212;&#29992;&#31243;&#24207;&#21644;&#33879;&#21517;&#30340;COMPAS&#25968;&#25454;&#28608;&#21457;&#30340;&#26377;&#20559;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32463;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, training data often capture the behaviour of multiple subgroups of some underlying human population. This behaviour can often be modelled as observations of an unknown dynamical system with an unobserved state. When the training data for the subgroups are not controlled carefully, however, under-representation bias arises. To counter under-representation bias, we introduce two natural notions of fairness in time-series forecasting problems: subgroup fairness and instantaneous fairness. These notions extend predictive parity to the learning of dynamical systems. We also show globally convergent methods for the fairness-constrained learning problems using hierarchies of convexifications of non-commutative polynomial optimisation problems. We also show that by exploiting sparsity in the convexifications, we can reduce the run time of our methods considerably. Our empirical results on a biased data set motivated by insurance applications and the well-known COMPAS data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25361;&#25112;&#20102;DNNs&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#25552;&#20986;&#24403;&#21069;&#20248;&#21270;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#30340;DNNs&#22312;&#22238;&#24402;&#20219;&#21153;&#19978;&#21487;&#20197;&#31215;&#32047;&#30693;&#35782;&#65292;&#21363;&#20351;&#20219;&#21153;&#37325;&#26032;&#20986;&#29616;&#65292;&#20063;&#19981;&#20250;&#36951;&#24536;&#36807;&#21435;&#30340;&#30693;&#35782;&#65292;&#36825;&#31181;&#30693;&#35782;&#31215;&#32047;&#21487;&#20197;&#25913;&#21892;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.04543</link><description>&lt;p&gt;
&#25361;&#25112;&#20851;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24120;&#35265;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Challenging Common Assumptions about Catastrophic Forgetting. (arXiv:2207.04543v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;DNNs&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#25552;&#20986;&#24403;&#21069;&#20248;&#21270;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#30340;DNNs&#22312;&#22238;&#24402;&#20219;&#21153;&#19978;&#21487;&#20197;&#31215;&#32047;&#30693;&#35782;&#65292;&#21363;&#20351;&#20219;&#21153;&#37325;&#26032;&#20986;&#29616;&#65292;&#20063;&#19981;&#20250;&#36951;&#24536;&#36807;&#21435;&#30340;&#30693;&#35782;&#65292;&#36825;&#31181;&#30693;&#35782;&#31215;&#32047;&#21487;&#20197;&#25913;&#21892;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#33021;&#22815;&#36880;&#27493;&#23398;&#20064;&#21644;&#31215;&#32047;&#30693;&#35782;&#30340;&#23398;&#20064;&#26234;&#33021;&#20307;&#26159;&#36830;&#32493;&#23398;&#20064;(CL)&#30740;&#31350;&#39046;&#22495;&#30340;&#26680;&#24515;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#35757;&#32451;&#22312;&#26032;&#25968;&#25454;&#19978;&#36890;&#24120;&#20250;&#25439;&#23475;&#36807;&#21435;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;&#22312;CL&#25991;&#29486;&#20013;&#65292;&#36825;&#31181;&#25928;&#24212;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;(CF)&#12290;&#23613;&#31649;CF&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#19988;&#24050;&#25552;&#20986;&#20102;&#22823;&#37327;&#26041;&#27861;&#26469;&#35299;&#20915;&#38750;&#37325;&#21472;&#20219;&#21153;&#30701;&#24207;&#21015;&#30340;CF&#38382;&#39064;&#65292;&#20294;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;CF&#24635;&#26159;&#23548;&#33268;&#36807;&#21435;&#20219;&#21153;&#30340;&#24615;&#33021;&#36805;&#36895;&#32780;&#26174;&#33879;&#22320;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;CL&#22238;&#24402;&#35774;&#32622;&#19979;&#65292;SGD&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#32047;&#31215;&#30693;&#35782;&#12290;&#24403;&#20219;&#21153;&#37325;&#26032;&#21457;&#29983;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#23588;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#21487;&#33021;&#20250;&#24819;&#30693;&#36947;&#65292;&#26159;&#21542;&#20351;&#29992;SGD&#25110;&#20219;&#20309;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#30340;DNNs&#20250;&#20197;&#36825;&#31181;&#26041;&#24335;&#31215;&#32047;&#30693;&#35782;&#12290;&#36825;&#20123;&#29616;&#35937;&#20250;&#23545;&#23558;DNNs&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;&#36830;&#32493;&#24773;&#22659;&#20135;&#29983;&#26377;&#36259;&#30340;&#24433;&#21709;&#12290;&#23454;&#38469;&#19978;&#65292;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#25216;&#26415;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#20026;&#22823;&#35268;&#27169;&#38382;&#39064;&#25552;&#20379;&#39640;&#25928;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#24120;&#35265;&#30340;DNNs&#36973;&#21463;CF&#30340;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#24403;&#21069;&#30340;&#20248;&#21270;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#29616;&#35937;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22238;&#24402;&#20219;&#21153;&#19978;&#20351;&#29992;SGD&#35757;&#32451;&#30340;DNNs&#21487;&#20197;&#31215;&#32047;&#30693;&#35782;&#65292;&#21363;&#20351;&#20219;&#21153;&#37325;&#26032;&#20986;&#29616;&#65292;&#20063;&#19981;&#20250;&#36951;&#24536;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#30693;&#35782;&#31215;&#32047;&#21487;&#20197;&#25913;&#21892;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building learning agents that can progressively learn and accumulate knowledge is the core goal of the continual learning (CL) research field. Unfortunately, training a model on new data usually compromises the performance on past data. In the CL literature, this effect is referred to as catastrophic forgetting (CF). CF has been largely studied, and a plethora of methods have been proposed to address it on short sequences of non-overlapping tasks. In such setups, CF always leads to a quick and significant drop in performance in past tasks. Nevertheless, despite CF, recent work showed that SGD training on linear models accumulates knowledge in a CL regression setup. This phenomenon becomes especially visible when tasks reoccur. We might then wonder if DNNs trained with SGD or any standard gradient-based optimization accumulate knowledge in such a way. Such phenomena would have interesting consequences for applying DNNs to real continual scenarios. Indeed, standard gradient-based optimiz
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26420;&#32032;&#30340;&#20551;&#35774;&#65292;&#21363;&#29616;&#23454;&#19990;&#30028;&#30340;&#20998;&#24067;&#30001;&#19981;&#21516;&#22495;&#20013;&#30340;&#28508;&#22312;&#19981;&#21464;&#30340;&#22522;&#20803;&#20998;&#24067;&#65288;I.E.D&#65289;&#32452;&#25104;&#65292;&#20854;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;&#38376;&#25511;&#22495;&#21333;&#20803;&#65288;GDUs&#65289;&#32452;&#25104;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#23545;&#30475;&#19981;&#35265;&#30340;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.12444</link><description>&lt;p&gt;
&#22810;&#28304;&#22495;&#19968;&#33324;&#21270;&#30340;&#38376;&#25511;&#39046;&#22495;&#21333;&#20803;
&lt;/p&gt;
&lt;p&gt;
Gated Domain Units for Multi-source Domain Generalization. (arXiv:2206.12444v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26420;&#32032;&#30340;&#20551;&#35774;&#65292;&#21363;&#29616;&#23454;&#19990;&#30028;&#30340;&#20998;&#24067;&#30001;&#19981;&#21516;&#22495;&#20013;&#30340;&#28508;&#22312;&#19981;&#21464;&#30340;&#22522;&#20803;&#20998;&#24067;&#65288;I.E.D&#65289;&#32452;&#25104;&#65292;&#20854;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;&#38376;&#25511;&#22495;&#21333;&#20803;&#65288;GDUs&#65289;&#32452;&#25104;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#23545;&#30475;&#19981;&#35265;&#30340;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#36716;&#31227;&#29616;&#35937;&#65288;DS&#65289;&#21457;&#29983;&#20110;&#27979;&#35797;&#26102;&#38388;&#25968;&#25454;&#38598;&#19982;&#35757;&#32451;&#26102;&#38388;&#25968;&#25454;&#38598;&#19981;&#21516;&#26102;&#30340;&#24773;&#20917;&#65292;&#36825;&#21487;&#33021;&#20250;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26126;&#26174;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#32570;&#20047;&#26377;&#20851;&#27979;&#35797;&#26102;&#25968;&#25454;&#20998;&#24067;&#30340;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20551;&#35774;&#29616;&#23454;&#19990;&#30028;&#30340;&#20998;&#24067;&#30001;&#19981;&#21516;&#22495;&#20013;&#30340;&#28508;&#22312;&#19981;&#21464;&#30340;&#22522;&#20803;&#20998;&#24067;&#65288;I.E.D&#65289;&#32452;&#25104;&#12290;&#36825;&#20010;&#20551;&#35774;&#24847;&#21619;&#30528;&#35299;&#31354;&#38388;&#20013;&#23384;&#22312;&#19968;&#20010;&#19981;&#21464;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#30475;&#19981;&#35265;&#30340;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#24615;&#36136;&#26469;&#36827;&#34892;&#22495;&#19968;&#33324;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;&#38376;&#25511;&#22495;&#21333;&#20803;&#65288;GDUs&#65289;&#32452;&#25104;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#23427;&#23398;&#20064;&#27599;&#20010;&#28508;&#22312;&#22522;&#20803;&#20998;&#24067;&#30340;&#34920;&#31034;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#26032;&#35266;&#27979;&#20540;&#19982;&#27599;&#20010;&#22522;&#20803;&#20998;&#24067;&#30340;&#34920;&#31034;&#36827;&#34892;&#27604;&#36739;&#65292;&#21019;&#24314;&#19968;&#20010;&#21152;&#26435;&#30340;&#23398;&#20064;&#26426;&#38598;&#21512;&#12290;&#25105;&#20204;&#30340;&#28789;&#27963;&#26694;&#26550;&#36824;&#21487;&#20197;&#36866;&#24212;&#26126;&#30830;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The phenomenon of distribution shift (DS) occurs when a dataset at test time differs from the dataset at training time, which can significantly impair the performance of a machine learning model in practical settings due to a lack of knowledge about the data's distribution at test time. To address this problem, we postulate that real-world distributions are composed of latent Invariant Elementary Distributions (I.E.D) across different domains. This assumption implies an invariant structure in the solution space that enables knowledge transfer to unseen domains. To exploit this property for domain generalization, we introduce a modular neural network layer consisting of Gated Domain Units (GDUs) that learn a representation for each latent elementary distribution. During inference, a weighted ensemble of learning machines can be created by comparing new observations with the representations of each elementary distribution. Our flexible framework also accommodates scenarios where explicit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;Weisfeiler-Lehman&#31639;&#27861;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;NC-GNN&#26694;&#26550;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.02059</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36793;&#30340;Weisfeiler-Lehman&#31639;&#27861;&#22686;&#24378;GNN
&lt;/p&gt;
&lt;p&gt;
Empowering GNNs via Edge-Aware Weisfeiler-Lehman Algorithm. (arXiv:2206.02059v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;Weisfeiler-Lehman&#31639;&#27861;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;NC-GNN&#26694;&#26550;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#34920;&#36798;&#33021;&#21147;&#34987;&#24050;&#30693;&#30340;&#19968;&#32500;Weisfeiler-Lehman (1-WL)&#31639;&#27861;&#19978;&#30028;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;GNN&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#35201;&#20040;&#38656;&#35201;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#35201;&#20040;&#28041;&#21450;&#39640;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#25805;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#21487;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#34920;&#36798;&#21147;&#30340;GNN&#26694;&#26550;&#65292;&#20445;&#25345;&#20102;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#20043;&#38388;&#30340;&#36793;&#32536;&#26469;&#25480;&#26435;1-WL&#36827;&#34892;&#22270;&#21516;&#26500;&#27979;&#35797;&#65292;&#20174;&#32780;&#20135;&#29983;NC-1-WL&#12290; NC-1-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#22312;&#29702;&#35770;&#19978;&#34987;&#26174;&#31034;&#20026;&#20005;&#26684;&#39640;&#20110;1-WL&#19988;&#20302;&#20110;3-WL&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NC-GNN&#26694;&#26550;&#20316;&#20026;NC-1-WL&#30340;&#21487;&#21306;&#20998;&#31070;&#32463;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;NC-GNN&#23454;&#29616;&#21487;&#35777;&#26126;&#19982;NC-1-WL&#19968;&#26679;&#24378;&#22823;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;NC-GNN&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing graph neural networks (GNNs) are known to have their expressiveness upper-bounded by 1-dimensional Weisfeiler-Lehman (1-WL) algorithm. To achieve more powerful GNNs, existing attempts either require ad hoc features, or involve operations that incur high time and space complexities. In this work, we propose a general and provably powerful GNN framework that preserves the scalability of the message passing scheme. In particular, we first propose to empower 1-WL for graph isomorphism test by considering edges among neighbors, giving rise to NC-1-WL. The expressiveness of NC-1-WL is shown to be strictly above 1-WL and below 3-WL theoretically. Further, we propose the NC-GNN framework as a differentiable neural version of NC-1-WL. Our simple implementation of NC-GNN is provably as powerful as NC-1-WL. Experiments demonstrate that our NC-GNN performs effectively and efficiently on various benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#31232;&#30095;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#36880;&#27493;&#32422;&#26463;&#27169;&#22411;&#30340;&#21442;&#25968;&#38598;&#65292;&#38477;&#20302;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#65292;&#21516;&#26102;&#36798;&#21040;&#39640;&#24615;&#33021;&#30340;&#31232;&#30095;&#21270;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#31232;&#30095;&#27169;&#22411;&#21487;&#20197;&#32553;&#23567;&#21407;&#22987;&#27169;&#22411;&#30340;&#21313;&#20998;&#20043;&#19968;&#65292;&#32780;&#20934;&#30830;&#29575;&#19981;&#38477;&#20302;&#25110;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2204.12430</link><description>&lt;p&gt;
&#32852;&#37030;&#36882;&#36827;&#31232;&#30095;&#21270;&#65288;&#28165;&#29702;&#12289;&#21512;&#24182;&#12289;&#35843;&#25972;&#65289;+&#65306;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#31232;&#30095;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Federated Progressive Sparsification (Purge, Merge, Tune)+. (arXiv:2204.12430v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#31232;&#30095;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#36880;&#27493;&#32422;&#26463;&#27169;&#22411;&#30340;&#21442;&#25968;&#38598;&#65292;&#38477;&#20302;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#65292;&#21516;&#26102;&#36798;&#21040;&#39640;&#24615;&#33021;&#30340;&#31232;&#30095;&#21270;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#31232;&#30095;&#27169;&#22411;&#21487;&#20197;&#32553;&#23567;&#21407;&#22987;&#27169;&#22411;&#30340;&#21313;&#20998;&#20043;&#19968;&#65292;&#32780;&#20934;&#30830;&#29575;&#19981;&#38477;&#20302;&#25110;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#35757;&#32451;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;FedSparsify&#65292;&#19968;&#31181;&#22522;&#20110;&#28176;&#36827;&#26435;&#37325;&#24133;&#24230;&#21098;&#26525;&#30340;&#31232;&#30095;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#20960;&#20010;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#30001;&#20110;&#32593;&#32476;&#30340;&#22823;&#23567;&#36234;&#26469;&#36234;&#23567;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#24471;&#21040;&#38477;&#20302;&#12290;&#20854;&#27425;&#65292;&#27169;&#22411;&#36880;&#27493;&#38480;&#21046;&#20026;&#36739;&#23567;&#30340;&#21442;&#25968;&#38598;&#65292;&#26377;&#21033;&#20110;&#21512;&#24182;&#26412;&#22320;&#27169;&#22411;&#65292;&#25552;&#39640;&#39640;&#32423;&#31232;&#30095;&#29575;&#19979;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;&#26368;&#32456;&#31232;&#30095;&#27169;&#22411;&#26174;&#33879;&#32553;&#23567;&#65292;&#25913;&#21892;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#24182;&#20248;&#21270;&#20102;&#21152;&#23494;&#36890;&#20449;&#26399;&#38388;&#30340;&#25805;&#20316;&#24310;&#36831;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FedSparsify&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#39640;&#31232;&#30095;&#24615;&#21644;&#23398;&#20064;&#24615;&#33021;&#30340;&#23376;&#32593;&#32476;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21098;&#26525;&#21644;&#38750;&#21098;&#26525;&#22522;&#32447;&#65292;&#25105;&#20204;&#30340;&#31232;&#30095;&#27169;&#22411;&#22312;&#36798;&#21040;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#32553;&#23567;&#21407;&#22987;&#27169;&#22411;&#30340;&#21313;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve federated training of neural networks, we develop FedSparsify, a sparsification strategy based on progressive weight magnitude pruning. Our method has several benefits. First, since the size of the network becomes increasingly smaller, computation and communication costs during training are reduced. Second, the models are incrementally constrained to a smaller set of parameters, which facilitates alignment/merging of the local models and improved learning performance at high sparsification rates. Third, the final sparsified model is significantly smaller, which improves inference efficiency and optimizes operations latency during encrypted communication. We show experimentally that FedSparsify learns a subnetwork of both high sparsity and learning performance. Our sparse models can reach a tenth of the size of the original model with the same or better accuracy compared to existing pruning and nonpruning baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28789;&#27963;&#35774;&#35745;&#30340;&#22810;&#37327;&#23376;&#27604;&#29305;&#38376;&#25805;&#20316;&#30340;&#37327;&#23376;&#21464;&#20998;&#25351;&#20196;&#38598;(QuVIS)&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26102;&#38388;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#31934;&#30830;&#30340;&#37327;&#23376;&#35745;&#31639;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20986;&#20102;&#26356;&#20302;&#30340;&#35823;&#24046;&#31215;&#32047;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2203.15574</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#20998;&#25351;&#20196;&#38598;&#30340;&#37327;&#23376;&#32534;&#35793;&#20248;&#21270;&#65306;&#23454;&#29616;&#39640;&#31934;&#24230;&#19982;&#24555;&#36895;&#37327;&#23376;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Quantum compiling with variational instruction set for accurate and fast quantum computing. (arXiv:2203.15574v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28789;&#27963;&#35774;&#35745;&#30340;&#22810;&#37327;&#23376;&#27604;&#29305;&#38376;&#25805;&#20316;&#30340;&#37327;&#23376;&#21464;&#20998;&#25351;&#20196;&#38598;(QuVIS)&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26102;&#38388;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#31934;&#30830;&#30340;&#37327;&#23376;&#35745;&#31639;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20986;&#20102;&#26356;&#20302;&#30340;&#35823;&#24046;&#31215;&#32047;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#25351;&#20196;&#38598;(QIS)&#23450;&#20041;&#20026;&#22312;&#25511;&#21046;&#37327;&#23376;&#27604;&#29305;&#29366;&#24577;&#19979;&#21487;&#20197;&#29289;&#29702;&#23454;&#29616;&#30340;&#19968;&#31995;&#21015;&#37327;&#23376;&#38376;&#25805;&#20316;&#65292;&#20854;&#22312;&#37327;&#23376;&#35745;&#31639;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28789;&#27963;&#35774;&#35745;&#30340;&#22810;&#37327;&#23376;&#27604;&#29305;&#38376;&#25805;&#20316;&#30340;&#37327;&#23376;&#21464;&#20998;&#25351;&#20196;&#38598;(QuVIS)&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26102;&#38388;&#20248;&#21270;&#31639;&#27861;&#26469;&#21464;&#20998;&#23454;&#29616;&#37327;&#23376;&#27604;&#29305;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#21644;&#31934;&#30830;&#30340;&#37327;&#23376;&#35745;&#31639;&#12290;&#19982;&#26631;&#20934;QIS &#22914;&#37327;&#23376;&#24494;&#25351;&#20196;&#38598;(QuMIS)&#30456;&#27604;&#65292;QuVIS &#29992;&#20110;&#22810;&#37327;&#23376;&#27604;&#29305;&#20132;&#25442;&#21644;&#37327;&#23376;&#20613;&#37324;&#21494;&#21464;&#25442;&#31561;&#38376;&#25805;&#20316;&#20855;&#26377;&#26356;&#20302;&#30340;&#35823;&#24046;&#31215;&#32047;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#22312;&#30456;&#21516;&#37327;&#23376;&#30828;&#20214;&#35201;&#27714;&#19979;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#21487;&#22823;&#24133;&#25552;&#21319;&#37327;&#23376;&#35745;&#31639;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quantum instruction set (QIS) is defined as the quantum gates that are physically realizable by controlling the qubits in a quantum hardware. Compiling quantum circuits into the product of the gates in a properly-defined QIS is a fundamental step in quantum computing. We here propose the \R{quantum variational instruction set (QuVIS)} formed by flexibly-designed multi-qubit gates for higher speed and accuracy of quantum computing. The controlling of qubits for realizing the gates in a QuVIS are variationally achieved using the fine-grained time optimization algorithm. Significant reductions on both the error accumulation and time cost are demonstrated in realizing the swaps of multiple qubits and quantum Fourier transformations, compared with the compiling by the standard QIS such as \RR{the quantum microinstruction set} (QuMIS, formed by several one- and two-qubit gates including the one-qubit rotations and controlled-NOT gate). With the same requirement on quantum hardware, the t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#39044;&#38450;&#24615;&#23398;&#20064;&#8221;&#26694;&#26550;&#65292;&#20197;&#22312;&#28385;&#36275;&#23545;&#32422;&#26463;&#26631;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#20445;&#35777;&#20855;&#26377;&#20984;&#32422;&#26463;&#21644;&#19968;&#33324;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#30340;DNN&#35299;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#26080;&#38656;&#21518;&#22788;&#29702;&#12290;&#36890;&#36807;&#31995;&#32479;&#26631;&#23450;&#19981;&#31561;&#24335;&#32422;&#26463;&#65292;&#25105;&#20204;&#39044;&#31034;&#39044;&#27979;&#35823;&#24046;&#24182;&#30830;&#20445;&#25152;&#24471;&#21040;&#30340;&#35299;&#20173;&#28982;&#26159;&#21487;&#34892;&#30340;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#26679;&#26412;&#24863;&#30693;&#30340;&#35757;&#32451;&#31639;&#27861;&#20197;&#25552;&#39640;DNN&#30340;&#26368;&#20248;&#24615;&#33021;&#32780;&#19981;&#29306;&#29298;&#21487;&#34892;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2112.08091</link><description>&lt;p&gt;
&#22522;&#20110;&#20984;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#20013;DNN&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#20445;&#35777;&#21450;&#20854;&#22312;&#30452;&#27969;&#26368;&#20248;&#28526;&#27969;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ensuring DNN Solution Feasibility for Optimization Problems with Convex Constraints and Its Application to DC Optimal Power Flow Problems. (arXiv:2112.08091v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#39044;&#38450;&#24615;&#23398;&#20064;&#8221;&#26694;&#26550;&#65292;&#20197;&#22312;&#28385;&#36275;&#23545;&#32422;&#26463;&#26631;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#20445;&#35777;&#20855;&#26377;&#20984;&#32422;&#26463;&#21644;&#19968;&#33324;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#30340;DNN&#35299;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#26080;&#38656;&#21518;&#22788;&#29702;&#12290;&#36890;&#36807;&#31995;&#32479;&#26631;&#23450;&#19981;&#31561;&#24335;&#32422;&#26463;&#65292;&#25105;&#20204;&#39044;&#31034;&#39044;&#27979;&#35823;&#24046;&#24182;&#30830;&#20445;&#25152;&#24471;&#21040;&#30340;&#35299;&#20173;&#28982;&#26159;&#21487;&#34892;&#30340;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#26679;&#26412;&#24863;&#30693;&#30340;&#35757;&#32451;&#31639;&#27861;&#20197;&#25552;&#39640;DNN&#30340;&#26368;&#20248;&#24615;&#33021;&#32780;&#19981;&#29306;&#29298;&#21487;&#34892;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#29992;&#20110;&#35299;&#20915;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26041;&#26696;&#26102;&#65292;&#30830;&#20445;&#35299;&#30340;&#21487;&#34892;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#30001;&#20110;DNN&#22266;&#26377;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#39044;&#38450;&#24615;&#23398;&#20064;&#8221;&#26694;&#26550;&#65292;&#20197;&#22312;&#28385;&#36275;&#23545;&#32422;&#26463;&#26631;&#23450;&#30340;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#20445;&#35777;&#20855;&#26377;&#20984;&#32422;&#26463;&#21644;&#19968;&#33324;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#30340;DNN&#35299;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#26080;&#38656;&#21518;&#22788;&#29702;&#12290;&#25105;&#20204;&#26080;&#22833;&#19968;&#33324;&#24615;&#22320;&#20851;&#27880;&#21482;&#26377;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#26631;&#23450;DNN&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#19981;&#31561;&#24335;&#32422;&#26463;&#65292;&#20174;&#32780;&#39044;&#31034;&#39044;&#27979;&#35823;&#24046;&#24182;&#30830;&#20445;&#25152;&#24471;&#21040;&#30340;&#35299;&#20173;&#28982;&#26159;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#26631;&#23450;&#37327;&#21644;DNN&#22823;&#23567;&#36275;&#20197;&#30830;&#20445;&#36890;&#29992;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#26679;&#26412;&#24863;&#30693;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;DNN&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#21487;&#34892;&#24615;&#20445;&#35777;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#20010;DNN&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring solution feasibility is a key challenge in developing Deep Neural Network (DNN) schemes for solving constrained optimization problems, due to inherent DNN prediction errors. In this paper, we propose a ``preventive learning'' framework to guarantee DNN solution feasibility for problems with convex constraints and general objective functions without post-processing, upon satisfying a mild condition on constraint calibration. Without loss of generality, we focus on problems with only inequality constraints. We systematically calibrate inequality constraints used in DNN training, thereby anticipating prediction errors and ensuring the resulting solutions remain feasible. We characterize the calibration magnitudes and the DNN size sufficient for ensuring universal feasibility. We propose a new Adversarial-Sample Aware training algorithm to improve DNN's optimality performance without sacrificing feasibility guarantee. Overall, the framework provides two DNNs. The first one from ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#21644;&#27491;&#21017;&#21270;&#28508;&#21464;&#37327;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22411;&#30697;&#21305;&#37197;&#24230;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#30740;&#31350;&#25968;&#25454;&#30697;&#21644;&#27169;&#22411;&#30697;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#35780;&#20272;&#25311;&#21512;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2111.00875</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28508;&#21464;&#37327;&#29983;&#25104;&#27169;&#22411;&#30340;&#30697;&#21305;&#37197;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A moment-matching metric for latent variable generative models. (arXiv:2111.00875v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.00875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#21644;&#27491;&#21017;&#21270;&#28508;&#21464;&#37327;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22411;&#30697;&#21305;&#37197;&#24230;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#30740;&#31350;&#25968;&#25454;&#30697;&#21644;&#27169;&#22411;&#30697;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#35780;&#20272;&#25311;&#21512;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#26080;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#26102;&#65292;&#35780;&#20272;&#25311;&#21512;&#27169;&#22411;&#30340;&#36136;&#37327;&#26159;&#22256;&#38590;&#30340;&#12290;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#20284;&#28982;&#30340;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27169;&#22411;&#27604;&#36739;&#25110;&#27491;&#21017;&#21270;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#30697;&#12290;&#20854;&#27010;&#24565;&#26159;&#20351;&#29992;&#30697;&#33539;&#25968;&#65288;&#22914;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#65289;&#30740;&#31350;&#25968;&#25454;&#30697;&#21644;&#27169;&#22411;&#30697;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;&#21644;&#27491;&#21017;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It can be difficult to assess the quality of a fitted model when facing unsupervised learning problems. Latent variable models, such as variation autoencoders and Gaussian mixture models, are often trained with likelihood-based approaches. In scope of Goodhart's law, when a metric becomes a target it ceases to be a good metric and therefore we should not use likelihood to assess the quality of the fit of these models. The solution we propose is a new metric for model comparison or regularization that relies on moments. The concept is to study the difference between the data moments and the model moments using a matrix norm, such as the Frobenius norm. We show how to use this new metric for model comparison and then for regularization. It is common to draw samples from the fitted distribution when evaluating latent variable models and we show that our proposed metric is faster to compute and has a smaller variance that this alternative. We conclude this article with a proof of concept o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; DR &#26041;&#27861;&#65292;&#29992;&#20110;&#20013;&#38388;&#26465;&#20214;&#32467;&#26524;&#27169;&#22411;&#30340; DR &#34920;&#31034;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#20248;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#38754;&#20020;&#39640;&#32500;&#28151;&#28102;&#21464;&#37327;&#26102;&#20063;&#33021;&#23454;&#29616;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.04924</link><description>&lt;p&gt;
&#39640;&#32500;&#25512;&#26029;&#19979;&#30340;&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Inference for Dynamic Treatment Effects. (arXiv:2110.04924v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; DR &#26041;&#27861;&#65292;&#29992;&#20110;&#20013;&#38388;&#26465;&#20214;&#32467;&#26524;&#27169;&#22411;&#30340; DR &#34920;&#31034;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#20248;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#38754;&#20020;&#39640;&#32500;&#28151;&#28102;&#21464;&#37327;&#26102;&#20063;&#33021;&#23454;&#29616;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#65292;&#20272;&#35745;&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#24403;&#38754;&#20020;&#39640;&#32500;&#28151;&#28102;&#21464;&#37327;&#26102;&#12290;&#21452;&#37325;&#31283;&#20581; (DR) &#26041;&#27861;&#22240;&#20854;&#28789;&#27963;&#24615;&#32780;&#25104;&#20026;&#20272;&#35745;&#27835;&#30103;&#25928;&#24212;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20851;&#27880;&#39044;&#26399;&#32467;&#26524;&#30340; DR &#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#26368;&#20248;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; DR &#26041;&#27861;&#65292;&#29992;&#20110;&#20013;&#38388;&#26465;&#20214;&#32467;&#26524;&#27169;&#22411;&#30340; DR &#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#20248;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#12290;&#21482;&#35201;&#27599;&#20010;&#26292;&#38706;&#26102;&#38388;&#21644;&#27835;&#30103;&#36335;&#24452;&#37117;&#24688;&#24403;&#22320;&#21442;&#25968;&#21270;&#20102;&#33267;&#23569;&#19968;&#20010;&#36741;&#21161;&#20989;&#25968;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21363;&#20351;&#22312;&#38754;&#20020;&#39640;&#32500;&#28151;&#28102;&#21464;&#37327;&#26102;&#20063;&#33021;&#23454;&#29616;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#30340;&#36827;&#27493;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#12290;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#30340;&#20851;&#38190;&#26159;&#25105;&#20204;&#30340;&#26032; DR &#34920;&#31034;&#65292;&#23427;&#22312;&#38656;&#35201;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#24369;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating dynamic treatment effects is a crucial endeavor in causal inference, particularly when confronted with high-dimensional confounders. Doubly robust (DR) approaches have emerged as promising tools for estimating treatment effects due to their flexibility. However, we showcase that the traditional DR approaches that only focus on the DR representation of the expected outcomes may fall short of delivering optimal results. In this paper, we propose a novel DR representation for intermediate conditional outcome models that leads to superior robustness guarantees. The proposed method achieves consistency even with high-dimensional confounders, as long as at least one nuisance function is appropriately parametrized for each exposure time and treatment path. Our results represent a significant step forward as they provide new robustness guarantees. The key to achieving these results is our new DR representation, which offers superior inferential performance while requiring weaker ass
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#22914;&#20309;&#23558;&#20960;&#21313;&#24180;&#30340;&#28145;&#24230;&#23398;&#20064;&#12289;&#26799;&#24230;&#19979;&#38477;&#12289;&#21453;&#21521;&#20256;&#25773;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#30340;&#32463;&#39564;&#25945;&#35757;&#24212;&#29992;&#20110;&#29983;&#29289;&#21487;&#34892;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#33033;&#20914;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#29983;&#29289;&#21487;&#34892;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2109.12894</link><description>&lt;p&gt;
&#20174;&#28145;&#24230;&#23398;&#20064;&#20013;&#27762;&#21462;&#32463;&#39564;&#25945;&#35757;&#65292;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Spiking Neural Networks Using Lessons From Deep Learning. (arXiv:2109.12894v5 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#22914;&#20309;&#23558;&#20960;&#21313;&#24180;&#30340;&#28145;&#24230;&#23398;&#20064;&#12289;&#26799;&#24230;&#19979;&#38477;&#12289;&#21453;&#21521;&#20256;&#25773;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#30340;&#32463;&#39564;&#25945;&#35757;&#24212;&#29992;&#20110;&#29983;&#29289;&#21487;&#34892;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#33033;&#20914;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#29983;&#29289;&#21487;&#34892;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#26159;&#23547;&#25214;&#28789;&#24863;&#20197;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#23436;&#32654;&#20043;&#22320;&#12290;&#25105;&#20204;&#30340;&#31361;&#35302;&#21644;&#31070;&#32463;&#20803;&#30340;&#20869;&#37096;&#36816;&#20316;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#21487;&#33021;&#21576;&#29616;&#30340;&#26679;&#23376;&#30340;&#19968;&#30629;&#12290;&#26412;&#25991;&#26088;&#22312;&#20171;&#32461;&#22914;&#20309;&#23558;&#20960;&#21313;&#24180;&#30340;&#28145;&#24230;&#23398;&#20064;&#12289;&#26799;&#24230;&#19979;&#38477;&#12289;&#21453;&#21521;&#20256;&#25773;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#30340;&#32463;&#39564;&#25945;&#35757;&#24212;&#29992;&#20110;&#29983;&#29289;&#21487;&#34892;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#33033;&#20914;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#24494;&#22937;&#30340;&#30456;&#20114;&#20316;&#29992;&#65307;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#24212;&#29992;&#21040;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476; (SNNs) &#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65307;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#21644;&#33033;&#20914;&#26102;&#24207;&#20381;&#36182;&#24615;&#21487;&#22609;&#24615;&#20043;&#38388;&#24494;&#22937;&#30340;&#32852;&#31995;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#22914;&#20309;&#21521;&#30528;&#29983;&#29289;&#21487;&#34892;&#30340;&#22312;&#32447;&#23398;&#20064;&#21457;&#23637;&#12290;&#19968;&#20123;&#24819;&#27861;&#22312;&#31070;&#32463;&#24418;&#24577;&#24037;&#31243;&#30028;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#21644;&#20351;&#29992;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#22312;&#26412;&#25991;&#20013;&#39318;&#27425;&#21576;&#29616;&#25110;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;SNN&#39046;&#22495;&#20173;&#22788;&#20110;&#24320;&#21457;&#21021;&#26399;&#65292;&#38656;&#35201;&#35299;&#20915;&#35768;&#22810;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#29702;&#35299;&#22823;&#33041;&#21644;&#24320;&#21457;&#26356;&#26377;&#25928;&#21644;&#26234;&#33021;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This paper serves as a tutorial and perspective showing how to apply the lessons learnt from several decades of research in deep learning, gradient descent, backpropagation and neuroscience to biologically plausible spiking neural neural networks.  We also explore the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to spiking neural networks (SNNs); the subtle link between temporal backpropagation and spike timing dependent plasticity, and how deep learning might move towards biologically plausible online learning. Some ideas are well accepted and commonly used amongst the neuromorphic engineering community, while others are presented or justified for the first time here.  The fiel
&lt;/p&gt;</description></item><item><title>MRCpy&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#26368;&#23567;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#40065;&#26834;&#39118;&#38505;&#26368;&#23567;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#21033;&#29992;0-1&#25439;&#22833;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#25552;&#20379;&#20102;&#32039;&#23494;&#30340;&#26399;&#26395;&#25439;&#22833;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2108.01952</link><description>&lt;p&gt;
MRCpy&#65306;&#19968;&#31181;&#29992;&#20110;&#26368;&#23567;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
MRCpy: A Library for Minimax Risk Classifiers. (arXiv:2108.01952v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.01952
&lt;/p&gt;
&lt;p&gt;
MRCpy&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#26368;&#23567;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#40065;&#26834;&#39118;&#38505;&#26368;&#23567;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#21033;&#29992;0-1&#25439;&#22833;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#25552;&#20379;&#20102;&#32039;&#23494;&#30340;&#26399;&#26395;&#25439;&#22833;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29616;&#26377;&#30340;&#30417;&#30563;&#20998;&#31867;&#24211;&#37117;&#26159;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20351;&#29992;&#20195;&#29702;&#25439;&#22833;&#25216;&#26415;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;MRCpy&#24211;&#65292;&#35813;&#24211;&#23454;&#29616;&#20102;&#22522;&#20110;&#40065;&#26834;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26368;&#23567;&#21270;&#39118;&#38505;&#20998;&#31867;&#22120;&#65288;MRC&#65289;&#65292;&#24182;&#21487;&#21033;&#29992;0-1&#25439;&#22833;&#12290;&#36825;&#31181;&#25216;&#26415;&#20135;&#29983;&#20102;&#35768;&#22810;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#32039;&#23494;&#30340;&#26399;&#26395;&#25439;&#22833;&#30028;&#38480;&#12290;MRCpy&#20026;&#19981;&#21516;&#21464;&#37327;&#30340;MRC&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#24182;&#36981;&#24490;&#27969;&#34892;Python&#24211;&#30340;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;MRCpy&#36824;&#25552;&#20379;&#20102;&#23454;&#29616;&#19968;&#20123;&#27969;&#34892;&#25216;&#26415;&#30340;&#21151;&#33021;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#30475;&#20316;&#26159;MRC&#65292;&#20363;&#22914;L1&#27491;&#21017;&#21270;&#36923;&#36753;&#22238;&#24402;&#65292;0-1&#23545;&#25239;&#24615;&#21644;&#26368;&#22823;&#29109;&#26426;&#12290;&#27492;&#22806;&#65292;MRCpy&#36824;&#23454;&#29616;&#20102;&#26368;&#36817;&#30340;&#29305;&#24449;&#26144;&#23556;&#65292;&#22914;&#20613;&#37324;&#21494;&#65292;ReLU&#21644;&#38408;&#20540;&#29305;&#24449;&#12290;&#35813;&#24211;&#37319;&#29992;&#38754;&#21521;&#23545;&#35937;&#30340;&#26041;&#27861;&#35774;&#35745;&#65292;&#26041;&#20415;&#21327;&#20316;&#32773;&#21644;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing libraries for supervised classification implement techniques that are based on empirical risk minimization and utilize surrogate losses. We present MRCpy library that implements minimax risk classifiers (MRCs) that are based on robust risk minimization and can utilize 0-1-loss. Such techniques give rise to a manifold of classification methods that can provide tight bounds on the expected loss. MRCpy provides a unified interface for different variants of MRCs and follows the standards of popular Python libraries. The presented library also provides implementation for popular techniques that can be seen as MRCs such as L1-regularized logistic regression, zero-one adversarial, and maximum entropy machines. In addition, MRCpy implements recent feature mappings such as Fourier, ReLU, and threshold features. The library is designed with an object-oriented approach that facilitates collaborators and users.
&lt;/p&gt;</description></item><item><title>GaNDLF&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#19988;&#26131;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#30340;&#38590;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#28145;&#24230;&#23398;&#20064;&#21487;&#37325;&#22797;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#25193;&#23637;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2103.01006</link><description>&lt;p&gt;
GaNDLF&#65306;&#19968;&#31181;&#36890;&#29992;&#32454;&#33268;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#20013;&#21487;&#25193;&#23637;&#30340;&#31471;&#23545;&#31471;&#20020;&#24202;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
GaNDLF: A Generally Nuanced Deep Learning Framework for Scalable End-to-End Clinical Workflows in Medical Imaging. (arXiv:2103.01006v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.01006
&lt;/p&gt;
&lt;p&gt;
GaNDLF&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#19988;&#26131;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#30340;&#38590;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#28145;&#24230;&#23398;&#20064;&#21487;&#37325;&#22797;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#25193;&#23637;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#31185;&#23398;&#21644;&#20020;&#24202;&#31038;&#21306;&#20013;&#37117;&#26377;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#23454;&#29616;&#30340;&#21487;&#21464;&#24615;&#20250;&#24433;&#21709;&#23427;&#20204;&#30340;&#20877;&#29616;&#24615;&#12289;&#36716;&#21270;&#24615;&#21644;&#37096;&#32626;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;&#31038;&#21306;&#39537;&#21160;&#30340;&#36890;&#29992;&#32454;&#33268;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;GaNDLF&#65289;&#65292;&#26088;&#22312;&#38477;&#20302;&#36825;&#20123;&#38556;&#30861;&#12290; GaNDLF&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#12289;&#35757;&#32451;&#21644;&#25512;&#26029;&#26426;&#21046;&#26356;&#21152;&#31283;&#23450;&#65292;&#21487;&#37325;&#22797;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#24191;&#27867;&#30340;&#25216;&#26415;&#32972;&#26223;&#12290; GaNDLF&#26088;&#22312;&#20026;&#35745;&#31639;&#31934;&#23494;&#21307;&#23398;&#20013;&#25152;&#26377;&#28145;&#24230;&#23398;&#20064;&#30456;&#20851;&#20219;&#21153;&#25552;&#20379;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;GaNDLF&#20998;&#26512;&#25918;&#23556;&#23398;&#21644;&#32452;&#32455;&#23398;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#20855;&#26377;&#20869;&#32622;&#30340;&#25903;&#25345;k-fold&#20132;&#21449;&#39564;&#35777;&#65292;&#25968;&#25454;&#25193;&#20805;&#65292;&#22810;&#31181;&#27169;&#24577;&#21644;&#36755;&#20986;&#31867;&#21035;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#29992;&#20363;&#12289;&#35299;&#21078;&#37096;&#20301;&#21644;&#35745;&#31639;&#20219;&#21153;&#30340;&#23450;&#37327;&#24615;&#33021;&#35780;&#20272;&#25903;&#25345;GaNDLF&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#26368;&#23569;&#30340;&#24615;&#33021;&#25439;&#22833;&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#24635;&#20043;&#65292;GaNDLF&#26159;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#21644;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#24433;&#20687;&#20020;&#24202;&#24037;&#20316;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) has the potential to optimize machine learning in both the scientific and clinical communities. However, greater expertise is required to develop DL algorithms, and the variability of implementations hinders their reproducibility, translation, and deployment. Here we present the community-driven Generally Nuanced Deep Learning Framework (GaNDLF), with the goal of lowering these barriers. GaNDLF makes the mechanism of DL development, training, and inference more stable, reproducible, interpretable, and scalable, without requiring an extensive technical background. GaNDLF aims to provide an end-to-end solution for all DL-related tasks in computational precision medicine. We demonstrate the ability of GaNDLF to analyze both radiology and histology images, with built-in support for k-fold cross-validation, data augmentation, multiple modalities and output classes. Our quantitative performance evaluation on numerous use cases, anatomies, and computational tasks supports G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GraSP-RL&#26694;&#26550;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#20197;&#35299;&#20915;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20013;&#29366;&#24577;&#31354;&#38388;&#38590;&#20197;&#22788;&#29702;&#12289;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2009.03836</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#29983;&#20135;&#35745;&#21010;&#38382;&#39064;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks-based Scheduler for Production planning problems using Reinforcement Learning. (arXiv:2009.03836v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.03836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GraSP-RL&#26694;&#26550;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#20197;&#35299;&#20915;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20013;&#29366;&#24577;&#31354;&#38388;&#38590;&#20197;&#22788;&#29702;&#12289;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#20294;&#23545;&#20110;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#20351;&#29992;&#30690;&#37327;&#21270;&#26426;&#22120;&#29305;&#24449;&#20316;&#20026;&#29366;&#24577;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#65288;1&#65289;&#26426;&#22120;&#21333;&#20803;&#21644;&#20316;&#19994;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#27809;&#26377;&#23436;&#20840;&#25429;&#33719;&#65292;&#65288;2&#65289;&#29366;&#24577;&#31354;&#38388;&#38543;&#30528;&#26426;&#22120;/&#20316;&#19994;&#25968;&#37327;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#65288;3&#65289;&#20195;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GraSP-RL&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#29983;&#20135;&#35745;&#21010;&#38382;&#39064;&#35843;&#24230;&#22120;&#12290;&#23427;&#23558;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#25552;&#21462;&#29305;&#24449;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#34429;&#28982;&#22270;&#24418;&#26412;&#36523;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#65292;&#20294;&#20351;&#29992;GNN&#25552;&#21462;&#30340;&#29305;&#24449;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#24403;&#21069;&#29983;&#20135;&#29366;&#24577;&#30340;&#20016;&#23500;&#32534;&#30721;&#65292;&#28982;&#21518;&#34987;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#29992;&#20110;&#36873;&#25321;&#19979;&#19968;&#20010;&#20316;&#19994;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35843;&#24230;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#38382;&#39064;&#65292;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#23545;&#27492;&#36827;&#34892;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is increasingly adopted in job shop scheduling problems (JSSP). But RL for JSSP is usually done using a vectorized representation of machine features as the state space. It has three major problems: (1) the relationship between the machine units and the job sequence is not fully captured, (2) exponential increase in the size of the state space with increasing machines/jobs, and (3) the generalization of the agent to unseen scenarios. We present a novel framework - GraSP-RL, GRAph neural network-based Scheduler for Production planning problems using Reinforcement Learning. It represents JSSP as a graph and trains the RL agent using features extracted using a graph neural network (GNN). While the graph is itself in the non-euclidean space, the features extracted using the GNNs provide a rich encoding of the current production state in the euclidean space, which is then used by the RL agent to select the next job. Further, we cast the scheduling problem as a de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#34701;&#21512;&#31639;&#27861;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#8220;&#21333;&#27425;&#8221;&#30693;&#35782;&#36801;&#31227;&#65292;&#24182;&#19988;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#27604;&#31616;&#21333;&#24179;&#22343;&#21644;&#38598;&#25104;&#27169;&#22411;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/1910.05653</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model Fusion via Optimal Transport. (arXiv:1910.05653v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.05653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#34701;&#21512;&#31639;&#27861;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#8220;&#21333;&#27425;&#8221;&#30693;&#35782;&#36801;&#31227;&#65292;&#24182;&#19988;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#27604;&#31616;&#21333;&#24179;&#22343;&#21644;&#38598;&#25104;&#27169;&#22411;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#32467;&#21512;&#19981;&#21516;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#33539;&#20363;&#12290;&#23613;&#31649;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#24418;&#25104;&#19968;&#20010;&#27169;&#22411;&#38598;&#21512;&#24182;&#24179;&#22343;&#23427;&#20204;&#30340;&#21508;&#33258;&#39044;&#27979;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#38480;&#21046;&#65288;&#20197;&#20869;&#23384;&#21644;&#35745;&#31639;&#30340;&#26041;&#24335;&#21576;&#32447;&#24615;&#22686;&#38271;&#20110;&#27169;&#22411;&#25968;&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#26080;&#27861;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#34701;&#21512;&#31639;&#27861;&#65292;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26469;&#65288;&#36719;&#65289;&#23545;&#40784;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#20803;&#65292;&#28982;&#21518;&#24179;&#22343;&#23427;&#20204;&#30340;&#30456;&#20851;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#21487;&#20197;&#22312;&#24322;&#26500;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#8220;&#21333;&#27425;&#8221;&#30693;&#35782;&#36801;&#31227;&#65288;&#21363;&#19981;&#38656;&#35201;&#20219;&#20309;&#37325;&#26032;&#35757;&#32451;&#65289;&#22312;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#31034;&#33539;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#31616;&#21333;&#24179;&#22343;&#20197;&#21450;&#22914;&#20309;&#22312;&#26631;&#20934;&#21367;&#31215;&#32593;&#32476;&#65288;&#22914;VGG11&#65289;&#12289;&#27531;&#24046;&#32593;&#32476;&#19978;&#36827;&#34892;&#24555;&#36895;&#20248;&#21270;&#26367;&#20195;&#38598;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining different models is a widely used paradigm in machine learning applications. While the most common approach is to form an ensemble of models and average their individual predictions, this approach is often rendered infeasible by given resource constraints in terms of memory and computation, which grow linearly with the number of models. We present a layer-wise model fusion algorithm for neural networks that utilizes optimal transport to (soft-) align neurons across the models before averaging their associated parameters.  We show that this can successfully yield "one-shot" knowledge transfer (i.e, without requiring any retraining) between neural networks trained on heterogeneous non-i.i.d. data. In both i.i.d. and non-i.i.d. settings , we illustrate that our approach significantly outperforms vanilla averaging, as well as how it can serve as an efficient replacement for the ensemble with moderate fine-tuning, for standard convolutional networks (like VGG11), residual networks
&lt;/p&gt;</description></item><item><title>S-ConvNet&#21644;All-ConvNet&#27169;&#22411;&#20026;&#31070;&#32463;&#32908;&#32905;&#27963;&#21160;&#35782;&#21035;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#65292;&#34920;&#29616;&#20986;&#38750;&#24120;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/1906.03381</link><description>&lt;p&gt;
S-ConvNet: &#19968;&#31181;&#28145;&#24230;&#34920;&#38754;&#32908;&#30005;&#22270;&#20687;&#31070;&#32463;&#32908;&#32905;&#27963;&#21160;&#35782;&#21035;&#30340;&#27973;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
S-ConvNet: A Shallow Convolutional Neural Network Architecture for Neuromuscular Activity Recognition Using Instantaneous High-Density Surface EMG Images. (arXiv:1906.03381v1 [eess.SP] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1906.03381
&lt;/p&gt;
&lt;p&gt;
S-ConvNet&#21644;All-ConvNet&#27169;&#22411;&#20026;&#31070;&#32463;&#32908;&#32905;&#27963;&#21160;&#35782;&#21035;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#65292;&#34920;&#29616;&#20986;&#38750;&#24120;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#30636;&#26102;&#39640;&#23494;&#24230;&#34920;&#38754;&#32908;&#30005;&#22270;&#20687;&#36827;&#34892;&#31070;&#32463;&#32908;&#32905;&#27963;&#21160;&#35782;&#21035;&#30340;&#27010;&#24565;&#20026;&#24320;&#21457;&#26356;&#27969;&#30021;&#21644;&#33258;&#28982;&#30340;&#32908;&#32905;-&#35745;&#31639;&#26426;&#25509;&#21475;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#38750;&#24120;&#22823;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ConvNet)&#26550;&#26500;&#21644;&#22797;&#26434;&#30340;&#35757;&#32451;&#26041;&#26696;&#26469;&#36827;&#34892;HD-sEMG&#22270;&#20687;&#35782;&#21035;&#65292;&#38656;&#35201;&#22312;&#38750;&#24120;&#22823;&#35268;&#27169;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#32593;&#32476;&#26550;&#26500;&#65292;&#22240;&#27492;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#21363;&#26102;HD-sEMG&#22270;&#20687;&#30340;&#31070;&#32463;&#32908;&#32905;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#8212;&#8212;S-ConvNet&#21644;All-ConvNet&#27169;&#22411;&#12290;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;S-ConvNet&#21644;All-ConvNet&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#27604;&#22522;&#20110;&#30636;&#26102;HD-sEMG&#22270;&#20687;&#30340;&#31070;&#32463;&#32908;&#32905;&#27963;&#21160;&#35782;&#21035;&#30340;&#26356;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of neuromuscular activity recognition using instantaneous high-density surface electromyography (HD-sEMG) images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the existing approaches employed a very large deep convolutional neural network (ConvNet) architecture and complex training schemes for HD-sEMG image recognition, which requires the network architecture to be pre-trained on a very large-scale labeled training dataset, as a result, it makes computationally very expensive. To overcome this problem, we propose S-ConvNet and All-ConvNet models, a simple yet efficient framework for learning instantaneous HD-sEMG images from scratch for neuromuscular activity recognition. Without using any pre-trained models, our proposed S-ConvNet and All-ConvNet demonstrate very competitive recognition accuracy to the more complex state of the art for neuromuscular activity recognition based on instantaneous HD-sEMG images, while u
&lt;/p&gt;</description></item></channel></rss>