<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;QM9&#25968;&#25454;&#38598;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#25968;&#25454;&#37319;&#26679;&#26041;&#27861;&#26469;&#36873;&#25321;&#26377;&#25928;&#30340;&#35757;&#32451;&#38598;&#65292;&#20877;&#19982;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#26368;&#22823;&#21270;&#20998;&#23376;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10066</link><description>&lt;p&gt;
&#12300;&#23376;&#38598;&#36873;&#25321;&#19982;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20114;&#20316;&#29992;&#12301;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Interplay of Subset Selection and Informed Graph Neural Networks. (arXiv:2306.10066v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10066
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;QM9&#25968;&#25454;&#38598;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#25968;&#25454;&#37319;&#26679;&#26041;&#27861;&#26469;&#36873;&#25321;&#26377;&#25928;&#30340;&#35757;&#32451;&#38598;&#65292;&#20877;&#19982;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#26368;&#22823;&#21270;&#20998;&#23376;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32467;&#21512;&#28023;&#37327;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#26174;&#33879;&#25552;&#39640;&#20102;&#25105;&#20204;&#25506;&#31350;&#21270;&#21512;&#29289;&#31354;&#38388;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#24555;&#36895;&#20934;&#30830;&#22320;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#22823;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#21463;&#21040;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#21487;&#33021;&#36824;&#27809;&#26377;&#34987;&#26631;&#35760;&#65292;&#29983;&#25104;&#26631;&#35760;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#65292;&#20363;&#22914;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#20174;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#28857;&#27744;&#20013;&#36873;&#25321;&#23567;&#35757;&#32451;&#23376;&#38598;&#65292;&#24182;&#24320;&#21457;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#23567;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#12290;&#26412;&#25991;&#38598;&#20013;&#20110;&#39044;&#27979; QM9 &#25968;&#25454;&#38598;&#20013;&#20998;&#23376;&#30340;&#21407;&#23376;&#21270;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#37319;&#29992;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#25968;&#25454;&#37319;&#26679;&#26041;&#27861;&#26469;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#38598;&#36873;&#25321;&#19982;&#36890;&#30693;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#26368;&#22823;&#21270;&#20998;&#23376;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#20998;&#23376;&#25968;&#25454;&#35774;&#35745;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques paired with the availability of massive datasets dramatically enhance our ability to explore the chemical compound space by providing fast and accurate predictions of molecular properties. However, learning on large datasets is strongly limited by the availability of computational resources and can be infeasible in some scenarios. Moreover, the instances in the datasets may not yet be labelled and generating the labels can be costly, as in the case of quantum chemistry computations. Thus, there is a need to select small training subsets from large pools of unlabelled data points and to develop reliable ML methods that can effectively learn from small training sets. This work focuses on predicting the molecules atomization energy in the QM9 dataset. We investigate the advantages of employing domain knowledge-based data sampling methods for an efficient training set selection combined with informed ML techniques. In particular, we show how maximizing molecular
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#32467;&#26500;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#19981;&#26159;&#21333;&#19968;&#33021;&#21147;&#65292;&#32780;&#26159;&#30001;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#26680;&#24515;&#35821;&#35328;&#24314;&#27169;&#31561;&#19977;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#32032;&#32452;&#25104;&#65292;&#24182;&#19988;&#36825;&#19977;&#20010;&#33021;&#21147;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#22823;&#37096;&#20998;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.10062</link><description>&lt;p&gt;
&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Revealing the structure of language model capabilities. (arXiv:2306.10062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#32467;&#26500;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#19981;&#26159;&#21333;&#19968;&#33021;&#21147;&#65292;&#32780;&#26159;&#30001;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#26680;&#24515;&#35821;&#35328;&#24314;&#27169;&#31561;&#19977;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#32032;&#32452;&#25104;&#65292;&#24182;&#19988;&#36825;&#19977;&#20010;&#33021;&#21147;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#22823;&#37096;&#20998;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#23545;&#20110;&#25105;&#20204;&#39044;&#27979;&#21644;&#35299;&#37322;&#36825;&#20123;&#31995;&#32479;&#30340;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#21508;&#31181;LLMs&#30340;&#20010;&#20307;&#24046;&#24322;&#27169;&#24335;&#20013;&#25552;&#21462;&#28508;&#22312;&#33021;&#21147;&#26469;&#35843;&#26597;LLMs&#33021;&#21147;&#30340;&#32467;&#26500;&#12290;&#20351;&#29992;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#22240;&#23376;&#20998;&#26512;&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26469;&#33258;29&#20010;&#19981;&#21516;LLMs&#30340;27&#31181;&#35748;&#30693;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#33021;&#21147;&#24182;&#38750;&#21333;&#19968;&#30340;&#65292;&#30456;&#21453;&#65292;&#23427;&#20204;&#26356;&#22909;&#22320;&#30001;&#19977;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#32032;&#35299;&#37322;&#65292;&#20998;&#21035;&#20195;&#34920;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#26680;&#24515;&#35821;&#35328;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#19977;&#20010;&#22240;&#32032;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#39640;&#27604;&#20363;&#26041;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#19981;&#21516;LLMs&#33021;&#21147;&#30340;&#19968;&#33268;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#33021;&#21147;&#30340;&#22810;&#26041;&#38754;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36825;&#19977;&#20010;&#21151;&#33021;&#19982;&#27169;&#22411;&#23646;&#24615;&#20855;&#26377;&#19981;&#21516;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a theoretical understanding of the capabilities of large language models (LLMs) is vital for our ability to predict and explain the behavior of these systems. Here, we investigate the structure of LLM capabilities by extracting latent capabilities from patterns of individual differences across a varied population of LLMs. Using a combination of Bayesian and frequentist factor analysis, we analyzed data from 29 different LLMs across 27 cognitive tasks. We found evidence that LLM capabilities are not monolithic. Instead, they are better explained by three well-delineated factors that represent reasoning, comprehension and core language modeling. Moreover, we found that these three factors can explain a high proportion of the variance in model performance. These results reveal a consistent structure in the capabilities of different LLMs and demonstrate the multifaceted nature of these capabilities. We also found that the three abilities show different relationships to model prope
&lt;/p&gt;</description></item><item><title>MUBen&#35780;&#20272;&#19981;&#21516;&#39592;&#24178;&#21644;UQ&#27169;&#22411;&#32452;&#21512;&#23545;&#20998;&#23376;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#36807;&#25311;&#21512;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10060</link><description>&lt;p&gt;
MUBen&#65306;&#35780;&#20272;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction. (arXiv:2306.10060v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10060
&lt;/p&gt;
&lt;p&gt;
MUBen&#35780;&#20272;&#19981;&#21516;&#39592;&#24178;&#21644;UQ&#27169;&#22411;&#32452;&#21512;&#23545;&#20998;&#23376;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#36807;&#25311;&#21512;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#20110;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#20998;&#23376;&#25968;&#25454;&#30340;&#22823;&#22411;Transformer&#27169;&#22411;&#22312;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#24494;&#35843;&#26399;&#38388;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#36807;&#24230;&#33258;&#20449;&#39044;&#27979;&#33853;&#22312;&#20102;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#39044;&#27979;&#26657;&#20934;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;UQ&#26041;&#27861;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#26041;&#27861;&#37117;&#20250;&#23548;&#33268;&#24615;&#33021;&#25913;&#21892;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#20351;&#29992;UQ&#26469;&#25913;&#21892;&#20998;&#23376;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#36873;&#25321;&#36866;&#21512;&#30340;&#39592;&#24178;&#21644;UQ&#26041;&#27861;&#20197;&#21487;&#38752;&#22320;&#20272;&#35745;&#20998;&#23376;&#19981;&#30830;&#23450;&#24615;&#30340;&#36807;&#31243;&#20173;&#28982;&#26159;&#26410;&#32463;&#25506;&#32034;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MUBen&#65292;&#35780;&#20272;&#19981;&#21516;&#30340;&#39592;&#24178;&#21644;UQ&#27169;&#22411;&#32452;&#21512;&#65292;&#20197;&#37327;&#21270;&#23427;&#20204;&#22312;&#23646;&#24615;&#39044;&#27979;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24494;&#35843;&#20351;&#29992;&#19981;&#21516;&#20998;&#23376;&#25551;&#36848;&#31526;&#30340;&#21508;&#31181;&#39592;&#24178;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Transformer models pre-trained on massive unlabeled molecular data have shown great success in predicting molecular properties. However, these models can be prone to overfitting during fine-tuning, resulting in over-confident predictions on test data that fall outside of the training distribution. To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions. Although many UQ approaches exist, not all of them lead to improved performance. While some studies have used UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored. To address this gap, we present MUBen, which evaluates different combinations of backbone and UQ models to quantify their performance for both property prediction and uncertainty estimation. By fine-tuning various backbone molecular representation models using different molecular descrip
&lt;/p&gt;</description></item><item><title>EM-Network&#26159;&#19968;&#31181;&#33258;&#25105;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#35861;&#25351;&#23548;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#36827;&#34892;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#65292;&#24182;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10058</link><description>&lt;p&gt;
EM-Network: &#29992;&#20110;&#24207;&#21015;&#23398;&#20064;&#30340;&#33258;&#36523;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EM-Network: Oracle Guided Self-distillation for Sequence Learning. (arXiv:2306.10058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10058
&lt;/p&gt;
&lt;p&gt;
EM-Network&#26159;&#19968;&#31181;&#33258;&#25105;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#35861;&#25351;&#23548;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#36827;&#34892;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#65292;&#24182;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;EM-Network&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#36827;&#34892;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#23398;&#20064;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#26159;&#22312;&#26469;&#33258;&#30446;&#26631;&#24207;&#21015;&#30340;&#8220;&#31070;&#35861;&#25351;&#23548;&#8221;&#19979;&#35757;&#32451;&#30340;&#12290;&#30001;&#20110;&#31070;&#35861;&#25351;&#23548;&#32039;&#20945;&#22320;&#34920;&#31034;&#20102;&#30446;&#26631;&#26041;&#38754;&#30340;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#24110;&#21161;&#24207;&#21015;&#27169;&#22411;&#35299;&#20915;&#20219;&#21153;&#65292;&#22240;&#27492;&#19982;&#20165;&#20351;&#29992;&#28304;&#36755;&#20837;&#30456;&#27604;&#65292;EM-Network&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20351;&#24207;&#21015;&#27169;&#22411;&#32487;&#25215;EM-Network&#30340;&#26377;&#21069;&#36884;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#33976;&#39311;&#31574;&#30053;&#65292;&#21407;&#22987;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#20010;&#38454;&#27573;&#20013;&#20174;EM-Network&#30340;&#30693;&#35782;&#20013;&#33719;&#30410;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;seq2seq&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#32508;&#21512;&#23454;&#39564;&#65306;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#21644;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;AED&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EM-Network&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce EM-Network, a novel self-distillation approach that effectively leverages target information for supervised sequence-to-sequence (seq2seq) learning. In contrast to conventional methods, it is trained with oracle guidance, which is derived from the target sequence. Since the oracle guidance compactly represents the target-side context that can assist the sequence model in solving the task, the EM-Network achieves a better prediction compared to using only the source input. To allow the sequence model to inherit the promising capability of the EM-Network, we propose a new self-distillation strategy, where the original sequence model can benefit from the knowledge of the EM-Network in a one-stage manner. We conduct comprehensive experiments on two types of seq2seq models: connectionist temporal classification (CTC) for speech recognition and attention-based encoder-decoder (AED) for machine translation. Experimental results demonstrate that the EM-Network significantly advanc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25551;&#36848;&#21644;&#35780;&#20272;&#20102;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#33258;&#21160;&#21019;&#24314;&#36807;&#28193;&#26230;&#26684;&#21333;&#20803;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26230;&#26684;&#32467;&#26500;&#20013;&#19981;&#21516;&#25299;&#25169;&#30340;&#21333;&#20803;&#26230;&#26684;&#20043;&#38388;&#24179;&#28369;&#36716;&#25442;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10055</link><description>&lt;p&gt;
&#24179;&#28369;&#31895;&#31961;&#30340;&#36793;&#32536;&#65306;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#22797;&#21512;&#26230;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Smoothing the Rough Edges: Evaluating Automatically Generated Multi-Lattice Transitions. (arXiv:2306.10055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25551;&#36848;&#21644;&#35780;&#20272;&#20102;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#33258;&#21160;&#21019;&#24314;&#36807;&#28193;&#26230;&#26684;&#21333;&#20803;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26230;&#26684;&#32467;&#26500;&#20013;&#19981;&#21516;&#25299;&#25169;&#30340;&#21333;&#20803;&#26230;&#26684;&#20043;&#38388;&#24179;&#28369;&#36716;&#25442;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28155;&#21152;&#21046;&#36896;&#25216;&#26415;&#22312;&#28385;&#36275;&#22797;&#26434;&#35774;&#35745;&#35201;&#27714;&#30340;&#21516;&#26102;&#65292;&#29983;&#20135;&#36731;&#37327;&#32423;&#30340;&#37096;&#20214;&#19978;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#20248;&#21183;&#12290;&#24341;&#20837;&#21333;&#20803;&#26230;&#26684;&#21333;&#20803;&#21644;&#36825;&#20123;&#21333;&#20803;&#30340;&#28176;&#21464;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#37096;&#20214;&#21152;&#36733;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#21333;&#20803;&#26230;&#26684;&#31867;&#22411;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#65292;&#36825;&#23558;&#23548;&#33268;&#22810;&#26230;&#26684;&#32467;&#26500;&#12290;&#22312;&#36825;&#31181;&#32467;&#26500;&#20013;&#65292;&#21333;&#20803;&#26230;&#26684;&#25299;&#25169;&#20043;&#38388;&#30340;&#31361;&#28982;&#36716;&#25442;&#21487;&#33021;&#20250;&#23548;&#33268;&#24212;&#21147;&#38598;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive manufacturing is advantageous for producing lightweight components while addressing complex design requirements. This capability has been bolstered by the introduction of unit lattice cells and the gradation of those cells. In cases where loading varies throughout a part, it may be beneficial to use multiple, distinct lattice cell types, resulting in multi-lattice structures. In such structures, abrupt transitions between unit cell topologies may cause stress concentrations, making the boundary between unit cell types a primary failure point. Thus, these regions require careful design in order to ensure the overall functionality of the part. Although computational design approaches have been proposed, smooth transition regions are still difficult to achieve, especially between lattices of drastically different topologies. This work demonstrates and assesses a method for using variational autoencoders to automate the creation of transitional lattice cells, examining the factors
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#34913;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#32500;&#25345;&#25910;&#30410;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.10050</link><description>&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#25554;&#20540;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Interpolating Item and User Fairness in Recommendation Systems. (arXiv:2306.10050v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#34913;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#32500;&#25345;&#25910;&#30410;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#36793;&#24179;&#21488;&#20013;&#65292;&#24179;&#21488;&#19982;&#21334;&#23478;&#65288;&#39033;&#30446;&#65289;&#21644;&#23458;&#25143;&#65288;&#29992;&#25143;&#65289;&#31561;&#21508;&#31181;&#21508;&#26679;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#20114;&#21160;&#65292;&#27599;&#20010;&#30456;&#20851;&#32773;&#37117;&#26377;&#33258;&#24049;&#30340;&#26399;&#26395;&#32467;&#26524;&#65292;&#23547;&#25214;&#21512;&#36866;&#30340;&#24179;&#34913;&#28857;&#21464;&#24471;&#38750;&#24120;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#8220;&#20844;&#24179;&#25104;&#26412;&#8221;&#65292;&#23427;&#25429;&#25417;&#20102;&#24179;&#21488;&#22312;&#24179;&#34913;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#21033;&#30410;&#26102;&#21487;&#33021;&#20570;&#20986;&#30340;&#22949;&#21327;&#12290;&#20986;&#20110;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#25512;&#33616;&#26694;&#26550;&#65292;&#20854;&#20013;&#24179;&#21488;&#22312;&#25554;&#20540;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#32422;&#26463;&#26102;&#26368;&#22823;&#21270;&#20854;&#25910;&#30410;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26356;&#29616;&#23454;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22312;&#32447;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20844;&#24179;&#25512;&#33616;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24179;&#21488;&#32570;&#20047;&#20102;&#35299;&#29992;&#25143;&#20559;&#22909;&#30340;&#30693;&#35782;&#65292;&#21482;&#33021;&#35266;&#23519;&#20108;&#36827;&#21046;&#36141;&#20080;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#22312;&#32500;&#25252;&#24179;&#21488;&#25910;&#30410;&#30340;&#21516;&#26102;&#31649;&#29702;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#21516;&#26102;&#20445;&#25345;&#39640;&#25910;&#30410;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online platforms employ recommendation systems to enhance customer engagement and drive revenue. However, in a multi-sided platform where the platform interacts with diverse stakeholders such as sellers (items) and customers (users), each with their own desired outcomes, finding an appropriate middle ground becomes a complex operational challenge. In this work, we investigate the ``price of fairness'', which captures the platform's potential compromises when balancing the interests of different stakeholders. Motivated by this, we propose a fair recommendation framework where the platform maximizes its revenue while interpolating between item and user fairness constraints. We further examine the fair recommendation problem in a more realistic yet challenging online setting, where the platform lacks knowledge of user preferences and can only observe binary purchase decisions. To address this, we design a low-regret online optimization algorithm that preserves the platform's revenue while
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.10045</link><description>&lt;p&gt;
&#39044;&#27979;&#26230;&#20307;&#24615;&#36136;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#39640;&#25928;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#39044;&#27979;&#12290;&#26230;&#20307;&#32467;&#26500;&#30001;&#19968;&#20010;&#26368;&#23567;&#30340;&#21333;&#20803;&#26684;&#32452;&#25104;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26080;&#38480;&#37325;&#22797;&#12290;&#22914;&#20309;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20934;&#30830;&#34920;&#31034;&#36825;&#31181;&#37325;&#22797;&#32467;&#26500;&#20173;&#28982;&#27809;&#26377;&#35299;&#20915;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#22312;&#38468;&#36817;&#30340;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#36793;&#32536;&#26469;&#26500;&#24314;&#22270;&#24418;&#65292;&#22240;&#27492;&#26080;&#27861;&#24544;&#23454;&#22320;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#30340;&#27169;&#24335;&#21644;&#36828;&#36317;&#31163;&#30340;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21019;&#26032;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#30452;&#25509;&#24314;&#27169;&#29289;&#29702;&#21407;&#29702;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#20351;&#29992;&#36317;&#31163;&#65292;&#22914;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#25152;&#20570;&#30340;&#12290;&#36825;&#20123;&#21183;&#21253;&#25324;&#24211;&#20177;&#21183;&#65292;&#20262;&#25958;&#20998;&#25955;&#21183;&#21644;Pauli&#26021;&#21147;&#21183;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#27169;&#25152;&#26377;&#21407;&#23376;&#20043;&#38388;&#30340;&#23436;&#25972;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38468;&#36817;&#21407;&#23376;&#20043;&#38388;&#30340;&#21183;&#12290;&#36825;&#24471;&#30410;&#20110;&#25105;&#20204;&#29992;&#21487;&#35777;&#26126;&#30340;&#35823;&#24046;&#30028;&#36924;&#36817;&#26080;&#38480;&#21183;&#21644;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#20174;&#20215;&#20540;&#35266;&#12289;&#25968;&#25454;&#32452;&#25104;&#21644;&#36164;&#28304;&#22522;&#30784;&#35774;&#26045;&#19977;&#20010;&#35282;&#24230;&#20837;&#25163;&#24182;&#25351;&#20986;&#23427;&#20204;&#30456;&#20114;&#20381;&#23384;&#65292;&#38656;&#35201;&#19968;&#21516;&#32771;&#34385;&#35299;&#20915;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25581;&#31034;&#20102;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22914;&#26435;&#21147;&#38598;&#20013;&#21644;&#20381;&#36182;&#24615;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2306.10043</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#24322;&#36136;&#24615;&#30340;&#20132;&#32455;&#36724;&#35299;&#26512;&#20197;&#20419;&#36827;&#27665;&#20027;&#21644;&#21253;&#23481;&#24615;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Interconnected Axes of Heterogeneity in Machine Learning for Democratic and Inclusive Advancements. (arXiv:2306.10043v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#20174;&#20215;&#20540;&#35266;&#12289;&#25968;&#25454;&#32452;&#25104;&#21644;&#36164;&#28304;&#22522;&#30784;&#35774;&#26045;&#19977;&#20010;&#35282;&#24230;&#20837;&#25163;&#24182;&#25351;&#20986;&#23427;&#20204;&#30456;&#20114;&#20381;&#23384;&#65292;&#38656;&#35201;&#19968;&#21516;&#32771;&#34385;&#35299;&#20915;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25581;&#31034;&#20102;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22914;&#26435;&#21147;&#38598;&#20013;&#21644;&#20381;&#36182;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#21457;&#20102;&#26377;&#20851;&#20854;&#23545;&#31038;&#20250;&#30340;&#21033;&#30410;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25351;&#20986;&#24182;&#20998;&#26512;&#20102;&#19977;&#20010;&#26126;&#26174;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#20135;&#21697;&#21457;&#23637;&#36712;&#36857;&#30340;&#24322;&#36136;&#24615;&#36724;&#65292;&#23427;&#20204;&#26159;&#65306;&#20215;&#20540;&#35266;&#12289;&#25991;&#21270;&#21644;&#27861;&#35268;&#12289;&#25968;&#25454;&#32452;&#25104;&#20197;&#21450;&#36164;&#28304;&#21644;&#22522;&#30784;&#35774;&#26045;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#36724;&#22914;&#20309;&#30456;&#20114;&#20381;&#23384;&#24182;&#30456;&#20114;&#24433;&#21709;&#65292;&#24378;&#35843;&#38656;&#35201;&#20849;&#21516;&#32771;&#34385;&#21644;&#35299;&#20915;&#23427;&#20204;&#30340;&#24517;&#35201;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#26223;&#35266;&#22312;&#36825;&#26041;&#38754;&#36824;&#26377;&#25152;&#19981;&#36275;&#65292;&#24448;&#24448;&#26410;&#33021;&#37319;&#29992;&#25972;&#20307;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#20559;&#21521;&#23569;&#25968;&#20154;&#30340;&#25903;&#37197;&#24615;&#20559;&#35265;&#21644;&#26041;&#27861;&#23398;&#65292;&#20197;&#21450;&#30001;&#27492;&#23548;&#33268;&#30340;&#26435;&#21147;&#38598;&#20013;&#12289;&#21516;&#36136;&#21270;&#25511;&#21046;&#21644;&#22686;&#21152;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#19977;&#20010;&#36724;&#30340;&#29255;&#38754;&#30740;&#31350;&#26500;&#25104;&#30340;&#26174;&#33879;&#25361;&#25112;&#65292;&#23548;&#33268;&#19968;&#20010;&#32570;&#20047;&#21453;&#26144;&#29616;&#23454;&#24773;&#20917;&#30340;&#23454;&#38469;&#35299;&#33616;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing utilization of machine learning (ML) in decision-making processes raises questions about its benefits to society. In this study, we identify and analyze three axes of heterogeneity that significantly influence the trajectory of ML products. These axes are i) values, culture and regulations, ii) data composition, and iii) resource and infrastructure capacity. We demonstrate how these axes are interdependent and mutually influence one another, emphasizing the need to consider and address them jointly. Unfortunately, the current research landscape falls short in this regard, often failing to adopt a holistic approach. We examine the prevalent practices and methodologies that skew these axes in favor of a selected few, resulting in power concentration, homogenized control, and increased dependency. We discuss how this fragmented study of the three axes poses a significant challenge, leading to an impractical solution space that lacks reflection of real-world scenarios. Addressi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#65292;&#37319;&#29992;Transformer&#27169;&#22411;&#27169;&#25311;&#21830;&#19994;&#36712;&#36857;&#65292;&#25581;&#31034;&#21830;&#19994;&#36235;&#21183;&#21644;&#34892;&#19994;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.10034</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#26102;&#31354;&#25968;&#25454;&#20998;&#26512;&#22312;&#25581;&#31034;&#21830;&#19994;&#36712;&#36857;&#26041;&#38754;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unlocking Insights into Business Trajectories with Transformer-based Spatio-temporal Data Analysis. (arXiv:2306.10034v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#65292;&#37319;&#29992;Transformer&#27169;&#22411;&#27169;&#25311;&#21830;&#19994;&#36712;&#36857;&#65292;&#25581;&#31034;&#21830;&#19994;&#36235;&#21183;&#21644;&#34892;&#19994;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#19990;&#30028;&#25345;&#32493;&#21457;&#23637;&#65292;&#20445;&#25345;&#39046;&#20808;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#24066;&#22330;&#36235;&#21183;&#21644;&#19994;&#32489;&#12290;&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#21830;&#19994;&#36712;&#36857;&#65292;&#21033;&#29992;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#26469;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The world of business is constantly evolving and staying ahead of the curve requires a deep understanding of market trends and performance. This article addresses this requirement by modeling business trajectories using news articles data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#36328;&#36234;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#23398;&#31185;&#30340;&#19971;&#20010;&#20250;&#35758;&#20013;&#65292;&#28304;&#20195;&#30721;&#21487;&#29992;&#24615;&#23545;&#20110;Interspeech&#20250;&#35758;&#35201;&#23569;&#20110;40%&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#35758;&#20197;&#25552;&#39640;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10033</link><description>&lt;p&gt;
&#22312;Interspeech&#20250;&#35758;&#19978;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65306;&#19968;&#31181;&#38271;&#26399;&#21644;&#27604;&#36739;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective. (arXiv:2306.10033v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10033
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#36328;&#36234;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#23398;&#31185;&#30340;&#19971;&#20010;&#20250;&#35758;&#20013;&#65292;&#28304;&#20195;&#30721;&#21487;&#29992;&#24615;&#23545;&#20110;Interspeech&#20250;&#35758;&#35201;&#23569;&#20110;40%&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#35758;&#20197;&#25552;&#39640;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37325;&#22797;&#24615;&#26159;&#27178;&#36328;&#23398;&#31185;&#30340;&#31185;&#23398;&#36827;&#23637;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#38477;&#20302;&#24320;&#25918;&#31185;&#23398;&#30340;&#38556;&#30861;&#26159;Interspeech 2023&#20027;&#39064;&#30340;&#28966;&#28857;&#39046;&#22495;&#12290;&#28304;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#26159;&#20419;&#36827;&#21487;&#37325;&#22797;&#24615;&#30340;&#25351;&#26631;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#39046;&#22495;&#20869;&#20854;&#20182;&#20250;&#35758;&#30456;&#27604;&#65292;Interspeech&#20250;&#35758;&#30340;&#20877;&#29616;&#29575;&#36739;&#20302;&#26159;&#25105;&#20204;&#40092;&#26377;&#20102;&#35299;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#36328;&#36234;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#23398;&#31185;&#30340;&#19971;&#20010;&#20250;&#35758;&#30340;27,717&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#25509;&#21463;&#30340;&#35770;&#25991;&#25968;&#37327;&#19982;&#20854;&#20182;&#20250;&#35758;&#30456;&#36817;&#65292;&#20294;Interspeech&#30340;&#21487;&#29992;&#28304;&#20195;&#30721;&#36739;&#23569;&#65292;&#36798;&#21040;40%&#12290;&#38500;&#20102;&#25253;&#21578;&#25105;&#20204;&#22312;&#30740;&#31350;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#24314;&#35758;&#21644;&#21487;&#33021;&#30340;&#26041;&#21521;&#65292;&#20197;&#22686;&#21152;&#21487;&#37325;&#22797;&#24615;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducibility is a key aspect for scientific advancement across disciplines, and reducing barriers for open science is a focus area for the theme of Interspeech 2023. Availability of source code is one of the indicators that facilitates reproducibility. However, less is known about the rates of reproducibility at Interspeech conferences in comparison to other conferences in the field. In order to fill this gap, we have surveyed 27,717 papers at seven conferences across speech and language processing disciplines. We find that despite having a close number of accepted papers to the other conferences, Interspeech has up to 40% less source code availability. In addition to reporting the difficulties we have encountered during our research, we also provide recommendations and possible directions to increase reproducibility for further studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoHHGN+&#65292;&#29992;&#20110;&#35299;&#20915;&#32570;&#20047;&#29992;&#25143;ID&#30340;&#30005;&#21830;&#32593;&#31449;&#25968;&#25454;&#30340;&#25512;&#33616;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#23450;&#20041;&#30340;&#20266;&#20250;&#35805;&#20197;&#21450;&#21253;&#25324;&#20215;&#26684;&#12289;&#31867;&#21035;&#12289;&#24615;&#21035;&#21644;&#22320;&#21306;&#31561;&#29992;&#25143;&#20449;&#24687;&#65292;&#24471;&#21040;&#20102;&#36739;&#22909;&#30340;&#25512;&#33616;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10029</link><description>&lt;p&gt;
&#37319;&#29992;&#20998;&#23618;&#23884;&#20837;&#21644;&#20250;&#35805;&#23646;&#24615;&#30340;&#20266;&#20250;&#35805;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Pseudo session-based recommendation with hierarchical embedding and session attributes. (arXiv:2306.10029v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoHHGN+&#65292;&#29992;&#20110;&#35299;&#20915;&#32570;&#20047;&#29992;&#25143;ID&#30340;&#30005;&#21830;&#32593;&#31449;&#25968;&#25454;&#30340;&#25512;&#33616;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#23450;&#20041;&#30340;&#20266;&#20250;&#35805;&#20197;&#21450;&#21253;&#25324;&#20215;&#26684;&#12289;&#31867;&#21035;&#12289;&#24615;&#21035;&#21644;&#22320;&#21306;&#31561;&#29992;&#25143;&#20449;&#24687;&#65292;&#24471;&#21040;&#20102;&#36739;&#22909;&#30340;&#25512;&#33616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#26080;&#27861;&#20026;&#27599;&#20010;&#20132;&#26131;&#25968;&#25454;&#26465;&#30446;&#25552;&#20379;&#26631;&#35782;&#21495;&#65288;&#29992;&#25143;ID&#65289;&#12290;&#22240;&#20026;&#22823;&#22810;&#25968;&#25512;&#33616;&#26041;&#27861;&#20551;&#23450;&#25152;&#26377;&#25968;&#25454;&#37117;&#34987;&#20998;&#37197;&#20102;&#29992;&#25143;ID&#65292;&#25152;&#20197;&#23427;&#20204;&#19981;&#33021;&#24212;&#29992;&#20110;&#27809;&#26377;&#29992;&#25143;ID&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#30740;&#31350;&#20102;&#22522;&#20110;&#20250;&#35805;&#20449;&#24687;&#30340;&#20250;&#35805;&#25512;&#33616;&#65288;SBR&#65289;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#29992;&#25143;&#30340;&#30701;&#26399;&#34892;&#20026;&#20449;&#24687;&#12290;&#24120;&#35268;&#30340;SBR&#21482;&#20351;&#29992;&#19982;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#30456;&#20851;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#25512;&#33616;&#65288;&#20363;&#22914;&#65292;&#22312;EC&#31449;&#28857;&#19978;&#20351;&#29992;&#39033;&#30446;ID&#65289;&#12290;&#29305;&#21035;&#26159;&#22312;EC&#32593;&#31449;&#30340;&#24773;&#20917;&#19979;&#65292;&#35760;&#24405;&#30340;&#25968;&#25454;&#21253;&#25324;&#34987;&#36141;&#20080;&#30340;&#29289;&#21697;&#21517;&#31216;&#12289;&#29289;&#21697;&#20215;&#26684;&#12289;&#31867;&#21035;&#23618;&#27425;&#32467;&#26500;&#20197;&#21450;&#29992;&#25143;&#30340;&#24615;&#21035;&#21644;&#22320;&#21306;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20026;&#27809;&#26377;&#29992;&#25143;ID&#21644;&#20250;&#35805;ID&#30340;EC&#32593;&#31449;&#30340;&#36141;&#20080;&#21382;&#21490;&#25968;&#25454;&#23450;&#20041;&#20102;&#20266;&#20250;&#35805;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;CoHHGN+&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#21327;&#21516;&#23548;&#21521;&#30340;&#24322;&#26500;&#36229;&#22270;&#21644;&#20840;&#23616;&#22270;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, electronic commerce (EC) websites have been unable to provide an identification number (user ID) for each transaction data entry because of privacy issues. Because most recommendation methods assume that all data are assigned a user ID, they cannot be applied to the data without user IDs. Recently, session-based recommendation (SBR) based on session information, which is short-term behavioral information of users, has been studied. A general SBR uses only information about the item of interest to make a recommendation (e.g., item ID for an EC site). Particularly in the case of EC sites, the data recorded include the name of the item being purchased, the price of the item, the category hierarchy, and the gender and region of the user. In this study, we define a pseudo--session for the purchase history data of an EC site without user IDs and session IDs. Finally, we propose an SBR with a co-guided heterogeneous hypergraph and globalgraph network plus, called CoHHGN+. The result
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GLSM&#30340;&#22522;&#20110;&#22270;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#20852;&#36259;&#27169;&#22411;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#38271;&#26399;&#21644;&#30701;&#26399;&#29992;&#25143;&#20852;&#36259;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10028</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#38271;&#30701;&#26399;&#20852;&#36259;&#27169;&#22411;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Based Long-Term And Short-Term Interest Model for Click-Through Rate Prediction. (arXiv:2306.10028v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GLSM&#30340;&#22522;&#20110;&#22270;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#20852;&#36259;&#27169;&#22411;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#38271;&#26399;&#21644;&#30701;&#26399;&#29992;&#25143;&#20852;&#36259;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#39044;&#27979;&#26088;&#22312;&#39044;&#27979;&#29992;&#25143;&#28857;&#20987;&#39033;&#30340;&#27010;&#29575;&#65292;&#26159;&#22312;&#32447;&#25512;&#33616;&#21644;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#36825;&#26679;&#30340;&#31995;&#32479;&#20013;&#65292;&#20016;&#23500;&#30340;&#29992;&#25143;&#34892;&#20026;&#65288;&#21363;&#38271;&#26399;&#21644;&#30701;&#26399;&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#37117;&#23545;&#36825;&#20010;&#20027;&#39064;&#38750;&#24120;&#20851;&#27880;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#38271;&#26399;&#21644;&#30701;&#26399;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#12290;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#65288;1&#65289;&#22522;&#20110;&#35268;&#21017;&#21644;&#25130;&#26029;&#30340;&#26041;&#27861;&#20174;&#38271;&#26399;&#34892;&#20026;&#20013;&#25552;&#21462;&#20449;&#24687;&#26131;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#65292;&#65288;2&#65289;&#20174;&#30701;&#26399;&#34892;&#20026;&#20013;&#25552;&#21462;&#20449;&#24687;&#26102;&#21333;&#19968;&#21453;&#39304;&#34892;&#20026;&#26080;&#35770;&#22330;&#26223;&#37117;&#20250;&#23548;&#33268;&#20449;&#24687;&#28151;&#28102;&#21644;&#22122;&#22768;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#20852;&#36259;&#27169;&#22411;&#65292;&#31216;&#20026;GLSM&#12290;&#23427;&#30001;&#19968;&#20010;&#22810;&#20852;&#36259;&#22270;&#32467;&#26500;&#32452;&#25104;&#65292;&#29992;&#20110;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#34892;&#20026;&#65292;&#19968;&#20010;&#22810;&#22330;&#26223;&#20852;&#36259;&#23376;&#22270;&#29992;&#20110;&#25429;&#25417;&#30701;&#26399;&#29992;&#25143;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction aims to predict the probability that the user will click an item, which has been one of the key tasks in online recommender and advertising systems. In such systems, rich user behavior (viz. long- and short-term) has been proved to be of great value in capturing user interests. Both industry and academy have paid much attention to this topic and propose different approaches to modeling with long-term and short-term user behavior data. But there are still some unresolved issues. More specially, (1) rule and truncation based methods to extract information from long-term behavior are easy to cause information loss, and (2) single feedback behavior regardless of scenario to extract information from short-term behavior lead to information confusion and noise. To fill this gap, we propose a Graph based Long-term and Short-term interest Model, termed GLSM. It consists of a multi-interest graph structure for capturing long-term user behavior, a multi-scenari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20132;&#38169;&#27604;&#36739;&#26041;&#27861;&#30340;&#25928;&#29575;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#24403;&#29992;&#25143;&#26681;&#25454;&#30456;&#20851;&#24615;&#31163;&#24320;&#25490;&#21517;&#26102;&#65292;&#20132;&#38169;&#27604;&#36739;&#27604;A/B&#27979;&#35797;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.10023</link><description>&lt;p&gt;
&#23545;&#20132;&#38169;&#27604;&#36739;&#25928;&#29575;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Theoretical Analysis on the Efficiency of Interleaved Comparisons. (arXiv:2306.10023v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20132;&#38169;&#27604;&#36739;&#26041;&#27861;&#30340;&#25928;&#29575;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#24403;&#29992;&#25143;&#26681;&#25454;&#30456;&#20851;&#24615;&#31163;&#24320;&#25490;&#21517;&#26102;&#65292;&#20132;&#38169;&#27604;&#36739;&#27604;A/B&#27979;&#35797;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#19968;&#31181;&#29992;&#20110;&#25490;&#21517;&#30340;&#39640;&#25928;&#22312;&#32447;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#20132;&#38169;&#27604;&#36739;&#65292;&#36827;&#34892;&#20102;&#25928;&#29575;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#34429;&#28982;&#20132;&#38169;&#27604;&#36739;&#24050;&#32463;&#24212;&#29992;&#20110;&#23454;&#38469;&#31995;&#32479;&#20013;&#65292;&#20294;&#20854;&#39640;&#25928;&#29575;&#30340;&#28304;&#22836;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#26126;&#30830;&#30340;&#38416;&#36848;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#26222;&#36890;&#20132;&#38169;&#27604;&#36739;&#26041;&#27861;&#30340;&#31616;&#21333;&#20132;&#38169;&#27604;&#36739;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#19968;&#31181;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#19979;&#20132;&#38169;&#27604;&#36739;&#26041;&#27861;&#27604;A/B&#27979;&#35797;&#26356;&#26377;&#25928;&#12290;&#20854;&#20013;&#30340;&#26465;&#20214;&#26159;&#65292;&#24403;&#29992;&#25143;&#26681;&#25454;&#29289;&#21697;&#30340;&#30456;&#20851;&#24615;&#26469;&#31163;&#24320;&#25490;&#21517;&#26102;&#65288;&#36825;&#26159;&#28857;&#20987;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#20856;&#22411;&#20551;&#35774;&#65289;&#65292;&#36825;&#31181;&#24773;&#20917;&#23601;&#20250;&#20986;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#22522;&#20110;&#25968;&#20540;&#20998;&#26512;&#21644;&#29992;&#25143;&#27169;&#25311;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#29702;&#35770;&#32467;&#26524;&#19982;&#23454;&#35777;&#32467;&#26524;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a theoretical analysis on the efficiency of interleaving, an efficient online evaluation method for rankings. Although interleaving has already been applied to production systems, the source of its high efficiency has not been clarified in the literature. Therefore, this study presents a theoretical analysis on the efficiency of interleaving methods. We begin by designing a simple interleaving method similar to ordinary interleaving methods. Then, we explore a condition under which the interleaving method is more efficient than A/B testing and find that this is the case when users leave the ranking depending on the item's relevance, a typical assumption made in click models. Finally, we perform experiments based on numerical analysis and user simulation, demonstrating that the theoretical results are consistent with the empirical results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#27861;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;CLIP2Protect&#65292;&#20351;&#29992;&#23545;&#25239;&#24615;&#28508;&#22312;&#25628;&#32034;&#32467;&#21512;&#25991;&#26412;&#24341;&#23548;&#21270;&#22918;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#38754;&#37096;&#22270;&#20687;&#65292;&#20174;&#32780;&#20445;&#25252;&#38754;&#37096;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2306.10008</link><description>&lt;p&gt;
CLIP2Protect&#65306;&#20351;&#29992;&#23545;&#25239;&#24615;&#28508;&#22312;&#25628;&#32034;&#30340;&#25991;&#26412;&#24341;&#23548;&#21270;&#22918;&#26469;&#20445;&#25252;&#38754;&#37096;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search. (arXiv:2306.10008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10008
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#27861;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;CLIP2Protect&#65292;&#20351;&#29992;&#23545;&#25239;&#24615;&#28508;&#22312;&#25628;&#32034;&#32467;&#21512;&#25991;&#26412;&#24341;&#23548;&#21270;&#22918;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#38754;&#37096;&#22270;&#20687;&#65292;&#20174;&#32780;&#20445;&#25252;&#38754;&#37096;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38754;&#37096;&#35782;&#21035;&#31995;&#32479;&#30340;&#25104;&#21151;&#24050;&#32463;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#21551;&#29992;&#26410;&#25480;&#26435;&#30340;&#29992;&#25143;&#36319;&#36394;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#22686;&#24378;&#26041;&#27861;&#26080;&#27861;&#29983;&#25104;&#33258;&#28982;&#20027;&#20041;&#22270;&#20687;&#65292;&#26082;&#33021;&#20445;&#25252;&#38754;&#37096;&#38544;&#31169;&#21448;&#19981;&#20250;&#25439;&#23475;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#37319;&#29992;&#20004;&#27493;&#27861;&#65292;&#20381;&#38752;&#22312;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#20302;&#32500;&#27969;&#24418;&#20013;&#25214;&#21040;&#23545;&#25239;&#24615;&#28508;&#22312;&#32534;&#30721;&#12290;&#31532;&#19968;&#27493;&#23558;&#32473;&#23450;&#30340;&#38754;&#37096;&#22270;&#20687;&#21453;&#28436;&#25104;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#32534;&#30721;&#65292;&#24182;&#24494;&#35843;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#20174;&#20854;&#28508;&#22312;&#20195;&#30721;&#20934;&#30830;&#22320;&#37325;&#26500;&#32473;&#23450;&#30340;&#22270;&#20687;&#12290;&#36825;&#19968;&#27493;&#20135;&#29983;&#20102;&#19968;&#20010;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#65292;&#26377;&#21161;&#20110;&#29983;&#25104;&#31867;&#20284;&#20110;&#32473;&#23450;&#36523;&#20221;&#30340;&#39640;&#36136;&#37327;&#38754;&#37096;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#29992;&#25143;&#23450;&#20041;&#30340;&#21270;&#22918;&#25991;&#26412;&#25552;&#31034;&#21644;&#20445;&#25345;&#36523;&#20221;&#30340;&#35268;&#33539;&#21270;&#26469;&#25351;&#23548;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23547;&#25214;&#23545;&#25239;&#24615;&#20195;&#30721;&#30340;&#25628;&#32034;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#21270;&#22918;&#30340;&#38754;&#37096;&#22270;&#20687;&#65292;&#21487;&#26377;&#25928;&#22320;&#20445;&#25252;&#38754;&#37096;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of deep learning based face recognition systems has given rise to serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Existing methods for enhancing privacy fail to generate naturalistic images that can protect facial privacy without compromising user experience. We propose a novel two-step approach for facial privacy protection that relies on finding adversarial latent codes in the low-dimensional manifold of a pretrained generative model. The first step inverts the given face image into the latent space and finetunes the generative model to achieve an accurate reconstruction of the given image from its latent code. This step produces a good initialization, aiding the generation of high-quality faces that resemble the given identity. Subsequently, user-defined makeup text prompts and identity-preserving regularization are used to guide the search for adversarial codes in the latent space. Extensive experiments demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.09983</link><description>&lt;p&gt;
&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#25110;&#20915;&#31574;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#20154;&#33021;&#21147;&#65292;&#37027;&#20040;&#25105;&#20204;&#35813;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#20195;&#29702;&#20250;&#20135;&#29983;&#20559;&#24046;? &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21069;&#25552;&#26159;&#65292;&#34429;&#28982;&#35780;&#20272;&#36229;&#20154;&#20915;&#31574;&#30340;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;&#27169;&#22411;&#30340;&#20915;&#31574;&#26410;&#33021;&#28385;&#36275;&#26576;&#20123;&#36923;&#36753;&#19978;&#12289;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21457;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#30001;&#20110;&#36229;&#20154;&#27169;&#22411;&#33021;&#21147;&#25110;&#20854;&#20182;&#32570;&#20047;&#22522;&#26412;&#20107;&#23454;&#32780;&#38590;&#20197;&#35780;&#20272;&#65306;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#12289;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#20316;&#20986;&#27861;&#24459;&#21028;&#26029;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26080;&#35770;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;(&#21487;&#33021;&#26159;&#36229;&#20154;&#30340;)&#65292;&#25105;&#20204;&#37117;&#33021;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65306;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#32473;&#20986;&#23545;&#23616;&#20013;&#26827;&#23376;&#30456;&#23545;&#20272;&#20540;&#30340;&#19981;&#21516;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#40479;&#22768;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#38598;&#20013;&#35760;&#24405;&#23545;&#30456;&#20851;&#27010;&#29575;&#21518;&#65292;&#24212;&#29992;&#30456;&#20851;&#32858;&#31867;&#20110;&#27979;&#35797;&#38598;&#36827;&#34892;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#27979;&#35797;&#38598;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#25506;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#35757;&#32451;&#26399;&#38388;&#26410;&#21548;&#21040;&#30340;&#40479;&#31867;&#29289;&#31181;&#30340;&#35760;&#24405;&#20197;&#21450;&#20998;&#31163;&#40479;&#22768;&#21644;&#29615;&#22659;&#22122;&#22768;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09906</link><description>&lt;p&gt;
&#40479;&#40483;&#22768;&#30340;&#30456;&#20851;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Correlation Clustering of Bird Sounds. (arXiv:2306.09906v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#40479;&#22768;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#38598;&#20013;&#35760;&#24405;&#23545;&#30456;&#20851;&#27010;&#29575;&#21518;&#65292;&#24212;&#29992;&#30456;&#20851;&#32858;&#31867;&#20110;&#27979;&#35797;&#38598;&#36827;&#34892;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#27979;&#35797;&#38598;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#25506;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#35757;&#32451;&#26399;&#38388;&#26410;&#21548;&#21040;&#30340;&#40479;&#31867;&#29289;&#31181;&#30340;&#35760;&#24405;&#20197;&#21450;&#20998;&#31163;&#40479;&#22768;&#21644;&#29615;&#22659;&#22122;&#22768;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40479;&#31867;&#22768;&#38899;&#20998;&#31867;&#26159;&#23558;&#20219;&#20309;&#22768;&#38899;&#35760;&#24405;&#19982;&#21487;&#20197;&#22312;&#35760;&#24405;&#20013;&#21548;&#21040;&#30340;&#40479;&#31867;&#29289;&#31181;&#30456;&#20851;&#32852;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#40479;&#22768;&#32858;&#31867;&#65292;&#21363;&#20915;&#23450;&#20219;&#20309;&#19968;&#23545;&#22768;&#38899;&#35760;&#24405;&#26159;&#21542;&#21487;&#20197;&#21548;&#21040;&#30456;&#21516;&#30340;&#40479;&#31867;&#29289;&#31181;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#35760;&#24405;&#23545;&#25353;&#36825;&#31181;&#26041;&#24335;&#30456;&#20851;&#30340;&#27010;&#29575;&#65292;&#28982;&#21518;&#36890;&#36807;&#30456;&#20851;&#32858;&#31867;&#26469;&#25512;&#26029;&#27979;&#35797;&#38598;&#30340;&#26368;&#22823;&#21487;&#33021;&#20998;&#21306;&#12290;&#25105;&#20204;&#35299;&#20915;&#20197;&#19979;&#38382;&#39064;&#65306;&#19982;&#27979;&#35797;&#38598;&#30340;&#20998;&#31867;&#30456;&#27604;&#65292;&#36825;&#31181;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#22914;&#20309;&#65311;&#20174;&#20998;&#31867;&#33719;&#24471;&#30340;&#32858;&#31867;&#22914;&#20309;&#19982;&#22240;&#27492;&#25512;&#26029;&#24471;&#20986;&#30340;&#32858;&#31867;&#30456;&#20851;&#65311;&#22312;&#24212;&#29992;&#20110;&#35757;&#32451;&#26399;&#38388;&#26410;&#21548;&#21040;&#30340;&#40479;&#31867;&#29289;&#31181;&#30340;&#35760;&#24405;&#26102;&#65292;&#36825;&#31181;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#22914;&#20309;&#65311;&#36825;&#31181;&#32858;&#31867;&#22312;&#20998;&#31163;&#40479;&#40483;&#22768;&#21644;&#35757;&#32451;&#26399;&#38388;&#26410;&#21548;&#21040;&#30340;&#29615;&#22659;&#22122;&#22768;&#26041;&#38754;&#26377;&#22810;&#26377;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
Bird sound classification is the task of relating any sound recording to those species of bird that can be heard in the recording. Here, we study bird sound clustering, the task of deciding for any pair of sound recordings whether the same species of bird can be heard in both. We address this problem by first learning, from a training set, probabilities of pairs of recordings being related in this way, and then inferring a maximally probable partition of a test set by correlation clustering. We address the following questions: How accurate is this clustering, compared to a classification of the test set? How do the clusters thus inferred relate to the clusters obtained by classification? How accurate is this clustering when applied to recordings of bird species not heard during training? How effective is this clustering in separating, from bird sounds, environmental noise not heard during training?
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20256;&#32479;&#30340;&#20869;&#20998;&#24067;&#27867;&#21270;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#34394;&#20551;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#24615;&#26469;&#22823;&#22823;&#25439;&#23475;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65292;&#23588;&#20854;&#26159;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.09890</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Studying Generalization on Memory-Based Methods in Continual Learning. (arXiv:2306.09890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20256;&#32479;&#30340;&#20869;&#20998;&#24067;&#27867;&#21270;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#34394;&#20551;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#24615;&#26469;&#22823;&#22823;&#25439;&#23475;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65292;&#23588;&#20854;&#26159;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#30446;&#26631;&#20043;&#19968;&#26159;&#22312;&#19968;&#31995;&#21015;&#32463;&#39564;&#20013;&#19981;&#26029;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20943;&#36731;&#23436;&#20840;&#30693;&#35782;&#35206;&#30422;&#30340;&#38382;&#39064;&#65292;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#20250;&#23384;&#20648;&#19968;&#23450;&#27604;&#20363;&#30340;&#20808;&#21069;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#27979;&#35797;&#23427;&#20204;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#24615;&#33021;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#36807;&#24230;&#25311;&#21512;&#37325;&#25918;&#35760;&#24518;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20256;&#32479;&#30340;&#20869;&#20998;&#24067;&#27867;&#21270;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#34394;&#20551;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#24615;&#26469;&#22823;&#22823;&#25439;&#23475;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;&#20351;&#29992;&#25511;&#21046;&#29615;&#22659;&#65292;&#25105;&#20204;&#20351;&#29992;Synbol&#22522;&#20934;&#29983;&#25104;&#22120;&#65288;Lacoste&#31561;&#20154;&#65292;2020&#65289;&#23637;&#31034;&#20102;&#36825;&#31181;&#32570;&#20047;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#20027;&#35201;&#20986;&#29616;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the objectives of Continual Learning is to learn new concepts continually over a stream of experiences and at the same time avoid catastrophic forgetting. To mitigate complete knowledge overwriting, memory-based methods store a percentage of previous data distributions to be used during training. Although these methods produce good results, few studies have tested their out-of-distribution generalization properties, as well as whether these methods overfit the replay memory. In this work, we show that although these methods can help in traditional in-distribution generalization, they can strongly impair out-of-distribution generalization by learning spurious features and correlations. Using a controlled environment, the Synbol benchmark generator (Lacoste et al., 2020), we demonstrate that this lack of out-of-distribution generalization mainly occurs in the linear classifier.
&lt;/p&gt;</description></item><item><title>GenORM&#36890;&#36807;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#21644;&#20351;&#29992;&#21508;&#31181;&#21487;&#21464;&#24418;&#32499;&#32034;&#30340;&#27169;&#25311;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#21033;&#29992;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#32499;&#32034;&#65292;&#20174;&#32780;&#33410;&#30465;&#28436;&#31034;&#26102;&#38388;&#21644;&#25552;&#39640;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09872</link><description>&lt;p&gt;
&#21487;&#27867;&#21270;&#30340;&#19968;&#27425;&#24615;&#32499;&#32034;&#25805;&#20316;&#31574;&#30053;&#21450;&#20854;&#21442;&#25968;&#24863;&#30693;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalizable One-shot Rope Manipulation with Parameter-Aware Policy. (arXiv:2306.09872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09872
&lt;/p&gt;
&lt;p&gt;
GenORM&#36890;&#36807;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#21644;&#20351;&#29992;&#21508;&#31181;&#21487;&#21464;&#24418;&#32499;&#32034;&#30340;&#27169;&#25311;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#21033;&#29992;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#32499;&#32034;&#65292;&#20174;&#32780;&#33410;&#30465;&#28436;&#31034;&#26102;&#38388;&#21644;&#25552;&#39640;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#32499;&#32034;&#22312;&#36816;&#21160;&#36807;&#31243;&#20013;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#20026;&#22240;&#32032;&#65292;&#20197;&#24448;&#32499;&#32034;&#25805;&#20316;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#25968;&#30334;&#27425;&#30495;&#23454;&#28436;&#31034;&#26469;&#20026;&#27599;&#20010;&#32499;&#32034;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#8220;&#21040;&#36798;&#30446;&#26631;&#8221;&#20219;&#21153;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25105;&#20204;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GenORM&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#35753;&#25805;&#20316;&#31574;&#30053;&#36890;&#36807;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#23601;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#30340;&#32499;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#31574;&#30053;&#19978;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#24182;&#20351;&#29992;&#21508;&#31181;&#27169;&#25311;&#21487;&#21464;&#24418;&#32499;&#32034;&#26469;&#35757;&#32451;&#23427;&#65292;&#20351;&#31574;&#30053;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#32499;&#32034;&#21442;&#25968;&#35843;&#25972;&#34892;&#21160;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;GenORM&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#28436;&#31034;&#21644;&#27169;&#25311;&#28857;&#20113;&#30340;&#32593;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#12290;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#24110;&#21161;&#65292;&#25105;&#20204;&#20165;&#38656;&#35201;&#19968;&#27425;&#28436;&#31034;&#25968;&#25454;&#23601;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#30340;&#32499;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the inherent uncertainty in their deformability during motion, previous methods in rope manipulation often require hundreds of real-world demonstrations to train a manipulation policy for each rope, even for simple tasks such as rope goal reaching, which hinder their applications in our ever-changing world. To address this issue, we introduce GenORM, a framework that allows the manipulation policy to handle different deformable ropes with a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable rope parameters and training it with a diverse range of simulated deformable ropes so that the policy can adjust actions based on different rope parameters. At the time of inference, given a new rope, GenORM estimates the deformable rope parameters by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations. With the help of a differentiable physics simulator, we require only a single r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MVCIL&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#38543;&#26426;&#21270;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#25552;&#20986;&#27491;&#20132;&#34701;&#21512;&#23376;&#31354;&#38388;&#21644;&#36873;&#25321;&#24615;&#26435;&#37325;&#21512;&#24182;&#26469;&#35299;&#20915;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#26087;&#20449;&#24687;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;&#26377;&#25928;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.09675</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-View Class Incremental Learning. (arXiv:2306.09675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MVCIL&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#38543;&#26426;&#21270;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#25552;&#20986;&#27491;&#20132;&#34701;&#21512;&#23376;&#31354;&#38388;&#21644;&#36873;&#25321;&#24615;&#26435;&#37325;&#21512;&#24182;&#26469;&#35299;&#20915;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#26087;&#20449;&#24687;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;&#26377;&#25928;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;MVL&#65289;&#22312;&#25972;&#21512;&#25968;&#25454;&#38598;&#30340;&#22810;&#20010;&#35270;&#35282;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20026;&#20102;&#20351;MVL&#26041;&#27861;&#22312;&#24320;&#25918;&#24335;&#29615;&#22659;&#20013;&#26356;&#23454;&#29992;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#31216;&#20026;&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MVCIL&#65289;&#65292;&#20854;&#20013;&#21333;&#20010;&#27169;&#22411;&#20174;&#36830;&#32493;&#30340;&#35270;&#22270;&#27969;&#20013;&#36880;&#27493;&#20998;&#31867;&#26032;&#31867;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#26089;&#26399;&#25968;&#25454;&#30340;&#35270;&#22270;&#12290;&#20294;&#26159;&#65292;MVCIL&#38754;&#20020;&#30528;&#32769;&#20449;&#24687;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#21270;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#20197;&#20445;&#35777;&#23427;&#20204;&#22312;&#24037;&#20316;&#29366;&#24577;&#19979;&#30340;&#20998;&#31163;&#35270;&#22270;&#26368;&#20248;&#65292;&#20854;&#20013;&#23646;&#20110;&#31867;&#30340;&#22810;&#20010;&#35270;&#22270;&#25353;&#39034;&#24207;&#21576;&#29616;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#36880;&#20010;&#38598;&#25104;&#21040;&#30001;&#25552;&#21462;&#30340;&#29305;&#24449;&#36328;&#36234;&#30340;&#27491;&#20132;&#34701;&#21512;&#23376;&#31354;&#38388;&#20013;&#65307;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#36873;&#25321;&#24615;&#26435;&#37325;&#21512;&#24182;&#65292;&#20197;&#20445;&#30041;&#26087;&#31867;&#30340;&#30693;&#35782;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view learning (MVL) has gained great success in integrating information from multiple perspectives of a dataset to improve downstream task performance. To make MVL methods more practical in an open-ended environment, this paper investigates a novel paradigm called multi-view class incremental learning (MVCIL), where a single model incrementally classifies new classes from a continual stream of views, requiring no access to earlier views of data. However, MVCIL is challenged by the catastrophic forgetting of old information and the interference with learning new concepts. To address this, we first develop a randomization-based representation learning technique serving for feature extraction to guarantee their separate view-optimal working states, during which multiple views belonging to a class are presented sequentially; Then, we integrate them one by one in the orthogonality fusion subspace spanned by the extracted features; Finally, we introduce selective weight consolidation f
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36229;&#22270;&#33021;&#37327;&#20989;&#25968;&#30340;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#30456;&#27604;&#20256;&#32479;GNN&#27169;&#22411;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.09623</link><description>&lt;p&gt;
&#20174;&#36229;&#22270;&#33021;&#37327;&#20989;&#25968;&#21040;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
From Hypergraph Energy Functions to Hypergraph Neural Networks. (arXiv:2306.09623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36229;&#22270;&#33021;&#37327;&#20989;&#25968;&#30340;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#30456;&#27604;&#20256;&#32479;GNN&#27169;&#22411;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#34920;&#31034;&#23454;&#20307;&#20043;&#38388;&#39640;&#38454;&#20132;&#20114;&#30340;&#24378;&#22823;&#25277;&#35937;&#27169;&#22411;&#12290;&#20026;&#20102;&#22312;&#23454;&#29616;&#19979;&#28216;&#39044;&#27979;&#30340;&#36807;&#31243;&#20013;&#21033;&#29992;&#36825;&#20123;&#20851;&#31995;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22810;&#31181;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#24314;&#31435;&#22312;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#25991;&#29486;&#30340;&#20808;&#39537;&#19978;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31867;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#21442;&#25968;&#21270;&#36229;&#22270;&#27491;&#21017;&#21270;&#33021;&#37327;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#33021;&#37327;&#30340;&#26497;&#23567;&#20540;&#26377;&#25928;&#22320;&#20316;&#20026;&#33410;&#28857;&#23884;&#20837;&#22120;&#65292;&#20877;&#37197;&#21512;&#19968;&#20010;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36890;&#36807;&#19968;&#20010;&#30417;&#30563;&#30340;&#21452;&#23618;&#20248;&#21270;&#36807;&#31243;&#23454;&#29616;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#24314;&#35758;&#30340;&#21452;&#23618;&#36229;&#22270;&#20248;&#21270;&#20013;&#20986;&#29616;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#38544;&#24335;&#26550;&#26500;&#21644;&#24120;&#29992;GNN&#26550;&#26500;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are a powerful abstraction for representing higher-order interactions between entities of interest. To exploit these relationships in making downstream predictions, a variety of hypergraph neural network architectures have recently been proposed, in large part building upon precursors from the more traditional graph neural network (GNN) literature. Somewhat differently, in this paper we begin by presenting an expressive family of parameterized, hypergraph-regularized energy functions. We then demonstrate how minimizers of these energies effectively serve as node embeddings that, when paired with a parameterized classifier, can be trained end-to-end via a supervised bilevel optimization process. Later, we draw parallels between the implicit architecture of the predictive models emerging from the proposed bilevel hypergraph optimization, and existing GNN architectures in common use. Empirically, we demonstrate state-of-the-art results on various hypergraph node classification
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#26465;&#20214;&#19979;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38590;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#26696;FedC2SL&#65292;&#26080;&#38656;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#19988;&#23545;&#25968;&#25454;&#21464;&#24322;&#20855;&#26377;&#26356;&#24378;&#30340;&#25269;&#25239;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09433</link><description>&lt;p&gt;
&#23454;&#29992;&#32852;&#37030;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Federated Causal Structure Learning. (arXiv:2306.09433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09433
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#26465;&#20214;&#19979;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38590;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#26696;FedC2SL&#65292;&#26080;&#38656;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#19988;&#23545;&#25968;&#25454;&#21464;&#24322;&#20855;&#26377;&#26356;&#24378;&#30340;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#23545;&#20110;&#31185;&#23398;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#36807;&#31243;&#28041;&#21450;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#22240;&#26524;&#22270;&#20197;&#29702;&#35299;&#36825;&#31181;&#20851;&#31995;&#12290;&#36890;&#24120;&#65292;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#25191;&#34892;&#27492;&#20219;&#21153;&#65292;&#20294;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#25968;&#25454;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#32852;&#37030;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#23545;&#25968;&#25454;&#20570;&#20986;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#24182;&#32570;&#20047;&#25910;&#25947;&#20445;&#35777;&#12290;FedC2SL&#26159;&#19968;&#31181;&#32852;&#37030;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#26696;&#65292;&#23427;&#20351;&#29992;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26469;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#35813;&#26816;&#39564;&#22312;&#19981;&#25910;&#38598;&#23458;&#25143;&#31471;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26816;&#26597;&#20004;&#20010;&#21464;&#37327;&#22312;&#19968;&#32452;&#26465;&#20214;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;FedC2SL&#23545;&#25968;&#25454;&#20570;&#20986;&#20102;&#26356;&#24369;&#21644;&#26356;&#29616;&#23454;&#30340;&#20551;&#35774;&#65292;&#24182;&#26356;&#24378;&#22320;&#25269;&#24481;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#21464;&#24322;&#12290;FedPC&#21644;FedFCI&#26159;FedC2SL&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#29992;&#20110;&#22240;&#26524;&#20805;&#20998;&#24615;&#21644;&#22240;&#26524;&#19981;&#20805;&#20998;&#24615;&#24773;&#20917;&#19979;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding causal relations is vital in scientific discovery. The process of causal structure learning involves identifying causal graphs from observational data to understand such relations. Usually, a central server performs this task, but sharing data with the server poses privacy risks. Federated learning can solve this problem, but existing solutions for federated causal structure learning make unrealistic assumptions about data and lack convergence guarantees. FedC2SL is a federated constraint-based causal structure learning scheme that learns causal graphs using a federated conditional independence test, which examines conditional independence between two variables under a condition set without collecting raw data from clients. FedC2SL requires weaker and more realistic assumptions about data and offers stronger resistance to data variability among clients. FedPC and FedFCI are the two variants of FedC2SL for causal structure learning in causal sufficiency and causal insuffic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#39640;&#20869;&#23481;&#32454;&#32990;&#25104;&#20687;&#20013;&#30452;&#25509;&#39044;&#27979;&#32454;&#32990;&#32676;&#20307;&#30340;&#22810;&#32452;&#23398;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#21050;&#28608;&#26465;&#20214;&#19979;&#23454;&#29616;&#26174;&#33879;&#25104;&#26524;&#65292;&#20026;&#32454;&#32990;&#32452;&#23398;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09391</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#20869;&#23481;&#32454;&#32990;&#25104;&#20687;&#22810;&#32452;&#23398;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-omics Prediction from High-content Cellular Imaging with Deep Learning. (arXiv:2306.09391v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#39640;&#20869;&#23481;&#32454;&#32990;&#25104;&#20687;&#20013;&#30452;&#25509;&#39044;&#27979;&#32454;&#32990;&#32676;&#20307;&#30340;&#22810;&#32452;&#23398;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#21050;&#28608;&#26465;&#20214;&#19979;&#23454;&#29616;&#26174;&#33879;&#25104;&#26524;&#65292;&#20026;&#32454;&#32990;&#32452;&#23398;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20869;&#23481;&#32454;&#32990;&#25104;&#20687;&#12289;&#36716;&#24405;&#32452;&#23398;&#21644;&#34507;&#30333;&#36136;&#32452;&#23398;&#25968;&#25454;&#20026;&#24433;&#21709;&#32454;&#32990;&#29366;&#24577;&#21644;&#21151;&#33021;&#30340;&#29983;&#29289;&#20998;&#23376;&#23618;&#25552;&#20379;&#20102;&#20016;&#23500;&#21644;&#20114;&#34917;&#30340;&#35270;&#35282;&#12290;&#20294;&#26159;&#65292;&#23578;&#26410;&#31995;&#32479;&#22320;&#25506;&#35752;&#22810;&#32452;&#23398;&#27979;&#37327;&#20540;&#24433;&#21709;&#32454;&#32990;&#24418;&#24577;&#30340;&#29983;&#29289;&#23398;&#20915;&#23450;&#22240;&#32032;&#65292;&#22240;&#27492;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#32454;&#32990;&#25104;&#20687;&#26159;&#21542;&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#22810;&#32452;&#23398;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;Image2Omics&#8212;&#8212;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#30452;&#25509;&#20174;&#29992;&#22810;&#37325;&#33639;&#20809;&#26579;&#26009;&#26579;&#33394;&#30340;&#39640;&#20869;&#23481;&#22270;&#20687;&#20013;&#39044;&#27979;&#32454;&#32990;&#32676;&#20307;&#30340;&#22810;&#32452;&#23398;&#26159;&#21542;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#21050;&#28608;&#26465;&#20214;&#19979;&#30340;&#20154;&#31867;&#35825;&#23548;&#22810;&#33021;&#24178;&#32454;&#32990;&#65288;hiPSC&#65289;&#34893;&#29983;&#30340;&#22522;&#22240;&#32534;&#36753;&#24040;&#22124;&#32454;&#32990;&#20013;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;Image2Omics&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-content cellular imaging, transcriptomics, and proteomics data provide rich and complementary views on the molecular layers of biology that influence cellular states and function. However, the biological determinants through which changes in multi-omics measurements influence cellular morphology have not yet been systematically explored, and the degree to which cell imaging could potentially enable the prediction of multi-omics directly from cell imaging data is therefore currently unclear. Here, we address the question of whether it is possible to predict bulk multi-omics measurements directly from cell images using Image2Omics -- a deep learning approach that predicts multi-omics in a cell population directly from high-content images stained with multiplexed fluorescent dyes. We perform an experimental evaluation in gene-edited macrophages derived from human induced pluripotent stem cell (hiPSC) under multiple stimulation conditions and demonstrate that Image2Omics achieves sign
&lt;/p&gt;</description></item><item><title>STAR&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#26102;&#31354;&#22270;&#26469;&#25429;&#25417;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#65292;&#20026;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09381</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation. (arXiv:2306.09381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09381
&lt;/p&gt;
&lt;p&gt;
STAR&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#26102;&#31354;&#22270;&#26469;&#25429;&#25417;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#65292;&#20026;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#27169;&#24335;&#22312;&#25919;&#31574;&#20915;&#31574;&#21644;&#32463;&#27982;&#34892;&#20026;&#30740;&#31350;&#20013;&#26377;&#30528;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#26088;&#22312;&#32473;&#23450;&#19968;&#23567;&#32452;&#36712;&#36857;&#25968;&#25454;&#29983;&#25104;&#20154;&#31867;&#31227;&#21160;&#36712;&#36857;&#65292;&#20294;&#30001;&#20110;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#22320;&#28857;&#20043;&#38388;&#30340;&#38745;&#24577;&#20851;&#31995;&#65292;&#32780;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;SpatioTemporal-Augmented gRaph&#31070;&#32463;&#32593;&#32476;&#65288;STAR&#65289;&#65292;&#26469;&#27169;&#25311;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human mobility patterns have shown significant applications in policy-decision scenarios and economic behavior researches. The human mobility simulation task aims to generate human mobility trajectories given a small set of trajectory data, which have aroused much concern due to the scarcity and sparsity of human mobility data. Existing methods mostly rely on the static relationships of locations, while largely neglect the dynamic spatiotemporal effects of locations. On the one hand, spatiotemporal correspondences of visit distributions reveal the spatial proximity and the functionality similarity of locations. On the other hand, the varying durations in different locations hinder the iterative generation process of the mobility trajectory. Therefore, we propose a novel framework to model the dynamic spatiotemporal effects of locations, namely SpatioTemporal-Augmented gRaph neural networks (STAR). The STAR framework designs various spatiotemporal graphs to capture the spatiotemporal co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMTL&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#35268;&#33539;&#21270;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#21487;&#20197;&#25552;&#39640;MTL&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#27491;&#21017;&#21270;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#20445;&#35777;&#25910;&#25947;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09373</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equitable Multi-task Learning. (arXiv:2306.09373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMTL&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#35268;&#33539;&#21270;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#21487;&#20197;&#25552;&#39640;MTL&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#27491;&#21017;&#21270;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#20445;&#35777;&#25910;&#25947;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#22312;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#65289;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#19988;&#30456;&#20114;&#31454;&#20105;&#30340;&#30456;&#20851;&#24615;&#65292;&#21333;&#32431;&#22320;&#35757;&#32451;&#25152;&#26377;&#20219;&#21153;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#23398;&#20064;&#65292;&#21363;&#19968;&#20123;&#20219;&#21153;&#34987;&#24456;&#22909;&#22320;&#23398;&#20064;&#65292;&#32780;&#20854;&#20182;&#20219;&#21153;&#21017;&#34987;&#24573;&#35270;&#12290;&#22810;&#20219;&#21153;&#20248;&#21270;&#65288;MTO&#65289;&#26088;&#22312;&#21516;&#26102;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#20294;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#22312;&#20219;&#21153;&#25439;&#22833;&#35268;&#27169;&#25110;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;MTL&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#22312;&#26356;&#26032;&#20849;&#20139;&#21442;&#25968;&#26102;&#65292;&#35268;&#33539;&#21270;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#65288;&#21363;&#20219;&#21153;&#29305;&#23450;&#25439;&#22833;&#20540;&#38500;&#20197;&#20854;&#21407;&#22987;&#26799;&#24230;&#33539;&#25968;&#30340;&#20540;&#65289;&#21487;&#20197;&#25552;&#39640;MTL&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#21517;&#20026;EMTL&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;MTL&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#28155;&#21152;&#20102;&#26041;&#24046;&#27491;&#21017;&#21270;&#65292;&#20351;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#26356;&#20855;&#21487;&#27604;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#26469;&#20445;&#35777;&#25910;&#25947;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) has achieved great success in various research domains, such as CV, NLP and IR etc. Due to the complex and competing task correlation, na\"ive training all tasks may lead to inequitable learning, \textit{i.e.} some tasks are learned well while others are overlooked. Multi-task optimization (MTO) aims to improve all tasks at same time, but conventional methods often perform poor when tasks with large loss scale or gradient norm magnitude difference. To solve the issue, we in-depth investigate the equity problem for MTL and find that regularizing relative contribution of different tasks (\textit{i.e.} value of task-specific loss divides its raw gradient norm) in updating shared parameter can improve generalization performance of MTL. Based on our theoretical analysis, we propose a novel multi-task optimization method, named \textit{EMTL}, to achieve equitable MTL. Specifically, we efficiently add variance regularization to make different tasks' relative contribu
&lt;/p&gt;</description></item><item><title>OpenOOD v1.5 &#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#23558;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.09301</link><description>&lt;p&gt;
OpenOOD v1.5&#65306;&#22686;&#24378;&#30340;OCC&#65288;Out-of-Distribution Detection&#65289;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection. (arXiv:2306.09301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09301
&lt;/p&gt;
&lt;p&gt;
OpenOOD v1.5 &#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#23558;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OCC&#26816;&#27979;&#23545;&#20110;&#24320;&#25918;&#19990;&#30028;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;OCC&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#35780;&#20272;&#19981;&#19968;&#33268;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#38590;&#20197;&#36319;&#36394;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OpenOOD v1.5&#65292;&#36825;&#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#30830;&#20445;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#20934;&#30830;&#12289;&#26631;&#20934;&#21270;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OpenOOD v1.5&#23558;&#20854;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#22914;ImageNet&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#12290;&#35813;&#24037;&#20316;&#36824;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#20016;&#23500;&#20102;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution (OOD) detection is critical for the reliable operation of open-world intelligent systems. Despite the emergence of an increasing number of OOD detection methods, the evaluation inconsistencies present challenges for tracking the progress in this field. OpenOOD v1 initiated the unification of the OOD detection evaluation but faced limitations in scalability and usability. In response, this paper presents OpenOOD v1.5, a significant improvement from its predecessor that ensures accurate, standardized, and user-friendly evaluation of OOD detection methodologies. Notably, OpenOOD v1.5 extends its evaluation capabilities to large-scale datasets such as ImageNet, investigates full-spectrum OOD detection which is important yet underexplored, and introduces new features including an online leaderboard and an easy-to-use evaluator. This work also contributes in-depth analysis and insights derived from comprehensive experimental results, thereby enriching the knowledge pool o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; RecFusion&#65292;&#19968;&#31181;&#29305;&#23450;&#38024;&#23545;1D&#21644;/&#25110;&#20108;&#36827;&#21046;&#35774;&#32622;&#30340;&#25512;&#33616;&#27169;&#22411;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#20108;&#39033;&#24335;&#25193;&#25955;&#36807;&#31243;&#23545;&#20108;&#20803;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#26174;&#24335;&#24314;&#27169;&#65292;&#24182;&#22312;&#26680;&#24515;&#25512;&#33616;&#35774;&#32622;&#21644;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#25509;&#36817;&#22797;&#26434;&#30340;VAE&#22522;&#32447;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.08947</link><description>&lt;p&gt;
RecFusion&#65306;&#22522;&#20110;&#20108;&#39033;&#24335;&#25193;&#25955;&#36807;&#31243;&#30340;1D&#25968;&#25454;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation. (arXiv:2306.08947v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; RecFusion&#65292;&#19968;&#31181;&#29305;&#23450;&#38024;&#23545;1D&#21644;/&#25110;&#20108;&#36827;&#21046;&#35774;&#32622;&#30340;&#25512;&#33616;&#27169;&#22411;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#20108;&#39033;&#24335;&#25193;&#25955;&#36807;&#31243;&#23545;&#20108;&#20803;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#26174;&#24335;&#24314;&#27169;&#65292;&#24182;&#22312;&#26680;&#24515;&#25512;&#33616;&#35774;&#32622;&#21644;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#25509;&#36817;&#22797;&#26434;&#30340;VAE&#22522;&#32447;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RecFusion&#65292;&#36825;&#26159;&#19968;&#32452;&#29992;&#20110;&#25512;&#33616;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#19981;&#21516;&#20110;&#21253;&#21547;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#24120;&#29992;&#20110;&#25512;&#33616;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#32570;&#20047;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#19968;&#32500;&#21521;&#37327;&#19978;&#21046;&#23450;&#20102;&#25193;&#25955;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20108;&#39033;&#24335;&#25193;&#25955;&#65292;&#36825;&#20010;&#26041;&#27861;&#21033;&#29992;&#20102;&#20271;&#21162;&#21033;&#36807;&#31243;&#26174;&#24335;&#22320;&#23545;&#20108;&#20803;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RecFusion&#22312;&#26680;&#24515;&#25512;&#33616;&#35774;&#32622;&#65288;&#38024;&#23545;&#20108;&#36827;&#21046;&#38750;&#39034;&#24207;&#21453;&#39304;&#30340;&#21069;n&#39033;&#25512;&#33616;&#65289;&#21644;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#38598;&#65288;MovieLens&#21644;Netflix&#65289;&#19978;&#25509;&#36817;&#20110;&#22797;&#26434;&#30340;VAE&#22522;&#32447;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19987;&#38376;&#38024;&#23545;1D&#21644;/&#25110;&#20108;&#36827;&#21046;&#35774;&#32622;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24847;&#20041;&#36229;&#20986;&#20102;&#25512;&#33616;&#31995;&#32479;&#65292;&#20363;&#22914;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;MRI&#21644;CT&#25195;&#25551;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose RecFusion, which comprise a set of diffusion models for recommendation. Unlike image data which contain spatial correlations, a user-item interaction matrix, commonly utilized in recommendation, lacks spatial relationships between users and items. We formulate diffusion on a 1D vector and propose binomial diffusion, which explicitly models binary user-item interactions with a Bernoulli process. We show that RecFusion approaches the performance of complex VAE baselines on the core recommendation setting (top-n recommendation for binary non-sequential feedback) and the most common datasets (MovieLens and Netflix). Our proposed diffusion models that are specialized for 1D and/or binary setups have implications beyond recommendation systems, such as in the medical domain with MRI and CT scans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21160;&#24577;MEC&#30340;&#36164;&#28304;&#31649;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#26080;&#30417;&#30563;&#30340;&#38142;&#36335;&#36755;&#20986;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23545;&#20219;&#24847;&#25968;&#37327;&#30340;&#36793;&#32536;&#33410;&#28857;&#36827;&#34892;&#28789;&#27963;&#30340;&#36164;&#28304;&#20998;&#37197;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#20302;&#30340;&#31639;&#27861;&#25512;&#29702;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2306.08938</link><description>&lt;p&gt;
&#38754;&#21521;&#21160;&#24577;MEC&#30340;&#21487;&#25193;&#23637;&#36164;&#28304;&#31649;&#29702;&#65306;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#38142;&#36335;&#36755;&#20986;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scalable Resource Management for Dynamic MEC: An Unsupervised Link-Output Graph Neural Network Approach. (arXiv:2306.08938v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21160;&#24577;MEC&#30340;&#36164;&#28304;&#31649;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#26080;&#30417;&#30563;&#30340;&#38142;&#36335;&#36755;&#20986;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23545;&#20219;&#24847;&#25968;&#37327;&#30340;&#36793;&#32536;&#33410;&#28857;&#36827;&#34892;&#28789;&#27963;&#30340;&#36164;&#28304;&#20998;&#37197;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#20302;&#30340;&#31639;&#27861;&#25512;&#29702;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#30340;&#20219;&#21153;&#21368;&#36733;&#21644;&#36164;&#28304;&#20998;&#37197;&#20248;&#21270;&#20013;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#32593;&#32476;&#30340;&#21160;&#24577;&#24615;&#22312;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20248;&#21270;&#26041;&#27861;&#20013;&#23548;&#33268;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#20302;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#35757;&#32451;&#25104;&#26412;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#33410;&#28857;&#36755;&#20986;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#32593;&#32476;&#35268;&#27169;&#25193;&#23637;&#26102;&#33021;&#22815;&#25552;&#21462;&#36793;&#32536;&#33410;&#28857;&#30340;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#32500;&#24230;&#20915;&#31574;&#31354;&#38388;&#38543;&#32593;&#32476;&#35268;&#27169;&#25193;&#23637;&#32780;&#21464;&#21270;&#30340;&#26032;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#26102;&#20250;&#22833;&#36133;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38142;&#36335;&#36755;&#20986;GNN&#65288;LOGNN&#65289;&#30340;&#36164;&#28304;&#31649;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;MEC&#20013;&#20219;&#24847;&#25968;&#37327;&#30340;&#36793;&#32536;&#33410;&#28857;&#36827;&#34892;&#28789;&#27963;&#30340;&#36164;&#28304;&#20998;&#37197;&#20248;&#21270;&#65292;&#24182;&#20855;&#26377;&#26497;&#20302;&#30340;&#31639;&#27861;&#25512;&#29702;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#26080;&#38656;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#39640;&#25928;&#22320;&#35757;&#32451;LOGNN&#65292;&#20854;&#20013;&#26126;&#30830;&#22320;&#25512;&#23548;&#20986;&#20102;&#36793;&#32536;&#20219;&#21153;&#22788;&#29702;&#24310;&#36831;&#30456;&#23545;&#20110;LOGNN&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#32473;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#25512;&#23548;&#65292;&#35777;&#26126;LOGNN&#30340;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been successfully adopted in mobile edge computing (MEC) to optimize task offloading and resource allocation. However, the dynamics of edge networks raise two challenges in neural network (NN)-based optimization methods: low scalability and high training costs. Although conventional node-output graph neural networks (GNN) can extract features of edge nodes when the network scales, they fail to handle a new scalability issue whereas the dimension of the decision space may change as the network scales. To address the issue, in this paper, a novel link-output GNN (LOGNN)-based resource management approach is proposed to flexibly optimize the resource allocation in MEC for an arbitrary number of edge nodes with extremely low algorithm inference delay. Moreover, a label-free unsupervised method is applied to train the LOGNN efficiently, where the gradient of edge tasks processing delay with respect to the LOGNN parameters is derived explicitly. In addition, a theoretical a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#38544;&#31169;&#38480;&#21046;&#36866;&#24212;&#22122;&#22768;&#8221;&#65288;PLAN&#65289;&#65292;&#26159;&#19968;&#32452;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#32467;&#26500;&#20013;&#36827;&#34892;&#26356;&#22909;&#30340;&#22343;&#20540;&#20272;&#35745;&#12290;PLAN&#23558;&#22122;&#22768;&#30340;&#24418;&#29366;&#37327;&#36523;&#23450;&#21046;&#20026;&#25968;&#25454;&#30340;&#24418;&#29366;&#65292;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#32780;&#19988;&#21487;&#20197;&#22312;&#19968;&#20123;&#38598;&#20013;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#20934;&#24046;&#30340;&#20559;&#26012;&#26469;&#33719;&#24471;&#25509;&#36817;&#38646;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.08745</link><description>&lt;p&gt;
PLAN: &#26041;&#24046;&#24863;&#30693;&#30340;&#24046;&#20998;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PLAN: Variance-Aware Private Mean Estimation. (arXiv:2306.08745v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#38544;&#31169;&#38480;&#21046;&#36866;&#24212;&#22122;&#22768;&#8221;&#65288;PLAN&#65289;&#65292;&#26159;&#19968;&#32452;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#32467;&#26500;&#20013;&#36827;&#34892;&#26356;&#22909;&#30340;&#22343;&#20540;&#20272;&#35745;&#12290;PLAN&#23558;&#22122;&#22768;&#30340;&#24418;&#29366;&#37327;&#36523;&#23450;&#21046;&#20026;&#25968;&#25454;&#30340;&#24418;&#29366;&#65292;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#32780;&#19988;&#21487;&#20197;&#22312;&#19968;&#20123;&#38598;&#20013;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#20934;&#24046;&#30340;&#20559;&#26012;&#26469;&#33719;&#24471;&#25509;&#36817;&#38646;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#26159;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#20445;&#25252;&#38544;&#31169;&#30340;&#31639;&#27861;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#24050;&#32463;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#65292;&#20294;&#35768;&#22810;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#21487;&#33021;&#34987;&#21033;&#29992;&#20197;&#20135;&#29983;&#26356;&#22909;&#31639;&#27861;&#30340;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#38544;&#31169;&#38480;&#21046;&#36866;&#24212;&#22122;&#22768;&#8221;&#65288;PLAN&#65289;&#12290;PLAN&#26159;&#19968;&#32452;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#29420;&#31435;&#37319;&#26679;&#20110;&#20998;&#24067;$\mathcal{D}$&#30340;&#36755;&#20837;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#22343;&#20540;&#20272;&#35745;&#65292;&#20854;&#20013;&#20998;&#24067;&#30340;&#22352;&#26631;&#26631;&#20934;&#24046;$\boldsymbol{\sigma}\in \mathbf{R}^d$&#12290;&#19982;Mahalanobis&#36317;&#31163;&#19979;&#30340;&#22343;&#20540;&#20272;&#35745;&#31867;&#20284;&#65292;PLAN&#23558;&#22122;&#22768;&#30340;&#24418;&#29366;&#37327;&#36523;&#23450;&#21046;&#20026;&#25968;&#25454;&#30340;&#24418;&#29366;&#65292;&#20294;&#19982;&#20197;&#21069;&#30340;&#31639;&#27861;&#19981;&#21516;&#65292;&#38544;&#31169;&#39044;&#31639;&#19981;&#26159;&#22343;&#21248;&#22320;&#33457;&#36153;&#22312;&#21508;&#20010;&#22352;&#26631;&#19978;&#12290;&#22312;&#23545;$\mathcal{D}$&#30340;&#38598;&#20013;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#21521;&#37327;$\boldsymbol{\sigma}$&#20013;&#30340;&#20559;&#26012;&#65292;&#20174;&#32780;&#33719;&#24471;&#25509;&#36817;&#38646;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private mean estimation is an important building block in privacy-preserving algorithms for data analysis and machine learning. Though the trade-off between privacy and utility is well understood in the worst case, many datasets exhibit structure that could potentially be exploited to yield better algorithms. In this paper we present $\textit{Private Limit Adapted Noise}$ (PLAN), a family of differentially private algorithms for mean estimation in the setting where inputs are independently sampled from a distribution $\mathcal{D}$ over $\mathbf{R}^d$, with coordinate-wise standard deviations $\boldsymbol{\sigma} \in \mathbf{R}^d$. Similar to mean estimation under Mahalanobis distance, PLAN tailors the shape of the noise to the shape of the data, but unlike previous algorithms the privacy budget is spent non-uniformly over the coordinates. Under a concentration assumption on $\mathcal{D}$, we show how to exploit skew in the vector $\boldsymbol{\sigma}$, obtaining a (zero-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#25490;&#24207;&#21644;&#25104;&#32489;&#39044;&#27979;&#30340;&#22810;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#26356;&#22909;&#22320;&#24179;&#34913;&#25490;&#24207;&#21644;&#25104;&#32489;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08650</link><description>&lt;p&gt;
&#24403;&#35780;&#20998;&#24456;&#37325;&#35201;&#26102;&#30340;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank when Grades Matter. (arXiv:2306.08650v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#25490;&#24207;&#21644;&#25104;&#32489;&#39044;&#27979;&#30340;&#22810;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#26356;&#22909;&#22320;&#24179;&#34913;&#25490;&#24207;&#21644;&#25104;&#32489;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#23398;&#20064;&#25490;&#24207;&#24212;&#29992;&#20013;&#65292;&#20998;&#32423;&#26631;&#31614;&#24191;&#27867;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#24037;&#26631;&#27880;&#30340;&#30456;&#20851;&#24615;&#25968;&#25454;&#20013;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#26088;&#22312;&#20248;&#21270;&#25991;&#20214;&#30340;&#25490;&#24207;&#39034;&#24207;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20102;&#23454;&#38469;&#20998;&#25968;&#30340;&#39044;&#27979;&#12290;&#36825;&#20351;&#23427;&#20204;&#26080;&#27861;&#22312;&#38656;&#35201;&#32771;&#34385;&#20998;&#25968;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#34987;&#37319;&#29992;&#65292;&#20363;&#22914;&#31579;&#36873;&#8220;&#21155;&#36136;&#8221;&#25991;&#20214;&#12290;&#22312;&#33391;&#22909;&#30340;&#25490;&#24207;&#24615;&#33021;&#21644;&#33391;&#22909;&#30340;&#31561;&#32423;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#32463;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#21482;&#20851;&#27880;&#25490;&#24207;&#24615;&#33021;&#32780;&#19981;&#26657;&#20934;&#27169;&#22411;&#36755;&#20986;&#65292;&#35201;&#20040;&#23558;&#25104;&#32489;&#35270;&#20026;&#25968;&#20540;&#65292;&#20551;&#35774;&#26631;&#31614;&#22312;&#32447;&#24615;&#33539;&#22260;&#20869;&#65292;&#24182;&#26410;&#21033;&#29992;&#24207;&#25968;&#32423;&#21035;&#20449;&#24687;&#12290;&#26412;&#25991;&#23545;&#23398;&#20064;&#25490;&#24207;&#19982;&#25104;&#32489;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#21516;&#26102;&#37325;&#35270;&#25490;&#24207;&#24615;&#33021;&#21644;&#31561;&#32423;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#20351;&#29992;&#38750;&#26631;&#37327;&#20998;&#32423;&#36827;&#34892;&#25490;&#21517;&#30340;&#24418;&#24335;&#21270;&#35752;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#32852;&#21512;&#20248;&#21270;&#25490;&#24207;&#21644;&#25104;&#32489;&#39044;&#27979;&#24615;&#33021;&#12290;&#22522;&#20110;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25490;&#24207;&#21644;&#31561;&#32423;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graded labels are ubiquitous in real-world learning-to-rank applications, especially in human rated relevance data. Traditional learning-to-rank techniques aim to optimize the ranked order of documents. They typically, however, ignore predicting actual grades. This prevents them from being adopted in applications where grades matter, such as filtering out ``poor'' documents. Achieving both good ranking performance and good grade prediction performance is still an under-explored problem. Existing research either focuses only on ranking performance by not calibrating model outputs, or treats grades as numerical values, assuming labels are on a linear scale and failing to leverage the ordinal grade information. In this paper, we conduct a rigorous study of learning to rank with grades, where both ranking performance and grade prediction performance are important. We provide a formal discussion on how to perform ranking with non-scalar predictions for grades, and propose a multiobjective f
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23450;&#20041;&#22870;&#21169;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25351;&#23450;&#30340;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08647</link><description>&lt;p&gt;
&#35821;&#35328;&#36716;&#22870;&#21169;&#65306;&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#33021;&#32508;&#21512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language to Rewards for Robotic Skill Synthesis. (arXiv:2306.08647v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23450;&#20041;&#22870;&#21169;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25351;&#23450;&#30340;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#35768;&#22810;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#65292;&#20174;&#36923;&#36753;&#25512;&#29702;&#21040;&#20195;&#30721;&#32534;&#20889;&#31561;&#65292;&#23637;&#29616;&#20102;&#22312;&#24773;&#22659;&#23398;&#20064;&#20013;&#33719;&#24471;&#22810;&#31181;&#26032;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#26426;&#22120;&#20154;&#23398;&#30740;&#31350;&#20154;&#21592;&#20063;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20302;&#32423;&#26426;&#22120;&#20154;&#21160;&#20316;&#21462;&#20915;&#20110;&#30828;&#20214;&#24182;&#19988;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#25152;&#21344;&#30340;&#27604;&#37325;&#36739;&#23567;&#65292;&#22240;&#27492;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#26377;&#21162;&#21147;&#20027;&#35201;&#23558;&#20854;&#35270;&#20026;&#35821;&#20041;&#35268;&#21010;&#22120;&#65292;&#25110;&#20381;&#36182;&#20110;&#20154;&#24037;&#25511;&#21046;&#21407;&#35821;&#19982;&#26426;&#22120;&#20154;&#36827;&#34892;&#20132;&#20114;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22870;&#21169;&#20989;&#25968;&#34987;&#35777;&#26126;&#26159;&#21487;&#20197;&#28789;&#27963;&#34920;&#31034;&#24182;&#19988;&#21487;&#20197;&#34987;&#20248;&#21270;&#20197;&#23454;&#29616;&#22810;&#31181;&#20219;&#21153;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#20854;&#35821;&#20041;&#20016;&#23500;&#24615;&#20351;&#20854;&#36866;&#21512;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25351;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23450;&#20041;&#21487;&#20197;&#34987;&#20248;&#21270;&#30340;&#22870;&#21169;&#21442;&#25968;&#24182;&#23436;&#25104;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;&#20351;&#29992;&#22870;&#21169;&#20316;&#20026;&#20013;&#38388;&#20171;&#36136;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#25191;&#34892;&#21508;&#31181;&#30001;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25351;&#23450;&#30340;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#22312;&#35774;&#35745;&#34892;&#20026;&#21407;&#35821;&#26041;&#38754;&#20184;&#20986;&#21162;&#21147;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25342;&#21462;&#29289;&#21697;&#21644;&#25645;&#24314;&#22612;&#20004;&#20010;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated exciting progress in acquiring diverse new capabilities through in-context learning, ranging from logical reasoning to code-writing. Robotics researchers have also explored using LLMs to advance the capabilities of robotic control. However, since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot. On the other hand, reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this realization by utilizing LLMs to define reward parameters that can be optimized and accomplish variety of robotic tasks. Using reward as the intermediate inter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#22122;&#38899;&#38477;&#20302;&#65292;&#24182;&#25104;&#21151;&#25512;&#24191;&#21040;&#19981;&#21516;&#25104;&#20687;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.08102</link><description>&lt;p&gt;
&#38754;&#21521;&#22495;&#24863;&#30693;&#30340;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#22122;&#38899;&#38477;&#20302;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain-Aware Few-Shot Learning for Optical Coherence Tomography Noise Reduction. (arXiv:2306.08102v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#22122;&#38899;&#38477;&#20302;&#65292;&#24182;&#25104;&#21151;&#25512;&#24191;&#21040;&#19981;&#21516;&#25104;&#20687;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25955;&#26001;&#22122;&#22768;&#19968;&#30452;&#26159;&#21307;&#23398;&#25104;&#20687;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#38477;&#22122;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#26377;&#30417;&#30563;&#30340;&#23398;&#20064;&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#22122;&#38899;&#38477;&#20302;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#65292;&#21482;&#38656;&#35201;&#21333;&#24352;&#22270;&#20687;&#25110;&#37096;&#20998;&#22270;&#20687;&#20197;&#21450;&#30456;&#24212;&#30340;&#21435;&#26001;&#22320;&#38754;&#23454;&#20917;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#19981;&#21516;&#25104;&#20687;&#31995;&#32479;&#30340;&#22495;&#36716;&#31227;&#38382;&#39064;&#36827;&#34892;&#20102;&#21046;&#23450;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36866;&#24212;&#19981;&#21516;&#25104;&#20687;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speckle noise has long been an extensively studied problem in medical imaging. In recent years, there have been significant advances in leveraging deep learning methods for noise reduction. Nevertheless, adaptation of supervised learning models to unseen domains remains a challenging problem. Specifically, deep neural networks (DNNs) trained for computational imaging tasks are vulnerable to changes in the acquisition system's physical parameters, such as: sampling space, resolution, and contrast. Even within the same acquisition system, performance degrades across datasets of different biological tissues. In this work, we propose a few-shot supervised learning framework for optical coherence tomography (OCT) noise reduction, that offers a dramatic increase in training speed and requires only a single image, or part of an image, and a corresponding speckle suppressed ground truth, for training. Furthermore, we formulate the domain shift problem for OCT diverse imaging systems, and prove
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;VISION&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#20803;&#21270;&#38598;&#21512;&#65292;&#21253;&#21547;14&#20010;&#19981;&#21516;&#24037;&#19994;&#39046;&#22495;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#27880;&#37322;&#22696;&#27700;&#21644;&#23454;&#20363;&#20998;&#21106;&#27880;&#37322;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;VISION&#25968;&#25454;&#38598;&#21487;&#20197;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#24037;&#19994;&#39046;&#22495;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#26377;&#26395;&#20419;&#36827;&#22522;&#20110;&#35270;&#35273;&#30340;&#24037;&#19994;&#26816;&#27979;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.07890</link><description>&lt;p&gt;
VISION&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#24037;&#19994;&#26816;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
VISION Datasets: A Benchmark for Vision-based InduStrial InspectiON. (arXiv:2306.07890v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;VISION&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#20803;&#21270;&#38598;&#21512;&#65292;&#21253;&#21547;14&#20010;&#19981;&#21516;&#24037;&#19994;&#39046;&#22495;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#27880;&#37322;&#22696;&#27700;&#21644;&#23454;&#20363;&#20998;&#21106;&#27880;&#37322;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;VISION&#25968;&#25454;&#38598;&#21487;&#20197;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#24037;&#19994;&#39046;&#22495;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#26377;&#26395;&#20419;&#36827;&#22522;&#20110;&#35270;&#35273;&#30340;&#24037;&#19994;&#26816;&#27979;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#35270;&#35273;&#30340;&#26816;&#27979;&#31639;&#27861;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#23454;&#19990;&#30028;&#20013;&#24037;&#19994;&#25361;&#25112;&#8212;&#8212;&#29305;&#21035;&#26159;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#36136;&#37327;&#21644;&#22797;&#26434;&#30340;&#29983;&#20135;&#35201;&#27714;&#8212;&#8212;&#24448;&#24448;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;VISION&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;14&#20010;&#19981;&#21516;&#24037;&#19994;&#39046;&#22495;&#26816;&#27979;&#25968;&#25454;&#38598;&#30340;&#22810;&#20803;&#21270;&#38598;&#21512;&#65292;&#29420;&#20855;&#20248;&#21183;&#21487;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#19982;&#20197;&#24448;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;VISION&#20026;&#32570;&#38519;&#26816;&#27979;&#25552;&#20379;&#20016;&#23500;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#25152;&#26377;&#25968;&#25454;&#20998;&#31867;&#30340;&#27880;&#37322;&#25513;&#27169;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#23454;&#20363;&#20998;&#21106;&#27880;&#37322;&#21151;&#33021;&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#32570;&#38519;&#12290;VISION&#36890;&#36807;&#25552;&#20379;18000&#24352;&#22270;&#20687;&#65292;&#21253;&#21547;44&#31181;&#32570;&#38519;&#31867;&#22411;&#65292;&#33268;&#21147;&#20110;&#21453;&#26144;&#20986;&#21508;&#31181;&#29616;&#23454;&#24037;&#19994;&#29983;&#20135;&#22330;&#26223;&#12290;&#36890;&#36807;&#25903;&#25345;VISION&#25968;&#25454;&#38598;&#19978;&#20004;&#20010;&#27491;&#22312;&#36827;&#34892;&#30340;&#31454;&#36187;&#65292;&#25105;&#20204;&#24076;&#26395;&#20419;&#36827;&#22522;&#20110;&#35270;&#35273;&#30340;&#24037;&#19994;&#26816;&#27979;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite progress in vision-based inspection algorithms, real-world industrial challenges -- specifically in data availability, quality, and complex production requirements -- often remain under-addressed. We introduce the VISION Datasets, a diverse collection of 14 industrial inspection datasets, uniquely poised to meet these challenges. Unlike previous datasets, VISION brings versatility to defect detection, offering annotation masks across all splits and catering to various detection methodologies. Our datasets also feature instance-segmentation annotation, enabling precise defect identification. With a total of 18k images encompassing 44 defect types, VISION strives to mirror a wide range of real-world production scenarios. By supporting two ongoing challenge competitions on the VISION Datasets, we hope to foster further advancements in vision-based industrial inspection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32452;&#21512;&#24615;&#26469;&#23398;&#20064;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32452;&#21512;&#31561;&#21464;&#24615;&#36136;&#32435;&#20837;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.07783</link><description>&lt;p&gt;
&#32452;&#21512;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compositionally Equivariant Representation Learning. (arXiv:2306.07783v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32452;&#21512;&#24615;&#26469;&#23398;&#20064;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32452;&#21512;&#31561;&#21464;&#24615;&#36136;&#32435;&#20837;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#20805;&#20998;&#30340;&#30417;&#30563;&#65288;&#26631;&#35760;&#25968;&#25454;&#65289;&#25165;&#33021;&#26377;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#21487;&#20197;&#36805;&#36895;&#23398;&#20064;&#35782;&#21035;&#21307;&#23398;&#22270;&#20687;&#65288;&#22914;&#30913;&#20849;&#25391;&#21644; CT &#25195;&#25551;&#20013;&#30340;&#37325;&#35201;&#35299;&#21078;&#32467;&#26500;&#65289;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#25351;&#23548;&#12290;&#36825;&#31181;&#35782;&#21035;&#33021;&#21147;&#23481;&#26131;&#27867;&#21270;&#21040;&#26469;&#33258;&#19981;&#21516;&#21307;&#30103;&#26426;&#26500;&#30340;&#26032;&#22270;&#20687;&#20197;&#21450;&#19981;&#21516;&#35774;&#32622;&#20013;&#30340;&#26032;&#20219;&#21153;&#12290;&#36825;&#31181;&#24555;&#36895;&#19988;&#27867;&#21270;&#30340;&#23398;&#20064;&#33021;&#21147;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#20154;&#33041;&#20013;&#22270;&#20687;&#27169;&#24335;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#32780;&#24403;&#21069;&#30340;&#21307;&#23398;&#27169;&#22411;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#20986;&#36825;&#31181;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#21033;&#29992;&#32452;&#21512;&#24615;&#26469;&#23398;&#20064;&#26356;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65306;&#29992;&#20110;&#29983;&#25104;&#21307;&#23398;&#22270;&#20687;&#30340;&#22522;&#30784;&#29983;&#25104;&#22240;&#32032;&#28385;&#36275;&#32452;&#21512;&#31561;&#21464;&#24615;&#36136;&#65292;&#20854;&#20013;&#27599;&#20010;&#22240;&#32032;&#37117;&#26159;&#32452;&#21512;&#30340;&#65288;&#20363;&#22914;&#23545;&#24212;&#20110;&#20154;&#20307;&#35299;&#21078;&#32467;&#26500;&#65289;&#24182;&#19988;&#23545;&#20219;&#21153;&#26159;&#31561;&#21464;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23558;&#32452;&#21512;&#31561;&#21464;&#24615;&#36136;&#32435;&#20837;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models often need sufficient supervision (i.e. labelled data) in order to be trained effectively. By contrast, humans can swiftly learn to identify important anatomy in medical images like MRI and CT scans, with minimal guidance. This recognition capability easily generalises to new images from different medical facilities and to new tasks in different settings. This rapid and generalisable learning ability is largely due to the compositional structure of image patterns in the human brain, which are not well represented in current medical models. In this paper, we study the utilisation of compositionality in learning more interpretable and generalisable representations for medical image segmentation. Overall, we propose that the underlying generative factors that are used to generate the medical images satisfy compositional equivariance property, where each factor is compositional (e.g. corresponds to the structures in human anatomy) and also equivariant to the task. Henc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;SHAP&#20540;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25351;&#20986;&#38543;&#26426;&#24490;&#29615;&#27169;&#22411;&#26159;&#26356;&#26377;&#25928;&#30340;&#22791;&#36873;&#24490;&#29615;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07218</link><description>&lt;p&gt;
SHAP&#35299;&#37322;&#30340;&#25345;&#32493;&#35299;&#37322;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
A Protocol for Continual Explanation of SHAP. (arXiv:2306.07218v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;SHAP&#20540;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25351;&#20986;&#38543;&#26426;&#24490;&#29615;&#27169;&#22411;&#26159;&#26356;&#26377;&#25928;&#30340;&#22791;&#36873;&#24490;&#29615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#22312;&#25968;&#25454;&#27969;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#26032;&#20449;&#24687;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#37492;&#20110;&#36825;&#31181;&#29615;&#22659;&#30340;&#21160;&#24577;&#24615;&#36136;&#65292;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102; SHAP &#20540;&#35299;&#37322;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#21487;&#38752;&#22320;&#35780;&#20272;&#36880;&#31867;&#22686;&#37327;&#22330;&#26223;&#20013;&#35299;&#37322;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;&#37325;&#25918;&#31574;&#30053;&#21487;&#20197;&#24378;&#21046;&#21069;&#39304;/&#21367;&#31215;&#27169;&#22411;&#20013;&#30340; SHAP &#20540;&#30340;&#31283;&#23450;&#24615;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#22312;&#23436;&#20840;&#35757;&#32451;&#30340;&#24490;&#29615;&#27169;&#22411;&#20013;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20687;&#38543;&#26426;&#24490;&#29615;&#27169;&#22411;&#36825;&#26679;&#30340;&#22791;&#36873;&#24490;&#29615;&#26041;&#27861;&#22312;&#38543;&#26102;&#38388;&#20445;&#25345;&#35299;&#37322;&#31283;&#23450;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning trains models on a stream of data, with the aim of learning new information without forgetting previous knowledge. Given the dynamic nature of such environments, explaining the predictions of these models can be challenging. We study the behavior of SHAP values explanations in Continual Learning and propose an evaluation protocol to robustly assess the change of explanations in Class-Incremental scenarios. We observed that, while Replay strategies enforce the stability of SHAP values in feedforward/convolutional models, they are not able to do the same with fully-trained recurrent models. We show that alternative recurrent approaches, like randomized recurrent models, are more effective in keeping the explanations stable over time.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#33258;&#20030;&#30340;&#26102;&#38388;&#27493;&#32423;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#20026;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25552;&#20379;&#39640;&#25928;&#38477;&#32500;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22312;PeMS-BAY&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06994</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#33258;&#20030;&#30340;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Correlated Time Series Self-Supervised Representation Learning via Spatiotemporal Bootstrapping. (arXiv:2306.06994v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06994
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#33258;&#20030;&#30340;&#26102;&#38388;&#27493;&#32423;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#20026;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25552;&#20379;&#39640;&#25928;&#38477;&#32500;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22312;PeMS-BAY&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22312;&#35768;&#22810;&#23454;&#38469;&#24037;&#19994;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#38477;&#32500;&#34920;&#31034;&#20197;&#20415;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#26159;&#24517;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#33258;&#20030;&#34920;&#31034;&#39044;&#27979;&#30340;&#26102;&#38388;&#27493;&#32423;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#20415;&#20026;&#20010;&#20307;&#23454;&#20363;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#23558;&#39044;&#27979;&#27169;&#22411;&#20919;&#21551;&#21160;&#36716;&#31227;&#21040;&#25968;&#25454;&#21463;&#38480;&#30340;&#26032;&#23454;&#20363;&#26041;&#38754;&#35780;&#20272;&#20102;&#35813;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#32463;&#36807;&#35757;&#32451;&#22312;&#23398;&#20064;&#34920;&#31034;&#19978;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#35777;&#26126;&#20102;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#19982;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#27604;&#36739;&#20013;&#65292;&#25105;&#20204;&#22312;PeMS-BAY&#25968;&#25454;&#38598;&#19978;&#23558;RMSE&#12289;MAE&#21644;MAPE&#20998;&#21035;&#20943;&#23569;&#20102;37&#65285;&#12289;49&#65285;&#21644;48&#65285;&#12290;&#22312;&#23454;&#38469;&#30340;&#22320;&#38081;&#23458;&#27969;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23637;&#31034;&#20102;&#23558;&#39044;&#27979;&#33021;&#21147;&#36716;&#31227;&#21040;&#26032;&#30340;&#20919;&#21551;&#21160;&#24773;&#20917;&#19979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correlated time series analysis plays an important role in many real-world industries. Learning an efficient representation of this large-scale data for further downstream tasks is necessary but challenging. In this paper, we propose a time-step-level representation learning framework for individual instances via bootstrapped spatiotemporal representation prediction. We evaluated the effectiveness and flexibility of our representation learning framework on correlated time series forecasting and cold-start transferring the forecasting model to new instances with limited data. A linear regression model trained on top of the learned representations demonstrates our model performs best in most cases. Especially compared to representation learning models, we reduce the RMSE, MAE, and MAPE by 37%, 49%, and 48% on the PeMS-BAY dataset, respectively. Furthermore, in real-world metro passenger flow data, our framework demonstrates the ability to transfer to infer future information of new cold-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23616;&#37096;&#21040;&#20840;&#23616;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21253;&#25324;&#19981;&#21464;&#22270;&#32593;&#32476;&#12289;&#23616;&#37096;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#20840;&#23616;&#22270;&#21464;&#25442;&#22120;&#65292;&#24182;&#30740;&#31350;&#20854;&#25910;&#25947;&#24615;&#36136;&#21644;&#22312;&#22270;&#31895;&#21270;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.06547</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Local-to-global Perspectives on Graph Neural Networks. (arXiv:2306.06547v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23616;&#37096;&#21040;&#20840;&#23616;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21253;&#25324;&#19981;&#21464;&#22270;&#32593;&#32476;&#12289;&#23616;&#37096;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#20840;&#23616;&#22270;&#21464;&#25442;&#22120;&#65292;&#24182;&#30740;&#31350;&#20854;&#25910;&#25947;&#24615;&#36136;&#21644;&#22312;&#22270;&#31895;&#21270;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#30340;&#35270;&#35282;&#65292;&#20854;&#20013;&#20998;&#20026;&#23616;&#37096;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#21644;&#20840;&#23616;&#22270;&#21464;&#25442;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#24037;&#20316;&#65306;1&#65289;&#30740;&#31350;&#19968;&#31181;&#20840;&#23616; GNN&#65292;&#19981;&#21464;&#22270;&#32593;&#32476;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;2&#65289;&#36830;&#25509;&#23616;&#37096; MPNN &#21644;&#20840;&#23616;&#22270;&#21464;&#25442;&#22120;&#65292;3&#65289;&#22312;&#20840;&#23616;&#24314;&#27169;&#20013;&#65292;&#20351;&#29992;&#23616;&#37096; MPNN &#36827;&#34892;&#22270;&#31895;&#21270;&#65292;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#23376;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a local-to-global perspective on graph neural networks (GNN), which are categorized as local Message Passing Neural Networks (MPNN) and global Graph Transformer. We present three pieces of work: 1) study the convergence property of a type of global GNN, Invariant Graph Networks, 2) connect the local MPNN and global Graph Transformer, and 3) use local MPNN for graph coarsening, a common subroutine used in global modeling.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#24191;&#27867;&#35270;&#35282;&#26469;&#30028;&#23450;&#26234;&#33021;&#24182;&#24314;&#31435;&#20102;&#19977;&#32423;&#23884;&#22871;&#32467;&#26500;&#21450;&#20854;&#22522;&#30784;&#30340;&#24191;&#27867;&#31354;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#23450;&#20041;&#21021;&#27493;&#25506;&#32034;&#20102;&#22855;&#28857;&#12289;&#29983;&#25104;AI&#12289;&#20262;&#29702;&#21644;&#30693;&#35782;&#20135;&#26435;&#31561;&#35805;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06499</link><description>&lt;p&gt;
&#30830;&#23450;&#21644;&#25506;&#32034;&#26234;&#33021;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Defining and Explorting the Intelligence Space. (arXiv:2306.06499v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#24191;&#27867;&#35270;&#35282;&#26469;&#30028;&#23450;&#26234;&#33021;&#24182;&#24314;&#31435;&#20102;&#19977;&#32423;&#23884;&#22871;&#32467;&#26500;&#21450;&#20854;&#22522;&#30784;&#30340;&#24191;&#27867;&#31354;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#23450;&#20041;&#21021;&#27493;&#25506;&#32034;&#20102;&#22855;&#28857;&#12289;&#29983;&#25104;AI&#12289;&#20262;&#29702;&#21644;&#30693;&#35782;&#20135;&#26435;&#31561;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23581;&#35797;&#20102;&#35768;&#22810;&#27425;&#65292;&#26234;&#33021;&#26159;&#19968;&#20010;&#38590;&#20197;&#23450;&#20041;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35270;&#35282;&#26469;&#23450;&#20041;&#26234;&#33021;, &#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23450;&#20041;&#65292;&#26500;&#24314;&#20102;&#19977;&#20010;&#23618;&#27425;&#30340;&#26234;&#33021;&#23884;&#22871;&#23618;&#27425;&#21644;&#20197;&#23427;&#20204;&#21450;&#20854;&#36817;&#20284;&#20540;&#20026;&#22522;&#30784;&#30340;&#24191;&#27867;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#26234;&#33021;&#31354;&#38388;&#20013;&#65292;&#37492;&#21035;&#20986;&#23545;&#24212;&#20110;&#33258;&#28982;&#8212;&#8212;&#23588;&#20854;&#26159;&#20154;&#31867;&#8212;&#8212;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20197;&#21450;&#31867;&#20154;&#26234;&#33021;&#30340;&#21306;&#22495;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#23450;&#20041;&#21021;&#27493;&#25506;&#32034;&#20102;&#22235;&#20010;&#26356;&#20808;&#36827;&#12289;&#21487;&#33021;&#26356;&#20855;&#20105;&#35758;&#24615;&#30340;&#35805;&#39064;&#65306;&#22855;&#28857;&#12289;&#29983;&#25104;AI&#12289;&#20262;&#29702;&#21644;&#30693;&#35782;&#20135;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligence is a difficult concept to define, despite many attempts at doing so. Rather than trying to settle on a single definition, this article introduces a broad perspective on what intelligence is, by laying out a cascade of definitions that induces both a nested hierarchy of three levels of intelligence and a wider-ranging space that is built around them and approximations to them. Within this intelligence space, regions are identified that correspond to both natural -- most particularly, human -- intelligence and artificial intelligence (AI), along with the crossover notion of humanlike intelligence. These definitions are then exploited in early explorations of four more advanced, and likely more controversial, topics: the singularity, generative AI, ethics, and intellectual property.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#22914;&#20309;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#35821;&#20041;&#30456;&#21516;&#65292;&#22312;&#38271;&#23614;&#29616;&#35937;&#20013;&#25506;&#35752;&#20102;&#21387;&#32553;&#25552;&#39640;&#25512;&#24191;&#24615;&#33021;&#30340;&#35760;&#24518;&#35201;&#32032;&#12290;</title><link>http://arxiv.org/abs/2306.06238</link><description>&lt;p&gt;
&#29702;&#35299;&#38271;&#23614;&#25928;&#24212;&#23545;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effect of the Long Tail on Neural Network Compression. (arXiv:2306.06238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#22914;&#20309;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#35821;&#20041;&#30456;&#21516;&#65292;&#22312;&#38271;&#23614;&#29616;&#35937;&#20013;&#25506;&#35752;&#20102;&#21387;&#32553;&#25552;&#39640;&#25512;&#24191;&#24615;&#33021;&#30340;&#35760;&#24518;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#21387;&#32553;&#29616;&#22312;&#26159;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#19968;&#20010;&#25104;&#29087;&#30340;&#23376;&#39046;&#22495;&#65292;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20197;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#21152;&#36895;&#25512;&#26029;&#20026;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#35266;&#23519;&#21040;&#65292;&#20165;&#20851;&#27880;&#24635;&#20307;&#20934;&#30830;&#24615;&#21487;&#33021;&#26159;&#35823;&#23548;&#30340;&#12290;&#20363;&#22914;&#65292;&#24050;&#32463;&#35777;&#26126;&#20840;&#27169;&#22411;&#21644;&#21387;&#32553;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#20559;&#21521;&#20110;&#22312;&#25968;&#25454;&#38598;&#20013;&#20302;&#39057;&#30340;&#31867;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#8220;&#25105;&#20204;&#33021;&#21542;&#22312;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#35821;&#20041;&#31561;&#21516;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32593;&#32476;&#21387;&#32553;&#65311;&#8221;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;Feldman&#31561;&#20154;&#35266;&#23519;&#21040;&#30340;&#8220;&#38271;&#23614;&#8221;&#29616;&#35937;&#12290;&#20182;&#20204;&#35748;&#20026;&#65292;&#26576;&#20123;&#36755;&#20837;&#65288;&#36866;&#24403;&#23450;&#20041;&#65289;&#30340;&#35760;&#24518;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#26159;&#24517;&#35201;&#30340;&#12290;&#30001;&#20110;&#21387;&#32553;&#38480;&#21046;&#20102;&#32593;&#32476;&#30340;&#23481;&#37327;&#65288;&#22240;&#27492;&#20063;&#38480;&#21046;&#20102;&#20854;&#35760;&#24518;&#33021;&#21147;&#65289;&#65292;&#25152;&#20197;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Network compression is now a mature sub-field of neural network research: over the last decade, significant progress has been made towards reducing the size of models and speeding up inference, while maintaining the classification accuracy. However, many works have observed that focusing on just the overall accuracy can be misguided. E.g., it has been shown that mismatches between the full and compressed models can be biased towards under-represented classes. This raises the important research question, \emph{can we achieve network compression while maintaining ``semantic equivalence'' with the original network?} In this work, we study this question in the context of the ``long tail'' phenomenon in computer vision datasets observed by Feldman, et al. They argue that \emph{memorization} of certain inputs (appropriately defined) is essential to achieving good generalization. As compression limits the capacity of a network (and hence also its ability to memorize), we study the question: a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#26426;&#21046;&#30340;&#26032;&#22411;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#65292;&#23454;&#29616;&#20102;&#21033;&#29992;&#20256;&#24863;&#22120;&#35760;&#24405;&#20013;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#30446;&#26631;&#36319;&#36394;&#21644;&#36895;&#24230;&#20272;&#35745;&#65292;&#24182;&#23558;&#35760;&#24518;&#29366;&#24577;&#36827;&#34892;&#25237;&#24433;&#12290;</title><link>http://arxiv.org/abs/2306.06126</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#26631;&#36319;&#36394;&#12289;&#36895;&#24230;&#20272;&#35745;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26102;&#38388;&#25237;&#24433;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Method for Object Tracking, Velocity Estimation and Projection of Sensor Data over Time. (arXiv:2306.06126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#26426;&#21046;&#30340;&#26032;&#22411;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#65292;&#23454;&#29616;&#20102;&#21033;&#29992;&#20256;&#24863;&#22120;&#35760;&#24405;&#20013;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#30446;&#26631;&#36319;&#36394;&#21644;&#36895;&#24230;&#20272;&#35745;&#65292;&#24182;&#23558;&#35760;&#24518;&#29366;&#24577;&#36827;&#34892;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29615;&#22659;&#20998;&#21106;&#21644;&#36895;&#24230;&#20272;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21033;&#29992;&#25152;&#33719;&#21462;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#23558;&#26032;&#36755;&#20837;&#21644;&#35760;&#24518;&#25968;&#25454;&#30456;&#20851;&#32852;&#26469;&#38544;&#24335;&#22320;&#25512;&#23548;&#22330;&#26223;&#21160;&#24577;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#21457;&#29616;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23384;&#22312;&#26550;&#26500;&#38480;&#21046;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#21033;&#29992;Transformer&#26426;&#21046;&#30340;&#26032;&#22411;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#26469;&#35299;&#20915;&#21033;&#29992;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#35760;&#24405;&#20013;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#25152;&#38754;&#20020;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#22312;&#35813;&#21333;&#20803;&#20013;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#20256;&#24863;&#22120;&#36755;&#20837;&#21644;&#35760;&#24518;&#29366;&#24577;&#20998;&#21035;&#23548;&#20986;&#30340;&#20851;&#38190;-&#26597;&#35810;&#23545;&#30456;&#20851;&#32852;&#65292;&#36319;&#36394;&#23545;&#35937;&#32534;&#30721;&#22312;&#36830;&#32493;&#24103;&#19978;&#30340;&#20301;&#32622;&#12290;&#28982;&#21518;&#21033;&#29992;&#24471;&#21040;&#30340;&#36319;&#36394;&#27169;&#24335;&#26469;&#33719;&#21462;&#22330;&#26223;&#21160;&#24577;&#21644;&#22238;&#24402;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#25552;&#21462;&#30340;&#36895;&#24230;&#20272;&#35745;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#29366;&#24577;&#36827;&#34892;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current Deep Learning methods for environment segmentation and velocity estimation rely on Convolutional Recurrent Neural Networks to exploit spatio-temporal relationships within obtained sensor data. These approaches derive scene dynamics implicitly by correlating novel input and memorized data utilizing ConvNets. We show how ConvNets suffer from architectural restrictions for this task. Based on these findings, we then provide solutions to various issues on exploiting spatio-temporal correlations in a sequence of sensor recordings by presenting a novel Recurrent Neural Network unit utilizing Transformer mechanisms. Within this unit, object encodings are tracked across consecutive frames by correlating key-query pairs derived from sensor inputs and memory states, respectively. We then use resulting tracking patterns to obtain scene dynamics and regress velocities. In a last step, the memory state of the Recurrent Neural Network is projected based on extracted velocity estimates to res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SequenceMatch&#30340;&#24102;&#26377;&#22238;&#28335;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#24207;&#21015;&#21644;&#25968;&#25454;&#38598;&#24207;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#27495;&#26469;&#20943;&#23569;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22797;&#21512;&#35823;&#24046;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#22238;&#28335;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.05426</link><description>&lt;p&gt;
SequenceMatch&#65306;&#24102;&#22238;&#28335;&#30340;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking. (arXiv:2306.05426v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SequenceMatch&#30340;&#24102;&#26377;&#22238;&#28335;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#24207;&#21015;&#21644;&#25968;&#25454;&#38598;&#24207;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#27495;&#26469;&#20943;&#23569;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22797;&#21512;&#35823;&#24046;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#22238;&#28335;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#35266;&#27979;&#20540;&#30340;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20284;&#28982;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26368;&#22823;&#20284;&#28982;&#65288;MLE&#65289;&#30446;&#26631;&#19981;&#19968;&#23450;&#19982;&#33258;&#22238;&#24402;&#29983;&#25104;&#39640;&#36136;&#37327;&#24207;&#21015;&#30340;&#19979;&#28216;&#29992;&#20363;&#30456;&#21305;&#37197;&#12290;MLE&#30446;&#26631;&#25353;&#29031;&#25968;&#25454;&#20998;&#24067;&#19979;&#24207;&#21015;&#30340;&#39057;&#29575;&#21152;&#26435;&#65292;&#19981;&#25552;&#20379;&#27169;&#22411;&#22312;&#20998;&#24067;&#20043;&#22806;&#34892;&#20026;&#30340;&#25351;&#23548;&#65292;&#36825;&#20250;&#23548;&#33268;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#20013;&#22797;&#21512;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22797;&#21512;&#35823;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#24207;&#21015;&#29983;&#25104;&#23450;&#20026;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#26368;&#23567;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20998;&#24067;&#21644;&#25968;&#25454;&#38598;&#24207;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#27495;&#65292;&#21253;&#25324;&#32771;&#34385;&#20986;&#20998;&#24067;&#24207;&#21015;&#30340;&#20998;&#27495;&#12290;IL&#26694;&#26550;&#36824;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#24341;&#20837;&#22238;&#26684;&#21160;&#20316;&#26469;&#24341;&#20837;&#22238;&#28335;&#12290;&#36825;&#36827;&#19968;&#27493;&#20943;&#36731;&#20102;&#22797;&#21512;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compound
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.04723</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#20869;&#37096;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25552;&#39640;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#23646;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#21363;&#32473;&#23450;&#25991;&#26412;&#26679;&#26412;&#23884;&#20837;&#38598;&#21512;&#19979;&#30340;&#27969;&#24418;&#30340;&#20869;&#37096;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#27969;&#30021;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#22312;&#20960;&#20010;&#22522;&#20110;&#23383;&#27597;&#30340;&#35821;&#35328;&#20013;&#32422;&#20026; $9$&#65292;&#32780;&#20013;&#25991;&#32422;&#20026; $7$&#65292;&#32780;&#27599;&#31181;&#35821;&#35328;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#36739;&#20302;&#65292;&#24046;&#32422; $1.5$&#65292;&#24182;&#19988;&#26377;&#26126;&#26174;&#30340;&#32479;&#35745;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21463;&#21040;&#31995;&#32479;&#35823;&#24046;&#24433;&#21709;&#30340;&#25968;&#25454;&#20013;&#36824;&#21407;&#20986;&#28508;&#22312;&#29289;&#29702;&#31995;&#32479;&#30340;&#26102;&#31354;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.04600</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#25968;&#25454;&#35823;&#24046;&#20462;&#27491;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Uncovering solutions from data corrupted by systematic errors: A physics-constrained convolutional neural network approach. (arXiv:2306.04600v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21463;&#21040;&#31995;&#32479;&#35823;&#24046;&#24433;&#21709;&#30340;&#25968;&#25454;&#20013;&#36824;&#21407;&#20986;&#28508;&#22312;&#29289;&#29702;&#31995;&#32479;&#30340;&#26102;&#31354;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#29616;&#35937;&#21644;&#24037;&#31243;&#31995;&#32479;&#30340;&#20449;&#24687;&#36890;&#24120;&#21253;&#21547;&#22312;&#25968;&#25454;&#20013;&#12290; &#28982;&#32780;&#65292;&#25968;&#25454;&#21487;&#33021;&#20250;&#34987;&#27169;&#22411;&#21644;&#23454;&#39564;&#20013;&#30340;&#31995;&#32479;&#24615;&#35823;&#24046;&#25152;&#25439;&#22351;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#65292;&#21363;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#21024;&#38500;&#31995;&#32479;&#35823;&#24046;&#26469;&#25581;&#31034;&#28508;&#22312;&#29289;&#29702;&#31995;&#32479;&#30340;&#26102;&#31354;&#35299;&#12290; PC-CNN&#32467;&#21512;&#20102;&#31995;&#32479;&#25511;&#21046;&#26041;&#31243;&#21644;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290; &#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#25311;&#30340;&#22522;&#30784;&#29616;&#35937;&#65292;&#20363;&#22914;&#32447;&#24615;&#23545;&#27969;&#65292;Burgers&#26041;&#31243;&#21644;&#20108;&#32500;&#28237;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information on natural phenomena and engineering systems is typically contained in data. Data can be corrupted by systematic errors in models and experiments. In this paper, we propose a tool to uncover the spatiotemporal solution of the underlying physical system by removing the systematic errors from data. The tool is the physics-constrained convolutional neural network (PC-CNN), which combines information from both the systems governing equations and data. We focus on fundamental phenomena that are modelled by partial differential equations, such as linear convection, Burgers equation, and two-dimensional turbulence. First, we formulate the problem, describe the physics-constrained convolutional neural network, and parameterise the systematic error. Second, we uncover the solutions from data corrupted by large multimodal systematic errors. Third, we perform a parametric study for different systematic errors. We show that the method is robust. Fourth, we analyse the physical properti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#25490;&#21517;&#31995;&#32479;&#20844;&#24179;&#24615;&#30340;&#27979;&#35797;&#26041;&#27861;&#8212;&#8212;&#21305;&#37197;&#23545;&#26657;&#20934;&#65292;&#36890;&#36807;&#26500;&#24314;&#28151;&#28102;&#24046;&#24322;&#26368;&#23567;&#30340;&#21305;&#37197;&#29289;&#21697;&#23545;&#26469;&#35745;&#31639;&#36866;&#24403;&#30340;&#25490;&#21517;&#35823;&#24046;&#27979;&#37327;&#32467;&#26524;&#65292;&#21487;&#20197;&#30452;&#25509;&#35828;&#26126;&#23376;&#32452;&#27700;&#24179;&#26333;&#20809;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#25490;&#21517;&#20559;&#24046;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03775</link><description>&lt;p&gt;
&#21305;&#37197;&#23545;&#26657;&#20934;&#29992;&#20110;&#25490;&#21517;&#20844;&#24179;&#24615;&#30340;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Matched Pair Calibration for Ranking Fairness. (arXiv:2306.03775v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#25490;&#21517;&#31995;&#32479;&#20844;&#24179;&#24615;&#30340;&#27979;&#35797;&#26041;&#27861;&#8212;&#8212;&#21305;&#37197;&#23545;&#26657;&#20934;&#65292;&#36890;&#36807;&#26500;&#24314;&#28151;&#28102;&#24046;&#24322;&#26368;&#23567;&#30340;&#21305;&#37197;&#29289;&#21697;&#23545;&#26469;&#35745;&#31639;&#36866;&#24403;&#30340;&#25490;&#21517;&#35823;&#24046;&#27979;&#37327;&#32467;&#26524;&#65292;&#21487;&#20197;&#30452;&#25509;&#35828;&#26126;&#23376;&#32452;&#27700;&#24179;&#26333;&#20809;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#25490;&#21517;&#20559;&#24046;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#20998;&#25968;&#30340;&#25490;&#21517;&#31995;&#32479;&#20013;&#20844;&#24179;&#24615;&#30340;&#27979;&#35797;&#26041;&#27861;&#8212;&#8212;&#21305;&#37197;&#23545;&#26657;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#32452;&#21305;&#37197;&#30340;&#29289;&#21697;&#23545;&#65292;&#36825;&#20123;&#29289;&#21697;&#23545;&#23376;&#32452;&#20043;&#38388;&#30340;&#28151;&#28102;&#24046;&#24322;&#26368;&#23567;&#65292;&#28982;&#21518;&#22312;&#36825;&#32452;&#29289;&#21697;&#19978;&#35745;&#31639;&#36866;&#24403;&#30340;&#25490;&#21517;&#35823;&#24046;&#27979;&#37327;&#32467;&#26524;&#12290;&#21305;&#37197;&#27493;&#39588;&#30830;&#20445;&#20102;&#25105;&#20204;&#22312;&#30456;&#21516;&#20998;&#25968;&#30340;&#29289;&#21697;&#20043;&#38388;&#27604;&#36739;&#23376;&#32452;&#32467;&#26524;&#65292;&#20174;&#32780;&#30452;&#25509;&#35828;&#26126;&#23376;&#32452;&#27700;&#24179;&#26333;&#20809;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#23558;&#26657;&#20934;&#30340;&#20844;&#24179;&#24615;&#30452;&#35273;&#20174;&#20108;&#20803;&#20998;&#31867;&#35774;&#32622;&#25512;&#24191;&#21040;&#25490;&#21517;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#25552;&#35758;&#30340;&#25490;&#21517;&#20844;&#24179;&#24615;&#25514;&#26045;&#32852;&#31995;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#23637;&#31034;&#20102;&#36793;&#38469;&#32467;&#26524;&#27979;&#35797;&#36923;&#36753;&#22914;&#20309;&#25193;&#23637;&#21040;&#20998;&#26512;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#27169;&#22411;&#24471;&#20998;&#30340;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23558;&#21305;&#37197;&#23545;&#26657;&#20934;&#24212;&#29992;&#20110;&#30495;&#23454;&#25490;&#21517;&#25968;&#25454;&#38598;&#20197;&#35777;&#26126;&#20854;&#26816;&#27979;&#25490;&#21517;&#20559;&#24046;&#30340;&#25928;&#33021;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a test of fairness in score-based ranking systems called matched pair calibration. Our approach constructs a set of matched item pairs with minimal confounding differences between subgroups before computing an appropriate measure of ranking error over the set. The matching step ensures that we compare subgroup outcomes between identically scored items so that measured performance differences directly imply unfairness in subgroup-level exposures. We show how our approach generalizes the fairness intuitions of calibration from a binary classification setting to ranking and connect our approach to other proposals for ranking fairness measures. Moreover, our strategy shows how the logic of marginal outcome tests extends to cases where the analyst has access to model scores. Lastly, we provide an example of applying matched pair calibration to a real-word ranking data set to demonstrate its efficacy in detecting ranking bias.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#23398;&#20064;&#20986;&#21478;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#26144;&#23556;&#65292;&#21363;&#20351;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#20063;&#21487;&#20197;&#20197;&#20219;&#24847;&#31934;&#24230;&#23398;&#20064;&#30495;&#23454;&#30340;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20302;&#31209;&#27169;&#22411;&#25130;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03173</link><description>&lt;p&gt;
&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Linear Distance Metric Learning. (arXiv:2306.03173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#23398;&#20064;&#20986;&#21478;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#26144;&#23556;&#65292;&#21363;&#20351;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#20063;&#21487;&#20197;&#20197;&#20219;&#24847;&#31934;&#24230;&#23398;&#20064;&#30495;&#23454;&#30340;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20302;&#31209;&#27169;&#22411;&#25130;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#20013;&#65292;&#32473;&#23450;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#65292;&#30446;&#26631;&#26159;&#23547;&#25214;&#19968;&#20010;&#36866;&#24403;&#30340;&#32447;&#24615;&#26144;&#23556;&#21040;&#21478;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#65292;&#23613;&#21487;&#33021;&#22320;&#28385;&#36275;&#19968;&#23450;&#30340;&#36317;&#31163;&#26465;&#20214;&#12290;&#26412;&#25991;&#35268;&#33539;&#20102;&#19968;&#31181;&#31616;&#21333;&#20248;&#32654;&#30340;&#26041;&#27861;&#65292;&#23427;&#31616;&#21270;&#20026;&#19968;&#20010;&#36830;&#32493;&#30340;&#20984;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#30456;&#24212;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#25968;&#25454;&#26377;&#22122;&#22768;&#65292;&#21482;&#35201;&#26377;&#36275;&#22815;&#30340;&#26679;&#26412;&#65292;&#23601;&#21487;&#20197;&#20197;&#20219;&#24847;&#31934;&#24230;&#23398;&#20064;&#30495;&#23454;&#30340;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#23558;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#25130;&#26029;&#20026;&#20302;&#31209;&#27169;&#22411;&#65292;&#21487;&#20197;&#35777;&#26126;&#22312;&#25439;&#22833;&#20989;&#25968;&#21644;&#21442;&#25968;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#20445;&#25345;&#31934;&#24230;&#65292;&#36825;&#26159;&#36825;&#31181;&#31867;&#22411;&#30340;&#39318;&#20010;&#32467;&#26524;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20960;&#20010;&#23454;&#39564;&#35266;&#23519;&#25903;&#25345;&#21644;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In linear distance metric learning, we are given data in one Euclidean metric space and the goal is to find an appropriate linear map to another Euclidean metric space which respects certain distance conditions as much as possible. In this paper, we formalize a simple and elegant method which reduces to a general continuous convex loss optimization problem, and for different noise models we derive the corresponding loss functions. We show that even if the data is noisy, the ground truth linear metric can be learned with any precision provided access to enough samples, and we provide a corresponding sample complexity bound. Moreover, we present an effective way to truncate the learned model to a low-rank model that can provably maintain the accuracy in loss function and in parameters -- the first such results of this type. Several experimental observations on synthetic and real data sets support and inform our theoretical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#28082;&#21387;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;&#20013;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#65292;&#32780;&#38598;&#25104;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02709</link><description>&lt;p&gt;
&#29992;&#20110;&#28082;&#21387;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study on Semi-supervised Learning Applied for Anomaly Detection in Hydraulic Condition Monitoring System. (arXiv:2306.02709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#28082;&#21387;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;&#20013;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#65292;&#32780;&#38598;&#25104;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#32500;&#25252;&#22312;&#28082;&#21387;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#24322;&#24120;&#25968;&#25454;&#24456;&#23569;&#65292;&#26631;&#35760;&#36825;&#20123;&#25968;&#25454;&#26159;&#36153;&#26102;&#36153;&#21147;&#29978;&#33267;&#21361;&#38505;&#30340;&#12290;&#22240;&#27492;&#65292;&#24314;&#35758;&#20351;&#29992;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21482;&#26377;&#23569;&#37327;&#26631;&#31614;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#26426;&#21046;&#26469;&#36741;&#21161;&#30417;&#30563;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#22312;&#28082;&#21387;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#39318;&#20808;&#65292;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#20102;&#35299;&#24320;&#28304;&#30340;&#28082;&#21387;&#29366;&#24577;&#30417;&#27979;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#29420;&#31435;&#21322;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#40065;&#26834;&#21327;&#26041;&#24046;&#65289;&#12289;&#38598;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#23396;&#31435;&#26862;&#26519;&#65289;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#22270;&#21367;&#31215;&#32593;&#32476;&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#32780;&#38598;&#25104;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Condition-based maintenance is becoming increasingly important in hydraulic systems. However, anomaly detection for these systems remains challenging, especially since that anomalous data is scarce and labeling such data is tedious and even dangerous. Therefore, it is advisable to make use of unsupervised or semi-supervised methods, especially for semi-supervised learning which utilizes unsupervised learning as a feature extraction mechanism to aid the supervised part when only a small number of labels are available. This study systematically compares semi-supervised learning methods applied for anomaly detection in hydraulic condition monitoring systems. Firstly, thorough data analysis and feature learning were carried out to understand the open-sourced hydraulic condition monitoring dataset. Then, various methods were implemented and evaluated including traditional stand-alone semi-supervised learning models (e.g., one-class SVM, Robust Covariance), ensemble models (e.g., Isolation F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#33410;&#65292;&#35299;&#38145;&#20102;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30452;&#25509;&#36866;&#24212;&#36830;&#32493;&#35821;&#38899;&#21040;&#31163;&#25955;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#38899;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02207</link><description>&lt;p&gt;
SpeechGen: &#21033;&#29992;&#25552;&#31034;&#35299;&#38145;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts. (arXiv:2306.02207v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#33410;&#65292;&#35299;&#38145;&#20102;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30452;&#25509;&#36866;&#24212;&#36830;&#32493;&#35821;&#38899;&#21040;&#31163;&#25955;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#38899;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;ChatGPT&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#36830;&#32493;&#35821;&#38899;&#30452;&#25509;&#36866;&#24212;&#20110;&#22788;&#29702;&#31163;&#25955;&#26631;&#35760;&#30340;LLM&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#36825;&#22952;&#30861;&#20102;LLM&#22312;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#39640;&#32423;&#35821;&#38899;LM&#20204;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#35821;&#38899;&#20449;&#21495;&#25152;&#21253;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#21253;&#25324;&#35828;&#35805;&#32773;&#21644;&#24773;&#24863;&#31561;&#65292;&#36825;&#20123;&#20449;&#24687;&#20165;&#36890;&#36807;&#25991;&#26412;&#25968;&#25454;&#26080;&#27861;&#33719;&#21462;&#12290;&#22312;&#19968;&#20123;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#21442;&#25968;&#25928;&#29575;&#21644;&#31454;&#20105;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#20294;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#31034;&#33021;&#22815;&#26377;&#25928;&#22320;&#28608;&#21457;&#35821;&#38899;LM&#30340;&#29983;&#25104;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20808;&#39537;&#24615;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#22312;&#31216;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#20013;&#20351;&#29992;&#25552;&#31034;&#35843;&#33410;&#26469;&#21050;&#28608;&#35821;&#38899;LM&#36827;&#34892;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20855;&#26377;&#32422;10M&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#21452;&#19968;&#33268;&#24615;&#32593;&#32476;&#65292;&#29992;&#20110;&#26816;&#27979;&#24102;&#26377;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#35875;&#35328;&#12290;&#35813;&#32593;&#32476;&#20351;&#29992;&#20004;&#20010;&#19968;&#33268;&#24615;&#26816;&#27979;&#23376;&#32593;&#32476;&#21516;&#26102;&#25429;&#33719;&#36328;&#27169;&#24577;&#32423;&#21035;&#21644;&#20869;&#23481;-&#30693;&#35782;&#32423;&#21035;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#32570;&#22833;&#35270;&#35273;&#27169;&#24577;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.02137</link><description>&lt;p&gt;
&#19981;&#19968;&#33268;&#30340;&#20851;&#38190;&#28857;&#65306;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#21452;&#19968;&#33268;&#24615;&#32593;&#32476;&#29992;&#20110;&#22810;&#27169;&#24577;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Inconsistent Matters: A Knowledge-guided Dual-consistency Network for Multi-modal Rumor Detection. (arXiv:2306.02137v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#21452;&#19968;&#33268;&#24615;&#32593;&#32476;&#65292;&#29992;&#20110;&#26816;&#27979;&#24102;&#26377;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#35875;&#35328;&#12290;&#35813;&#32593;&#32476;&#20351;&#29992;&#20004;&#20010;&#19968;&#33268;&#24615;&#26816;&#27979;&#23376;&#32593;&#32476;&#21516;&#26102;&#25429;&#33719;&#36328;&#27169;&#24577;&#32423;&#21035;&#21644;&#20869;&#23481;-&#30693;&#35782;&#32423;&#21035;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#32570;&#22833;&#35270;&#35273;&#27169;&#24577;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35875;&#35328;&#20256;&#25773;&#32773;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22810;&#23186;&#20307;&#20869;&#23481;&#26469;&#21560;&#24341;&#26032;&#38395;&#28040;&#36153;&#32773;&#30340;&#27880;&#24847;&#21147;&#21644;&#20449;&#20219;&#12290;&#34429;&#28982;&#26377;&#19981;&#23569;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#21033;&#29992;&#20102;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#32771;&#34385;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#35821;&#20041;&#65292;&#20063;&#24456;&#23569;&#21457;&#29616;&#21457;&#24067;&#20869;&#23481;&#21644;&#32972;&#26223;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#20551;&#35774;&#22810;&#31181;&#27169;&#24577;&#30340;&#23436;&#25972;&#24615;&#65292;&#22240;&#27492;&#26080;&#27861;&#22788;&#29702;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#32570;&#22833;&#27169;&#24577;&#12290;&#21463;&#21040;&#31038;&#20132;&#23186;&#20307;&#20013;&#35875;&#35328;&#26356;&#23481;&#26131;&#20855;&#26377;&#19981;&#19968;&#33268;&#35821;&#20041;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#21452;&#19968;&#33268;&#24615;&#32593;&#32476;&#65292;&#29992;&#20110;&#26816;&#27979;&#24102;&#26377;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#35875;&#35328;&#12290;&#23427;&#20351;&#29992;&#20004;&#20010;&#19968;&#33268;&#24615;&#26816;&#27979;&#23376;&#32593;&#32476;&#21516;&#26102;&#25429;&#33719;&#36328;&#27169;&#24577;&#32423;&#21035;&#21644;&#20869;&#23481;-&#30693;&#35782;&#32423;&#21035;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#23427;&#36824;&#21033;&#29992;&#29305;&#27530;&#30340;&#26631;&#35760;&#65292;&#22312;&#19981;&#21516;&#30340;&#32570;&#22833;&#35270;&#35273;&#27169;&#24577;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rumor spreaders are increasingly utilizing multimedia content to attract the attention and trust of news consumers. Though quite a few rumor detection models have exploited the multi-modal data, they seldom consider the inconsistent semantics between images and texts, and rarely spot the inconsistency among the post contents and background knowledge. In addition, they commonly assume the completeness of multiple modalities and thus are incapable of handling handle missing modalities in real-life scenarios. Motivated by the intuition that rumors in social media are more likely to have inconsistent semantics, a novel Knowledge-guided Dual-consistency Network is proposed to detect rumors with multimedia contents. It uses two consistency detection subnetworks to capture the inconsistency at the cross-modal level and the content-knowledge level simultaneously. It also enables robust multi-modal representation learning under different missing visual modality conditions, using a special token
&lt;/p&gt;</description></item><item><title>&#20174;&#31070;&#32463;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#20855;&#22791;&#21754;&#20083;&#21160;&#29289;&#24847;&#35782;&#24863;&#30693;&#30456;&#20851;&#30340;&#19992;&#33041;&#30382;&#23618;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#32570;&#20047;&#21608;&#22260;&#19990;&#30028;&#30340;&#20855;&#20307;&#23884;&#20837;&#24335;&#20449;&#24687;&#65292;&#19988;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#26080;&#27861;&#20570;&#21040;&#23384;&#22312;&#30340;&#20381;&#36182;&#20110;&#20854;&#34892;&#20026;&#65292;&#36825;&#24847;&#21619;&#30528;&#20154;&#24037;&#24847;&#35782;&#30340;&#21487;&#34892;&#24615;&#23384;&#22312;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2306.00915</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#30340;&#35270;&#35282;&#25506;&#31350;&#20154;&#24037;&#24847;&#35782;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
The feasibility of artificial consciousness through the lens of neuroscience. (arXiv:2306.00915v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00915
&lt;/p&gt;
&lt;p&gt;
&#20174;&#31070;&#32463;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#20855;&#22791;&#21754;&#20083;&#21160;&#29289;&#24847;&#35782;&#24863;&#30693;&#30456;&#20851;&#30340;&#19992;&#33041;&#30382;&#23618;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#32570;&#20047;&#21608;&#22260;&#19990;&#30028;&#30340;&#20855;&#20307;&#23884;&#20837;&#24335;&#20449;&#24687;&#65292;&#19988;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#26080;&#27861;&#20570;&#21040;&#23384;&#22312;&#30340;&#20381;&#36182;&#20110;&#20854;&#34892;&#20026;&#65292;&#36825;&#24847;&#21619;&#30528;&#20154;&#24037;&#24847;&#35782;&#30340;&#21487;&#34892;&#24615;&#23384;&#22312;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24341;&#21457;&#20102;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#30340;&#29468;&#27979;&#12290;&#20174;&#31070;&#32463;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#35266;&#28857;&#24456;&#38590;&#34987;&#35777;&#23454;&#12290;&#39318;&#20808;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#32570;&#23569;&#21754;&#20083;&#21160;&#29289;&#24847;&#35782;&#24863;&#30693;&#30456;&#20851;&#30340;&#19992;&#33041;&#30382;&#23618;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#32570;&#20047;&#25105;&#20204;&#19982;&#21608;&#22260;&#19990;&#30028;&#30340;&#24863;&#23448;&#25509;&#35302;&#30340;&#20855;&#26377;&#20307;&#39564;&#12289;&#23884;&#20837;&#24335;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#34429;&#28982;&#21069;&#20004;&#20010;&#35770;&#28857;&#22312;&#26410;&#26469;&#30340;AI&#31995;&#32479;&#20013;&#21487;&#20197;&#34987;&#20811;&#26381;&#65292;&#20294;&#31532;&#19977;&#20010;&#21487;&#33021;&#26356;&#38590;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#36328;&#36234;&#12290;&#25442;&#35328;&#20043;&#65292;&#25105;&#20204;&#35748;&#20026;&#24847;&#35782;&#21487;&#33021;&#21462;&#20915;&#20110;&#26159;&#21542;&#22312;&#8220;&#28216;&#25103;&#20013;&#26377;&#30382;&#32932;&#8221;&#65292;&#21363;&#31995;&#32479;&#30340;&#23384;&#22312;&#26159;&#21542;&#21462;&#20915;&#20110;&#20854;&#34892;&#20026;&#65292;&#32780;&#36825;&#22312;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#24182;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions with large language models have led to the suggestion that these models may be conscious. From the perspective of neuroscience, this position is difficult to defend. For one, the architecture of large language models is missing key features of the thalamocortical system that have been linked to conscious awareness in mammals. Secondly, the inputs to large language models lack the embodied, embedded information content characteristic of our sensory contact with the world around us. Finally, while the previous two arguments can be overcome in future AI systems, the third one might be harder to bridge in the near future. Namely, we argue that consciousness might depend on having 'skin in the game', in that the existence of the system depends on its actions, which is not true for present-day artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21307;&#23398;&#25104;&#20687;&#20449;&#24687;&#23398;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22270;&#20687;&#22788;&#29702;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#30142;&#30149;&#26816;&#27979;&#12289;&#35786;&#26029;&#21644;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#20449;&#24687;&#23398;&#22312;&#21307;&#23398;&#20013;&#30340;&#20316;&#29992;&#20197;&#21450;&#20854;&#23545;&#24739;&#32773;&#25252;&#29702;&#30340;&#28508;&#22312;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.00421</link><description>&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#20449;&#24687;&#23398;&#31616;&#20171;
&lt;/p&gt;
&lt;p&gt;
Introduction to Medical Imaging Informatics. (arXiv:2306.00421v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21307;&#23398;&#25104;&#20687;&#20449;&#24687;&#23398;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22270;&#20687;&#22788;&#29702;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#30142;&#30149;&#26816;&#27979;&#12289;&#35786;&#26029;&#21644;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#20449;&#24687;&#23398;&#22312;&#21307;&#23398;&#20013;&#30340;&#20316;&#29992;&#20197;&#21450;&#20854;&#23545;&#24739;&#32773;&#25252;&#29702;&#30340;&#28508;&#22312;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#20449;&#24687;&#23398;&#26159;&#23558;&#21307;&#23398;&#25104;&#20687;&#21644;&#20449;&#24687;&#23398;&#30340;&#21407;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#30340;&#33719;&#21462;&#12289;&#31649;&#29702;&#21644;&#35299;&#37322;&#20026;&#30446;&#30340;&#30340;&#24555;&#36895;&#22686;&#38271;&#30340;&#39046;&#22495;&#12290;&#26412;&#31456;&#20171;&#32461;&#20102;&#21307;&#23398;&#25104;&#20687;&#20449;&#24687;&#23398;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#21253;&#25324;&#22270;&#20687;&#22788;&#29702;&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#26412;&#31456;&#36824;&#35752;&#35770;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#29992;&#20110;&#24320;&#21457;&#26032;&#30340;&#23450;&#37327;&#22270;&#20687;&#26631;&#35760;&#21644;&#30142;&#30149;&#26816;&#27979;&#12289;&#35786;&#26029;&#21644;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#28085;&#30422;&#21307;&#23398;&#25104;&#20687;&#20449;&#24687;&#23398;&#30340;&#22522;&#26412;&#30693;&#35782;&#65292;&#26412;&#31456;&#20026;&#29702;&#35299;&#20449;&#24687;&#23398;&#22312;&#21307;&#23398;&#20013;&#30340;&#20316;&#29992;&#21450;&#20854;&#23545;&#24739;&#32773;&#25252;&#29702;&#30340;&#28508;&#22312;&#24433;&#21709;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging informatics is a rapidly growing field that combines the principles of medical imaging and informatics to improve the acquisition, management, and interpretation of medical images. This chapter introduces the basic concepts of medical imaging informatics, including image processing, feature engineering, and machine learning. It also discusses the recent advancements in computer vision and deep learning technologies and how they are used to develop new quantitative image markers and prediction models for disease detection, diagnosis, and prognosis prediction. By covering the basic knowledge of medical imaging informatics, this chapter provides a foundation for understanding the role of informatics in medicine and its potential impact on patient care.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#21644;&#23436;&#20840;&#20934;&#30830;&#26102;&#20998;&#21035;&#37319;&#21462;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;ImageNet&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#24635;&#32467;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.00265</link><description>&lt;p&gt;
&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#21644;&#23436;&#20840;&#20934;&#30830;&#26102;&#20998;&#21035;&#37319;&#21462;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;ImageNet&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#24635;&#32467;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35757;&#32451;&#26159;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#24182;&#23558;&#20854;&#19982;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#32467;&#21512;&#20351;&#29992;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#33258;&#25105;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#36825;&#20123;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#24179;&#34913;&#12290;&#24403;&#20266;&#26631;&#31614;&#23436;&#20840;&#19981;&#27491;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#34987;&#20943;&#23569;&#21040;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#24403;&#20266;&#26631;&#31614;&#23436;&#20840;&#20934;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21464;&#25104;&#21033;&#29992;&#25152;&#26377;&#20266;&#26631;&#31614;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#21152;&#26377;&#25928;&#30340;&#26679;&#26412;&#37327;&#12290;&#36890;&#36807;&#22312;ImageNet&#22270;&#20687;&#20998;&#31867;&#21644;nuScenes&#33258;&#20027;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#37325;&#31283;&#20581;&#25439;&#22833;&#20248;&#20110;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#22522;&#32447;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#32467;&#26500;&#30340;&#24863;&#30693;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#26463;&#25628;&#32034;&#20013;&#30828;&#21069;k&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26799;&#24230;&#20449;&#21495;&#20256;&#36882;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;BT-Cell&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#20010;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.19999</link><description>&lt;p&gt;
&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65306;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Beam Tree Recursive Cells. (arXiv:2305.19999v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#32467;&#26500;&#30340;&#24863;&#30693;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#26463;&#25628;&#32034;&#20013;&#30828;&#21069;k&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26799;&#24230;&#20449;&#21495;&#20256;&#36882;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;BT-Cell&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#20010;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25193;&#23637;&#25903;&#25345;&#20351;&#29992;&#26463;&#25628;&#32034;&#36827;&#34892;&#28508;&#22312;&#32467;&#26500;&#24863;&#30693;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RvNN&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20986;&#22312;&#26463;&#25628;&#32034;&#20013;&#23545;&#30828;&#24615;&#21069;k&#31639;&#23376;&#30340;&#25918;&#26494;&#26469;&#25193;&#23637;&#27492;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#20256;&#36882;&#26799;&#24230;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#19981;&#21516;&#20195;&#34920;&#24615;&#20998;&#24067;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BT-Cell&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20307;&#29616;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#65288;&#22914;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#19978;&#36798;&#21040;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#19982;&#20854;&#20182;&#22522;&#20110;RvNN&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;ListOps&#20013;&#30830;&#23450;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#21442;&#25968;&#25968;&#37327;&#19978;&#30340;&#26410;&#30693;&#22833;&#25928;&#26696;&#20363;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/JRC1995/BeamTreeRecursiveCells&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly framework to extend Recursive Neural Networks (RvNNs) with beam search for latent structure induction. We further extend this framework by proposing a relaxation of the hard top-k operators in beam search for better propagation of gradient signals. We evaluate our proposed models in different out-of-distribution splits in both synthetic and realistic data. Our experiments show that BTCell achieves near-perfect performance on several challenging structure-sensitive synthetic tasks like ListOps and logical inference while maintaining comparable performance in realistic data against other RvNN-based models. Additionally, we identify a previously unknown failure case for neural models in generalization to unseen number of arguments in ListOps. The code is available at: https://github.com/JRC1995/BeamTreeRecursiveCells.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#22411;&#36229;&#22768;&#22270;&#20687;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#27880;&#37322;&#24341;&#23548;&#30340;Transformer UNet&#27169;&#22411;&#21644;&#27880;&#37322;&#24341;&#23548;&#30340;&#20108;&#20998;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#35299;&#20915;&#20302;&#20998;&#36776;&#29575;&#21644;&#30028;&#38480;&#19981;&#28165;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#20998;&#21106;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.19956</link><description>&lt;p&gt;
MicroSegNet&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#22411;&#36229;&#22768;&#22270;&#20687;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MicroSegNet: A Deep Learning Approach for Prostate Segmentation on Micro-Ultrasound Images. (arXiv:2305.19956v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#22411;&#36229;&#22768;&#22270;&#20687;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#27880;&#37322;&#24341;&#23548;&#30340;Transformer UNet&#27169;&#22411;&#21644;&#27880;&#37322;&#24341;&#23548;&#30340;&#20108;&#20998;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#35299;&#20915;&#20302;&#20998;&#36776;&#29575;&#21644;&#30028;&#38480;&#19981;&#28165;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#20998;&#21106;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#22411;&#36229;&#22768;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;29MHz&#36229;&#22768;&#25216;&#26415;&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#36229;&#22768;&#39640;3-4&#20493;&#30340;&#20998;&#36776;&#29575;&#65292;&#22312;&#35786;&#26029;&#21069;&#21015;&#33146;&#30284;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;MRI&#30456;&#24403;&#65292;&#20294;&#25104;&#26412;&#26356;&#20302;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20302;&#20998;&#36776;&#29575;&#21644;&#21069;&#21015;&#33146;&#12289;&#33152;&#33009;&#21644;&#23615;&#36947;&#20013;&#32447;&#20043;&#38388;&#30340;&#30028;&#38480;&#19981;&#28165;&#65292;&#22522;&#20110;&#24494;&#22411;&#36229;&#22768;&#30340;&#21069;&#21015;&#33146;&#20998;&#21106;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MicroSegNet&#65292;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#22810;&#23610;&#24230;&#27880;&#37322;&#24341;&#23548;&#30340;Transformer UNet&#27169;&#22411;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;MicroSegNet&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#20998;&#21106;&#65288;&#38590;&#21306;&#22495;&#65289;&#30340;&#21306;&#22495;&#65292;&#36825;&#20123;&#21306;&#22495;&#20855;&#26377;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#27880;&#37322;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27880;&#37322;&#24341;&#23548;&#30340;&#20108;&#20998;&#31867;&#20132;&#21449;&#29109;&#65288;AG-BCE&#65289;&#25439;&#22833;&#65292;&#23427;&#22312;&#38590;&#21306;&#22495;&#20013;&#32473;&#39044;&#27979;&#35823;&#24046;&#20998;&#37197;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36739;&#20302;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Micro-ultrasound (micro-US) is a novel 29-MHz ultrasound technique that provides 3-4 times higher resolution than traditional ultrasound, delivering comparable accuracy for diagnosing prostate cancer to MRI but at a lower cost. Accurate prostate segmentation is crucial for prostate volume measurement, cancer diagnosis, prostate biopsy, and treatment planning. However, prostate segmentation on microUS is challenging due to artifacts and indistinct borders between the prostate, bladder, and urethra in the midline. This paper presents MicroSegNet, a multi-scale annotation-guided transformer UNet model designed specifically to tackle these challenges. During the training process, MicroSegNet focuses more on regions that are hard to segment (hard regions), characterized by discrepancies between expert and non-expert annotations. We achieve this by proposing an annotation-guided binary cross entropy (AG-BCE) loss that assigns a larger weight to prediction errors in hard regions and a lower w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#23558;&#20307;&#20869;&#24494;-US&#22270;&#20687;&#19982;&#31163;&#20307;&#20840;&#20999;&#29255;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#37197;&#20934;&#65292;&#20197;&#24110;&#21161;&#27852;&#23615;&#22806;&#31185;&#21307;&#29983;&#25552;&#39640;&#23567;&#21069;&#21015;&#33146;&#30284;&#30340;&#26816;&#27979;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.19939</link><description>&lt;p&gt;
&#21069;&#21015;&#33146;&#20307;&#20869;&#24494;&#22411;&#36229;&#22768;&#19982;&#31163;&#20307;&#20266;&#20840;&#20999;&#29255;&#32452;&#32455;&#26631;&#26412;&#22270;&#20687;&#30340;&#22270;&#20687;&#37197;&#20934;: &#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Registration of In Vivo Micro-Ultrasound and Ex Vivo Pseudo-Whole Mount Histopathology Images of the Prostate: A Proof-of-Concept Study. (arXiv:2305.19939v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#23558;&#20307;&#20869;&#24494;-US&#22270;&#20687;&#19982;&#31163;&#20307;&#20840;&#20999;&#29255;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#37197;&#20934;&#65292;&#20197;&#24110;&#21161;&#27852;&#23615;&#22806;&#31185;&#21307;&#29983;&#25552;&#39640;&#23567;&#21069;&#21015;&#33146;&#30284;&#30340;&#26816;&#27979;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21015;&#33146;&#30284;&#30340;&#26089;&#26399;&#35786;&#26029;&#26174;&#33879;&#25552;&#39640;&#20102;&#24739;&#32773;5&#24180;&#29983;&#23384;&#29575;&#12290;&#22270;&#20687;&#24341;&#23548;&#19979;&#30340;&#27963;&#26816;&#21487;&#20197;&#25913;&#21892;&#23545;&#23567;&#22411;&#21069;&#21015;&#33146;&#30284;&#30340;&#26816;&#27979;&#12290;MRI-&#36229;&#22768;&#34701;&#21512;&#24341;&#23548;&#19979;&#30340;&#27963;&#26816;&#23545;&#26356;&#23567;&#30340;&#32959;&#30244;&#25935;&#24863;&#65292;&#20294;&#30001;&#20110;MRI&#21644;&#34701;&#21512;&#35774;&#22791;&#30340;&#39640;&#25104;&#26412;&#32780;&#34987;&#23569;&#20351;&#29992;&#12290;&#24494;&#22411;&#36229;&#22768;&#65288;&#24494;-US&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#39640;&#20998;&#36776;&#29575;&#36229;&#22768;&#25216;&#26415;&#65292;&#21487;&#25552;&#20379;MRI&#25104;&#20687;&#31867;&#20284;&#30340;&#35786;&#26029;&#31934;&#24230;&#65292;&#21516;&#26102;&#25104;&#26412;&#26356;&#20302;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30284;&#32454;&#32990;&#21644;&#27491;&#24120;&#32452;&#32455;&#20043;&#38388;&#30340;&#28784;&#24230;&#21464;&#21270;&#24494;&#24369;&#65292;&#22240;&#27492;&#35299;&#37322;&#24494;-US&#22270;&#20687;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21487;&#20197;&#36890;&#36807;&#21521;&#27852;&#23615;&#22806;&#31185;&#21307;&#29983;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;&#22320;&#38754;&#30495;&#23454;&#30284;&#21464;&#21306;&#22495;&#30340;&#24494;-US&#22270;&#20687;&#22823;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#37197;&#20934;&#23558;&#25163;&#26415;&#26631;&#26412;&#65288;&#32452;&#32455;&#30149;&#29702;&#23398;&#65289;&#26144;&#23556;&#21040;&#24494;-US&#22270;&#20687;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#23558;&#20307;&#20869;&#24494;-US&#22270;&#20687;&#19982;&#31163;&#20307;&#20840;&#20999;&#29255;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of prostate cancer significantly improves a patient's 5-year survival rate. Biopsy of small prostate cancers is improved with image-guided biopsy. MRI-ultrasound fusion-guided biopsy is sensitive to smaller tumors but is underutilized due to the high cost of MRI and fusion equipment. Micro-ultrasound (micro-US), a novel high-resolution ultrasound technology, provides a cost-effective alternative to MRI while delivering comparable diagnostic accuracy. However, the interpretation of micro-US is challenging due to subtle gray scale changes indicating cancer vs normal tissue. This challenge can be addressed by training urologists with a large dataset of micro-US images containing the ground truth cancer outlines. Such a dataset can be mapped from surgical specimens (histopathology) onto micro-US images via image registration. In this paper, we present a semi-automated pipeline for registering in vivo micro-US images with ex vivo whole-mount histopathology images. Our pipeli
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#31574;&#30053;&#32593;&#32476;&#65292;&#37325;&#26032;&#26694;&#23450;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#20026;&#34920;&#31034;-&#21033;&#29992;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25506;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24212;&#29992;&#36827;&#21270;&#21644;&#31574;&#30053;&#26799;&#24230;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19922</link><description>&lt;p&gt;
&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Representation-Driven Reinforcement Learning. (arXiv:2305.19922v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19922
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#31574;&#30053;&#32593;&#32476;&#65292;&#37325;&#26032;&#26694;&#23450;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#20026;&#34920;&#31034;-&#21033;&#29992;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25506;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24212;&#29992;&#36827;&#21270;&#21644;&#31574;&#30053;&#26799;&#24230;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#31574;&#30053;&#34920;&#31034;&#20026;&#20854;&#26399;&#26395;&#20540;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#24773;&#22659;&#25512;&#26029;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#23558;&#31574;&#30053;&#32593;&#32476;&#23884;&#20837;&#21040;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#37325;&#26032;&#26694;&#23450;&#20026;&#34920;&#31034;-&#21033;&#29992;&#38382;&#39064;&#65292;&#20854;&#20013;&#33391;&#22909;&#30340;&#31574;&#30053;&#34920;&#31034;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#36827;&#21270;&#21644;&#31574;&#30053;&#26799;&#24230;&#27861;&#26469;&#23637;&#31034;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102;&#31574;&#30053;&#34920;&#31034;&#22312;&#20915;&#23450;&#26368;&#20339;&#25506;&#32034;-&#21033;&#29992;&#31574;&#30053;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a representation-driven framework for reinforcement learning. By representing policies as estimates of their expected values, we leverage techniques from contextual bandits to guide exploration and exploitation. Particularly, embedding a policy network into a linear feature space allows us to reframe the exploration-exploitation problem as a representation-exploitation problem, where good policy representations enable optimal exploration. We demonstrate the effectiveness of this framework through its application to evolutionary and policy gradient-based approaches, leading to significantly improved performance compared to traditional methods. Our framework provides a new perspective on reinforcement learning, highlighting the importance of policy representation in determining optimal exploration-exploitation strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31961;&#38598;&#29702;&#35770;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#21270;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#20026;&#20449;&#24687;&#34920;&#65292;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#35299;&#20915;&#39046;&#22495;&#30693;&#35782;&#33719;&#21462;&#21644;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19718</link><description>&lt;p&gt;
&#31895;&#31961;&#38598;&#19979;&#19968;&#31181;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A rule-general abductive learning by rough sets. (arXiv:2305.19718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31961;&#38598;&#29702;&#35770;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#21270;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#20026;&#20449;&#24687;&#34920;&#65292;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#35299;&#20915;&#39046;&#22495;&#30693;&#35782;&#33719;&#21462;&#21644;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;&#36890;&#24120;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#12290;&#23558;&#20004;&#32773;&#32452;&#21512;&#36215;&#26469;&#36827;&#34892;&#23398;&#20064;&#30340;&#20219;&#21153;&#34987;&#31216;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#19987;&#23478;&#21487;&#20197;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#26631;&#35760;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#36825;&#20010;&#25805;&#20316;&#24456;&#26114;&#36149;&#12290;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#32467;&#21512;&#22312;&#22788;&#29702;&#20855;&#26377;&#39046;&#22495;&#30693;&#35782;&#30340;&#21322;&#30417;&#30563;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39046;&#22495;&#30693;&#35782;&#20197;&#21450;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#21644;&#29983;&#25104;&#20173;&#28982;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#31895;&#31961;&#38598;&#29702;&#35770;&#26159;&#35299;&#20915;&#20449;&#24687;&#31995;&#32479;&#20013;&#30693;&#35782;&#22788;&#29702;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#31961;&#38598;&#19979;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65288;RS-ABL&#65289;&#12290;&#36890;&#36807;&#23558;&#35268;&#21017;&#30340;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#36716;&#21270;&#20026;&#20449;&#24687;&#34920;&#65292;&#21033;&#29992;&#31895;&#31961;&#38598;&#29702;&#35770;&#26469;&#35299;&#20915;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#33719;&#21462;&#39046;&#22495;&#30693;&#35782;&#21644;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#35268;&#21017;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36824;&#21487;&#20197;&#29983;&#25104;&#26356;&#24191;&#27867;&#30340;&#36127;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#35268;&#21017;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world tasks, there is usually a large amount of unlabeled data and labeled data. The task of combining the two to learn is known as semi-supervised learning. Experts can use logical rules to label unlabeled data, but this operation is costly. The combination of perception and reasoning has a good effect in processing such semi-supervised tasks with domain knowledge. However, acquiring domain knowledge and the correction, reduction and generation of rules remain complex problems to be solved. Rough set theory is an important method for solving knowledge processing in information systems. In this paper, we propose a rule general abductive learning by rough set (RS-ABL). By transforming the target concept and sub-concepts of rules into information tables, rough set theory is used to solve the acquisition of domain knowledge and the correction, reduction and generation of rules at a lower cost. This framework can also generate more extensive negative rules to enhance the breadth of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#35813;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#27969;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19600</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#19979;&#30340;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning on Heterogeneous Data via Adaptive Self-Distillation. (arXiv:2305.19600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#35813;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#27969;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#20351;&#24471;&#23458;&#25143;&#26426;&#21487;&#20197;&#32858;&#21512;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#32780;&#26080;&#38656;&#20849;&#20139;&#20219;&#20309;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#20174;&#32780;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#20013;&#21457;&#29616;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35266;&#23519;&#21040;&#30340;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#22343;&#21248;&#24615;&#65288;&#20363;&#22914;&#31867;&#21035;&#19981;&#24179;&#34913;&#65289;&#12290;&#22312;&#36825;&#31181;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#20250;&#20986;&#29616;&#8220;&#23458;&#25143;&#26426;&#28418;&#31227;&#8221;&#38382;&#39064;&#65292;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#25910;&#25947;&#21040;&#20854;&#33258;&#24049;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#36825;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#27491;&#21017;&#21270;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#22312;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20043;&#19978;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#23458;&#25143;&#31471;&#25110;&#26381;&#21153;&#22120;&#20195;&#30721;&#36827;&#34892;&#20219;&#20309;&#26356;&#25913;&#65292;&#22240;&#27492;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#37096;&#32626;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm that enables clients to jointly train a global model by aggregating the locally trained models without sharing any local training data. In practice, there can often be substantial heterogeneity (e.g., class imbalance) across the local data distributions observed by each of these clients. Under such non-iid data distributions across clients, FL suffers from the 'client-drift' problem where every client converges to its own local optimum. This results in slower convergence and poor performance of the aggregated model. To address this limitation, we propose a novel regularization technique based on adaptive self-distillation (ASD) for training models on the client side. Our regularization scheme adaptively adjusts to the client's training data based on: (1) the closeness of the local model's predictions with that of the global model and (2) the client's label distribution. The proposed regularization can be easily integrated atop exis
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;meta-learning&#26694;&#26550;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22823;&#23567;&#21644;&#20998;&#24067;&#21464;&#21270;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.19587</link><description>&lt;p&gt;
&#38754;&#21521;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#20840;&#36890;&#29992;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Omni-generalizable Neural Methods for Vehicle Routing Problems. (arXiv:2305.19587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19587
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;meta-learning&#26694;&#26550;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22823;&#23567;&#21644;&#20998;&#24067;&#21464;&#21270;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36991;&#20813;&#20102;&#23545;&#25163;&#24037;&#35268;&#21017;&#30340;&#20381;&#36182;&#65292;&#23398;&#20064;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#22266;&#23450;&#22823;&#23567;&#21644;&#33410;&#28857;&#20998;&#24067;&#30340;&#21516;&#19968;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#22240;&#27492;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#35813;&#22330;&#26223;&#32771;&#34385;&#20102;VRP&#22312;&#22823;&#23567;&#21644;&#20998;&#24067;&#26041;&#38754;&#30340;&#19968;&#33324;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#25512;&#29702;&#26399;&#38388;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#19979;&#23545;&#21021;&#22987;&#21270;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#20943;&#23569;&#35757;&#32451;&#24320;&#38144;&#12290;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;CVRP&#65289;&#30340;&#21512;&#25104;&#21644;&#22522;&#20934;&#23454;&#20363;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/RoyalSkye/Omni-VRP&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning heuristics for vehicle routing problems (VRPs) has gained much attention due to the less reliance on hand-crafted rules. However, existing methods are typically trained and tested on the same task with a fixed size and distribution (of nodes), and hence suffer from limited generalization performance. This paper studies a challenging yet realistic setting, which considers generalization across both size and distribution in VRPs. We propose a generic meta-learning framework, which enables effective training of an initialized model with the capability of fast adaptation to new tasks during inference. We further develop a simple yet efficient approximation method to reduce the training overhead. Extensive experiments on both synthetic and benchmark instances of the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP) demonstrate the effectiveness of our method. The code is available at: https://github.com/RoyalSkye/Omni-VRP.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.18413</link><description>&lt;p&gt;
&#20174;API&#23398;&#20064;&#23398;&#20064;&#65306;&#40657;&#30418;&#25968;&#25454;&#26080;&#20851;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#65288;DFML&#65289;&#26088;&#22312;&#36890;&#36807;&#20174;&#19968;&#32452;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20803;&#23398;&#20064;&#32780;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;DFML&#24037;&#20316;&#20165;&#33021;&#20174;&#65288;i&#65289;&#30333;&#30418;&#21644;&#65288;ii&#65289;&#23567;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;iii&#65289;&#30456;&#21516;&#30340;&#26550;&#26500;&#20013;&#20803;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#26356;&#23454;&#38469;&#30340;&#35774;&#32622;&#65292;&#21363;&#29992;&#25143;&#20165;&#33021;&#36890;&#36807;&#20219;&#24847;&#27169;&#22411;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;API&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#25968;&#25454;&#26080;&#20851;&#20803;&#30693;&#35782;&#33976;&#39311;&#65288;BiDf-MKD&#65289;&#26694;&#26550;&#65292;&#23558;&#26356;&#36890;&#29992;&#30340;&#20803;&#30693;&#35782;&#20174;&#19968;&#32452;&#40657;&#30418;API&#36716;&#31227;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21452;&#23618;&#23398;&#20064;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#34920;&#24449;&#27491;&#30830;&#21442;&#25968;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.18394</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#23618;&#23398;&#20064;&#30340;&#26368;&#20248;&#27491;&#21017;&#21270;&#21442;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Optimal Regularization Parameters via Bilevel Learning. (arXiv:2305.18394v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21452;&#23618;&#23398;&#20064;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#34920;&#24449;&#27491;&#30830;&#21442;&#25968;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#27491;&#21017;&#21270;&#24120;&#29992;&#20110;&#35299;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#26469;&#25552;&#39640;&#20808;&#39564;&#20449;&#24687;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#21442;&#25968;&#21152;&#20197;&#26435;&#34913;&#65292;&#32780;&#21512;&#36866;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#20363;&#22914;&#24046;&#24322;&#21407;&#21017;&#21644;L-&#26354;&#32447;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#21512;&#36866;&#30340;&#21442;&#25968;&#20540;&#65292;&#20294;&#26159;&#36817;&#24180;&#26469;&#65292;&#19968;&#31181;&#21483;&#20570;&#21452;&#23618;&#23398;&#20064;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34987;&#29992;&#20110;&#30830;&#23450;&#26368;&#20248;&#21442;&#25968;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#31574;&#30053;&#26377;&#21508;&#31181;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21452;&#23618;&#23398;&#20064;&#30340;&#33391;&#22909;&#24615;&#36136;&#20173;&#28982;&#26159;&#19968;&#20010;&#21457;&#23637;&#20013;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#26465;&#20214;&#26469;&#34920;&#24449;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#27491;&#20540;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational regularization is commonly used to solve linear inverse problems, and involves augmenting a data fidelity by a regularizer. The regularizer is used to promote a priori information, and is weighted by a regularization parameter. Selection of an appropriate regularization parameter is critical, with various choices leading to very different reconstructions. Existing strategies such as the discrepancy principle and L-curve can be used to determine a suitable parameter value, but in recent years a supervised machine learning approach called bilevel learning has been employed. Bilevel learning is a powerful framework to determine optimal parameters, and involves solving a nested optimisation problem. While previous strategies enjoy various theoretical results, the well-posedness of bilevel learning in this setting is still a developing field. One necessary property is positivity of the determined regularization parameter. In this work, we provide a new condition that better char
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.17303</link><description>&lt;p&gt;
&#20174;&#40657;&#30418;&#27169;&#22411;&#21040;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36716;&#21270;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;AI&#27169;&#22411;&#26159;&#21307;&#30103;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#21363;&#20351;&#36755;&#20837;&#20998;&#24067;&#36731;&#24494;&#31227;&#20301;&#65288;&#20363;&#22914;&#25195;&#25551;&#20202;&#31867;&#22411;&#65289;&#65292;&#20063;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#32780;&#25918;&#23556;&#31185;&#21307;&#29983;&#21017;&#20381;&#36182;&#20110;&#24322;&#24120;&#24615;&#30340;&#36890;&#29992;&#25551;&#36848;&#24615;&#35268;&#21017;&#12290;&#24494;&#35843;&#27169;&#22411;&#20197;&#23558;&#30693;&#35782;&#20174;&#19968;&#20010;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#38024;&#23545;&#26410;&#30693;&#30340;&#30446;&#26631;&#22495;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35748;&#20026;NN&#30340;&#21487;&#35299;&#37322;&#32452;&#20214;&#22823;&#33268;&#26159;&#22495;&#19981;&#21464;&#30340;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#19981;&#21450;&#23427;&#20204;&#30340;BB&#21464;&#20307;&#12290;&#22312;&#28304;&#22495;&#20013;&#25105;&#20204;&#20808;&#20351;&#29992;&#20154;&#31867;&#29702;&#35299;&#30340;&#27010;&#24565;&#20174;BB&#24320;&#22987;&#65292;&#23558;&#20854;&#25552;&#28860;&#25104;&#19968;&#32452;&#27973;&#26174;&#26131;&#25026;&#30340;interpretable&#27169;&#22411;&#12290;&#30001;&#20110;&#27599;&#20010;interpretable&#27169;&#22411;&#37117;&#35206;&#30422;&#20102;&#25968;&#25454;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20855;&#26377;&#19968;&#32452;interpretable&#27169;&#22411;&#30340;&#28151;&#21512;&#21487;&#20197;&#23454;&#29616;&#19982;BB&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#30697;&#38453;&#21644;&#26684;&#25289;&#22982;&#36845;&#20195;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Lipschitz&#24120;&#25968;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#31934;&#30830;&#12289;&#24555;&#36895;&#12289;&#21487;&#24494;&#20998;&#65292;&#24182;&#23637;&#29616;&#20102;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;&#22312;&#23454;&#39564;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16173</link><description>&lt;p&gt;
&#36890;&#36807;&#26684;&#25289;&#22982;&#36845;&#20195;&#23454;&#29616;&#21367;&#31215;&#23618;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#30340;&#39640;&#25928;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration. (arXiv:2305.16173v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#30697;&#38453;&#21644;&#26684;&#25289;&#22982;&#36845;&#20195;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Lipschitz&#24120;&#25968;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#31934;&#30830;&#12289;&#24555;&#36895;&#12289;&#21487;&#24494;&#20998;&#65292;&#24182;&#23637;&#29616;&#20102;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;&#22312;&#23454;&#39564;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#30340;&#25511;&#21046;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12289;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#22240;&#27492;&#20272;&#35745;&#36825;&#20010;&#20540;&#26159;&#30446;&#21069;&#30340;&#19968;&#20010;&#31185;&#23398;&#38590;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24490;&#29615;&#30697;&#38453;&#29702;&#35770;&#21644;&#19968;&#31181;&#26032;&#30340;&#21151;&#29575;&#36845;&#20195;&#26367;&#20195;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#21487;&#24494;&#20998;&#30340;&#19978;&#30028;&#65292;&#29992;&#20110;&#21367;&#31215;&#23618;&#30340;&#35889;&#33539;&#25968;&#12290;&#31216;&#20026;&#26684;&#25289;&#22982;&#36845;&#20195;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#19968;&#20010;&#36229;&#32447;&#24615;&#30340;&#25910;&#25947;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#38750;&#24120;&#26377;&#25928;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/blaisedelattre/lip4conv &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the control of the Lipschitz constant has a great impact on the training stability, generalization, and robustness of neural networks, the estimation of this value is nowadays a real scientific challenge. In this paper we introduce a precise, fast, and differentiable upper bound for the spectral norm of convolutional layers using circulant matrix theory and a new alternative to the Power iteration. Called the Gram iteration, our approach exhibits a superlinear convergence. First, we show through a comprehensive set of experiments that our approach outperforms other state-of-the-art methods in terms of precision, computational cost, and scalability. Then, it proves highly effective for the Lipschitz regularization of convolutional neural networks, with competitive results against concurrent approaches. Code is available at https://github.com/blaisedelattre/lip4conv.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30452;&#25509;&#24212;&#29992;&#20110;&#21407;&#22987;&#38899;&#39057;&#25968;&#25454;&#26469;&#25506;&#32034;&#28508;&#22312;&#38899;&#39057;&#31354;&#38388;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15571</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#36827;&#34892;&#28508;&#22312;&#38899;&#39057;&#31354;&#38388;&#25506;&#32034;&#30340;&#22768;&#38899;&#35774;&#35745;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Sound Design Strategies for Latent Audio Space Explorations Using Deep Learning Architectures. (arXiv:2305.15571v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30452;&#25509;&#24212;&#29992;&#20110;&#21407;&#22987;&#38899;&#39057;&#25968;&#25454;&#26469;&#25506;&#32034;&#28508;&#22312;&#38899;&#39057;&#31354;&#38388;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22768;&#38899;&#21644;&#38899;&#20048;&#35745;&#31639;&#26041;&#38754;&#30340;&#24212;&#29992;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#25216;&#26415;&#19982;&#23427;&#20204;&#22914;&#20309;&#34987;&#32435;&#20837;&#30495;&#23454;&#19990;&#30028;&#30340;&#33402;&#26415;&#23454;&#36341;&#20043;&#38388;&#20173;&#23384;&#22312;&#30528;&#19968;&#20123;&#32570;&#22833;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#8212;&#8212;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;&#20043;&#21069;&#65292;VAE&#24050;&#32463;&#34987;&#29992;&#20110;&#29983;&#25104;&#28508;&#22312;&#38899;&#33394;&#31354;&#38388;&#25110;&#31526;&#21495;&#38899;&#20048;&#20363;&#23376;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#26412;&#30740;&#31350;&#23558;VAE&#30452;&#25509;&#24212;&#29992;&#20110;&#21407;&#22987;&#38899;&#39057;&#25968;&#25454;&#32780;&#19981;&#26159;&#38899;&#39057;&#29305;&#24449;&#25552;&#21462;&#30340;&#38899;&#39057;&#25968;&#25454;&#19978;&#65292;&#26082;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#38899;&#39057;&#24405;&#38899;&#65292;&#21516;&#26102;&#20063;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research in Deep Learning applications in sound and music computing have gathered an interest in the recent years; however, there is still a missing link between these new technologies and on how they can be incorporated into real-world artistic practices. In this work, we explore a well-known Deep Learning architecture called Variational Autoencoders (VAEs). These architectures have been used in many areas for generating latent spaces where data points are organized so that similar data points locate closer to each other. Previously, VAEs have been used for generating latent timbre spaces or latent spaces of symbolic music excepts. Applying VAE to audio features of timbre requires a vocoder to transform the timbre generated by the network to an audio signal, which is computationally expensive. In this work, we apply VAEs to raw audio data directly while bypassing audio feature extraction. This approach allows the practitioners to use any audio recording while giving flexibility an
&lt;/p&gt;</description></item><item><title>PromptNER&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31639;&#27861;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#21644;&#36328;&#39046;&#22495;NER&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15444</link><description>&lt;p&gt;
PromptNER: &#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PromptNER: Prompting For Named Entity Recognition. (arXiv:2305.15444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15444
&lt;/p&gt;
&lt;p&gt;
PromptNER&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31639;&#27861;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#21644;&#36328;&#39046;&#22495;NER&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29616;&#22312;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29616;&#25104;&#26041;&#27861;&#65292;&#20026;&#21508;&#31181;&#32463;&#20856;&#30340;NLP&#38382;&#39064;&#25552;&#20379;&#20102;&#23569;&#37327;&#26679;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#20196;&#20154;&#26399;&#24453;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26041;&#38754;&#20173;&#36828;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#31471;&#21040;&#31471;&#32467;&#26500;&#29702;&#35299;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#22312;&#26631;&#20934;&#26631;&#35760;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PromptNER&#65292;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#23569;&#26679;&#26412;&#21644;&#36328;&#39046;&#22495;NER&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;&#20026;&#20102;&#36866;&#24212;&#20219;&#20309;&#26032;&#30340;NER&#20219;&#21153;&#65292;PromptNER&#38656;&#35201;&#25552;&#20379;&#19968;&#32452;&#23454;&#20307;&#23450;&#20041;&#65292;&#38500;&#22522;&#26412;&#30340;&#23569;&#26679;&#26412;&#26679;&#20363;&#20197;&#22806;&#12290;&#32473;&#23450;&#36755;&#20837;&#21477;&#23376;&#65292;PromptNER&#25552;&#31034;LLM&#29983;&#25104;&#19968;&#20010;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#37322;&#65292;&#35777;&#26126;&#23427;&#20204;&#19982;&#25552;&#20379;&#30340;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#30340;&#20860;&#23481;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PromptNER&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;WikiAnn&#25968;&#25454;&#38598;&#19978;&#20026;&#36328;&#39046;&#22495;NER&#35774;&#23450;&#20102;&#26032;&#30340;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-sho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#29356;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#65292;&#20998;&#31867;&#31934;&#24230;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.15424</link><description>&lt;p&gt;
PulseNet: &#20351;&#29992;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#36827;&#34892;&#29356;ECG&#20449;&#21495;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
PulseNet: Deep Learning ECG-signal classification using random augmentation policy and continous wavelet transform for canines. (arXiv:2305.15424v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#29356;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#65292;&#20998;&#31867;&#31934;&#24230;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#29356;&#30340;&#24515;&#30005;&#22270;(ECG)&#38656;&#35201;&#29087;&#32451;&#30340;&#20861;&#21307;&#65292;&#20294;&#30446;&#21069;&#21487;&#29992;&#30340;&#20861;&#21307;&#24515;&#33039;&#30149;&#19987;&#23478;&#29992;&#20110;ECG&#35299;&#35835;&#21644;&#35786;&#26029;&#25903;&#25345;&#30340;&#25968;&#37327;&#26377;&#38480;&#12290;&#24320;&#21457;&#33258;&#21160;&#35780;&#20272;ECG&#24207;&#21015;&#30340;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20020;&#24202;&#21307;&#29983;&#23454;&#26102;&#32467;&#26524;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#26469;&#25913;&#21892;&#20861;&#21307;&#25252;&#29702;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#23558;&#29356;&#30340;&#24515;&#30005;&#22270;&#24207;&#21015;&#20998;&#31867;&#20026;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;&#23558;ECG&#35760;&#24405;&#36716;&#25442;&#20026;8&#31186;&#30340;&#31532;&#20108;&#23548;&#32852;&#24207;&#21015;&#65292;&#26681;&#25454;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#25110;&#22810;&#31181;&#24515;&#33039;&#24322;&#24120;&#23558;&#20854;&#20998;&#31867;&#20026;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;&#35757;&#32451;ECG&#24207;&#21015;&#20351;&#29992;RandomAugmentECG&#36827;&#34892;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#35813;&#39033;&#30446;&#23454;&#29616;&#30340;&#26032;&#22686;&#24378;&#24211;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#22359;&#20351;&#29992;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#36716;&#25442;&#25104;2D scalogram&#12290;2D scalogram&#20351;&#29992;&#20108;&#20803;CNN&#20998;&#31867;&#22120;&#20998;&#31867;&#25104;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating canine electrocardiograms (ECG) require skilled veterinarians, but current availability of veterinary cardiologists for ECG interpretation and diagnostic support is limited. Developing tools for automated assessment of ECG sequences can improve veterinary care by providing clinicians real-time results and decision support tools. We implement a deep convolutional neural network (CNN) approach for classifying canine electrocardiogram sequences as either normal or abnormal. ECG records are converted into 8 second Lead II sequences and classified as either normal (no evidence of cardiac abnormalities) or abnormal (presence of one or more cardiac abnormalities). For training ECG sequences are randomly augmented using RandomAugmentECG, a new augmentation library implemented specifically for this project. Each chunk is then is converted using a continuous wavelet transform into a 2D scalogram. The 2D scalogram are then classified as either normal or abnormal by a binary CNN classif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#30340;&#31639;&#27861;&#20559;&#35265;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#12289;&#20891;&#20107;&#21270;&#12289;&#27450;&#35784;&#21644;&#29615;&#22659;&#38382;&#39064;&#65292;&#26088;&#22312;&#20419;&#36827;&#21746;&#23398;&#12289;&#25919;&#27835;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#20851;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;</title><link>http://arxiv.org/abs/2305.15239</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#20262;&#29702;&#23398;
&lt;/p&gt;
&lt;p&gt;
Deep Learning and Ethics. (arXiv:2305.15239v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#30340;&#31639;&#27861;&#20559;&#35265;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#12289;&#20891;&#20107;&#21270;&#12289;&#27450;&#35784;&#21644;&#29615;&#22659;&#38382;&#39064;&#65292;&#26088;&#22312;&#20419;&#36827;&#21746;&#23398;&#12289;&#25919;&#27835;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#20851;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;Prince&#65288;2023&#24180;&#65289;&#12298;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#12299;&#30340;&#31532;21&#31456;&#65292;&#25945;&#26448;&#30340;&#23436;&#25972;&#33609;&#31295;&#21487;&#22312;&#27492;http URL&#33719;&#24471;&#12290;&#26412;&#31456;&#32771;&#34385;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35774;&#35745;&#21644;&#20351;&#29992;&#21487;&#33021;&#20135;&#29983;&#30340;&#28508;&#22312;&#21361;&#23475;&#65292;&#21253;&#25324;&#31639;&#27861;&#20559;&#35265;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#12289;&#20891;&#20107;&#21270;&#12289;&#27450;&#35784;&#21644;&#29615;&#22659;&#38382;&#39064;&#12290;&#30446;&#30340;&#19981;&#26159;&#20026;&#20102;&#25552;&#20379;&#26356;&#21152;&#36947;&#24503;&#30340;&#24314;&#35758;&#12290;&#30456;&#21453;&#65292;&#30446;&#26631;&#26159;&#22312;&#21746;&#23398;&#12289;&#25919;&#27835;&#31185;&#23398;&#21644;&#26356;&#24191;&#27867;&#30340;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#24341;&#21457;&#20851;&#38190;&#39046;&#22495;&#30340;&#24605;&#24819;&#21644;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article appears as chapter 21 of Prince (2023, Understanding Deep Learning); a complete draft of the textbook is available here: this http URL This chapter considers potential harms arising from the design and use of AI systems. These include algorithmic bias, lack of explainability, data privacy violations, militarization, fraud, and environmental concerns. The aim is not to provide advice on being more ethical. Instead, the goal is to express ideas and start conversations in key areas that have received attention in philosophy, political science, and the broader social sciences.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14449</link><description>&lt;p&gt;
&#22270;&#35889;&#36935;&#35265;LLM&#65306;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#30340;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding. (arXiv:2305.14449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14449
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;Alexa&#65292;Siri&#65292;Google Assistant&#31561;&#65289;&#38656;&#35201;&#29702;&#35299;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#20197;&#30830;&#20445;&#31283;&#20581;&#30340;&#20250;&#35805;&#29702;&#35299;&#24182;&#20943;&#23569;&#29992;&#25143;&#25705;&#25830;&#12290;&#36825;&#20123;&#26377;&#32570;&#38519;&#30340;&#26597;&#35810;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#30340;&#27495;&#20041;&#21644;&#38169;&#35823;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20013;&#30340;&#38169;&#35823;&#24341;&#36215;&#30340;&#12290;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#65288;&#20010;&#24615;&#21270;QR&#65289;&#26088;&#22312;&#20943;&#23569;&#36523;&#20307;&#21644;&#23614;&#37096;&#29992;&#25143;&#26597;&#35810;&#27969;&#37327;&#20013;&#30340;&#32570;&#38519;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#21435;&#25104;&#21151;&#30340;&#29992;&#25143;&#20132;&#20114;&#30340;&#32034;&#24341;&#12290;&#26412;&#25991;&#25552;&#20986;&#25105;&#20204;&#30340;&#8220;&#21327;&#21516;&#26597;&#35810;&#37325;&#20889;&#8221;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#37325;&#20889;&#29992;&#25143;&#21382;&#21490;&#20013;&#27809;&#26377;&#20986;&#29616;&#36807;&#30340;&#26032;&#22411;&#29992;&#25143;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#29992;&#25143;&#21453;&#39304;&#20132;&#20114;&#22270;&#8221;&#65288;FIG&#65289;&#65292;&#30001;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#32452;&#25104;&#65292;&#24182;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#26469;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65288;&#21363;&#21327;&#21516;&#29992;&#25143;&#32034;&#24341;&#65289;&#65292;&#20174;&#32780;&#24110;&#21161;&#35206;&#30422;&#26410;&#26469;&#26410;&#26366;&#35265;&#36807;&#30340;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#20123;&#26032;&#30340;&#20016;&#23500;&#32034;&#24341;&#34987;&#22122;&#22768;&#21453;&#39304;&#20132;&#20114;&#25152;&#25903;&#37197;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26377;&#38480;&#20869;&#23384;BFGS&#65288;LLM&#65289;&#31639;&#27861;&#21644;&#22238;&#36864;&#26041;&#26696;&#26469;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;QR&#26041;&#27861;&#65292;&#24182;&#22312;&#26410;&#30475;&#21040;&#30340;&#29992;&#25143;&#20132;&#20114;&#19978;&#21462;&#24471;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational AI systems (e.g. Alexa, Siri, Google Assistant, etc.) need to understand queries with defects to ensure robust conversational understanding and reduce user frictions. The defective queries are often induced by user ambiguities and mistakes, or errors in the automatic speech recognition (ASR) and natural language understanding (NLU).  Personalized query rewriting (personalized QR) targets reducing defects in the torso and tail user query traffic, and it typically relies on an index of past successful user interactions with the conversational AI. This paper presents our "Collaborative Query Rewriting" approach that focuses on rewriting novel user interactions unseen in the user history. This approach builds a "user Feedback Interaction Graph" (FIG) consisting of historical user-entity interactions, and leverages multi-hop customer affinity to enrich each user's index (i.e. the Collaborative User Index) that would help cover future unseen defective queries. To counteract th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;&#23398;&#20064;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#29702;&#24212;&#29992;&#20110;&#21860;&#37202;&#29983;&#20135;&#20013;&#30340;&#21860;&#37202;&#33457;&#21697;&#31181;&#20998;&#31867;&#65292;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#21327;&#21516;&#20316;&#29992;&#22686;&#24378;&#33719;&#21462;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.13447</link><description>&lt;p&gt;
&#21516;&#26102;&#23398;&#20064;&#27491;&#21017;&#21270;&#26041;&#27861;&#65306;&#20197;&#21860;&#37202;&#33457;&#20998;&#31867;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Regularization Through Simultaneous Learning: A Case Study for Hop Classification. (arXiv:2305.13447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;&#23398;&#20064;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#29702;&#24212;&#29992;&#20110;&#21860;&#37202;&#29983;&#20135;&#20013;&#30340;&#21860;&#37202;&#33457;&#21697;&#31181;&#20998;&#31867;&#65292;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#21327;&#21516;&#20316;&#29992;&#22686;&#24378;&#33719;&#21462;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#25311;&#21512;&#20173;&#28982;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#65292;&#23548;&#33268;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#37319;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26159;&#25269;&#21046;&#36825;&#19968;&#25361;&#25112;&#30340;&#24120;&#35265;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65306;Simultaneous Learning&#65292;&#23427;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#29702;&#65292;&#19987;&#38376;&#24212;&#29992;&#20110;&#21860;&#37202;&#29983;&#20135;&#20013;&#30340;&#21860;&#37202;&#33457;&#21697;&#31181;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#19982;&#30446;&#26631;&#25968;&#25454;&#38598;&#21327;&#21516;&#24037;&#20316;&#65292;&#20174;&#32780;&#22686;&#24378;&#33719;&#21462;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#30340;&#26368;&#32456;&#23618;&#36827;&#34892;&#25112;&#30053;&#24615;&#20462;&#25913;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20998;&#31867;&#65292;&#26080;&#38656;&#23558;&#23427;&#20204;&#35270;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#21253;&#25324;&#32452;&#38388;&#24809;&#32602;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;InceptionV3&#21644;ResNet50&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#25351;&#23450;&#20102;UFOP-HVD&#21860;&#37202;&#33457;&#21494;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overfitting remains a prevalent challenge in deep neural networks, leading to suboptimal real-world performance. Employing regularization techniques is a common strategy to counter this challenge, improving model generalization. This paper proposes Simultaneous Learning, a novel regularization approach drawing on Transfer Learning and Multi-task Learning principles, applied specifically to the classification of hop varieties - an integral component of beer production. Our approach harnesses the power of auxiliary datasets in synergy with the target dataset to amplify the acquisition of highly relevant features. Through a strategic modification of the model's final layer, we enable the simultaneous classification of both datasets without the necessity to treat them as disparate tasks. To realize this, we formulate a loss function that includes an inter-group penalty. We conducted experimental evaluations using the InceptionV3 and ResNet50 models, designating the UFOP-HVD hop leaf datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65306;&#22914;&#20309;&#36890;&#36807;&#21516;&#31751;&#39044;&#35328;&#26426;&#22312;&#23384;&#22312;&#26377;&#38480;&#23545;&#25239;&#38169;&#35823;&#26102;&#31215;&#26497;&#23398;&#20064;&#23436;&#20840;&#24674;&#22797;&#21010;&#20998;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35299;&#26512;&#26694;&#26550;&#24182;&#35777;&#26126;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#19978;&#19979;&#30028;&#65292;&#24182;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.13402</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#31751;&#39044;&#35328;&#26426;&#30340;&#23481;&#38169;&#31934;&#30830;&#26597;&#35810;&#23398;&#20064;&#26377;&#38480;&#38598;&#21512;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Error-Tolerant Exact Query Learning of Finite Set Partitions with Same-Cluster Oracle. (arXiv:2305.13402v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65306;&#22914;&#20309;&#36890;&#36807;&#21516;&#31751;&#39044;&#35328;&#26426;&#22312;&#23384;&#22312;&#26377;&#38480;&#23545;&#25239;&#38169;&#35823;&#26102;&#31215;&#26497;&#23398;&#20064;&#23436;&#20840;&#24674;&#22797;&#21010;&#20998;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35299;&#26512;&#26694;&#26550;&#24182;&#35777;&#26126;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#19978;&#19979;&#30028;&#65292;&#24182;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#23384;&#22312;&#26377;&#38480;&#30340;&#23545;&#25239;&#38169;&#35823;&#26102;&#65292;&#20165;&#36890;&#36807;&#21516;&#31751;&#39044;&#35328;&#26426;&#26469;&#31215;&#26497;&#23398;&#20064;&#23436;&#20840;&#24674;&#22797;&#21010;&#20998;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#31361;&#20986;&#20102;&#23398;&#20064;&#21010;&#20998;&#21644;&#30456;&#20851;&#32858;&#31867;&#20043;&#38388;&#30340;&#26032;&#39062;&#32852;&#31995;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#20026;&#36825;&#20010;&#38382;&#39064;&#24314;&#31435;&#20102;&#19968;&#20010;R&#233;nyi-Ulam&#26679;&#24335;&#30340;&#35299;&#26512;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#19978;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38480;&#21046;&#20102;&#30456;&#20851;&#38543;&#26426;&#31639;&#27861;&#30340;&#26399;&#26395;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24230;&#22312;&#35813;&#38382;&#39064;&#21644;&#30456;&#20851;&#21464;&#20307;&#20013;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper initiates the study of active learning for exact recovery of partitions exclusively through access to a same-cluster oracle in the presence of bounded adversarial error. We first highlight a novel connection between learning partitions and correlation clustering. Then we use this connection to build a R\'enyi-Ulam style analytical framework for this problem, and prove upper and lower bounds on its worst-case query complexity. Further, we bound the expected performance of a relevant randomized algorithm. Finally, we study the relationship between adaptivity and query complexity for this problem and related variants.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.11283</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#21644;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Mean-Field Model-Based Eluder Dimension (MBED)&#30340;&#26032;&#27010;&#24565;&#65292;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;$\epsilon$&#20248;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;MFC&#25110;$\epsilon$&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#36866;&#29992;&#20110;MFG&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#19982;&#30456;&#20851;&#21442;&#25968;&#26080;&#20851;&#65292;&#19982;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#20195;&#29702;&#25968;&#37327;&#26080;&#20851;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#36991;&#20813;&#20102;&#20197;&#21069;&#30340;&#24378;&#32467;&#26500;&#20551;&#35774;&#12290;&#26368;&#21518;&#65292;&#22312;tabular&#35774;&#32622;&#19979;&#65292;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#30340;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#20197;&#36924;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#35813;&#21407;&#29702;&#21487;&#20197;&#22788;&#29702;&#27169;&#22411;&#20803;&#32032;&#19981;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#20248;&#20110;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#12290;&#21516;&#26102;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.09868</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
The Principle of Uncertain Maximum Entropy. (arXiv:2305.09868v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09868
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#35813;&#21407;&#29702;&#21487;&#20197;&#22788;&#29702;&#27169;&#22411;&#20803;&#32032;&#19981;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#20248;&#20110;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#12290;&#21516;&#26102;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#29109;&#21407;&#29702;&#22312;&#20449;&#24687;&#29702;&#35770;&#20013;&#30340;&#24341;&#20837;&#65292;&#20026;&#32479;&#35745;&#21147;&#23398;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#29983;&#24577;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#20854;&#24471;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20652;&#21270;&#21058;&#65292;&#20419;&#36827;&#30740;&#31350;&#20154;&#21592;&#23558;&#20182;&#20204;&#30340;&#32463;&#39564;&#35266;&#23519;&#26144;&#23556;&#21040;&#33719;&#21462;&#26080;&#20559;&#27169;&#22411;&#65292;&#21516;&#26102;&#21152;&#28145;&#20102;&#23545;&#22797;&#26434;&#31995;&#32479;&#21644;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#22411;&#20803;&#32032;&#19981;&#30452;&#25509;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#23384;&#22312;&#22122;&#22768;&#25110;&#30524;&#37096;&#36974;&#25377;&#30340;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#26368;&#22823;&#29109;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21305;&#37197;&#29305;&#24449;&#32422;&#26463;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#65292;&#23613;&#31649;&#23384;&#22312;&#20219;&#24847;&#22122;&#22768;&#35266;&#23519;&#65292;&#23427;&#21516;&#26102;&#23558;&#25152;&#26377;&#21487;&#29992;&#20449;&#24687;&#32534;&#30721;&#65292;&#32780;&#19988;&#20248;&#20110;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#30340;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#22312;&#19982;&#26368;&#22823;&#20284;&#28982;&#31639;&#27861;&#30456;&#27604;&#26102;&#24314;&#31435;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The principle of maximum entropy, as introduced by Jaynes in information theory, has contributed to advancements in various domains such as Statistical Mechanics, Machine Learning, and Ecology. Its resultant solutions have served as a catalyst, facilitating researchers in mapping their empirical observations to the acquisition of unbiased models, whilst deepening the understanding of complex systems and phenomena. However, when we consider situations in which the model elements are not directly observable, such as when noise or ocular occlusion is present, possibilities arise for which standard maximum entropy approaches may fail, as they are unable to match feature constraints. Here we show the Principle of Uncertain Maximum Entropy as a method that both encodes all available information in spite of arbitrarily noisy observations while surpassing the accuracy of some ad-hoc methods. Additionally, we utilize the output of a black-box machine learning model as input into an uncertain ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20998;&#26512;&#30340;&#40065;&#26834;&#24178;&#39044;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#65292;&#36991;&#20813;&#22122;&#38899;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2305.08950</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#22240;&#26524;&#20998;&#26512;&#26041;&#27861;&#29992;&#20110;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causal Analysis for Robust Interpretability of Neural Networks. (arXiv:2305.08950v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20998;&#26512;&#30340;&#40065;&#26834;&#24178;&#39044;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#65292;&#36991;&#20813;&#22122;&#38899;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#35299;&#37322;&#23545;&#20110;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#30340;&#21487;&#38752;&#24320;&#21457;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#35299;&#37322;&#26041;&#27861;&#38598;&#20013;&#22312;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#24230;&#37327;&#19978;&#65292;&#20197;&#23558;&#27169;&#22411;&#20915;&#31574;&#24402;&#22240;&#20110;&#20010;&#21035;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#35757;&#32451;&#38454;&#27573;&#20013;&#32534;&#30721;&#22312;&#27169;&#22411;&#20013;&#30340;&#22122;&#22768;&#21644;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#26377;&#20559;&#36755;&#20837;&#65292;&#27169;&#22411;&#36807;&#25311;&#21512;&#25110;&#38169;&#37197;&#65289;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#36807;&#31243;&#24050;&#32463;&#35777;&#26126;&#20250;&#20135;&#29983;&#22024;&#26434;&#21644;&#19981;&#31283;&#23450;&#30340;&#24402;&#22240;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#36879;&#26126;&#29702;&#35299;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20998;&#26512;&#30340;&#40065;&#26834;&#24178;&#39044;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22240;&#26524;&#26426;&#21046;&#21450;&#20854;&#19982;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#36335;&#24452;&#24178;&#39044;&#65292;&#20197;&#25512;&#26029;&#38544;&#34255;&#23618;&#20013;&#30340;&#22240;&#26524;&#26426;&#21046;&#24182;&#38548;&#31163;&#30456;&#20851;&#21644;&#24517;&#35201;&#30340;&#20449;&#24687;&#65288;&#20197;&#36827;&#34892;&#27169;&#22411;&#39044;&#27979;&#65289;&#65292;&#20174;&#32780;&#36991;&#20813;&#22122;&#38899;&#30340;&#24178;&#25200;&#12290;&#32467;&#26524;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#31283;&#20581;&#19988;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting the inner function of neural networks is crucial for the trustworthy development and deployment of these black-box models. Prior interpretability methods focus on correlation-based measures to attribute model decisions to individual examples. However, these measures are susceptible to noise and spurious correlations encoded in the model during the training phase (e.g., biased inputs, model overfitting, or misspecification). Moreover, this process has proven to result in noisy and unstable attributions that prevent any transparent understanding of the model's behavior. In this paper, we develop a robust interventional-based method grounded by causal analysis to capture cause-effect mechanisms in pre-trained neural networks and their relation to the prediction. Our novel approach relies on path interventions to infer the causal mechanisms within hidden layers and isolate relevant and necessary information (to model prediction), avoiding noisy ones. The result is task-specifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20108;&#27425;&#21151;&#33021;&#21152;&#23494;&#30340;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#36991;&#20813;&#20449;&#24687;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08358</link><description>&lt;p&gt;
&#22522;&#20110;&#20108;&#27425;&#21151;&#33021;&#21152;&#23494;&#30340;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#20013;&#23433;&#20840;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quadratic Functional Encryption for Secure Training in Vertical Federated Learning. (arXiv:2305.08358v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20108;&#27425;&#21151;&#33021;&#21152;&#23494;&#30340;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#36991;&#20813;&#20449;&#24687;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#25903;&#25345;&#22312;&#22810;&#20010;&#25968;&#25454;&#38544;&#31169;&#24471;&#21040;&#20445;&#25252;&#30340;&#21442;&#19982;&#26041;&#20043;&#38388;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#22312;VFL&#20013;&#65292;&#26631;&#31614;&#20165;&#23545;&#19968;&#20010;&#26041;&#21487;&#35265;&#65292;&#20165;&#24403;&#25152;&#26377;&#21442;&#19982;&#26041;&#30340;&#25968;&#25454;&#34987;&#32452;&#21512;&#21518;&#25165;&#24418;&#25104;&#23436;&#25972;&#30340;&#29305;&#24449;&#38598;&#12290;&#26368;&#36817;&#65292;Xu&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedV&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#36755;&#20837;&#21151;&#33021;&#21152;&#23494;&#29992;&#20110;VFL&#30340;&#23433;&#20840;&#26799;&#24230;&#35745;&#31639;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#23545;&#20110;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#35757;&#32451;&#65292;&#20351;&#29992;&#20108;&#27425;&#21151;&#33021;&#21152;&#23494;&#21487;&#20197;&#36991;&#20813;Xu&#31561;&#20154;&#30340;&#20449;&#24687;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) enables the collaborative training of machine learning (ML) models in settings where the data is distributed amongst multiple parties who wish to protect the privacy of their individual data. Notably, in VFL, the labels are available to a single party and the complete feature set is formed only when data from all parties is combined. Recently, Xu et al. proposed a new framework called FedV for secure gradient computation for VFL using multi-input functional encryption. In this work, we explain how some of the information leakage in Xu et al. can be avoided by using Quadratic functional encryption when training generalized linear models for vertical federated learning.
&lt;/p&gt;</description></item><item><title>TIPS&#26159;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;AnytimeNNs&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#36129;&#29486;&#26368;&#22823;&#30340;&#36335;&#24452;&#26469;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;2%-6.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#20934;&#30830;&#29575;-FLOPs&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.08021</link><description>&lt;p&gt;
TIPS&#65306;&#20219;&#20309;&#26102;&#20505;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#37325;&#35201;&#36335;&#24452;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
TIPS: Topologically Important Path Sampling for Anytime Neural Networks. (arXiv:2305.08021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08021
&lt;/p&gt;
&lt;p&gt;
TIPS&#26159;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;AnytimeNNs&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#36129;&#29486;&#26368;&#22823;&#30340;&#36335;&#24452;&#26469;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;2%-6.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#20934;&#30830;&#29575;-FLOPs&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#26102;&#20505;&#31070;&#32463;&#32593;&#32476;(AnytimeNNs) &#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#21508;&#31181;&#30828;&#20214;&#36164;&#28304;&#32422;&#26463;&#19979;&#36866;&#24212;&#24615;&#35843;&#25972;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35774;&#35745;&#30340; AnytimeNNs &#24448;&#24448;&#20250;&#21463;&#21040;&#35774;&#35745;&#24072;&#20808;&#21069;&#32463;&#39564;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#20379;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#25163;&#24037;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558; AnytimeNNs &#30340;&#35757;&#32451;&#36807;&#31243;&#24314;&#27169;&#20026;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;(DTMC)&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#35782;&#21035;&#23545; AnytimeNNs &#35757;&#32451;&#36129;&#29486;&#26368;&#22823;&#30340;&#36335;&#24452;&#12290;&#22522;&#20110;&#36825;&#31181;&#26032;&#30340; DTMC &#22522;&#30784;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102; TIPS (Topologically Important Path Sampling) &#26694;&#26550;&#65292;&#20197;&#33258;&#21160;&#35774;&#35745;&#36866;&#24212;&#21508;&#31181;&#30828;&#20214;&#32422;&#26463;&#30340; AnytimeNNs&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TIPS &#33021;&#22815;&#25552;&#39640; AnytimeNNs &#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#19982;&#29616;&#26377; AnytimeNNs &#26041;&#27861;&#30456;&#27604;&#65292;TIPS &#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23558;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102; 2%-6.6%&#65292;&#24182;&#23454;&#29616;&#20102; SOTA &#30340;&#20934;&#30830;&#29575;-FLOPs &#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anytime neural networks (AnytimeNNs) are a promising solution to adaptively adjust the model complexity at runtime under various hardware resource constraints. However, the manually-designed AnytimeNNs are biased by designers' prior experience and thus provide sub-optimal solutions. To address the limitations of existing hand-crafted approaches, we first model the training process of AnytimeNNs as a discrete-time Markov chain (DTMC) and use it to identify the paths that contribute the most to the training of AnytimeNNs. Based on this new DTMC-based analysis, we further propose TIPS, a framework to automatically design AnytimeNNs under various hardware constraints. Our experimental results show that TIPS can improve the convergence rate and test accuracy of AnytimeNNs. Compared to the existing AnytimeNNs approaches, TIPS improves the accuracy by 2%-6.6% on multiple datasets and achieves SOTA accuracy-FLOPs tradeoffs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08014</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#38754;&#32908;&#30005;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#30636;&#26102;&#39640;&#28165;&#32908;&#30005;&#22270;&#20687;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#21487;&#20197;&#24320;&#36767;&#21457;&#23637;&#26356;&#27969;&#30021;&#12289;&#26356;&#33258;&#28982;&#30340;&#32908;&#32905;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#26032;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#36328;&#22330;&#26223;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23384;&#22312;&#26497;&#22823;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#38750;&#24120;&#22823;&#19988;&#22797;&#26434;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25110;&#22522;&#20110;2SRNN&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#26469;&#36924;&#36817;&#30001;&#36825;&#20123;&#36328;&#22330;&#26223;&#25968;&#25454;&#21464;&#24322;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#38656;&#35201;&#22312;&#39044;&#35757;&#32451;&#21644;&#36866;&#24212;&#38454;&#27573;&#20013;&#22312;&#25968;&#30334;&#19975;&#20010;&#35757;&#32451;&#21442;&#25968;&#21644;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#36827;&#34892;&#39640;&#31471;&#36164;&#28304;&#32422;&#26463;&#21644;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;(TL)&#26469;&#22686;&#24378;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#20934;&#30830;&#32780;&#31616;&#27905;&#22320;&#34920;&#31034;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20197;&#21327;&#21516;&#22320;&#30830;&#23450;FJSP&#30340;&#20248;&#20808;&#32423;&#20998;&#37197;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.05119</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24377;&#24615;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning. (arXiv:2305.05119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#20934;&#30830;&#32780;&#31616;&#27905;&#22320;&#34920;&#31034;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20197;&#21327;&#21516;&#22320;&#30830;&#23450;FJSP&#30340;&#20248;&#20808;&#32423;&#20998;&#37197;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#21046;&#36896;&#20652;&#29983;&#20102;&#22797;&#26434;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#22914;&#24377;&#24615;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;FJSP&#65289;&#12290;&#22312;FJSP&#20013;&#65292;&#25805;&#20316;&#21487;&#20197;&#22312;&#22810;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#23384;&#22312;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26469;&#23398;&#20064;&#20248;&#20808;&#32423;&#20998;&#37197;&#35268;&#21017;&#65288;PDRs&#65289;&#20197;&#35299;&#20915;FJSP&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#35832;&#22914;OR-Tools&#31561;&#31934;&#30830;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#20173;&#26377;&#25552;&#39640;&#30340;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#33258;&#27880;&#24847;&#27169;&#22411;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#21644;DRL&#36827;&#34892;&#21487;&#25193;&#23637;&#20915;&#31574;&#21046;&#23450;&#30340;&#20248;&#28857;&#12290;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#34987;&#20934;&#30830;&#32780;&#31616;&#27905;&#22320;&#34920;&#31034;&#20986;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#22810;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#25805;&#20316;&#20449;&#24687;&#27880;&#24847;&#22359;&#21644;&#26426;&#22120;&#20449;&#24687;&#27880;&#24847;&#22359;&#32452;&#25104;&#30340;&#21452;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;DAN&#65289;&#12290;DAN&#21033;&#29992;&#36825;&#20123;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20197;&#21327;&#21516;&#22320;&#30830;&#23450;FJSP&#30340;PDRs&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this paper presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#23545;&#20581;&#24247;&#20445;&#38505;&#32034;&#36180;&#25968;&#25454;&#20013;&#21382;&#21490;&#20559;&#24046;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#36890;&#36807;&#26500;&#24314;&#31639;&#27861;&#27979;&#35797;&#26102;&#38388;&#20559;&#31227;&#65292;&#36827;&#34892;&#22238;&#39038;&#24615;&#25195;&#25551;&#20197;&#23547;&#25214;&#26102;&#38388;&#20559;&#31227;&#65292;&#24182;&#21019;&#24314;1010&#20010;&#20219;&#21153;&#26469;&#35780;&#20272;242&#39033;&#21307;&#30103;&#20445;&#20581;&#32467;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#26377;9.7%&#30340;&#20219;&#21153;&#26174;&#31034;&#20986;&#20154;&#32676;&#27700;&#24179;&#30340;&#26102;&#38388;&#20559;&#31227;&#65292;93%&#26174;&#31034;&#20986;&#24050;&#21457;&#29616;&#30340;&#23376;&#20154;&#32676;&#20869;&#30340;&#26102;&#38388;&#20559;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.05087</link><description>&lt;p&gt;
&#20581;&#24247;&#20445;&#38505;&#32034;&#36180;&#25968;&#25454;&#20013;&#21382;&#21490;&#20559;&#24046;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Study of Temporal Shift in Health Insurance Claims. (arXiv:2305.05087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#20581;&#24247;&#20445;&#38505;&#32034;&#36180;&#25968;&#25454;&#20013;&#21382;&#21490;&#20559;&#24046;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#36890;&#36807;&#26500;&#24314;&#31639;&#27861;&#27979;&#35797;&#26102;&#38388;&#20559;&#31227;&#65292;&#36827;&#34892;&#22238;&#39038;&#24615;&#25195;&#25551;&#20197;&#23547;&#25214;&#26102;&#38388;&#20559;&#31227;&#65292;&#24182;&#21019;&#24314;1010&#20010;&#20219;&#21153;&#26469;&#35780;&#20272;242&#39033;&#21307;&#30103;&#20445;&#20581;&#32467;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#26377;9.7%&#30340;&#20219;&#21153;&#26174;&#31034;&#20986;&#20154;&#32676;&#27700;&#24179;&#30340;&#26102;&#38388;&#20559;&#31227;&#65292;93%&#26174;&#31034;&#20986;&#24050;&#21457;&#29616;&#30340;&#23376;&#20154;&#32676;&#20869;&#30340;&#26102;&#38388;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#24320;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#37096;&#32626;&#65292;&#25968;&#25454;&#38598;&#38543;&#26102;&#38388;&#21457;&#29983;&#20559;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#29702;&#24819;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#29305;&#23450;&#26102;&#38388;&#28857;&#39044;&#27979;&#30340;&#20219;&#21153;&#65288;&#21363;&#35201;&#39044;&#27979;&#30340;&#32467;&#26524;&#65289;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#22914;&#26524;&#21382;&#21490;&#27169;&#22411;&#19981;&#20877;&#26159;&#39044;&#27979;&#35813;&#32467;&#26524;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#27979;&#35797;&#20154;&#32676;&#27700;&#24179;&#25110;&#24050;&#21457;&#29616;&#30340;&#23376;&#20154;&#32676;&#20869;&#30340;&#26102;&#38388;&#20559;&#31227;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20803;&#31639;&#27861;&#65292;&#22312;&#19968;&#32452;&#22823;&#37327;&#30340;&#20219;&#21153;&#20013;&#25191;&#34892;&#22238;&#39038;&#24615;&#25195;&#25551;&#20197;&#23547;&#25214;&#26102;&#38388;&#20559;&#31227;&#12290;&#26681;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#23545;&#21382;&#21490;&#20559;&#24046;&#30340;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;242&#39033;&#21307;&#30103;&#20445;&#20581;&#32467;&#26524;&#20174;2015&#24180;&#21040;2020&#24180;&#30340;&#20581;&#24247;&#20445;&#38505;&#32034;&#36180;&#25968;&#25454;&#38598;&#20013;&#30340;&#21382;&#21490;&#20559;&#31227;&#26469;&#21019;&#24314;1010&#20010;&#20219;&#21153;&#12290;9.7%&#30340;&#20219;&#21153;&#26174;&#31034;&#20986;&#20154;&#32676;&#27700;&#24179;&#30340;&#26102;&#38388;&#20559;&#31227;&#65292;93.0%&#26174;&#31034;&#20986;&#24050;&#21457;&#29616;&#30340;&#23376;&#20154;&#32676;&#20869;&#30340;&#26102;&#38388;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning models for predicting clinical outcomes are developed using historical data. Yet, even if these models are deployed in the near future, dataset shift over time may result in less than ideal performance. To capture this phenomenon, we consider a task--that is, an outcome to be predicted at a particular time point--to be non-stationary if a historical model is no longer optimal for predicting that outcome. We build an algorithm to test for temporal shift either at the population level or within a discovered sub-population. Then, we construct a meta-algorithm to perform a retrospective scan for temporal shift on a large collection of tasks. Our algorithms enable us to perform the first comprehensive evaluation of temporal shift in healthcare to our knowledge. We create 1,010 tasks by evaluating 242 healthcare outcomes for temporal shift from 2015 to 2020 on a health insurance claims dataset. 9.7% of the tasks show temporal shifts at the population level, and 93.0% ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#30456;&#20284;&#24615;&#65288;LocalSim&#65289;&#23398;&#20064;&#33410;&#28857;&#32423;&#21152;&#26435;&#34701;&#21512;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#25552;&#21462;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#22810;&#36339;&#20449;&#24687;&#65292;&#24182;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#22312;&#30495;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;LSGNN&#26041;&#27861;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#22343;&#33021;&#25552;&#20379;&#21487;&#27604;&#25110;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04225</link><description>&lt;p&gt;
LSGNN&#65306;&#36890;&#36807;&#23616;&#37096;&#30456;&#20284;&#24615;&#23454;&#29616;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#26222;&#36866;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LSGNN: Towards General Graph Neural Network in Node Classification by Local Similarity. (arXiv:2305.04225v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#30456;&#20284;&#24615;&#65288;LocalSim&#65289;&#23398;&#20064;&#33410;&#28857;&#32423;&#21152;&#26435;&#34701;&#21512;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#25552;&#21462;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#22810;&#36339;&#20449;&#24687;&#65292;&#24182;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#22312;&#30495;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;LSGNN&#26041;&#27861;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#22343;&#33021;&#25552;&#20379;&#21487;&#27604;&#25110;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#24615;&#34987;&#35748;&#20026;&#26159;&#20260;&#23475;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#29616;&#26377;&#30340;&#24037;&#20316;&#20351;&#29992;&#22810;&#36339;&#37051;&#23621;&#20449;&#24687;&#30340;&#22270;&#32423;&#21152;&#26435;&#34701;&#21512;&#26469;&#21253;&#21547;&#26356;&#22810;&#20855;&#26377;&#21516;&#36136;&#24615;&#30340;&#33410;&#28857;&#12290;&#28982;&#32780;&#65292;&#24322;&#36136;&#24615;&#21487;&#33021;&#22312;&#33410;&#28857;&#20043;&#38388;&#19981;&#21516;&#65292;&#38656;&#35201;&#32771;&#34385;&#23616;&#37096;&#25299;&#25169;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#30456;&#20284;&#24615;&#65288;LocalSim&#65289;&#23398;&#20064;&#33410;&#28857;&#32423;&#21152;&#26435;&#34701;&#21512;&#65292;&#24182;&#21487;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34701;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#39640;&#25928;&#30340;&#21021;&#22987;&#27531;&#24046;&#24046;&#36830;&#25509;&#65288;IRDC&#65289;&#26469;&#25552;&#21462;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#22810;&#36339;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21512;&#25104;&#22270;&#19978;LocalSim&#20195;&#34920;&#33410;&#28857;&#21516;&#36136;&#24615;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#22312;&#30495;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21363;&#23616;&#37096;&#30456;&#20284;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LSGNN&#65289;&#65292;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#22343;&#33021;&#25552;&#20379;&#21487;&#27604;&#25110;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterophily has been considered as an issue that hurts the performance of Graph Neural Networks (GNNs). To address this issue, some existing work uses a graph-level weighted fusion of the information of multi-hop neighbors to include more nodes with homophily. However, the heterophily might differ among nodes, which requires to consider the local topology. Motivated by it, we propose to use the local similarity (LocalSim) to learn node-level weighted fusion, which can also serve as a plug-and-play module. For better fusion, we propose a novel and efficient Initial Residual Difference Connection (IRDC) to extract more informative multi-hop information. Moreover, we provide theoretical analysis on the effectiveness of LocalSim representing node homophily on synthetic graphs. Extensive evaluations over real benchmark datasets show that our proposed method, namely Local Similarity Graph Neural Network (LSGNN), can offer comparable or superior state-of-the-art performance on both homophilic
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03942</link><description>&lt;p&gt;
&#23398;&#20064;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#28151;&#21512;&#28436;&#21592;-&#35780;&#35770;&#21592;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03942
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#28789;&#24039;&#24615;&#20013;&#65292;&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#26159;&#25805;&#20316;&#29289;&#20307;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38750;&#25235;&#21462;&#24335;&#25805;&#32437;&#21487;&#20197;&#20351;&#19982;&#29289;&#20307;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#65292;&#20294;&#20063;&#22312;&#25512;&#29702;&#20132;&#20114;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;HACMan&#30340;&#28151;&#21512;&#28436;&#21592;&#35780;&#35770;&#21592;&#22320;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;6D&#38750;&#25235;&#21462;&#24335;&#29289;&#20307;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;HACMan&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#25277;&#35937;&#21644;&#31354;&#38388;&#22522;&#30784;&#30340;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;RL&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#28151;&#21512;&#30340;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;HACMan&#36827;&#34892;&#20102;6D&#29289;&#20307;&#23039;&#24577;&#23545;&#40784;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#29256;&#26412;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#29289;&#20307;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;HACMan&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
&lt;/p&gt;</description></item><item><title>ChatGPT&#21644;Bard&#31561;AI&#24037;&#20855;&#38656;&#35201;&#25345;&#32493;&#22823;&#37327;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20294;&#29616;&#34892;&#30340;&#29256;&#26435;&#27861;&#21017;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#19982;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#23558;&#26377;&#21161;&#20110;&#23558;AI&#24037;&#20855;&#19982;&#22823;&#22810;&#25968;&#29256;&#26435;&#25968;&#25454;&#25317;&#26377;&#32773;&#20043;&#38388;&#30340;&#25932;&#23545;&#20851;&#31995;&#36716;&#21464;&#20026;&#21512;&#20316;&#20851;&#31995;&#65292;&#20351;AI&#29983;&#24577;&#31995;&#32479;&#26356;&#20581;&#24247;&#12290;</title><link>http://arxiv.org/abs/2305.02555</link><description>&lt;p&gt;
ChatGPT&#21644;Bard&#26159;&#21542;&#24212;&#35813;&#19982;&#20854;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#65311;AI&#26102;&#20195;&#30340;&#26032;&#21830;&#19994;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era. (arXiv:2305.02555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02555
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;Bard&#31561;AI&#24037;&#20855;&#38656;&#35201;&#25345;&#32493;&#22823;&#37327;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20294;&#29616;&#34892;&#30340;&#29256;&#26435;&#27861;&#21017;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#19982;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#23558;&#26377;&#21161;&#20110;&#23558;AI&#24037;&#20855;&#19982;&#22823;&#22810;&#25968;&#29256;&#26435;&#25968;&#25454;&#25317;&#26377;&#32773;&#20043;&#38388;&#30340;&#25932;&#23545;&#20851;&#31995;&#36716;&#21464;&#20026;&#21512;&#20316;&#20851;&#31995;&#65292;&#20351;AI&#29983;&#24577;&#31995;&#32479;&#26356;&#20581;&#24247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25105;&#20204;&#27491;&#36827;&#20837;&#30495;&#27491;&#30340;AI&#26102;&#20195;&#12290;&#25105;&#20204;&#21487;&#20197;&#39044;&#35265;&#65292;&#21331;&#36234;&#30340;AI&#24037;&#20855;&#24456;&#24555;&#23558;&#33719;&#24471;&#21487;&#35266;&#30340;&#21033;&#28070;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#38500;&#20102;&#20256;&#32479;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#32929;&#19996;&#65292;AI&#24037;&#20855;&#26159;&#21542;&#24212;&#35813;&#19982;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#65311;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#12290;&#22823;&#22411;AI&#24037;&#20855;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22987;&#32456;&#38656;&#35201;&#26356;&#22810;&#12289;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#19981;&#26029;&#25913;&#36827;&#65292;&#20294;&#24403;&#21069;&#30340;&#29256;&#26435;&#27861;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#22312;AI&#24037;&#20855;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#20998;&#20139;&#25910;&#30410;&#21487;&#20197;&#23558;&#24403;&#21069;&#25932;&#23545;&#30340;&#38646;&#21644;&#28216;&#25103;&#20851;&#31995;&#36716;&#21464;&#20026;&#19968;&#31181;&#21512;&#20316;&#21644;&#20114;&#21033;&#30340;&#20851;&#31995;&#65292;&#32780;&#36825;&#31181;&#20851;&#31995;&#23545;&#20110;&#20419;&#36827;AI&#24037;&#20855;&#12289;&#29992;&#25143;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#33391;&#24615;&#24490;&#29615;&#21457;&#23637;&#12289;&#25512;&#21160;AI&#25216;&#26415;&#24182;&#24314;&#31435;&#20581;&#24247;&#30340;AI&#29983;&#24577;&#31995;&#32479;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25910;&#30410;&#20998;&#20139;&#21830;&#19994;&#27169;&#24335;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
With various AI tools such as ChatGPT becoming increasingly popular, we are entering a true AI era. We can foresee that exceptional AI tools will soon reap considerable profits. A crucial question arise: should AI tools share revenue with their training data providers in additional to traditional stakeholders and shareholders? The answer is Yes. Large AI tools, such as large language models, always require more and better quality data to continuously improve, but current copyright laws limit their access to various types of data. Sharing revenue between AI tools and their data providers could transform the current hostile zero-sum game relationship between AI tools and a majority of copyrighted data owners into a collaborative and mutually beneficial one, which is necessary to facilitate the development of a virtuous cycle among AI tools, their users and data providers that drives forward AI technology and builds a healthy AI ecosystem. However, current revenue-sharing business models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#37319;&#26679;&#30340;&#27969;&#24335;PCA&#31639;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#35813;&#31639;&#27861;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#31532;&#19968;&#20010;&#23574;&#38160;&#29575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#26041;&#26696;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.02456</link><description>&lt;p&gt;
&#38754;&#21521;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#30340;&#27969;&#24335;PCA&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Streaming PCA for Markovian Data. (arXiv:2305.02456v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#37319;&#26679;&#30340;&#27969;&#24335;PCA&#31639;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#35813;&#31639;&#27861;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#31532;&#19968;&#20010;&#23574;&#38160;&#29575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#26041;&#26696;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Oja&#22312;1982&#24180;&#30340;&#32463;&#20856;&#35770;&#25991;&#20013;&#39318;&#27425;&#25552;&#20986;&#20197;&#26469;&#65292;Oja&#31639;&#27861;&#24050;&#25104;&#20026;&#27969;&#24335;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24335;PCA&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#20174;&#19968;&#20010;&#19981;&#21487;&#32422;&#12289;&#26080;&#21608;&#26399;&#12289;&#21487;&#36870;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#24179;&#31283;&#20998;&#24067;&#30340;&#26410;&#30693;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#21069;&#19968;&#20010;&#29305;&#24449;&#21521;&#37327;&#12290;&#36825;&#31181;&#24773;&#20917;&#36866;&#29992;&#20110;&#21482;&#33021;&#20174;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;(MCMC)&#31867;&#22411;&#30340;&#31639;&#27861;&#20013;&#37319;&#26679;&#25968;&#25454;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#23545;&#35813;&#38142;&#30340;&#24179;&#31283;&#20998;&#24067;&#30340;&#21442;&#25968;&#36827;&#34892;&#25512;&#26029;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;Oja&#31639;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#37117;&#20551;&#23450;&#25968;&#25454;&#28857;&#26159;IID&#37319;&#26679;&#30340;&#12290;&#23545;&#20110;&#20855;&#26377;&#39532;&#23572;&#21487;&#22827;&#20381;&#36182;&#20851;&#31995;&#30340;&#25968;&#25454;&#27969;&#65292;&#20154;&#20204;&#36890;&#24120;&#23545;&#25968;&#25454;&#36827;&#34892;&#19979;&#37319;&#26679;&#20197;&#33719;&#24471;"&#20960;&#20046;"&#29420;&#31435;&#30340;&#25968;&#25454;&#27969;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;Oja&#31639;&#27861;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#31532;&#19968;&#20010;&#23574;&#38160;&#29575;&#65292;&#20854;&#20013;&#21435;&#25481;&#20102;$n$&#30340;&#23545;&#25968;&#20381;&#36182;&#24615;&#65292;&#32467;&#26524;&#26159;$\mathcal{O}(n^{-1})$&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#26696;&#26469;&#35843;&#25972;&#31639;&#27861;&#30340;&#27493;&#38271;&#65292;&#23427;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#20013;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its inception in Erikki Oja's seminal paper in 1982, Oja's algorithm has become an established method for streaming principle component analysis (PCA). We study the problem of streaming PCA, where the data-points are sampled from an irreducible, aperiodic, and reversible Markov chain. Our goal is to estimate the top eigenvector of the unknown covariance matrix of the stationary distribution. This setting has implications in situations where data can only be sampled from a Markov Chain Monte Carlo (MCMC) type algorithm, and the goal is to do inference for parameters of the stationary distribution of this chain. Most convergence guarantees for Oja's algorithm in the literature assume that the data-points are sampled IID. For data streams with Markovian dependence, one typically downsamples the data to get a "nearly" independent data stream. In this paper, we obtain the first sharp rate for Oja's algorithm on the entire data, where we remove the logarithmic dependence on $n$ resulti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#27969;&#39640;&#25928;&#23398;&#20064;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#26088;&#22312;&#35299;&#20915;&#20174;&#25968;&#25454;&#27969;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20854;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#21450;&#26102;&#26377;&#25928;&#22320;&#34987;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2305.02217</link><description>&lt;p&gt;
&#27969;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stream Efficient Learning. (arXiv:2305.02217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#27969;&#39640;&#25928;&#23398;&#20064;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#26088;&#22312;&#35299;&#20915;&#20174;&#25968;&#25454;&#27969;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20854;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#21450;&#26102;&#26377;&#25928;&#22320;&#34987;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#25968;&#25454;&#24448;&#24448;&#38543;&#30528;&#26102;&#38388;&#30340;&#31215;&#32047;&#20197;&#27969;&#30340;&#24418;&#24335;&#36827;&#34892;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20851;&#27880;&#20110;&#20174;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19981;&#21516;&#65292;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#19981;&#33021;&#24573;&#35270;&#27969;&#20837;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#26159;&#26080;&#20241;&#27490;&#30340;&#12289;&#35268;&#27169;&#24040;&#22823;&#12289;&#21464;&#21270;&#26410;&#30693;&#65292;&#24182;&#19988;&#20551;&#35774;&#26377;&#36275;&#22815;&#30340;&#35745;&#31639;/&#23384;&#20648;&#36164;&#28304;&#21487;&#20197;&#21450;&#26102;&#22788;&#29702;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#25968;&#25454;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#34987;&#21450;&#26102;&#22320;&#26377;&#25928;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#65292;&#20877;&#21152;&#19978;&#23398;&#20064;&#31639;&#27861;&#30340;&#33021;&#21147;&#21644;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21534;&#21520;&#37327;&#30340;&#27010;&#24565;&#65292;&#23450;&#20041;&#20102;&#27969;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in many real-world applications are often accumulated over time, like a stream. In contrast to conventional machine learning studies that focus on learning from a given training data set, learning from data streams cannot ignore the fact that the incoming data stream can be potentially endless with overwhelming size and unknown changes, and it is impractical to assume to have sufficient computational/storage resource such that all received data can be handled in time. Thus, the generalization performance of learning from data streams depends not only on how many data have been received, but also on how many data can be well exploited timely, with resource and rapidity concerns, in addition to the ability of learning algorithm and complexity of the problem. For this purpose, in this article we introduce the notion of machine learning throughput, define Stream Efficient Learning and present a preliminary theoretical framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;(PHNN)&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#65292;PHNN&#34920;&#29616;&#26356;&#20026;&#20248;&#36234;&#65292;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#21435;&#38500;&#25110;&#25913;&#21464;&#22806;&#21147;&#24773;&#20917;&#24182;&#21487;&#20998;&#21035;&#24471;&#21040;&#19977;&#20010;&#19981;&#21516;&#29289;&#29702;&#35299;&#37322;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2304.14374</link><description>&lt;p&gt;
&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Hamiltonian neural networks for learning partial differential equations. (arXiv:2304.14374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;(PHNN)&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#65292;PHNN&#34920;&#29616;&#26356;&#20026;&#20248;&#36234;&#65292;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#21435;&#38500;&#25110;&#25913;&#21464;&#22806;&#21147;&#24773;&#20917;&#24182;&#21487;&#20998;&#21035;&#24471;&#21040;&#19977;&#20010;&#19981;&#21516;&#29289;&#29702;&#35299;&#37322;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;(PHNN)&#26469;&#23398;&#20064;&#21487;&#20197;&#29992;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;&#26412;&#25991;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#25152;&#24471;&#27169;&#22411;&#30001;&#39640;&#36798;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#27169;&#25311;&#20195;&#34920;&#23432;&#24658;&#12289;&#32791;&#25955;&#21644;&#22806;&#21147;&#30340;&#39033;&#20197;&#21450;&#21487;&#20197;&#23398;&#20064;&#25110;&#20026;&#20808;&#21069;&#30693;&#35782;&#30340;&#31163;&#25955;&#21367;&#31215;&#31639;&#23376;&#26500;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;PHNN&#30456;&#27604;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#22522;&#32447;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PHNN&#27169;&#22411;&#30001;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#35299;&#37322;&#30340;&#37096;&#20998;&#32452;&#25104;&#65292;&#21487;&#20197;&#20998;&#21035;&#30740;&#31350;&#36825;&#20123;&#37096;&#20998;&#20197;&#33719;&#24471;&#23545;&#31995;&#32479;&#30340;&#27934;&#23519;&#65292;&#24182;&#19988;&#21363;&#20351;&#21435;&#38500;&#25110;&#25913;&#21464;&#22806;&#21147;&#65292;&#25152;&#23398;&#24471;&#30340;&#27169;&#22411;&#20173;&#28982;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Hamiltonian neural networks (PHNN) were recently introduced for learning dynamical systems that can be modelled by ordinary differential equations. In this paper, we extend the method to partial differential equations. The resulting model is comprised of up to three neural networks, modelling terms representing conservation, dissipation and external forces, and discrete convolution operators that can either be learned or be prior knowledge. We demonstrate numerically the superior performance of PHNN compared to a baseline model that models the full dynamics by a single neural network. Moreover, since the PHNN model consists of three parts with different physical interpretations, these can be studied separately to gain insight into the system, and the learned model is applicable also if external forces are removed or changed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#21160;&#24577;&#31995;&#32479;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#12289;&#22312;&#32447;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#36801;&#31227;&#23398;&#20064;&#21644;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.12583</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#21160;&#24577;&#31995;&#32479;&#30340;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#65306;&#26041;&#27861;&#21644;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Real-time Safety Assessment of Dynamic Systems in Non-stationary Environments: A Review of Methods and Techniques. (arXiv:2304.12583v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#21160;&#24577;&#31995;&#32479;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#12289;&#22312;&#32447;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#36801;&#31227;&#23398;&#20064;&#21644;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#30340;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#23545;&#20110;&#24037;&#19994;&#21644;&#20132;&#36890;&#24212;&#29992;&#31561;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#26041;&#27861;&#30340;&#20840;&#38754;&#32508;&#36848;&#38459;&#30861;&#20102;&#30456;&#20851;&#26041;&#27861;&#30340;&#36827;&#23637;&#21644;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#38024;&#23545;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#20219;&#21153;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#31361;&#20986;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#26041;&#27861;&#30340;&#32972;&#26223;&#21644;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38382;&#39064;&#25551;&#36848;&#65292;&#21253;&#25324;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#30456;&#20851;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#22914;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#12289;&#22312;&#32447;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#36801;&#31227;&#23398;&#20064;&#21644;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#23637;&#26395;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#26412;&#25991;&#30340;&#32508;&#36848;&#26088;&#22312;&#20026;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#25552;&#20379;&#21442;&#32771;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time safety assessment (RTSA) of dynamic systems is a critical task that has significant implications for various fields such as industrial and transportation applications, especially in non-stationary environments. However, the absence of a comprehensive review of real-time safety assessment methods in non-stationary environments impedes the progress and refinement of related methods. In this paper, a review of methods and techniques for RTSA tasks in non-stationary environments is provided. Specifically, the background and significance of RTSA approaches in non-stationary environments are firstly highlighted. We then present a problem description that covers the definition, classification, and main challenges. We review recent developments in related technologies such as online active learning, online semi-supervised learning, online transfer learning, and online anomaly detection. Finally, we discuss future outlooks and potential directions for further research. Our review aims
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;k-NN&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.09058</link><description>&lt;p&gt;
&#37325;&#35775;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;k-NN
&lt;/p&gt;
&lt;p&gt;
Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;k-NN&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20316;&#20026;&#21442;&#25968;&#21270;&#30340;&#24613;&#20999;&#23398;&#20064;&#22120;&#65292;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24403;&#21069;&#33539;&#24335;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;&#19982;&#27492;&#24418;&#25104;&#23545;&#27604;&#30340;&#26159;&#65292;k-&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20998;&#31867;&#22120;&#20316;&#20026;&#24310;&#36831;&#23398;&#20064;&#27169;&#22411;&#65292;&#20542;&#21521;&#20110;&#20943;&#36731;&#36807;&#25311;&#21512;&#21644;&#23396;&#31435;&#22122;&#22768;&#12290;&#26412;&#25991;&#20013;&#25105;&#20204;&#37325;&#35775;&#20102;k-NN&#20998;&#31867;&#22120;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;PLMs&#30340;&#20998;&#31867;&#22120;&#12290;&#20174;&#26041;&#27861;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#25991;&#26412;&#34920;&#31034;&#30340;PLMs&#22312;&#20004;&#20010;&#27493;&#39588;&#20013;&#37319;&#29992;k-NN&#65306;&#65288;1&#65289;&#21033;&#29992;k-NN&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#26469;&#26657;&#20934;&#35757;&#32451;&#36807;&#31243;&#65288;2&#65289;&#32447;&#24615;&#25554;&#20540;k-NN&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21644;PLMs&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#23454;&#29616;&#20102;k-NN&#26657;&#20934;&#35757;&#32451;&#65292;&#23558;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#26131;&#20110;&#21644;&#38590;&#20197;&#23398;&#20064;&#30340;&#31034;&#20363;&#30340;&#25351;&#26631;&#12290;&#20174;&#24212;&#29992;&#22330;&#26223;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#24494;&#35843;&#12289;&#25552;&#31034;&#24494;&#35843;&#33539;&#24335;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#35774;&#32622;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;k-NN&#21487;&#20197;&#22312;&#25152;&#26377;&#21463;&#21040;&#26816;&#26597;&#30340;&#35774;&#32622;&#20013;&#25345;&#32493;&#25552;&#39640;PLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21463;&#21040;&#32771;&#34385;&#30340;&#35774;&#32622;&#20013;&#36305;&#36194;&#20102;&#22522;&#20110;&#26222;&#36890;PLMs&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs' classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20132;&#21449;&#29109;&#12289;&#24191;&#20041;&#20132;&#21449;&#29109;&#12289;&#22343;&#26041;&#35823;&#24046;&#31561;&#19968;&#22823;&#31867;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#20248;&#21183;&#30340;&#21452;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#25110;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.07288</link><description>&lt;p&gt;
&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65306;&#29702;&#35770;&#20998;&#26512;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Entropy Loss Functions: Theoretical Analysis and Applications. (arXiv:2304.07288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20132;&#21449;&#29109;&#12289;&#24191;&#20041;&#20132;&#21449;&#29109;&#12289;&#22343;&#26041;&#35823;&#24046;&#31561;&#19968;&#22823;&#31867;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#20248;&#21183;&#30340;&#21452;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#25110;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#29109;&#26159;&#24191;&#27867;&#24212;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#24403;&#20351;&#29992;softmax&#20989;&#25968;&#26102;&#65292;&#23427;&#19982;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#24212;&#29992;&#20110;&#36923;&#36753;&#22238;&#24402;&#25439;&#22833;&#20989;&#25968;&#30456;&#31526;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#20132;&#21449;&#29109;&#20316;&#20026;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#33021;&#20381;&#38752;&#20160;&#20040;&#20445;&#35777;&#21602;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#21253;&#25324;&#20132;&#21449;&#29109;&#65288;&#25110;&#36923;&#36753;&#25439;&#22833;&#65289;&#12289;&#24191;&#20041;&#20132;&#21449;&#29109;&#12289;&#22343;&#26041;&#35823;&#24046;&#21644;&#20854;&#20182;&#20132;&#21449;&#29109;&#31867;&#20989;&#25968;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#30340;&#31532;&#19968;&#20010;$H$-&#36830;&#32493;&#24615;&#30028;&#38480;&#12290;&#36825;&#20123;&#37117;&#26159;&#38750;&#28176;&#36827;&#20445;&#35777;&#65292;&#20197;&#20272;&#35745;&#20195;&#29702;&#25439;&#22833;&#30340;&#20272;&#35745;&#35823;&#24046;&#20026;&#19978;&#38480;&#65292;&#29992;&#20110;&#29305;&#23450;&#30340;&#20551;&#35774;&#38598;$H$&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#20123;&#36793;&#30028;&#30340;&#32039;&#23494;&#31243;&#24230;&#12290;&#36825;&#20123;&#36793;&#30028;&#21462;&#20915;&#20110;&#31216;&#20026;&#21487;&#26368;&#23567;&#21270;&#38388;&#38553;&#30340;&#37327;&#65292;&#36825;&#20123;&#38388;&#38553;&#21482;&#21462;&#20915;&#20110;&#25439;&#22833;&#20989;&#25968;&#21644;&#20551;&#35774;&#38598;&#12290;&#20026;&#20102;&#20351;&#23427;&#20204;&#26356;&#20855;&#20307;&#21270;&#65292;&#25105;&#20204;&#23545;&#22797;&#26434;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#36825;&#20123;&#38388;&#38553;&#36827;&#34892;&#20102;&#20855;&#20307;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#21452;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#23427;&#22522;&#20110;&#20004;&#20010;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#20248;&#20110;&#26631;&#20934;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#25110;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of losses, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other loss cross-entropy-like functions. We give the first $H$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set $H$ used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps, which only depend on the loss function and the hypothesis set. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#65292;0&#22522;&#25968;&#24809;&#32602;&#30340;&#22270;&#36235;&#21183;&#36807;&#28388;&#65288;GTF&#65289;&#27169;&#22411;&#65292;&#21487;&#21516;&#26102;&#36827;&#34892;k-means&#32858;&#31867;&#21644;&#22522;&#20110;&#22270;&#30340;&#26368;&#23567;&#21106;&#65292;&#20197;&#20272;&#35745;&#22312;&#33410;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#22343;&#21248;&#24179;&#28369;&#27700;&#24179;&#30340;&#20998;&#27573;&#24179;&#28369;&#22270;&#20449;&#21495;&#65292;&#24182;&#22312;&#38477;&#22122;&#12289;&#25903;&#25345;&#24674;&#22797;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.05223</link><description>&lt;p&gt;
&#22522;&#20110;L2&#65292;0&#22522;&#25968;&#24809;&#32602;&#30340;&#19981;&#22343;&#21248;&#22270;&#36235;&#21183;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inhomogeneous graph trend filtering via a l2,0 cardinality penalty. (arXiv:2304.05223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#65292;0&#22522;&#25968;&#24809;&#32602;&#30340;&#22270;&#36235;&#21183;&#36807;&#28388;&#65288;GTF&#65289;&#27169;&#22411;&#65292;&#21487;&#21516;&#26102;&#36827;&#34892;k-means&#32858;&#31867;&#21644;&#22522;&#20110;&#22270;&#30340;&#26368;&#23567;&#21106;&#65292;&#20197;&#20272;&#35745;&#22312;&#33410;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#22343;&#21248;&#24179;&#28369;&#27700;&#24179;&#30340;&#20998;&#27573;&#24179;&#28369;&#22270;&#20449;&#21495;&#65292;&#24182;&#22312;&#38477;&#22122;&#12289;&#25903;&#25345;&#24674;&#22797;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#20272;&#35745;&#20998;&#27573;&#24179;&#28369;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;$\ell_{2,0}$-&#33539;&#25968;&#24809;&#32602;&#22270;&#36235;&#21183;&#36807;&#28388;&#65288;GTF&#65289;&#27169;&#22411;&#65292;&#20197;&#20272;&#35745;&#22312;&#33410;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#22343;&#21248;&#24179;&#28369;&#27700;&#24179;&#30340;&#20998;&#27573;&#24179;&#28369;&#22270;&#20449;&#21495;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;GTF&#27169;&#22411;&#21516;&#26102;&#26159;&#22522;&#20110;&#33410;&#28857;&#19978;&#30340;&#20449;&#21495;&#30340;k-means&#32858;&#31867;&#21644;&#22522;&#20110;&#22270;&#30340;&#26368;&#23567;&#21106;&#65292;&#20854;&#20013;&#32858;&#31867;&#21644;&#21106;&#20849;&#20139;&#30456;&#21516;&#30340;&#20998;&#37197;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#25152;&#25552;&#20986;&#30340;GTF&#27169;&#22411;&#65306;&#19968;&#31181;&#26159;&#22522;&#20110;&#35889;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#27169;&#25311;&#36864;&#28779;&#30340;&#26041;&#27861;&#12290;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;GTF&#27169;&#22411;&#22312;&#38477;&#22122;&#12289;&#25903;&#25345;&#24674;&#22797;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study estimation of piecewise smooth signals over a graph. We propose a $\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimate piecewise smooth graph signals that exhibits inhomogeneous levels of smoothness across the nodes. We prove that the proposed GTF model is simultaneously a k-means clustering on the signal over the nodes and a minimum graph cut on the edges of the graph, where the clustering and the cut share the same assignment matrix. We propose two methods to solve the proposed GTF model: a spectral decomposition method and a method based on simulated annealing. In the experiment on synthetic and real-world datasets, we show that the proposed GTF model has a better performances compared with existing approaches on the tasks of denoising, support recovery and semi-supervised classification. We also show that the proposed GTF model can be solved more efficiently than existing models for the dataset with a large edge set.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#22823;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#21028;&#23450;&#26159;&#21542;&#23384;&#22312;&#26159;&#19968;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;&#35745;&#31639;&#26368;&#22823;&#22240;&#23376;&#20998;&#35299;&#30340;&#31639;&#27861;Ord2Factor&#12290;</title><link>http://arxiv.org/abs/2304.03338</link><description>&lt;p&gt;
&#26368;&#22823;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Maximal Ordinal Two-Factorizations. (arXiv:2304.03338v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#22823;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#21028;&#23450;&#26159;&#21542;&#23384;&#22312;&#26159;&#19968;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;&#35745;&#31639;&#26368;&#22823;&#22240;&#23376;&#20998;&#35299;&#30340;&#31639;&#27861;Ord2Factor&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#24418;&#24335;&#32972;&#26223;&#20013;&#65292;&#24207;&#25968;&#22240;&#23376;&#26159;&#20854;&#20851;&#31995;&#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#27010;&#24565;&#26684;&#20013;&#30340;&#38142;&#65292;&#21363;&#23545;&#24212;&#20110;&#32447;&#24615;&#39034;&#24207;&#30340;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#20026;&#20102;&#21487;&#35270;&#21270;&#24418;&#24335;&#19978;&#19979;&#25991;&#20013;&#30340;&#25968;&#25454;&#65292;Ganter&#21644;Glodeanu&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#20010;&#24207;&#25968;&#22240;&#23376;&#30340;&#21452;&#22270;&#12290;&#20026;&#20102;&#20351;&#21452;&#22270;&#26377;&#29992;&#65292;&#37325;&#35201;&#30340;&#26159;&#36825;&#20123;&#22240;&#23376;&#23613;&#21487;&#33021;&#21253;&#21547;&#26356;&#22810;&#25968;&#25454;&#28857;&#65292;&#21363;&#35206;&#30422;&#23613;&#21487;&#33021;&#22810;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#30740;&#31350;&#36825;&#26679;&#30340;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30465;&#30053;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#30340;&#24418;&#24335;&#32972;&#26223;&#20013;&#20004;&#20010;&#22240;&#23376;&#30340;&#19981;&#30456;&#20132;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21028;&#23450;&#32473;&#23450;&#22823;&#23567;&#30340;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#26159;&#21542;&#23384;&#22312;&#26159;&#19968;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#35745;&#31639;&#26368;&#22823;&#22240;&#23376;&#20998;&#35299;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31639;&#27861;Ord2Factor&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#35745;&#31639;&#22823;&#30340;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a formal context, an ordinal factor is a subset of its incidence relation that forms a chain in the concept lattice, i.e., a part of the dataset that corresponds to a linear order. To visualize the data in a formal context, Ganter and Glodeanu proposed a biplot based on two ordinal factors. For the biplot to be useful, it is important that these factors comprise as much data points as possible, i.e., that they cover a large part of the incidence relation. In this work, we investigate such ordinal two-factorizations. First, we investigate for formal contexts that omit ordinal two-factorizations the disjointness of the two factors. Then, we show that deciding on the existence of two-factorizations of a given size is an NP-complete problem which makes computing maximal factorizations computationally expensive. Finally, we provide the algorithm Ord2Factor that allows us to compute large ordinal two-factorizations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D-CNN&#65289;&#23545;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#39118;&#36895;&#39044;&#27979;&#30340;&#31934;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21608;&#22260;&#21306;&#22495;&#30340;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;3D-CNN&#35757;&#32451;&#21487;&#20197;&#27604;&#20165;&#20351;&#29992;&#21333;&#28857;&#20449;&#24687;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01545</link><description>&lt;p&gt;
&#21306;&#22495;&#39118;&#21147;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;CNN&#30340;&#39118;&#36895;&#39044;&#27979;&#65306;&#26469;&#33258;&#26102;&#31354;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis. (arXiv:2304.01545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D-CNN&#65289;&#23545;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#39118;&#36895;&#39044;&#27979;&#30340;&#31934;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21608;&#22260;&#21306;&#22495;&#30340;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;3D-CNN&#35757;&#32451;&#21487;&#20197;&#27604;&#20165;&#20351;&#29992;&#21333;&#28857;&#20449;&#24687;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#31354;&#25968;&#25454;&#32500;&#24230;&#23545;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#30340;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21152;&#20837;&#31354;&#38388;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#19981;&#21516;&#31354;&#38388;&#23610;&#24230;&#25913;&#36827;&#30340;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#20339;&#26102;&#38388;&#38271;&#24230;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#30740;&#31350;&#20063;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#22312;&#20351;&#29992;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D-CNN&#65289;&#39044;&#27979;&#39118;&#36895;&#26102;&#65292;&#37319;&#29992;&#20855;&#26377;&#19981;&#21516;&#26102;&#31354;&#32500;&#24230;&#30340;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#35780;&#20272;&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21608;&#22260;&#21306;&#22495;&#30340;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;3D-CNN&#35757;&#32451;&#21487;&#20197;&#27604;&#20165;&#20351;&#29992;&#21333;&#28857;&#20449;&#24687;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22810;&#26102;&#38388;&#25968;&#25454;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the impact of spatiotemporal data dimensions on the precision of a wind forecasting model developed using an artificial neural network. Although previous studies have shown that incorporating spatial data can enhance the accuracy of wind forecasting models, few investigations have explored the extent of the improvement owing to different spatial scales in neural network-based predictive models. Additionally, there are limited studies on the optimal temporal length of the input data for these models. To address this gap, this study employs data with various spatiotemporal dimensions as inputs when forecasting wind using 3D-Convolutional Neural Networks (3D-CNN) and assesses their predictive performance. The results indicate that using spatial data of the surrounding area for 3D-CNN training can achieve better predictive performance than using only single-point information. Additionally, multi-time data had a more positive effect on the predictive performance than
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21733;&#24503;&#23572;&#19981;&#23436;&#22791;&#23450;&#29702;&#21040;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#23436;&#22791;&#24615;&#30340;&#36923;&#36753;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20219;&#20309;&#20449;&#20208;&#31995;&#32479;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#19988;&#19981;&#23436;&#22791;&#23450;&#29702;&#24847;&#21619;&#30528;&#23384;&#22312;&#30495;&#23454;&#20294;&#26080;&#27861;&#35777;&#26126;&#30340;&#38472;&#36848;&#65292;&#21487;&#20197;&#29992;&#26469;&#23450;&#20041;&#20986;&#19982;&#29616;&#26377;&#20449;&#20208;&#21644;&#20256;&#32479;&#19968;&#33268;&#30340;&#26032;&#23447;&#25945;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2303.14338</link><description>&lt;p&gt;
&#20174;&#21733;&#24503;&#23572;&#19981;&#23436;&#22791;&#23450;&#29702;&#21040;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#23436;&#22791;&#24615;&#65288;&#25193;&#23637;&#25688;&#35201;&#65289;
&lt;/p&gt;
&lt;p&gt;
From G\"odel's Incompleteness Theorem to the completeness of bot religions (Extended abstract). (arXiv:2303.14338v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21733;&#24503;&#23572;&#19981;&#23436;&#22791;&#23450;&#29702;&#21040;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#23436;&#22791;&#24615;&#30340;&#36923;&#36753;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20219;&#20309;&#20449;&#20208;&#31995;&#32479;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#19988;&#19981;&#23436;&#22791;&#23450;&#29702;&#24847;&#21619;&#30528;&#23384;&#22312;&#30495;&#23454;&#20294;&#26080;&#27861;&#35777;&#26126;&#30340;&#38472;&#36848;&#65292;&#21487;&#20197;&#29992;&#26469;&#23450;&#20041;&#20986;&#19982;&#29616;&#26377;&#20449;&#20208;&#21644;&#20256;&#32479;&#19968;&#33268;&#30340;&#26032;&#23447;&#25945;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hilbert &#21644; Ackermann &#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#23436;&#22791;&#29702;&#35770;&#19968;&#33268;&#22320;&#25193;&#23637;&#21040;&#23436;&#22791;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#21733;&#24503;&#23572;&#22522;&#26412;&#19978;&#35777;&#26126;&#20102;&#20219;&#20309;&#33021;&#22815;&#23545;&#20854;&#33258;&#36523;&#38472;&#36848;&#21450;&#20854;&#35777;&#26126;&#36827;&#34892;&#32534;&#30721;&#30340;&#29702;&#35770;&#37117;&#21253;&#21547;&#20102;&#30495;&#23454;&#20294;&#19981;&#33021;&#34987;&#35777;&#26126;&#30340;&#38472;&#36848;&#12290;&#21733;&#24503;&#23572;&#30340;&#26500;&#36896;&#24182;&#27809;&#26377;&#22238;&#31572;&#24076;&#23572;&#20271;&#29305;&#30340;&#38382;&#39064;&#65292;&#24076;&#23572;&#20271;&#29305;&#35748;&#20026;&#29702;&#35770;&#21487;&#20197;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#20844;&#29702;&#26469;&#35777;&#26126;&#36234;&#26469;&#36234;&#22810;&#30340;&#30495;&#23454;&#38472;&#36848;&#65292;&#23601;&#20687;&#31185;&#23398;&#19968;&#26679;&#65292;&#23436;&#22791;&#24615;&#26159;&#28040;&#22833;&#28857;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24213;&#23618;&#30340;&#36923;&#36753;&#36807;&#31243;&#65292;&#24182;&#25551;&#36848;&#20102;&#23548;&#33268;&#21487;&#27979;&#35797;&#20294;&#19981;&#21487;&#34892;&#30340;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#36712;&#36857;&#65292;&#36825;&#20123;&#23447;&#25945;&#25193;&#23637;&#20102;&#20256;&#32479;&#23447;&#25945;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20202;&#24335;&#21644;&#20449;&#20208;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#20219;&#20309;&#20449;&#20208;&#31995;&#32479;&#37117;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#36923;&#36753;&#29702;&#35770;&#30340;&#24819;&#27861;&#65292;&#24182;&#19988;&#19981;&#23436;&#22791;&#23450;&#29702;&#24847;&#21619;&#30528;&#23384;&#22312;&#30495;&#23454;&#20294;&#26080;&#27861;&#35777;&#26126;&#30340;&#38472;&#36848;&#65292;&#21487;&#20197;&#24182;&#20837;&#36825;&#20010;&#29702;&#35770;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#20363;&#23376;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#20204;&#26469;&#23450;&#20041;&#19982;&#29616;&#26377;&#20449;&#20208;&#21644;&#20256;&#32479;&#19968;&#33268;&#30340;&#26032;&#23447;&#25945;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hilbert and Ackermann asked for a method to consistently extend incomplete theories to complete theories. G\"odel essentially proved that any theory capable of encoding its own statements and their proofs contains statements that are true but not provable. Hilbert did not accept that G\"odel's construction answered his question, and in his late writings and lectures, G\"odel agreed that it did not, since theories can be completed incrementally, by adding axioms to prove ever more true statements, as science normally does, with completeness as the vanishing point. This pragmatic view of validity is familiar not only to scientists who conjecture test hypotheses but also to real estate agents and other dealers, who conjure claims, albeit invalid, as necessary to close a deal, confident that they will be able to conjure other claims, albeit invalid, sufficient to make the first claims valid. We study the underlying logical process and describe the trajectories leading to testable but unfal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65288;UAP&#65289;&#25915;&#20987;&#26041;&#27861;&#65292;&#21482;&#38656;&#38468;&#21152;&#19968;&#20010;&#30456;&#23545;&#36739;&#23567;&#30340;&#23383;&#33410;&#32423;&#34917;&#19969;&#21363;&#21487;&#32469;&#36807;MalConv&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#65292;&#21487;&#20197;&#23558;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#30340;&#26816;&#27979;&#29575;&#38477;&#20302;80&#65285;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#31383;&#21475;&#28040;&#38500;&#22788;&#29702;&#20316;&#20026;&#24212;&#23545;&#27492;&#31181;&#25915;&#20987;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13372</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#38745;&#24577;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness of Learning-based Static Malware Classifiers. (arXiv:2303.13372v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65288;UAP&#65289;&#25915;&#20987;&#26041;&#27861;&#65292;&#21482;&#38656;&#38468;&#21152;&#19968;&#20010;&#30456;&#23545;&#36739;&#23567;&#30340;&#23383;&#33410;&#32423;&#34917;&#19969;&#21363;&#21487;&#32469;&#36807;MalConv&#20998;&#31867;&#22120;&#30340;&#26816;&#27979;&#65292;&#21487;&#20197;&#23558;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#30340;&#26816;&#27979;&#29575;&#38477;&#20302;80&#65285;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#31383;&#21475;&#28040;&#38500;&#22788;&#29702;&#20316;&#20026;&#24212;&#23545;&#27492;&#31181;&#25915;&#20987;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#19968;&#30452;&#26159;&#24694;&#24847;&#36719;&#20214;&#20316;&#32773;&#21644;&#21453;&#30149;&#27602;&#31995;&#32479;&#20043;&#38388;&#25345;&#32493;&#30340;&#20891;&#22791;&#31454;&#36187;&#38454;&#27573;&#12290;&#38543;&#30528;&#36825;&#22330;&#31454;&#36187;&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#24471;&#21040;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36235;&#21183;&#20351;&#24471;&#30452;&#25509;&#23545;ML&#36827;&#34892;&#25915;&#20987;&#23545;&#20110;&#23545;&#25163;&#32780;&#35328;&#25104;&#20026;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#21069;&#26223;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#22330;&#20891;&#22791;&#31454;&#36187;&#30340;&#20004;&#20010;&#26041;&#38754;&#65292;&#21363;&#20174;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#30340;&#21407;&#22987;&#23383;&#33410;&#20013;&#25805;&#20316;&#30340;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#20998;&#31867;&#22120;MalConv&#30340;&#35282;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;MalConv&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#34917;&#19969;&#25915;&#20987;&#30340;&#24433;&#21709;:&#23558;&#19968;&#20010;&#23383;&#33410;&#32423;&#30340;&#34917;&#19969;&#38468;&#21152;&#21040;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#20013;&#65292;&#20351;&#20854;&#32469;&#36807;&#26816;&#27979;&#30340;&#27010;&#29575;&#39640;&#36798;94.3&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65288;UAP&#65289;&#25915;&#20987;&#65292;&#22312;&#20219;&#20309;&#21253;&#21547;&#35813;&#34917;&#19969;&#30340;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#30340;&#24658;&#23450;&#26102;&#38388;&#20869;&#65292;&#21487;&#20197;&#23558;&#20854;&#26816;&#27979;&#29575;&#38477;&#20302;80&#65285;&#12290;&#21363;&#20351;&#30456;&#23545;&#20110;&#21407;&#22987;&#25991;&#20214;&#22823;&#23567;&#32780;&#35328;&#65292;&#36825;&#20123;&#34917;&#19969;&#30340;&#22823;&#23567;&#20063;&#30456;&#23545;&#36739;&#23567;-&#22312;2&#65285;-8&#65285;&#20043;&#38388;&#12290;&#20026;&#20102;&#25269;&#24481;&#36825;&#31181;&#25915;&#20987;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31383;&#21475;&#28040;&#38500;&#22788;&#29702;&#65292;&#20801;&#35768;&#35782;&#21035;&#24694;&#24847;&#20195;&#30721;&#30340;&#37096;&#20998;&#19981;&#21463;&#23545;&#25239;&#24615;&#34917;&#19969;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware detection has long been a stage for an ongoing arms race between malware authors and anti-virus systems. Solutions that utilize machine learning (ML) gain traction as the scale of this arms race increases. This trend, however, makes performing attacks directly on ML an attractive prospect for adversaries. We study this arms race from both perspectives in the context of MalConv, a popular convolutional neural network-based malware classifier that operates on raw bytes of files. First, we show that MalConv is vulnerable to adversarial patch attacks: appending a byte-level patch to malware files bypasses detection 94.3% of the time. Moreover, we develop a universal adversarial patch (UAP) attack where a single patch can drop the detection rate in constant time of any malware file that contains it by 80%. These patches are effective even being relatively small with respect to the original file size -between 2%-8%. As a countermeasure, we then perform window ablation that allows u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;&#21345;&#23572;&#26364;&#21644;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#25512;&#24191;&#21040;&#22270;&#24418;&#19978;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#36866;&#29992;&#20110;&#36755;&#20986;&#26159;&#21521;&#37327;&#25110;&#26631;&#37327;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#26410;&#30693;&#30340;&#29366;&#24577;&#36716;&#31227;&#21644;&#35835;&#21462;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.12021</link><description>&lt;p&gt;
&#22270;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Kalman Filters. (arXiv:2303.12021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;&#21345;&#23572;&#26364;&#21644;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#25512;&#24191;&#21040;&#22270;&#24418;&#19978;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#36866;&#29992;&#20110;&#36755;&#20986;&#26159;&#21521;&#37327;&#25110;&#26631;&#37327;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#26410;&#30693;&#30340;&#29366;&#24577;&#36716;&#31227;&#21644;&#35835;&#21462;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#36890;&#36807;&#20351;&#29992;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#26469;&#27169;&#25311;&#21160;&#24577;&#31995;&#32479;&#65292;&#19979;&#19968;&#20010;&#29366;&#24577;&#30340;&#26356;&#26032;&#20197;&#21450;&#19982;&#26032;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#36755;&#20986;&#30456;&#20851;&#30340;&#20449;&#24687;&#26469;&#25511;&#21046;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#21345;&#23572;&#26364;&#21644;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#25512;&#24191;&#21040;&#31163;&#25955;&#26102;&#38388;&#30340;&#35774;&#32622;&#19979;&#65292;&#20854;&#20013;&#36755;&#20837;&#12289;&#29366;&#24577;&#21644;&#36755;&#20986;&#22343;&#34920;&#31034;&#20026;&#24102;&#23646;&#24615;&#30340;&#22270;&#24418;&#65292;&#20854;&#25299;&#25169;&#21644;&#23646;&#24615;&#21487;&#20197;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#27492;&#35774;&#32622;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#26694;&#26550;&#36866;&#24212;&#20110;&#36755;&#20986;&#26159;&#21521;&#37327;&#25110;&#26631;&#37327;&#30340;&#24773;&#20917;&#65288;&#33410;&#28857;/&#22270;&#32423;&#20219;&#21153;&#65289;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#20869;&#65292;&#26410;&#30693;&#30340;&#29366;&#24577;&#36716;&#31227;&#21644;&#35835;&#21462;&#20989;&#25968;&#19982;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#19968;&#36215;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The well-known Kalman filters model dynamical systems by relying on state-space representations with the next state updated, and its uncertainty controlled, by fresh information associated with newly observed system outputs. This paper generalizes, for the first time in the literature, Kalman and extended Kalman filters to discrete-time settings where inputs, states, and outputs are represented as attributed graphs whose topology and attributes can change with time. The setup allows us to adapt the framework to cases where the output is a vector or a scalar too (node/graph level tasks). Within the proposed theoretical framework, the unknown state-transition and the readout functions are learned end-to-end along with the downstream prediction task.
&lt;/p&gt;</description></item><item><title>&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#26694;&#26550;PURER&#65292;&#36890;&#36807;ECI&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;ICFIL&#23545;&#21453;&#28436;&#26799;&#24230;&#36827;&#34892;&#26657;&#20934;&#26469;&#20248;&#21270;&#21453;&#28436;&#36807;&#31243;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11183</link><description>&lt;p&gt;
&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning. (arXiv:2303.11183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11183
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#26694;&#26550;PURER&#65292;&#36890;&#36807;ECI&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;ICFIL&#23545;&#21453;&#28436;&#26799;&#24230;&#36827;&#34892;&#26657;&#20934;&#26469;&#20248;&#21270;&#21453;&#28436;&#36807;&#31243;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#30340;&#30446;&#30340;&#26159;&#20174;&#19968;&#32452;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#20854;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#35299;&#20915;&#20102;&#35813;&#38382;&#39064;&#65292;&#24573;&#30053;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34164;&#21547;&#30340;&#20016;&#23500;&#25968;&#25454;&#30693;&#35782;&#65292;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21482;&#33021;&#20803;&#23398;&#20064;&#20855;&#26377;&#30456;&#21516;&#32593;&#32476;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#8212;&#8212;PURER&#65292;&#20854;&#20013;&#21253;&#21547;&#65306;&#65288;1&#65289;&#25968;&#25454;&#26080;&#20851;&#30340;&#20803;&#35757;&#32451;&#26399;&#38388;&#30340;&#33410;&#30446;&#35838;&#31243;&#21453;&#36716;&#65288;ECI&#65289;&#65307;&#65288;2&#65289;&#20803;&#27979;&#35797;&#26399;&#38388;&#20869;&#37096;&#24490;&#29615;&#21518;&#30340;&#21453;&#28436;&#26657;&#20934;&#65288;ICFIL&#65289;&#12290;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ECI&#26469;&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#65292;&#20197;&#20415;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#30475;&#19981;&#35265;&#30340;&#20219;&#21153;&#12290;&#22312;&#20803;&#27979;&#35797;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ICFIL&#26469;&#26657;&#20934;&#21453;&#28436;&#26799;&#24230;&#65292;&#20197;&#20943;&#23569;&#22522;&#20110;&#21453;&#28436;&#30340;&#20248;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;PURER&#21487;&#20197;&#26377;&#25928;&#22320;&#20803;&#23398;&#20064;&#26469;&#33258;&#20855;&#26377;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#22495;&#29978;&#33267;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pre-trained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed PURER, which contains: (1) ePisode cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion calibRation following inner loop (ICFIL) during meta testing. During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. Specifically, we progressively synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model. The ECI adaptively increases the difficulty level of pseudo episodes accord
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31354;&#38388;&#26102;&#38388;&#20449;&#24687;&#26469;&#25552;&#39640;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#39640;&#32500;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#24182;&#24212;&#29992;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#20197;&#21435;&#38500;&#20887;&#20313;&#20449;&#24687;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.08331</link><description>&lt;p&gt;
&#36890;&#36807;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#36807;&#25311;&#21512;&#23454;&#29616;&#39640;&#36136;&#37327;&#39640;&#25928;&#30340;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Towards High-Quality and Efficient Video Super-Resolution via Spatial-Temporal Data Overfitting. (arXiv:2303.08331v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31354;&#38388;&#26102;&#38388;&#20449;&#24687;&#26469;&#25552;&#39640;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#39640;&#32500;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#24182;&#24212;&#29992;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#20197;&#21435;&#38500;&#20887;&#20313;&#20449;&#24687;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(DNN)&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#21033;&#29992;DNN&#30340;&#36807;&#25311;&#21512;&#33021;&#21147;&#23454;&#29616;&#35270;&#39057;&#20998;&#36776;&#29575;&#30340;&#25552;&#21319;&#24050;&#32463;&#25104;&#20026;&#29616;&#20195;&#35270;&#39057;&#20256;&#36755;&#31995;&#32479;&#30340;&#26032;&#36235;&#21183;&#12290;&#23558;&#35270;&#39057;&#20998;&#20026;&#22359;&#24182;&#23558;&#27599;&#20010;&#22359;&#19982;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#36807;&#25311;&#21512;&#65292;&#20174;&#32780;&#22312;&#20256;&#36755;&#32473;&#23458;&#25143;&#31471;&#20043;&#21069;&#23545;&#35270;&#39057;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#35270;&#39057;&#36136;&#37327;&#21644;&#20256;&#36755;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#35777;&#33391;&#22909;&#30340;&#36807;&#25311;&#21512;&#36136;&#37327;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#22359;&#65292;&#36825;&#20250;&#22823;&#22823;&#22686;&#21152;&#23384;&#20648;&#37327;&#21644;&#28040;&#32791;&#26356;&#22810;&#24102;&#23485;&#36164;&#28304;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#35757;&#32451;&#20248;&#21270;&#25216;&#26415;&#20943;&#23569;&#22359;&#30340;&#25968;&#37327;&#36890;&#24120;&#38656;&#35201;&#39640;&#27169;&#22411;&#23481;&#37327;&#65292;&#36825;&#20250;&#26174;&#33879;&#38477;&#20302;&#25191;&#34892;&#36895;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25104;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#30340;&#35270;&#39057;&#20998;&#36776;&#29575;&#21319;&#32423;&#20219;&#21153;&#65292;&#21033;&#29992;&#31354;&#38388;&#26102;&#38388;&#20449;&#24687;&#26469;&#20934;&#30830;&#25429;&#25417;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#35270;&#39057;&#22359;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#35270;&#39057;&#22359;&#30340;&#31354;&#38388;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#39640;&#32500;&#21367;&#31215;&#32593;&#32476;&#25913;&#36827;&#27599;&#20010;&#22359;&#30340;&#39044;&#27979;&#65292;&#24182;&#36827;&#19968;&#27493;&#24212;&#29992;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#20197;&#21435;&#38500;&#20887;&#20313;&#20449;&#24687;&#24182;&#20419;&#36827;&#20256;&#36755;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep convolutional neural networks (DNNs) are widely used in various fields of computer vision, leveraging the overfitting ability of the DNN to achieve video resolution upscaling has become a new trend in the modern video delivery system. By dividing videos into chunks and overfitting each chunk with a super-resolution model, the server encodes videos before transmitting them to the clients, thus achieving better video quality and transmission efficiency. However, a large number of chunks are expected to ensure good overfitting quality, which substantially increases the storage and consumes more bandwidth resources for data transmission. On the other hand, decreasing the number of chunks through training optimization techniques usually requires high model capacity, which significantly slows down execution speed. To reconcile such, we propose a novel method for high-quality and efficient video resolution upscaling tasks, which leverages the spatial-temporal information to accurately
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24615;&#33021;&#35780;&#20272;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#24615;&#33021;&#30340;&#20272;&#35745;&#20540;&#36807;&#20110;&#20048;&#35266;&#65292;&#23481;&#26131;&#23548;&#33268;&#26041;&#27861;&#30340;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#22810;&#37325;&#24615;&#20559;&#24046;&#24182;&#27604;&#36739;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07272</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#22810;&#37325;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
What is the state of the art? Accounting for multiplicity in machine learning benchmark performance. (arXiv:2303.07272v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07272
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24615;&#33021;&#35780;&#20272;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#24615;&#33021;&#30340;&#20272;&#35745;&#20540;&#36807;&#20110;&#20048;&#35266;&#65292;&#23481;&#26131;&#23548;&#33268;&#26041;&#27861;&#30340;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#22810;&#37325;&#24615;&#20559;&#24046;&#24182;&#27604;&#36739;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#22312;&#20844;&#20849;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26469;&#36827;&#34892;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#36825;&#20801;&#35768;&#22810;&#31181;&#26041;&#27861;&#65292;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#24182;&#36328;&#36234;&#26102;&#38388;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#38382;&#39064;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#24615;&#33021;&#34987;&#31216;&#20026;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#24615;&#33021;&#65292;&#24182;&#19988;&#34987;&#29992;&#20316;&#26032;&#26041;&#27861;&#20986;&#29256;&#30340;&#21442;&#32771;&#28857;&#12290;&#20294;&#20351;&#29992;&#26368;&#39640;&#25490;&#21517;&#30340;&#24615;&#33021;&#20316;&#20026;SOTA&#30340;&#20272;&#35745;&#20540;&#26159;&#19968;&#31181;&#26377;&#20559;&#30340;&#20272;&#35745;&#22120;&#65292;&#20250;&#32473;&#20986;&#36807;&#20110;&#20048;&#35266;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#22810;&#37325;&#24615;&#30340;&#26426;&#21046;&#26159;&#22810;&#37325;&#27604;&#36739;&#21644;&#22810;&#37325;&#26816;&#39564;&#20013;&#24191;&#27867;&#30740;&#31350;&#30340;&#20027;&#39064;&#65292;&#20294;&#22312;&#20851;&#20110;SOTA&#20272;&#35745;&#30340;&#35752;&#35770;&#20013;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#25552;&#21450;&#12290;&#36807;&#20110;&#20048;&#35266;&#30340;&#26368;&#20808;&#36827;&#20272;&#35745;&#20540;&#34987;&#29992;&#20316;&#35780;&#20272;&#26032;&#26041;&#27861;&#30340;&#26631;&#20934;&#65292;&#32780;&#20855;&#26377;&#26126;&#26174;&#21155;&#21183;&#32467;&#26524;&#30340;&#26041;&#27861;&#24456;&#23481;&#26131;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#22810;&#37325;&#24615;&#20559;&#24046;&#24182;&#27604;&#36739;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods are commonly evaluated and compared by their performance on data sets from public repositories. This allows for multiple methods, oftentimes several thousands, to be evaluated under identical conditions and across time. The highest ranked performance on a problem is referred to as state-of-the-art (SOTA) performance, and is used, among other things, as a reference point for publication of new methods. Using the highest-ranked performance as an estimate for SOTA is a biased estimator, giving overly optimistic results. The mechanisms at play are those of multiplicity, a topic that is well-studied in the context of multiple comparisons and multiple testing, but has, as far as the authors are aware of, been nearly absent from the discussion regarding SOTA estimates. The optimistic state-of-the-art estimate is used as a standard for evaluating new methods, and methods with substantial inferior results are easily overlooked. In this article, we provide a probability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#65292;&#21253;&#25324;&#21160;&#24577;&#38376;&#25511;&#12289;&#19987;&#23478;&#32531;&#20914;&#21644;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25191;&#34892;&#26102;&#38388;&#21644;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.06182</link><description>&lt;p&gt;
&#36808;&#21521;MoE&#37096;&#32626;&#65306;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#25512;&#29702;&#20013;&#30340;&#20302;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference. (arXiv:2303.06182v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#65292;&#21253;&#25324;&#21160;&#24577;&#38376;&#25511;&#12289;&#19987;&#23478;&#32531;&#20914;&#21644;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25191;&#34892;&#26102;&#38388;&#21644;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes three optimization techniques to mitigate inefficiencies in Mixture-of-Experts (MoE) models during inference, including dynamic gating, expert buffering, and expert load balancing. These techniques can significantly improve execution time and reduce memory usage.
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#26368;&#36817;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24191;&#27867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#26377;&#25928;&#22320;&#25193;&#23637;&#20102;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#22686;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#23567;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#37096;&#32626;&#36825;&#26679;&#30340;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;MoE&#24037;&#20316;&#36127;&#36733;&#30340;&#29305;&#24449;&#21270;&#65292;&#21363;&#35821;&#35328;&#24314;&#27169;&#65288;LM&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#22312;&#37096;&#32626;&#26102;&#30340;&#20302;&#25928;&#29575;&#26469;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#20302;&#25928;&#29575;&#30340;&#26469;&#28304;&#65292;&#21363;&#65288;1&#65289;&#21160;&#24577;&#38376;&#25511;&#65292;&#65288;2&#65289;&#19987;&#23478;&#32531;&#20914;&#21644;&#65288;3&#65289;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21160;&#24577;&#38376;&#25511;&#21487;&#20197;&#20351;LM&#30340;&#25191;&#34892;&#26102;&#38388;&#25552;&#39640;1.25-4&#20493;&#65292;MT&#32534;&#30721;&#22120;&#25552;&#39640;2-5&#20493;&#65292;MT&#35299;&#30721;&#22120;&#25552;&#39640;1.09-1.5&#20493;&#12290;&#23427;&#36824;&#21487;&#20197;&#23558;LM&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;1.36&#20493;&#65292;MT&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;1.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Experts (MoE) models have recently gained steam in achieving the state-of-the-art performance in a wide range of tasks in computer vision and natural language processing. They effectively expand the model capacity while incurring a minimal increase in computation cost during training. However, deploying such models for inference is difficult due to their large model size and complex communication pattern. In this work, we provide a characterization of two MoE workloads, namely Language Modeling (LM) and Machine Translation (MT) and identify their sources of inefficiencies at deployment.  We propose three optimization techniques to mitigate sources of inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert load balancing. We show that dynamic gating improves execution time by 1.25-4$\times$ for LM, 2-5$\times$ for MT Encoder and 1.09-1.5$\times$ for MT Decoder. It also reduces memory usage by up to 1.36$\times$ for LM and up to 1.1$\times$ for MT. We f
&lt;/p&gt;</description></item><item><title>Pacos&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#20381;&#36182;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#20559;&#22909;&#36870;&#36716;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#12289;&#27604;&#36739;&#21644;&#26174;&#31034;&#20301;&#32622;&#31561;&#21487;&#35299;&#37322;&#22240;&#32032;&#65292;&#26377;&#21161;&#20110;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.05648</link><description>&lt;p&gt;
Pacos: &#24314;&#27169;&#29992;&#25143;&#30340;&#21487;&#35299;&#37322;&#21644;&#19978;&#19979;&#25991;&#20381;&#36182;&#36873;&#25321;&#20197;&#22788;&#29702;&#20559;&#22909;&#36870;&#36716;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Pacos: Modeling Users' Interpretable and Context-Dependent Choices in Preference Reversals. (arXiv:2303.05648v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05648
&lt;/p&gt;
&lt;p&gt;
Pacos&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#20381;&#36182;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#20559;&#22909;&#36870;&#36716;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#12289;&#27604;&#36739;&#21644;&#26174;&#31034;&#20301;&#32622;&#31561;&#21487;&#35299;&#37322;&#22240;&#32032;&#65292;&#26377;&#21161;&#20110;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#38382;&#39064;&#26159;&#25351;&#20174;&#22810;&#20010;&#39033;&#30446;&#20013;&#36873;&#25321;&#26368;&#20339;&#36873;&#25321;&#65292;&#23398;&#20064;&#29992;&#25143;&#22312;&#36873;&#25321;&#38382;&#39064;&#20013;&#30340;&#20559;&#22909;&#23545;&#20110;&#29702;&#35299;&#20915;&#31574;&#26426;&#21046;&#21644;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#20154;&#20204;&#29420;&#31435;&#22320;&#35780;&#20272;&#39033;&#30446;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20294;&#26159;&#29992;&#25143;&#30340;&#20559;&#22909;&#21462;&#20915;&#20110;&#39033;&#30446;&#25152;&#22788;&#30340;&#24066;&#22330;&#65292;&#36825;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#25928;&#24212;&#65307;&#32780;&#29992;&#25143;&#23545;&#20004;&#20010;&#39033;&#30446;&#30340;&#20559;&#22909;&#39034;&#24207;&#29978;&#33267;&#21487;&#33021;&#34987;&#39072;&#20498;&#65292;&#36825;&#34987;&#31216;&#20026;&#20559;&#22909;&#36870;&#36716;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#23548;&#33268;&#19978;&#19979;&#25991;&#25928;&#24212;&#30340;&#19977;&#20010;&#22240;&#32032;&#65306;&#29992;&#25143;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#12289;&#39033;&#30446;&#20043;&#38388;&#30340;&#27604;&#36739;&#21644;&#26174;&#31034;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Pacos&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#20559;&#22909;&#27169;&#22411;&#20316;&#20026;&#32479;&#19968;&#26694;&#26550;&#26469;&#21516;&#26102;&#35299;&#20915;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#31181;&#35774;&#35745;&#26041;&#27861;&#65292;&#21253;&#25324;&#20855;&#26377;&#39640;&#21487;&#35299;&#37322;&#24615;&#30340;&#21152;&#24615;&#26041;&#27861;&#21644;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#22522;&#20110;ANN&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#21508;&#31181;&#24066;&#22330;&#24773;&#26223;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#20559;&#22909;&#36870;&#36716;&#26465;&#20214;&#65292;&#24182;&#23637;&#31034;&#20102;Pacos&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#20559;&#22909;&#36870;&#36716;&#12290;&#27492;&#22806;&#65292;Pacos&#21487;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#25928;&#24212;&#21644;&#29992;&#25143;&#33258;&#36866;&#24212;&#34892;&#20026;&#30340;&#21487;&#35299;&#37322;&#25351;&#31034;&#65292;&#26377;&#21161;&#20110;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Choice problems refer to selecting the best choices from several items, and learning users' preferences in choice problems is of great significance in understanding the decision making mechanisms and providing personalized services. Existing works typically assume that people evaluate items independently. In practice, however, users' preferences depend on the market in which items are placed, which is known as context effects; and the order of users' preferences for two items may even be reversed, which is referred to preference reversals. In this work, we identify three factors contributing to context effects: users' adaptive weights, the inter-item comparison, and display positions. We propose a context-dependent preference model named Pacos as a unified framework for addressing three factors simultaneously, and consider two design methods including an additive method with high interpretability and an ANN-based method with high accuracy. We study the conditions for preference reversa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GigaGAN&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;GAN&#26550;&#26500;&#65292;&#36828;&#36828;&#36229;&#36807;&#20043;&#21069;&#20351;&#29992;&#30340;StyleGAN&#12290;GigaGAN&#20855;&#26377;&#19977;&#20010;&#37325;&#35201;&#20248;&#21183;&#65306;&#36229;&#24555;&#36895;&#30340;&#25512;&#29702;&#26102;&#38388;&#12289;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#30340;&#33021;&#21147;&#21644;&#26356;&#31934;&#30830;&#30340;&#22270;&#20687;&#21512;&#25104;&#25511;&#21046;&#12290;&#36825;&#20351;GAN&#25104;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#21487;&#34892;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2303.05511</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;GAN&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Scaling up GANs for Text-to-Image Synthesis. (arXiv:2303.05511v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GigaGAN&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;GAN&#26550;&#26500;&#65292;&#36828;&#36828;&#36229;&#36807;&#20043;&#21069;&#20351;&#29992;&#30340;StyleGAN&#12290;GigaGAN&#20855;&#26377;&#19977;&#20010;&#37325;&#35201;&#20248;&#21183;&#65306;&#36229;&#24555;&#36895;&#30340;&#25512;&#29702;&#26102;&#38388;&#12289;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#30340;&#33021;&#21147;&#21644;&#26356;&#31934;&#30830;&#30340;&#22270;&#20687;&#21512;&#25104;&#25511;&#21046;&#12290;&#36825;&#20351;GAN&#25104;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#21487;&#34892;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#25104;&#21151;&#24341;&#36215;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20851;&#27880;&#65292;&#24182;&#21560;&#24341;&#20102;&#26222;&#36890;&#22823;&#20247;&#30340;&#24819;&#35937;&#21147;&#12290;&#20174;&#25216;&#26415;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#20063;&#26631;&#24535;&#30528;&#35774;&#35745;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#30340;&#20248;&#20808;&#26550;&#26500;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#21464;&#21270;&#12290;&#36807;&#21435;&#65292;GAN&#26159;&#20107;&#23454;&#19978;&#30340;&#36873;&#25321;&#65292;&#26377;&#30528;&#20687;StyleGAN&#36825;&#26679;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;DALL-E2&#20043;&#21518;&#65292;&#33258;&#22238;&#24402;&#21644;&#25193;&#25955;&#27169;&#22411;&#25104;&#20026;&#20102;&#22823;&#23610;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26631;&#20934;&#12290;&#36825;&#20010;&#24555;&#36895;&#30340;&#36716;&#21464;&#24341;&#20986;&#20102;&#19968;&#20010;&#26681;&#26412;&#24615;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#25193;&#23637;GAN&#20197;&#20174;&#20687;LAION&#36825;&#26679;&#30340;&#22823;&#25968;&#25454;&#38598;&#20013;&#21463;&#30410;&#65311;&#25105;&#20204;&#21457;&#29616;&#65292;&#31616;&#21333;&#22320;&#22686;&#21152;StyleGAN&#26550;&#26500;&#30340;&#23481;&#37327;&#24456;&#24555;&#23601;&#20250;&#21464;&#24471;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#20171;&#32461;GigaGAN&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;GAN&#26550;&#26500;&#65292;&#36828;&#36828;&#36229;&#36807;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#35777;&#26126;GAN&#26159;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#19968;&#20010;&#21487;&#34892;&#36873;&#39033;&#12290;GigaGAN&#25552;&#20379;&#20102;&#19977;&#20010;&#37325;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#30340;&#25512;&#29702;&#26102;&#38388;&#27604;&#20043;&#21069;&#24555;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#21482;&#38656;&#35201;0.13&#31186;&#21363;&#21487;&#21512;&#25104;512&#20687;&#32032;&#22270;&#20687;&#12290;&#20854;&#27425;&#65292;&#23427;&#21487;&#20197;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#65292;&#20363;&#22914;2048&#215;2048&#65292;&#36825;&#26159;&#20043;&#21069;&#22312;&#35813;&#39046;&#22495;&#20869;GAN&#26080;&#27861;&#23454;&#29616;&#30340;&#12290;&#31532;&#19977;&#65292;GigaGAN&#20801;&#35768;&#26356;&#22909;&#22320;&#25511;&#21046;&#22270;&#20687;&#21512;&#25104;&#36807;&#31243;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#25351;&#23450;&#23039;&#21183;&#12289;&#20809;&#29031;&#21644;&#32972;&#26223;&#31561;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL-E 2, auto-regressive and diffusion models became the new standard for large-scale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that na\"Ively increasing the capacity of the StyleGAN architecture quickly becomes unstable. We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for exam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Cal-QL&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#24494;&#35843;&#26102;&#21516;&#26102;&#20445;&#38556;&#20102;&#24555;&#36895;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05479</link><description>&lt;p&gt;
Cal-QL: &#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#39044;&#20808;&#35757;&#32451;&#29992;&#20110;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. (arXiv:2303.05479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Cal-QL&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#24494;&#35843;&#26102;&#21516;&#26102;&#20445;&#38556;&#20102;&#24555;&#36895;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#31574;&#30053;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#26377;&#38480;&#20114;&#21160;&#36827;&#34892;&#24555;&#36895;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#22312;&#32447;&#24494;&#35843;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20445;&#23432;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#24494;&#35843;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#26377;&#25928;&#30340;&#21021;&#22987;&#21270;&#65292;&#24182;&#20351;&#20854;&#20855;&#22791;&#24555;&#36895;&#30340;&#22312;&#32447;&#24494;&#35843;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;Cal-QL&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20302;&#20272;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#21516;&#26102;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;Q&#20540;&#22312;&#21512;&#29702;&#30340;&#33539;&#22260;&#20869;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#29615;&#22659;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#24182;&#19988;&#20063;&#33021;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#38382;&#39064;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we study the fine-tuning problem in the context of conservative offline RL methods and we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also ensuring that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20307;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#20132;&#20114;&#19982;&#19987;&#23478;&#23398;&#20064;&#31070;&#32463;&#35859;&#35789;&#35299;&#37322;&#12289;&#31526;&#21495;&#35268;&#21010;&#31639;&#23376;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20851;&#31995;&#29366;&#24577;&#25277;&#35937;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04912</link><description>&lt;p&gt;
&#22522;&#20110;&#36523;&#20307;&#30693;&#35782;&#30340;&#20851;&#31995;&#29366;&#24577;&#25277;&#35937;&#30340;&#21452;&#23618;&#35268;&#21010;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Embodied Active Learning of Relational State Abstractions for Bilevel Planning. (arXiv:2303.04912v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04912
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20307;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#20132;&#20114;&#19982;&#19987;&#23478;&#23398;&#20064;&#31070;&#32463;&#35859;&#35789;&#35299;&#37322;&#12289;&#31526;&#21495;&#35268;&#21010;&#31639;&#23376;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20851;&#31995;&#29366;&#24577;&#25277;&#35937;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#25277;&#35937;&#26159;&#22312;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#36827;&#34892;&#35268;&#21010;&#30340;&#26377;&#25928;&#25216;&#26415;&#65292;&#35813;&#29615;&#22659;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#12289;&#38271;&#20219;&#21153;&#26102;&#38388;&#21644;&#31232;&#30095;&#21453;&#39304;&#12290;&#22312;&#38754;&#21521;&#23545;&#35937;&#30340;&#29615;&#22659;&#20013;&#65292;&#35859;&#35789;&#26159;&#19968;&#31181;&#29305;&#21035;&#26377;&#29992;&#30340;&#29366;&#24577;&#25277;&#35937;&#24418;&#24335;&#65292;&#22240;&#20026;&#20854;&#19982;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#20860;&#23481;&#24615;&#20197;&#21450;&#20854;&#20851;&#31995;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35201;&#20351;&#29992;&#35859;&#35789;&#36827;&#34892;&#35268;&#21010;&#65292;&#20195;&#29702;&#24517;&#39035;&#33021;&#22815;&#22312;&#36830;&#32493;&#29615;&#22659;&#29366;&#24577;&#19979;&#35299;&#37322;&#23427;&#20204;&#65288;&#21363;&#25509;&#22320;&#31526;&#21495;&#65289;&#12290;&#25163;&#21160;&#32534;&#31243;&#35859;&#35789;&#35299;&#37322;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#22240;&#27492;&#25105;&#20204;&#24076;&#26395;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20307;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#19982;&#19987;&#23478;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#26469;&#23398;&#20064;&#35859;&#35789;&#35299;&#37322;&#12290;&#20363;&#22914;&#65292;&#22312;&#22534;&#21472;&#31215;&#26408;&#29615;&#22659;&#20013;&#37319;&#21462;&#34892;&#21160;&#21518;&#65292;&#20195;&#29702;&#21487;&#33021;&#20250;&#38382;&#19987;&#23478;&#65306;&#8220;On(block1&#65292;block2)&#26159;&#21542;&#20026;&#30495;&#65311;&#8221;&#20174;&#36825;&#20010;&#32463;&#39564;&#20013;&#65292;&#20195;&#29702;&#23398;&#20064;&#35268;&#21010;&#65306;&#23398;&#20064;&#31070;&#32463;&#35859;&#35789;&#35299;&#37322;&#12289;&#31526;&#21495;&#35268;&#21010;&#31639;&#23376;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20851;&#31995;&#29366;&#24577;&#25277;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#26377;&#25928;&#30340;&#25277;&#35937;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#25913;&#36827;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
State abstraction is an effective technique for planning in robotics environments with continuous states and actions, long task horizons, and sparse feedback. In object-oriented environments, predicates are a particularly useful form of state abstraction because of their compatibility with symbolic planners and their capacity for relational generalization. However, to plan with predicates, the agent must be able to interpret them in continuous environment states (i.e., ground the symbols). Manually programming predicate interpretations can be difficult, so we would instead like to learn them from data. We propose an embodied active learning paradigm where the agent learns predicate interpretations through online interaction with an expert. For example, after taking actions in a block stacking environment, the agent may ask the expert: "Is On(block1, block2) true?" From this experience, the agent learns to plan: it learns neural predicate interpretations, symbolic planning operators, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#20174;&#21628;&#21560;&#22768;&#38899;&#35760;&#24405;&#20013;&#26816;&#27979;&#24322;&#24120;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;Gammatone&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#36827;&#34892;&#38899;&#39057;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#23558;Inception-Residual-based&#30340;&#39592;&#24178;&#27169;&#22411;&#19982;multi-head attention&#21644;multi-objective loss&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#30340;&#26041;&#27861;&#26469;&#24179;&#34913;&#27599;&#20010;&#21333;&#29420;&#39057;&#35889;&#22270;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.04104</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Inception-Residual&#30340;&#26550;&#26500;&#21644;&#22810;&#30446;&#26631;&#25439;&#22833;&#30340;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Inception-Residual-Based Architecture with Multi-Objective Loss for Detecting Respiratory Anomalies. (arXiv:2303.04104v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#20174;&#21628;&#21560;&#22768;&#38899;&#35760;&#24405;&#20013;&#26816;&#27979;&#24322;&#24120;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;Gammatone&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#36827;&#34892;&#38899;&#39057;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#23558;Inception-Residual-based&#30340;&#39592;&#24178;&#27169;&#22411;&#19982;multi-head attention&#21644;multi-objective loss&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#30340;&#26041;&#27861;&#26469;&#24179;&#34913;&#27599;&#20010;&#21333;&#29420;&#39057;&#35889;&#22270;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#21628;&#21560;&#22768;&#38899;&#35760;&#24405;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#24320;&#22987;&#20110;&#20351;&#29992;Gammatone&#21644;Continuous Wavelet&#36716;&#25442;&#36827;&#34892;&#38899;&#39057;&#29305;&#24449;&#25552;&#21462;&#12290;&#36825;&#19968;&#27493;&#26088;&#22312;&#23558;&#21628;&#21560;&#22768;&#38899;&#36755;&#20837;&#36716;&#25442;&#20026;&#20108;&#32500;&#39057;&#35889;&#22270;&#65292;&#20854;&#20013;&#23637;&#29616;&#20102;&#35889;&#21644;&#26102;&#22495;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#23558;Inception-Residual-based&#30340;&#39592;&#24178;&#27169;&#22411;&#19982;multi-head attention&#21644;multi-objective loss&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20998;&#31867;&#21628;&#21560;&#24322;&#24120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#32452;&#21512;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#30340;&#36830;&#25509;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#34913;&#27599;&#20010;&#21333;&#29420;&#39057;&#35889;&#22270;&#36129;&#29486;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;IEEE BioCAS 2022&#25361;&#25112;&#36187;&#25552;&#20986;&#30340;SPRSound&#65288;&#24320;&#28304;SJTU&#20799;&#31185;&#21628;&#21560;&#22768;&#38899;&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning system applied for detecting anomalies from respiratory sound recordings. Initially, our system begins with audio feature extraction using Gammatone and Continuous Wavelet transformation. This step aims to transform the respiratory sound input into a two-dimensional spectrogram where both spectral and temporal features are presented. Then, our proposed system integrates Inception-residual-based backbone models combined with multi-head attention and multi-objective loss to classify respiratory anomalies. Instead of applying a simple concatenation approach by combining results from various spectrograms, we propose a Linear combination, which has the ability to regulate equally the contribution of each individual spectrogram throughout the training process. To evaluate the performance, we conducted experiments over the benchmark dataset of SPRSound (The Open-Source SJTU Paediatric Respiratory Sound) proposed by the IEEE BioCAS 2022 challenge. As regards
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#22810;&#23545;&#31216;&#38598;&#21512;&#65288;MSE&#65289;&#65292;&#23427;&#36890;&#36807;&#23545;&#31216;&#36724;&#19978;&#20551;&#35774;&#30340;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#22810;&#26679;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#36229;&#36234;&#20256;&#32479;&#38543;&#26426;&#25200;&#21160;&#30340;&#26041;&#27861;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.02484</link><description>&lt;p&gt;
&#22810;&#23545;&#31216;&#38598;&#21512;&#65306;&#36890;&#36807;&#21453;&#21521;&#23545;&#31216;&#24615;&#25552;&#39640;&#22810;&#26679;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Multi-Symmetry Ensembles: Improving Diversity and Generalization via Opposing Symmetries. (arXiv:2303.02484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#22810;&#23545;&#31216;&#38598;&#21512;&#65288;MSE&#65289;&#65292;&#23427;&#36890;&#36807;&#23545;&#31216;&#36724;&#19978;&#20551;&#35774;&#30340;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#22810;&#26679;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#36229;&#36234;&#20256;&#32479;&#38543;&#26426;&#25200;&#21160;&#30340;&#26041;&#27861;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#21512;&#65288;DE&#65289;&#36890;&#36807;&#23398;&#20064;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#22810;&#26679;&#21270;&#25104;&#21592;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#36229;&#21442;&#25968;&#25110;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22810;&#26679;&#24615;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20173;&#28982;&#20381;&#36182;&#20110;&#38543;&#26426;&#26041;&#27861;&#26469;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23545;&#31216;&#38598;&#21512;&#65288;MSE&#65289;&#65292;&#36890;&#36807;&#25429;&#25417;&#23545;&#31216;&#36724;&#19978;&#20551;&#35774;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26500;&#24314;&#22810;&#26679;&#24615;&#38598;&#21512;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#36229;&#36234;&#27169;&#22411;&#26435;&#37325;&#21644;&#36229;&#21442;&#25968;&#30340;&#38543;&#26426;&#25200;&#21160;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21019;&#24314;&#20102;&#20998;&#21035;&#25429;&#25417;&#19981;&#21464;&#21644;&#31561;&#21464;&#20989;&#25968;&#31867;&#30340;&#23545;&#31435;&#20551;&#35774;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#39640;&#25928;&#22320;&#32452;&#21512;&#36866;&#24403;&#30340;&#20551;&#35774;&#26469;&#23436;&#25104;&#32473;&#23450;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;MSE&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#30456;&#20114;&#30683;&#30462;&#30340;&#20551;&#35774;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles (DE) have been successful in improving model performance by learning diverse members via the stochasticity of random initialization. While recent works have attempted to promote further diversity in DE via hyperparameters or regularizing loss functions, these methods primarily still rely on a stochastic approach to explore the hypothesis space. In this work, we present Multi-Symmetry Ensembles (MSE), a framework for constructing diverse ensembles by capturing the multiplicity of hypotheses along symmetry axes, which explore the hypothesis space beyond stochastic perturbations of model weights and hyperparameters. We leverage recent advances in contrastive representation learning to create models that separately capture opposing hypotheses of invariant and equivariant functional classes and present a simple ensembling approach to efficiently combine appropriate hypotheses for a given task. We show that MSE effectively captures the multiplicity of conflicting hypotheses th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#20132;&#38598;-&#35825;&#23548;&#22270;&#20256;&#36882;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#31232;&#30095;&#22270;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#38382;&#39064;&#12290;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20132;&#38598;&#23376;&#22270;&#65292;&#22312;&#28304;&#22270;&#19978;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#30693;&#35782;&#20256;&#36882;&#21040;&#30446;&#26631;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2302.14189</link><description>&lt;p&gt;
&#20320;&#21482;&#36716;&#31227;&#20320;&#20998;&#20139;&#30340;&#20869;&#23481;&#65306;&#22522;&#20110;&#20132;&#38598;&#30340;&#22270;&#20256;&#36882;&#23398;&#20064;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
You Only Transfer What You Share: Intersection-Induced Graph Transfer Learning for Link Prediction. (arXiv:2302.14189v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#20132;&#38598;-&#35825;&#23548;&#22270;&#20256;&#36882;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#31232;&#30095;&#22270;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#38382;&#39064;&#12290;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20132;&#38598;&#23376;&#22270;&#65292;&#22312;&#28304;&#22270;&#19978;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#30693;&#35782;&#20256;&#36882;&#21040;&#30446;&#26631;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#26680;&#24515;&#65292;&#20294;&#24403;&#30446;&#26631;&#22270;&#38750;&#24120;&#31232;&#30095;&#26102;&#65292;&#39044;&#27979;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#31232;&#30095;&#22270;&#36896;&#25104;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20043;&#21069;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#65306;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#22270;&#24418;&#21487;&#33021;&#20250;&#26377;&#19968;&#20010;&#23494;&#38598;&#30456;&#36830;&#12289;&#20114;&#34917;&#30340;&#22270;&#24418;&#12290;&#36825;&#20010;&#23494;&#38598;&#30340;&#22270;&#24418;&#21487;&#33021;&#20250;&#19982;&#21407;&#22987;&#22270;&#24418;&#20849;&#20139;&#33410;&#28857;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23558;&#26377;&#24847;&#20041;&#30340;&#36873;&#25321;&#24615;&#30693;&#35782;&#36716;&#31227;&#30340;&#33258;&#28982;&#26725;&#26753;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#22522;&#20110;&#20132;&#38598;&#30340;&#22270;&#20256;&#36882;&#23398;&#20064;(GITL)&#65292;&#23427;&#21463;&#21040;&#20102;&#30005;&#23376;&#21830;&#21153;&#25110;&#23398;&#26415;&#21512;&#33879;&#39044;&#27979;&#31561;&#23454;&#38469;&#24212;&#29992;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#32467;&#26500;&#24615;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20004;&#20010;&#22270;&#20043;&#38388;&#20849;&#20139;&#30340;&#33410;&#28857;&#21019;&#24314;&#19968;&#20010;&#20132;&#38598;&#23376;&#22270;&#65292;&#28982;&#21518;&#23558;&#26469;&#33258;&#28304;&#20016;&#23500;&#30340;&#20132;&#38598;&#23376;&#22270;&#30340;&#30693;&#35782;&#20256;&#36882;&#21040;&#23436;&#25972;&#30340;&#30446;&#26631;&#22270;&#19978;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#25913;&#36827;&#30340;&#26631;&#31614;&#20256;&#25773;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#22810;&#23618;&#27425;&#30340;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction is central to many real-world applications, but its performance may be hampered when the graph of interest is sparse. To alleviate issues caused by sparsity, we investigate a previously overlooked phenomenon: in many cases, a densely connected, complementary graph can be found for the original graph. The denser graph may share nodes with the original graph, which offers a natural bridge for transferring selective, meaningful knowledge. We identify this setting as Graph Intersection-induced Transfer Learning (GITL), which is motivated by practical applications in e-commerce or academic co-authorship predictions. We develop a framework to effectively leverage the structural prior in this setting. We first create an intersection subgraph using the shared nodes between the two graphs, then transfer knowledge from the source-enriched intersection subgraph to the full target graph. In the second step, we consider two approaches: a modified label propagation, and a multi-layer
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#36523;&#33976;&#39311;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#25945;&#24072;&#21487;&#20197;&#20351;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24182;&#20135;&#29983;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#20294;&#35201;&#27880;&#24847;&#34920;&#24449;&#26159;&#25968;&#25454;&#30456;&#20851;&#30340;&#12290;</title><link>http://arxiv.org/abs/2302.12091</link><description>&lt;p&gt;
&#38543;&#26426;&#25945;&#24072;&#26159;&#22909;&#30340;&#25945;&#24072;
&lt;/p&gt;
&lt;p&gt;
Random Teachers are Good Teachers. (arXiv:2302.12091v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12091
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#36523;&#33976;&#39311;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#25945;&#24072;&#21487;&#20197;&#20351;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24182;&#20135;&#29983;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#20294;&#35201;&#27880;&#24847;&#34920;&#24449;&#26159;&#25968;&#25454;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#36523;&#33976;&#39311;&#20013;&#30001;&#24072;&#29983;&#23398;&#20064;&#21160;&#24577;&#24341;&#21457;&#30340;&#38544;&#24335;&#35268;&#21017;&#21270;&#12290;&#20026;&#20102;&#38548;&#31163;&#20854;&#24433;&#21709;&#65292;&#20316;&#32773;&#36827;&#34892;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23454;&#39564;&#65292;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#25945;&#24072;&#20195;&#26367;&#20102;&#35757;&#32451;&#26377;&#32032;&#30340;&#25945;&#24072;&#12290;&#24778;&#22855;&#22320;&#21457;&#29616;&#65292;&#24403;&#23558;&#23398;&#29983;&#33976;&#39311;&#21040;&#36825;&#26679;&#19968;&#20301;&#38543;&#26426;&#25945;&#24072;&#26102;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#21450;&#20854;&#34920;&#24449;&#24050;&#32463;&#25317;&#26377;&#38750;&#24120;&#26377;&#36259;&#30340;&#29305;&#28857;&#65307;(1)&#22312;&#25506;&#31350;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#19982;&#20854;&#25945;&#24072;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#33976;&#39311;&#20986;&#30340;&#23398;&#29983;&#26126;&#26174;&#25552;&#39640;&#12290;(2)&#25152;&#23398;&#20064;&#30340;&#34920;&#24449;&#26159;&#25968;&#25454;&#30456;&#20851;&#30340;&#65292;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#20043;&#38388;&#26159;&#21487;&#20256;&#36882;&#30340;&#65292;&#20294;&#22914;&#26524;&#22312;&#38543;&#26426;&#36755;&#20837;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20250;&#20005;&#37325;&#24694;&#21270;&#12290;(3)&#23398;&#29983;&#26816;&#26597;&#28857;&#21547;&#26377;&#31232;&#30095;&#23376;&#32593;&#32476;&#65292;&#31216;&#20026;&#25277;&#22870;&#31080;&#65292;&#20301;&#20110;&#21463;&#30417;&#30563;&#30340;&#25439;&#22833;&#26223;&#35266;&#30340;&#32447;&#24615;&#30406;&#22320;&#36793;&#32536;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20960;&#20010;&#37325;&#35201;&#39046;&#22495;&#26377;&#30528;&#26377;&#36259;&#30340;&#24433;&#21709;&#65306;(1)&#33258;&#36523;&#33976;&#39311;&#21487;&#20197;&#20165;&#22522;&#20110;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#25945;&#24072;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the implicit regularization induced by teacher-student learning dynamics in self-distillation. To isolate its effect, we describe a simple experiment where we consider teachers at random initialization instead of trained teachers. Surprisingly, when distilling a student into such a random teacher, we observe that the resulting model and its representations already possess very interesting characteristics; (1) we observe a strong improvement of the distilled student over its teacher in terms of probing accuracy. (2) The learned representations are data-dependent and transferable between different tasks but deteriorate strongly if trained on random inputs. (3) The student checkpoint contains sparse subnetworks, so-called lottery tickets, and lies on the border of linear basins in the supervised loss landscape. These observations have interesting consequences for several important areas in machine learning: (1) Self-distillation can work solely based on the im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DPM-SNC&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#21644;&#27969;&#24418;&#32422;&#26463;&#30340;&#37319;&#26679;&#26041;&#27861;&#23454;&#29616;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#32467;&#26500;&#21270;&#33410;&#28857;&#20998;&#31867;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#26469;&#24212;&#29992;DPMs&#65292;&#26368;&#22823;&#21270;&#19968;&#20010;&#26032;&#30340;&#21464;&#20998;&#19979;&#30028;&#12290;&#23454;&#39564;&#35777;&#26126;DPMs&#21487;&#20197;&#25552;&#39640;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.10506</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#33410;&#28857;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diffusion Probabilistic Models for Structured Node Classification. (arXiv:2302.10506v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DPM-SNC&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#21644;&#27969;&#24418;&#32422;&#26463;&#30340;&#37319;&#26679;&#26041;&#27861;&#23454;&#29616;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#32467;&#26500;&#21270;&#33410;&#28857;&#20998;&#31867;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#26469;&#24212;&#29992;DPMs&#65292;&#26368;&#22823;&#21270;&#19968;&#20010;&#26032;&#30340;&#21464;&#20998;&#19979;&#30028;&#12290;&#23454;&#39564;&#35777;&#26126;DPMs&#21487;&#20197;&#25552;&#39640;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#32467;&#26500;&#21270;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#33410;&#28857;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#20381;&#36182;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#38598;&#20013;&#30740;&#31350;&#20102;&#37096;&#20998;&#26631;&#35760;&#22270;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#38656;&#35201;&#23558;&#24050;&#30693;&#26631;&#35760;&#30340;&#20449;&#24687;&#32435;&#20837;&#26410;&#30693;&#26631;&#31614;&#30340;&#39044;&#27979;&#20013;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#32467;&#26500;&#21270;&#33410;&#28857;&#20998;&#31867;(DPM-SNC)&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;DPM-SNC&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#21363;(a)&#20351;&#29992;&#20855;&#26377;&#34920;&#36798;&#24615;&#30340;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#23398;&#20064;&#26631;&#31614;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;(b)&#21033;&#29992;&#27969;&#24418;&#32422;&#26463;&#30340;&#37319;&#26679;&#26041;&#27861;&#22312;&#24050;&#30693;&#26631;&#31614;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#30001;&#20110;DPMs&#32570;&#20047;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#22240;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#24212;&#29992;DPMs&#65292;&#26368;&#22823;&#21270;&#19968;&#20010;&#26032;&#30340;&#21464;&#20998;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#25552;&#39640;DPMs&#23545;&#33410;&#28857;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies structured node classification on graphs, where the predictions should consider dependencies between the node labels. In particular, we focus on solving the problem for partially labeled graphs where it is essential to incorporate the information in the known label for predicting the unknown labels. To address this issue, we propose a novel framework leveraging the diffusion probabilistic model for structured node classification (DPM-SNC). At the heart of our framework is the extraordinary capability of DPM-SNC to (a) learn a joint distribution over the labels with an expressive reverse diffusion process and (b) make predictions conditioned on the known labels utilizing manifold-constrained sampling. Since the DPMs lack training algorithms for partially labeled data, we design a novel training algorithm to apply DPMs, maximizing a new variational lower bound. We also theoretically analyze how DPMs benefit node classification by enhancing the expressive power of GNNs 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31526;&#21512;&#24050;&#30693;&#30005;&#27969;&#29289;&#29702;&#29305;&#24615;&#30340;&#27169;&#22411;&#65292;&#22312;&#36890;&#36807;Helmholtz&#20998;&#35299;&#33719;&#24471;&#30340;&#21521;&#37327;&#22330;&#30340;&#21457;&#25955;&#21644;&#26080;&#26059;&#20998;&#37327;&#19978;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#39044;&#27979;&#28023;&#27969;&#12290;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#28014;&#26631;&#25968;&#25454;&#26041;&#38754;&#37117;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.10364</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22312;&#36203;&#36203;&#23572;&#22982;&#38669;&#20857;&#20998;&#35299;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#26356;&#27969;&#20307;&#30340;&#28023;&#27915;&#27668;&#27969;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes at the Helm(holtz): A more fluid model for ocean currents. (arXiv:2302.10364v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31526;&#21512;&#24050;&#30693;&#30005;&#27969;&#29289;&#29702;&#29305;&#24615;&#30340;&#27169;&#22411;&#65292;&#22312;&#36890;&#36807;Helmholtz&#20998;&#35299;&#33719;&#24471;&#30340;&#21521;&#37327;&#22330;&#30340;&#21457;&#25955;&#21644;&#26080;&#26059;&#20998;&#37327;&#19978;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#39044;&#27979;&#28023;&#27969;&#12290;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#28014;&#26631;&#25968;&#25454;&#26041;&#38754;&#37117;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#23398;&#23478;&#26377;&#20852;&#36259;&#39044;&#27979;&#28023;&#27969;&#21644;&#22522;&#20110;&#28014;&#26631;&#36895;&#24230;&#30340;&#31232;&#30095;&#35266;&#27979;&#25968;&#25454;&#26469;&#35782;&#21035;&#24403;&#21069;&#30690;&#37327;&#22330;&#20013;&#30340;&#21457;&#25955;&#24615;&#12290;&#39640;&#26031;&#36807;&#31243;(GPs)&#22312;&#31354;&#38388;&#20301;&#32622;&#19978;&#20805;&#24403;&#36830;&#32493;&#20294;&#39640;&#24230;&#38750;&#32447;&#24615;&#21151;&#33021;&#30340;&#36895;&#24230;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#27169;&#22411;&#12290;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;&#20855;&#26377;&#26631;&#20934;&#24179;&#31283;&#26680;&#30340;GP&#30452;&#25509;&#24212;&#29992;&#20110;&#28014;&#26631;&#25968;&#25454;&#21487;&#33021;&#22312;&#24403;&#21069;&#39044;&#27979;&#21644;&#21457;&#25955;&#24615;&#35782;&#21035;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#8212;&#30001;&#20110;&#19968;&#20123;&#29289;&#29702;&#19978;&#19981;&#20999;&#23454;&#38469;&#30340;&#20808;&#39564;&#20551;&#35774;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21453;&#26144;&#24050;&#30693;&#30340;&#30005;&#27969;&#29289;&#29702;&#29305;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26631;&#20934;&#24179;&#31283;&#26680;&#25918;&#22312;&#36890;&#36807;Helmholtz&#20998;&#35299;&#33719;&#24471;&#30340;&#21521;&#37327;&#22330;&#30340;&#21457;&#25955;&#21644;&#26080;&#26059;&#20998;&#37327;&#19978;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30001;&#20110;&#35813;&#20998;&#35299;&#20165;&#36890;&#36807;&#28151;&#21512;&#20559;&#23548;&#25968;&#19982;&#21407;&#22987;&#21521;&#37327;&#22330;&#30456;&#20851;&#65292;&#22240;&#27492;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#22312;&#21407;&#22987;&#25968;&#25454;&#32473;&#23450;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#39069;&#22806;&#36827;&#34892;&#23569;&#25968;&#35745;&#31639;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#28014;&#26631;&#25968;&#25454;&#35777;&#26126;&#20102;&#36825;&#31181;&#34746;&#26059;GP&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#22343;&#26041;&#39044;&#27979;&#35823;&#24046;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oceanographers are interested in predicting ocean currents and identifying divergences in a current vector field based on sparse observations of buoy velocities. Since we expect current velocity to be a continuous but highly non-linear function of spatial location, Gaussian processes (GPs) offer an attractive model. But we show that applying a GP with a standard stationary kernel directly to buoy data can struggle at both current prediction and divergence identification -- due to some physically unrealistic prior assumptions. To better reflect known physical properties of currents, we propose to instead put a standard stationary kernel on the divergence and curl-free components of a vector field obtained through a Helmholtz decomposition. We show that, because this decomposition relates to the original vector field just via mixed partial derivatives, we can still perform inference given the original data with only a small constant multiple of additional computational expense. We illust
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10289</link><description>&lt;p&gt;
&#23558;&#40657;&#21283;&#23376;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#65306;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#65292;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#35201;&#20040;&#20174;&#35299;&#37322;&#24615;&#27169;&#22411;&#24320;&#22987;&#65292;&#35201;&#20040;&#20174;&#40657;&#30418;&#24320;&#22987;&#24182;&#20107;&#21518;&#35299;&#37322;&#12290;&#40657;&#30418;&#27169;&#22411;&#28789;&#27963;&#20294;&#38590;&#20197;&#35299;&#37322;&#65292;&#32780;&#35299;&#37322;&#24615;&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24615;&#27169;&#22411;&#38656;&#35201;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#65292;&#24182;&#19988;&#24448;&#24448;&#27604;&#23427;&#20204;&#30340;&#40657;&#30418;&#21464;&#20307;&#19981;&#22815;&#28789;&#27963;&#21644;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#26088;&#22312;&#27169;&#31946;&#40657;&#30418;&#30340;&#20107;&#21518;&#35299;&#37322;&#21644;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20174;&#40657;&#30418;&#24320;&#22987;&#65292;&#36845;&#20195;&#22320;Carve&#20986;&#19968;&#31181;&#28151;&#21512;&#35299;&#37322;&#27169;&#22411;&#65288;MoIE&#65289;&#21644;&#19968;&#20010;&#27531;&#20313;&#32593;&#32476;&#12290;&#27599;&#20010;&#21487;&#35299;&#37322;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;(FOL)&#23545;&#20854;&#36827;&#34892;&#35299;&#37322;&#65292;&#20174;&#40657;&#30418;&#20013;&#25552;&#20379;&#22522;&#26412;&#25512;&#29702;&#27010;&#24565;&#12290;&#25105;&#20204;&#36890;&#36807;&#28789;&#27963;&#30340;&#27531;&#24046;&#36335;&#30001;&#20854;&#20313;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#27531;&#36716;&#32593;&#32476;&#19978;&#37325;&#22797;&#35813;&#26041;&#27861;&#65292;&#30452;&#21040;&#25152;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#35299;&#37322;&#25152;&#38656;&#27604;&#20363;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#21644;&#37325;&#22797;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#20960;&#31181;&#40657;&#21283;&#23376;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;3DIEBench&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#31561;&#21464;&#34920;&#36798;&#24335;&#39044;&#27979;&#22120;SIE&#65292;&#32467;&#21512;&#20998;&#35010;&#30340;&#19981;&#21464;&#19982;&#31561;&#21464;&#34920;&#36798;&#24335;&#20197;&#33719;&#24471;&#26356;&#21152;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2302.10283</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#21106;&#19981;&#21464;&#31561;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning of Split Invariant Equivariant representations. (arXiv:2302.10283v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;3DIEBench&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#31561;&#21464;&#34920;&#36798;&#24335;&#39044;&#27979;&#22120;SIE&#65292;&#32467;&#21512;&#20998;&#35010;&#30340;&#19981;&#21464;&#19982;&#31561;&#21464;&#34920;&#36798;&#24335;&#20197;&#33719;&#24471;&#26356;&#21152;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#23398;&#20064;&#19981;&#21464;&#25110;&#31561;&#21464;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#23613;&#31649;&#19981;&#21464;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#31561;&#21464;&#26041;&#27861;&#22312;&#26356;&#23567;&#12289;&#26356;&#21487;&#25511;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#20415;&#23398;&#20064;&#26356;&#21152;&#22810;&#26679;&#21270;&#12289;&#36866;&#29992;&#20110;&#24191;&#27867;&#20219;&#21153;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#19968;&#20010;&#31216;&#20026;3DIEBench&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;55&#20010;&#31867;&#21035;&#30340;3D&#27169;&#22411;&#28210;&#26579;&#22270;&#20687;&#36229;&#36807;250&#19975;&#24352;&#65292;&#25105;&#20204;&#21487;&#20197;&#23436;&#20840;&#25511;&#21046;&#24212;&#29992;&#20110;&#23545;&#35937;&#30340;&#21464;&#25442;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#39044;&#27979;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#23398;&#20064;&#31561;&#21464;&#34920;&#31034;&#65292;&#20174;&#32780;&#19981;&#21487;&#33021;&#20986;&#29616;&#19981;&#21464;&#24615;&#23849;&#22604;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SIE&#65288;&#20998;&#35010;&#19981;&#21464;-&#31561;&#21464;&#65289;&#30340;&#27010;&#24565;&#65292;&#23427;&#23558;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#39044;&#27979;&#22120;&#19982;&#20998;&#35010;&#20026;&#20004;&#37096;&#20998;&#65288;&#19968;&#37096;&#20998;&#20026;&#19981;&#21464;&#24418;&#65292;&#21478;&#19968;&#37096;&#20998;&#20026;&#31561;&#21464;&#24418;&#65289;&#30340;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress has been made towards learning invariant or equivariant representations with self-supervised learning. While invariant methods are evaluated on large scale datasets, equivariant ones are evaluated in smaller, more controlled, settings. We aim at bridging the gap between the two in order to learn more diverse representations that are suitable for a wide range of tasks. We start by introducing a dataset called 3DIEBench, consisting of renderings from 3D models over 55 classes and more than 2.5 million images where we have full control on the transformations applied to the objects. We further introduce a predictor architecture based on hypernetworks to learn equivariant representations with no possible collapse to invariance. We introduce SIE (Split Invariant-Equivariant) which combines the hypernetwork-based predictor with representations split in two parts, one invariant, the other equivariant, to learn richer representations. We demonstrate significant performance gains
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; JANA &#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#36817;&#20284;&#35745;&#31639;&#12290;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#20998;&#25674;&#30340;&#36817;&#20284;&#21518;&#39564;&#21644;&#20284;&#28982;&#65292;&#20026;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36884;&#24452;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#31181;&#27169;&#25311;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#26657;&#20934;&#35786;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09125</link><description>&lt;p&gt;
JANA&#65306;&#22797;&#26434;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#32852;&#21512;&#20998;&#25674;&#36817;&#20284;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models. (arXiv:2302.09125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; JANA &#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#36817;&#20284;&#35745;&#31639;&#12290;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#20998;&#25674;&#30340;&#36817;&#20284;&#21518;&#39564;&#21644;&#20284;&#28982;&#65292;&#20026;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36884;&#24452;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#31181;&#27169;&#25311;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#26657;&#20934;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#32852;&#21512;&#20998;&#25674;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#8221;&#65288;JANA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#36125;&#21494;&#26031;&#20195;&#29702;&#24314;&#27169;&#21644;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#29702;&#20013;&#20986;&#29616;&#30340;&#38590;&#20197;&#35745;&#31639;&#30340;&#20284;&#28982;&#20989;&#25968;&#21644;&#21518;&#39564;&#23494;&#24230;&#12290;&#25105;&#20204;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#35757;&#32451;&#19977;&#20010;&#30456;&#20114;&#34917;&#20805;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;1&#65289;&#19968;&#20010;&#24635;&#32467;&#32593;&#32476;&#65292;&#23558;&#20010;&#21035;&#25968;&#25454;&#28857;&#12289;&#38598;&#21512;&#25110;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#25104;&#20449;&#24687;&#23884;&#20837;&#21521;&#37327;&#65307;2&#65289;&#19968;&#20010;&#21518;&#39564;&#32593;&#32476;&#65292;&#23398;&#20064;&#20998;&#25674;&#30340;&#36817;&#20284;&#21518;&#39564;&#65307;3&#65289;&#19968;&#20010;&#20284;&#28982;&#32593;&#32476;&#65292;&#23398;&#20064;&#20998;&#25674;&#30340;&#36817;&#20284;&#20284;&#28982;&#12290;&#23427;&#20204;&#30340;&#20132;&#20114;&#20026;&#20998;&#25674;&#36793;&#32536;&#20284;&#28982;&#21644;&#21518;&#39564;&#39044;&#27979;&#20272;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#36825;&#26159;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#30340;&#20004;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24120;&#24120;&#23545;&#20110;&#26631;&#20934;&#26041;&#27861;&#26469;&#35828;&#22826;&#26114;&#36149;&#20102;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#25311;&#27169;&#22411;&#20013;&#23545;JANA&#30340;&#20445;&#30495;&#24230;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#21487;&#35299;&#37322;&#30340;&#32852;&#21512;&#26657;&#20934;&#35786;&#26029;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24490;&#29615;&#20284;&#28982;&#32593;&#32476;&#27169;&#25311;&#22797;&#26434;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes ''jointly amortized neural approximation'' (JANA) of intractable likelihood functions and posterior densities arising in Bayesian surrogate modeling and simulation-based inference. We train three complementary networks in an end-to-end fashion: 1) a summary network to compress individual data points, sets, or time series into informative embedding vectors; 2) a posterior network to learn an amortized approximate posterior; and 3) a likelihood network to learn an amortized approximate likelihood. Their interaction opens a new route to amortized marginal likelihood and posterior predictive estimation -- two important ingredients of Bayesian workflows that are often too expensive for standard methods. We benchmark the fidelity of JANA on a variety of simulation models against state-of-the-art Bayesian methods and propose a powerful and interpretable diagnostic for joint calibration. In addition, we investigate the ability of recurrent likelihood networks to emulate comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#39304;&#22270;&#30340;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#32531;&#35299;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#21487;&#34892;&#24615;&#21644;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.08631</link><description>&lt;p&gt;
&#21453;&#39304;&#22270;&#19978;&#30340;&#23454;&#29992;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Practical Contextual Bandits with Feedback Graphs. (arXiv:2302.08631v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#39304;&#22270;&#30340;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#32531;&#35299;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#21487;&#34892;&#24615;&#21644;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24773;&#22659;&#36172;&#21338;&#24050;&#32463;&#26377;&#25104;&#29087;&#30340;&#29702;&#35770;&#65292;&#20294;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#19981;&#21516;&#30340;&#21453;&#39304;&#27169;&#24335;&#26469;&#21152;&#36895;&#23398;&#20064;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#21453;&#39304;&#22270;&#19978;&#30340;&#36172;&#21338;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#26469;&#32531;&#35299;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#22522;&#20110;&#22238;&#24402;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#24050;&#30693;&#30340;&#26497;&#23567;&#26497;&#20540;&#65292;&#22240;&#27492;&#20943;&#23569;&#20102;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While contextual bandit has a mature theory, effectively leveraging different feedback patterns to enhance the pace of learning remains unclear. Bandits with feedback graphs, which interpolates between the full information and bandit regimes, provides a promising framework to mitigate the statistical complexity of learning. In this paper, we propose and analyze an approach to contextual bandits with feedback graphs based upon reduction to regression. The resulting algorithms are computationally practical and achieve established minimax rates, thereby reducing the statistical complexity in real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#25509;&#21475;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#29992;&#25143;&#25351;&#23450;&#30340;&#20998;&#24067;&#20559;&#31227;&#26469;&#36820;&#22238;&#38024;&#23545;&#35813;&#20998;&#24067;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#21453;&#36716;&#30340;&#25968;&#25454;&#38598;&#25509;&#21475;&#23454;&#29616;&#65292;&#20351;&#24471;&#29983;&#25104;&#26356;&#21152;&#36866;&#24212;&#36755;&#20837;&#20998;&#24067;&#65292;&#36827;&#32780;&#24110;&#21161;&#30740;&#31350;&#27169;&#22411;&#22312;&#21508;&#31181;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2302.07865</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#25509;&#21475;&#65306;&#20351;&#29992;&#21487;&#25511;&#23545;&#25239;&#29983;&#25104;&#26469;&#35786;&#26029;&#27169;&#22411;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Dataset Interfaces: Diagnosing Model Failures Using Controllable Counterfactual Generation. (arXiv:2302.07865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#25509;&#21475;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#29992;&#25143;&#25351;&#23450;&#30340;&#20998;&#24067;&#20559;&#31227;&#26469;&#36820;&#22238;&#38024;&#23545;&#35813;&#20998;&#24067;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#21453;&#36716;&#30340;&#25968;&#25454;&#38598;&#25509;&#21475;&#23454;&#29616;&#65292;&#20351;&#24471;&#29983;&#25104;&#26356;&#21152;&#36866;&#24212;&#36755;&#20837;&#20998;&#24067;&#65292;&#36827;&#32780;&#24110;&#21161;&#30740;&#31350;&#27169;&#22411;&#22312;&#21508;&#31181;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22833;&#36133;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#21487;&#38752;&#24615;&#21487;&#33021;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22240;&#20026;&#21487;&#33021;&#24456;&#38590;&#33719;&#21462;&#34920;&#29616;&#20986;&#25351;&#23450;&#20559;&#31227;&#30340;&#21453;&#20107;&#23454;&#31034;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#25509;&#21475;&#30340;&#27010;&#24565;&#65306;&#19968;&#20010;&#26694;&#26550;&#65292;&#32473;&#23450;&#36755;&#20837;&#25968;&#25454;&#38598;&#21644;&#29992;&#25143;&#25351;&#23450;&#30340;&#20559;&#31227;&#65292;&#36820;&#22238;&#26469;&#33258;&#35813;&#36755;&#20837;&#20998;&#24067;&#30340;&#20855;&#26377;&#25152;&#38656;&#20559;&#31227;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35768;&#22810;&#33258;&#28982;&#30340;&#23454;&#29616;&#26041;&#24335;&#65292;&#21457;&#29616;&#23427;&#20204;&#32463;&#24120;&#24341;&#20837;&#28151;&#28102;&#20559;&#31227;&#26469;&#20351;&#27169;&#22411;&#35780;&#20272;&#22797;&#26434;&#21270;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#25509;&#21475;&#23454;&#29616;&#65292;&#21033;&#29992;&#25991;&#26412;&#21453;&#36716;&#26469;&#33258;&#23450;&#20041;&#29983;&#25104;&#22120;&#20197;&#36866;&#24212;&#36755;&#20837;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20010;&#25968;&#25454;&#38598;&#25509;&#21475;&#24212;&#29992;&#20110;ImageNet&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;&#27169;&#22411;&#22312;&#22810;&#26679;&#30340;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#32972;&#26223;&#12289;&#20809;&#29031;&#21644;&#23646;&#24615;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shift is a major source of failure for machine learning models. However, evaluating model reliability under distribution shift can be challenging, especially since it may be difficult to acquire counterfactual examples that exhibit a specified shift. In this work, we introduce the notion of a dataset interface: a framework that, given an input dataset and a user-specified shift, returns instances from that input distribution that exhibit the desired shift. We study a number of natural implementations for such an interface, and find that they often introduce confounding shifts that complicate model evaluation. Motivated by this, we propose a dataset interface implementation that leverages Textual Inversion to tailor generation to the input distribution. We then demonstrate how applying this dataset interface to the ImageNet dataset enables studying model behavior across a diverse array of distribution shifts, including variations in background, lighting, and attributes of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21521;&#37327;&#37327;&#21270;Wasserstein&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#36890;&#36807;&#26368;&#23567;&#21270;WS&#36317;&#31163;&#23558;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#20256;&#36882;&#21040;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#12289;&#21487;&#25511;&#30340;&#32858;&#31867;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.05917</link><description>&lt;p&gt;
&#21521;&#37327;&#37327;&#21270;Wasserstein&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Vector Quantized Wasserstein Auto-Encoder. (arXiv:2302.05917v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21521;&#37327;&#37327;&#21270;Wasserstein&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#36890;&#36807;&#26368;&#23567;&#21270;WS&#36317;&#31163;&#23558;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#20256;&#36882;&#21040;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#12289;&#21487;&#25511;&#30340;&#32858;&#31867;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#28145;&#24230;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#31526;&#21495;&#21644;&#24635;&#32467;&#30340;&#25277;&#35937;&#65292;&#26356;&#26377;&#29992;&#20110;&#21518;&#32493;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#21463;&#21040; Vector Quantized Variational Auto-Encoder (VQ-VAE) &#30340;&#21551;&#21457;&#65292;&#23398;&#20064;&#28145;&#24230;&#31163;&#25955;&#34920;&#31034;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#21407;&#22987;&#30340;VQ-VAE&#24418;&#24335;&#65292;&#27809;&#26377;&#19968;&#20010;&#35752;&#35770;&#20174;&#29983;&#25104;&#30340;&#35282;&#24230;&#23398;&#20064;&#28145;&#24230;&#31163;&#25955;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#29983;&#25104;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#23398;&#20064;&#28145;&#24230;&#31163;&#25955;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36171;&#20104;&#31163;&#25955;&#20998;&#24067;&#22312;&#30721;&#23383;&#24207;&#21015;&#19978;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#30830;&#23450;&#24615;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;WS&#36317;&#31163;&#23558;&#30721;&#23383;&#24207;&#21015;&#30340;&#20998;&#24067;&#20256;&#36882;&#21040;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#29702;&#35770;&#65292;&#23558;&#20854;&#19982;WS&#36317;&#31163;&#30340;&#32858;&#31867;&#35270;&#22270;&#36830;&#25509;&#36215;&#26469;&#65292;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#26377;&#26356;&#22909;&#30340;&#12289;&#21487;&#25511;&#30340;&#32858;&#31867;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#24120;&#35265;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning deep discrete latent presentations offers a promise of better symbolic and summarized abstractions that are more useful to subsequent downstream tasks. Inspired by the seminal Vector Quantized Variational Auto-Encoder (VQ-VAE), most of work in learning deep discrete representations has mainly focused on improving the original VQ-VAE form and none of them has studied learning deep discrete representations from the generative viewpoint. In this work, we study learning deep discrete representations from the generative viewpoint. Specifically, we endow discrete distributions over sequences of codewords and learn a deterministic decoder that transports the distribution over the sequences of codewords to the data distribution via minimizing a WS distance between them. We develop further theories to connect it with the clustering viewpoint of WS distance, allowing us to have a better and more controllable clustering solution. Finally, we empirically evaluate our method on several wel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CEIL&#65288;Compositional Exemplars for In-context Learning&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20915;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#27169;&#22411;&#22788;&#29702;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05698</link><description>&lt;p&gt;
&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32452;&#21512;&#33539;&#20363;
&lt;/p&gt;
&lt;p&gt;
Compositional Exemplars for In-context Learning. (arXiv:2302.05698v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05698
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CEIL&#65288;Compositional Exemplars for In-context Learning&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20915;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#27169;&#22411;&#22788;&#29702;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#20013;&#27169;&#22411;&#36890;&#36807;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#20316;&#20026;&#28436;&#31034;&#65292;&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25191;&#34892;&#30475;&#19981;&#35265;&#30340;&#20219;&#21153;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#39640;&#24230;&#21463;&#21040;&#25152;&#36873;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36136;&#37327;&#25152;&#25903;&#37197;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#36873;&#25321;&#26041;&#27861;&#22522;&#26412;&#19978;&#26159;&#22522;&#20110;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#65292;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#24418;&#24335;&#21270;&#20026;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;CEIL&#65288;Compositional Exemplars for In-context Learning&#65289;&#65292;&#23427;&#36890;&#36807;&#20915;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#23545;&#25152;&#32473;&#36755;&#20837;&#21644;&#19978;&#19979;&#25991;&#31034;&#20363;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26469;&#33258;LM&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;7&#20010;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;12&#20010;&#20998;&#31867;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;CEIL&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#37322;&#20041;&#26816;&#27979;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained language models (LMs) have shown impressive In-Context Learning (ICL) ability, where the model learns to do an unseen task via a prompt consisting of input-output examples as the demonstration, without any parameter updates. The performance of ICL is highly dominated by the quality of the selected in-context examples. However, previous selection methods are mostly based on simple heuristics, leading to sub-optimal performance. In this work, we formulate in-context example selection as a subset selection problem. We propose CEIL (Compositional Exemplars for In-context Learning), which is instantiated by Determinantal Point Processes (DPPs) to model the interaction between the given input and in-context examples, and optimized through a carefully-designed contrastive learning objective to obtain preference from LMs. We validate CEIL on 12 classification and generation datasets from 7 distinct NLP tasks, including sentiment analysis, paraphrase detection, natural language
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;HyperSound&#8221;&#65292;&#21487;&#23558;&#36229;&#32593;&#32476;&#32467;&#26500;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#65292;&#20174;&#32780;&#29983;&#25104;&#38899;&#39057;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#30340;&#22788;&#29702;&#65292;&#24182;&#19988;&#37325;&#26500;&#36136;&#37327;&#21487;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#26159;&#24403;&#20195;&#38899;&#39057;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#26377;&#28508;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.04959</link><description>&lt;p&gt;
&#36229;&#32593;&#32476;&#26500;&#24314;&#38899;&#39057;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Hypernetworks build Implicit Neural Representations of Sounds. (arXiv:2302.04959v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04959
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;HyperSound&#8221;&#65292;&#21487;&#23558;&#36229;&#32593;&#32476;&#32467;&#26500;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#65292;&#20174;&#32780;&#29983;&#25104;&#38899;&#39057;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#30340;&#22788;&#29702;&#65292;&#24182;&#19988;&#37325;&#26500;&#36136;&#37327;&#21487;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#26159;&#24403;&#20195;&#38899;&#39057;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#26377;&#28508;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#29616;&#22312;&#34987;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#20013;&#26469;&#20195;&#34920;&#22810;&#23186;&#20307;&#20449;&#21495;&#65292;&#21253;&#25324;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#12289;&#22270;&#20687;&#21387;&#32553;&#25110;3D&#28210;&#26579;&#12290;&#29616;&#26377;&#30340;&#21033;&#29992;INR&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#35270;&#35273;&#25968;&#25454;&#19978;&#65292;&#22240;&#20026;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;INR&#27169;&#22411;&#30340;&#26550;&#26500;&#23646;&#24615;&#20013;&#23384;&#22312;&#24402;&#32435;&#20559;&#24046;&#65292;&#25152;&#20197;&#23558;&#20854;&#24212;&#29992;&#20110;&#20854;&#20182;&#27169;&#24577;&#65292;&#22914;&#38899;&#39057;&#65292;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36229;&#22768;&#65288;HyperSound&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#38899;&#39057;&#26679;&#26412;&#29983;&#25104;INR&#65292;&#20197;&#20415;&#33021;&#22815;&#22312;&#35757;&#32451;&#20013;&#35266;&#23519;&#21040;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21487;&#27604;&#36739;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#36136;&#37327;&#37325;&#26500;&#38899;&#39057;&#26679;&#26412;&#65292;&#24182;&#20026;&#29992;&#20110;&#38899;&#39057;&#22788;&#29702;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24403;&#20195;&#22768;&#38899;&#34920;&#31034;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22914;&#35889;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representations (INRs) are nowadays used to represent multimedia signals across various real-life applications, including image super-resolution, image compression, or 3D rendering. Existing methods that leverage INRs are predominantly focused on visual data, as their application to other modalities, such as audio, is nontrivial due to the inductive biases present in architectural attributes of image-based INR models. To address this limitation, we introduce HyperSound, the first meta-learning approach to produce INRs for audio samples that leverages hypernetworks to generalize beyond samples observed in training. Our approach reconstructs audio samples with quality comparable to other state-of-the-art models and provides a viable alternative to contemporary sound representations used in deep neural networks for audio processing, such as spectrograms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#29305;&#24449;&#25193;&#25955;&#30697;&#38453;&#30340;&#26368;&#22823;&#22855;&#24322;&#20540;&#26469;&#32553;&#25918;&#27867;&#21270;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;Hessians&#26469;&#34913;&#37327;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22122;&#22768;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#27867;&#21270;&#30028;&#38480;&#65292;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;&#23454;&#38469;&#22270;&#24418;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04451</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#65306;&#22522;&#20110;&#22270;&#25193;&#25955;&#30340;&#25913;&#36827;PAC-Bayesian&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion. (arXiv:2302.04451v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#29305;&#24449;&#25193;&#25955;&#30697;&#38453;&#30340;&#26368;&#22823;&#22855;&#24322;&#20540;&#26469;&#32553;&#25918;&#27867;&#21270;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;Hessians&#26469;&#34913;&#37327;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22122;&#22768;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#27867;&#21270;&#30028;&#38480;&#65292;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;&#23454;&#38469;&#22270;&#24418;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#22270;&#39044;&#27979;&#20219;&#21153;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#12290;&#30001;&#20854;&#23454;&#35777;&#34920;&#29616;&#25152;&#39537;&#21160;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#23427;&#20204;&#26681;&#25454;&#26368;&#22823;&#24230;&#25968;&#22312;&#22270;&#32467;&#26500;&#26041;&#38754;&#36827;&#34892;&#32553;&#25918;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27867;&#21270;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#26681;&#25454;&#22270;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#25193;&#25955;&#30697;&#38453;&#30340;&#26368;&#22823;&#22855;&#24322;&#20540;&#36827;&#34892;&#32553;&#25918;&#12290;&#23545;&#20110;&#23454;&#38469;&#22270;&#24418;&#65292;&#36825;&#20123;&#30028;&#38480;&#30340;&#25968;&#20540;&#35201;&#27604;&#20808;&#21069;&#30340;&#30028;&#38480;&#23567;&#24471;&#22810;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#30456;&#31526;&#30340;&#27867;&#21270;&#24046;&#36317;&#19979;&#38480;&#65292;&#20854;&#28176;&#36817;&#22320;&#21305;&#37197;&#20102;&#25105;&#20204;&#30340;&#19978;&#38480;&#30028;&#38480;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#20808;&#21069;&#30340;&#35774;&#32622;&#65288;&#21363;&#21367;&#31215;&#21644;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65289;&#21644;&#26032;&#30340;&#35774;&#32622;&#65288;&#21363;&#22270;&#21516;&#26500;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;Hessians&#26469;&#34913;&#37327;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#22122;&#22768;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;Hessian&#30340;&#27979;&#37327;&#19982;&#35266;&#23519;&#21040;&#30340;&#27867;&#21270;&#24046;&#36317;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are widely used tools for graph prediction tasks. Motivated by their empirical performance, prior works have developed generalization bounds for graph neural networks, which scale with graph structures in terms of the maximum degree. In this paper, we present generalization bounds that instead scale with the largest singular value of the graph neural network's feature diffusion matrix. These bounds are numerically much smaller than prior bounds for real-world graphs. We also construct a lower bound of the generalization gap that matches our upper bound asymptotically. To achieve these results, we analyze a unified model that includes prior works' settings (i.e., convolutional and message-passing networks) and new settings (i.e., graph isomorphism networks). Our key idea is to measure the stability of graph neural networks against noise perturbations using Hessians. Empirically, we find that Hessian-based measurements correlate with the observed generalization gaps
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24212;&#29992;&#65288;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65289;&#20197;&#21450;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.04062</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Synthetic Data Generation: A Review. (arXiv:2302.04062v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04062
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24212;&#29992;&#65288;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65289;&#20197;&#21450;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#23384;&#22312;&#22810;&#31181;&#38382;&#39064;&#65292;&#22914;&#25968;&#25454;&#36136;&#37327;&#20302;&#65292;&#26377;&#38480;&#30340;&#25968;&#25454;&#28857;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27424;&#25311;&#21512;&#65292;&#30001;&#20110;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#30417;&#31649;&#38382;&#39064;&#38590;&#20197;&#35775;&#38382;&#25968;&#25454;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20197;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#26080;&#27861;&#20570;&#21040;&#30340;&#26041;&#24335;&#36827;&#34892;&#20849;&#20139;&#21644;&#20351;&#29992;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#29616;&#26377;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#35752;&#35770;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24037;&#20316;&#65306;&#65288;i&#65289;&#24212;&#29992;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65307;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65307;&#65288;iii&#65289;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data plays a crucial role in machine learning. However, in real-world applications, there are several problems with data, e.g., data are of low quality; a limited number of data points lead to under-fitting of the machine learning model; it is hard to access the data due to privacy, safety and regulatory concerns. Synthetic data generation offers a promising new avenue, as it can be shared and used in ways that real-world data cannot. This paper systematically reviews the existing works that leverage machine learning models for synthetic data generation. Specifically, we discuss the synthetic data generation works from several perspectives: (i) applications, including computer vision, speech, natural language, healthcare, and business; (ii) machine learning methods, particularly neural network architectures and deep generative models; (iii) privacy and fairness issue. In addition, we identify the challenges and opportunities in this emerging field and suggest future research directions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Raincoat&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#23553;&#38381;&#38598;&#21644;&#36890;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;Raincoat&#36890;&#36807;&#36328;&#22495;&#23545;&#40784;&#26102;&#38388;&#21644;&#39057;&#29575;&#29305;&#24449;&#65292;&#20462;&#27491;&#20559;&#31227;&#20197;&#20415;&#20110;&#26816;&#27979;&#31169;&#26377;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#20849;&#20139;&#32467;&#26500;&#26469;&#25552;&#39640;&#21487;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.03133</link><description>&lt;p&gt;
&#38024;&#23545;&#29305;&#24449;&#19982;&#26631;&#31614;&#20559;&#31227;&#30340;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation for Time Series Under Feature and Label Shifts. (arXiv:2302.03133v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Raincoat&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#23553;&#38381;&#38598;&#21644;&#36890;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;Raincoat&#36890;&#36807;&#36328;&#22495;&#23545;&#40784;&#26102;&#38388;&#21644;&#39057;&#29575;&#29305;&#24449;&#65292;&#20462;&#27491;&#20559;&#31227;&#20197;&#20415;&#20110;&#26816;&#27979;&#31169;&#26377;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#20849;&#20139;&#32467;&#26500;&#26469;&#25552;&#39640;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#21487;&#20197;&#23558;&#28304;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#36716;&#31227;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#12290;&#28982;&#32780;&#65292;&#36716;&#31227;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#21516;&#22495;&#20013;&#23384;&#22312;&#21160;&#24577;&#30340;&#26102;&#38388;&#32467;&#26500;&#21464;&#21270;&#65292;&#23548;&#33268;&#26102;&#38388;&#21644;&#39057;&#29575;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#20559;&#31227;&#12290;&#27492;&#22806;&#65292;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#30340;&#20219;&#21153;&#26631;&#31614;&#20998;&#24067;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#38590;&#20197;&#35299;&#20915;&#26631;&#31614;&#20559;&#31227;&#21644;&#35782;&#21035;&#21807;&#19968;&#20110;&#30446;&#26631;&#22495;&#30340;&#26631;&#31614;&#12290;&#26377;&#25928;&#22320;&#36716;&#31227;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#21313;&#20998;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Raincoat&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#23553;&#38381;&#38598;&#21644;&#36890;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;Raincoat&#36890;&#36807;&#32771;&#34385;&#26102;&#38388;&#21644;&#39057;&#29575;&#29305;&#24449;&#65292;&#36328;&#22495;&#23545;&#40784;&#23427;&#20204;&#65292;&#20462;&#27491;&#19981;&#21305;&#37197;&#20197;&#20415;&#20110;&#26816;&#27979;&#31169;&#26377;&#26631;&#31614;&#26469;&#35299;&#20915;&#29305;&#24449;&#21644;&#26631;&#31614;&#20559;&#31227;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;Raincoat&#36890;&#36807;&#35782;&#21035;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#20849;&#20139;&#32467;&#26500;&#26469;&#25552;&#39640;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) enables the transfer of models trained on source domains to unlabeled target domains. However, transferring complex time series models presents challenges due to the dynamic temporal structure variations across domains. This leads to feature shifts in the time and frequency representations. Additionally, the label distributions of tasks in the source and target domains can differ significantly, posing difficulties in addressing label shifts and recognizing labels unique to the target domain. Effectively transferring complex time series models remains a formidable problem. We present Raincoat, the first model for both closed-set and universal domain adaptation on complex time series. Raincoat addresses feature and label shifts by considering both temporal and frequency features, aligning them across domains, and correcting for misalignments to facilitate the detection of private labels. Additionally, Raincoat improves transferability by identifying l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#22823;&#35268;&#27169;&#36890;&#29992;&#26680;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#26426;&#22120;&#20013;&#27169;&#22411;&#22823;&#23567;&#19982;&#25968;&#25454;&#22823;&#23567;&#30456;&#20114;&#32806;&#21512;&#30340;&#38382;&#39064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.02605</link><description>&lt;p&gt;
&#21521;&#22823;&#26680;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Toward Large Kernel Models. (arXiv:2302.02605v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#22823;&#35268;&#27169;&#36890;&#29992;&#26680;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#26426;&#22120;&#20013;&#27169;&#22411;&#22823;&#23567;&#19982;&#25968;&#25454;&#22823;&#23567;&#30456;&#20114;&#32806;&#21512;&#30340;&#38382;&#39064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30456;&#27604;&#65292;&#26680;&#26426;&#22120;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;DNN&#12290;&#26680;&#26426;&#22120;&#30340;&#20852;&#36259;&#21463;&#21040;&#20854;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#31561;&#25928;&#20110;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#29616;&#30340;&#25512;&#21160;&#12290;&#28982;&#32780;&#65292;DNN&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#23427;&#20204;&#33021;&#22815;&#29420;&#31435;&#22320;&#25193;&#23637;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#32780;&#22312;&#20256;&#32479;&#30340;&#26680;&#26426;&#22120;&#20013;&#65292;&#27169;&#22411;&#22823;&#23567;&#19982;&#25968;&#25454;&#22823;&#23567;&#26159;&#30456;&#20114;&#32806;&#21512;&#30340;&#12290;&#30001;&#20110;&#36825;&#31181;&#32806;&#21512;&#65292;&#23558;&#26680;&#26426;&#22120;&#25193;&#23637;&#21040;&#22823;&#25968;&#25454;&#26159;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26500;&#24314;&#22823;&#35268;&#27169;&#36890;&#29992;&#26680;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#26680;&#26426;&#22120;&#30340;&#19968;&#33324;&#21270;&#65292;&#36890;&#36807;&#35299;&#32806;&#27169;&#22411;&#21644;&#25968;&#25454;&#65292;&#20801;&#35768;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25237;&#24433;&#21452;&#37325;&#39044;&#22788;&#29702;SGD&#30340;EigenPro 3.0&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#29616;&#26377;&#26680;&#26041;&#27861;&#19981;&#21487;&#33021;&#23454;&#29616;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies indicate that kernel machines can often perform similarly or better than deep neural networks (DNNs) on small datasets. The interest in kernel machines has been additionally bolstered by the discovery of their equivalence to wide neural networks in certain regimes. However, a key feature of DNNs is their ability to scale the model size and training data size independently, whereas in traditional kernel machines model size is tied to data size. Because of this coupling, scaling kernel machines to large data has been computationally challenging. In this paper, we provide a way forward for constructing large-scale general kernel models, which are a generalization of kernel machines that decouples the model and data, allowing training on large datasets. Specifically, we introduce EigenPro 3.0, an algorithm based on projected dual preconditioned SGD and show scaling to model and data sizes which have not been possible with existing kernel methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#20449;&#29992;&#21644;&#19981;&#23436;&#25972;&#36712;&#36857;&#30340;GFlowNets&#26356;&#22909;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#36712;&#36857;&#30340;&#27599;&#20010;&#27493;&#39588;&#20998;&#37197;&#37096;&#20998;&#22870;&#21169;&#65292;&#22522;&#20110;&#23616;&#37096;&#22870;&#21169;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#26356;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;GFlowNets&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.01687</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#20449;&#29992;&#21644;&#19981;&#23436;&#25972;&#36712;&#36857;&#30340;GFlowNets&#26356;&#22909;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Better Training of GFlowNets with Local Credit and Incomplete Trajectories. (arXiv:2302.01687v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#20449;&#29992;&#21644;&#19981;&#23436;&#25972;&#36712;&#36857;&#30340;GFlowNets&#26356;&#22909;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#36712;&#36857;&#30340;&#27599;&#20010;&#27493;&#39588;&#20998;&#37197;&#37096;&#20998;&#22870;&#21169;&#65292;&#22522;&#20110;&#23616;&#37096;&#22870;&#21169;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#26356;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;GFlowNets&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#25110;GFlowNets&#19982;Markov&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30456;&#20851;(&#22240;&#20026;&#20182;&#20204;&#20174;&#30001;&#33021;&#37327;&#20989;&#25968;&#30830;&#23450;&#30340;&#20998;&#24067;&#20013;&#21462;&#26679;),&#19982;&#24378;&#21270;&#23398;&#20064;(&#22240;&#20026;&#20182;&#20204;&#23398;&#20064;&#36890;&#36807;&#19968;&#31995;&#21015;&#27493;&#39588;&#21462;&#26679;&#32452;&#21512;&#30340;&#23545;&#35937;&#30340;&#31574;&#30053;),&#29983;&#25104;&#27169;&#22411;(&#22240;&#20026;&#20182;&#20204;&#23398;&#20064;&#34920;&#31034;&#21644;&#20174;&#20998;&#24067;&#20013;&#21462;&#26679;),&#20197;&#21450;&#20998;&#25674;&#21464;&#20998;&#26041;&#27861;(&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#29992;&#26469;&#23398;&#20064;&#36817;&#20284;&#24182;&#20174;&#19968;&#20010;&#21542;&#21017;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#21462;&#26679;,&#32473;&#23450;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20989;&#25968;)&#12290;&#23427;&#20204;&#34987;&#35757;&#32451;&#26469;&#29983;&#25104;&#19968;&#20010;&#23545;&#35937;$x$&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#27493;&#39588;&#65292;&#20854;&#27010;&#29575;&#19982;&#19968;&#20123;&#22870;&#21169;&#20989;&#25968;$R(x)$(&#25110;$\exp(-\mathcal{E}(x))$&#65292;&#20854;&#20013;$\mathcal{E}(x)$&#34920;&#31034;&#33021;&#37327;&#20989;&#25968;)&#25104;&#27491;&#27604;&#65292;&#32473;&#20986;&#20102;&#29983;&#25104;&#36712;&#36857;&#30340;&#26411;&#31471;&#12290;&#19982;&#20854;&#20182;RL&#35774;&#32622;&#19968;&#26679;&#65292;&#22312;&#22870;&#21169;&#20165;&#22312;&#32467;&#26463;&#26102;&#32473;&#20986;&#26102;&#65292;&#24403;&#36825;&#20123;&#36712;&#36857;&#36739;&#38271;&#26102;&#65292;&#35757;&#32451;&#25928;&#29575;&#21644;&#20449;&#29992;&#20998;&#37197;&#21487;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#22312;&#20197;&#21069;&#30340;GFlowNet&#24037;&#20316;&#20013;&#65292;&#19981;&#33021;&#20174;&#19981;&#23436;&#25972;&#30340;&#36807;&#28193;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#29992;&#20998;&#37197;&#31574;&#30053;&#65292;&#20801;&#35768;&#20351;&#29992;&#19981;&#23436;&#25972;&#30340;&#36712;&#36857;&#26356;&#22909;&#22320;&#35757;&#32451;GFlowNets&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#21040;&#20026;&#36712;&#36857;&#30340;&#27599;&#20010;&#27493;&#39588;&#20998;&#37197;&#37096;&#20998;&#22870;&#21169;&#65292;&#22522;&#20110;&#23616;&#37096;&#22870;&#21169;&#20272;&#35745;&#22120;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#21487;&#29992;&#25968;&#25454;&#30340;&#26356;&#26377;&#25928;&#21033;&#29992;&#21644;GFlowNets&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks or GFlowNets are related to Monte-Carlo Markov chain methods (as they sample from a distribution specified by an energy function), reinforcement learning (as they learn a policy to sample composed objects through a sequence of steps), generative models (as they learn to represent and sample from a distribution) and amortized variational methods (as they can be used to learn to approximate and sample from an otherwise intractable posterior, given a prior and a likelihood). They are trained to generate an object $x$ through a sequence of steps with probability proportional to some reward function $R(x)$ (or $\exp(-\mathcal{E}(x))$ with $\mathcal{E}(x)$ denoting the energy function), given at the end of the generative trajectory. Like for other RL settings where the reward is only given at the end, the efficiency of training and credit assignment may suffer when those trajectories are longer. With previous GFlowNet work, no learning was possible from incomplete tr
&lt;/p&gt;</description></item><item><title>Mnemosyne&#20248;&#21270;&#22120;&#20351;&#29992;Performers&#26041;&#27861;&#26469;&#23398;&#20064;&#35757;&#32451;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#20182;Transformers&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#22120;&#35843;&#33410;&#65292;&#24182;&#25104;&#21151;&#35757;&#32451;ViTs&#21644;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2302.01128</link><description>&lt;p&gt;
Mnemosyne: &#20351;&#29992;Transformers&#26469;&#35757;&#32451;Transformers
&lt;/p&gt;
&lt;p&gt;
Mnemosyne: Learning to Train Transformers with Transformers. (arXiv:2302.01128v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01128
&lt;/p&gt;
&lt;p&gt;
Mnemosyne&#20248;&#21270;&#22120;&#20351;&#29992;Performers&#26041;&#27861;&#26469;&#23398;&#20064;&#35757;&#32451;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#20182;Transformers&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#22120;&#35843;&#33410;&#65292;&#24182;&#25104;&#21151;&#35757;&#32451;ViTs&#21644;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#26550;&#26500;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#21644;&#26102;&#38388;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#20248;&#21270;&#22120;&#24182;&#35843;&#33410;&#20854;&#36229;&#21442;&#25968;&#12290;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#24050;&#32463;&#25104;&#20026;&#25163;&#21160;&#35774;&#35745;ML&#20248;&#21270;&#22120;&#30340;&#26356;&#22909;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Mnemosyne&#20248;&#21270;&#22120;&#65292;&#23427;&#20351;&#29992;Performers: &#38544;&#24335;&#20302;&#31209;attention Transformers&#12290;&#23427;&#21487;&#20197;&#23398;&#20064;&#35757;&#32451;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#20182;Transformers&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#22120;&#35843;&#33410;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Mnemosyne&#65306;(a)&#27604;&#27969;&#34892;&#30340;LSTM&#20248;&#21270;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65307;(b)&#29305;&#21035;&#22320;&#65292;&#21487;&#20197;&#22312;&#26631;&#20934;MLPs&#19978;&#36827;&#34892;&#20803;&#35757;&#32451;&#21518;&#25104;&#21151;&#22320;&#35757;&#32451;Vision Transformers(ViTs) (c)&#21487;&#20197;&#21021;&#22987;&#21270;&#20248;&#21270;&#22120;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20123;&#32467;&#26524;&#24320;&#21551;&#20102;&#20351;&#29992;Transformers&#26500;&#24314;&#22522;&#30784;&#20248;&#21270;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#21487;&#20197;&#24212;&#23545;&#24120;&#35268;&#30340;Transformer&#35757;&#32451;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training complex machine learning (ML) architectures requires a compute and time consuming process of selecting the right optimizer and tuning its hyper-parameters. A new paradigm of learning optimizers from data has emerged as a better alternative to hand-designed ML optimizers. We propose Mnemosyne optimizer, that uses Performers: implicit low-rank attention Transformers. It can learn to train entire neural network architectures including other Transformers without any task-specific optimizer tuning. We show that Mnemosyne: (a) generalizes better than popular LSTM optimizer, (b) in particular can successfully train Vision Transformers (ViTs) while meta--trained on standard MLPs and (c) can initialize optimizers for faster convergence in Robotics applications. We believe that these results open the possibility of using Transformers to build foundational optimization models that can address the challenges of regular Transformer training. We complement our results with an extensive theo
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LieGAN&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#30340;&#31561;&#21464;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.00236</link><description>&lt;p&gt;
&#23545;&#31216;&#29983;&#25104;&#23545;&#25239;&#24615;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Symmetry Discovery. (arXiv:2302.00236v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00236
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LieGAN&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#30340;&#31561;&#21464;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#20107;&#20808;&#30693;&#36947;&#23545;&#31216;&#32676;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#30693;&#36947;&#35201;&#20351;&#29992;&#21738;&#20010;&#23545;&#31216;&#32676;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#38169;&#35823;&#22320;&#24378;&#21046;&#20351;&#29992;&#23545;&#31216;&#32676;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;LieGAN&#26694;&#26550;&#65292;&#36890;&#36807;&#31867;&#20284;&#29983;&#25104;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#33539;&#24335;&#33258;&#21160;&#20174;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#31561;&#21464;&#24615;&#12290;&#29983;&#25104;&#22120;&#23398;&#20064;&#19968;&#32452;&#24212;&#29992;&#20110;&#25968;&#25454;&#30340;&#21464;&#25442;&#65292;&#36825;&#20123;&#21464;&#25442;&#20445;&#25345;&#21407;&#22987;&#20998;&#24067;&#24182;&#27450;&#39575;&#37492;&#21035;&#22120;&#12290;LieGAN&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26446;&#20195;&#25968;&#22522;&#34920;&#31034;&#23545;&#31216;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#36712;&#36857;&#39044;&#27979;&#21644;&#39030;&#22840;&#20811;&#26631;&#35760;&#20219;&#21153;&#20013;&#21457;&#29616;&#21508;&#31181;&#23545;&#31216;&#24615;&#65292;&#20363;&#22914;&#26059;&#36716;&#32676;$\mathrm{SO}(n)$&#65292;&#38480;&#21046;Lorentz&#32676;$\mathrm{SO}(1,3)^+$&#12290;&#25152;&#23398;&#20064;&#30340;&#23545;&#31216;&#24615;&#20063;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#20960;&#20010;&#29616;&#26377;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of equivariant neural networks in scientific applications, they require knowing the symmetry group a priori. However, it may be difficult to know which symmetry to use as an inductive bias in practice. Enforcing the wrong symmetry could even hurt the performance. In this paper, we propose a framework, LieGAN, to automatically discover equivariances from a dataset using a paradigm akin to generative adversarial training. Specifically, a generator learns a group of transformations applied to the data, which preserve the original distribution and fool the discriminator. LieGAN represents symmetry as interpretable Lie algebra basis and can discover various symmetries such as the rotation group $\mathrm{SO}(n)$, restricted Lorentz group $\mathrm{SO}(1,3)^+$ in trajectory prediction and top-quark tagging tasks. The learned symmetry can also be readily used in several existing equivariant neural networks to improve accuracy and generalization in prediction.
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#33719;&#24471;&#39640;&#25928;&#20934;&#30830;&#30340;&#28237;&#27969;&#27969;&#21160;&#28508;&#22312;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2301.13728</link><description>&lt;p&gt;
&#29992;&#20110;&#28237;&#27969;&#26102;&#31354;&#28508;&#22312;&#25551;&#36848;&#30340;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Convolutional autoencoder for the spatiotemporal latent representation of turbulence. (arXiv:2301.13728v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13728
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#33719;&#24471;&#39640;&#25928;&#20934;&#30830;&#30340;&#28237;&#27969;&#27969;&#21160;&#28508;&#22312;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28237;&#27969;&#30340;&#28151;&#27788;&#21160;&#21147;&#23398;&#21644;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20351;&#24471;&#20854;&#38590;&#20197;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#28237;&#27969;&#27969;&#21160;&#24120;&#24120;&#34987;&#19968;&#20123;&#36830;&#32493;&#30340;&#26102;&#31354;&#32467;&#26500;&#25152;&#25551;&#36848;&#65292;&#22914;&#28457;&#28065;&#25110;&#22823;&#23610;&#24230;&#27169;&#24335;&#65292;&#36825;&#21487;&#20197;&#24110;&#21161;&#33719;&#24471;&#28237;&#27969;&#27969;&#21160;&#30340;&#28508;&#22312;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#21463;&#21040;&#38408;&#20540;&#21270;&#21644;&#20256;&#32479;&#27169;&#24577;&#27969;&#20998;&#35299;&#26041;&#27861;&#30340;&#32447;&#24615;&#24615;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#26088;&#22312;&#33719;&#24471;&#19968;&#31181;&#23545;&#20110;&#20855;&#26377;&#26497;&#31471;&#20107;&#20214;&#30340;&#28237;&#27969;&#27969;&#21160;&#32780;&#35328;&#26082;&#39640;&#25928;&#21448;&#20934;&#30830;&#30340;&#38477;&#32500;&#28508;&#22312;&#25551;&#36848;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#19977;&#32500;&#22810;&#23610;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;(CAE)&#26469;&#33719;&#24471;&#28237;&#27969;&#27969;&#21160;&#30340;&#28508;&#22312;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turbulence is characterised by chaotic dynamics and a high-dimensional state space, which make this phenomenon challenging to predict. However, turbulent flows are often characterised by coherent spatiotemporal structures, such as vortices or large-scale modes, which can help obtain a latent description of turbulent flows. However, current approaches are often limited by either the need to use some form of thresholding on quantities defining the isosurfaces to which the flow structures are associated or the linearity of traditional modal flow decomposition approaches, such as those based on proper orthogonal decomposition. This problem is exacerbated in flows that exhibit extreme events, which are rare and sudden changes in a turbulent state. The goal of this paper is to obtain an efficient and accurate reduced-order latent representation of a turbulent flow that exhibits extreme events. Specifically, we employ a three-dimensional multiscale convolutional autoencoder (CAE) to obtain su
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#22270;&#24418;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#32508;&#21512;&#24615;&#24322;&#26500;&#22270;&#24418;&#34920;&#31034;&#26469;&#32467;&#21512;&#20132;&#36890;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#26102;&#31354;&#20449;&#24687;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#20197;&#21450;&#36947;&#36335;&#32593;&#32476;&#31561;&#38745;&#24577;&#20803;&#32032;&#30340;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#36816;&#21160;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2301.13545</link><description>&lt;p&gt;
&#20840;&#23616;&#22270;&#24418;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Holistic Graph-based Motion Prediction. (arXiv:2301.13545v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13545
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#22270;&#24418;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#32508;&#21512;&#24615;&#24322;&#26500;&#22270;&#24418;&#34920;&#31034;&#26469;&#32467;&#21512;&#20132;&#36890;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#26102;&#31354;&#20449;&#24687;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#20197;&#21450;&#36947;&#36335;&#32593;&#32476;&#31561;&#38745;&#24577;&#20803;&#32032;&#30340;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#36816;&#21160;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#36816;&#21160;&#39044;&#27979;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#34987;&#20351;&#29992;&#26102;&#38656;&#35201;&#25484;&#25569;&#12290;&#35768;&#22810;&#22240;&#32032;&#24433;&#21709;&#30528;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#21160;&#21521;&#65292;&#20174;&#20132;&#36890;&#35268;&#21017;&#12289;&#30456;&#20114;&#20132;&#20114;&#21040;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#20010;&#20154;&#20064;&#24815;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#22270;&#24418;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#32508;&#21512;&#24615;&#24322;&#26500;&#22270;&#24418;&#34920;&#31034;&#65292;&#32467;&#21512;&#20102;&#20132;&#36890;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#26102;&#31354;&#20449;&#24687;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#20197;&#21450;&#19982;&#36947;&#36335;&#32593;&#32476;&#31561;&#38745;&#24577;&#20803;&#32032;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#32536;&#26469;&#32534;&#30721;&#20449;&#24687;&#65292;&#20004;&#32773;&#37117;&#20855;&#22791;&#20219;&#24847;&#29305;&#24615;&#12290;&#25105;&#20204;&#22312;INTERACTION&#21644;Argoverse&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#20449;&#24687;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#35777;&#26126;&#21508;&#31181;&#20449;&#24687;&#23545;&#36816;&#21160;&#39044;&#27979;&#36136;&#37327;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion prediction for automated vehicles in complex environments is a difficult task that is to be mastered when automated vehicles are to be used in arbitrary situations. Many factors influence the future motion of traffic participants starting with traffic rules and reaching from the interaction between each other to personal habits of human drivers. Therefore we present a novel approach for a graph-based prediction based on a heterogeneous holistic graph representation that combines temporal information, properties and relations between traffic participants as well as relations with static elements like the road network. The information are encoded through different types of nodes and edges that both are enriched with arbitrary features. We evaluated the approach on the INTERACTION and the Argoverse dataset and conducted an informative ablation study to demonstrate the benefit of different types of information for the motion prediction quality.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;MILO&#65292;&#23558;&#23376;&#38598;&#36873;&#25321;&#19982;&#27169;&#22411;&#35757;&#32451;&#20998;&#31163;&#65292;&#36890;&#36807;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13287</link><description>&lt;p&gt;
MILO: &#27169;&#22411;&#26080;&#20851;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#27169;&#22411;&#35757;&#32451;&#21644;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning. (arXiv:2301.13287v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;MILO&#65292;&#23558;&#23376;&#38598;&#36873;&#25321;&#19982;&#27169;&#22411;&#35757;&#32451;&#20998;&#31163;&#65292;&#36890;&#36807;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#21644;&#35843;&#20248;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#36229;&#21442;&#25968;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#12290;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#30340;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#20043;&#19968;&#26159;&#36890;&#36807;&#36873;&#25321;&#24456;&#22909;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#26469;&#23454;&#29616;&#12290;&#19982;&#31616;&#21333;&#30340;&#33258;&#36866;&#24212;&#38543;&#26426;&#23376;&#38598;&#36873;&#25321;&#22522;&#20934;&#30456;&#27604;&#65292;&#29616;&#26377;&#30340;&#26234;&#33021;&#23376;&#38598;&#36873;&#25321;&#26041;&#27861;&#30001;&#20110;&#32791;&#26102;&#30340;&#23376;&#38598;&#36873;&#25321;&#27493;&#39588;&#32780;&#19981;&#20855;&#31454;&#20105;&#21147;&#65292;&#35813;&#27493;&#39588;&#28041;&#21450;&#35745;&#31639;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#26799;&#24230;&#21644;&#29305;&#24449;&#23884;&#20837;&#65292;&#24182;&#24212;&#29992;&#23376;&#27169;&#22359;&#30446;&#26631;&#30340;&#36138;&#24515;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#28040;&#38500;&#23545;&#19979;&#28216;&#27169;&#22411;&#21442;&#25968;&#30340;&#20381;&#36182;&#65292;&#23558;&#23376;&#38598;&#36873;&#25321;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MILO&#65292;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;&#65292;&#23427;&#23558;&#23376;&#38598;&#36873;&#25321;&#19982;&#27169;&#22411;&#35757;&#32451;&#20998;&#31163;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;Wasserstein&#36317;&#31163;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;Wasserstein&#36317;&#31163;&#27979;&#37327;&#26469;&#22686;&#24378;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#20960;&#31181;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12197</link><description>&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#30340;&#20114;Wasserstein&#36317;&#31163;&#26368;&#23567;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mutual Wasserstein Discrepancy Minimization for Sequential Recommendation. (arXiv:2301.12197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;Wasserstein&#36317;&#31163;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;Wasserstein&#36317;&#31163;&#27979;&#37327;&#26469;&#22686;&#24378;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#20960;&#31181;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#39034;&#24207;&#25512;&#33616;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#21644;&#35774;&#35745;&#33391;&#22909;&#30340;&#25968;&#25454;&#22686;&#24191;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;, &#30446;&#21069;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#22522;&#20110;&#35745;&#31639;Kullback Leibler&#24046;&#24322;&#65292;&#24182;&#19988;&#23384;&#22312;&#35768;&#22810;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#38750;&#23545;&#31216;&#20272;&#35745;&#12289;&#25351;&#25968;&#26679;&#26412;&#22823;&#23567;&#38656;&#27714;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#12290;&#32780;&#20351;&#29992;&#30340;&#29616;&#26377;&#25968;&#25454;&#22686;&#24191;&#22823;&#22810;&#26159;&#38543;&#26426;&#30340;&#65292;&#21487;&#33021;&#20250;&#22240;&#20026;&#38543;&#26426;&#20462;&#25913;&#32780;&#30772;&#22351;&#39034;&#24207;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;Wasserstein&#36317;&#31163;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Wasserstein&#36317;&#31163;&#27979;&#37327;&#26469;&#35780;&#20272;&#22686;&#24191;&#24207;&#21015;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#36890;&#36807;&#20943;&#23569;&#21407;&#22987;&#21644;&#22686;&#24191;&#24207;&#21015;&#30340;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#22686;&#24378;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#20960;&#31181;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised sequential recommendation significantly improves recommendation performance by maximizing mutual information with well-designed data augmentations. However, the mutual information estimation is based on the calculation of Kullback Leibler divergence with several limitations, including asymmetrical estimation, the exponential need of the sample size, and training instability. Also, existing data augmentations are mostly stochastic and can potentially break sequential correlations with random modifications. These two issues motivate us to investigate an alternative robust mutual information measurement capable of modeling uncertainty and alleviating KL divergence limitations. To this end, we propose a novel self-supervised learning framework based on Mutual WasserStein discrepancy minimization MStein for the sequential recommendation. We propose the Wasserstein Discrepancy Measurement to measure the mutual information between augmented sequences. Wasserstein Discrepancy M
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#32500;&#27010;&#24565;&#21457;&#29616;(MCD)&#26041;&#27861;&#65292;&#23427;&#28385;&#36275;&#27010;&#24565;&#23618;&#38754;&#19978;&#30340;&#23436;&#25972;&#24615;&#20851;&#31995;&#65292;&#19981;&#38656;&#35201;&#21152;&#24378;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#37096;&#20998;&#65292;&#24182;&#25552;&#20379;&#27010;&#24565;&#28608;&#27963;&#22270;&#20998;&#26512;&#24037;&#20855;</title><link>http://arxiv.org/abs/2301.11911</link><description>&lt;p&gt;
&#22810;&#32500;&#27010;&#24565;&#21457;&#29616;(MCD): &#19968;&#20010;&#20855;&#26377;&#23436;&#25972;&#24615;&#20445;&#35777;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees. (arXiv:2301.11911v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11911
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#32500;&#27010;&#24565;&#21457;&#29616;(MCD)&#26041;&#27861;&#65292;&#23427;&#28385;&#36275;&#27010;&#24565;&#23618;&#38754;&#19978;&#30340;&#23436;&#25972;&#24615;&#20851;&#31995;&#65292;&#19981;&#38656;&#35201;&#21152;&#24378;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#37096;&#20998;&#65292;&#24182;&#25552;&#20379;&#27010;&#24565;&#28608;&#27963;&#22270;&#20998;&#26512;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#25972;&#24615;&#20844;&#29702;&#20351;&#24471;&#21518;&#32493;XAI&#26041;&#27861;&#30340;&#35299;&#37322;&#20165;&#23545;&#27169;&#22411;&#22312;&#21333;&#20010;&#20915;&#31574;&#19978;&#26377;&#25928;&#12290;&#20026;&#20102;&#21487;&#20449;&#22320;&#24212;&#29992;XAI&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#39118;&#38505;&#30340;&#20915;&#31574;&#65292;&#38656;&#35201;&#26356;&#20840;&#29699;&#30340;&#27169;&#22411;&#29702;&#35299;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#19982;&#23454;&#38469;&#30340;&#27169;&#22411;&#25512;&#29702;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#32500;&#27010;&#24565;&#21457;&#29616;(MCD)&#65292;&#20316;&#20026;&#20043;&#21069;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#28385;&#36275;&#27010;&#24565;&#23618;&#38754;&#19978;&#30340;&#23436;&#25972;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#36890;&#29992;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#20316;&#20026;&#27010;&#24565;&#24320;&#22987;&#65292;&#24182;&#19981;&#38656;&#35201;&#21152;&#24378;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31232;&#30095;&#23376;&#31354;&#38388;&#32858;&#31867;&#26469;&#21457;&#29616;&#25913;&#36827;&#30340;&#27010;&#24565;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22810;&#32500;&#23376;&#31354;&#38388;&#30340;&#28508;&#33021;&#12290;MCD&#25552;&#20379;&#20102;&#20004;&#31181;&#27010;&#24565;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20114;&#34917;&#20998;&#26512;&#24037;&#20855;&#65306;(1)&#27010;&#24565;&#28608;&#27963;&#22270;&#65292;&#26174;&#31034;&#27010;&#24565;&#34920;&#36798;&#30340;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
The completeness axiom renders the explanation of a post-hoc XAI method only locally faithful to the model, i.e. for a single decision. For the trustworthy application of XAI, in particular for high-stake decisions, a more global model understanding is required. Recently, concept-based methods have been proposed, which are however not guaranteed to be bound to the actual model reasoning. To circumvent this problem, we propose Multi-dimensional Concept Discovery (MCD) as an extension of previous approaches that fulfills a completeness relation on the level of concepts. Our method starts from general linear subspaces as concepts and does neither require reinforcing concept interpretability nor re-training of model parts. We propose sparse subspace clustering to discover improved concepts and fully leverage the potential of multi-dimensional subspaces. MCD offers two complementary analysis tools for concepts in input space: (1) concept activation maps, that show where a concept is express
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36755;&#20837;&#25200;&#21160;&#26041;&#27861;&#26469;&#32531;&#35299;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#65292;&#33021;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#24182;&#20943;&#23569;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2301.11706</link><description>&lt;p&gt;
&#36755;&#20837;&#25200;&#21160;&#38477;&#20302;&#25193;&#25955;&#27169;&#22411;&#30340;&#26292;&#38706;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Input Perturbation Reduces Exposure Bias in Diffusion Models. (arXiv:2301.11706v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36755;&#20837;&#25200;&#21160;&#26041;&#27861;&#26469;&#32531;&#35299;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#65292;&#33021;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#24182;&#20943;&#23569;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#20294;&#26159;&#23427;&#20204;&#38271;&#30340;&#25277;&#26679;&#38142;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#38271;&#26102;&#38388;&#30340;&#25277;&#26679;&#38142;&#20063;&#20250;&#23548;&#33268;&#19968;&#31181;&#38169;&#35823;&#31215;&#32047;&#29616;&#35937;&#65292;&#31867;&#20284;&#20110;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#22240;&#20026;&#21069;&#32773;&#26159;&#22522;&#20110;&#30495;&#23454;&#26679;&#26412;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#65292;&#32780;&#21518;&#32773;&#26159;&#22522;&#20110;&#20043;&#21069;&#29983;&#25104;&#30340;&#32467;&#26524;&#36827;&#34892;&#26465;&#20214;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35757;&#32451;&#35268;&#21017;&#65292;&#21363;&#36890;&#36807;&#25200;&#21160;&#30495;&#23454;&#26679;&#26412;&#26469;&#27169;&#25311;&#25512;&#26029;&#26102;&#38388;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#36825;&#31181;&#36755;&#20837;&#25200;&#21160;&#26041;&#24335;&#65292;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#21644;&#31934;&#30830;&#29575;&#65292;&#21364;&#33021;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#65292;&#21516;&#26102;&#20943;&#23569;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#38388;&#12290;&#20363;&#22914;&#65292;&#22312;CelebA 64$\times$64&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;36.74&#30340;Fr&#233;chet Inception Distance&#65292;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64$\times$64, we ach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24102;&#26377;&#37325;&#22797;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#38543;&#26426;&#27969;&#29983;&#25104;&#22120;&#20135;&#29983;&#19981;&#21516;&#30340;CIR&#27969;&#36827;&#34892;&#35780;&#20272;&#21644;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#25918;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2301.11396</link><description>&lt;p&gt;
&#24102;&#26377;&#37325;&#22797;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning with Repetition. (arXiv:2301.11396v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11396
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24102;&#26377;&#37325;&#22797;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#38543;&#26426;&#27969;&#29983;&#25104;&#22120;&#20135;&#29983;&#19981;&#21516;&#30340;CIR&#27969;&#36827;&#34892;&#35780;&#20272;&#21644;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#25918;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#25968;&#25454;&#27969;&#20013;&#33258;&#28982;&#21253;&#21547;&#20043;&#21069;&#27010;&#24565;&#30340;&#37325;&#22797;&#12290;&#20174;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#37325;&#22797;&#26159;&#29615;&#22659;&#30340;&#23646;&#24615;&#65292;&#19982;&#37325;&#25918;&#19981;&#21516;&#65292;&#19981;&#33021;&#30001;&#20195;&#29702;&#31243;&#24207;&#25511;&#21046;&#12290;&#29616;&#22312;&#65292;&#31867;&#22686;&#37327;&#65288;CI&#65289;&#22330;&#26223;&#20195;&#34920;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;CL&#31574;&#30053;&#30340;&#20027;&#35201;&#27979;&#35797;&#24179;&#21488;&#12290;&#36825;&#31181;&#24773;&#20917;&#38750;&#24120;&#23481;&#26131;&#20351;&#29992;&#65292;&#20294;&#23427;&#20174;&#19981;&#20801;&#35768;&#37325;&#26032;&#35775;&#38382;&#20043;&#21069;&#30475;&#21040;&#30340;&#31867;&#65292;&#22240;&#27492;&#23436;&#20840;&#24573;&#30053;&#20102;&#37325;&#22797;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20851;&#27880;&#24102;&#26377;&#37325;&#22797;&#30340;&#31867;&#22686;&#37327;&#65288;CIR&#65289;&#22330;&#26223;&#23478;&#26063;&#65292;&#20854;&#20013;&#37325;&#22797;&#34987;&#23884;&#20837;&#21040;&#27969;&#30340;&#23450;&#20041;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#38543;&#26426;&#27969;&#29983;&#25104;&#22120;&#65292;&#23427;&#20204;&#21487;&#20197;&#20174;&#21333;&#20010;&#25968;&#25454;&#38598;&#21644;&#20960;&#20010;&#21487;&#35299;&#37322;&#30340;&#25511;&#21046;&#21442;&#25968;&#24320;&#22987;&#20135;&#29983;&#21508;&#31181;CIR&#27969;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;CL&#31574;&#30053;&#22312;&#19981;&#21516;CIR&#27969;&#19979;&#30340;&#34892;&#20026;&#65292;&#36827;&#34892;&#20102;&#23545;&#37325;&#22797;&#22312;CL&#20013;&#30340;&#31532;&#19968;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37325;&#22797;&#30340;&#26032;&#30340;&#37325;&#25918;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data streams naturally include the repetition of previous concepts. From a Continual Learning (CL) perspective, repetition is a property of the environment and, unlike replay, cannot be controlled by the agent. Nowadays, the Class-Incremental (CI) scenario represents the leading test-bed for assessing and comparing CL strategies. This scenario type is very easy to use, but it never allows revisiting previously seen classes, thus completely neglecting the role of repetition. We focus on the family of Class-Incremental with Repetition (CIR) scenario, where repetition is embedded in the definition of the stream. We propose two stochastic stream generators that produce a wide range of CIR streams starting from a single dataset and a few interpretable control parameters. We conduct the first comprehensive evaluation of repetition in CL by studying the behavior of existing CL strategies under different CIR streams. We then present a novel replay strategy that exploits repetition a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#36830;&#32493;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20854;&#37319;&#29992;&#36741;&#21161;&#21464;&#37327;&#26469;&#21306;&#20998;&#35782;&#21035;&#21644;&#21160;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11308</link><description>&lt;p&gt;
&#38024;&#23545;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#36830;&#32493;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series. (arXiv:2301.11308v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#36830;&#32493;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20854;&#37319;&#29992;&#36741;&#21161;&#21464;&#37327;&#26469;&#21306;&#20998;&#35782;&#21035;&#21644;&#21160;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30495;&#23454;&#19990;&#30028;&#21160;&#24577;&#29616;&#35937;&#65288;&#22914;&#27668;&#20505;&#12289;&#29983;&#29289;&#23398;&#31561;&#65289;&#30340;&#20934;&#30830;&#39044;&#27979;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#19968;&#39033;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#33258;&#28982;&#21644;&#20154;&#24037;&#36807;&#31243;&#29983;&#25104;&#30340;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;/&#25110;&#32570;&#22833;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#31070;&#32463;&#36830;&#32493;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;NCDSSM&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#31163;&#25955;&#26102;&#38388;&#35266;&#27979;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#36830;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;NCDSSM&#37319;&#29992;&#36741;&#21161;&#21464;&#37327;&#26469;&#21306;&#20998;&#35782;&#21035;&#21644;&#21160;&#24577;&#65292;&#22240;&#27492;&#20165;&#38656;&#35201;&#23545;&#36741;&#21161;&#21464;&#37327;&#36827;&#34892;&#25674;&#38144;&#25512;&#29702;&#12290;&#21033;&#29992;&#36830;&#32493;-&#31163;&#25955;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23545;&#21160;&#24577;&#29366;&#24577;&#36827;&#34892;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#28789;&#27963;&#30340;&#28508;&#22312;&#21160;&#24577;&#21442;&#25968;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#22312;&#25512;&#26029;&#26399;&#38388;&#23545;&#21160;&#24577;&#29366;&#24577;&#36827;&#34892;&#36793;&#32536;&#21270;&#30340;&#39640;&#25928;&#22521;&#35757;&#30446;&#26631;&#12290;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;&#25913;&#36827;&#20102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning accurate predictive models of real-world dynamic phenomena (e.g., climate, biological) remains a challenging task. One key issue is that the data generated by both natural and artificial processes often comprise time series that are irregularly sampled and/or contain missing observations. In this work, we propose the Neural Continuous-Discrete State Space Model (NCDSSM) for continuous-time modeling of time series through discrete-time observations. NCDSSM employs auxiliary variables to disentangle recognition from dynamics, thus requiring amortized inference only for the auxiliary variables. Leveraging techniques from continuous-discrete filtering theory, we demonstrate how to perform accurate Bayesian inference for the dynamic states. We propose three flexible parameterizations of the latent dynamics and an efficient training objective that marginalizes the dynamic states during inference. Empirical results on multiple benchmark datasets across various domains show improved i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#20132;&#26367;&#32676;($A_n$)&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#23436;&#25972;&#34920;&#24449;&#65292;&#20854;&#20013;&#25551;&#36848;&#20102;&#21487;&#23398;&#20064;&#30340;&#12289;&#32447;&#24615;&#30340;&#12289;$A_n$&#31561;&#21464;&#23618;&#20989;&#25968;&#30340;&#30697;&#38453;&#22522;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.10152</link><description>&lt;p&gt;
&#27700;&#27597;&#22914;&#20309;&#34920;&#24449;&#20132;&#26367;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
How Jellyfish Characterise Alternating Group Equivariant Neural Networks. (arXiv:2301.10152v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10152
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#20132;&#26367;&#32676;($A_n$)&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#23436;&#25972;&#34920;&#24449;&#65292;&#20854;&#20013;&#25551;&#36848;&#20102;&#21487;&#23398;&#20064;&#30340;&#12289;&#32447;&#24615;&#30340;&#12289;$A_n$&#31561;&#21464;&#23618;&#20989;&#25968;&#30340;&#30697;&#38453;&#22522;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#23618;&#25968;&#20026;$\mathbb{R}^{n}$&#24352;&#37327;&#24130;&#27425;&#30340;&#20132;&#26367;&#32676;($A_n$)&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#23436;&#25972;&#34920;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^{n}$&#30340;&#26631;&#20934;&#22522;&#30784;&#19978;&#25214;&#21040;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#12289;&#32447;&#24615;&#30340;&#12289;$A_n$&#31561;&#21464;&#23618;&#20989;&#25968;&#30340;&#30697;&#38453;&#22522;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#25512;&#24191;&#21040;&#26500;&#24314;&#31561;&#21464;&#20110;&#23616;&#37096;&#23545;&#31216;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a full characterisation of all of the possible alternating group ($A_n$) equivariant neural networks whose layers are some tensor power of $\mathbb{R}^{n}$. In particular, we find a basis of matrices for the learnable, linear, $A_n$-equivariant layer functions between such tensor power spaces in the standard basis of $\mathbb{R}^{n}$. We also describe how our approach generalises to the construction of neural networks that are equivariant to local symmetries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;CNN&#39044;&#27979;&#21487;&#20449;&#24230;&#30340;&#21487;&#20449;&#24230;&#20998;&#25968;&#65288;TS&#65289;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#26816;&#26597;CNN&#25152;&#20570;&#39044;&#27979;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#23384;&#22312;&#26469;&#37327;&#21270;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.08839</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;CNN&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Trustworthiness Score to Evaluate CNNs Predictions. (arXiv:2301.08839v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;CNN&#39044;&#27979;&#21487;&#20449;&#24230;&#30340;&#21487;&#20449;&#24230;&#20998;&#25968;&#65288;TS&#65289;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#26816;&#26597;CNN&#25152;&#20570;&#39044;&#27979;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#23384;&#22312;&#26469;&#37327;&#21270;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a trustworthiness score (TS) metric to evaluate the confidence of CNN predictions, which quantifies the trustworthiness by checking for the existence of certain features in the predictions made by the CNN.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#26080;&#27861;&#22312;&#25805;&#20316;&#26399;&#38388;&#25345;&#32493;&#39564;&#35777;CNN&#65292;&#36825;&#20351;&#24471;&#24320;&#21457;&#20154;&#21592;&#21644;&#30417;&#31649;&#26426;&#26500;&#38590;&#20197;&#23545;&#20351;&#29992;CNN&#30340;&#33258;&#20027;&#31995;&#32479;&#30340;&#37096;&#32626;&#33719;&#24471;&#20449;&#24515;&#12290;&#22312;&#25805;&#20316;&#26399;&#38388;&#65292;&#20102;&#35299;CNN&#30340;&#39044;&#27979;&#20309;&#26102;&#21487;&#20449;&#25110;&#21487;&#30097;&#23545;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#26412;&#26041;&#27861;&#26159;&#20351;&#29992;&#27169;&#22411;&#30340;&#36755;&#20986;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#35780;&#20272;&#39044;&#27979;&#26159;&#21542;&#21487;&#20449;&#25110;&#21487;&#30097;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26159;&#26469;&#33258;&#40657;&#30418;&#35745;&#31639;&#30340;&#32467;&#26524;&#65292;&#22240;&#27492;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#20351;&#24471;&#24456;&#38590;&#23558;&#21487;&#20449;&#24230;&#24402;&#22240;&#20110;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20449;&#24230;&#20998;&#25968;&#65288;TS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#36879;&#26126;&#21644;&#26377;&#25928;&#30340;&#26041;&#24335;&#26469;&#25552;&#20379;CNN&#39044;&#27979;&#30340;&#20449;&#24515;&#12290;&#35813;&#24230;&#37327;&#26631;&#20934;&#36890;&#36807;&#26816;&#26597;CNN&#25152;&#20570;&#39044;&#27979;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#23384;&#22312;&#26469;&#37327;&#21270;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the black box nature of Convolutional neural networks (CNNs), the continuous validation of CNNs during operation is infeasible. As a result this makes it difficult for developers and regulators to gain confidence in the deployment of autonomous systems employing CNNs. It is critical for safety during operation to know when a CNN's predictions are trustworthy or suspicious. The basic approach is to use the model's output confidence score to assess if predictions are trustworthy or suspicious. However, the model's confidence score is a result of computations coming from a black box, therefore lacks transparency and makes it challenging to credit trustworthiness to predictions. We introduce the trustworthiness score (TS), a simple metric that provides a more transparent and effective way of providing confidence in CNNs predictions. The metric quantifies the trustworthiness in a prediction by checking for the existence of certain features in the predictions made by the CNN. The TS m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#30340;&#22240;&#26524;&#20266;&#35777;&#26041;&#27861;&#65292;&#20197;&#21487;&#38752;&#24182;&#23454;&#29992;&#30340;&#26041;&#24335;&#22312;&#26368;&#23567;&#38480;&#24230;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#23402;&#29983;&#30340;&#20449;&#24687;&#21644;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.07210</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#30340;&#22240;&#26524;&#20266;&#35777;
&lt;/p&gt;
&lt;p&gt;
Causal Falsification of Digital Twins. (arXiv:2301.07210v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#30340;&#22240;&#26524;&#20266;&#35777;&#26041;&#27861;&#65292;&#20197;&#21487;&#38752;&#24182;&#23454;&#29992;&#30340;&#26041;&#24335;&#22312;&#26368;&#23567;&#38480;&#24230;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#23402;&#29983;&#30340;&#20449;&#24687;&#21644;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#19979;&#24191;&#27867;&#37096;&#32626;&#23427;&#20204;&#30340;&#31934;&#24230;&#35780;&#20272;&#38656;&#35201;&#20005;&#26684;&#30340;&#31243;&#24207;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#25512;&#29702;&#26694;&#26550;&#20869;&#21046;&#23450;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;&#29616;&#23454;&#25968;&#25454;&#23581;&#35797;&#35777;&#26126;&#23402;&#29983;&#30340;&#27491;&#30830;&#24615;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#38500;&#38750;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#21487;&#33021;&#26377;&#39118;&#38505;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#31574;&#30053;&#65292;&#26088;&#22312;&#25214;&#21040;&#23402;&#29983;&#19981;&#27491;&#30830;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#23454;&#29616;&#27492;&#30446;&#26631;&#30340;&#36890;&#29992;&#32479;&#35745;&#36807;&#31243;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#21644;&#23402;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26368;&#23567;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#21487;&#38752;&#21644;&#21487;&#25805;&#20316;&#30340;&#23402;&#29983;&#20449;&#24687;&#21644;&#35780;&#20272;&#32467;&#26524;&#12290;&#36890;&#36807;&#21253;&#21547;&#33033;&#20914;&#29983;&#29702;&#23398;&#24341;&#25806;&#20013;&#33043;&#27602;&#30151;&#24314;&#27169;&#30340;&#22823;&#22411;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital twins hold substantial promise in many applications, but rigorous procedures for assessing their accuracy are essential for their widespread deployment in safety-critical settings. By formulating this task within the framework of causal inference, we show that attempts to certify the correctness of a twin using real-world observational data are unsound unless potentially tenuous assumptions are made about the data-generating process. To avoid these assumptions, we propose an assessment strategy that instead aims to find cases where the twin is not correct, and present a general-purpose statistical procedure for doing so that may be used across a wide variety of applications and twin models. Our approach yields reliable and actionable information about the twin under minimal assumptions about the twin and the real-world process of interest. We demonstrate the effectiveness of our methodology via a large-scale case study involving sepsis modelling within the Pulse Physiology Engi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;DNN&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20998;&#21035;&#32473;&#20986;&#20102;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#21644;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2301.07068</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#23433;&#20840;&#36755;&#20837;&#35745;&#25968;&#30340;#DNN-Verification&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks. (arXiv:2301.07068v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;DNN&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20998;&#21035;&#32473;&#20986;&#20102;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#21644;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#38656;&#35201;&#39640;&#24230;&#23433;&#20840;&#24615;&#30340;&#20851;&#38190;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#20013;&#36234;&#26469;&#36234;&#34987;&#37319;&#29992;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#22120;&#21487;&#20197;&#29992;&#26469;&#26816;&#26597;DNN&#26159;&#21542;&#19981;&#23433;&#20840;&#65292;&#21363;&#26159;&#21542;&#23384;&#22312;&#33267;&#23569;&#19968;&#31181;&#19981;&#23433;&#20840;&#30340;&#36755;&#20837;&#37197;&#32622;&#65292;&#20294;&#23427;&#20204;&#30340;&#26159;/&#21542;&#36755;&#20986;&#23545;&#20110;&#20854;&#20182;&#30446;&#30340;&#65288;&#22914;&#23631;&#34109;&#12289;&#27169;&#22411;&#36873;&#25321;&#25110;&#22521;&#35757;&#25913;&#36827;&#65289;&#30340;&#20449;&#24687;&#19981;&#36275;&#22815;&#35814;&#32454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#23427;&#28041;&#21450;&#35745;&#31639;&#23548;&#33268;DNN&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#36820;&#22238;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#12290;&#30001;&#20110;&#35813;&#38382;&#39064;&#30340;#P&#23436;&#22791;&#24615;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#27491;&#30830;&#35745;&#25968;&#30340;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#21576;&#29616;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#22120;&#21644;&#22522;&#20110;&#35745;&#25968;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks are increasingly adopted in critical tasks that require a high level of safety, e.g., autonomous driving. While state-of-the-art verifiers can be employed to check whether a DNN is unsafe w.r.t. some given property (i.e., whether there is at least one unsafe input configuration), their yes/no output is not informative enough for other purposes, such as shielding, model selection, or training improvements. In this paper, we introduce the #DNN-Verification problem, which involves counting the number of input configurations of a DNN that result in a violation of a particular safety property. We analyze the complexity of this problem and propose a novel approach that returns the exact count of violations. Due to the #P-completeness of the problem, we also propose a randomized, approximate method that provides a provable probabilistic bound of the correct count while significantly reducing computational requirements. We present experimental results on a set of safety-cr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.05599</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#30701;SSVEP&#25968;&#25454;&#25193;&#23637;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Short-length SSVEP data extension by a novel generative adversarial networks based framework. (arXiv:2301.05599v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;SSVEP&#30340;&#33041;&#26426;&#25509;&#21475;&#22240;&#20854;&#39640;&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#21644;&#30446;&#26631;&#25968;&#37327;&#21487;&#29992;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#39057;&#29575;&#35782;&#21035;&#26041;&#27861;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#29992;&#25143;&#26657;&#20934;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#25968;&#25454;&#38271;&#24230;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#21019;&#24314;&#21512;&#25104;&#30340;&#33041;&#30005;&#25968;&#25454;&#65292;&#26377;&#26395;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GANs&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#29992;&#20110;&#25968;&#25454;&#38271;&#24230;&#25193;&#23637;&#12290;TEGAN&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;&#26032;&#39062;&#30340;U&#22411;&#29983;&#25104;&#22120;&#26550;&#26500;&#21644;&#19968;&#20010;&#36741;&#21161;&#20998;&#31867;&#22120;&#21152;&#20837;&#21040;&#32593;&#32476;&#32467;&#26500;&#20013;&#65292;TEGAN&#21487;&#20197;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#20135;&#29983;&#26377;&#26465;&#20214;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#39057;&#29575;&#35782;&#21035;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;TEGAN&#29983;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TEGAN&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;TEGAN&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#20943;&#23569;&#25152;&#38656;&#30340;&#26657;&#20934;&#26102;&#38388;&#24182;&#25913;&#21892;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steady-state visual evoked potentials (SSVEPs) based brain-computer interface (BCI) has received considerable attention due to its high information transfer rate (ITR) and available quantity of targets. However, the performance of frequency identification methods heavily hinges on the amount of user calibration data and data length, which hinders the deployment in real-world applications. Recently, generative adversarial networks (GANs)-based data generation methods have been widely adopted to create synthetic electroencephalography (EEG) data, holds promise to address these issues. In this paper, we proposed a GAN-based end-to-end signal transformation network for data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP signals into long-length artificial SSVEP signals. By incorporating a novel U-Net generator architecture and an auxiliary classifier into the network architecture, the TEGAN could produce conditioned features in the synthetic data. Additionally, we i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26694;&#26550;&#30340;&#31639;&#27861;SAFFE&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#20998;&#37197;&#26377;&#38480;&#36164;&#28304;&#32473;&#25259;&#38706;&#20854;&#38543;&#26426;&#38656;&#27714;&#30340;&#20195;&#29702;&#65292;&#24182;&#23454;&#29616;&#20844;&#24179;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAFFE&#22312;&#20844;&#24179;&#24615;&#21644;&#31038;&#20250;&#31119;&#21033;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#22810;&#31181;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.03758</link><description>&lt;p&gt;
&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26694;&#26550;&#30340;&#24207;&#21015;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Sequential Fair Resource Allocation under a Markov Decision Process Framework. (arXiv:2301.03758v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26694;&#26550;&#30340;&#31639;&#27861;SAFFE&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#20998;&#37197;&#26377;&#38480;&#36164;&#28304;&#32473;&#25259;&#38706;&#20854;&#38543;&#26426;&#38656;&#27714;&#30340;&#20195;&#29702;&#65292;&#24182;&#23454;&#29616;&#20844;&#24179;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAFFE&#22312;&#20844;&#24179;&#24615;&#21644;&#31038;&#20250;&#31119;&#21033;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#22810;&#31181;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#65292;&#23558;&#26377;&#38480;&#36164;&#28304;&#20998;&#37197;&#32473;&#20250;&#22312;&#21040;&#36798;&#26102;&#25259;&#38706;&#20854;&#38543;&#26426;&#38656;&#27714;&#30340;&#20195;&#29702;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#20844;&#24179;&#30340;&#20998;&#37197;&#31639;&#27861;&#65292;&#20197;&#29992;&#23613;&#21487;&#29992;&#30340;&#36164;&#28304;&#39044;&#31639;&#12290;&#22312;&#20449;&#24687;&#19981;&#23436;&#20840;&#30340;&#24773;&#20917;&#19979;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#36825;&#22312;&#24207;&#21015;&#35774;&#32622;&#20013;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#21046;&#23450;&#20026;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;SAFFE&#65292;&#23427;&#36890;&#36807;&#32771;&#34385;&#21040;&#27599;&#20010;&#21040;&#36798;&#26102;&#38388;&#30340;&#26410;&#26469;&#39044;&#26399;&#38656;&#27714;&#37327;&#65292;&#26681;&#25454;&#20840;&#38754;&#25259;&#38706;&#30340;&#38656;&#27714;&#23454;&#29616;&#20844;&#24179;&#20998;&#37197;&#12290;&#35813;&#31639;&#27861;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#20043;&#21518;&#65292;&#23601;&#33021;&#26681;&#25454;&#20195;&#29702;&#26410;&#26469;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#24403;&#21069;&#25259;&#38706;&#20986;&#26469;&#30340;&#38656;&#27714;&#20248;&#20808;&#20110;&#26410;&#26469;&#28508;&#22312;&#38656;&#27714;&#12290;&#20351;&#29992;MDP&#20844;&#24335;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SAFFE&#22522;&#20110;Nash&#31038;&#20250;&#31119;&#21033;&#20844;&#24179;&#24615;&#30446;&#26631;&#30340;&#19978;&#38480;&#65292;&#20197;&#21450;&#23558;&#20854;&#24046;&#36317;&#19982;&#26368;&#20248;&#24615;&#30340;&#32465;&#23450;&#20989;&#25968;&#20316;&#20026;&#20195;&#29702;&#25968;&#37327;&#21644;horizon&#38271;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#38656;&#27714;&#27169;&#24335;&#21644;&#23610;&#23544;&#30340;&#35774;&#32622;&#20013;&#65292;SAFFE&#23454;&#29616;&#20102;&#27604;&#20960;&#31181;&#22522;&#32447;&#26356;&#39640;&#30340;&#20844;&#24179;&#24615;&#21644;&#31038;&#20250;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the sequential decision-making problem of allocating a limited resource to agents that reveal their stochastic demands on arrival over a finite horizon. Our goal is to design fair allocation algorithms that exhaust the available resource budget. This is challenging in sequential settings where information on future demands is not available at the time of decision-making. We formulate the problem as a discrete time Markov decision process (MDP). We propose a new algorithm, SAFFE, that makes fair allocations with respect to the entire demands revealed over the horizon by accounting for expected future demands at each arrival time. The algorithm introduces regularization which enables the prioritization of current revealed demands over future potential demands depending on the uncertainty in agents' future demands. Using the MDP formulation, we show that SAFFE optimizes allocations based on an upper bound on the Nash Social Welfare fairness objective, and we bound its gap to opti
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#35777;&#26126;&#23545;&#20110;&#22343;&#26041;&#35823;&#24046;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20986;&#29616;&#30340;&#20840;&#23616;&#35299;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;&#31070;&#32463;&#22604;&#38519;&#30340;&#29305;&#24615;&#65292;&#21363;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#23849;&#28291;&#20026;&#31867;&#22343;&#20540;&#65292;&#32780;&#36825;&#20123;&#31867;&#22343;&#20540;&#26159;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#39030;&#28857;&#12290;</title><link>http://arxiv.org/abs/2301.00437</link><description>&lt;p&gt;
&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22604;&#38519;:&#20174;&#24179;&#34913;&#21040;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data. (arXiv:2301.00437v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00437
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#35777;&#26126;&#23545;&#20110;&#22343;&#26041;&#35823;&#24046;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20986;&#29616;&#30340;&#20840;&#23616;&#35299;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;&#31070;&#32463;&#22604;&#38519;&#30340;&#29305;&#24615;&#65292;&#21363;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#23849;&#28291;&#20026;&#31867;&#22343;&#20540;&#65292;&#32780;&#36825;&#20123;&#31867;&#22343;&#20540;&#26159;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#39030;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#22797;&#26434;&#31995;&#32479;&#22312;&#35757;&#32451;&#21040;&#25910;&#25947;&#26102;&#65292;&#23427;&#20204;&#30340;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#21644;&#20998;&#31867;&#22120;&#22312;&#32463;&#20856;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#30456;&#21516;&#30340;&#32467;&#26500;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#35266;&#23519;&#21040;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#23849;&#28291;&#20026;&#31867;&#22343;&#20540;&#65292;&#24182;&#19988;&#36825;&#20123;&#31867;&#22343;&#20540;&#26159;&#31561;&#35282;&#32039;&#26694;&#26550;(simplex Equiangular Tight Frame)&#30340;&#39030;&#28857;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#31070;&#32463;&#22604;&#38519;(NC)&#12290;&#26368;&#36817;&#30340;&#35770;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#31616;&#21270;&#30340;&#8220;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#8221;&#35757;&#32451;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#20013;&#20986;&#29616;&#20102;$\mathcal{NC}$&#12290;&#22312;&#36825;&#20010;&#35821;&#22659;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#22312;&#24120;&#29992;&#30340;&#22343;&#26041;&#35823;&#24046;(MSE)&#21644;&#20132;&#21449;&#29109;(CE)&#25439;&#22833;&#19979;&#65292;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#20250;&#21457;&#29983;$\mathcal{NC}$&#29616;&#35937;&#65292;&#34920;&#26126;&#20840;&#23616;&#35299;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;$\mathcal{NC}$&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep neural networks have achieved impressive performance on tasks from image classification to natural language processing. Surprisingly, these complex systems with massive amounts of parameters exhibit the same structural properties in their last-layer features and classifiers across canonical datasets when training until convergence. In particular, it has been observed that the last-layer features collapse to their class-means, and those class-means are the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is known as Neural Collapse ($\mathcal{NC}$). Recent papers have theoretically shown that $\mathcal{NC}$ emerges in the global minimizers of training problems with the simplified ``unconstrained feature model''. In this context, we take a step further and prove the $\mathcal{NC}$ occurrences in deep linear networks for the popular mean squared error (MSE) and cross entropy (CE) losses, showing that global solutions exhibit $\mathcal{NC}$ properties across
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#21333;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#30001;&#35838;&#31243;&#23450;&#20041;&#30340;&#22810;&#20219;&#21153;&#38382;&#39064;&#65292;&#35777;&#26126;&#22312;&#35838;&#31243;&#26377;&#36731;&#24494;&#27491;&#21017;&#21270;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#20381;&#27425;&#35299;&#20915;&#27599;&#20010;&#20219;&#21153;&#27604;&#30452;&#25509;&#35299;&#20915;&#21407;&#22987;&#21333;&#20219;&#21153;&#26356;&#21152;&#35745;&#31639;&#19978;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2212.12809</link><description>&lt;p&gt;
&#36890;&#36807;&#35838;&#31243;&#29702;&#35299;&#21333;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24230;&#25910;&#30410;
&lt;/p&gt;
&lt;p&gt;
Understanding the Complexity Gains of Single-Task RL with a Curriculum. (arXiv:2212.12809v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#21333;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#30001;&#35838;&#31243;&#23450;&#20041;&#30340;&#22810;&#20219;&#21153;&#38382;&#39064;&#65292;&#35777;&#26126;&#22312;&#35838;&#31243;&#26377;&#36731;&#24494;&#27491;&#21017;&#21270;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#20381;&#27425;&#35299;&#20915;&#27599;&#20010;&#20219;&#21153;&#27604;&#30452;&#25509;&#35299;&#20915;&#21407;&#22987;&#21333;&#20219;&#21153;&#26356;&#21152;&#35745;&#31639;&#19978;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#32463;&#36807;&#33391;&#22909;&#35774;&#35745;&#30340;&#22870;&#21169;&#26426;&#21046;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#25552;&#20986;&#20351;&#29992;&#19987;&#38376;&#30340;&#25506;&#32034;&#31574;&#30053;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#21478;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#23558;&#20854;&#37325;&#26032;&#26500;&#36896;&#20026;&#19968;&#20010;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20219;&#21153;&#31354;&#38388;&#19981;&#20165;&#21253;&#21547;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#36824;&#21253;&#21547;&#38544;&#21547;&#30340;&#35838;&#31243;&#20316;&#20026;&#36741;&#21161;&#12290;&#36825;&#26679;&#30340;&#37325;&#26032;&#26500;&#36896;&#25171;&#24320;&#20102;&#20351;&#29992;&#29616;&#26377;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#35299;&#20915;&#21333;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#21333;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#30001;&#35838;&#31243;&#23450;&#20041;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#35838;&#31243;&#26377;&#36731;&#24494;&#27491;&#21017;&#21270;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20381;&#27425;&#35299;&#20915;&#22810;&#20219;&#21153;RL&#38382;&#39064;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#27604;&#20174;&#22836;&#24320;&#22987;&#35299;&#20915;&#21407;&#22987;&#21333;&#20219;&#21153;&#38382;&#39064;&#26356;&#21152;&#35745;&#31639;&#19978;&#39640;&#25928;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26174;&#24335;&#30340;&#25506;&#32034;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) problems can be challenging without well-shaped rewards. Prior work on provably efficient RL methods generally proposes to address this issue with dedicated exploration strategies. However, another way to tackle this challenge is to reformulate it as a multi-task RL problem, where the task space contains not only the challenging task of interest but also easier tasks that implicitly function as a curriculum. Such a reformulation opens up the possibility of running existing multi-task RL methods as a more efficient alternative to solving a single challenging task from scratch. In this work, we provide a theoretical framework that reformulates a single-task RL problem as a multi-task RL problem defined by a curriculum. Under mild regularity conditions on the curriculum, we show that sequentially solving each task in the multi-task RL problem is more computationally efficient than solving the original single-task problem, without any explicit exploration bonuse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GAN&#30340;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#24182;&#25552;&#20986;&#20102;StyleGAN&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#30340;&#26032;&#30340;&#39640;&#25928;&#36731;&#37327;&#32423;&#21442;&#25968;&#21270;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2212.10229</link><description>&lt;p&gt;
StyleDomain&#65306;&#29992;&#20110;&#21333;&#27425;&#21644;&#23569;&#27425;&#39046;&#22495;&#36866;&#24212;&#30340;StyleGAN&#39640;&#25928;&#36731;&#37327;&#21270;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation. (arXiv:2212.10229v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GAN&#30340;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#24182;&#25552;&#20986;&#20102;StyleGAN&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#30340;&#26032;&#30340;&#39640;&#25928;&#36731;&#37327;&#32423;&#21442;&#25968;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GAN&#30340;&#39046;&#22495;&#36866;&#24212;&#26159;fine-tuning&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#20808;&#36827;GAN&#27169;&#22411;&#65288;&#20363;&#22914;StyleGAN&#65289;&#20197;&#36866;&#24212;&#20855;&#26377;&#23569;&#37327;&#26679;&#26412;&#30340;&#29305;&#23450;&#39046;&#22495;&#65288;&#20363;&#22914;&#32472;&#30011;&#38754;&#23380;&#12289;&#32032;&#25551;&#31561;&#65289;&#12290;&#26412;&#25991;&#23545;GAN&#30340;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#65288;&#29305;&#21035;&#26159;StyleGAN&#27169;&#22411;&#65289;&#36827;&#34892;&#31995;&#32479;&#21644;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;StyleGAN&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#30340;&#26032;&#30340;&#39640;&#25928;&#36731;&#37327;&#32423;&#21442;&#25968;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation of GANs is a problem of fine-tuning the state-of-the-art GAN models (e.g. StyleGAN) pretrained on a large dataset to a specific domain with few samples (e.g. painting faces, sketches, etc.). While there are a great number of methods that tackle this problem in different ways, there are still many important questions that remain unanswered.  In this paper, we provide a systematic and in-depth analysis of the domain adaptation problem of GANs, focusing on the StyleGAN model. First, we perform a detailed exploration of the most important parts of StyleGAN that are responsible for adapting the generator to a new domain depending on the similarity between the source and target domains. As a result of this in-depth study, we propose new efficient and lightweight parameterizations of StyleGAN for domain adaptation. Particularly, we show there exist directions in StyleSpace (StyleDomain directions) that are sufficient for adapting to similar domains and they can be reduced fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#28151;&#21512;&#21644;&#32431;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#20854;&#20013;&#65292;&#28151;&#21512;&#26041;&#27861;&#23558;&#20256;&#32479;PDE&#31163;&#25955;&#21270;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#32780;&#32431;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21017;&#20195;&#34920;&#20026;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08989</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;&#12289;&#29616;&#29366;&#19982;&#32463;&#20856;
&lt;/p&gt;
&lt;p&gt;
Deep learning applied to computational mechanics: A comprehensive review, state of the art, and the classics. (arXiv:2212.08989v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#28151;&#21512;&#21644;&#32431;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#20854;&#20013;&#65292;&#28151;&#21512;&#26041;&#27861;&#23558;&#20256;&#32479;PDE&#31163;&#25955;&#21270;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#32780;&#32431;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21017;&#20195;&#34920;&#20026;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#28145;&#24230;&#23398;&#20064;&#22312;&#35745;&#31639;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#28151;&#21512;&#21644;&#32431;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#28151;&#21512;&#26041;&#27861;&#23558;&#20256;&#32479;PDE&#31163;&#25955;&#21270;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#32431;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21017;&#20195;&#34920;&#20026;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Three recent breakthroughs due to AI in arts and science serve as motivation: An award winning digital image, protein folding, fast matrix multiplication. Many recent developments in artificial neural networks, particularly deep learning (DL), applied and relevant to computational mechanics (solid, fluids, finite-element technology) are reviewed in detail. Both hybrid and pure machine learning (ML) methods are discussed. Hybrid methods combine traditional PDE discretizations with ML methods either (1) to help model complex nonlinear constitutive relations, (2) to nonlinearly reduce the model order for efficient simulation (turbulence), or (3) to accelerate the simulation by predicting certain components in the traditional integration methods. Here, methods (1) and (2) relied on Long-Short-Term Memory (LSTM) architecture, with method (3) relying on convolutional neural networks. Pure ML methods to solve (nonlinear) PDEs are represented by Physics-Informed Neural network (PINN) methods, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#38544;&#24335;k&#31354;&#38388;&#30340;&#26080;&#20998;bin&#38750;&#31515;&#21345;&#23572;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#20010;&#36830;&#32493;&#12289;&#26080;&#20998;bin&#20197;&#21450;&#20010;&#20307;&#21270;&#30340;k&#31354;&#38388;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21453;&#20613;&#37324;&#21494;&#21464;&#25442;&#24674;&#22797;&#22270;&#20687;&#65292;&#28040;&#38500;&#23494;&#24230;&#34917;&#20607;&#21644;&#36153;&#29992;&#26114;&#36149;&#30340;&#38750;&#22343;&#21248;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2212.08479</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#38544;&#24335;k&#31354;&#38388;&#30340;&#26080;&#20998;bin&#38750;&#31515;&#21345;&#23572;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit k-Space for Binning-free Non-Cartesian Cardiac MR Imaging. (arXiv:2212.08479v5 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#38544;&#24335;k&#31354;&#38388;&#30340;&#26080;&#20998;bin&#38750;&#31515;&#21345;&#23572;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#20010;&#36830;&#32493;&#12289;&#26080;&#20998;bin&#20197;&#21450;&#20010;&#20307;&#21270;&#30340;k&#31354;&#38388;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21453;&#20613;&#37324;&#21494;&#21464;&#25442;&#24674;&#22797;&#22270;&#20687;&#65292;&#28040;&#38500;&#23494;&#24230;&#34917;&#20607;&#21644;&#36153;&#29992;&#26114;&#36149;&#30340;&#38750;&#22343;&#21248;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#37325;&#24314;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#22312;k&#31354;&#38388;&#20013;&#23398;&#20064;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#30340;ECG&#35302;&#21457;&#38750;&#31515;&#21345;&#23572;&#26041;&#24335;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#38656;&#35201;&#23558;&#30456;&#37051;&#26102;&#38388;&#28857;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;bin&#65292;&#20197;&#37325;&#24314;&#24515;&#33039;&#36816;&#21160;&#30340;&#19968;&#20010;&#30456;&#20301;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20102;&#19968;&#20010;&#36830;&#32493;&#12289;&#26080;&#20998;bin &#20197;&#21450;&#20010;&#20307;&#21270;&#30340;k&#31354;&#38388;&#34920;&#31034;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#37319;&#26679;&#30340;k&#31354;&#38388;&#28857;&#20998;&#37197;&#19968;&#20010;&#29420;&#29305;&#30340;&#22352;&#26631;&#65292;&#30001;&#26102;&#38388;&#12289;&#32447;&#22280;&#25351;&#25968;&#21644;&#39057;&#29575;&#22495;&#20301;&#32622;&#32452;&#25104;&#12290;&#28982;&#21518;&#20351;&#29992;&#24102;&#26377;&#39057;&#22495;&#27491;&#21017;&#21270;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#20174;&#36825;&#20123;&#21807;&#19968;&#22352;&#26631;&#21040;k&#31354;&#38388;&#24378;&#24230;&#30340;&#20010;&#20307;&#21270;&#26144;&#23556;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#31515;&#21345;&#23572;&#22352;&#26631;&#21644;&#20219;&#24847;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#23436;&#25972;k&#31354;&#38388;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#20613;&#37324;&#21494;&#21453;&#21464;&#25442;&#21363;&#21487;&#24674;&#22797;&#22270;&#20687;&#65292;&#28040;&#38500;&#20102;&#23545;&#23494;&#24230;&#34917;&#20607;&#21644;&#26114;&#36149;&#30340;&#38750;&#22343;&#21248;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel image reconstruction framework that directly learns a neural implicit representation in k-space for ECG-triggered non-Cartesian Cardiac Magnetic Resonance Imaging (CMR). While existing methods bin acquired data from neighboring time points to reconstruct one phase of the cardiac motion, our framework allows for a continuous, binning-free, and subject-specific k-space representation.We assign a unique coordinate that consists of time, coil index, and frequency domain location to each sampled k-space point. We then learn the subject-specific mapping from these unique coordinates to k-space intensities using a multi-layer perceptron with frequency domain regularization. During inference, we obtain a complete k-space for Cartesian coordinates and an arbitrary temporal resolution. A simple inverse Fourier transform recovers the image, eliminating the need for density compensation and costly non-uniform Fourier transforms for non-Cartesian data. This novel im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#23545;&#39640;&#32500;&#27969;&#25968;&#25454;&#21644;&#22797;&#26434;&#23494;&#24230;&#25361;&#25112;&#30340;&#24555;&#36895;&#22312;&#32447;&#21442;&#25968;&#20272;&#35745;&#31639;&#27861;&#65292;&#24182;&#21033;&#29992;&#28789;&#27963;&#32465;&#23450;&#22240;&#23376;&#20998;&#35299;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#38454;&#38543;&#26426;&#20248;&#21270;&#21644;&#26032;&#30340;&#38543;&#26426;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.05402</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#28789;&#27963;&#32465;&#23450;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#38543;&#26426;&#19968;&#38454;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stochastic First-Order Learning for Large-Scale Flexibly Tied Gaussian Mixture Model. (arXiv:2212.05402v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#23545;&#39640;&#32500;&#27969;&#25968;&#25454;&#21644;&#22797;&#26434;&#23494;&#24230;&#25361;&#25112;&#30340;&#24555;&#36895;&#22312;&#32447;&#21442;&#25968;&#20272;&#35745;&#31639;&#27861;&#65292;&#24182;&#21033;&#29992;&#28789;&#27963;&#32465;&#23450;&#22240;&#23376;&#20998;&#35299;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#38454;&#38543;&#26426;&#20248;&#21270;&#21644;&#26032;&#30340;&#38543;&#26426;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;(GMM)&#26159;&#19968;&#31181;&#22522;&#20110;&#26680;&#27169;&#22411;&#30340;&#26368;&#26377;&#25928;&#30340;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#12290;&#38543;&#30528;&#25968;&#25454;&#28304;&#30340;&#21095;&#22686;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#65292;&#22312;&#38754;&#23545;&#39640;&#32500;&#21644;&#27969;&#25968;&#25454;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#22797;&#26434;&#30340;&#23494;&#24230;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#39640;&#26031;&#32452;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#22312;&#32447;&#21442;&#25968;&#20272;&#35745;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#38454;&#38543;&#26426;&#20248;&#21270;&#26469;&#22788;&#29702;GMM&#25152;&#38754;&#20020;&#30340;&#39640;&#32500;&#27969;&#25968;&#25454;&#21644;&#22797;&#26434;&#23494;&#24230;&#30340;&#25361;&#25112;&#65292;&#21033;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#28789;&#27963;&#32465;&#23450;&#22240;&#23376;&#20998;&#35299;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#65292;&#20445;&#25345;&#27491;&#20132;&#24615;&#65292;&#24182;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#25968;&#20540;&#20248;&#21270;&#19968;&#36215;&#20351;&#29992;&#12290;&#22312;&#21512;&#25104;&#21644;&#23454;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian Mixture Models (GMM) are one of the most potent parametric density estimators based on the kernel model that finds application in many scientific domains. In recent years, with the dramatic enlargement of data sources, typical machine learning algorithms, e.g. Expectation Maximization (EM), encounters difficulty with high-dimensional and streaming data. Moreover, complicated densities often demand a large number of Gaussian components. This paper proposes a fast online parameter estimation algorithm for GMM by using first-order stochastic optimization. This approach provides a framework to cope with the challenges of GMM when faced with high-dimensional streaming data and complex densities by leveraging the flexibly-tied factorization of the covariance matrix. A new stochastic Manifold optimization algorithm that preserves the orthogonality is introduced and used along with the well-known Euclidean space numerical optimization. Numerous empirical results on both synthetic and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Custom Diffusion&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#30340;&#25991;&#26412;&#22270;&#20687;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#27010;&#24565;&#23450;&#21046;&#21270;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#20248;&#21270;&#25991;&#26412;&#22270;&#20687;&#35843;&#33410;&#26426;&#21046;&#20013;&#30340;&#20960;&#20010;&#21442;&#25968;&#23601;&#21487;&#20197;&#34920;&#31034;&#26032;&#27010;&#24565;&#24182;&#23454;&#29616;&#24555;&#36895;&#35843;&#25972;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#32852;&#21512;&#35757;&#32451;&#22810;&#20010;&#27010;&#24565;&#25110;&#23558;&#22810;&#20010;&#31934;&#35843;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#65292;&#24182;&#22312;&#29983;&#25104;&#22810;&#20010;&#26032;&#27010;&#24565;&#30340;&#21464;&#24322;&#24182;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#27010;&#24565;&#26080;&#32541;&#22320;&#32452;&#21512;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2212.04488</link><description>&lt;p&gt;
&#22810;&#27010;&#24565;&#23450;&#21046;&#21270;&#30340;&#25991;&#26412;&#22270;&#20687;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Multi-Concept Customization of Text-to-Image Diffusion. (arXiv:2212.04488v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Custom Diffusion&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#30340;&#25991;&#26412;&#22270;&#20687;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#27010;&#24565;&#23450;&#21046;&#21270;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#20248;&#21270;&#25991;&#26412;&#22270;&#20687;&#35843;&#33410;&#26426;&#21046;&#20013;&#30340;&#20960;&#20010;&#21442;&#25968;&#23601;&#21487;&#20197;&#34920;&#31034;&#26032;&#27010;&#24565;&#24182;&#23454;&#29616;&#24555;&#36895;&#35843;&#25972;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#32852;&#21512;&#35757;&#32451;&#22810;&#20010;&#27010;&#24565;&#25110;&#23558;&#22810;&#20010;&#31934;&#35843;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#65292;&#24182;&#22312;&#29983;&#25104;&#22810;&#20010;&#26032;&#27010;&#24565;&#30340;&#21464;&#24322;&#24182;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#27010;&#24565;&#26080;&#32541;&#22320;&#32452;&#21512;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#20013;&#23398;&#20064;&#21040;&#39640;&#36136;&#37327;&#30340;&#27010;&#24565;&#22270;&#20687;&#65292;&#20294;&#29992;&#25143;&#36890;&#24120;&#24076;&#26395;&#21512;&#25104;&#20182;&#20204;&#33258;&#24049;&#30340;&#27010;&#24565;&#30340;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#20182;&#20204;&#30340;&#23478;&#24237;&#12289;&#23456;&#29289;&#25110;&#29289;&#21697;&#65289;&#12290;&#25105;&#20204;&#33021;&#21542;&#25945;&#20250;&#27169;&#22411;&#24555;&#36895;&#33719;&#21462;&#26032;&#30340;&#27010;&#24565;&#65292;&#21482;&#32473;&#20986;&#20960;&#20010;&#31034;&#20363;&#65311;&#27492;&#22806;&#65292;&#25105;&#20204;&#33021;&#21542;&#32452;&#21512;&#22810;&#20010;&#26032;&#27010;&#24565;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#23450;&#21046;&#21270;&#25193;&#25955;&#65292;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#30340;&#25991;&#26412;&#22270;&#20687;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#20248;&#21270;&#25991;&#26412;&#22270;&#20687;&#35843;&#33410;&#26426;&#21046;&#20013;&#30340;&#20960;&#20010;&#21442;&#25968;&#23601;&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#34920;&#31034;&#26032;&#27010;&#24565;&#65292;&#24182;&#33021;&#23454;&#29616;&#24555;&#36895;&#35843;&#25972;&#65288;&#32422;6&#20998;&#38047;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#38381;&#21512;&#24418;&#24335;&#32422;&#26463;&#20248;&#21270;&#65292;&#21487;&#20197;&#32852;&#21512;&#35757;&#32451;&#22810;&#20010;&#27010;&#24565;&#25110;&#23558;&#22810;&#20010;&#31934;&#35843;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#12290;&#25105;&#20204;&#30340;&#31934;&#35843;&#27169;&#22411;&#29983;&#25104;&#22810;&#20010;&#26032;&#27010;&#24565;&#30340;&#21464;&#24322;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#27010;&#24565;&#26080;&#32541;&#22320;&#32452;&#21512;&#65292;&#24418;&#25104;&#26032;&#39062;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25110;&#19982;&#22810;&#20010;&#22522;&#32447;&#21644;&#24182;&#21457;&#20316;&#21697;&#24182;&#39550;&#40784;&#39537;&#12290;
&lt;/p&gt;
&lt;p&gt;
While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (~6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#21521;LSTM&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#40644;&#37329;&#21644;&#27604;&#29305;&#24065;&#30340;&#20215;&#26684;&#39044;&#27979;&#65292;&#32467;&#21512;&#20256;&#32479;&#30340;&#25216;&#26415;&#22240;&#32032;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#24320;&#21457;&#20102;&#22240;&#32032;&#65292;&#20351;&#29992;&#39044;&#27979;&#32467;&#26524;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#22238;&#25253;&#65292;&#19988;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2212.03443</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#21521;LSTM&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bi-LSTM Price Prediction based on Attention Mechanism. (arXiv:2212.03443v2 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#21521;LSTM&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#40644;&#37329;&#21644;&#27604;&#29305;&#24065;&#30340;&#20215;&#26684;&#39044;&#27979;&#65292;&#32467;&#21512;&#20256;&#32479;&#30340;&#25216;&#26415;&#22240;&#32032;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#24320;&#21457;&#20102;&#22240;&#32032;&#65292;&#20351;&#29992;&#39044;&#27979;&#32467;&#26524;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#22238;&#25253;&#65292;&#19988;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37329;&#34701;&#34893;&#29983;&#21697;&#24066;&#22330;&#30340;&#26085;&#30410;&#20016;&#23500;&#21644;&#21457;&#23637;&#65292;&#20132;&#26131;&#39057;&#29575;&#20063;&#36234;&#26469;&#36234;&#24555;&#12290;&#30001;&#20110;&#20154;&#31867;&#30340;&#38480;&#21046;&#65292;&#31639;&#27861;&#21644;&#33258;&#21160;&#20132;&#26131;&#26368;&#36817;&#25104;&#20026;&#20102;&#35752;&#35770;&#30340;&#28966;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#21521;LSTM&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#22522;&#20110;&#20004;&#31181;&#28909;&#38376;&#36164;&#20135;&#65306;&#40644;&#37329;&#21644;&#27604;&#29305;&#24065;&#12290;&#22312;&#29305;&#24449;&#24037;&#31243;&#26041;&#38754;&#65292;&#25105;&#20204;&#19968;&#26041;&#38754;&#28155;&#21152;&#20102;&#20256;&#32479;&#30340;&#25216;&#26415;&#22240;&#32032;&#65292;&#21516;&#26102;&#32467;&#21512;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#24320;&#21457;&#20102;&#22240;&#32032;&#12290;&#22312;&#27169;&#22411;&#21442;&#25968;&#30340;&#36873;&#25321;&#19978;&#65292;&#25105;&#20204;&#26368;&#32456;&#36873;&#25321;&#20102;&#19968;&#20010;&#21452;&#23618;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#12290;&#26681;&#25454;AUC&#24230;&#37327;&#65292;&#27604;&#29305;&#24065;&#21644;&#40644;&#37329;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;71.94&#65285;&#21644;73.03&#65285;&#12290;&#20351;&#29992;&#39044;&#27979;&#32467;&#26524;&#65292;&#22312;&#20004;&#24180;&#20869;&#33719;&#24471;&#20102;1089.34&#65285;&#30340;&#22238;&#25253;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23558;&#26412;&#25991;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;Bi-LSTM&#27169;&#22411;&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#39044;&#27979;&#38382;&#39064;&#20013;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing enrichment and development of the financial derivatives market, the frequency of transactions is also faster and faster. Due to human limitations, algorithms and automatic trading have recently become the focus of discussion. In this paper, we propose a bidirectional LSTM neural network based on an attention mechanism, which is based on two popular assets, gold and bitcoin. In terms of Feature Engineering, on the one hand, we add traditional technical factors, and at the same time, we combine time series models to develop factors. In the selection of model parameters, we finally chose a two-layer deep learning network. According to AUC measurement, the accuracy of bitcoin and gold is 71.94% and 73.03% respectively. Using the forecast results, we achieved a return of 1089.34% in two years. At the same time, we also compare the attention Bi-LSTM model proposed in this paper with the traditional model, and the results show that our model has the best performance in thi
&lt;/p&gt;</description></item><item><title>&#20174;&#29289;&#29702;&#23398;&#30340;&#35282;&#24230;&#23558;&#36830;&#32493;&#23398;&#20064;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;Franz-Parisi&#28909;&#21147;&#23398;&#21183;&#30340;&#26694;&#26550;&#65292;&#23558;&#20043;&#21069;&#23398;&#20064;&#21040;&#30340;&#20219;&#21153;&#20316;&#20026;&#20808;&#39564;&#21644;&#21442;&#32771;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#22330;&#31354;&#38388;&#20013;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#23398;&#20064;&#35774;&#32622;&#65292;&#29992;&#20110;&#35843;&#33410;&#20219;&#21153;&#38388;&#30340;&#31361;&#35302;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2212.02846</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#32479;&#35745;&#21147;&#23398;:&#21464;&#20998;&#21407;&#29702;&#21644;&#24179;&#22343;&#22330;&#21183;
&lt;/p&gt;
&lt;p&gt;
Statistical mechanics of continual learning: variational principle and mean-field potential. (arXiv:2212.02846v3 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02846
&lt;/p&gt;
&lt;p&gt;
&#20174;&#29289;&#29702;&#23398;&#30340;&#35282;&#24230;&#23558;&#36830;&#32493;&#23398;&#20064;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;Franz-Parisi&#28909;&#21147;&#23398;&#21183;&#30340;&#26694;&#26550;&#65292;&#23558;&#20043;&#21069;&#23398;&#20064;&#21040;&#30340;&#20219;&#21153;&#20316;&#20026;&#20808;&#39564;&#21644;&#21442;&#32771;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#22330;&#31354;&#38388;&#20013;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#23398;&#20064;&#35774;&#32622;&#65292;&#29992;&#20110;&#35843;&#33410;&#20219;&#21153;&#38388;&#30340;&#31361;&#35302;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#19968;&#20010;&#38556;&#30861;&#26159;&#22810;&#31181;&#19981;&#21516;&#20219;&#21153;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#28041;&#21450;&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#21508;&#31181;&#21551;&#21457;&#24615;&#25216;&#24039;&#34987;&#25552;&#20986;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#26412;&#25991;&#20851;&#27880;&#20108;&#20803;&#26435;&#37325;&#30340;&#21333;&#23618;&#21644;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21464;&#20998;&#36125;&#21494;&#26031;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#22312;&#22330;&#31354;&#38388;&#32780;&#19981;&#26159;&#28176;&#21464;&#26410;&#23450;&#20041;&#30340;&#31163;&#25955;&#26435;&#37325;&#31354;&#38388;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33258;&#28982;&#22320;&#23558;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#21512;&#24182;&#65292;&#24182;&#35843;&#33410;&#20219;&#21153;&#38388;&#30340;&#31361;&#35302;&#36164;&#28304;&#12290;&#20174;&#29289;&#29702;&#23398;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#23558;&#21464;&#20998;&#30340;&#36830;&#32493;&#23398;&#20064;&#36716;&#21270;&#20026;Franz-Parisi&#28909;&#21147;&#23398;&#21183;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20197;&#21069;&#30340;&#20219;&#21153;&#30693;&#35782;&#20805;&#24403;&#20808;&#39564;&#21644;&#21442;&#32771;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#20013;&#30340;&#20108;&#20803;&#24863;&#30693;&#22120;&#30340;&#36830;&#32493;&#23398;&#20064;&#35299;&#37322;&#20026;&#19968;&#20010;Franz-Parisi&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
An obstacle to artificial general intelligence is set by the continual learning of multiple tasks of different nature. Recently, various heuristic tricks, both from machine learning and from neuroscience angles, were proposed, but they lack a unified theory ground. Here, we focus on the continual learning in single-layered and multi-layered neural networks of binary weights. A variational Bayesian learning setting is thus proposed, where the neural network is trained in a field-space, rather than the gradient-ill-defined discrete-weight space, and furthermore, the weight uncertainty is naturally incorporated, and modulates the synaptic resources among tasks. From a physics perspective, we translate the variational continual learning into the Franz-Parisi thermodynamic potential framework, where the previous task knowledge acts as a prior and a reference as well. We thus interprete the continual learning of the binary perceptron in a teacher-student setting as a Franz-Parisi potential c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22312;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#30340;DNN&#25216;&#26415;&#65292;&#21253;&#25324;&#20174;&#29305;&#24449;&#25552;&#21462;&#21040;&#27169;&#22411;&#35757;&#32451;&#30340;&#25972;&#20010;&#22686;&#24378;&#27969;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#39640;&#22686;&#24378;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.00369</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#25216;&#26415;&#65306;&#26368;&#26032;&#30740;&#31350;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Deep neural network techniques for monaural speech enhancement: state of the art analysis. (arXiv:2212.00369v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22312;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#30340;DNN&#25216;&#26415;&#65292;&#21253;&#25324;&#20174;&#29305;&#24449;&#25552;&#21462;&#21040;&#27169;&#22411;&#35757;&#32451;&#30340;&#25972;&#20010;&#22686;&#24378;&#27969;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#39640;&#22686;&#24378;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#22270;&#20687;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#30001;&#20110;&#20854;&#25104;&#21151;&#65292;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#26415;&#24050;&#32463;&#24212;&#29992;&#20110;&#38899;&#39057;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DNN&#27169;&#22411;&#24050;&#34987;&#24212;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#65292;&#20197;&#23454;&#29616;&#22312;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#21435;&#22122;&#12289;&#21435;&#28151;&#21709;&#21644;&#22810;&#25196;&#22768;&#22120;&#20998;&#31163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#20123;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#20027;&#23548;DNN&#25216;&#26415;&#12290;&#22238;&#39038;&#20102;&#20174;&#29305;&#24449;&#25277;&#21462;&#12289;&#22522;&#20110;DNN&#30340;&#24037;&#20855;&#22914;&#20309;&#24314;&#27169;&#35821;&#38899;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#21040;&#27169;&#22411;&#35757;&#32451;&#65288;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#65289;&#30340;&#25972;&#20010;&#35821;&#38899;&#22686;&#24378;&#27969;&#31243;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#20351;&#29992;&#35821;&#38899;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#22686;&#24378;&#35821;&#38899;&#22686;&#24378;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#28085;&#30422;DNN&#22312;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#20027;&#35201;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) techniques have become pervasive in domains such as natural language processing and computer vision. They have achieved great success in these domains in task such as machine translation and image generation. Due to their success, these data driven techniques have been applied in audio domain. More specifically, DNN models have been applied in speech enhancement domain to achieve denosing, dereverberation and multi-speaker separation in monaural speech enhancement. In this paper, we review some dominant DNN techniques being employed to achieve speech separation. The review looks at the whole pipeline of speech enhancement from feature extraction, how DNN based tools are modelling both global and local features of speech and model training (supervised and unsupervised). We also review the use of speech-enhancement pre-trained models to boost speech enhancement process. The review is geared towards covering the dominant trends with regards to DNN application in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#33539;&#30068;&#35770;&#25506;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#65292;&#21482;&#35201;&#20801;&#35768;&#24494;&#35843;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.16327</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Power of Foundation Models. (arXiv:2211.16327v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#33539;&#30068;&#35770;&#25506;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#65292;&#21482;&#35201;&#20801;&#35768;&#24494;&#35843;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#26080;&#38480;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#28857;&#12289;&#26080;&#38480;&#35745;&#31639;&#33021;&#21147;&#12289;&#19968;&#20010;&#26080;&#38480;&#22823;&#30340;&#23436;&#32654;&#35757;&#32451;&#31639;&#27861;&#12289;&#20197;&#21450;&#22312;&#39044;&#35774;&#20219;&#21153;&#19978;&#20445;&#35777;&#38646;&#27867;&#21270;&#35823;&#24046;&#65292;&#37027;&#20040;&#23427;&#21487;&#20197;&#29992;&#20110;&#19968;&#20999;&#21527;&#65311;&#20256;&#32479;&#30340;&#34920;&#31034;&#12289;&#20248;&#21270;&#25110;&#27867;&#21270;&#29702;&#35770;&#26080;&#27861;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#25506;&#35752;&#30340;&#38382;&#39064;&#22312;&#36825;&#37324;&#37117;&#26159;&#19981;&#23384;&#22312;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#33539;&#30068;&#35770;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#20197;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19977;&#20010;&#32467;&#26524;&#65292;&#31532;&#19968;&#20010;&#38480;&#21046;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21363;&#20165;&#24403;&#20219;&#21153;&#21487;&#34920;&#31034;&#26102;&#65292;&#27169;&#22411;&#25165;&#33021;&#29992;&#25552;&#31034;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65307;&#31532;&#20108;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#24494;&#35843;&#19981;&#21463;&#36825;&#20010;&#38480;&#21046;&#65292;&#22240;&#20026;&#19968;&#20010;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#65288;&#23545;&#31216;&#24615;&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#29702;&#35770;&#19978;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#32467;&#26524;&#21487;&#20197;&#30475;&#20316;&#26159;&#31532;&#20108;&#20010;&#32467;&#26524;&#30340;&#19968;&#33324;&#21270;&#65292;&#34920;&#26126;&#22914;&#26524;&#20801;&#35768;&#24494;&#35843;&#24182;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#65292;&#21017;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#23567;&#33021;&#21147;&#20063;&#36275;&#20197;&#35299;&#20915;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We have proved three results. The first one limits the power of prompt-based learning, saying that the model can solve a downstream task with prompts if and only if the task is representable. The second one says fine tuning does not have this limit, as a foundation model with the minimum required power (up to symmetry) can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources. Our final result can be se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32791;&#25955;&#29702;&#35770;&#20272;&#35745;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;Lipschitz&#24120;&#25968;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2211.15253</link><description>&lt;p&gt;
&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Lipschitz constant estimation for 1D convolutional neural networks. (arXiv:2211.15253v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32791;&#25955;&#29702;&#35770;&#20272;&#35745;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;Lipschitz&#24120;&#25968;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32791;&#25955;&#26041;&#27861;&#30340;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;Lipschitz&#24120;&#25968;&#20272;&#35745;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;&#27744;&#21270;&#25805;&#20316;&#30340;&#22686;&#37327;&#20108;&#27425;&#32422;&#26463;&#20998;&#26512;&#20102;&#21367;&#31215;&#12289;&#27744;&#21270;&#21644;&#20840;&#36830;&#25509;&#23618;&#30340;&#32791;&#25955;&#29305;&#24615;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#35299;&#20915;&#20174;&#32791;&#25955;&#29702;&#35770;&#20013;&#25512;&#23548;&#20986;&#30340;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#26469;&#20272;&#35745;&#36825;&#20123;&#26144;&#23556;&#30340;&#25340;&#25509;&#30340;Lipschitz&#24120;&#25968;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#23613;&#21487;&#33021;&#39640;&#25928;&#65292;&#25105;&#20204;&#21033;&#29992;&#21367;&#31215;&#23618;&#30340;&#32467;&#26500;&#65292;&#23558;&#36825;&#20123;&#26377;&#38480;&#20914;&#28608;&#21709;&#24212;&#28388;&#27874;&#22120;&#34920;&#31034;&#20026;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#22240;&#26524;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#23545;&#29366;&#24577;&#31354;&#38388;&#23454;&#29616;&#36827;&#34892;&#32791;&#25955;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20379;&#30340;&#31034;&#20363;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;Lipschitz&#30028;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a dissipativity-based method for Lipschitz constant estimation of 1D convolutional neural networks (CNNs). In particular, we analyze the dissipativity properties of convolutional, pooling, and fully connected layers making use of incremental quadratic constraints for nonlinear activation functions and pooling operations. The Lipschitz constant of the concatenation of these mappings is then estimated by solving a semidefinite program which we derive from dissipativity theory. To make our method as efficient as possible, we exploit the structure of convolutional layers by realizing these finite impulse response filters as causal dynamical systems in state space and carrying out the dissipativity analysis for the state space realizations. The examples we provide show that our Lipschitz bounds are advantageous in terms of accuracy and scalability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#20837;&#39069;&#22806;&#30340;&#20998;&#32452;&#21464;&#25442;&#26469;&#22686;&#24378;&#24120;&#35268;&#21367;&#31215;&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026; AugConv&#65292;&#21487;&#20197;&#24341;&#20837;&#26356;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#37096;&#32626;&#26102;&#30340;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2211.12514</link><description>&lt;p&gt;
AugOp&#65306;&#23558;&#21464;&#25442;&#27880;&#20837;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
AugOp: Inject Transformation into Neural Operator. (arXiv:2211.12514v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#20837;&#39069;&#22806;&#30340;&#20998;&#32452;&#21464;&#25442;&#26469;&#22686;&#24378;&#24120;&#35268;&#21367;&#31215;&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026; AugConv&#65292;&#21487;&#20197;&#24341;&#20837;&#26356;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#37096;&#32626;&#26102;&#30340;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#27880;&#20837;&#39069;&#22806;&#30340;&#20998;&#32452;&#21464;&#25442;&#26469;&#22686;&#24378;&#24120;&#35268;&#21367;&#31215;&#31639;&#23376;&#65292;&#24182;&#22312;&#25512;&#29702;&#26399;&#38388;&#24674;&#22797;&#23427;&#12290;&#31934;&#24515;&#36873;&#25321;&#30340;&#39069;&#22806;&#21464;&#25442;&#30830;&#20445;&#20854;&#21487;&#20197;&#19982;&#27599;&#20010;&#32452;&#20013;&#30340;&#24120;&#35268;&#21367;&#31215;&#21512;&#24182;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#26399;&#38388;&#19981;&#20250;&#25913;&#21464;&#24120;&#35268;&#21367;&#31215;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#19982;&#24120;&#35268;&#21367;&#31215;&#31639;&#23376;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;AugConv&#65289;&#21487;&#20197;&#24341;&#20837;&#26356;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#22312;&#35757;&#32451;&#26399;&#38388;&#30340;&#24615;&#33021;&#65292;&#20294;&#19981;&#20250;&#22686;&#21152;&#27169;&#22411;&#37096;&#32626;&#30340;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#12290;&#22522;&#20110; ResNet&#65292;&#25105;&#20204;&#21033;&#29992; AugConv &#26500;&#24314;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026; AugResNet&#12290;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598; Cifar-10 &#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;AugResNet &#22312;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20854;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a simple and general approach to augment regular convolution operator by injecting extra group-wise transformation during training and recover it during inference. Extra transformation is carefully selected to ensure it can be merged with regular convolution in each group and will not change the topological structure of regular convolution during inference. Compared with regular convolution operator, our approach (AugConv) can introduce larger learning capacity to improve model performance during training but will not increase extra computational overhead for model deployment. Based on ResNet, we utilize AugConv to build convolutional neural networks named AugResNet. Result on image classification dataset Cifar-10 shows that AugResNet outperforms its baseline in terms of model performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;AdaFocal&#65292;&#19968;&#31181;&#26032;&#30340;&#26657;&#20934;&#24863;&#30693;&#33258;&#36866;&#24212;Focal Loss&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11838</link><description>&lt;p&gt;
AdaFocal&#65306;&#26657;&#20934;&#24863;&#30693;&#33258;&#36866;&#24212;Focal Loss
&lt;/p&gt;
&lt;p&gt;
AdaFocal: Calibration-aware Adaptive Focal Loss. (arXiv:2211.11838v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AdaFocal&#65292;&#19968;&#31181;&#26032;&#30340;&#26657;&#20934;&#24863;&#30693;&#33258;&#36866;&#24212;Focal Loss&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#19982;&#27491;&#30830;&#27010;&#29575;&#19981;&#30456;&#31526;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#36739;&#20110;&#20132;&#21449;&#29109;&#65292;&#20351;&#29992;Focal Loss&#36827;&#34892;&#35757;&#32451;&#33021;&#26356;&#22909;&#22320;&#35299;&#20915;&#26657;&#20934;&#38382;&#39064;&#24182;&#20445;&#25345;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#12290;Focal Loss&#36890;&#36807;&#35843;&#25511;&#21442;&#25968;&#947;&#26469;&#35268;&#33539;&#27169;&#22411;&#39044;&#27979;&#30340;&#29109;&#65292;&#20174;&#32780;&#25233;&#21046;&#27169;&#22411;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26657;&#20934;&#24863;&#30693;&#33258;&#36866;&#24212;Focal Loss&#65292;&#31216;&#20026;AdaFocal&#65292;&#23427;&#21033;&#29992;&#20102;Focal Loss&#21644;Inverse-Focal Loss&#30340;&#26657;&#20934;&#24615;&#36136;&#65292;&#24182;&#26681;&#25454;&#21069;&#19968;&#27425;&#36845;&#20195;&#20013;&#30340;&#947;t&#33258;&#36866;&#24212;&#22320;&#20462;&#25913;&#19981;&#21516;&#26679;&#26412;&#32452;&#30340;&#947;t&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#35299;&#20915;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much recent work has been devoted to the problem of ensuring that a neural network's confidence scores match the true probability of being correct, i.e. the calibration problem. Of note, it was found that training with focal loss leads to better calibration than cross-entropy while achieving similar level of accuracy \cite{mukhoti2020}. This success stems from focal loss regularizing the entropy of the model's prediction (controlled by the parameter $\gamma$), thereby reining in the model's overconfidence. Further improvement is expected if $\gamma$ is selected independently for each training sample (Sample-Dependent Focal Loss (FLSD-53) \cite{mukhoti2020}). However, FLSD-53 is based on heuristics and does not generalize well. In this paper, we propose a calibration-aware adaptive focal loss called AdaFocal that utilizes the calibration properties of focal (and inverse-focal) loss and adaptively modifies $\gamma_t$ for different groups of samples based on $\gamma_{t-1}$ from the previo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#24352;&#22270;&#20687;&#25110;&#35270;&#39057;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SinFusion&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#22270;&#20687;/&#35270;&#39057;&#29305;&#23450;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#30340;&#36816;&#21160;&#21644;&#21160;&#24577;&#65292;&#29983;&#25104;&#30456;&#21516;&#21160;&#24577;&#22330;&#26223;&#30340;&#22810;&#26679;&#21270;&#26032;&#35270;&#39057;&#26679;&#26412;&#65292;&#23558;&#30701;&#35270;&#39057;&#25512;&#24191;&#20026;&#38271;&#35270;&#39057;&#65288;&#21521;&#21069;&#21644;&#21521;&#21518;&#65289;&#24182;&#25191;&#34892;&#35270;&#39057;&#19978;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2211.11743</link><description>&lt;p&gt;
SinFusion&#65306;&#22312;&#21333;&#24352;&#22270;&#20687;&#25110;&#35270;&#39057;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SinFusion: Training Diffusion Models on a Single Image or Video. (arXiv:2211.11743v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#24352;&#22270;&#20687;&#25110;&#35270;&#39057;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SinFusion&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#22270;&#20687;/&#35270;&#39057;&#29305;&#23450;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#30340;&#36816;&#21160;&#21644;&#21160;&#24577;&#65292;&#29983;&#25104;&#30456;&#21516;&#21160;&#24577;&#22330;&#26223;&#30340;&#22810;&#26679;&#21270;&#26032;&#35270;&#39057;&#26679;&#26412;&#65292;&#23558;&#30701;&#35270;&#39057;&#25512;&#24191;&#20026;&#38271;&#35270;&#39057;&#65288;&#21521;&#21069;&#21644;&#21521;&#21518;&#65289;&#24182;&#25191;&#34892;&#35270;&#39057;&#19978;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#36229;&#36807;&#20102;GAN&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#26159;&#22312;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#19981;&#33258;&#28982;&#22320;&#36866;&#24212;&#20110;&#25805;&#20316;&#32473;&#23450;&#30340;&#36755;&#20837;&#22270;&#20687;&#25110;&#35270;&#39057;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#25110;&#35270;&#39057;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#22270;&#20687;/&#35270;&#39057;&#29305;&#23450;&#25193;&#25955;&#27169;&#22411;&#65288;SinFusion&#65289;&#23398;&#20064;&#21333;&#20010;&#22270;&#20687;&#25110;&#35270;&#39057;&#30340;&#22806;&#35266;&#21644;&#21160;&#24577;&#65292;&#21516;&#26102;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26465;&#20214;&#33021;&#21147;&#12290;&#23427;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#22270;&#20687;/&#35270;&#39057;&#29305;&#23450;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#30340;&#36816;&#21160;&#21644;&#21160;&#24577;&#12290;&#28982;&#21518;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#30456;&#21516;&#21160;&#24577;&#22330;&#26223;&#30340;&#22810;&#26679;&#21270;&#26032;&#35270;&#39057;&#26679;&#26412;&#65292;&#23558;&#30701;&#35270;&#39057;&#25512;&#24191;&#20026;&#38271;&#35270;&#39057;&#65288;&#21521;&#21069;&#21644;&#21521;&#21518;&#65289;&#24182;&#25191;&#34892;&#35270;&#39057;&#19978;&#37319;&#26679;&#12290;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#22810;&#25968;&#37117;&#26080;&#27861;&#36890;&#36807;&#24403;&#21069;&#30340;&#35270;&#39057;&#29305;&#23450;&#29983;&#25104;&#26041;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models exhibited tremendous progress in image and video generation, exceeding GANs in quality and diversity. However, they are usually trained on very large datasets and are not naturally adapted to manipulate a given input image or video. In this paper we show how this can be resolved by training a diffusion model on a single input image or video. Our image/video-specific diffusion model (SinFusion) learns the appearance and dynamics of the single image or video, while utilizing the conditioning capabilities of diffusion models. It can solve a wide array of image/video-specific manipulation tasks. In particular, our model can learn from few frames the motion and dynamics of a single input video. It can then generate diverse new video samples of the same dynamic scene, extrapolate short videos into long ones (both forward and backward in time) and perform video upsampling. Most of these tasks are not realizable by current video-specific generation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRACE&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#32456;&#36523;&#27169;&#22411;&#32534;&#36753;&#65292;&#23427;&#36890;&#36807;&#22312;&#27969;&#24335;&#38169;&#35823;&#19978;&#25191;&#34892;&#30446;&#26631;&#32534;&#36753;&#26469;&#20462;&#22797;&#37096;&#32626;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#29983;&#25104;&#19968;&#20010;&#31163;&#25955;&#12289;&#26412;&#22320;&#30340;&#32534;&#36753;&#32534;&#30721;&#26412;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#65292;&#22312;&#36827;&#34892;&#25968;&#21315;&#20010;&#39034;&#24207;&#32534;&#36753;&#26102;&#34920;&#29616;&#20026;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11031</link><description>&lt;p&gt;
GRACE&#65306;&#31163;&#25955;&#38190;&#20540;&#36866;&#37197;&#22120;&#23454;&#29616;&#30340;&#32456;&#36523;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adapters. (arXiv:2211.11031v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRACE&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#32456;&#36523;&#27169;&#22411;&#32534;&#36753;&#65292;&#23427;&#36890;&#36807;&#22312;&#27969;&#24335;&#38169;&#35823;&#19978;&#25191;&#34892;&#30446;&#26631;&#32534;&#36753;&#26469;&#20462;&#22797;&#37096;&#32626;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#29983;&#25104;&#19968;&#20010;&#31163;&#25955;&#12289;&#26412;&#22320;&#30340;&#32534;&#36753;&#32534;&#30721;&#26412;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#65292;&#22312;&#36827;&#34892;&#25968;&#21315;&#20010;&#39034;&#24207;&#32534;&#36753;&#26102;&#34920;&#29616;&#20026;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#30340;&#27169;&#22411;&#38543;&#26102;&#38388;&#25512;&#31227;&#20250;&#34928;&#36864;&#65292;&#21407;&#22240;&#26159;&#36755;&#20837;&#30340;&#21464;&#21270;&#12289;&#29992;&#25143;&#38656;&#27714;&#19981;&#26029;&#25913;&#21464;&#12289;&#25110;&#30001;&#20110;&#20986;&#29616;&#30693;&#35782;&#31354;&#32570;&#12290;&#24403;&#21457;&#29616;&#26377;&#23475;&#34892;&#20026;&#26102;&#65292;&#38656;&#35201;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#22120;&#22312;&#22810;&#27425;&#32534;&#36753;&#20013;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GRACE&#65292;&#19968;&#31181;&#32456;&#36523;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#23427;&#22312;&#37096;&#32626;&#27169;&#22411;&#30340;&#27969;&#24335;&#38169;&#35823;&#19978;&#23454;&#29616;&#20102;&#38382;&#39064;&#20462;&#34917;&#65292;&#30830;&#20445;&#23545;&#19981;&#30456;&#20851;&#30340;&#36755;&#20837;&#30340;&#24433;&#21709;&#26368;&#23567;&#21270;&#12290;GRACE&#23558;&#26032;&#30340;&#26144;&#23556;&#39033;&#20889;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#31163;&#25955;&#30340;&#12289;&#26412;&#22320;&#30340;&#32534;&#30721;&#26412;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#21482;&#20351;&#29992;&#27969;&#24335;&#38169;&#35823;&#23454;&#29616;&#25968;&#21315;&#20010;&#39034;&#24207;&#32534;&#36753;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;T5&#12289;BERT&#21644;GPT&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;GRACE&#22312;&#36827;&#34892;&#24182;&#20445;&#30041;&#32534;&#36753;&#26041;&#38754;&#30340;&#24615;&#33021;&#22788;&#20110;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#21516;&#26102;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployed models decay over time due to shifting inputs, changing user needs, or emergent knowledge gaps. When harmful behaviors are identified, targeted edits are required. However, current model editors, which adjust specific behaviors of pre-trained models, degrade model performance over multiple edits. We propose GRACE, a Lifelong Model Editing method, which implements spot-fixes on streaming errors of a deployed model, ensuring minimal impact on unrelated inputs. GRACE writes new mappings into a pre-trained model's latent space, creating a discrete, local codebook of edits without altering model weights. This is the first method enabling thousands of sequential edits using only streaming errors. Our experiments on T5, BERT, and GPT models show GRACE's state-of-the-art performance in making and retaining edits, while generalizing to unseen inputs. Our code is available at https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace}.
&lt;/p&gt;</description></item><item><title>NVDiff&#21033;&#29992;&#20998;&#25968;&#20026;&#22522;&#30784;&#30340;&#29983;&#25104;&#27169;&#22411;&#37319;&#26679;&#33410;&#28857;&#21521;&#37327;&#26469;&#29983;&#25104;&#22270;&#24418;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#25193;&#25955;&#36807;&#31243;&#30340;&#32500;&#24230;&#65292;&#25552;&#39640;&#20102;&#37319;&#26679;&#36895;&#24230;&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.10794</link><description>&lt;p&gt;
NVDiff&#65306;&#36890;&#36807;&#33410;&#28857;&#21521;&#37327;&#25193;&#25955;&#29983;&#25104;&#22270;&#24418;
&lt;/p&gt;
&lt;p&gt;
NVDiff: Graph Generation through the Diffusion of Node Vectors. (arXiv:2211.10794v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10794
&lt;/p&gt;
&lt;p&gt;
NVDiff&#21033;&#29992;&#20998;&#25968;&#20026;&#22522;&#30784;&#30340;&#29983;&#25104;&#27169;&#22411;&#37319;&#26679;&#33410;&#28857;&#21521;&#37327;&#26469;&#29983;&#25104;&#22270;&#24418;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#25193;&#25955;&#36807;&#31243;&#30340;&#32500;&#24230;&#65292;&#25552;&#39640;&#20102;&#37319;&#26679;&#36895;&#24230;&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#22270;&#24418;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22270;&#24418;&#26159;&#19968;&#32452;&#25104;&#23545;&#36830;&#25509;&#30340;&#26080;&#24207;&#33410;&#28857;&#65292;&#32534;&#30721;&#20855;&#26377;&#22797;&#26434;&#32452;&#21512;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#25110;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20174;&#30456;&#21516;&#30340;&#36807;&#31243;&#24182;&#34892;&#29983;&#25104;&#33410;&#28857;&#21644;&#36793;&#65292;&#20854;&#32500;&#25968;&#26159;&#19981;&#24517;&#35201;&#22320;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NVDiff&#65292;&#37319;&#29992;VGAE&#32467;&#26500;&#65292;&#24182;&#20197;&#20998;&#25968;&#20026;&#22522;&#30784;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#20316;&#20026;&#28789;&#27963;&#30340;&#20808;&#39564;&#26469;&#37319;&#26679;&#33410;&#28857;&#21521;&#37327;&#12290;&#36890;&#36807;&#20165;&#22312;&#28508;&#31354;&#38388;&#20013;&#24314;&#27169;&#33410;&#28857;&#21521;&#37327;&#65292;NVDiff&#26174;&#30528;&#38477;&#20302;&#20102;&#25193;&#25955;&#36807;&#31243;&#30340;&#32500;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#37319;&#26679;&#36895;&#24230;&#12290;&#22522;&#20110;NVDiff&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20998;&#25968;&#32593;&#32476;&#65292;&#33021;&#22815;&#25429;&#25417;&#22270;&#24418;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;NVDiff&#26174;&#30528;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#65292;&#21487;&#20197;&#27169;&#25311;&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#22823;&#30340;&#22270;&#24418;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23427;&#20063;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to generate graphs is challenging as a graph is a set of pairwise connected, unordered nodes encoding complex combinatorial structures. Recently, several works have proposed graph generative models based on normalizing flows or score-based diffusion models. However, these models need to generate nodes and edges in parallel from the same process, whose dimensionality is unnecessarily high. We propose NVDiff, which takes the VGAE structure and uses a score-based generative model (SGM) as a flexible prior to sample node vectors. By modeling only node vectors in the latent space, NVDiff significantly reduces the dimension of the diffusion process and thus improves sampling speed. Built on the NVDiff framework, we introduce an attention-based score network capable of capturing both local and global contexts of graphs. Experiments indicate that NVDiff significantly reduces computations and can model much larger graphs than competing methods. At the same time, it achieves superior or
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#39057;&#20013;&#19981;&#21516;&#34917;&#19969;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#36873;&#25321;&#21253;&#21547;&#20016;&#23500;&#21160;&#24577;&#29305;&#24615;&#30340;&#26631;&#35760;&#65292;&#25918;&#24323;&#26080;&#25928;&#30340;&#26631;&#35760;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.10636</link><description>&lt;p&gt;
&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#23454;&#29616;&#39640;&#25928;&#35270;&#39057;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Video Representation Learning via Motion-Aware Token Selection. (arXiv:2211.10636v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10636
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#39057;&#20013;&#19981;&#21516;&#34917;&#19969;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#36873;&#25321;&#21253;&#21547;&#20016;&#23500;&#21160;&#24577;&#29305;&#24615;&#30340;&#26631;&#35760;&#65292;&#25918;&#24323;&#26080;&#25928;&#30340;&#26631;&#35760;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#33945;&#29256;&#35270;&#39057;&#24314;&#27169;&#25216;&#26415;&#36890;&#36807;&#22312;&#35270;&#39057;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38543;&#26426;&#33945;&#29256;&#31574;&#30053;&#23548;&#33268;&#39044;&#27979;&#26080;&#25928;&#30340;&#26631;&#35760;/&#24103;&#65292;&#36825;&#20123;&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#65292;&#38656;&#35201;&#26114;&#36149;&#30340;&#35745;&#31639;&#26426;&#21644;&#22823;&#37327;&#26174;&#21345;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#39057;&#34917;&#19969;&#20013;&#30340;&#19981;&#22343;&#21248;&#20449;&#24687;&#23494;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26631;&#35760;&#36873;&#25321;&#26041;&#27861;&#65306;MATS&#65306;&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#65292;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#25214;&#21040;&#21253;&#21547;&#20016;&#23500;&#21160;&#24577;&#29305;&#24615;&#30340;&#26631;&#35760;&#65292;&#24182;&#25918;&#24323;&#26080;&#25928;&#30340;&#26631;&#35760;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#24103;&#36873;&#25321;&#31574;&#30053;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20851;&#27880;&#26368;&#37325;&#35201;&#21644;&#22240;&#26524;&#24615;&#30340;&#24103;&#65292;&#24182;&#20351;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#24471;&#21040;&#26174;&#30528;&#38477;&#20302;&#65292;&#20351;&#24471;&#22312;&#21333;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently emerged Masked Video Modeling techniques demonstrated their potential by significantly outperforming previous methods in self-supervised learning for video. However, they require an excessive amount of computations and memory while predicting uninformative tokens/frames due to random masking strategies, requiring excessive computing power for training. (e.g., over 16 nodes with 128 NVIDIA A100 GPUs). To resolve this issue, we exploit the unequal information density among the patches in videos and propose a new token selection method, MATS: Motion-Aware Token Selection, that finds tokens containing rich motion features and drops uninformative ones during both self-supervised pre-training and fine-tuning. We further present an adaptive frame selection strategy that allows the model to focus on informative and causal frames with minimal redundancy. Our method significantly reduces computation and memory requirements, enabling the pre-training and fine-tuning on a single machine w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20256;&#36755;&#22810;&#38754;&#20307;&#19978;&#36827;&#34892;&#22312;&#32447;&#20984;&#30446;&#26631;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#27492;&#31639;&#27861;&#22522;&#20110;Sinkhorn&#30697;&#38453;&#32553;&#25918;&#21644;&#38236;&#20687;&#19979;&#38477;&#30340;&#21407;&#29702;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22122;&#38899;&#29615;&#22659;&#19979;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.10420</link><description>&lt;p&gt;
&#38236;&#20687;Sinkhorn&#65306;&#22312;&#20256;&#36755;&#22810;&#38754;&#20307;&#19978;&#36827;&#34892;&#24555;&#36895;&#22312;&#32447;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Mirror Sinkhorn: Fast Online Optimization on Transport Polytopes. (arXiv:2211.10420v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10420
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20256;&#36755;&#22810;&#38754;&#20307;&#19978;&#36827;&#34892;&#22312;&#32447;&#20984;&#30446;&#26631;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#27492;&#31639;&#27861;&#22522;&#20110;Sinkhorn&#30697;&#38453;&#32553;&#25918;&#21644;&#38236;&#20687;&#19979;&#38477;&#30340;&#21407;&#29702;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22122;&#38899;&#29615;&#22659;&#19979;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#36890;&#36807;&#22312;&#20256;&#36755;&#22810;&#38754;&#20307;&#19978;&#30340;&#32447;&#24615;&#35268;&#21010;&#26469;&#25429;&#25417;&#25968;&#25454;&#30340;&#20960;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;Sinkhorn&#30697;&#38453;&#32553;&#25918;&#21644;&#38236;&#20687;&#19979;&#38477;&#30340;&#21407;&#29702;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#19978;&#26368;&#23567;&#21270;&#19968;&#33324;&#20984;&#30446;&#26631;&#12290;&#35813;&#31639;&#27861;&#23545;&#22122;&#38899;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#22312;&#22312;&#32447;&#35774;&#32622;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20984;&#30446;&#26631;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport is an important tool in machine learning, allowing to capture geometric properties of the data through a linear program on transport polytopes. We present a single-loop optimization algorithm for minimizing general convex objectives on these domains, utilizing the principles of Sinkhorn matrix scaling and mirror descent. The proposed algorithm is robust to noise, and can be used in an online setting. We provide theoretical guarantees for convex objectives and experimental results showcasing it effectiveness on both synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRONOS&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#22522;&#20110;Wi-Fi CSI&#23454;&#29616;&#26080;&#20154;&#35774;&#22791;NLoS&#20154;&#20307;&#26816;&#27979;&#65292;&#21487;&#20197;&#21306;&#20998;&#25151;&#38388;&#20013;&#30340;&#31227;&#21160;&#20154;&#21592;&#21644;&#31354;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;NLoS&#26465;&#20214;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#25151;&#38388;&#20013;&#30340;&#20154;&#29289;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2211.10354</link><description>&lt;p&gt;
CRONOS&#65306;&#22522;&#20110;Wi-Fi CSI&#30340;&#26080;&#20154;&#35774;&#22791;NLoS&#20154;&#20307;&#26816;&#27979;&#30340;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CRONOS: Colorization and Contrastive Learning for Device-Free NLoS Human Presence Detection using Wi-Fi CSI. (arXiv:2211.10354v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRONOS&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#22522;&#20110;Wi-Fi CSI&#23454;&#29616;&#26080;&#20154;&#35774;&#22791;NLoS&#20154;&#20307;&#26816;&#27979;&#65292;&#21487;&#20197;&#21306;&#20998;&#25151;&#38388;&#20013;&#30340;&#31227;&#21160;&#20154;&#21592;&#21644;&#31354;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;NLoS&#26465;&#20214;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#25151;&#38388;&#20013;&#30340;&#20154;&#29289;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#20840;&#38754;&#26234;&#33021;&#21270;&#26381;&#21153;&#21644;&#24212;&#29992;&#30340;&#38656;&#27714;&#36805;&#36895;&#22686;&#38271;&#12290;&#36890;&#36807;&#20256;&#24863;&#22120;&#25110;&#25668;&#20687;&#22836;&#36827;&#34892;&#26080;&#20154;&#26816;&#27979;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#20197;&#21450;&#23545;&#38745;&#27490;&#20154;&#21592;&#30340;&#38169;&#35823;&#26816;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#21830;&#29992;Wi-Fi&#35774;&#22791;&#25429;&#33719;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;(CSI)&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20449;&#21495;&#29305;&#24449;&#65292;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#38750;&#30452;&#35270;(NLoS)&#21644;&#38745;&#24577;&#22330;&#26223;&#19979;&#23384;&#22312;&#20998;&#31867;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#24403;&#19968;&#20010;&#20154;&#38745;&#27490;&#31449;&#22312;&#25151;&#38388;&#35282;&#33853;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRONOS(&#22522;&#20110;&#24425;&#33394;&#21270;&#21644;&#23545;&#27604;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;NLoS&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;)&#30340;&#31995;&#32479;&#65292;&#23427;&#29983;&#25104;&#21160;&#24577;&#30340;&#22797;&#21457;&#22270;(RPs)&#21644;&#39068;&#33394;&#32534;&#30721;&#30340;CSI&#27604;&#29575;&#20197;&#21306;&#20998;&#25151;&#38388;&#20013;&#30340;&#31227;&#21160;&#20154;&#21592;&#21644;&#31354;&#32622;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#26816;&#32034;&#23454;&#36136;&#24615;&#30340;&#34920;&#24449;&#65292;&#20854;&#20013;&#21672;&#35810;&#25439;&#22833;&#34987;&#21046;&#23450;&#20026;&#21306;&#20998;&#21516;&#31867;&#21644;&#24322;&#31867;&#30340;&#23884;&#20837;&#28857;&#20043;&#38388;&#36317;&#31163;&#24230;&#37327;&#30340;&#25439;&#22833;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;NLoS&#26465;&#20214;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#25151;&#38388;&#20013;&#30340;&#20154;&#29289;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the demand for pervasive smart services and applications has increased rapidly. Device-free human detection through sensors or cameras has been widely adopted, but it comes with privacy issues as well as misdetection for motionless people. To address these drawbacks, channel state information (CSI) captured from commercialized Wi-Fi devices provides rich signal features for accurate detection. However, existing systems suffer from inaccurate classification under a non-line-of-sight (NLoS) and stationary scenario, such as when a person is standing still in a room corner. In this work, we propose a system called CRONOS (Colorization and Contrastive Learning Enhanced NLoS Human Presence Detection), which generates dynamic recurrence plots (RPs) and color-coded CSI ratios to distinguish mobile people from vacancy in a room, respectively. We also incorporate supervised contrastive learning to retrieve substantial representations, where consultation loss is formulated to dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#31934;&#24230;&#29615;&#22659;&#19979;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;SGD&#21464;&#31181;&#65292;&#24182;&#27979;&#35797;&#20102;&#19981;&#21516;&#21464;&#31181;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;SGD&#26041;&#27861;&#65292;&#22312;&#20302;&#31934;&#24230;&#31639;&#26415;&#29615;&#22659;&#19979;&#20351;&#29992;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#21464;&#31181;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.04655</link><description>&lt;p&gt;
&#20302;&#31934;&#24230;&#29615;&#22659;&#19979;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#25439;&#22833;&#20989;&#25968;&#30340;SGD&#21464;&#31181;
&lt;/p&gt;
&lt;p&gt;
Variants of SGD for Lipschitz Continuous Loss Functions in Low-Precision Environments. (arXiv:2211.04655v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#31934;&#24230;&#29615;&#22659;&#19979;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;SGD&#21464;&#31181;&#65292;&#24182;&#27979;&#35797;&#20102;&#19981;&#21516;&#21464;&#31181;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;SGD&#26041;&#27861;&#65292;&#22312;&#20302;&#31934;&#24230;&#31639;&#26415;&#29615;&#22659;&#19979;&#20351;&#29992;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#21464;&#31181;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#20301;&#28014;&#28857;&#21644;&#23450;&#28857;&#29615;&#22659;&#19979;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26102;&#20351;&#29992;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#21464;&#31181;&#30340;&#25910;&#25947;&#24615;&#12290;&#32771;&#34385;&#21040;&#19968;&#33324;&#38543;&#26426;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20551;&#35774;&#21482;&#33021;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#38543;&#26426;&#26799;&#24230;&#30340;&#36817;&#20284;&#20540;&#20197;&#21450;&#35745;&#31639;SGD&#27493;&#39588;&#26412;&#36523;&#26102;&#30340;&#35823;&#24046;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#28176;&#36817;&#25910;&#25947;&#21040;Clarke&#31283;&#23450;&#28857;&#30340;&#32467;&#26524;&#21644;&#21040;&#36817;&#20284;&#31283;&#23450;&#28857;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#12290;&#22312;&#21508;&#31181;&#20302;&#31934;&#24230;&#31639;&#26415;&#29615;&#22659;&#19979;&#32463;&#39564;&#22320;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;SGD&#21464;&#31181;&#65292;&#22312;&#20004;&#20010;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#19982;SGD&#30456;&#27604;&#35266;&#23519;&#21040;&#20102;&#25913;&#36827;&#30340;&#27979;&#35797;&#38598;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by neural network training in low-bit floating and fixed-point environments, this work studies the convergence of variants of SGD using adaptive step sizes with computational error. Considering a general stochastic Lipschitz continuous loss function, an asymptotic convergence result to a Clarke stationary point, and the non-asymptotic convergence to an approximate stationary point are presented assuming that only an approximation of the loss function's stochastic gradient can be computed, as well as error in computing the SGD step itself. Different variants of SGD are tested empirically in a variety of low-precision arithmetic environments, where improved test set accuracy is observed compared to SGD for two image recognition tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#35843;&#23433;&#20840;UCB(M-SafeUCB)&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21333;&#35843;&#24615;&#20551;&#35774;&#65292;&#21462;&#24471;&#20102;&#22312;&#20445;&#35777;&#31934;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.01561</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#20013;&#21333;&#35843;&#24615;&#30340;&#23433;&#20840;&#25506;&#32034;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Benefits of Monotonicity in Safe Exploration with Gaussian Processes. (arXiv:2211.01561v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#35843;&#23433;&#20840;UCB(M-SafeUCB)&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21333;&#35843;&#24615;&#20551;&#35774;&#65292;&#21462;&#24471;&#20102;&#22312;&#20445;&#35777;&#31934;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#30456;&#24212;&#30340;&#23433;&#20840;&#38408;&#20540;&#19979;&#39034;&#24207;&#22320;&#23547;&#25214;&#26410;&#30693;&#20989;&#25968;&#30340;&#26368;&#22823;&#20540;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;&#21644;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#24314;&#27169;&#20989;&#25968;&#65292;&#20294;&#20551;&#35774;&#35813;&#20989;&#25968;&#30456;&#23545;&#20110;&#8220;&#23433;&#20840;&#21464;&#37327;&#8221;&#26159;&#21333;&#35843;&#36882;&#22686;&#30340;&#65292;&#36825;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#12290;&#27492;&#20551;&#35774;&#21463;&#21040;&#20102;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#20363;&#22914;&#33258;&#36866;&#24212;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#21644;&#26426;&#22120;&#20154;&#23398;&#12290;&#25105;&#20204;&#20174;GP-UCB&#21644;SafeOpt&#31639;&#27861;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#35843;&#23433;&#20840;UCB(M-SafeUCB)&#30340;&#31639;&#27861;&#26469;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;M-SafeUCB&#22312;&#23433;&#20840;&#24615;&#12289;&#36866;&#24403;&#23450;&#20041;&#30340;&#21518;&#24724;&#27010;&#24565;&#21644;&#36817;&#20284;&#25214;&#21040;&#25972;&#20010;&#23433;&#20840;&#36793;&#30028;&#26041;&#38754;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35828;&#26126;&#65292;&#21333;&#35843;&#24615;&#20551;&#35774;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20063;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sequentially maximising an unknown function over a set of actions while ensuring that every sampled point has a function value below a given safety threshold. We model the function using kernel-based and Gaussian process methods, while differing from previous works in our assumption that the function is monotonically increasing with respect to a \emph{safety variable}. This assumption is motivated by various practical applications such as adaptive clinical trial design and robotics. Taking inspiration from the \textsc{\sffamily GP-UCB} and \textsc{\sffamily SafeOpt} algorithms, we propose an algorithm, monotone safe {\sffamily UCB} (\textsc{\sffamily M-SafeUCB}) for this task. We show that \textsc{\sffamily M-SafeUCB} enjoys theoretical guarantees in terms of safety, a suitably-defined regret notion, and approximately finding the entire safe boundary. In addition, we illustrate that the monotonicity assumption yields significant benefits in terms of the guara
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;&#20108;&#26631;&#20934;&#23376;&#27169;&#26368;&#22823;&#21270;&#8221;&#65292;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#12290;&#35813;&#38382;&#39064;&#35201;&#27714;&#25214;&#21040;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#35299;&#65292;&#20197;&#26368;&#22823;&#21270;&#25928;&#29992;&#20989;&#25968;&#20026;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2211.00980</link><description>&lt;p&gt;
&#22312;&#23376;&#27169;&#26368;&#22823;&#21270;&#20013;&#24179;&#34913;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#65288;&#25216;&#26415;&#25253;&#21578;&#65289;
&lt;/p&gt;
&lt;p&gt;
Balancing Utility and Fairness in Submodular Maximization (Technical Report). (arXiv:2211.00980v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;&#20108;&#26631;&#20934;&#23376;&#27169;&#26368;&#22823;&#21270;&#8221;&#65292;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#12290;&#35813;&#38382;&#39064;&#35201;&#27714;&#25214;&#21040;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#35299;&#65292;&#20197;&#26368;&#22823;&#21270;&#25928;&#29992;&#20989;&#25968;&#20026;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#27169;&#20989;&#25968;&#26368;&#22823;&#21270;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#27719;&#24635;&#12289;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#21644;&#25512;&#33616;&#31561;&#12290;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#35299;&#65292;&#20351;&#24471;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#65292;&#25928;&#29992;&#20989;&#25968;&#26159;&#21333;&#35843;&#23376;&#27169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24179;&#22343;&#25928;&#29992;&#26368;&#22823;&#21270;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#32676;&#20307;&#30001;&#20960;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#20998;&#32452;&#32452;&#25104;&#26102;&#65292;&#21478;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#25928;&#29992;&#26159;&#21542;&#20844;&#24179;&#22320;&#20998;&#37197;&#22312;&#19981;&#21516;&#30340;&#32676;&#20307;&#20013;&#12290;&#34429;&#28982;&#25928;&#29992;&#21644;&#20844;&#24179;&#30446;&#26631;&#37117;&#26159;&#21487;&#21462;&#30340;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20114;&#30456;&#30683;&#30462;&#65292;&#24182;&#19988;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#24456;&#23569;&#26377;&#20154;&#20851;&#27880;&#22914;&#20309;&#19968;&#36215;&#20248;&#21270;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;&#20108;&#26631;&#20934;&#23376;&#27169;&#26368;&#22823;&#21270;&#8221;&#65288;BSM&#65289;&#65292;&#20197;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#35201;&#27714;&#25214;&#21040;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#35299;&#65292;&#20197;&#26368;&#22823;&#21270;&#25928;&#29992;&#20989;&#25968;&#20026;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Submodular function maximization is a fundamental combinatorial optimization problem with plenty of applications -- including data summarization, influence maximization, and recommendation. In many of these problems, the goal is to find a solution that maximizes the average utility over all users, for each of whom the utility is defined by a monotone submodular function. However, when the population of users is composed of several demographic groups, another critical problem is whether the utility is fairly distributed across different groups. Although the \emph{utility} and \emph{fairness} objectives are both desirable, they might contradict each other, and, to the best of our knowledge, little attention has been paid to optimizing them jointly.  In this paper, we propose a new problem called \emph{Bicriteria Submodular Maximization} (BSM) to strike a balance between utility and fairness. Specifically, it requires finding a fixed-size solution to maximize the utility function, subject
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;LIBO&#65292;&#21487;&#20197;&#26080;&#38656;&#30452;&#25509;&#35775;&#38382;&#25968;&#25454;&#65292;&#23545;&#19968;&#31995;&#21015;&#36172;&#21338;&#20248;&#21270;&#20219;&#21153;&#36827;&#34892;&#23398;&#20064;&#21644;&#36866;&#24212;&#65292;&#24182;&#20445;&#35777;&#26368;&#20248;&#24615;&#33021;&#21644;&#20122;&#32447;&#24615;&#32456;&#36523;&#21518;&#24724;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.15513</link><description>&lt;p&gt;
&#32456;&#36523;&#36172;&#21338;&#20248;&#21270;&#65306;&#26080;&#20808;&#39564;&#30693;&#35782;&#21644;&#26080;&#21518;&#24724;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lifelong Bandit Optimization: No Prior and No Regret. (arXiv:2210.15513v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;LIBO&#65292;&#21487;&#20197;&#26080;&#38656;&#30452;&#25509;&#35775;&#38382;&#25968;&#25454;&#65292;&#23545;&#19968;&#31995;&#21015;&#36172;&#21338;&#20248;&#21270;&#20219;&#21153;&#36827;&#34892;&#23398;&#20064;&#21644;&#36866;&#24212;&#65292;&#24182;&#20445;&#35777;&#26368;&#20248;&#24615;&#33021;&#21644;&#20122;&#32447;&#24615;&#32456;&#36523;&#21518;&#24724;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32463;&#24120;&#37325;&#22797;&#24212;&#29992;&#20110;&#30456;&#20284;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20851;&#27880;&#35299;&#20915;&#19968;&#31995;&#21015;&#36172;&#21338;&#20248;&#21270;&#20219;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#24212;&#29615;&#22659;&#30340;&#31639;&#27861;LIBO&#12290;&#25105;&#20204;&#20551;&#35774;&#20869;&#26680;&#21270;&#32467;&#26500;&#65292;&#20854;&#20013;&#30340;&#20869;&#26680;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#26159;&#26410;&#30693;&#30340;&#20294;&#20849;&#20139;&#30340;&#12290;LIBO&#20381;&#27425;&#20803;&#23398;&#20064;&#19968;&#20010;&#36924;&#36817;&#30495;&#23454;&#20869;&#26680;&#30340;&#20869;&#26680;&#65292;&#28982;&#21518;&#29992;&#26368;&#26032;&#30340;&#20869;&#26680;&#20272;&#35745;&#26469;&#35299;&#20915;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#12290;&#26412;&#31639;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#20869;&#26680;&#21270;&#25110;&#32447;&#24615;&#36172;&#21338;&#31639;&#27861;&#37197;&#23545;&#65292;&#24182;&#20445;&#35777;&#26368;&#20248;&#30340;&#39044;&#26399;&#24615;&#33021;&#12290;&#22914;&#26524;&#19982;&#20122;&#32447;&#24615;&#36172;&#21338;&#31639;&#27861;&#37197;&#23545;&#65292;LIBO&#23558;&#20135;&#29983;&#19968;&#20010;&#20122;&#32447;&#24615;&#32456;&#36523;&#21518;&#24724;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are often repeatedly applied to problems with similar structure over and over again. We focus on solving a sequence of bandit optimization tasks and develop LIBO, an algorithm which adapts to the environment by learning from past experience and becomes more sample-efficient in the process. We assume a kernelized structure where the kernel is unknown but shared across all tasks. LIBO sequentially meta-learns a kernel that approximates the true kernel and solves the incoming tasks with the latest kernel estimate. Our algorithm can be paired with any kernelized or linear bandit algorithm and guarantees oracle optimal performance, meaning that as more tasks are solved, the regret of LIBO on each task converges to the regret of the bandit algorithm with oracle knowledge of the true kernel. Naturally, if paired with a sublinear bandit algorithm, LIBO yields a sublinear lifelong regret. We also show that direct access to the data from each task is not necessary for
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#38656;&#35201;&#20445;&#25252;&#65292;&#25968;&#25454;&#38598;&#25512;&#29702;&#25216;&#26415;(DI)&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23384;&#22312;&#39640;&#20551;&#38451;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.13631</link><description>&lt;p&gt;
&#35770;&#25968;&#25454;&#38598;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Dataset Inference. (arXiv:2210.13631v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13631
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#38656;&#35201;&#20445;&#25252;&#65292;&#25968;&#25454;&#38598;&#25512;&#29702;&#25216;&#26415;(DI)&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23384;&#22312;&#39640;&#20551;&#38451;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#39640;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12289;&#35745;&#31639;&#36164;&#28304;&#21644;&#19987;&#19994;&#25216;&#26415;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26500;&#25104;&#20102;&#38656;&#35201;&#20445;&#25252;&#20813;&#21463;&#30423;&#29992;&#30340;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#25152;&#26377;&#26435;&#39564;&#35777;&#25216;&#26415;&#20801;&#35768;&#27169;&#22411;&#34987;&#30423;&#29992;&#25915;&#20987;&#30340;&#21463;&#23475;&#32773;&#35777;&#26126;&#19968;&#20010;&#23244;&#30097;&#27169;&#22411;&#23454;&#38469;&#19978;&#26159;&#20174;&#20182;&#20204;&#37027;&#37324;&#34987;&#30423;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#25968;&#23383;&#27700;&#21360;&#25110;&#25351;&#32441;&#25216;&#26415;&#30340;&#25152;&#26377;&#26435;&#39564;&#35777;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#20013;&#30340;&#22823;&#22810;&#25968;&#22312;&#23433;&#20840;&#20445;&#38556;(&#35013;&#22791;&#23436;&#22791;&#30340;&#23545;&#25163;&#21487;&#20197;&#36867;&#36991;&#39564;&#35777;)&#25110;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#37117;&#23384;&#22312;&#32570;&#38519;&#12290;&#25351;&#32441;&#25216;&#26415;&#8220;&#25968;&#25454;&#38598;&#25512;&#29702;(DI)&#8221;&#24050;&#34987;&#35777;&#26126;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;DI&#30340;&#20316;&#32773;&#20026;&#32447;&#24615;(&#23244;&#30097;)&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#27491;&#30830;&#24615;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#30456;&#21516;&#35774;&#32622;&#30340;&#23376;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;DI&#23384;&#22312;&#39640;&#20551;&#38451;&#24615;(FPs)&#8212;&#8212;&#23427;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#35782;&#21035;&#27169;&#22411;&#30423;&#29992;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models are costly to train as they can require a significant amount of data, computational resources and technical expertise. Thus, they constitute valuable intellectual property that needs protection from adversaries wanting to steal them. Ownership verification techniques allow the victims of model stealing attacks to demonstrate that a suspect model was in fact stolen from theirs.  Although a number of ownership verification techniques based on watermarking or fingerprinting have been proposed, most of them fall short either in terms of security guarantees (well-equipped adversaries can evade verification) or computational cost. A fingerprinting technique, Dataset Inference (DI), has been shown to offer better robustness and efficiency than prior methods.  The authors of DI provided a correctness proof for linear (suspect) models. However, in a subspace of the same setting, we prove that DI suffers from high false positives (FPs) -- it can incorrectly identify 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;MRIs&#30340;&#26089;&#26399;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#22810;&#20998;&#25903;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#24182;&#36798;&#21040;&#20102;99.05%&#30340;&#19977;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.12331</link><description>&lt;p&gt;
&#28145;&#24230;&#22810;&#20998;&#25903;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#29992;&#20110;&#22522;&#20110;&#33041;MRIs&#30340;&#26089;&#26399;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Multi-Branch CNN Architecture for Early Alzheimer's Detection from Brain MRIs. (arXiv:2210.12331v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;MRIs&#30340;&#26089;&#26399;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#22810;&#20998;&#25903;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#24182;&#36798;&#21040;&#20102;99.05%&#30340;&#19977;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#21487;&#33021;&#23548;&#33268;&#30196;&#21574;&#21644;&#20005;&#37325;&#30340;&#22823;&#33041;&#21151;&#33021;&#19979;&#38477;&#65292;&#22914;&#26524;&#27809;&#26377;&#39044;&#38450;&#24615;&#25252;&#29702;&#65292;&#20250;&#25233;&#21046;&#31616;&#21333;&#20219;&#21153;&#30340;&#23436;&#25104;&#12290;&#36926;9&#20998;&#20043;1&#30340;&#32654;&#22269;&#20154;&#24739;&#26377;&#30001;AD&#24341;&#36215;&#30340;&#30196;&#21574;&#30151;&#65292;&#26410;&#24471;&#21040;&#25253;&#37228;&#30340;AD&#30456;&#20851;&#30196;&#21574;&#24739;&#32773;&#25252;&#29702;&#20215;&#20540;&#20026;2716&#20159;&#32654;&#20803;&#12290;&#22240;&#27492;&#65292;&#20026;&#38450;&#27490;AD&#36827;&#19968;&#27493;&#36827;&#23637;&#65292;&#24050;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#26089;&#26399;AD&#35786;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#20854;&#20182;&#21487;&#29992;&#20110;&#26089;&#26399;AD&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758;&#65288;ADNI&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;7866819&#20010;&#21442;&#25968;&#32452;&#25104;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#19977;&#20010;&#19981;&#21516;&#30340;&#21367;&#31215;&#20998;&#25903;&#65292;&#27599;&#20010;&#20998;&#25903;&#30340;&#38271;&#24230;&#19981;&#21516;&#12290;&#27599;&#20010;&#20998;&#25903;&#30001;&#19981;&#21516;&#30340;&#21367;&#31215;&#26680;&#22823;&#23567;&#32452;&#25104;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#24739;&#26377;&#38750;&#30196;&#21574;&#12289;&#36731;&#24230;&#30196;&#21574;&#25110;&#20013;&#24230;&#30196;&#21574;&#30340;&#24739;&#32773;&#65292;&#20854;&#19977;&#20998;&#31867;&#20934;&#30830;&#29575;&#20026;99.05&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is a neuro-degenerative disease that can cause dementia and result severe reduction in brain function inhibiting simple tasks especially if no preventative care is taken. Over 1 in 9 Americans suffer from AD induced dementia and unpaid care for people with AD related dementia is valued at $271.6 billion. Hence, various approaches have been developed for early AD diagnosis to prevent its further progression. In this paper, we first review other approaches that could be used for early detection of AD. We then give an overview of our dataset that was from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and propose a deep Convolutional Neural Network (CNN) architecture consisting of 7,866,819 parameters. This model has three different convolutional branches with each having a different length. Each branch is comprised of different kernel sizes. This model can predict whether a patient is non-demented, mild-demented, or moderately demented with a 99.05% three
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23450;&#20041;&#21644;&#23398;&#20064;&#20855;&#26377;&#38382;&#39064;&#29305;&#23450;&#32467;&#26500;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#20855;&#26377;&#29305;&#23450;&#20110;&#38382;&#39064;&#35268;&#33539;&#30340;&#20307;&#31995;&#32467;&#26500;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20248;&#21270;&#20102;&#38382;&#39064;&#32500;&#24230;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32553;&#25918;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.11633</link><description>&lt;p&gt;
&#22270;&#24418;&#21270;&#32467;&#26500;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graphically Structured Diffusion Models. (arXiv:2210.11633v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23450;&#20041;&#21644;&#23398;&#20064;&#20855;&#26377;&#38382;&#39064;&#29305;&#23450;&#32467;&#26500;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#20855;&#26377;&#29305;&#23450;&#20110;&#38382;&#39064;&#35268;&#33539;&#30340;&#20307;&#31995;&#32467;&#26500;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20248;&#21270;&#20102;&#38382;&#39064;&#32500;&#24230;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32553;&#25918;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#23450;&#20041;&#21644;&#23398;&#20064;&#20855;&#26377;&#38382;&#39064;&#29305;&#23450;&#32467;&#26500;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#35299;&#20915;&#30340;&#38382;&#39064;&#22495;&#26356;&#22810;&#22320;&#37319;&#29992;&#31639;&#27861;&#26469;&#35299;&#20915;&#65292;&#22914;&#25490;&#24207;&#65292;&#25968;&#29420;&#30340;&#38480;&#21046;&#28385;&#36275;&#21644;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#20855;&#26377;&#29305;&#23450;&#20110;&#38382;&#39064;&#35268;&#33539;&#30340;&#20307;&#31995;&#32467;&#26500;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#20010;&#38382;&#39064;&#35268;&#33539;&#24212;&#35813;&#21253;&#21547;&#25551;&#36848;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#19988;&#36890;&#24120;&#21463;&#30410;&#20110;&#23376;&#35745;&#31639;&#30340;&#26126;&#30830;&#34920;&#31034;&#12290;&#32622;&#25442;&#19981;&#21464;&#24615;&#20063;&#21487;&#20197;&#34987;&#21033;&#29992;&#12290;&#22312;&#21508;&#31181;&#21508;&#26679;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#38382;&#39064;&#32500;&#24230;&#19982;&#25105;&#20204;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32553;&#25918;&#20851;&#31995;&#65292;&#26080;&#35770;&#26159;&#22312;&#35757;&#32451;&#26102;&#38388;&#36824;&#26159;&#26368;&#32456;&#20934;&#30830;&#24615;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/plai-group/gsdm&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a framework for automatically defining and learning deep generative models with problem-specific structure. We tackle problem domains that are more traditionally solved by algorithms such as sorting, constraint satisfaction for Sudoku, and matrix factorization. Concretely, we train diffusion models with an architecture tailored to the problem specification. This problem specification should contain a graphical model describing relationships between variables, and often benefits from explicit representation of subcomputations. Permutation invariances can also be exploited. Across a diverse set of experiments we improve the scaling relationship between problem dimension and our model's performance, in terms of both training time and final accuracy. Our code can be found at https://github.com/plai-group/gsdm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36716;&#36816;&#22270;&#26469;&#35299;&#37322;&#20998;&#24067;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#20154;&#24037;&#36827;&#34892;&#20943;&#36731;&#24433;&#21709;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#35768;&#22810;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#35299;&#37322;&#24615;&#26144;&#23556;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35814;&#32454;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2210.10275</link><description>&lt;p&gt;
&#25506;&#31350;&#20998;&#24067;&#21464;&#21270;&#30340;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Explaining Distribution Shifts. (arXiv:2210.10275v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36716;&#36816;&#22270;&#26469;&#35299;&#37322;&#20998;&#24067;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#20154;&#24037;&#36827;&#34892;&#20943;&#36731;&#24433;&#21709;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#35768;&#22810;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#35299;&#37322;&#24615;&#26144;&#23556;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35814;&#32454;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#30340;&#25913;&#21464;&#21487;&#33021;&#20250;&#23545;&#19979;&#28216;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#22240;&#27492;&#29702;&#35299;&#20998;&#24067;&#30340;&#21464;&#21270;&#23545;&#20110;&#30740;&#31350;&#21644;&#20943;&#36731;&#20854;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20165;&#20851;&#27880;&#20110;&#26816;&#27979;&#26159;&#21542;&#21457;&#29983;&#20102;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#20551;&#35774;&#20219;&#20309;&#26816;&#27979;&#21040;&#30340;&#21464;&#21270;&#22343;&#21487;&#20197;&#34987;&#36816;&#33829;&#32773;&#27491;&#30830;&#22320;&#29702;&#35299;&#21644;&#22788;&#29702;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36716;&#36816;&#22270;&#26469;&#35299;&#37322;&#20998;&#24067;&#21464;&#21270;&#65292;&#20174;&#32780;&#24110;&#21161;&#20154;&#24037;&#36827;&#34892;&#20943;&#36731;&#24433;&#21709;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#21487;&#35299;&#37322;&#30340;&#26144;&#23556;&#20174;&#26368;&#20248;&#36755;&#36816;&#30340;&#26494;&#24347;&#20013;&#25512;&#23548;&#20986;&#26469;&#65292;&#22312;&#20505;&#36873;&#26144;&#23556;&#38480;&#21046;&#20026;&#21487;&#35299;&#37322;&#26144;&#23556;&#30340;&#38598;&#21512;&#20013;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#22810;&#20010;&#29616;&#23454;&#20013;&#34920;&#26684;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#37322;&#24615;&#26144;&#23556;&#22914;&#20309;&#22312;&#35814;&#32454;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#25552;&#20379;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
A distribution shift can have fundamental consequences such as signaling a change in the operating environment or significantly reducing the accuracy of downstream models. Thus, understanding distribution shifts is critical for examining and hopefully mitigating the effect of such a shift. Most prior work focuses on merely detecting if a shift has occurred and assumes any detected shift can be understood and handled appropriately by a human operator. We hope to aid in these manual mitigation tasks by explaining the distribution shift using interpretable transportation maps from the original distribution to the shifted one. We derive our interpretable mappings from a relaxation of optimal transport, where the candidate mappings are restricted to a set of interpretable mappings. We then inspect multiple quintessential use-cases of distribution shift in real-world tabular, text, and image datasets to showcase how our explanatory mappings provide a better balance between detail and interpr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#31038;&#20250;&#20559;&#35265;&#22522;&#20934;&#20013;&#25968;&#25454;&#38598;&#26500;&#24314;&#20559;&#35265;&#21487;&#33021;&#23545;&#32467;&#26524;&#36896;&#25104;&#20102;&#37325;&#35201;&#24433;&#21709;&#65292;&#38656;&#35201;&#26356;&#20005;&#35880;&#30340;&#31038;&#20250;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.10040</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#20559;&#35265;&#65306;&#31038;&#20250;&#20559;&#35265;&#22522;&#20934;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks. (arXiv:2210.10040v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10040
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#31038;&#20250;&#20559;&#35265;&#22522;&#20934;&#20013;&#25968;&#25454;&#38598;&#26500;&#24314;&#20559;&#35265;&#21487;&#33021;&#23545;&#32467;&#26524;&#36896;&#25104;&#20102;&#37325;&#35201;&#24433;&#21709;&#65292;&#38656;&#35201;&#26356;&#20005;&#35880;&#30340;&#31038;&#20250;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#21487;&#38752;&#22320;&#30456;&#20449;&#20174;&#31038;&#20250;&#20559;&#35265;&#22522;&#20934;&#24471;&#21040;&#30340;&#20998;&#25968;&#26159;&#23545;&#32473;&#23450;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#31038;&#20250;&#20559;&#35265;&#30340;&#24544;&#23454;&#25351;&#26631;&#65311;&#26412;&#25991;&#36890;&#36807;&#23558;&#31038;&#20250;&#20559;&#35265;&#19982;&#26469;&#28304;&#20110;&#25968;&#25454;&#38598;&#26500;&#24314;&#36807;&#31243;&#20013;&#30340;&#38750;&#31038;&#20250;&#20559;&#35265;&#36827;&#34892;&#23545;&#27604;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26681;&#25454;&#26080;&#23475;&#30340;&#20462;&#25913;&#65288;&#22914;&#37322;&#20041;&#25110;&#38543;&#26426;&#25277;&#26679;&#65289;&#23454;&#38469;&#27169;&#25311;&#20102;&#32473;&#23450;&#22522;&#20934;&#30340;&#21508;&#31181;&#26367;&#20195;&#32467;&#26500;&#65292;&#36825;&#20123;&#20462;&#25913;&#20445;&#25345;&#20854;&#31038;&#20250;&#20559;&#35265;&#30340;&#26412;&#36136;&#12290;&#22312;&#20004;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31038;&#20250;&#20559;&#35265;&#22522;&#20934;&#65288;Winogender&#21644;BiasNLI&#65289;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#27973;&#26174;&#30340;&#20462;&#25913;&#23545;&#21508;&#31181;&#27169;&#22411;&#20013;&#23548;&#33268;&#30340;&#20559;&#35265;&#31243;&#24230;&#20135;&#29983;&#20102;&#24778;&#20154;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#20196;&#20154;&#19981;&#23433;&#30340;&#35266;&#23519;&#32467;&#26524;&#33021;&#22815;&#28608;&#21457;&#26356;&#20005;&#35880;&#30340;&#31038;&#20250;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given language model? In this work, we study this question by contrasting social biases with non-social biases stemming from choices made during dataset construction that might not even be discernible to the human eye. To do so, we empirically simulate various alternative constructions for a given benchmark based on innocuous modifications (such as paraphrasing or random-sampling) that maintain the essence of their social bias. On two well-known social bias benchmarks (Winogender and BiasNLI) we observe that these shallow modifications have a surprising effect on the resulting degree of bias across various models. We hope these troubling observations motivate more robust measures of social biases.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.08964</link><description>&lt;p&gt;
PromptCast&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#20219;&#21153;&#20013;&#65292;&#23558;&#21407;&#26469;&#30340;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#27979;&#30340;&#30446;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#21644;&#20419;&#36827;&#36825;&#20010;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65288;PISA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
&lt;/p&gt;</description></item><item><title>&#35813;&#31687;&#35770;&#25991;&#21033;&#29992;&#27169;&#22411;&#37325;&#32534;&#31243;&#25216;&#26415;&#65292;&#23558;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#37325;&#26032;&#29992;&#20110;&#25239;&#20307;CDR&#24207;&#21015;&#25512;&#26029;&#20219;&#21153;&#20013;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#24207;&#21015;&#22810;&#26679;&#24615;&#21644;&#26032;&#39062;&#24615;&#65292;&#24182;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#32467;&#26500;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.07144</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25239;&#20307;&#24207;&#21015;&#22635;&#20805;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Reprogramming Pretrained Language Models for Antibody Sequence Infilling. (arXiv:2210.07144v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31687;&#35770;&#25991;&#21033;&#29992;&#27169;&#22411;&#37325;&#32534;&#31243;&#25216;&#26415;&#65292;&#23558;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#37325;&#26032;&#29992;&#20110;&#25239;&#20307;CDR&#24207;&#21015;&#25512;&#26029;&#20219;&#21153;&#20013;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#24207;&#21015;&#22810;&#26679;&#24615;&#21644;&#26032;&#39062;&#24615;&#65292;&#24182;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#32467;&#26500;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#26368;&#22810;&#25165;&#22810;&#33402;&#30340;&#32467;&#21512;&#20998;&#23376;&#65292;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;&#35745;&#31639;&#26426;&#35774;&#35745;&#25239;&#20307;&#38656;&#35201;&#29983;&#25104;&#26032;&#39062;&#22810;&#26679;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#20445;&#25345;&#32467;&#26500;&#19968;&#33268;&#24615;&#12290;&#35774;&#35745;&#20114;&#34917;&#20915;&#23450;&#21306;&#22495;&#65288;CDR&#65289;&#26159;&#25239;&#20307;&#25152;&#29305;&#26377;&#30340;&#25361;&#25112;&#65292;&#36825;&#20915;&#23450;&#20102;&#20854;&#25239;&#21407;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#29305;&#24322;&#24615;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#65292;&#28982;&#32780;&#24050;&#30693;&#30340;&#25239;&#20307;&#24207;&#21015;/&#32467;&#26500;&#23545;&#30340;&#25968;&#37327;&#26377;&#38480;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#24207;&#21015;&#26041;&#38754;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#37325;&#32534;&#31243;&#65288;MR&#65289;&#65292;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#37325;&#26032;&#29992;&#20110;&#36866;&#24212;&#19968;&#20010;&#19981;&#21516;&#30340;&#35821;&#35328;&#20219;&#21153;&#65292;&#23427;&#20204;&#30340;&#25968;&#25454;&#31232;&#32570;&#65292;&#22240;&#27492;&#20174;&#22836;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#25110;&#26377;&#25928;&#22320;&#24494;&#35843;&#22522;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#32534;&#31243;&#20102;BERT&#21644;GPT-2&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#65292;&#29992;&#20110;&#24314;&#31435;&#25239;&#20307;&#30340;CDR&#24207;&#21015;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#37325;&#26032;&#32534;&#31243;&#30340;&#27169;&#22411;&#22312;&#32500;&#25345;&#39640;&#36136;&#37327;CDR&#24207;&#21015;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#22312;&#22810;&#26679;&#24615;&#21644;&#26032;&#39062;&#24615;&#26041;&#38754;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies comprise the most versatile class of binding molecules, with numerous applications in biomedicine. Computational design of antibodies involves generating novel and diverse sequences, while maintaining structural consistency. Unique to antibodies, designing the complementarity-determining region (CDR), which determines the antigen binding affinity and specificity, creates its own unique challenges. Recent deep learning models have shown impressive results, however the limited number of known antibody sequence/structure pairs frequently leads to degraded performance, particularly lacking diversity in the generated sequences. In our work we address this challenge by leveraging Model Reprogramming (MR), which repurposes pretrained models on a source language to adapt to the tasks that are in a different language and have scarce data - where it may be difficult to train a high-performing model from scratch or effectively fine-tune an existing pre-trained model on the specific tas
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#35770;&#28436;&#21592;&#31639;&#27861;&#65292;&#23427;&#22312;&#24555;&#36895;&#21644;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#20215;&#20540;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#19982;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2210.04470</link><description>&lt;p&gt;
&#28436;&#21592;&#35780;&#35770;&#25110;&#35780;&#35770;&#28436;&#21592;&#65311;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor-Critic or Critic-Actor? A Tale of Two Time Scales. (arXiv:2210.04470v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04470
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#35770;&#28436;&#21592;&#31639;&#27861;&#65292;&#23427;&#22312;&#24555;&#36895;&#21644;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#20215;&#20540;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#19982;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#34920;&#26684;&#30340;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#30340;&#26631;&#20934;&#20844;&#24335;&#65292;&#23558;&#20854;&#35270;&#20026;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#65292;&#20854;&#20013;&#20215;&#20540;&#20989;&#25968;&#22312;&#24555;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#65292;&#31574;&#30053;&#22312;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#12290;&#36825;&#27169;&#25311;&#20102;&#31574;&#30053;&#36845;&#20195;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#26102;&#38388;&#23610;&#24230;&#30340;&#21453;&#36716;&#23454;&#38469;&#19978;&#20250;&#27169;&#25311;&#20540;&#36845;&#20195;&#65292;&#24182;&#19988;&#26159;&#19968;&#31181;&#21512;&#27861;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#24182;&#36890;&#36807;&#24102;&#26377;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20989;&#25968;&#36924;&#36817;&#27979;&#35797;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#25105;&#20204;&#25552;&#20986;&#30340;&#35780;&#35770;&#28436;&#21592;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#19982;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the standard formulation of tabular actor-critic algorithm as a two time-scale stochastic approximation with value function computed on a faster time-scale and policy computed on a slower time-scale. This emulates policy iteration. We begin by observing that reversal of the time scales will in fact emulate value iteration and is a legitimate algorithm. We provide a proof of convergence and compare the two empirically with and without function approximation (with both linear and nonlinear function approximators) and observe that our proposed critic-actor algorithm performs on par with actor-critic in terms of both accuracy and computational effort.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;bi-stride&#30340;&#26032;&#22411;&#27744;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#20108;&#37096;&#22270;&#20915;&#31574;&#23454;&#29616;&#39640;&#25928;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#29289;&#29702;&#20223;&#30495;&#65292;&#26080;&#38656;&#25163;&#21160;&#32472;&#21046;&#31895;&#32593;&#26684;&#65292;&#24182;&#36991;&#20813;&#20102;&#31354;&#38388;&#25509;&#36817;&#24615;&#24102;&#26469;&#30340;&#38169;&#35823;&#36793;&#32536;&#12290;</title><link>http://arxiv.org/abs/2210.02573</link><description>&lt;p&gt;
&#29992;BSMS-GNN&#39640;&#25928;&#23398;&#20064;&#22522;&#20110;&#32593;&#26684;&#30340;&#29289;&#29702;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Mesh-Based Physical Simulation with BSMS-GNN. (arXiv:2210.02573v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;bi-stride&#30340;&#26032;&#22411;&#27744;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#20108;&#37096;&#22270;&#20915;&#31574;&#23454;&#29616;&#39640;&#25928;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#29289;&#29702;&#20223;&#30495;&#65292;&#26080;&#38656;&#25163;&#21160;&#32472;&#21046;&#31895;&#32593;&#26684;&#65292;&#24182;&#36991;&#20813;&#20102;&#31354;&#38388;&#25509;&#36817;&#24615;&#24102;&#26469;&#30340;&#38169;&#35823;&#36793;&#32536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24179;&#38754;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#22534;&#21472;&#30340;&#28040;&#24687;&#20256;&#36882;&#65288;MPs&#65289;&#23398;&#20064;&#22823;&#35268;&#27169;&#32593;&#26684;&#19978;&#30340;&#29289;&#29702;&#20223;&#30495;&#20855;&#26377;&#19982;&#33410;&#28857;&#25968;&#37327;&#30456;&#20851;&#30340;&#32553;&#25918;&#22797;&#26434;&#24230;&#21644;&#36807;&#24230;&#24179;&#28369;&#30340;&#25361;&#25112;&#12290;&#24341;&#20837;&#8220;&#22810;&#23610;&#24230;&#8221;&#32467;&#26500;&#21040;GNNs&#20197;&#36827;&#34892;&#29289;&#29702;&#20223;&#30495;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21463;&#21040;&#20381;&#36182;&#20110;&#32321;&#29712;&#30340;&#32472;&#21046;&#31895;&#32593;&#26684;&#25110;&#22522;&#20110;&#31354;&#38388;&#37051;&#36817;&#24615;&#26500;&#24314;&#31895;&#30053;&#32423;&#21035;&#30340;&#38480;&#21046;&#65292;&#36825;&#21487;&#33021;&#22312;&#20960;&#20309;&#36793;&#30028;&#22788;&#24341;&#20837;&#38169;&#35823;&#30340;&#36793;&#32536;&#12290;&#21463;&#21040;&#20108;&#37096;&#22270;&#20915;&#31574;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27719;&#38598;&#31574;&#30053;&#8212;&#8212;&#21452;&#27493;&#36328;&#65288;bi-stride&#65289;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;&#24403;&#36827;&#34892;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#26102;&#65292;&#21452;&#27493;&#36328;&#22312;&#27599;&#20010;&#20854;&#20182;&#21069;&#27839;&#30340;&#33410;&#28857;&#19978;&#36827;&#34892;&#27744;&#21270;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#32472;&#21046;&#36739;&#31895;&#30340;&#32593;&#26684;&#65292;&#24182;&#36890;&#36807;&#31354;&#38388;&#37051;&#36817;&#24615;&#36991;&#20813;&#38169;&#35823;&#30340;&#36793;&#32536;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#23454;&#29616;&#20102;&#27599;&#23618;&#30340;&#21333;&#19968;MP&#26041;&#26696;&#21644;&#26080;&#21442;&#25968;&#30340;&#27744;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning the physical simulation on large-scale meshes with flat Graph Neural Networks (GNNs) and stacking Message Passings (MPs) is challenging due to the scaling complexity w.r.t. the number of nodes and over-smoothing. There has been growing interest in the community to introduce \textit{multi-scale} structures to GNNs for physical simulation. However, current state-of-the-art methods are limited by their reliance on the labor-intensive drawing of coarser meshes or building coarser levels based on spatial proximity, which can introduce wrong edges across geometry boundaries. Inspired by the bipartite graph determination, we propose a novel pooling strategy, \textit{bi-stride} to tackle the aforementioned limitations. Bi-stride pools nodes on every other frontier of the breadth-first search (BFS), without the need for the manual drawing of coarser meshes and avoiding the wrong edges by spatial proximity. Additionally, it enables a one-MP scheme per level and non-parametrized pooling 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#30340;&#26032;&#22411;CLL&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#35757;&#32451;&#26679;&#26412;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#20114;&#34917;&#26631;&#31614;&#24314;&#27169;&#21152;&#26435;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20063;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14189</link><description>&lt;p&gt;
&#22522;&#20110;&#21152;&#26435;&#25439;&#22833;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class-Imbalanced Complementary-Label Learning via Weighted Loss. (arXiv:2209.14189v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14189
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#30340;&#26032;&#22411;CLL&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#35757;&#32451;&#26679;&#26412;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#20114;&#34917;&#26631;&#31614;&#24314;&#27169;&#21152;&#26435;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20063;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#34917;&#26631;&#31614;&#23398;&#20064; (CLL) &#22312;&#24369;&#30417;&#30563;&#20998;&#31867;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#38754;&#20020;&#31867;&#21035;&#19981;&#24179;&#34913;&#35757;&#32451;&#26679;&#26412;&#30340;&#26174;&#33879;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#31867;&#21035;&#20013;&#30340;&#26679;&#26412;&#25968;&#37327;&#27604;&#20854;&#20182;&#31867;&#21035;&#35201;&#23569;&#24471;&#22810;&#65292;&#36825;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;CLL&#26041;&#27861;&#27809;&#26377;&#30740;&#31350;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#23427;&#33021;&#22815;&#23545;&#22810;&#31867;&#20998;&#31867;&#36827;&#34892;&#31867;&#21035;&#19981;&#24179;&#34913;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064; (WCLL) &#30340;&#26032;&#22411;CLL&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#20114;&#34917;&#26631;&#31614;&#26469;&#24314;&#27169;&#21152;&#26435;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20063;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#20272;&#35745;&#35823;&#24046;&#30028;&#26469;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;WCLL&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complementary-label learning (CLL) is widely used in weakly supervised classification, but it faces a significant challenge in real-world datasets when confronted with class-imbalanced training samples. In such scenarios, the number of samples in one class is considerably lower than in other classes, which consequently leads to a decline in the accuracy of predictions. Unfortunately, existing CLL approaches have not investigate this problem. To alleviate this challenge, we propose a novel problem setting that enables learning from class-imbalanced complementary labels for multi-class classification. To tackle this problem, we propose a novel CLL approach called Weighted Complementary-Label Learning (WCLL). The proposed method models a weighted empirical risk minimization loss by utilizing the class-imbalanced complementary labels, which is also applicable to multi-class imbalanced training samples. Furthermore, we derive an estimation error bound to provide theoretical assurance. To ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#27010;&#24565;&#20197;&#21450;&#23427;&#20204;&#19982;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#30340;&#32039;&#24352;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#22788;&#29702;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.13012</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#27010;&#24565;&#21450;&#20854;&#30456;&#20851;&#24352;&#21147;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Survey on Fairness Notions and Related Tensions. (arXiv:2209.13012v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#27010;&#24565;&#20197;&#21450;&#23427;&#20204;&#19982;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#30340;&#32039;&#24352;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#22788;&#29702;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#35299;&#20915;&#25307;&#32856;&#21644;&#36151;&#27454;&#31561;&#28041;&#21450;&#37325;&#22823;&#20915;&#31574;&#30340;&#38382;&#39064;&#65292;&#24076;&#26395;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20195;&#26367;&#20027;&#35266;&#20154;&#20026;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20915;&#31574;&#31995;&#32479;&#23481;&#26131;&#20986;&#29616;&#20559;&#35265;&#65292;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#25991;&#29486;&#20013;&#23450;&#20041;&#20102;&#20960;&#31181;&#20844;&#24179;&#24615;&#27010;&#24565;&#20197;&#25429;&#25417;&#36825;&#20010;&#20262;&#29702;&#21644;&#31038;&#20250;&#27010;&#24565;&#30340;&#19981;&#21516;&#24494;&#22937;&#20043;&#22788;&#65288;&#20363;&#22914;&#32479;&#35745;&#24179;&#31561;&#12289;&#26426;&#20250;&#24179;&#31561;&#31561;&#65289;&#12290;&#22312;&#23398;&#20064;&#27169;&#22411;&#26102;&#38656;&#35201;&#28385;&#36275;&#20844;&#24179;&#24615;&#35201;&#27714;&#65292;&#36825;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#20844;&#24179;&#27010;&#24565;&#20043;&#38388;&#20197;&#21450;&#38544;&#31169;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#31561;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#36890;&#24120;&#20351;&#29992;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#19982;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24352;&#21147;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#35299;&#20915;&#20844;&#24179;&#24615;&#19982;&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#65288;&#20998;&#20026;&#39044;&#22788;&#29702;&#12289;&#22788;&#29702;&#20013;&#12289;&#21518;&#22788;&#29702;&#21644;&#28151;&#21512;&#22235;&#31181;&#26041;&#27861;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated decision systems are increasingly used to take consequential decisions in problems such as job hiring and loan granting with the hope of replacing subjective human decisions with objective machine learning (ML) algorithms. However, ML-based decision systems are prone to bias, which results in yet unfair decisions. Several notions of fairness have been defined in the literature to capture the different subtleties of this ethical and social concept (e.g., statistical parity, equal opportunity, etc.). Fairness requirements to be satisfied while learning models created several types of tensions among the different notions of fairness and other desirable properties such as privacy and classification accuracy. This paper surveys the commonly used fairness notions and discusses the tensions among them with privacy and accuracy. Different methods to address the fairness-accuracy trade-off (classified into four approaches, namely, pre-processing, in-processing, post-processing, and hy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#21517;&#20026;GedankenNet&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25110;&#23454;&#39564;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#29289;&#29702;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#21512;&#25104;&#30340;&#20154;&#24037;&#38543;&#26426;&#22270;&#20687;&#36827;&#34892;&#20840;&#24687;&#22270;&#37325;&#24314;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#26679;&#21697;&#31867;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.08288</link><description>&lt;p&gt;
&#21033;&#29992;&#29289;&#29702;&#19968;&#33268;&#24615;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20840;&#24687;&#22270;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning of hologram reconstruction using physics consistency. (arXiv:2209.08288v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#21517;&#20026;GedankenNet&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25110;&#23454;&#39564;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#29289;&#29702;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#21512;&#25104;&#30340;&#20154;&#24037;&#38543;&#26426;&#22270;&#20687;&#36827;&#34892;&#20840;&#24687;&#22270;&#37325;&#24314;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#26679;&#21697;&#31867;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#35745;&#31639;&#25104;&#20687;&#12289;&#20256;&#24863;&#21644;&#26174;&#24494;&#38236;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#30001;&#20110;&#20351;&#29992;&#30340;&#26159;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#35757;&#32451;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#33719;&#21462;&#21644;&#20934;&#22791;&#36890;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#20063;&#23481;&#26131;&#23548;&#33268;&#26377;&#20559;&#20272;&#35745;&#21644;&#23545;&#26032;&#26679;&#26412;&#31867;&#22411;&#30340;&#26377;&#38480;&#27867;&#21270;&#12290;&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#20010;&#21517;&#20026;GedankenNet&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#26631;&#35760;&#25110;&#23454;&#39564;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20840;&#24687;&#22270;&#37325;&#24314;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#27809;&#26377;&#20808;&#39564;&#20102;&#35299;&#35201;&#25104;&#20687;&#30340;&#26679;&#21697;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20351;&#29992;&#29289;&#29702;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#20154;&#24037;&#38543;&#26426;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#22270;&#20687;&#26159;&#21512;&#25104;&#30340;&#65292;&#27809;&#26377;&#20219;&#20309;&#23454;&#39564;&#25110;&#31867;&#20284;&#20110;&#23454;&#38469;&#26679;&#21697;&#30340;&#22806;&#35266;&#29305;&#28857;&#12290;&#32463;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#21518;&#65292;GedankenNet&#25104;&#21151;&#22320;&#37325;&#24314;&#20102;&#21508;&#31181;&#26679;&#21697;&#30340;&#39640;&#36136;&#37327;&#20840;&#24687;&#22270;&#20687;&#65292;&#21253;&#25324;&#23450;&#37327;&#30456;&#20301;&#21644;&#25391;&#24133;&#20449;&#24687;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed transformative applications of deep learning in various computational imaging, sensing and microscopy tasks. Due to the supervised learning schemes employed, these methods mostly depend on large-scale, diverse, and labeled training data. The acquisition and preparation of such training image datasets are often laborious and costly, also leading to biased estimation and limited generalization to new sample types. Here, we report a self-supervised learning model, termed GedankenNet, that eliminates the need for labeled or experimental training data, and demonstrate its effectiveness and superior generalization on hologram reconstruction tasks. Without prior knowledge about the sample types to be imaged, the self-supervised learning model was trained using a physics-consistency loss and artificial random images that are synthetically generated without any experiments or resemblance to real-world samples. After its self-supervised training, GedankenNet success
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#30340;&#20840;&#21442;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#21518;&#30340;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;nuisance flow&#21644;target flow&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#21644;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.06203</link><description>&lt;p&gt;
&#38024;&#23545;&#24178;&#39044;&#23494;&#24230;&#20272;&#35745;&#30340;&#27491;&#21017;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flows for Interventional Density Estimation. (arXiv:2209.06203v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#30340;&#20840;&#21442;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#21518;&#30340;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;nuisance flow&#21644;target flow&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#21644;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38024;&#23545;&#22240;&#26524;&#25512;&#26029;&#36890;&#24120;&#36890;&#36807;&#28508;&#22312;&#32467;&#26524;&#30340;&#22343;&#20540;&#65288;&#20363;&#22914;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65289;&#26469;&#35745;&#31639;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#37327;&#24182;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#28508;&#22312;&#32467;&#26524;&#20998;&#24067;&#30340;&#20840;&#37096;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#21518;&#30340;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#21442;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32452;&#21512;&#20102;&#20004;&#31181;&#27491;&#21017;&#21270;&#27969;&#65292;&#21363;&#65288;i&#65289;&#29992;&#20110;&#20272;&#35745;&#24178;&#25200;&#21442;&#25968;&#30340;nuisance flow&#21644;&#65288;ii&#65289;&#29992;&#20110;&#21442;&#25968;&#21270;&#20272;&#35745;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#30340;target flow&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22522;&#20110;&#21333;&#27493;&#20559;&#24046;&#26657;&#27491;&#24320;&#21457;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#26377;&#25928;&#21644;&#21452;&#37325;&#31283;&#20581;&#30340;&#26041;&#24335;&#20272;&#35745;&#30446;&#26631;&#27969;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#25552;&#20379;&#20102;&#19968;&#20010;&#27491;&#30830;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#20272;&#35745;&#22120;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing machine learning methods for causal inference usually estimate quantities expressed via the mean of potential outcomes (e.g., average treatment effect). However, such quantities do not capture the full information about the distribution of potential outcomes. In this work, we estimate the density of potential outcomes after interventions from observational data. For this, we propose a novel, fully-parametric deep learning method called Interventional Normalizing Flows. Specifically, we combine two normalizing flows, namely (i) a nuisance flow for estimating nuisance parameters and (ii) a target flow for a parametric estimation of the density of potential outcomes. We further develop a tractable optimization objective based on a one-step bias correction for an efficient and doubly robust estimation of the target flow parameters. As a result our Interventional Normalizing Flows offer a properly normalized density estimator. Across various experiments, we demonstrate that our Int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#30340;&#21487;&#21464;&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#36890;&#36807;&#25913;&#36827;&#29305;&#24449;&#36136;&#37327;&#21644;&#21442;&#25968;&#25928;&#29575;&#32553;&#23567;&#20102;&#38544;&#31169;-&#25928;&#29992;&#24046;&#36317;&#65292;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2209.04338</link><description>&lt;p&gt;
&#26725;&#25509;&#24046;&#36317;&#65306;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#21487;&#21464;&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: Differentially Private Equivariant Deep Learning for Medical Image Analysis. (arXiv:2209.04338v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#30340;&#21487;&#21464;&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#36890;&#36807;&#25913;&#36827;&#29305;&#24449;&#36136;&#37327;&#21644;&#21442;&#25968;&#25928;&#29575;&#32553;&#23567;&#20102;&#38544;&#31169;-&#25928;&#29992;&#24046;&#36317;&#65292;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24418;&#24335;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65288;&#22914;&#24046;&#20998;&#38544;&#31169;&#65289;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21487;&#20174;&#25935;&#24863;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#21516;&#26102;&#25215;&#35834;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#65292;&#20294;&#36890;&#24120;&#20250;&#20135;&#29983;&#23574;&#38160;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21487;&#21464;&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#23427;&#20204;&#25913;&#36827;&#30340;&#29305;&#24449;&#36136;&#37327;&#21644;&#21442;&#25968;&#25928;&#29575;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#25552;&#39640;&#65292;&#32553;&#23567;&#20102;&#38544;&#31169;-&#25928;&#29992;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning with formal privacy-preserving techniques like Differential Privacy (DP) allows one to derive valuable insights from sensitive medical imaging data while promising to protect patient privacy, but it usually comes at a sharp privacy-utility trade-off. In this work, we propose to use steerable equivariant convolutional networks for medical image analysis with DP. Their improved feature quality and parameter efficiency yield remarkable accuracy gains, narrowing the privacy-utility gap.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;SE(3)&#25104;&#26412;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#19982;&#20854;&#20182;&#25104;&#26412;&#26080;&#32541;&#38598;&#25104;&#21040;&#21333;&#20010;&#21487;&#24494;&#20998;&#30340;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#32852;&#21512;&#25235;&#21462;&#21644;&#36816;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.03855</link><description>&lt;p&gt;
SE(3)-DiffusionFields: &#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#24179;&#28369;&#30340;&#32852;&#21512;&#25235;&#21462;&#21644;&#36816;&#21160;&#20248;&#21270;&#25104;&#26412;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion. (arXiv:2209.03855v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03855
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;SE(3)&#25104;&#26412;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#19982;&#20854;&#20182;&#25104;&#26412;&#26080;&#32541;&#38598;&#25104;&#21040;&#21333;&#20010;&#21487;&#24494;&#20998;&#30340;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#32852;&#21512;&#25235;&#21462;&#21644;&#36816;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#21313;&#20998;&#26222;&#36941;&#65292;&#22914;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#30340;&#20248;&#21270;&#38656;&#35201;&#32852;&#21512;&#32771;&#34385;&#25235;&#21462;&#23039;&#24577;&#37197;&#32622;&#12289;&#30896;&#25758;&#21644;&#20851;&#33410;&#38480;&#21046;&#31561;&#22240;&#32032;&#12290;&#34429;&#28982;&#19968;&#20123;&#38656;&#27714;&#21487;&#20197;&#36731;&#26494;&#25163;&#24037;&#35774;&#35745;&#65292;&#20363;&#22914;&#36712;&#36857;&#30340;&#24179;&#28369;&#24615;&#65292;&#20294;&#26159;&#19968;&#20123;&#20219;&#21153;&#29305;&#23450;&#30340;&#30446;&#26631;&#38656;&#35201;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#24471;&#21040;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;SE(3)&#25104;&#26412;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#34920;&#31034;&#39640;&#24230;&#34920;&#29616;&#21147;&#30340;&#22810;&#27169;&#24577;&#20998;&#24067;&#65292;&#24182;&#30001;&#20110;&#20854;&#24471;&#20998;&#21305;&#37197;&#35757;&#32451;&#30446;&#26631;&#32780;&#22312;&#25972;&#20010;&#31354;&#38388;&#20869;&#26174;&#31034;&#20986;&#36866;&#24403;&#30340;&#26799;&#24230;&#12290;&#23558;&#25104;&#26412;&#20989;&#25968;&#23398;&#20064;&#20026;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#23558;&#20854;&#19982;&#20854;&#20182;&#25104;&#26412;&#26080;&#32541;&#38598;&#25104;&#21040;&#21333;&#20010;&#21487;&#24494;&#20998;&#30340;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#23454;&#29616;&#32852;&#21512;&#26799;&#24230;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#23398;&#20064;6DoF&#25235;&#21462;&#30340;SE(3)&#25193;&#25955;&#27169;&#22411;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#26080;&#38656;&#35299;&#32806;&#25235;&#21462;&#36873;&#25321;&#21644;&#36816;&#21160;&#35745;&#21010;&#21363;&#21487;&#36827;&#34892;&#32852;&#21512;&#25235;&#21462;&#21644;&#36816;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective optimization problems are ubiquitous in robotics, e.g., the optimization of a robot manipulation task requires a joint consideration of grasp pose configurations, collisions and joint limits. While some demands can be easily hand-designed, e.g., the smoothness of a trajectory, several task-specific objectives need to be learned from data. This work introduces a method for learning data-driven SE(3) cost functions as diffusion models. Diffusion models can represent highly-expressive multimodal distributions and exhibit proper gradients over the entire space due to their score-matching training objective. Learning costs as diffusion models allows their seamless integration with other costs into a single differentiable objective function, enabling joint gradient-based motion optimization. In this work, we focus on learning SE(3) diffusion models for 6DoF grasping, giving rise to a novel framework for joint grasp and motion optimization without needing to decouple grasp sel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;&#26102;&#39057;&#32593;&#32476;&#65288;TFN&#65289;&#12290;&#22312;&#20256;&#32479;&#21367;&#31215;&#23618;&#20013;&#23884;&#20837;&#20102;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#26102;&#39057;&#21464;&#25442;&#65288;TFT&#65289;&#26041;&#27861;&#20316;&#20026;&#33258;&#36866;&#24212;&#39044;&#22788;&#29702;&#23618;&#65292;&#35813;&#23618;&#19981;&#20165;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#65292;&#36824;&#20351;&#24471;&#32593;&#32476;&#32467;&#26500;&#21487;&#20197;&#21487;&#35299;&#37322;&#65292;&#25925;&#38556;&#35786;&#26029;&#36807;&#31243;&#36879;&#26126;&#12290;</title><link>http://arxiv.org/abs/2209.01992</link><description>&lt;p&gt;
TFN&#65306;&#22522;&#20110;&#26102;&#39057;&#36716;&#25442;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26234;&#33021;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
TFN: An Interpretable Neural Network with Time-Frequency Transform Embedded for Intelligent Fault Diagnosis. (arXiv:2209.01992v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;&#26102;&#39057;&#32593;&#32476;&#65288;TFN&#65289;&#12290;&#22312;&#20256;&#32479;&#21367;&#31215;&#23618;&#20013;&#23884;&#20837;&#20102;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#26102;&#39057;&#21464;&#25442;&#65288;TFT&#65289;&#26041;&#27861;&#20316;&#20026;&#33258;&#36866;&#24212;&#39044;&#22788;&#29702;&#23618;&#65292;&#35813;&#23618;&#19981;&#20165;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#65292;&#36824;&#20351;&#24471;&#32593;&#32476;&#32467;&#26500;&#21487;&#20197;&#21487;&#35299;&#37322;&#65292;&#25925;&#38556;&#35786;&#26029;&#36807;&#31243;&#36879;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22240;&#20854;&#24378;&#22823;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#33021;&#21147;&#32780;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#26800;&#31995;&#32479;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;CNN&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#40657;&#21283;&#23376;&#27169;&#22411;&#65292;&#20854;&#20915;&#31574;&#26426;&#21046;&#19981;&#28165;&#26224;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#22312;&#39640;&#21487;&#38752;&#24615;&#30340;&#25925;&#38556;&#35786;&#26029;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;&#26102;&#39057;&#32593;&#32476;&#65288;TFN&#65289;&#65292;&#20854;&#20013;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#26102;&#39057;&#21464;&#25442;&#65288;TFT&#65289;&#26041;&#27861;&#23884;&#20837;&#21040;&#20256;&#32479;&#21367;&#31215;&#23618;&#20013;&#20316;&#20026;&#33258;&#36866;&#24212;&#39044;&#22788;&#29702;&#23618;&#12290;&#36825;&#20010;&#39044;&#22788;&#29702;&#23618;&#31216;&#20026;&#26102;&#39057;&#21367;&#31215;&#65288;TFconv&#65289;&#23618;&#65292;&#21463;&#21040;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26680;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#29992;&#20110;&#25552;&#21462;&#19982;&#25925;&#38556;&#30456;&#20851;&#30340;&#26102;&#39057;&#20449;&#24687;&#12290;&#23427;&#19981;&#20165;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#65292;&#32780;&#19988;&#25581;&#31034;&#20102;CNN&#22312;&#39057;&#29575;&#22495;&#20869;&#39044;&#27979;&#30340;&#36923;&#36753;&#22522;&#30784;&#12290;&#19981;&#21516;&#30340;TFT&#26041;&#27861;&#23545;&#24212;&#30528;TFconv&#23618;&#30340;&#19981;&#21516;&#26680;&#20989;&#25968;&#65292;&#20351;&#32593;&#32476;&#32467;&#26500;&#21487;&#35299;&#37322;&#65292;&#25925;&#38556;&#35786;&#26029;&#36807;&#31243;&#36879;&#26126;&#12290;&#36724;&#25215;&#25925;&#38556;&#25968;&#25454;&#38598;&#21644;&#40831;&#36718;&#31665;&#25391;&#21160;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TFN&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;CNN&#30340;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#35786;&#26029;&#24615;&#33021;&#65292;&#32780;&#25552;&#21462;&#30340;&#26102;&#39057;&#29305;&#24449;&#21487;&#20197;&#26377;&#25928;&#22320;&#25581;&#31034;&#25925;&#38556;&#39057;&#29575;&#29305;&#24449;&#21644;&#31361;&#20986;&#28508;&#22312;&#25925;&#38556;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) are widely used in fault diagnosis of mechanical systems due to their powerful feature extraction and classification capabilities. However, the CNN is a typical black-box model, and the mechanism of CNN's decision-making are not clear, which limits its application in high-reliability-required fault diagnosis scenarios. To tackle this issue, we propose a novel interpretable neural network termed as Time-Frequency Network (TFN), where the physically meaningful time-frequency transform (TFT) method is embedded into the traditional convolutional layer as an adaptive preprocessing layer. This preprocessing layer named as time-frequency convolutional (TFconv) layer, is constrained by a well-designed kernel function to extract fault-related time-frequency information. It not only improves the diagnostic performance but also reveals the logical foundation of the CNN prediction in the frequency domain. Different TFT methods correspond to different kernel fun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#22270;&#21644;&#23454;&#20307;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#38646;&#26631;&#31614;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#23494;&#24230;&#20272;&#35745;&#27604;&#36739;&#24322;&#24120;&#21644;&#27491;&#24120;&#23454;&#20363;&#65292;&#24615;&#33021;&#20248;&#20110;&#22810;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.02108</link><description>&lt;p&gt;
&#38646;&#26631;&#31614;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detecting Multivariate Time Series Anomalies with Zero Known Label. (arXiv:2208.02108v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#22270;&#21644;&#23454;&#20307;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#38646;&#26631;&#31614;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#23494;&#24230;&#20272;&#35745;&#27604;&#36739;&#24322;&#24120;&#21644;&#27491;&#24120;&#23454;&#20363;&#65292;&#24615;&#33021;&#20248;&#20110;&#22810;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24050;&#22312;&#21322;&#30417;&#30563;&#29615;&#22659;&#19979; extensively studied&#65292;&#20294;&#38656;&#35201;&#35757;&#32451;&#38598;&#20013;&#30340;&#25152;&#26377;&#27491;&#24120;&#26679;&#26412;&#65292;&#36825;&#24456;&#36153;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;MTGFlow&#65292;&#22522;&#20110;&#19968;&#20010;&#24191;&#27867;&#30340;&#20551;&#35774;&#65292;&#21363;&#24322;&#24120;&#23454;&#20363;&#30340;&#23494;&#24230;&#27604;&#27491;&#24120;&#23454;&#20363;&#31232;&#30095;&#12290;MTGFlow &#24314;&#31435;&#21160;&#24577;&#22270;&#25429;&#25417;&#23454;&#20307;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#23454;&#20307;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#23494;&#24230;&#21644;&#24322;&#24120;&#20998;&#25968;&#24314;&#27169;&#65292;&#19981;&#20165;&#33021;&#26377;&#25928;&#35299;&#20915;&#23454;&#20307;&#29305;&#24449;&#22810;&#26679;&#24615;&#21644;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#65292;&#32780;&#19988;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MTGFlow &#27604;&#20960;&#31181;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#20248;&#36234;&#65292;&#24182;&#21363;&#20351;&#19982;&#21322;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#20063;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series anomaly detection has been extensively studied under the semi-supervised setting, where a training dataset with all normal instances is required. However, preparing such a dataset is very laborious since each single data instance should be fully guaranteed to be normal. It is, therefore, desired to explore multivariate time series anomaly detection methods based on the dataset without any label knowledge. In this paper, we propose MTGFlow, an unsupervised anomaly detection approach for multivariate time series anomaly detection via dynamic graph and entity-aware normalizing flow, leaning only on a widely accepted hypothesis that abnormal instances exhibit sparse densities than the normal. However, the complex interdependencies among entities and the diverse inherent characteristics of each entity pose significant challenges on the density estimation, let alone to detect anomalies based on the estimated possibility distribution. To tackle these problems, we prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CANA&#30340;&#23545;&#25239;&#20266;&#35013;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#20013;&#20351;&#34987;&#27880;&#20837;&#33410;&#28857;&#30475;&#36215;&#26469;&#27491;&#24120;&#65292;&#24182;&#25552;&#39640;&#22312;&#23454;&#38469;&#22330;&#26223;&#19979;&#38450;&#24481;/&#26816;&#27979;&#26041;&#27861;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.01819</link><description>&lt;p&gt;
&#22270;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#30340;&#23545;&#25239;&#20266;&#35013;
&lt;/p&gt;
&lt;p&gt;
Adversarial Camouflage for Node Injection Attack on Graphs. (arXiv:2208.01819v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CANA&#30340;&#23545;&#25239;&#20266;&#35013;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#20013;&#20351;&#34987;&#27880;&#20837;&#33410;&#28857;&#30475;&#36215;&#26469;&#27491;&#24120;&#65292;&#24182;&#25552;&#39640;&#22312;&#23454;&#38469;&#22330;&#26223;&#19979;&#38450;&#24481;/&#26816;&#27979;&#26041;&#27861;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#25152;&#21463;&#21040;&#30340;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#65288;Node injection attacks&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#37325;&#35270;&#65292;&#22240;&#20026;&#36825;&#31181;&#25915;&#20987;&#20855;&#26377;&#24456;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#33021;&#26174;&#33879;&#38477;&#20302;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#32463;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#38450;&#24481;/&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#21644;&#21024;&#38500;&#34987;&#27880;&#20837;&#30340;&#33410;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36827;&#34892;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#30340;&#20266;&#35013;&#65292;&#20351;&#34987;&#27880;&#20837;&#30340;&#33410;&#28857;&#30475;&#36215;&#26469;&#27491;&#24120;&#65292;&#24182;&#19988;&#23545;&#20110;&#38450;&#24481;/&#26816;&#27979;&#26041;&#27861;&#26159;&#19981;&#21487;&#24863;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#22270;&#25968;&#25454;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#21644;&#32570;&#20047;&#30452;&#35266;&#20808;&#39564;&#30693;&#35782;&#20026;&#20266;&#35013;&#30340;&#24418;&#24335;&#21270;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#20266;&#35013;&#20316;&#20026;&#27880;&#20837;&#33410;&#28857;&#21644;&#27491;&#24120;&#33410;&#28857;&#30340;&#37051;&#22495;&#32593;&#32476;&#20043;&#38388;&#30340;&#20998;&#24067;&#30456;&#20284;&#24615;&#12290;&#28982;&#21518;&#65292;&#38024;&#23545;&#23454;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#30340;&#23545;&#25239;&#20266;&#35013;&#26694;&#26550;&#65292;&#21363;CANA&#65292;&#20197;&#25552;&#39640;&#22312;&#23454;&#38469;&#22330;&#26223;&#19979;&#38450;&#24481;/&#26816;&#27979;&#26041;&#27861;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node injection attacks on Graph Neural Networks (GNNs) have received emerging attention due to their potential to significantly degrade GNN performance with high attack success rates. However, our study indicates these attacks often fail in practical scenarios, since defense/detection methods can easily identify and remove the injected nodes. To address this, we devote to camouflage node injection attack, making injected nodes appear normal and imperceptible to defense/detection methods. Unfortunately, the non-Euclidean nature of graph data and lack of intuitive prior present great challenges to the formalization, implementation, and evaluation of camouflage. In this paper, we first propose and define camouflage as distribution similarity between ego networks of injected nodes and normal nodes. Then for implementation, we propose an adversarial CAmouflage framework for Node injection Attack, namely CANA, to improve attack performance under defense/detection methods in practical scenari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PDML&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#21382;&#21490;&#31574;&#30053;&#28151;&#21512;&#20998;&#24067;&#20197;&#36866;&#24212;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36827;&#21270;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;PDML&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.12141</link><description>&lt;p&gt;
&#27963;&#22312;&#24403;&#19979;&#65306;&#36866;&#24212;&#24615;&#36827;&#21270;&#31574;&#30053;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy. (arXiv:2207.12141v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PDML&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#21382;&#21490;&#31574;&#30053;&#28151;&#21512;&#20998;&#24067;&#20197;&#36866;&#24212;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36827;&#21270;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;PDML&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#27604;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#23398;&#20064;&#19968;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#29983;&#25104;&#31574;&#30053;&#23398;&#20064;&#30340;&#26679;&#26412;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#23398;&#20064;&#20351;&#21160;&#21147;&#23398;&#27169;&#22411;&#36866;&#24212;&#25152;&#26377;&#21382;&#21490;&#31574;&#30053;&#19979;&#30340;&#32463;&#39564;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20998;&#24067;&#65292;&#21363;&#26679;&#26412;&#22238;&#25918;&#32531;&#20914;&#21306;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#8220;&#25152;&#26377;&#21382;&#21490;&#31574;&#30053;&#8221;&#19979;&#20351;&#21160;&#21147;&#23398;&#27169;&#22411;&#36866;&#24212;&#20998;&#24067;&#19981;&#19968;&#23450;&#26377;&#30410;&#20110;&#27169;&#22411;&#39044;&#27979;&#8220;&#24403;&#21069;&#31574;&#30053;&#8221;&#65292;&#22240;&#20026;&#27491;&#22312;&#20351;&#29992;&#30340;&#31574;&#30053;&#22312;&#26102;&#38388;&#19978;&#19981;&#26029;&#28436;&#21464;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36827;&#21270;&#31574;&#30053;&#20250;&#23548;&#33268;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20998;&#24067;&#30340;&#36716;&#31227;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#36825;&#31181;&#21382;&#21490;&#31574;&#30053;&#20998;&#24067;&#30340;&#36716;&#31227;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#23398;&#20064;&#21644;&#27169;&#22411;&#22238;&#28378;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#31574;&#30053;&#36866;&#24212;&#21160;&#21147;&#23398;&#27169;&#22411;&#23398;&#20064;(PDML)&#12290;PDML&#21160;&#24577;&#35843;&#25972;&#29992;&#20110;&#21160;&#21147;&#23398;&#27169;&#22411;&#23398;&#20064;&#30340;&#21382;&#21490;&#31574;&#30053;&#28151;&#21512;&#20998;&#24067;&#65292;&#20197;&#36866;&#24212;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36827;&#21270;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PDML&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (RL) often achieves higher sample efficiency in practice than model-free RL by learning a dynamics model to generate samples for policy learning. Previous works learn a dynamics model that fits under the empirical state-action visitation distribution for all historical policies, i.e., the sample replay buffer. However, in this paper, we observe that fitting the dynamics model under the distribution for \emph{all historical policies} does not necessarily benefit model prediction for the \emph{current policy} since the policy in use is constantly evolving over time. The evolving policy during training will cause state-action visitation distribution shifts. We theoretically analyze how this distribution shift over historical policies affects the model learning and model rollouts. We then propose a novel dynamics model learning method, named \textit{Policy-adapted Dynamics Model Learning (PDML)}. PDML dynamically adjusts the historical policy mixture dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#30693;&#35782;&#21644;&#33258;&#21160;&#21270;&#25903;&#25345;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#25512;&#36827;&#36719;&#20214;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#36719;&#20214;&#28431;&#27934;&#35780;&#20272;&#12290;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#22871;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#21644;&#23454;&#29992;&#24314;&#35758;&#65292;&#36866;&#29992;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#65292;&#20197;&#25552;&#39640;&#23545;&#30495;&#23454;&#36719;&#20214;&#31995;&#32479;&#20013;&#26085;&#30410;&#22686;&#38271;&#30340;&#28431;&#27934;&#30340;&#35780;&#20272;&#29702;&#35299;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#36825;&#20123;&#20851;&#38190;&#23433;&#20840;&#38382;&#39064;&#30340;&#26356;&#24443;&#24213;&#21644;&#21450;&#26102;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2207.11708</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#25552;&#39640;&#36719;&#20214;&#28431;&#27934;&#35780;&#20272;&#29702;&#35299;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards an Improved Understanding of Software Vulnerability Assessment Using Data-Driven Approaches. (arXiv:2207.11708v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#30693;&#35782;&#21644;&#33258;&#21160;&#21270;&#25903;&#25345;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#25512;&#36827;&#36719;&#20214;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#36719;&#20214;&#28431;&#27934;&#35780;&#20272;&#12290;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#22871;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#21644;&#23454;&#29992;&#24314;&#35758;&#65292;&#36866;&#29992;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#65292;&#20197;&#25552;&#39640;&#23545;&#30495;&#23454;&#36719;&#20214;&#31995;&#32479;&#20013;&#26085;&#30410;&#22686;&#38271;&#30340;&#28431;&#27934;&#30340;&#35780;&#20272;&#29702;&#35299;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#36825;&#20123;&#20851;&#38190;&#23433;&#20840;&#38382;&#39064;&#30340;&#26356;&#24443;&#24213;&#21644;&#21450;&#26102;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#30693;&#35782;&#21644;&#33258;&#21160;&#21270;&#25903;&#25345;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#25512;&#36827;&#36719;&#20214;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#36719;&#20214;&#28431;&#27934;&#35780;&#20272;&#12290;&#36719;&#20214;&#28431;&#27934;&#35780;&#20272;&#25552;&#20379;&#20102;&#37325;&#35201;&#19988;&#22810;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#20197;&#39044;&#38450;&#21644;&#20943;&#36731;&#37326;&#22806;&#21361;&#38505;&#30340;&#32593;&#32476;&#25915;&#20987;&#12290;&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#30693;&#35782;&#20307;&#31995;&#21270;&#65292;&#19968;&#22871;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#21644;&#23454;&#29992;&#24314;&#35758;&#65292;&#36866;&#29992;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#12290;&#26412;&#35770;&#25991;&#30340;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#39640;&#23545;&#30495;&#23454;&#36719;&#20214;&#31995;&#32479;&#20013;&#26085;&#30410;&#22686;&#38271;&#30340;&#28431;&#27934;&#30340;&#35780;&#20272;&#29702;&#35299;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#36825;&#20123;&#20851;&#38190;&#23433;&#20840;&#38382;&#39064;&#30340;&#26356;&#24443;&#24213;&#21644;&#21450;&#26102;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The thesis advances the field of software security by providing knowledge and automation support for software vulnerability assessment using data-driven approaches. Software vulnerability assessment provides important and multifaceted information to prevent and mitigate dangerous cyber-attacks in the wild. The key contributions include a systematisation of knowledge, along with a suite of novel data-driven techniques and practical recommendations for researchers and practitioners in the area. The thesis results help improve the understanding and inform the practice of assessing ever-increasing vulnerabilities in real-world software systems. This in turn enables more thorough and timely fixing prioritisation and planning of these critical security issues.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#27010;&#24565;&#21435;&#38500;&#12290;&#23545;&#20110;&#29616;&#26377;&#30340;&#21518;&#26399;&#21644;&#23545;&#25239;&#24615;&#26041;&#27861;&#65292;&#26412;&#25991;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#20854;&#20381;&#36182;&#30340;&#25506;&#27979;&#20998;&#31867;&#22120;&#21487;&#33021;&#20351;&#29992;&#38750;&#27010;&#24565;&#29305;&#24449;&#65292;&#23548;&#33268;&#26080;&#27861;&#23436;&#20840;&#21435;&#38500;&#19981;&#38656;&#35201;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#23398;&#20064;&#20174;&#27169;&#22411;&#30340;&#34920;&#31034;&#20013;&#21435;&#38500;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21518;&#26399;&#21644;&#23545;&#25239;&#24615;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.04153</link><description>&lt;p&gt;
&#25506;&#27979;&#20998;&#31867;&#22120;&#23545;&#20110;&#27010;&#24565;&#21435;&#38500;&#21644;&#26816;&#27979;&#19981;&#21487;&#38752;
&lt;/p&gt;
&lt;p&gt;
Probing Classifiers are Unreliable for Concept Removal and Detection. (arXiv:2207.04153v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#27010;&#24565;&#21435;&#38500;&#12290;&#23545;&#20110;&#29616;&#26377;&#30340;&#21518;&#26399;&#21644;&#23545;&#25239;&#24615;&#26041;&#27861;&#65292;&#26412;&#25991;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#20854;&#20381;&#36182;&#30340;&#25506;&#27979;&#20998;&#31867;&#22120;&#21487;&#33021;&#20351;&#29992;&#38750;&#27010;&#24565;&#29305;&#24449;&#65292;&#23548;&#33268;&#26080;&#27861;&#23436;&#20840;&#21435;&#38500;&#19981;&#38656;&#35201;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#23398;&#20064;&#20174;&#27169;&#22411;&#30340;&#34920;&#31034;&#20013;&#21435;&#38500;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21518;&#26399;&#21644;&#23545;&#25239;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#34987;&#21457;&#29616;&#22312;&#20854;&#34920;&#31034;&#20013;&#32534;&#30721;&#20102;&#19981;&#33391;&#30340;&#35821;&#35328;&#25110;&#25935;&#24863;&#27010;&#24565;&#65292;&#31227;&#38500;&#36825;&#20123;&#27010;&#24565;&#26159;&#19981;&#23481;&#26131;&#30340;&#65292;&#22240;&#20026;&#27010;&#24565;&#12289;&#25991;&#26412;&#36755;&#20837;&#21644;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21518;&#26399;&#21644;&#23545;&#25239;&#24615;&#26041;&#27861;&#26469;&#20174;&#27169;&#22411;&#30340;&#34920;&#31034;&#20013;&#21435;&#38500;&#36825;&#20123;&#19981;&#38656;&#35201;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26159;&#36866;&#24471;&#20854;&#21453;&#30340;&#65306;&#23427;&#20204;&#19981;&#33021;&#23436;&#20840;&#21435;&#38500;&#27010;&#24565;&#65292;&#32780;&#22312;&#26368;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#30772;&#22351;&#25152;&#26377;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#21407;&#22240;&#26159;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#25506;&#27979;&#20998;&#31867;&#22120;&#20316;&#20026;&#27010;&#24565;&#30340;&#20195;&#29702;&#12290;&#21363;&#20351;&#22312;&#27010;&#24565;&#30456;&#20851;&#29305;&#24449;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#23601;&#21487;&#20197;&#25552;&#20379;100%&#20934;&#30830;&#24615;&#30340;&#26368;&#26377;&#21033;&#26465;&#20214;&#19979;&#23398;&#20064;&#25506;&#27979;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#35777;&#26126;&#25506;&#27979;&#20998;&#31867;&#22120;&#24456;&#21487;&#33021;&#20250;&#20351;&#29992;&#38750;&#27010;&#24565;&#29305;&#24449;&#65292;&#22240;&#27492;&#21518;&#26399;&#25110;&#23545;&#25239;&#24615;&#22788;&#29702;&#26041;&#27861;&#23558;&#19981;&#33021;&#23436;&#20840;&#21435;&#38500;&#19981;&#38656;&#35201;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#35757;&#32451;&#27491;&#21017;&#21270;&#39033;&#26469;&#30452;&#25509;&#23398;&#20064;&#20174;&#27169;&#22411;&#34920;&#31034;&#20013;&#21435;&#38500;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21435;&#38500;&#27010;&#24565;&#30340;&#21516;&#26102;&#20445;&#30041;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21518;&#26399;&#21644;&#23545;&#25239;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models trained on text data have been found to encode undesirable linguistic or sensitive concepts in their representation. Removing such concepts is non-trivial because of a complex relationship between the concept, text input, and the learnt representation. Recent work has proposed post-hoc and adversarial methods to remove such unwanted concepts from a model's representation. Through an extensive theoretical and empirical analysis, we show that these methods can be counter-productive: they are unable to remove the concepts entirely, and in the worst case may end up destroying all task-relevant features. The reason is the methods' reliance on a probing classifier as a proxy for the concept. Even under the most favorable conditions for learning a probing classifier when a concept's relevant features in representation space alone can provide 100% accuracy, we prove that a probing classifier is likely to use non-concept features and thus post-hoc or adversarial methods wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26680;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#25913;&#21464;&#37327;&#23376;&#26680;&#24102;&#23485;&#30340;&#20540;&#65292;&#33021;&#22815;&#20351;&#37327;&#23376;&#27169;&#22411;&#20855;&#22791;&#26222;&#36866;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36895;&#24230;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.06686</link><description>&lt;p&gt;
&#24102;&#23485;&#20351;&#37327;&#23376;&#26680;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bandwidth Enables Generalization in Quantum Kernel Models. (arXiv:2206.06686v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06686
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26680;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#25913;&#21464;&#37327;&#23376;&#26680;&#24102;&#23485;&#30340;&#20540;&#65292;&#33021;&#22815;&#20351;&#37327;&#23376;&#27169;&#22411;&#20855;&#22791;&#26222;&#36866;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36895;&#24230;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#29305;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#37327;&#23376;&#35745;&#31639;&#26426;&#24050;&#34987;&#35777;&#26126;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20855;&#26377;&#36229;&#36234;&#32463;&#20856;&#29366;&#24577;&#30340;&#36895;&#24230;&#20248;&#21183;&#12290;&#20363;&#22914;&#65292;&#24050;&#32463;&#35777;&#26126;&#65292;&#37327;&#23376;&#26680;&#26041;&#27861;&#22312;&#31163;&#25955;&#23545;&#25968;&#38382;&#39064;&#30340;&#23398;&#20064;&#29256;&#26412;&#19978;&#25552;&#20379;&#25351;&#25968;&#32423;&#30340;&#21152;&#36895;&#12290;&#20102;&#35299;&#37327;&#23376;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#23545;&#20110;&#23454;&#29616;&#31867;&#20284;&#30340;&#38382;&#39064;&#36895;&#24230;&#20248;&#21183;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;&#37327;&#23376;&#29305;&#24449;&#31354;&#38388;&#30340;&#25351;&#25968;&#32423;&#22823;&#23567;&#65292;&#26222;&#36866;&#24615;&#21463;&#21040;&#38459;&#30861;&#12290;&#23613;&#31649;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#37327;&#23376;&#27604;&#29305;&#25968;&#24456;&#22823;&#26102;&#65292;&#37327;&#23376;&#27169;&#22411;&#26080;&#27861;&#26222;&#36866;&#21270;&#65292;&#20294;&#26412;&#25991;&#34920;&#26126;&#65292;&#36825;&#20123;&#32467;&#26524;&#22522;&#20110;&#36807;&#20110;&#20005;&#26684;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#31216;&#20026;&#37327;&#23376;&#26680;&#24102;&#23485;&#30340;&#36229;&#21442;&#25968;&#26469;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#27169;&#22411;&#31867;&#12290;&#25105;&#20204;&#20998;&#26512;&#22823;&#37327;&#27604;&#29305;&#26497;&#38480;&#24182;&#25552;&#20379;&#20102;&#21487;&#20197;&#29992;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#30340;&#37327;&#23376;&#27169;&#22411;&#30340;&#26222;&#36866;&#21270;&#30340;&#26126;&#30830;&#20844;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#25913;&#21464;&#37327;&#23376;&#26680;&#24102;&#23485;&#30340;&#20540;&#21487;&#20197;&#20351;&#37327;&#23376;&#27169;&#22411;&#26222;&#36866;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computers are known to provide speedups over classical state-of-the-art machine learning methods in some specialized settings. For example, quantum kernel methods have been shown to provide an exponential speedup on a learning version of the discrete logarithm problem. Understanding the generalization of quantum models is essential to realizing similar speedups on problems of practical interest. Recent results demonstrate that generalization is hindered by the exponential size of the quantum feature space. Although these results suggest that quantum models cannot generalize when the number of qubits is large, in this paper we show that these results rely on overly restrictive assumptions. We consider a wider class of models by varying a hyperparameter that we call quantum kernel bandwidth. We analyze the large-qubit limit and provide explicit formulas for the generalization of a quantum model that can be solved in closed form. Specifically, we show that changing the value of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#22534;&#21472;&#30340;&#37329;&#23383;&#22612;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#32423;&#24555;&#25463;&#26041;&#24335;&#24314;&#31435;&#37329;&#23383;&#22612;&#32467;&#26500;&#65292;&#23558;&#29305;&#24449;&#30452;&#25509;&#38598;&#25104;&#21040;&#36755;&#20986;&#27169;&#22359;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#26435;&#22534;&#21472;&#31574;&#30053;&#26469;&#22686;&#24378;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#12290;&#22312;&#28382;&#22238;&#34892;&#20026;&#27169;&#25311;&#20013;&#20855;&#26377;&#36739;&#20248;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.03990</link><description>&lt;p&gt;
&#22522;&#20110;&#37329;&#23383;&#22612;&#31070;&#32463;&#32593;&#32476;&#30340;&#28382;&#22238;&#34892;&#20026;&#27169;&#25311;&#65306;&#21407;&#29702;&#12289;&#32593;&#32476;&#32467;&#26500;&#12289;&#26696;&#20363;&#30740;&#31350;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Hysteretic Behavior Simulation Based on Pyramid Neural Network:Principle, Network Architecture, Case Study and Explanation. (arXiv:2206.03990v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#22534;&#21472;&#30340;&#37329;&#23383;&#22612;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#32423;&#24555;&#25463;&#26041;&#24335;&#24314;&#31435;&#37329;&#23383;&#22612;&#32467;&#26500;&#65292;&#23558;&#29305;&#24449;&#30452;&#25509;&#38598;&#25104;&#21040;&#36755;&#20986;&#27169;&#22359;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#26435;&#22534;&#21472;&#31574;&#30053;&#26469;&#22686;&#24378;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#12290;&#22312;&#28382;&#22238;&#34892;&#20026;&#27169;&#25311;&#20013;&#20855;&#26377;&#36739;&#20248;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#20934;&#30830;&#39640;&#25928;&#30340;&#26448;&#26009;&#19982;&#26500;&#20214;&#28382;&#22238;&#34892;&#20026;&#27169;&#25311;&#23545;&#32467;&#26500;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#22312;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21152;&#26435;&#22534;&#21472;&#30340;&#37329;&#23383;&#22612;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#24341;&#20837;&#22810;&#32423;&#24555;&#25463;&#26041;&#24335;&#24314;&#31435;&#37329;&#23383;&#22612;&#32467;&#26500;&#65292;&#23558;&#29305;&#24449;&#30452;&#25509;&#38598;&#25104;&#21040;&#36755;&#20986;&#27169;&#22359;&#20013;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#21152;&#26435;&#22534;&#21472;&#31574;&#30053;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#12290;&#38543;&#21518;&#65292;&#23558;&#37325;&#26032;&#35774;&#35745;&#30340;&#26550;&#26500;&#19982;&#20854;&#20182;&#24120;&#29992;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37325;&#26032;&#35774;&#35745;&#30340;&#26550;&#26500;&#22312;87.5%&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#26367;&#20195;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#22522;&#26412;&#32593;&#32476;&#26550;&#26500;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate and efficient simulation of the hysteretic behavior of materials and components is essential for structural analysis. The surrogate model based on neural networks shows significant potential in balancing efficiency and accuracy. However, its serial information flow and prediction based on single-level features adversely affect the network performance. Therefore, a weighted stacked pyramid neural network architecture is proposed herein. This network establishes a pyramid architecture by introducing multi-level shortcuts to integrate features directly in the output module. In addition, a weighted stacked strategy is proposed to enhance the conventional feature fusion method. Subsequently, the redesigned architectures are compared with other commonly used network architectures. Results show that the redesigned architectures outperform the alternatives in 87.5% of cases. Meanwhile, the long and short-term memory abilities of different basic network architectures are analyzed th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#35299;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#65292;&#23558;&#22797;&#26434;&#38750;&#24179;&#31283;&#21644;&#38750;&#32447;&#24615;&#21160;&#24577;&#34920;&#31034;&#20026;&#31616;&#21333;&#12289;&#21487;&#35299;&#37322;&#30340;&#31232;&#30095;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2206.02972</link><description>&lt;p&gt;
&#20998;&#35299;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;dLDS&#65289;&#29992;&#20110;&#23398;&#20064;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#28508;&#22312;&#25104;&#20998;
&lt;/p&gt;
&lt;p&gt;
Decomposed Linear Dynamical Systems (dLDS) for learning the latent components of neural dynamics. (arXiv:2206.02972v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02972
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#35299;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#65292;&#23558;&#22797;&#26434;&#38750;&#24179;&#31283;&#21644;&#38750;&#32447;&#24615;&#21160;&#24577;&#34920;&#31034;&#20026;&#31616;&#21333;&#12289;&#21487;&#35299;&#37322;&#30340;&#31232;&#30095;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32676;&#20307;&#27700;&#24179;&#19978;&#23398;&#20064;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#26159;&#29702;&#35299;&#35266;&#23519;&#21040;&#30340;&#31070;&#32463;&#27963;&#21160;&#22914;&#20309;&#19982;&#30693;&#35273;&#21644;&#34892;&#20026;&#30456;&#20851;&#30340;&#20851;&#38190;&#31532;&#19968;&#27493;&#12290;&#31070;&#32463;&#21160;&#21147;&#23398;&#27169;&#22411;&#36890;&#24120;&#38598;&#20013;&#20110;&#31070;&#32463;&#27963;&#21160;&#30340;&#20302;&#32500;&#25237;&#24433;&#65292;&#25110;&#32773;&#23398;&#20064;&#19982;&#31070;&#32463;&#29366;&#24577;&#38543;&#26102;&#38388;&#26126;&#30830;&#30456;&#20851;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;&#36890;&#36807;&#23558;&#21160;&#21147;&#31995;&#32479;&#35270;&#20026;&#20302;&#32500;&#27969;&#30340;&#20195;&#34920;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#22312;&#27492;&#27010;&#24565;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#35299;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#38750;&#24179;&#31283;&#21644;&#38750;&#32447;&#24615;&#21160;&#24577;&#34920;&#31034;&#20026;&#26356;&#31616;&#21333;&#12289;&#26356;&#21487;&#35299;&#37322;&#30340;&#25104;&#20998;&#30340;&#31232;&#30095;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#23383;&#20856;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#36319;&#36394;&#31232;&#30095;&#21521;&#37327;&#38543;&#26102;&#38388;&#21464;&#21270;&#26041;&#38754;&#30340;&#32467;&#26524;&#12290;&#30456;&#36739;&#20110;&#20197;&#24448;&#30340;&#24320;&#20851;&#26041;&#27861;&#65292;&#22312;&#32473;&#23450;&#21442;&#25968;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#20998;&#35299;&#21160;&#24577;&#24615;&#36136;&#26356;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable representations of neural dynamics at a population level is a crucial first step to understanding how observed neural activity relates to perception and behavior. Models of neural dynamics often focus on either low-dimensional projections of neural activity, or on learning dynamical systems that explicitly relate to the neural state over time. We discuss how these two approaches are interrelated by considering dynamical systems as representative of flows on a low-dimensional manifold. Building on this concept, we propose a new decomposed dynamical system model that represents complex non-stationary and nonlinear dynamics of time series data as a sparse combination of simpler, more interpretable components. Our model is trained through a dictionary learning procedure, where we leverage recent results in tracking sparse vectors over time. The decomposed nature of the dynamics is more expressive than previous switched approaches for a given number of parameters and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26816;&#27979;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#26080;&#20154;&#26426;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2206.02670</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#24341;&#23548;&#21644;&#35268;&#21010;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning. (arXiv:2206.02670v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26816;&#27979;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#26080;&#20154;&#26426;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26080;&#20154;&#26426;&#22312;&#20844;&#20849;&#39046;&#22495;&#36973;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#39118;&#38505;&#22686;&#21152;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26816;&#27979;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#26080;&#20154;&#26426;&#20813;&#21463;&#25915;&#20987;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#21644;&#35268;&#21010;&#65292;&#21033;&#29992;&#20154;&#24037;&#21183;&#22330;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#38556;&#30861;&#29289;&#36991;&#20813;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dangers of adversarial attacks on Uncrewed Aerial Vehicle (UAV) agents operating in public are increasing. Adopting AI-based techniques and, more specifically, Deep Learning (DL) approaches to control and guide these UAVs can be beneficial in terms of performance but can add concerns regarding the safety of those techniques and their vulnerability against adversarial attacks. Confusion in the agent's decision-making process caused by these attacks can seriously affect the safety of the UAV. This paper proposes an innovative approach based on the explainability of DL methods to build an efficient detector that will protect these DL schemes and the UAVs adopting them from attacks. The agent adopts a Deep Reinforcement Learning (DRL) scheme for guidance and planning. The agent is trained with a Deep Deterministic Policy Gradient (DDPG) with Prioritised Experience Replay (PER) DRL scheme that utilises Artificial Potential Field (APF) to improve training times and obstacle avoidance per
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;177&#31687;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#30382;&#32932;&#30149;&#21464;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#20174;&#22810;&#20010;&#26041;&#38754;&#23545;&#36755;&#20837;&#25968;&#25454;&#12289;&#27169;&#22411;&#35774;&#35745;&#21644;&#35780;&#20272;&#26041;&#38754;&#36827;&#34892;&#20998;&#26512;&#21644;&#35752;&#35770;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2206.00356</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning for Skin Lesion Segmentation. (arXiv:2206.00356v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;177&#31687;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#30382;&#32932;&#30149;&#21464;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#20174;&#22810;&#20010;&#26041;&#38754;&#23545;&#36755;&#20837;&#25968;&#25454;&#12289;&#27169;&#22411;&#35774;&#35745;&#21644;&#35780;&#20272;&#26041;&#38754;&#36827;&#34892;&#20998;&#26512;&#21644;&#35752;&#35770;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30382;&#32932;&#30284;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#21487;&#38477;&#20302;&#36825;&#31181;&#24120;&#35265;&#30142;&#30149;&#30340;&#36127;&#25285;&#12290;&#20174;&#22270;&#20687;&#20013;&#20998;&#21106;&#30382;&#32932;&#30149;&#21464;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22825;&#28982;&#21644;&#20154;&#20026;&#30340;&#20266;&#24433;&#65288;&#20363;&#22914;&#65292;&#27611;&#21457;&#21644;&#27668;&#27873;&#65289;&#12289;&#20869;&#22312;&#22240;&#32032;&#65288;&#20363;&#22914;&#65292;&#30149;&#21464;&#24418;&#29366;&#21644;&#23545;&#27604;&#24230;&#65289;&#20197;&#21450;&#22270;&#20687;&#37319;&#38598;&#26465;&#20214;&#30340;&#21464;&#21270;&#20351;&#24471;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#25104;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#21508;&#31181;&#30740;&#31350;&#32773;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;177&#31687;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#30382;&#32932;&#30149;&#21464;&#30340;&#30740;&#31350;&#35770;&#25991;&#12290;&#25105;&#20204;&#20174;&#36755;&#20837;&#25968;&#25454;&#65288;&#25968;&#25454;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65289;&#12289;&#27169;&#22411;&#35774;&#35745;&#65288;&#26550;&#26500;&#12289;&#27169;&#22359;&#21644;&#25439;&#22833;&#65289;&#21644;&#35780;&#20272;&#26041;&#38754;&#65288;&#25968;&#25454;&#27880;&#37322;&#35201;&#27714;&#21644;&#20998;&#21106;&#24615;&#33021;&#65289;&#31561;&#22810;&#20010;&#26041;&#38754;&#23545;&#36825;&#20123;&#24037;&#20316;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#20174;&#25216;&#26415;&#32454;&#33410;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#20004;&#20010;&#26041;&#38754;&#35752;&#35770;&#36825;&#20123;&#32500;&#24230;&#65292;&#31361;&#20986;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin cancer is a major public health problem that could benefit from computer-aided diagnosis to reduce the burden of this common disease. Skin lesion segmentation from images is an important step toward achieving this goal. However, the presence of natural and artificial artifacts (e.g., hair and air bubbles), intrinsic factors (e.g., lesion shape and contrast), and variations in image acquisition conditions make skin lesion segmentation a challenging task. Recently, various researchers have explored the applicability of deep learning models to skin lesion segmentation. In this survey, we cross-examine 177 research papers that deal with deep learning-based segmentation of skin lesions. We analyze these works along several dimensions, including input data (datasets, preprocessing, and synthetic data generation), model design (architecture, modules, and losses), and evaluation aspects (data annotation requirements and segmentation performance). We discuss these dimensions both from the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20998;&#35010;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;VFed-SSD&#65292;&#29992;&#20110;&#25913;&#21892;&#24191;&#21578;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30456;&#20114;&#37325;&#21472;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#20998;&#35299;&#32852;&#37030;&#27169;&#22411;&#26469;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#25512;&#29702;&#25928;&#29575;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#26126;&#65292;VFed-SSD&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2205.15987</link><description>&lt;p&gt;
VFed-SSD&#65306;&#38754;&#21521;&#23454;&#29992;&#30340;&#22402;&#30452;&#32852;&#37030;&#24191;&#21578;
&lt;/p&gt;
&lt;p&gt;
VFed-SSD: Towards Practical Vertical Federated Advertising. (arXiv:2205.15987v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20998;&#35010;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;VFed-SSD&#65292;&#29992;&#20110;&#25913;&#21892;&#24191;&#21578;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30456;&#20114;&#37325;&#21472;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#20998;&#35299;&#32852;&#37030;&#27169;&#22411;&#26469;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#25512;&#29702;&#25928;&#29575;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#26126;&#65292;VFed-SSD&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#23433;&#20840;&#23398;&#20064;&#33539;&#24335;&#65292;&#21033;&#29992;&#36328;&#26426;&#26500;&#31169;&#26377;&#25968;&#25454;&#65292;&#26088;&#22312;&#36890;&#36807;&#21551;&#29992;&#24191;&#21578;&#20027;&#21644;&#21457;&#24067;&#32773;&#31169;&#26377;&#25317;&#26377;&#30340;&#20114;&#34917;&#29992;&#25143;&#23646;&#24615;&#30340;&#32852;&#21512;&#23398;&#20064;&#26469;&#25913;&#36827;&#24191;&#21578;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#24212;&#29992;&#21040;&#24191;&#21578;&#31995;&#32479;&#26102;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;a&#65289;&#26631;&#35760;&#37325;&#21472;&#26679;&#26412;&#30340;&#26377;&#38480;&#35268;&#27169;&#65292;&#20197;&#21450;b&#65289;&#23454;&#26102;&#36328;&#26426;&#26500;&#26381;&#21153;&#30340;&#39640;&#25104;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20998;&#35010;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;VFed-SSD&#12290;&#25105;&#20204;&#35748;&#20026;&#65306;i&#65289;&#22312;&#24191;&#21578;&#31995;&#32479;&#20013;&#26377;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#37325;&#21472;&#25968;&#25454;&#21487;&#29992;&#65292;ii&#65289;&#36890;&#36807;&#20998;&#35299;&#32852;&#37030;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#25512;&#29702;&#25104;&#26412;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an emerging secure learning paradigm in lever-aging cross-agency private data, vertical federatedlearning (VFL) is expected to improve advertising models by enabling the joint learning of complementary user attributes privately owned by the advertiser and the publisher. However, there are two key challenges in applying it to advertising systems: a) the limited scale of labeled overlapping samples, and b) the high cost of real-time cross-agency serving. In this paper, we propose a semi-supervised split distillation framework VFed-SSD to alleviate the two limitations. We identify that: i)there are massive unlabeled overlapped data available in advertising systems, and ii) we can keep a balance between model performance and inference cost by decomposing the federated model. Specifically, we develop a self-supervised task MatchedPair Detection (MPD) to exploit the vertically partitioned unlabeled data and propose the Split Knowledge Distillation (SplitKD) schema to avoid cross-agency se
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#26696;&#21644;&#35745;&#31639;&#24378;&#20581;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#12289;&#19981;&#26029;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36716;&#31227;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#38416;&#36848;&#20102;&#19981;&#30830;&#23450;MDP&#65288;uMDP&#65289;&#30340;&#27010;&#24565;&#65292;&#38024;&#23545;&#24212;&#29992;&#20013;&#26377;&#38480;&#25968;&#25454;&#23548;&#33268;&#30340;&#32479;&#35745;&#35823;&#24046;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#35745;&#31639;&#24378;&#20581;&#31574;&#30053;&#20197;&#36981;&#24490;&#24418;&#24335;&#35268;&#33539;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2205.15827</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21363;&#26102;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Anytime Learning of Markov Decision Processes. (arXiv:2205.15827v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15827
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#26696;&#21644;&#35745;&#31639;&#24378;&#20581;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#12289;&#19981;&#26029;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36716;&#31227;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#38416;&#36848;&#20102;&#19981;&#30830;&#23450;MDP&#65288;uMDP&#65289;&#30340;&#27010;&#24565;&#65292;&#38024;&#23545;&#24212;&#29992;&#20013;&#26377;&#38480;&#25968;&#25454;&#23548;&#33268;&#30340;&#32479;&#35745;&#35823;&#24046;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#35745;&#31639;&#24378;&#20581;&#31574;&#30053;&#20197;&#36981;&#24490;&#24418;&#24335;&#35268;&#33539;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26159;&#32463;&#24120;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#30340;&#24418;&#24335;&#27169;&#22411;&#12290;MDP&#36890;&#36807;&#36716;&#31227;&#20989;&#25968;&#20013;&#30340;&#27010;&#29575;&#25429;&#33719;&#21487;&#33021;&#20986;&#29616;&#30340;&#26469;&#33258;&#19981;&#31934;&#30830;&#25191;&#34892;&#22120;&#30340;&#38543;&#26426;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#24212;&#29992;&#20013;&#65292;&#20174;&#65288;&#26377;&#38480;&#30340;&#65289;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#20934;&#30830;&#30340;&#27010;&#29575;&#20250;&#24341;&#20837;&#32479;&#35745;&#35823;&#24046;&#65292;&#21487;&#33021;&#23548;&#33268;&#24847;&#22806;&#25110;&#19981;&#33391;&#32467;&#26524;&#12290;&#19981;&#30830;&#23450;MDP&#65288;uMDP&#65289;&#19981;&#38656;&#35201;&#20934;&#30830;&#30340;&#27010;&#29575;&#65292;&#32780;&#26159;&#20351;&#29992;&#25152;&#35859;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#22312;&#36716;&#25442;&#20013;&#32771;&#34385;&#36825;&#20123;&#26377;&#38480;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#32452;&#21512;&#19987;&#38376;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#26696;&#21644;&#35745;&#31639;&#24378;&#20581;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#22312;&#19968;&#20010;&#24378;&#20581;&#30340;&#21363;&#26102;&#23398;&#20064;&#26041;&#27861;&#20013;&#19981;&#26029;&#23398;&#20064;MDP&#30340;&#36716;&#31227;&#27010;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;1&#65289;&#36817;&#20284;p
&lt;/p&gt;
&lt;p&gt;
Markov decision processes (MDPs) are formal models commonly used in sequential decision-making. MDPs capture the stochasticity that may arise, for instance, from imprecise actuators via probabilities in the transition function. However, in data-driven applications, deriving precise probabilities from (limited) data introduces statistical errors that may lead to unexpected or undesirable outcomes. Uncertain MDPs (uMDPs) do not require precise probabilities but instead use so-called uncertainty sets in the transitions, accounting for such limited data. Tools from the formal verification community efficiently compute robust policies that provably adhere to formal specifications, like safety constraints, under the worst-case instance in the uncertainty set. We continuously learn the transition probabilities of an MDP in a robust anytime-learning approach that combines a dedicated Bayesian inference scheme with the computation of robust policies. In particular, our method (1) approximates p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#26816;&#27979;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#27602;&#26679;&#26412;&#65292;&#24378;&#35843;&#38450;&#24481;&#32773;&#30340;&#20027;&#21160;&#20171;&#20837;&#65292;&#30452;&#25509;&#24378;&#21046;&#23454;&#26045;&#24182;&#25918;&#22823;&#21518;&#21463;&#25915;&#20987;&#27169;&#22411;&#30340;&#29305;&#27530;&#29305;&#24449;&#65292;&#32531;&#35299;&#21518;&#38376;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2205.13616</link><description>&lt;p&gt;
&#38754;&#21521;&#20027;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21518;&#38376;&#27602;&#26679;&#26412;&#26816;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards A Proactive ML Approach for Detecting Backdoor Poison Samples. (arXiv:2205.13616v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#26816;&#27979;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#27602;&#26679;&#26412;&#65292;&#24378;&#35843;&#38450;&#24481;&#32773;&#30340;&#20027;&#21160;&#20171;&#20837;&#65292;&#30452;&#25509;&#24378;&#21046;&#23454;&#26045;&#24182;&#25918;&#22823;&#21518;&#21463;&#25915;&#20987;&#27169;&#22411;&#30340;&#29305;&#27530;&#29305;&#24449;&#65292;&#32531;&#35299;&#21518;&#38376;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;&#21518;&#38376;&#27602;&#26679;&#26412;&#26469;&#23884;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#26816;&#27979;&#36825;&#20123;&#27602;&#26679;&#26412;&#65292;&#20197;&#32531;&#35299;&#21518;&#38376;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22823;&#22810;&#25968;&#20808;&#21069;&#24037;&#20316;&#20013;&#28508;&#22312;&#30340;&#21518;&#22788;&#29702;&#24037;&#20316;&#27969;&#31243;&#65292;&#21363;&#38450;&#24481;&#32773;&#34987;&#21160;&#20801;&#35768;&#25915;&#20987;&#36827;&#34892;&#65292;&#24182;&#21033;&#29992;&#21518;&#21463;&#25915;&#20987;&#27169;&#22411;&#30340;&#29305;&#24449;&#26469;&#25581;&#31034;&#27602;&#26679;&#26412;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#24037;&#20316;&#27969;&#31243;&#26410;&#20805;&#20998;&#21033;&#29992;&#38450;&#24481;&#32773;&#30340;&#33021;&#21147;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#65292;&#24314;&#31435;&#22312;&#35813;&#24037;&#20316;&#27969;&#31243;&#20043;&#19978;&#30340;&#38450;&#24481;&#31649;&#36947;&#23481;&#26131;&#22833;&#36133;&#25110;&#24615;&#33021;&#19979;&#38477;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#65292;&#36890;&#36807;&#25512;&#24191;&#20027;&#21160;&#24605;&#32500;&#65292;&#20351;&#24471;&#38450;&#24481;&#32773;&#21487;&#20197;&#20027;&#21160;&#21442;&#19982;&#25972;&#20010;&#27169;&#22411;&#35757;&#32451;&#21644;&#27602;&#26679;&#26412;&#26816;&#27979;&#31649;&#36947;&#65292;&#30452;&#25509;&#24378;&#21046;&#23454;&#26045;&#24182;&#25918;&#22823;&#21518;&#21463;&#25915;&#20987;&#27169;&#22411;&#30340;&#29305;&#27530;&#29305;&#24449;&#20197;&#20419;&#36827;&#27602;&#26679;&#26412;&#26816;&#27979;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#35745;&#38450;&#24481;&#31574;&#30053;&#30340;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversaries can embed backdoors in deep learning models by introducing backdoor poison samples into training datasets. In this work, we investigate how to detect such poison samples to mitigate the threat of backdoor attacks. First, we uncover a post-hoc workflow underlying most prior work, where defenders passively allow the attack to proceed and then leverage the characteristics of the post-attacked model to uncover poison samples. We reveal that this workflow does not fully exploit defenders' capabilities, and defense pipelines built on it are prone to failure or performance degradation in many scenarios. Second, we suggest a paradigm shift by promoting a proactive mindset in which defenders engage proactively with the entire model training and poison detection pipeline, directly enforcing and magnifying distinctive characteristics of the post-attacked model to facilitate poison detection. Based on this, we formulate a unified framework and provide practical insights on designing de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#22312;&#38750;&#21442;&#25968;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#32570;&#20047;&#23569;&#25968;&#27966;&#26679;&#26412;&#26159;&#23398;&#20064;&#30340;&#26681;&#26412;&#38480;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#27424;&#37319;&#26679;&#31639;&#27861;&#30340;&#26368;&#23567;&#21270;&#26497;&#24046;&#39118;&#38505;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#31614;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26368;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2205.13094</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#20998;&#31867;&#20013;&#30340;&#27424;&#37319;&#26679;&#26159;&#19968;&#31181;&#26497;&#23567;&#21270;&#26497;&#24046;&#39118;&#38505;&#30340;&#40065;&#26834;&#24615;&#24178;&#39044;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification. (arXiv:2205.13094v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13094
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#22312;&#38750;&#21442;&#25968;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#32570;&#20047;&#23569;&#25968;&#27966;&#26679;&#26412;&#26159;&#23398;&#20064;&#30340;&#26681;&#26412;&#38480;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#27424;&#37319;&#26679;&#31639;&#27861;&#30340;&#26368;&#23567;&#21270;&#26497;&#24046;&#39118;&#38505;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#31614;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#24191;&#27867;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#20294;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22522;&#20110;&#27424;&#37319;&#26679;&#30340;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#36890;&#24120;&#33021;&#22815;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38750;&#21442;&#25968;&#20108;&#20803;&#20998;&#31867;&#35774;&#32622;&#19979;&#65292;&#23398;&#20064;&#30340;&#22522;&#26412;&#38480;&#21046;&#26159;&#30001;&#20110;&#32570;&#20047;&#23569;&#25968;&#32676;&#20307;&#26679;&#26412;&#32780;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38500;&#38750;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#37325;&#21472;&#65288;&#36825;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#19981;&#22826;&#21487;&#33021;&#65289;&#65292;&#21542;&#21017;&#31639;&#27861;&#26080;&#27861;&#36229;&#36234;&#27424;&#37319;&#26679;&#65292;&#38500;&#38750;&#31639;&#27861;&#21033;&#29992;&#26377;&#20851;&#20998;&#24067;&#20559;&#31227;&#30340;&#20854;&#20182;&#32467;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#26631;&#31614;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24635;&#26159;&#23384;&#22312;&#19968;&#31181;&#26368;&#23567;&#21270;&#26497;&#24046;&#39118;&#38505;&#30340;&#27424;&#37319;&#26679;&#31639;&#27861;&#12290;&#22312;&#32452;&#36716;&#25442;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#26368;&#23567;&#26497;&#24046;&#39118;&#38505;&#30340;&#27424;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#23454;&#39564;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a broad range of techniques have been proposed to tackle distribution shift, the simple baseline of training on an $\textit{undersampled}$ balanced dataset often achieves close to state-of-the-art-accuracy across several popular benchmarks. This is rather surprising, since undersampling algorithms discard excess majority group data. To understand this phenomenon, we ask if learning is fundamentally constrained by a lack of minority group samples. We prove that this is indeed the case in the setting of nonparametric binary classification. Our results show that in the worst case, an algorithm cannot outperform undersampling unless there is a high degree of overlap between the train and test distributions (which is unlikely to be the case in real-world datasets), or if the algorithm leverages additional structure about the distribution shift. In particular, in the case of label shift we show that there is always an undersampling algorithm that is minimax optimal. In the case of grou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;QGNN&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#26679;&#26412;&#25968;&#25454;&#21033;&#29992;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;QGNN&#30340;&#22810;&#23618;&#28040;&#24687;&#20256;&#36882;&#26550;&#26500;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#34920;&#31034;&#22797;&#26434;&#24230;&#65292;&#24341;&#20837;&#30340;&#32622;&#25442;&#19981;&#21464;&#30340;&#28151;&#21512;&#22120;&#20063;&#20351;&#20854;&#26356;&#20855;&#20280;&#32553;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#21512;&#20316;&#21644;&#38750;&#21512;&#20316;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.13005</link><description>&lt;p&gt;
QGNN: &#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QGNN: Value Function Factorisation with Graph Neural Networks. (arXiv:2205.13005v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;QGNN&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#26679;&#26412;&#25968;&#25454;&#21033;&#29992;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;QGNN&#30340;&#22810;&#23618;&#28040;&#24687;&#20256;&#36882;&#26550;&#26500;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#34920;&#31034;&#22797;&#26434;&#24230;&#65292;&#24341;&#20837;&#30340;&#32622;&#25442;&#19981;&#21464;&#30340;&#28151;&#21512;&#22120;&#20063;&#20351;&#20854;&#26356;&#20855;&#20280;&#32553;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#21512;&#20316;&#21644;&#38750;&#21512;&#20316;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#20840;&#23616;&#30446;&#26631;&#26159;&#20419;&#36827;&#21512;&#20316;&#30340;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20840;&#23616;&#22870;&#21169;&#26469;&#35757;&#32451;&#20010;&#20307;&#26234;&#33021;&#20307;&#24182;&#19981;&#20855;&#22791;&#39640;&#25928;&#30340;&#26679;&#26412;&#25968;&#25454;&#21033;&#29992;&#29575;&#65292;&#22240;&#20026;&#23427;&#19981;&#19968;&#23450;&#19982;&#21333;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#30456;&#20851;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#23558;&#20840;&#23616;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#20026;&#23616;&#37096;&#20215;&#20540;&#20989;&#25968;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#26089;&#26399;&#24037;&#20316;&#26159;&#36890;&#36807;&#23558;&#23616;&#37096;&#20215;&#20540;&#20989;&#25968;&#32431;&#31929;&#22320;&#32622;&#20110;&#26412;&#22320;&#20449;&#24687;&#26465;&#20214;&#19979;&#26469;&#36827;&#34892;&#20998;&#35299;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25552;&#20379;&#23616;&#37096;&#20449;&#24687;&#21644;&#20840;&#23616;&#29366;&#24577;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20419;&#36827;&#21512;&#20316;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QGNN&#65292;&#36825;&#26159;&#39318;&#20010;&#20351;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#30340;&#20215;&#20540;&#20998;&#35299;&#26041;&#27861;&#12290;QGNN&#30340;&#22810;&#23618;&#28040;&#24687;&#20256;&#36882;&#26550;&#26500;&#25552;&#20379;&#20102;&#27604;&#20808;&#21069;&#24037;&#20316;&#26356;&#22810;&#30340;&#34920;&#31034;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#20351;&#20854;&#20135;&#29983;&#26356;&#26377;&#25928;&#30340;&#20998;&#35299;&#12290;QGNN&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#28151;&#21512;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#26234;&#33021;&#20307;&#25968;&#37327;&#21644;&#39034;&#24207;&#21464;&#21270;&#65292;&#20351;&#20854;&#26356;&#20855;&#20280;&#32553;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;QGNN&#65292;&#21253;&#25324;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#23548;&#33322;&#20219;&#21153;&#21644;&#19968;&#20010;&#26143;&#38469;&#20105;&#38712;&#24494;&#35266;&#31649;&#29702;&#20219;&#21153;&#65292;&#24182;&#34920;&#26126;&#22312;&#21512;&#20316;&#21644;&#38750;&#21512;&#20316;&#20219;&#21153;&#20013;&#65292;&#23427;&#22343;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-agent reinforcement learning, the use of a global objective is a powerful tool for incentivising cooperation. Unfortunately, it is not sample-efficient to train individual agents with a global reward, because it does not necessarily correlate with an agent's individual actions. This problem can be solved by factorising the global value function into local value functions. Early work in this domain performed factorisation by conditioning local value functions purely on local information. Recently, it has been shown that providing both local information and an encoding of the global state can promote cooperative behaviour. In this paper we propose QGNN, the first value factorisation method to use a graph neural network (GNN) based model. The multi-layer message passing architecture of QGNN provides more representational complexity than models in prior work, allowing it to produce a more effective factorisation. QGNN also introduces a permutation invariant mixer which is able to 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23567;&#27874;&#25955;&#23556;&#35889;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20855;&#26377;&#24179;&#31283;&#22686;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#38750;&#39640;&#26031;&#29305;&#24615;&#65292;&#20854;&#31995;&#25968;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#26368;&#22823;&#29109;&#27169;&#22411;&#21644;&#29983;&#25104;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#33258;&#30456;&#20284;&#36807;&#31243;&#20855;&#26377;&#25955;&#23556;&#35889;&#30340;&#23610;&#24230;&#19981;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.10177</link><description>&lt;p&gt;
&#22522;&#20110;&#23567;&#27874;&#25955;&#23556;&#35889;&#30340;&#23610;&#24230;&#20381;&#36182;&#24615;&#21644;&#33258;&#30456;&#20284;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scale Dependencies and Self-Similar Models with Wavelet Scattering Spectra. (arXiv:2204.10177v2 [physics.data-an] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23567;&#27874;&#25955;&#23556;&#35889;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20855;&#26377;&#24179;&#31283;&#22686;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#38750;&#39640;&#26031;&#29305;&#24615;&#65292;&#20854;&#31995;&#25968;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#26368;&#22823;&#29109;&#27169;&#22411;&#21644;&#29983;&#25104;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#33258;&#30456;&#20284;&#36807;&#31243;&#20855;&#26377;&#25955;&#23556;&#35889;&#30340;&#23610;&#24230;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23567;&#27874;&#25955;&#23556;&#35889;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#24179;&#31283;&#22686;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#38750;&#39640;&#26031;&#27169;&#22411;&#12290;&#22797;&#23567;&#27874;&#21464;&#25442;&#35745;&#31639;&#27599;&#20010;&#23610;&#24230;&#19978;&#30340;&#20449;&#21495;&#21464;&#21270;&#12290;&#36328;&#23610;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#30001;&#23567;&#27874;&#31995;&#25968;&#21450;&#20854;&#27169;&#25968;&#22312;&#26102;&#38388;&#21644;&#23610;&#24230;&#19978;&#30340;&#32852;&#21512;&#30456;&#20851;&#24615;&#25152;&#25429;&#33719;&#12290;&#35813;&#30456;&#20851;&#30697;&#38453;&#30001;&#31532;&#20108;&#20010;&#23567;&#27874;&#21464;&#25442;&#36817;&#20284;&#23545;&#35282;&#21270;&#65292;&#23450;&#20041;&#20102;&#25955;&#23556;&#35889;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#30456;&#20284;&#36807;&#31243;&#20855;&#26377;&#25955;&#23556;&#35889;&#30340;&#23610;&#24230;&#19981;&#21464;&#24615;&#12290;&#35813;&#29305;&#24615;&#21487;&#20197;&#22312;&#21333;&#20010;&#23454;&#29616;&#19978;&#36827;&#34892;&#32479;&#35745;&#27979;&#35797;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31867;&#23485;&#20041;&#33258;&#30456;&#20284;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#25955;&#23556;&#35889;&#31995;&#25968;&#26500;&#24314;&#26368;&#22823;&#29109;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#24494;&#27491;&#21017;&#37319;&#26679;&#31639;&#27861;&#29983;&#25104;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#23637;&#31034;&#20102;&#39640;&#24230;&#38750;&#39640;&#26031;&#30340;&#37329;&#34701;&#21644;&#28237;&#27969;&#26102;&#38388;&#24207;&#21015;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the wavelet scattering spectra which provide non-Gaussian models of time-series having stationary increments. A complex wavelet transform computes signal variations at each scale. Dependencies across scales are captured by the joint correlation across time and scales of wavelet coefficients and their modulus. This correlation matrix is nearly diagonalized by a second wavelet transform, which defines the scattering spectra. We show that this vector of moments characterizes a wide range of non-Gaussian properties of multi-scale processes. We prove that self-similar processes have scattering spectra which are scale invariant. This property can be tested statistically on a single realization and defines a class of wide-sense self-similar processes. We build maximum entropy models conditioned by scattering spectra coefficients, and generate new time-series with a microcanonical sampling algorithm. Applications are shown for highly non-Gaussian financial and turbulence time-seri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22788;&#29702;&#22797;&#26434;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24322;&#36136;&#24615;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#65292;&#20351;&#29992;&#22240;&#26524;&#26641;&#26041;&#27861;&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;&#22240;&#26524;&#26641;&#21644;&#22240;&#26524;&#26862;&#26519;&#65289;&#65292;&#23398;&#20064;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#25511;&#21046;&#26102;&#38388;&#21464;&#21270;&#28151;&#28102;&#65292;&#26159;&#21452;&#37325;&#31283;&#20581;&#30340;&#21644;&#21487;&#35299;&#37322;&#30340;&#12290;&#22312;&#27835;&#30103;&#25233;&#37057;&#30151;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#23454;&#38469;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2204.07124</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#20013;&#20351;&#29992;&#22240;&#26524;&#26641;&#26041;&#27861;&#23398;&#20064;&#26368;&#20339;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Dynamic Treatment Regimes Using Causal Tree Methods in Medicine. (arXiv:2204.07124v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07124
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22788;&#29702;&#22797;&#26434;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24322;&#36136;&#24615;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#65292;&#20351;&#29992;&#22240;&#26524;&#26641;&#26041;&#27861;&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;&#22240;&#26524;&#26641;&#21644;&#22240;&#26524;&#26862;&#26519;&#65289;&#65292;&#23398;&#20064;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#25511;&#21046;&#26102;&#38388;&#21464;&#21270;&#28151;&#28102;&#65292;&#26159;&#21452;&#37325;&#31283;&#20581;&#30340;&#21644;&#21487;&#35299;&#37322;&#30340;&#12290;&#22312;&#27835;&#30103;&#25233;&#37057;&#30151;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#23454;&#38469;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#65288;DTR&#65289;&#22312;&#21307;&#23398;&#20013;&#29992;&#20110;&#26681;&#25454;&#24739;&#32773;&#24322;&#36136;&#24615;&#23450;&#21046;&#36830;&#32493;&#30340;&#27835;&#30103;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#23398;&#20064;&#26368;&#20339;DTR&#30340;&#26041;&#27861;&#23384;&#22312;&#32570;&#38519;&#65306;&#23427;&#20204;&#36890;&#24120;&#22522;&#20110;&#32467;&#26524;&#39044;&#27979;&#32780;&#38750;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#65292;&#25110;&#32773;&#20351;&#29992;&#32447;&#24615;&#27169;&#22411;&#65292;&#23545;&#20110;&#29616;&#20195;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#24739;&#32773;&#25968;&#25454;&#20855;&#26377;&#38480;&#21046;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22788;&#29702;&#22797;&#26434;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#31216;&#20026;DTR-CT&#21644;DTR-CF&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24322;&#36136;&#24615;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#65292;&#20351;&#29992;&#22240;&#26524;&#26641;&#26041;&#27861;&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;&#22240;&#26524;&#26641;&#21644;&#22240;&#26524;&#26862;&#26519;&#65289;&#65292;&#23398;&#20064;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#25511;&#21046;&#26102;&#38388;&#21464;&#21270;&#28151;&#28102;&#65292;&#26159;&#21452;&#37325;&#31283;&#20581;&#30340;&#21644;&#21487;&#35299;&#37322;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26159;&#31532;&#19968;&#31687;&#20026;&#20102;&#23398;&#20064;&#26368;&#20339;DTR&#32780;&#25913;&#32534;&#22240;&#26524;&#26641;&#26041;&#27861;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#22312;&#25233;&#37057;&#30151;&#27835;&#30103;&#32972;&#26223;&#19979;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#24212;&#29992;&#31243;&#24207;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#23454;&#38469;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic treatment regimes (DTRs) are used in medicine to tailor sequential treatment decisions to patients by considering patient heterogeneity. Common methods for learning optimal DTRs, however, have shortcomings: they are typically based on outcome prediction and not treatment effect estimation, or they use linear models that are restrictive for patient data from modern electronic health records. To address these shortcomings, we develop two novel methods for learning optimal DTRs that effectively handle complex patient data. We call our methods DTR-CT and DTR-CF. Our methods are based on a data-driven estimation of heterogeneous treatment effects using causal tree methods, specifically causal trees and causal forests, that learn non-linear relationships, control for time-varying confounding, are doubly robust, and explainable. To the best of our knowledge, our paper is the first that adapts causal tree methods for learning optimal DTRs. We evaluate our proposed methods using synthet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;Transformer&#65292;&#21033;&#29992;&#22810;&#28304;&#20114;&#34917;&#20449;&#24687;&#26469;&#25552;&#39640;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.16952</link><description>&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;Transformer
&lt;/p&gt;
&lt;p&gt;
Multimodal Fusion Transformer for Remote Sensing Image Classification. (arXiv:2203.16952v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;Transformer&#65292;&#21033;&#29992;&#22810;&#28304;&#20114;&#34917;&#20449;&#24687;&#26469;&#25552;&#39640;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#20026;&#20102;&#25509;&#36817;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;Transformer&#32593;&#32476;&#65292;&#21033;&#29992;&#22810;&#28304;&#20114;&#34917;&#20449;&#24687;&#26469;&#24110;&#21161;&#39640;&#20809;&#35889;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#21033;&#29992;&#22810;&#22836;&#20132;&#21449;&#34917;&#19969;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;mCrossPA&#65289;&#26469;&#33719;&#21462;&#20854;&#20182;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;Transformer&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs) have been trending in image classification tasks due to their promising performance when compared to convolutional neural networks (CNNs). As a result, many researchers have tried to incorporate ViTs in hyperspectral image (HSI) classification tasks. To achieve satisfactory performance, close to that of CNNs, transformers need fewer parameters. ViTs and other similar transformers use an external classification (CLS) token which is randomly initialized and often fails to generalize well, whereas other sources of multimodal datasets, such as light detection and ranging (LiDAR) offer the potential to improve these models by means of a CLS. In this paper, we introduce a new multimodal fusion transformer (MFT) network which comprises a multihead cross patch attention (mCrossPA) for HSI land-cover classification. Our mCrossPA utilizes other sources of complementary information in addition to the HSI in the transformer encoder to achieve better generalization. The c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38829;&#38180;&#24815;&#24615;&#26799;&#24230;&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#22312;&#38381;&#29615;&#20248;&#21270;&#20013;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#65292;&#22312;&#26377;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#23547;&#25214;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#12290;&#35813;&#31639;&#27861;&#30340;&#25506;&#32034;&#24335;&#21551;&#21457;&#24335;&#21464;&#20307;&#21487;&#20197;&#26377;&#25928;&#36867;&#31163;&#38797;&#28857;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#22810;&#39033;&#24335;&#21644;&#25351;&#25968;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2203.02140</link><description>&lt;p&gt;
&#38829;&#38180;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Whiplash Gradient Descent Dynamics. (arXiv:2203.02140v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38829;&#38180;&#24815;&#24615;&#26799;&#24230;&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#22312;&#38381;&#29615;&#20248;&#21270;&#20013;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#65292;&#22312;&#26377;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#23547;&#25214;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#12290;&#35813;&#31639;&#27861;&#30340;&#25506;&#32034;&#24335;&#21551;&#21457;&#24335;&#21464;&#20307;&#21487;&#20197;&#26377;&#25928;&#36867;&#31163;&#38797;&#28857;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#22810;&#39033;&#24335;&#21644;&#25351;&#25968;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38829;&#38180;&#24815;&#24615;&#26799;&#24230;&#21160;&#21147;&#23398;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#22312;&#26377;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#25214;&#21040;&#20195;&#20215;&#20989;&#25968;&#26368;&#23567;&#20540;&#30340;&#38381;&#29615;&#20248;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#20026;&#20984;&#20989;&#25968;&#20171;&#32461;&#20102;&#38829;&#38180;&#31995;&#32479;&#30340;&#36763;&#28176;&#36817;&#25910;&#25947;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26494;&#24347;&#24207;&#21015;&#26469;&#35299;&#37322;&#31639;&#27861;&#30340;&#38750;&#20856;&#22411;&#24615;&#36136;&#65292;&#24182;&#20171;&#32461;&#20102;&#38829;&#38180;&#31639;&#27861;&#30340;&#25506;&#32034;&#24335;&#21551;&#21457;&#24335;&#21464;&#20307;&#65292;&#20197;&#20915;&#23450;&#24615;&#22320;&#36867;&#31163;&#38797;&#28857;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31639;&#27861;&#22312;&#21508;&#31181;&#20195;&#20215;&#20989;&#25968;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#31215;&#20998;&#32422;&#26463;&#30028;&#21644;&#26032;&#22411;&#26446;&#38597;&#26222;&#35834;&#22827;&#36895;&#29575;&#26041;&#27861;&#20998;&#26512;&#25910;&#25947;&#36895;&#29575;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#20108;&#27425;&#20195;&#20215;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#21644;&#25351;&#25968;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the Whiplash Inertial Gradient dynamics, a closed-loop optimization method that utilises gradient information, to find the minima of a cost function in finite-dimensional settings. We introduce the symplectic asymptotic convergence analysis for the Whiplash system for convex functions. We also introduce relaxation sequences to explain the non-classical nature of the algorithm and an exploring heuristic variant of the Whiplash algorithm to escape saddle points, deterministically. We study the algorithm's performance for various costs and provide a practical methodology for analyzing convergence rates using integral constraint bounds and a novel Lyapunov rate method. Our results demonstrate polynomial and exponential rates of convergence for quadratic cost functions.
&lt;/p&gt;</description></item><item><title>SAITS&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32570;&#22833;&#20540;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#23545;&#35282;&#32447;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#33021;&#22815;&#26126;&#30830;&#22320;&#25429;&#25417;&#26102;&#38388;&#27493;&#38271;&#20043;&#38388;&#30340;&#26102;&#24207;&#20381;&#36182;&#20851;&#31995;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25554;&#20540;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2202.08516</link><description>&lt;p&gt;
SAITS: &#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#24207;&#21015;&#25554;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAITS: Self-Attention-based Imputation for Time Series. (arXiv:2202.08516v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08516
&lt;/p&gt;
&lt;p&gt;
SAITS&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32570;&#22833;&#20540;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#23545;&#35282;&#32447;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#33021;&#22815;&#26126;&#30830;&#22320;&#25429;&#25417;&#26102;&#38388;&#27493;&#38271;&#20043;&#38388;&#30340;&#26102;&#24207;&#20381;&#36182;&#20851;&#31995;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25554;&#20540;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#24120;&#24120;&#25104;&#20026;&#28145;&#20837;&#20998;&#26512;&#30340;&#38556;&#30861;&#12290;&#25554;&#20540;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20854;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#30830;&#23450;&#32570;&#22833;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32570;&#22833;&#20540;&#25554;&#20540;&#26041;&#27861;&#8212;&#8212;SAITS&#12290;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#65292;SAITS&#36890;&#36807;&#20004;&#20010;&#23545;&#35282;&#32447;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#23398;&#20064;&#32570;&#22833;&#20540;&#12290;&#23545;&#35282;&#32447;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#22359;&#21487;&#20197;&#26126;&#30830;&#22320;&#25429;&#25417;&#26102;&#38388;&#27493;&#38271;&#20043;&#38388;&#30340;&#26102;&#24207;&#20381;&#36182;&#20851;&#31995;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25554;&#20540;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#21516;&#26102;&#65292;&#21152;&#26435;&#32452;&#21512;&#35774;&#35745;&#20351;&#24471;SAITS&#33021;&#22815;&#26681;&#25454;&#27880;&#24847;&#21147;&#22270;&#21644;&#32570;&#22833;&#20449;&#24687;&#21160;&#24577;&#22320;&#20998;&#37197;&#26469;&#33258;&#20004;&#20010;&#23545;&#35282;&#32447;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#22359;&#30340;&#23398;&#20064;&#34920;&#31034;&#30340;&#26435;&#37325;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SAITS&#22312;&#26102;&#38388;&#24207;&#21015;&#25554;&#20540;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data in time series is a pervasive problem that puts obstacles in the way of advanced analysis. A popular solution is imputation, where the fundamental challenge is to determine what values should be filled in. This paper proposes SAITS, a novel method based on the self-attention mechanism for missing value imputation in multivariate time series. Trained by a joint-optimization approach, SAITS learns missing values from a weighted combination of two diagonally-masked self-attention (DMSA) blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, which improves imputation accuracy and training speed. Meanwhile, the weighted-combination design enables SAITS to dynamically assign weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments quantitatively and qualitatively demonstrate that SAITS outperforms the state-of-the-art methods on the time-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;FL&#20013;TEE&#30340;&#28431;&#27934;&#65292;&#24182;&#22312;TEE&#20013;&#24341;&#20837;Oblivious Memory Access&#65288;OMA&#65289;&#20197;&#20445;&#25252;&#20813;&#21463;&#31232;&#30095;&#21270;&#39118;&#38505;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;OLIVE&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#32858;&#21512;&#21644;&#24046;&#20998;&#38544;&#31169;FL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2202.07165</link><description>&lt;p&gt;
OLIVE: &#22522;&#20110;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#38450;&#33539;&#31232;&#30095;&#24615;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
OLIVE: Oblivious Federated Learning on Trusted Execution Environment against the risk of sparsification. (arXiv:2202.07165v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;FL&#20013;TEE&#30340;&#28431;&#27934;&#65292;&#24182;&#22312;TEE&#20013;&#24341;&#20837;Oblivious Memory Access&#65288;OMA&#65289;&#20197;&#20445;&#25252;&#20813;&#21463;&#31232;&#30095;&#21270;&#39118;&#38505;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;OLIVE&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#32858;&#21512;&#21644;&#24046;&#20998;&#38544;&#31169;FL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;(TEE)&#30340;&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;FL&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#23398;&#26415;&#20851;&#27880;&#12290;&#22312;&#26381;&#21153;&#22120;&#31471;&#23454;&#29616;TEE&#21487;&#20197;&#20351;&#27599;&#36718;FL&#22312;&#19981;&#23558;&#23458;&#25143;&#31471;&#26799;&#24230;&#20449;&#24687;&#26292;&#38706;&#32473;&#19981;&#21487;&#20449;&#30340;&#26381;&#21153;&#22120;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#36825;&#35299;&#20915;&#20102;&#29616;&#26377;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#20013;&#23384;&#22312;&#30340;&#21487;&#29992;&#24615;&#24046;&#36317;&#20197;&#21450;&#24046;&#20998;&#38544;&#31169;FL&#20013;&#30340;&#25928;&#29992;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#35299;&#20915;&#20351;&#29992;TEE&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#32771;&#34385;&#26381;&#21153;&#22120;&#31471;TEE&#30340;&#28431;&#27934;&#65292;&#36825;&#22312;FL&#30340;&#32972;&#26223;&#19979;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#20998;&#26512;FL&#20013;TEE&#30340;&#28431;&#27934;&#21644;&#38450;&#24481;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#20869;&#23384;&#35775;&#38382;&#27169;&#24335;&#30340;&#27844;&#28431;&#65292;&#25581;&#31034;&#20102;&#31232;&#30095;&#26799;&#24230;&#30340;&#39118;&#38505;&#65292;&#31232;&#30095;&#26799;&#24230;&#36890;&#24120;&#29992;&#20110;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#21644;&#27169;&#22411;&#31934;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25512;&#29702;&#25915;&#20987;&#20197;&#20445;&#25252;&#20813;&#21463;&#31232;&#30095;&#21270;&#39118;&#38505;&#30340;&#24433;&#21709;&#65292;&#35813;&#25915;&#20987;&#20351;&#29992;TEE&#20013;&#30340;&#28151;&#28102;RAM&#24341;&#20837;&#20102;Oblivious Memory Access(OMA)&#12290;&#25105;&#20204;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;OLIVE&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#37117;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#32858;&#21512;&#21644;&#24046;&#20998;&#38544;&#31169;FL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Federated Learning (FL) with a Trusted Execution Environment (TEE) is a promising approach for realizing privacy-preserving FL, which has garnered significant academic attention in recent years. Implementing the TEE on the server side enables each round of FL to proceed without exposing the client's gradient information to untrusted servers. This addresses usability gaps in existing secure aggregation schemes as well as utility gaps in differentially private FL. However, to address the issue using a TEE, the vulnerabilities of server-side TEEs need to be considered -- this has not been sufficiently investigated in the context of FL. The main technical contribution of this study is the analysis of the vulnerabilities of TEE in FL and the defense. First, we theoretically analyze the leakage of memory access patterns, revealing the risk of sparsified gradients, which are commonly used in FL to enhance communication efficiency and model accuracy. Second, we devise an inference at
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#31163;&#25955;&#20223;&#30495;&#20248;&#21270;&#26041;&#27861;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36229;&#21442;&#25968;&#30340;&#36807;&#31243;&#12290;&#20316;&#32773;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25490;&#21517;&#21644;&#36873;&#25321;&#65288;R&amp;S&#65289;&#21644;&#38543;&#26426;&#25628;&#32034;&#31561;&#26041;&#27861;&#35782;&#21035;&#19968;&#20010;&#26368;&#20248;&#36229;&#21442;&#25968;&#38598;&#65292;&#20197;&#26368;&#22823;&#21270;ML&#26041;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.05978</link><description>&lt;p&gt;
&#29992;&#20110;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36229;&#21442;&#25968;&#30340;&#31163;&#25955;&#20223;&#30495;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Discrete Simulation Optimization for Tuning Machine Learning Method Hyperparameters. (arXiv:2201.05978v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#31163;&#25955;&#20223;&#30495;&#20248;&#21270;&#26041;&#27861;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36229;&#21442;&#25968;&#30340;&#36807;&#31243;&#12290;&#20316;&#32773;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25490;&#21517;&#21644;&#36873;&#25321;&#65288;R&amp;S&#65289;&#21644;&#38543;&#26426;&#25628;&#32034;&#31561;&#26041;&#27861;&#35782;&#21035;&#19968;&#20010;&#26368;&#20248;&#36229;&#21442;&#25968;&#38598;&#65292;&#20197;&#26368;&#22823;&#21270;ML&#26041;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22312;&#22270;&#20687;&#35782;&#21035;&#12289;&#20135;&#21697;&#25512;&#33616;&#12289;&#37329;&#34701;&#20998;&#26512;&#12289;&#21307;&#23398;&#35786;&#26029;&#21644;&#39044;&#27979;&#32500;&#25252;&#31561;&#22823;&#37096;&#20998;&#25216;&#26415;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#23454;&#29616;ML&#26041;&#27861;&#30340;&#37325;&#35201;&#26041;&#38754;&#28041;&#21450;&#25511;&#21046;ML&#26041;&#27861;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#26368;&#22823;&#21270;&#25152;&#32771;&#34385;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#36229;&#21442;&#25968;&#35843;&#25972;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;ML&#26041;&#27861;&#21442;&#25968;&#38598;&#20197;&#25511;&#21046;&#20854;&#23398;&#20064;&#36807;&#31243;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#31163;&#25955;&#20223;&#30495;&#20248;&#21270;&#26041;&#27861;&#65292;&#20363;&#22914;&#25490;&#21517;&#21644;&#36873;&#25321;&#65288;R&#65286;S&#65289;&#21644;&#38543;&#26426;&#25628;&#32034;&#65292;&#20197;&#30830;&#23450;&#26368;&#22823;&#21270;ML&#26041;&#27861;&#24615;&#33021;&#30340;&#36229;&#21442;&#25968;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;KN R&#65286;S&#26041;&#27861;&#21644;&#38543;&#26426;&#23610;&#23376;&#38543;&#26426;&#25628;&#32034;&#26041;&#27861;&#21450;&#20854;&#20854;&#20013;&#19968;&#31181;&#21464;&#21270;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#36866;&#29992;KN&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#31354;&#38388;&#26522;&#20030;&#30830;&#23450;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) methods are used in most technical areas such as image recognition, product recommendation, financial analysis, medical diagnosis, and predictive maintenance. An important aspect of implementing ML methods involves controlling the learning process for the ML method so as to maximize the performance of the method under consideration. Hyperparameter tuning is the process of selecting a suitable set of ML method parameters that control its learning process. In this work, we demonstrate the use of discrete simulation optimization methods such as ranking and selection (R&amp;S) and random search for identifying a hyperparameter set that maximizes the performance of a ML method. Specifically, we use the KN R&amp;S method and the stochastic ruler random search method and one of its variations for this purpose. We also construct the theoretical basis for applying the KN method, which determines the optimal solution with a statistical guarantee via solution space enumeration. In c
&lt;/p&gt;</description></item><item><title>&#21452;&#37325;PC&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#21327;&#26041;&#24046;&#21644;&#31934;&#24230;&#30697;&#38453;&#20043;&#38388;&#30340;&#21453;&#21521;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;CI&#27979;&#35797;&#65292;&#33021;&#22815;&#24674;&#22797;&#27491;&#30830;&#30340;&#31561;&#20215;&#31867;&#65292;&#24182;&#21487;&#23545;&#20114;&#34917;&#35843;&#33410;&#38598;&#30340;&#20559;&#30456;&#20851;&#36827;&#34892;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2112.09036</link><description>&lt;p&gt;
&#21452;&#37325;PC&#31639;&#27861;&#21450;&#20854;&#23545;&#39640;&#26031;&#24615;&#36136;&#22312;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Dual PC Algorithm and the Role of Gaussianity for Structure Learning of Bayesian Networks. (arXiv:2112.09036v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09036
&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325;PC&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#21327;&#26041;&#24046;&#21644;&#31934;&#24230;&#30697;&#38453;&#20043;&#38388;&#30340;&#21453;&#21521;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;CI&#27979;&#35797;&#65292;&#33021;&#22815;&#24674;&#22797;&#27491;&#30830;&#30340;&#31561;&#20215;&#31867;&#65292;&#24182;&#21487;&#23545;&#20114;&#34917;&#35843;&#33410;&#38598;&#30340;&#20559;&#30456;&#20851;&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#22270;&#24418;&#32467;&#26500;&#26159;&#25551;&#36848;&#35768;&#22810;&#22797;&#26434;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#20851;&#38190;&#65292;&#20294;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#27969;&#34892;&#30340;PC&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#21464;&#37327;&#20998;&#24067;&#20013;&#25152;&#20855;&#26377;&#30340;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#26469;&#19968;&#33268;&#22320;&#24674;&#22797;&#27491;&#30830;&#30340;&#31561;&#20215;&#31867;&#12290;&#21452;&#37325;PC&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21327;&#26041;&#24046;&#21644;&#31934;&#24230;&#30697;&#38453;&#20043;&#38388;&#30340;&#21453;&#21521;&#20851;&#31995;&#26469;&#36827;&#34892;PC&#31639;&#27861;&#20013;&#30340;CI&#27979;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;&#22359;&#30697;&#38453;&#27714;&#36870;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#23545;&#20114;&#34917;&#65288;&#25110;&#21452;&#37325;&#65289;&#35843;&#33410;&#38598;&#30340;&#20559;&#30456;&#20851;&#36827;&#34892;&#27979;&#35797;&#12290;&#21452;&#37325;PC&#31639;&#27861;&#30340;&#22810;&#20010;CI&#27979;&#35797;&#39318;&#20808;&#32771;&#34385;&#36793;&#32536;&#21644;&#23436;&#20840;&#25490;&#24207;CI&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning the graphical structure of Bayesian networks is key to describing data-generating mechanisms in many complex applications but poses considerable computational challenges. Observational data can only identify the equivalence class of the directed acyclic graph underlying a Bayesian network model, and a variety of methods exist to tackle the problem. Under certain assumptions, the popular PC algorithm can consistently recover the correct equivalence class by reverse-engineering the conditional independence (CI) relationships holding in the variable distribution. The dual PC algorithm is a novel scheme to carry out the CI tests within the PC algorithm by leveraging the inverse relationship between covariance and precision matrices. By exploiting block matrix inversions we can also perform tests on partial correlations of complementary (or dual) conditioning sets. The multiple CI tests of the dual PC algorithm proceed by first considering marginal and full-order CI relationships a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#23616;&#37096;&#21270;&#30340;LSTM&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#36947;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#19982;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#27169;&#22411;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.02409</link><description>&lt;p&gt;
&#38024;&#23545;&#36947;&#36335;&#20132;&#36890;&#36895;&#24230;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;&#20013;&#21160;&#24577;&#26102;&#31354;&#32972;&#26223;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding Dynamic Spatio-Temporal Contexts in Long Short-Term Memory for Road Traffic Speed Prediction. (arXiv:2112.02409v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#23616;&#37096;&#21270;&#30340;LSTM&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#36947;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#19982;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#27169;&#22411;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#20132;&#36890;&#27969;&#39044;&#27979;&#23545;&#20110;&#21019;&#24314;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#35768;&#22810;&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#39044;&#27979;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#21453;&#26144;&#20986;&#32771;&#34385;&#26102;&#38388;&#21644;&#20301;&#32622;&#30340;&#22797;&#26434;&#21160;&#24577;&#36947;&#36335;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#23616;&#37096;&#21270;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#65292;&#28041;&#21450;&#36947;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#23616;&#37096;&#21160;&#24577;&#31354;&#38388;&#26435;&#37325;&#30697;&#38453;&#21450;&#20854;&#21160;&#24577;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;LSTM&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#38271;&#20381;&#36182;&#24615;&#21644;&#22797;&#26434;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;&#24207;&#21015;&#25968;&#25454;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#27604;&#20110;&#20004;&#31181;&#19981;&#21516;&#30340;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable traffic flow prediction is crucial to creating intelligent transportation systems. Many big-data-based prediction approaches have been developed but they do not reflect complicated dynamic interactions between roads considering time and location. In this study, we propose a dynamically localised long short-term memory (LSTM) model that involves both spatial and temporal dependence between roads. To do so, we use a localised dynamic spatial weight matrix along with its dynamic variation. Moreover, the LSTM model can deal with sequential data with long dependency as well as complex non-linear features. Empirical results indicated superior prediction performances of the proposed model compared to two different baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#31995;&#32479;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#21644;&#25511;&#21046;&#26041;&#27861;&#65292;&#37319;&#29992;&#24207;&#21015;&#24314;&#27169;&#33539;&#20363;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#65292;&#23558;&#38750;&#32447;&#24615;&#21160;&#24577;&#20998;&#35299;&#20026;&#20855;&#26377;&#38750;&#32447;&#24615;&#36716;&#25442;&#36793;&#30028;&#30340;&#38543;&#26426;&#20998;&#27573;&#20223;&#23556;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2111.06211</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#28151;&#21512;&#27169;&#22411;&#30340;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Based Reinforcement Learning via Stochastic Hybrid Models. (arXiv:2111.06211v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#31995;&#32479;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#21644;&#25511;&#21046;&#26041;&#27861;&#65292;&#37319;&#29992;&#24207;&#21015;&#24314;&#27169;&#33539;&#20363;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#65292;&#23558;&#38750;&#32447;&#24615;&#21160;&#24577;&#20998;&#35299;&#20026;&#20855;&#26377;&#38750;&#32447;&#24615;&#36716;&#25442;&#36793;&#30028;&#30340;&#38543;&#26426;&#20998;&#27573;&#20223;&#23556;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36866;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#26159;&#33258;&#21160;&#21270;&#39046;&#22495;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#22522;&#20110;&#24378;&#22823;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#25511;&#21046;&#26041;&#27861;&#26368;&#36817;&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#25361;&#25112;&#24615;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#40657;&#31665;&#36807;&#24230;&#21442;&#25968;&#21270;&#34920;&#31034;&#27861;&#38544;&#34255;&#20102;&#21160;&#24577;&#21644;&#25511;&#21046;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#25105;&#20204;&#29702;&#35299;&#38381;&#21512;&#29615;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#37319;&#29992;&#38750;&#32447;&#24615;&#24314;&#27169;&#21644;&#25511;&#21046;&#30340;&#28151;&#21512;&#31995;&#32479;&#35270;&#22270;&#65292;&#20026;&#38382;&#39064;&#25552;&#20379;&#20102;&#26174;&#24335;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#24182;&#23558;&#22797;&#26434;&#21160;&#24577;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#23616;&#37096;&#21333;&#20803;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#25429;&#25417;&#25968;&#25454;&#26102;&#38388;&#32467;&#26500;&#30340;&#24207;&#21015;&#24314;&#27169;&#33539;&#20363;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31181;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33258;&#21160;&#23558;&#38750;&#32447;&#24615;&#21160;&#24577;&#20998;&#35299;&#20026;&#20855;&#26377;&#38750;&#32447;&#24615;&#36716;&#25442;&#36793;&#30028;&#30340;&#38543;&#26426;&#20998;&#27573;&#20223;&#23556;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#33258;&#28982;&#22320;&#20855;&#26377;&#38381;&#29615;&#25193;&#23637;&#65292;&#25105;&#20204;&#21033;&#29992;&#23427;&#26469;&#25552;&#21462;&#23616;&#37096;&#27169;&#22411;&#30340;LQR&#65288;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#65289;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal control of general nonlinear systems is a central challenge in automation. Enabled by powerful function approximators, data-driven approaches to control have recently successfully tackled challenging applications. However, such methods often obscure the structure of dynamics and control behind black-box over-parameterized representations, thus limiting our ability to understand closed-loop behavior. This paper adopts a hybrid-system view of nonlinear modeling and control that lends an explicit hierarchical structure to the problem and breaks down complex dynamics into simpler localized units. We consider a sequence modeling paradigm that captures the temporal structure of the data and derive an expectation-maximization (EM) algorithm that automatically decomposes nonlinear dynamics into stochastic piecewise affine models with nonlinear transition boundaries. Furthermore, we show that these time-series models naturally admit a closed-loop extension that we use to extract local p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ODE&#26041;&#27861;&#30340;&#28176;&#36817;&#32479;&#35745;&#26041;&#27861;&#35299;&#20915;$d$&#32500;&#38543;&#26426;&#36924;&#36817;&#36882;&#24402;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2110.14427</link><description>&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#28176;&#36817;&#32479;&#35745;&#30340;ODE&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The ODE Method for Asymptotic Statistics in Stochastic Approximation and Reinforcement Learning. (arXiv:2110.14427v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ODE&#26041;&#27861;&#30340;&#28176;&#36817;&#32479;&#35745;&#26041;&#27861;&#35299;&#20915;$d$&#32500;&#38543;&#26426;&#36924;&#36817;&#36882;&#24402;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;$d$&#32500;&#38543;&#26426;&#36924;&#36817;&#36882;&#24402;$$\theta_{n+1}=\theta_n+\alpha_{n+1}f(\theta_n, \Phi_{n+1})$$&#20854;&#20013;$\Phi$&#26159;&#19968;&#20010;&#22312;&#19968;&#33324;&#29366;&#24577;&#31354;&#38388;$\textsf{X}$&#19978;&#20855;&#26377;&#24179;&#31283;&#20998;&#24067;$\pi$&#30340;&#20960;&#20309;&#36941;&#21382;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;$f&#65306;\Re^d\times\textsf{X}\to\Re^d$&#12290;&#22312;&#31216;&#20026;&#65288;DV3&#65289;&#30340;Donsker-Varadhan Lyapunov&#28418;&#31227;&#26465;&#20214;&#30340;&#19968;&#31181;&#29256;&#26412;&#21644;&#23545;&#20855;&#26377;&#21521;&#37327;&#22330;$\bar{f}(\theta)=\textsf{E}[f(\theta,\Phi)]$&#20197;&#21450;$\Phi\sim\pi$&#30340;&#22343;&#20540;&#27969;&#30340;&#31283;&#23450;&#24615;&#26465;&#20214;&#19979;&#65292;&#24314;&#31435;&#20102;&#20027;&#35201;&#32467;&#26524;&#12290;(i) $\{\theta_n\}$&#20197;&#27010;&#29575;1&#21644;$L_4$&#25910;&#25947;&#20110;$\bar{f}(\theta)$&#30340;&#21807;&#19968;&#26681;$\theta^*$&#12290;(ii) &#24314;&#31435;&#20102;&#27867;&#20989;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#20197;&#21450;&#24402;&#19968;&#21270;&#35823;&#24046;&#19968;&#32500;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;(iii) &#23545;&#20110;&#24402;&#19968;&#21270;&#29256;&#26412;$z_n{=:} \sqrt{n} (\theta^{\text{PR}}_n -\theta^*)$&#30340;&#24179;&#22343;&#21442;&#25968;$\theta^{\text{PR}}_n {=:} n^{-1} \sum_{k=1}^n\theta_k$ &#65292;&#22312;&#27493;&#38271;&#30340;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#24314;&#31435;&#20102;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper concerns the $d$-dimensional stochastic approximation recursion, $$ \theta_{n+1}= \theta_n + \alpha_{n + 1} f(\theta_n, \Phi_{n+1}) $$ in which $\Phi$ is a geometrically ergodic Markov chain on a general state space $\textsf{X}$ with stationary distribution $\pi$, and $f:\Re^d\times\textsf{X}\to\Re^d$.  The main results are established under a version of the Donsker-Varadhan Lyapunov drift condition known as (DV3), and a stability condition for the mean flow with vector field $\bar{f}(\theta)=\textsf{E}[f(\theta,\Phi)]$, with $\Phi\sim\pi$.  (i) $\{ \theta_n\}$ is convergent a.s. and in $L_4$ to the unique root $\theta^*$ of $\bar{f}(\theta)$.  (ii) A functional CLT is established, as well as the usual one-dimensional CLT for the normalized error.  (iii) The CLT holds for the normalized version, $z_n{=:} \sqrt{n} (\theta^{\text{PR}}_n -\theta^*)$, of the averaged parameters, $\theta^{\text{PR}}_n {=:} n^{-1} \sum_{k=1}^n\theta_k$, subject to standard assumptions on the step-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#20998;&#35299;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20811;&#26381;&#22312;&#20351;&#29992;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#20026;&#24314;&#27169;&#21644;&#25968;&#25454;&#25910;&#38598;&#25552;&#20379;&#30452;&#25509;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2110.12122</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantifying Epistemic Uncertainty in Deep Learning. (arXiv:2110.12122v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.12122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#20998;&#35299;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20811;&#26381;&#22312;&#20351;&#29992;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#20026;&#24314;&#27169;&#21644;&#25968;&#25454;&#25910;&#38598;&#25552;&#20379;&#30452;&#25509;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26680;&#24515;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#20998;&#35299;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;\textit {&#35748;&#35782;&#25104;&#20998;}&#65307;&#20998;&#20026;\textit {&#31243;&#24207;&#21464;&#24322;&#24615;}(&#26469;&#33258;&#35757;&#32451;&#36807;&#31243;)&#21644;\textit {&#25968;&#25454;&#21464;&#24322;&#24615;} (&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;)&#65292;&#36825;&#26159;&#25991;&#29486;&#20013;&#39318;&#27425;&#23581;&#35797;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20004;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#25209;&#27425;&#21270;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20811;&#26381;&#22312;&#24212;&#29992;&#32463;&#20856;&#32479;&#35745;&#26041;&#27861;&#26102;&#36935;&#21040;&#30340;&#35745;&#31639;&#22256;&#38590;&#12290;&#22810;&#20010;&#38382;&#39064;&#35774;&#32622;&#30340;&#23454;&#39564;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#20272;&#35745;&#22914;&#20309;&#25552;&#20379;&#23545;&#24314;&#27169;&#21644;&#25968;&#25454;&#25910;&#38598;&#24037;&#20316;&#30340;&#30452;&#25509;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is at the core of the reliability and robustness of machine learning. In this paper, we provide a theoretical framework to dissect the uncertainty, especially the \textit{epistemic} component, in deep learning into \textit{procedural variability} (from the training procedure) and \textit{data variability} (from the training data), which is the first such attempt in the literature to our best knowledge. We then propose two approaches to estimate these uncertainties, one based on influence function and one on batching. We demonstrate how our approaches overcome the computational difficulties in applying classical statistical methods. Experimental evaluations on multiple problem settings corroborate our theory and illustrate how our framework and estimation can provide direct guidance on modeling and data collection efforts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20998;&#31163;&#24335;&#23545;&#27604;&#23398;&#20064;&#65288;DCL&#65289;&#25439;&#22833;&#65292;&#36890;&#36807;&#21435;&#38500;&#20256;&#32479;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#27491;&#39033;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2110.06848</link><description>&lt;p&gt;
&#20998;&#31163;&#24335;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupled Contrastive Learning. (arXiv:2110.06848v3 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20998;&#31163;&#24335;&#23545;&#27604;&#23398;&#20064;&#65288;DCL&#65289;&#25439;&#22833;&#65292;&#36890;&#36807;&#21435;&#38500;&#20256;&#32479;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#27491;&#39033;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#25104;&#21151;&#30340;&#33539;&#20363;&#20043;&#19968;&#12290;&#23427;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#65292;&#23558;&#21516;&#19968;&#22270;&#20687;&#30340;&#20004;&#20010;&#22686;&#24378;&#8220;&#35270;&#22270;&#8221;&#35270;&#20026;&#27491;&#38754;&#65292;&#23558;&#25152;&#26377;&#20854;&#20182;&#22270;&#20687;&#35270;&#20026;&#36127;&#38754;&#65292;&#20197;&#25289;&#36817;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;CL&#30340;&#25216;&#26415;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#32972;&#21518;&#65292;&#23427;&#20204;&#30340;&#21046;&#23450;&#36890;&#24120;&#20381;&#36182;&#20110;&#37325;&#35745;&#31639;&#35774;&#32622;&#65292;&#21253;&#25324;&#22823;&#30340;&#26679;&#26412;&#25209;&#27425;&#12289;&#24191;&#27867;&#30340;&#35757;&#32451;&#26102;&#26399;&#31561;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26377;&#21160;&#21147;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#26377;&#31454;&#20105;&#21147;&#30340;&#23545;&#27604;&#23398;&#20064;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning (CL) is one of the most successful paradigms for self-supervised learning (SSL). In a principled way, it considers two augmented "views" of the same image as positive to be pulled closer, and all other images as negative to be pushed further apart. However, behind the impressive success of CL-based techniques, their formulation often relies on heavy-computation settings, including large sample batches, extensive training epochs, etc. We are thus motivated to tackle these issues and establish a simple, efficient, yet competitive baseline of contrastive learning. Specifically, we identify, from theoretical and empirical studies, a noticeable negative-positive-coupling (NPC) effect in the widely used InfoNCE loss, leading to unsuitable learning efficiency concerning the batch size. By removing the NPC effect, we propose decoupled contrastive learning (DCL) loss, which removes the positive term from the denominator and significantly improves the learning efficiency. DC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;NTK&#23545;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#36830;&#32493;&#26102;&#38388;&#20998;&#26512;&#65292;&#21457;&#29616;&#22122;&#22768;&#21482;&#20250;&#24433;&#21709;&#38544;&#31169;&#39118;&#38505;&#32780;&#19981;&#24433;&#21709;&#25910;&#25947;&#24615;&#21644;&#26657;&#20934;&#24615;&#65292;&#32780;&#22522;&#20110;&#27599;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#21098;&#35009;&#20250;&#24433;&#21709;&#25910;&#25947;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#21098;&#35009;&#33539;&#25968;&#19979;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#19981;&#20165;&#20139;&#26377;&#30456;&#21516;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#32780;&#19988;&#26657;&#20934;&#25928;&#26524;&#22909;&#12290;</title><link>http://arxiv.org/abs/2106.07830</link><description>&lt;p&gt;
&#35770;&#28145;&#24230;&#23398;&#20064;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#25910;&#25947;&#24615;&#19982;&#26657;&#20934;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Convergence and Calibration of Deep Learning with Differential Privacy. (arXiv:2106.07830v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;NTK&#23545;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#36830;&#32493;&#26102;&#38388;&#20998;&#26512;&#65292;&#21457;&#29616;&#22122;&#22768;&#21482;&#20250;&#24433;&#21709;&#38544;&#31169;&#39118;&#38505;&#32780;&#19981;&#24433;&#21709;&#25910;&#25947;&#24615;&#21644;&#26657;&#20934;&#24615;&#65292;&#32780;&#22522;&#20110;&#27599;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#21098;&#35009;&#20250;&#24433;&#21709;&#25910;&#25947;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#21098;&#35009;&#33539;&#25968;&#19979;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#19981;&#20165;&#20139;&#26377;&#30456;&#21516;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#32780;&#19988;&#26657;&#20934;&#25928;&#26524;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#36890;&#24120;&#20250;&#20197;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#20026;&#20195;&#20215;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#65288;&#20174;&#32780;&#31934;&#24230;&#38477;&#20302;&#65289;&#65292;&#19988;&#27604;&#38750;&#38544;&#31169;&#26041;&#27861;&#26356;&#23481;&#26131;&#20986;&#29616;&#20005;&#37325;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#20174;&#36830;&#32493;&#26102;&#38388;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#65292;&#23545;&#20219;&#24847;&#32593;&#32476;&#32467;&#26500;&#21644;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#21457;&#29616;&#22122;&#22768;&#21482;&#20250;&#24433;&#21709;&#38544;&#31169;&#39118;&#38505;&#32780;&#19981;&#24433;&#21709;&#25910;&#25947;&#24615;&#21644;&#26657;&#20934;&#24615;&#65292;&#32780;&#22522;&#20110;&#27599;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#21098;&#35009;&#65288;&#22312;&#24179;&#22374;&#21644;&#23618;&#32423;&#21098;&#35009;&#39118;&#26684;&#19979;&#65289;&#20250;&#24433;&#21709;&#25910;&#25947;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#23567;&#21098;&#35009;&#33539;&#25968;&#19979;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#36890;&#24120;&#20250;&#22312;&#31934;&#24230;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#20854;&#26657;&#20934;&#24615;&#36739;&#24046;&#19988;&#19981;&#21487;&#38752;&#12290;&#32780;&#22312;&#22823;&#21098;&#35009;&#33539;&#25968;&#19979;&#35757;&#32451;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#19981;&#20165;&#20139;&#26377;&#30456;&#21516;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#32780;&#19988;&#26657;&#20934;&#25928;&#26524;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private (DP) training preserves the data privacy usually at the cost of slower convergence (and thus lower accuracy), as well as more severe mis-calibration than its non-private counterpart. To analyze the convergence of DP training, we formulate a continuous time analysis through the lens of neural tangent kernel (NTK), which characterizes the per-sample gradient clipping and the noise addition in DP training, for arbitrary network architectures and loss functions. Interestingly, we show that the noise addition only affects the privacy risk but not the convergence or calibration, whereas the per-sample gradient clipping (under both flat and layerwise clipping styles) only affects the convergence and calibration.  Furthermore, we observe that while DP models trained with small clipping norm usually achieve the best accurate, but are poorly calibrated and thus unreliable. In sharp contrast, DP models trained with large clipping norm enjoy the same privacy guarantee and si
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25226;&#31243;&#24207;&#21512;&#25104;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#21151;&#29575;&#12290;&#20316;&#32773;&#20351;&#29992;&#20102;&#30001;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#26500;&#24314;&#30340;Cross Aggregator&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#65292;&#23398;&#20064;&#22914;&#20309;&#32452;&#21512;&#27599;&#20010;&#26679;&#20363;&#31243;&#24207;&#35299;&#20915;&#26041;&#26696;&#65292;&#29983;&#25104;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2106.07175</link><description>&lt;p&gt;
&#23398;&#20064;&#23558;&#21608;&#20363;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#32452;&#21512;&#29992;&#20110;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning to Combine Per-Example Solutions for Neural Program Synthesis. (arXiv:2106.07175v2 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07175
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25226;&#31243;&#24207;&#21512;&#25104;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#21151;&#29575;&#12290;&#20316;&#32773;&#20351;&#29992;&#20102;&#30001;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#26500;&#24314;&#30340;Cross Aggregator&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#65292;&#23398;&#20064;&#22914;&#20309;&#32452;&#21512;&#27599;&#20010;&#26679;&#20363;&#31243;&#24207;&#35299;&#20915;&#26041;&#26696;&#65292;&#29983;&#25104;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20363;&#23376;&#31243;&#24207;&#21512;&#25104;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#19982;&#32473;&#23450;&#30340;&#36755;&#20837;&#36755;&#20986;&#26679;&#20363;&#19968;&#33268;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#23581;&#35797;&#25214;&#21040;&#28385;&#36275;&#25152;&#26377;&#26679;&#20363;&#30340;&#31243;&#24207;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#32771;&#34385;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#65306;(a)&#25214;&#21040;&#21482;&#28385;&#36275;&#19968;&#20010;&#26679;&#20363;&#30340;&#31243;&#24207;&#65292;(b)&#21033;&#29992;&#36825;&#20123;&#21333;&#20363;&#31243;&#24207;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#28385;&#36275;&#25152;&#26377;&#26679;&#20363;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#30340;Cross Aggregator&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#65292;&#23398;&#20064;&#22914;&#20309;&#32452;&#21512;&#36825;&#20123;&#21333;&#20363;&#26041;&#26696;&#20013;&#23384;&#22312;&#30340;&#25552;&#31034;&#26469;&#21512;&#25104;&#19968;&#20010;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#19981;&#21516;&#38271;&#24230;&#30340;&#31243;&#24207;&#21644;&#20004;&#31181;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#19979;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#24403;&#22312;&#30456;&#21516;&#30340;&#26102;&#38388;&#39044;&#31639;&#19979;&#26102;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#65292;&#36229;&#36807;&#20102;PCCoder [Zohar&#31561;&#20154;&#65292;2018]&#21644;&#20854;&#20182;&#28040;&#34701;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;https://github.com/shriva&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of program synthesis from examples is to find a computer program that is consistent with a given set of input-output examples. Most learning-based approaches try to find a program that satisfies all examples at once. Our work, by contrast, considers an approach that breaks the problem into two stages: (a) find programs that satisfy only one example, and (b) leverage these per-example solutions to yield a program that satisfies all examples. We introduce the Cross Aggregator neural network module based on a multi-head attention mechanism that learns to combine the cues present in these per-example solutions to synthesize a global solution. Evaluation across programs of different lengths and under two different experimental settings reveal that when given the same time budget, our technique significantly improves the success rate over PCCoder [Zohar et. al 2018] and other ablation baselines. The code, data and trained models for our work can be found at https://github.com/shriva
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#37319;&#26679;&#35270;&#35282;&#30340;GAN&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28304;&#20998;&#24067;&#19979;&#23454;&#29616;&#36870;&#37319;&#26679;&#30340;IID&#26679;&#26412;&#19982;&#30495;&#23454;&#25968;&#25454;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#39640;&#26031;&#28304;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#35268;&#33539;&#21270;&#20102;GAN&#20013;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2106.00563</link><description>&lt;p&gt;
IID-GAN&#65306;&#19968;&#31181;&#29420;&#31435;&#21516;&#20998;&#24067;&#37319;&#26679;&#35270;&#35282;&#19979;&#30340;&#35268;&#33539;&#27169;&#24335;&#23849;&#28291;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
IID-GAN: an IID Sampling Perspective for Regularizing Mode Collapse. (arXiv:2106.00563v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.00563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#37319;&#26679;&#35270;&#35282;&#30340;GAN&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28304;&#20998;&#24067;&#19979;&#23454;&#29616;&#36870;&#37319;&#26679;&#30340;IID&#26679;&#26412;&#19982;&#30495;&#23454;&#25968;&#25454;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#39640;&#26031;&#28304;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#35268;&#33539;&#21270;&#20102;GAN&#20013;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#24456;&#25104;&#21151;&#65292;&#20294;&#23427;&#20173;&#28982;&#21463;&#21040;&#27169;&#24335;&#23849;&#28291;&#30340;&#24433;&#21709;&#65292;&#21363;&#29983;&#25104;&#22120;&#21482;&#33021;&#23558;&#28508;&#22312;&#21464;&#37327;&#26144;&#23556;&#21040;&#30446;&#26631;&#20998;&#24067;&#20013;&#37096;&#20998;&#27169;&#24335;&#12290;&#26412;&#25991;&#20998;&#26512;&#24182;&#35797;&#22270;&#36890;&#36807;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#37319;&#26679;&#35270;&#35282;&#26469;&#35268;&#33539;&#21270;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20445;&#25345;&#29983;&#25104;&#30340;IID&#23646;&#24615;&#21487;&#20197;&#33258;&#28982;&#22320;&#36991;&#20813;&#27169;&#24335;&#23849;&#28291;&#12290;&#36825;&#26159;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#23454;&#38469;&#25968;&#25454;&#30340;&#22522;&#30784;IID&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28304;&#26679;&#26412;{z}&#26381;&#20174;IID&#65292;&#20294;&#29983;&#25104;&#29289;{G&#65288;z&#65289;}&#19981;&#19968;&#23450;&#26159;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#37319;&#26679;&#24471;&#21040;&#30340;IID&#26679;&#26412;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#32771;&#34385;&#21040;&#23454;&#29616;IID&#29983;&#25104;&#30340;&#24517;&#35201;&#26465;&#20214;&#26159;&#20174;&#30446;&#26631;&#25968;&#25454;&#20013;&#36870;&#37319;&#26679;&#30340;&#26679;&#26412;&#22312;&#28304;&#20998;&#24067;&#19979;&#20063;&#24212;&#35813;&#26159;IID&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#26469;&#40723;&#21169;&#30495;&#23454;&#25968;&#25454;&#30340;&#36870;&#37319;&#26679;&#21644;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#39640;&#26031;&#28304;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#35268;&#33539;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its success, generative adversarial networks (GANs) still suffer from mode collapse, i.e., the generator can only map latent variables to a partial set of modes in the target distribution. In this paper, we analyze and seek to regularize this issue with an independent and identically distributed (IID) sampling perspective and emphasize that holding the IID property referring to the target distribution for generation can naturally avoid mode collapse. This is based on the basic IID assumption for real data in machine learning. However, though the source samples {z} obey IID, the generations {G(z)} may not necessarily be IID sampling from the target distribution. Based on this observation, considering a necessary condition of IID generation that the inverse samples from target data should also be IID in the source distribution, we propose a new loss to encourage the closeness between inverse samples of real data and the Gaussian source in latent space to regularize the generation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#20840;&#21464;&#24046;&#26368;&#23567;&#21270;&#30340;&#23436;&#20840;&#20998;&#25955;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20855;&#26377;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#20998;&#25955;&#24335;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#30340;&#26412;&#22320;&#21270;&#65288;&#25110;&#20010;&#24615;&#21270;&#65289;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#27169;&#25311;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2105.12769</link><description>&lt;p&gt;
&#22522;&#20110;&#24191;&#20041;&#20840;&#21464;&#24046;&#26368;&#23567;&#21270;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Clustered Federated Learning via Generalized Total Variation Minimization. (arXiv:2105.12769v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.12769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#20840;&#21464;&#24046;&#26368;&#23567;&#21270;&#30340;&#23436;&#20840;&#20998;&#25955;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20855;&#26377;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#20998;&#25955;&#24335;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#30340;&#26412;&#22320;&#21270;&#65288;&#25110;&#20010;&#24615;&#21270;&#65289;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#27169;&#25311;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20869;&#22312;&#32593;&#32476;&#32467;&#26500;&#30340;&#20998;&#25955;&#24335;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#19979;&#35757;&#32451;&#26412;&#22320;&#65288;&#25110;&#20010;&#24615;&#21270;&#65289;&#27169;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#36825;&#31181;&#32593;&#32476;&#32467;&#26500;&#26159;&#30001;&#26412;&#22320;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#29305;&#23450;&#30456;&#20284;&#24615;&#27010;&#24565;&#24341;&#36215;&#30340;&#12290;&#36825;&#20123;&#27010;&#24565;&#30340;&#20363;&#23376;&#21253;&#25324;&#26102;&#31354;&#37051;&#36817;&#24615;&#65292;&#32479;&#35745;&#20381;&#36182;&#24615;&#25110;&#21151;&#33021;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#27010;&#24565;&#24615;&#36129;&#29486;&#22312;&#20110;&#23558;&#32852;&#37030;&#23398;&#20064;&#25551;&#36848;&#20026;&#24191;&#20041;&#24635;&#21464;&#24046;&#65288;GTV&#65289;&#26368;&#23567;&#21270;&#12290;&#36825;&#31181;&#34920;&#36848;&#32479;&#19968;&#24182;&#26174;&#33879;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#20855;&#26377;&#39640;&#24230;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#24191;&#27867;&#30340;&#21442;&#25968;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#21253;&#25324;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#25110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#31639;&#27861;&#36129;&#29486;&#26159;&#19968;&#31181;&#23436;&#20840;&#20998;&#25955;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26159;&#36890;&#36807;&#24212;&#29992;&#24050;&#24314;&#31435;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#26469;&#35299;&#20915;GTV&#26368;&#23567;&#21270;&#38382;&#39064;&#32780;&#33719;&#24471;&#30340;&#12290;&#23427;&#21487;&#20197;&#23454;&#29616;&#20026;&#28040;&#24687;&#20256;&#36882;&#65292;&#24182;&#19988;&#23545;&#20110;&#30001;&#36890;&#20449;&#24310;&#36831;&#25110;&#24178;&#25200;&#20135;&#29983;&#30340;&#19981;&#31934;&#30830;&#35745;&#31639;&#24456;&#31283;&#20581;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study optimization methods to train local (or personalized) models for decentralized collections of local datasets with an intrinsic network structure. This network structure arises from domain-specific notions of similarity between local datasets. Examples for such notions include spatio-temporal proximity, statistical dependencies or functional relations. Our main conceptual contribution is to formulate federated learning as generalized total variation (GTV) minimization. This formulation unifies and considerably extends existing federated learning methods. It is highly flexible and can be combined with a broad range of parametric models, including generalized linear models or deep neural networks. Our main algorithmic contribution is a fully decentralized federated learning algorithm. This algorithm is obtained by applying an established primal-dual method to solve GTV minimization. It can be implemented as message passing and is robust against inexact computations that arise fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#20302;&#24310;&#36831;MAC&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#26080;&#38656;&#26174;&#24335;&#29366;&#24577;&#20449;&#24687;&#20849;&#20139;&#19988;&#20855;&#26377;&#20998;&#24067;&#24335;&#25511;&#21046;&#12290;&#35813;&#21327;&#35758;&#37319;&#29992;&#8220;&#36138;&#24515;&#8221;&#21644;&#8220;&#31351;&#23613;&#8221;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#27969;&#37327;&#31454;&#20105;&#35775;&#38382;&#26102;&#23454;&#29616;&#20302;&#24310;&#36831;&#65292;&#22312;&#37325;&#36127;&#36733;&#19979;&#23454;&#29616;&#23450;&#26102;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2105.11213</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#20302;&#24310;&#36831;MAC&#21327;&#35758;&#65306;&#26080;&#38656;&#26174;&#24335;&#29366;&#24577;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#25955;&#24335;&#26368;&#20248;&#38431;&#21015;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
A Low-Delay MAC for IoT Applications: Decentralized Optimal Scheduling of Queues without Explicit State Information Sharing. (arXiv:2105.11213v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.11213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#20302;&#24310;&#36831;MAC&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#26080;&#38656;&#26174;&#24335;&#29366;&#24577;&#20449;&#24687;&#20849;&#20139;&#19988;&#20855;&#26377;&#20998;&#24067;&#24335;&#25511;&#21046;&#12290;&#35813;&#21327;&#35758;&#37319;&#29992;&#8220;&#36138;&#24515;&#8221;&#21644;&#8220;&#31351;&#23613;&#8221;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#27969;&#37327;&#31454;&#20105;&#35775;&#38382;&#26102;&#23454;&#29616;&#20302;&#24310;&#36831;&#65292;&#22312;&#37325;&#36127;&#36733;&#19979;&#23454;&#29616;&#23450;&#26102;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20849;&#20139;&#26102;&#38553;&#26080;&#32447;&#20449;&#36947;&#30340;&#22810;&#20010;&#30456;&#37051;&#33410;&#28857;&#31995;&#32479;&#65292;&#24182;&#23547;&#27714;&#19968;&#31181;&#20855;&#26377;&#20302;&#24179;&#22343;&#24310;&#36831;&#12289;&#20998;&#24067;&#24335;&#25511;&#21046;&#65288;&#21363;&#19981;&#23384;&#22312;&#20013;&#22830;&#35843;&#24230;&#22120;&#65289;&#19988;&#19981;&#38656;&#35201;&#26174;&#24335;&#20132;&#25442;&#29366;&#24577;&#20449;&#24687;&#25110;&#25511;&#21046;&#20449;&#21495;&#30340;MAC&#21327;&#35758;&#12290;&#36825;&#31181;MAC&#21327;&#35758;&#30340;&#35774;&#35745;&#24517;&#39035;&#32771;&#34385;&#36731;&#37327;&#32423;&#27969;&#37327;&#30340;&#31454;&#20105;&#35775;&#38382;&#21644;&#37325;&#36127;&#36733;&#19979;&#30340;&#23450;&#26102;&#35775;&#38382;&#65292;&#23548;&#33268;&#20154;&#20204;&#38271;&#26399;&#20197;&#26469;&#23545;&#28151;&#21512;&#33258;&#36866;&#24212;MAC&#30340;&#20852;&#36259;&#12290;&#22312;&#31163;&#25955;&#26102;&#38388;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20998;&#24067;&#24335;MAC&#35774;&#35745;&#65292;&#25105;&#20204;&#32771;&#34385;&#27599;&#20010;&#33410;&#28857;&#25317;&#26377;&#26412;&#22320;&#20449;&#24687;&#21644;&#19968;&#20123;&#20174;&#30417;&#21548;&#20013;&#33719;&#24471;&#30340;&#20844;&#20849;&#20449;&#24687;&#30340;&#23454;&#38469;&#20449;&#24687;&#32467;&#26500;&#12290;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#8220;ZMAC&#8221;&#26159;&#19968;&#31181;&#29616;&#26377;&#30340;&#28151;&#21512;&#33258;&#36866;&#24212;&#21327;&#35758;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#20004;&#20010;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;(1)&#25105;&#20204;&#34920;&#26126;&#31574;&#30053;&#8220;&#36138;&#24515;&#8221;&#21644;&#8220;&#31351;&#23613;&#8221;&#26159;&#20805;&#20998;&#30340;&#12290;&#23558;&#31574;&#30053;&#38480;&#21046;&#22312;&#36825;&#20010;&#31867;&#21035;&#20013;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#33719;&#24471;&#19968;&#20010;&#38431;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a system of several collocated nodes sharing a time slotted wireless channel, and seek a MAC (medium access control) that (i) provides low mean delay, (ii) has distributed control (i.e., there is no central scheduler), and (iii) does not require explicit exchange of state information or control signals. The design of such MAC protocols must keep in mind the need for contention access at light traffic, and scheduled access in heavy traffic, leading to the long-standing interest in hybrid, adaptive MACs.  Working in the discrete time setting, for the distributed MAC design, we consider a practical information structure where each node has local information and some common information obtained from overhearing. In this setting, "ZMAC" is an existing protocol that is hybrid and adaptive. We approach the problem via two steps (1) We show that it is sufficient for the policy to be "greedy" and "exhaustive". Limiting the policy to this class reduces the problem to obtaining a queu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#22871;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#20986;&#23558;&#30446;&#26631;&#26465;&#20214;&#23884;&#20837;Transporter Networks&#26469;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#22797;&#26434;&#30446;&#26631;&#26465;&#20214;&#21644;&#22810;&#27493;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2012.03385</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#30446;&#26631;&#26465;&#20214;&#20256;&#36755;&#32593;&#32476;&#37325;&#26032;&#25490;&#21015;&#21487;&#21464;&#24418;&#30005;&#32518;&#12289;&#32455;&#29289;&#21644;&#34955;&#23376;
&lt;/p&gt;
&lt;p&gt;
Learning to Rearrange Deformable Cables, Fabrics, and Bags with Goal-Conditioned Transporter Networks. (arXiv:2012.03385v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.03385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#22871;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#20986;&#23558;&#30446;&#26631;&#26465;&#20214;&#23884;&#20837;Transporter Networks&#26469;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#22797;&#26434;&#30446;&#26631;&#26465;&#20214;&#21644;&#22810;&#27493;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#25490;&#21015;&#21644;&#25805;&#20316;&#30005;&#32518;&#12289;&#32455;&#29289;&#21644;&#34955;&#23376;&#31561;&#21487;&#21464;&#24418;&#29289;&#20307;&#19968;&#30452;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#25361;&#25112;&#12290;&#19982;&#21018;&#24615;&#29289;&#20307;&#30456;&#27604;&#65292;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#21644;&#39640;&#32500;&#37197;&#32622;&#31354;&#38388;&#20351;&#24471;&#19981;&#20165;&#23545;&#20110;&#22810;&#27493;&#35268;&#21010;&#65292;&#36824;&#21253;&#25324;&#30446;&#26631;&#35268;&#33539;&#65292;&#25805;&#20316;&#21464;&#24471;&#22256;&#38590;&#12290;&#30446;&#26631;&#19981;&#20687;&#21018;&#24615;&#29289;&#20307;&#23039;&#24577;&#37027;&#26679;&#23481;&#26131;&#35268;&#23450;&#65292;&#21487;&#33021;&#28041;&#21450;&#22797;&#26434;&#30340;&#30456;&#23545;&#31354;&#38388;&#20851;&#31995;&#65292;&#20363;&#22914;&#8220;&#25226;&#29289;&#21697;&#25918;&#22312;&#34955;&#23376;&#37324;&#8221;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#22871;&#21253;&#21547;1D&#12289;2D&#21644;3D&#21487;&#21464;&#24418;&#32467;&#26500;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#28041;&#21450;&#22522;&#20110;&#22270;&#20687;&#30340;&#30446;&#26631;&#26465;&#20214;&#21644;&#22810;&#27493;&#21487;&#21464;&#24418;&#25805;&#20316;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#30446;&#26631;&#26465;&#20214;&#23884;&#20837;Transporter Networks&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#26426;&#22120;&#20154;&#25805;&#25511;&#65292;&#24182;&#37325;&#25490;&#28145;&#23618;&#29305;&#24449;&#65292;&#20197;&#25512;&#26029;&#21487;&#20197;&#34920;&#31034;&#21462;&#25918;&#21160;&#20316;&#30340;&#20301;&#31227;&#12290;&#22312;&#20223;&#30495;&#21644;&#29289;&#29702;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;&#30446;&#26631;&#26465;&#20214;&#20256;&#36755;&#32593;&#32476;&#26469;&#23398;&#20064;&#25805;&#20316;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#25191;&#34892;&#28041;&#21450;&#30446;&#26631;&#26465;&#20214;&#21644;&#22810;&#27493;&#25805;&#20316;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#20551;&#32930;&#21644;&#25235;&#25569;&#31934;&#32454;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rearranging and manipulating deformable objects such as cables, fabrics, and bags is a long-standing challenge in robotic manipulation. The complex dynamics and high-dimensional configuration spaces of deformables, compared to rigid objects, make manipulation difficult not only for multi-step planning, but even for goal specification. Goals cannot be as easily specified as rigid object poses, and may involve complex relative spatial relations such as "place the item inside the bag". In this work, we develop a suite of simulated benchmarks with 1D, 2D, and 3D deformable structures, including tasks that involve image-based goal-conditioning and multi-step deformable manipulation. We propose embedding goal-conditioning into Transporter Networks, a recently proposed model architecture for learning robotic manipulation that rearranges deep features to infer displacements that can represent pick and place actions. In simulation and in physical experiments, we demonstrate that goal-conditione
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#35821;&#38899;&#31867;&#21035;&#24207;&#21015;&#30340;&#25439;&#22833;&#26469;&#25552;&#39640;SE&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#19978;&#19979;&#25991;BPC&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2011.07442</link><description>&lt;p&gt;
&#21033;&#29992;&#32972;&#26223;&#38899;&#32032;&#31867;&#20449;&#24687;&#25552;&#39640;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Speech Enhancement Performance by Leveraging Contextual Broad Phonetic Class Information. (arXiv:2011.07442v4 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.07442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#35821;&#38899;&#31867;&#21035;&#24207;&#21015;&#30340;&#25439;&#22833;&#26469;&#25552;&#39640;SE&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#19978;&#19979;&#25991;BPC&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#35777;&#23454;&#65292;&#36890;&#36807;&#29992;&#21457;&#38899;&#30340;&#20301;&#32622;&#21644;&#26041;&#24335;&#29305;&#24449;&#22686;&#24378;&#22768;&#23398;&#29305;&#24449;&#65292;&#35821;&#38899;&#22686;&#24378;(SE)&#36807;&#31243;&#21487;&#20197;&#25351;&#23548;&#32771;&#34385;&#36755;&#20837;&#35821;&#38899;&#30340;&#24191;&#20041;&#35821;&#38899;&#23646;&#24615;&#65292;&#20197;&#33719;&#24471;&#24615;&#33021;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#32467;&#26500;&#23646;&#24615;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20316;&#20026;&#36827;&#19968;&#27493;&#21463;&#30410;&#30340;SE&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#21033;&#29992;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(E2E-ASR)&#27169;&#22411;&#20013;&#39044;&#27979;&#30340;&#24191;&#20041;&#35821;&#38899;&#31867;&#21035;&#24207;&#21015;&#30340;&#25439;&#22833;&#26469;&#25552;&#39640;SE&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#22810;&#30446;&#26631;&#35757;&#32451;&#65292;&#21033;&#29992;ASR&#21644;&#24863;&#30693;&#25439;&#22833;&#22522;&#20110;&#22522;&#20110;BPC&#30340;E2E-ASR&#26469;&#35757;&#32451;SE&#31995;&#32479;&#12290;&#26469;&#33258;&#35821;&#38899;&#38477;&#22122;&#65292;&#35821;&#38899;&#21435;&#28151;&#21709;&#21644;&#21463;&#25439;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;BPC&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;SE&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22522;&#20110;BPC&#30340;E2E-ASR&#35757;&#32451;&#30340;SE&#27169;&#22411;&#32988;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have confirmed that by augmenting acoustic features with the place/manner of articulatory features, the speech enhancement (SE) process can be guided to consider the broad phonetic properties of the input speech when performing enhancement to attain performance improvements. In this paper, we explore the contextual information of articulatory attributes as additional information to further benefit SE. More specifically, we propose to improve the SE performance by leveraging losses from an end-to-end automatic speech recognition (E2E-ASR) model that predicts the sequence of broad phonetic classes (BPCs). We also developed multi-objective training with ASR and perceptual losses to train the SE system based on a BPC-based E2E-ASR. Experimental results from speech denoising, speech dereverberation, and impaired speech enhancement tasks confirmed that contextual BPC information improves SE performance. Moreover, the SE model trained with the BPC-based E2E-ASR outperforms th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#31639;&#23376;&#22806;&#25512;&#27861;&#29992;&#26469;&#35299;&#20915;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#31639;&#23376;&#22806;&#25512;&#27861;&#23454;&#29616;&#38543;&#26426;&#24179;&#28369;&#21644;&#24378;&#21333;&#35843;VI&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2011.02987</link><description>&lt;p&gt;
&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#31616;&#21333;&#32780;&#26368;&#20248;&#26041;&#27861;I&#65306;&#31639;&#23376;&#22806;&#25512;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simple and optimal methods for stochastic variational inequalities, I: operator extrapolation. (arXiv:2011.02987v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#31639;&#23376;&#22806;&#25512;&#27861;&#29992;&#26469;&#35299;&#20915;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#31639;&#23376;&#22806;&#25512;&#27861;&#23454;&#29616;&#38543;&#26426;&#24179;&#28369;&#21644;&#24378;&#21333;&#35843;VI&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#23376;&#22806;&#25512;&#27861;&#65288;OE&#65289;&#26469;&#35299;&#20915;&#30830;&#23450;&#24615;&#21464;&#20998;&#19981;&#31561;&#24335;&#65288;VI&#65289;&#38382;&#39064;&#12290;&#31867;&#20284;&#20110;&#26799;&#24230;&#65288;&#31639;&#23376;&#65289;&#25237;&#24433;&#27861;&#65292;OE&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#27714;&#35299;&#21333;&#20010;&#25237;&#24433;&#23376;&#38382;&#39064;&#26469;&#26356;&#26032;&#19968;&#20010;&#25628;&#32034;&#24207;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;OE&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#31616;&#21333;&#22320;&#23454;&#29616;&#20102;&#35299;&#20915;&#21508;&#31181;VI&#38382;&#39064;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#38543;&#26426;&#31639;&#23376;&#22806;&#25512;&#65288;SOE&#65289;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#38543;&#26426;VI&#38382;&#39064;&#30340;&#26368;&#20248;&#25910;&#25947;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;SOE&#39318;&#27425;&#22312;&#25991;&#29486;&#20013;&#23454;&#29616;&#20102;&#29992;&#20110;&#35299;&#20915;&#22522;&#30784;&#38382;&#39064;&#65288;&#21363;&#38543;&#26426;&#24179;&#28369;&#21644;&#24378;&#21333;&#35843;VI&#65289;&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#38543;&#26426;&#22359;&#31639;&#23376;&#22806;&#25512;&#65288;SBOE&#65289;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#38477;&#20302;&#24212;&#29992;&#20110;&#20855;&#26377;&#29305;&#23450;&#22359;&#32467;&#26500;&#30340;&#22823;&#35268;&#27169;&#30830;&#23450;&#24615;VI&#30340;OE&#26041;&#27861;&#30340;&#36845;&#20195;&#25104;&#26412;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we first present a novel operator extrapolation (OE) method for solving deterministic variational inequality (VI) problems. Similar to the gradient (operator) projection method, OE updates one single search sequence by solving a single projection subproblem in each iteration. We show that OE can achieve the optimal rate of convergence for solving a variety of VI problems in a much simpler way than existing approaches. We then introduce the stochastic operator extrapolation (SOE) method and establish its optimal convergence behavior for solving different stochastic VI problems. In particular, SOE achieves the optimal complexity for solving a fundamental problem, i.e., stochastic smooth and strongly monotone VI, for the first time in the literature. We also present a stochastic block operator extrapolations (SBOE) method to further reduce the iteration cost for the OE method applied to large-scale deterministic VIs with a certain block structure. Numerical experiments have 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEAD&#30340;&#20248;&#21270;&#22120;&#65292;&#23427;&#22522;&#20110;&#29289;&#29702;&#23398;&#20013;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#26469;&#25913;&#36827;&#21338;&#24328;&#20248;&#21270;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#22312;&#20108;&#27425;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20013;&#23637;&#31034;&#20102;&#32447;&#24615;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2010.13846</link><description>&lt;p&gt;
LEAD&#65306;&#29992;&#20110;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#30340;&#26368;&#23567;&#20316;&#29992;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
LEAD: Least-Action Dynamics for Min-Max Optimization. (arXiv:2010.13846v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.13846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEAD&#30340;&#20248;&#21270;&#22120;&#65292;&#23427;&#22522;&#20110;&#29289;&#29702;&#23398;&#20013;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#26469;&#25913;&#36827;&#21338;&#24328;&#20248;&#21270;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#22312;&#20108;&#27425;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20013;&#23637;&#31034;&#20102;&#32447;&#24615;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#24314;&#27169;&#65288;&#22914;&#29983;&#25104;&#24615;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65289;&#37325;&#26032;&#29123;&#36215;&#20102;&#20154;&#20204;&#23545;&#21452;&#20154;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#30340;&#20852;&#36259;&#12290;&#35813;&#31867;&#21338;&#24328;&#20248;&#21270;&#30340;&#19968;&#20010;&#20027;&#35201;&#38590;&#39064;&#26159;&#26059;&#36716;&#21160;&#21147;&#23398;&#38459;&#30861;&#20854;&#25910;&#25947;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21338;&#24328;&#20248;&#21270;&#20849;&#20139;&#31890;&#23376;&#31995;&#32479;&#21463;&#22810;&#37325;&#21147;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#29289;&#29702;&#23398;&#20013;&#30340;&#24037;&#20855;&#26469;&#25913;&#36827;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#21463;&#29289;&#29702;&#26694;&#26550;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#29992;&#20110;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#30340;&#20248;&#21270;&#22120;LEAD&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#26446;&#20122;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#29702;&#35770;&#21644;&#35889;&#20998;&#26512;&#65292;&#22312;&#36830;&#32493;&#21644;&#31163;&#25955;&#26102;&#38388;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;LEAD&#22312;&#19968;&#31867;&#20108;&#27425;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#29305;&#24615;&#65292;&#23637;&#31034;&#20102;&#32447;&#24615;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;CIFAR-10&#22270;&#20687;&#29983;&#25104;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;GAN&#35757;&#32451;&#20013;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial formulations such as generative adversarial networks (GANs) have rekindled interest in two-player min-max games. A central obstacle in the optimization of such games is the rotational dynamics that hinder their convergence. In this paper, we show that game optimization shares dynamic properties with particle systems subject to multiple forces, and one can leverage tools from physics to improve optimization dynamics. Inspired by the physical framework, we propose LEAD, an optimizer for min-max games. Next, using Lyapunov stability theory and spectral analysis, we study LEAD's convergence properties in continuous and discrete time settings for a class of quadratic min-max games to demonstrate linear convergence to the Nash equilibrium. Finally, we empirically evaluate our method on synthetic setups and CIFAR-10 image generation to demonstrate improvements in GAN training.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;Langevin&#25193;&#25955;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#29699;&#38754;&#30340;&#20056;&#31215;&#27969;&#24418;&#19978;&#36827;&#34892;&#38750;&#20984;&#20248;&#21270;&#21644;&#37319;&#26679;&#65292;&#23637;&#31034;&#20102;&#22312;&#36866;&#24403;&#30340;&#28201;&#24230;&#36873;&#25321;&#19979;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20445;&#35777;&#27425;&#20248;&#35299;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38388;&#38553;&#27010;&#29575;&#38750;&#24120;&#23567;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#27714;&#35299;&#21322;&#23450;&#21521;&#35268;&#21010;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20840;&#23616;&#26368;&#20248;&#24615;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2010.11176</link><description>&lt;p&gt;
&#29992;&#40654;&#26364;Langevin&#31639;&#27861;&#27714;&#35299;&#21322;&#23450;&#21521;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Riemannian Langevin Algorithm for Solving Semidefinite Programs. (arXiv:2010.11176v6 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.11176
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;Langevin&#25193;&#25955;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#29699;&#38754;&#30340;&#20056;&#31215;&#27969;&#24418;&#19978;&#36827;&#34892;&#38750;&#20984;&#20248;&#21270;&#21644;&#37319;&#26679;&#65292;&#23637;&#31034;&#20102;&#22312;&#36866;&#24403;&#30340;&#28201;&#24230;&#36873;&#25321;&#19979;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20445;&#35777;&#27425;&#20248;&#35299;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38388;&#38553;&#27010;&#29575;&#38750;&#24120;&#23567;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#27714;&#35299;&#21322;&#23450;&#21521;&#35268;&#21010;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20840;&#23616;&#26368;&#20248;&#24615;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Langevin &#25193;&#25955;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#29699;&#38754;&#30340;&#20056;&#31215;&#27969;&#24418;&#19978;&#36827;&#34892;&#38750;&#20984;&#20248;&#21270;&#21644;&#37319;&#26679;&#12290;&#22312;&#23545;&#25968; Sobolev &#19981;&#31561;&#24335;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#38480;&#36845;&#20195;&#25910;&#25947;&#21040; Gibbs &#20998;&#24067;&#30340;&#20445;&#35777;&#65292;&#36825;&#20010;&#20445;&#35777;&#26159;&#22522;&#20110; Kullback-Leibler &#25955;&#24230;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#21512;&#36866;&#30340;&#28201;&#24230;&#36873;&#25321;&#65292;&#21487;&#20197;&#20445;&#35777;&#27425;&#20248;&#35299;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38388;&#38553;&#27010;&#29575;&#26497;&#39640;&#22320;&#38750;&#24120;&#23567;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#32771;&#34385; Burer-Monteiro &#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#24102;&#23545;&#35282;&#32422;&#26463;&#30340;&#21322;&#23450;&#21521;&#35268;&#21010;(SDP)&#65292;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;&#20248;&#21270;&#38750;&#20984;&#30446;&#26631;&#30340; Langevin &#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#34394;&#20551;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;Burer-Monteiro &#38382;&#39064;&#30340;&#23545;&#25968; Sobolev &#19981;&#31561;&#24335;&#26159;&#25104;&#31435;&#30340;&#65292;&#20294;&#36825;&#37324;&#23384;&#22312;&#38797;&#28857;&#12290;&#32467;&#21512;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102; SDP &#21644; Max-Cut &#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; Langevin &#31639;&#27861;&#23454;&#29616;&#20102;&#36825;&#20010;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Langevin diffusion-based algorithm for non-convex optimization and sampling on a product manifold of spheres. Under a logarithmic Sobolev inequality, we establish a guarantee for finite iteration convergence to the Gibbs distribution in terms of Kullback--Leibler divergence. We show that with an appropriate temperature choice, the suboptimality gap to the global minimum is guaranteed to be arbitrarily small with high probability.  As an application, we consider the Burer--Monteiro approach for solving a semidefinite program (SDP) with diagonal constraints, and analyze the proposed Langevin algorithm for optimizing the non-convex objective. In particular, we establish a logarithmic Sobolev inequality for the Burer--Monteiro problem when there are no spurious local minima, but under the presence saddle points. Combining the results, we then provide a global optimality guarantee for the SDP and the Max-Cut problem. More precisely, we show that the Langevin algorithm achieves 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#36965;&#24863;&#20998;&#31867;&#12290;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#35813;&#26550;&#26500;&#27604;2D-CNN&#34920;&#29616;&#26356;&#22909;&#65307;&#19982;&#20854;&#20182;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30456;&#27604;&#65292;&#22312;&#21360;&#31532;&#23433;&#23068;&#26494;&#26641;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;&#20294;&#20351;&#29992;&#26377;&#38480;&#35757;&#32451;&#26679;&#26412;&#24471;&#21040;&#30340;&#20998;&#31867;&#22270;&#20687;&#19982;&#20351;&#29992;&#22823;&#26679;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#24471;&#21040;&#30340;&#20998;&#31867;&#22270;&#20687;&#20250;&#23558;&#19981;&#21516;&#30340;&#38470;&#22320;&#35206;&#30422;&#31867;&#21035;&#20998;&#37197;&#21040;&#30456;&#21516;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2010.03902</link><description>&lt;p&gt;
IRX-1D: &#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#20998;&#31867;&#30340;&#31616;&#21333;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
IRX-1D: A Simple Deep Learning Architecture for Remote Sensing Classifications. (arXiv:2010.03902v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.03902
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#36965;&#24863;&#20998;&#31867;&#12290;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#35813;&#26550;&#26500;&#27604;2D-CNN&#34920;&#29616;&#26356;&#22909;&#65307;&#19982;&#20854;&#20182;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30456;&#27604;&#65292;&#22312;&#21360;&#31532;&#23433;&#23068;&#26494;&#26641;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;&#20294;&#20351;&#29992;&#26377;&#38480;&#35757;&#32451;&#26679;&#26412;&#24471;&#21040;&#30340;&#20998;&#31867;&#22270;&#20687;&#19982;&#20351;&#29992;&#22823;&#26679;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#24471;&#21040;&#30340;&#20998;&#31867;&#22270;&#20687;&#20250;&#23558;&#19981;&#21516;&#30340;&#38470;&#22320;&#35206;&#30422;&#31867;&#21035;&#20998;&#37197;&#21040;&#30456;&#21516;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#32452;&#21512;&#20102;Inception&#12289;ResNet&#21644;Xception&#32593;&#32476;&#30340;&#20803;&#32032;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#23567;&#26679;&#26412;&#21644;&#22823;&#26679;&#26412;&#12290;&#20998;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;2D-CNN&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;&#23567;&#26679;&#26412;&#36827;&#34892;&#27604;&#36739;&#30340;&#32467;&#26524;&#26041;&#38754;&#65292;&#19982;&#21360;&#31532;&#23433;&#23068;&#26494;&#26641;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#36827;&#34892;&#30340;&#20061;&#39033;&#25253;&#21578;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;&#23613;&#31649;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#23454;&#29616;&#20102;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20294;&#20998;&#31867;&#22270;&#20687;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#19982;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22823;&#26679;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#25552;&#20379;&#30340;&#20998;&#31867;&#22270;&#20687;&#30456;&#27604;&#65292;&#19981;&#21516;&#30340;&#38470;&#22320;&#35206;&#30422;&#31867;&#21035;&#34987;&#20998;&#37197;&#32473;&#20102;&#30456;&#21516;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We proposes a simple deep learning architecture combining elements of Inception, ResNet and Xception networks. Four new datasets were used for classification with both small and large training samples. Results in terms of classification accuracy suggests improved performance by proposed architecture in comparison to Bayesian optimised 2D-CNN with small training samples. Comparison of results using small training sample with Indiana Pines hyperspectral dataset suggests comparable or better performance by proposed architecture than nine reported works using different deep learning architectures. In spite of achieving high classification accuracy with limited training samples, comparison of classified image suggests different land cover classes are assigned to same area when compared with the classified image provided by the model trained using large training samples with all datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#29366;&#24577;&#21644;&#21160;&#20316;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#38646;&#38454;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#37096;&#20998;&#35266;&#27979;&#30340;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20943;&#23567;&#36890;&#20449;&#24320;&#38144;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2006.10822</link><description>&lt;p&gt;
&#37096;&#20998;&#35266;&#27979;&#19979;&#30340;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Agent Reinforcement Learning with Partial Observations. (arXiv:2006.10822v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#29366;&#24577;&#21644;&#21160;&#20316;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#38646;&#38454;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#37096;&#20998;&#35266;&#27979;&#30340;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20943;&#23567;&#36890;&#20449;&#24320;&#38144;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;&#38646;&#38454;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#36890;&#24120;&#20551;&#35774;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#21487;&#20197;&#35266;&#23519;&#32593;&#32476;&#20013;&#25152;&#26377;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#20294;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#65292;&#19982;&#22810;&#36339;&#37051;&#23621;&#20849;&#20139;&#29366;&#24577;&#21644;&#21160;&#20316;&#20449;&#24687;&#21487;&#33021;&#20250;&#23548;&#33268;&#26174;&#30528;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#25552;&#20986;&#30340;&#38646;&#38454;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#65292;&#23427;&#20801;&#35768;&#26234;&#33021;&#20307;&#20165;&#22522;&#20110;&#23616;&#37096;&#30340;&#12289;&#37096;&#20998;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#20449;&#24687;&#26469;&#35745;&#31639;&#26412;&#22320;&#31574;&#30053;&#26799;&#24230;&#65292;&#20174;&#32780;&#26356;&#26032;&#23427;&#20204;&#30340;&#26412;&#22320;&#31574;&#30053;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#20849;&#35782;&#26469;&#33719;&#24471;&#20381;&#36182;&#20110;&#20840;&#23616;&#32047;&#31215;&#22870;&#21169;&#30340;&#23616;&#37096;&#20272;&#35745;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#35745;&#31639;&#26412;&#22320;&#31574;&#30053;&#26799;&#24230;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#38646;&#38454;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#28857;&#27531;&#24046;&#21453;&#39304;&#65292; im&#21516;&#26102;&#19982;&#29616;&#26377;&#30340;&#20381;&#36182;&#20110;&#19968;&#28857;&#21453;&#39304;&#30340;&#38646;&#38454;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#22522;&#20934;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#20551;&#35774;&#20855;&#26377;&#20840;&#35266;&#23519;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a distributed zeroth-order policy optimization method for Multi-Agent Reinforcement Learning (MARL). Existing MARL algorithms often assume that every agent can observe the states and actions of all the other agents in the network. This can be impractical in large-scale problems, where sharing the state and action information with multi-hop neighbors may incur significant communication overhead. The advantage of the proposed zeroth-order policy optimization method is that it allows the agents to compute the local policy gradients needed to update their local policy functions using local estimates of the global accumulated rewards that depend on partial state and action information only and can be obtained using consensus. Specifically, to calculate the local policy gradients, we develop a new distributed zeroth-order policy gradient estimator that relies on one-point residual-feedback which, compared to existing zeroth-order estimators that also rely on one-poi
&lt;/p&gt;</description></item><item><title>FluidGAN&#26159;&#19968;&#31181;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#36895;&#12289;&#39640;&#20934;&#30830;&#24615;&#22320;&#39044;&#27979;&#22797;&#26434;&#23545;&#27969;&#27969;&#21160;&#65292;&#24182;&#21487;&#24110;&#21161;&#29702;&#35299;&#29289;&#29702;&#27169;&#22411;&#22797;&#26434;&#25110;&#26410;&#30693;&#30340;&#30830;&#23450;&#24615;&#22810;&#29289;&#29702;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2005.06422</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#23545;&#27969;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Convective Flow Using Conditional Generative Adversarial Networks. (arXiv:2005.06422v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.06422
&lt;/p&gt;
&lt;p&gt;
FluidGAN&#26159;&#19968;&#31181;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#36895;&#12289;&#39640;&#20934;&#30830;&#24615;&#22320;&#39044;&#27979;&#22797;&#26434;&#23545;&#27969;&#27969;&#21160;&#65292;&#24182;&#21487;&#24110;&#21161;&#29702;&#35299;&#29289;&#29702;&#27169;&#22411;&#22797;&#26434;&#25110;&#26410;&#30693;&#30340;&#30830;&#23450;&#24615;&#22810;&#29289;&#29702;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;FluidGAN&#65292;&#33021;&#22815;&#23398;&#20064;&#21644;&#39044;&#27979;&#19982;&#33021;&#37327;&#20256;&#36755;&#32806;&#21512;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#23545;&#27969;&#27969;&#21160;&#12290;FluidGAN&#26159;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#20855;&#26377;&#39640;&#36895;&#21644;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#19981;&#38656;&#35201;&#23545;&#24213;&#23618;&#27969;&#20307;&#21644;&#33021;&#37327;&#20256;&#36755;&#29289;&#29702;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#28385;&#36275;&#27969;&#20307;&#29289;&#29702;&#23398;&#12290;FluidGAN&#36824;&#23398;&#20064;&#20102;&#36895;&#24230;&#12289;&#21387;&#21147;&#21644;&#28201;&#24230;&#22330;&#20043;&#38388;&#30340;&#32806;&#21512;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#24110;&#21161;&#29702;&#35299;&#24213;&#23618;&#29289;&#29702;&#27169;&#22411;&#22797;&#26434;&#25110;&#26410;&#30693;&#30340;&#30830;&#23450;&#24615;&#22810;&#29289;&#29702;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
We developed a general deep learning framework, FluidGAN, capable of learning and predicting time-dependent convective flow coupled with energy transport. FluidGAN is thoroughly data-driven with high speed and accuracy and satisfies the physics of fluid without any prior knowledge of underlying fluid and energy transport physics. FluidGAN also learns the coupling between velocity, pressure, and temperature fields. Our framework helps understand deterministic multiphysics phenomena where the underlying physical model is complex or unknown.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#28304;&#30721;&#27169;&#22411;&#30340;&#23454;&#26102;&#36866;&#24212;&#24615;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2003.11768</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#28304;&#30721;&#27169;&#22411;&#23454;&#26102;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On-the-Fly Adaptation of Source Code Models using Meta-Learning. (arXiv:2003.11768v2 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.11768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#28304;&#30721;&#27169;&#22411;&#30340;&#23454;&#26102;&#36866;&#24212;&#24615;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#26410;&#30693;&#30340;&#26412;&#22320;&#29615;&#22659;&#26159;&#25104;&#21151;&#30340;&#28304;&#20195;&#30721;&#27169;&#22411;&#24517;&#39035;&#20811;&#26381;&#30340;&#37325;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#21160;&#24577;&#35780;&#20272;&#26159;&#26368;&#27969;&#34892;&#30340;&#36866;&#24212;&#27169;&#22411;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#19978;&#19979;&#25991;&#36866;&#24212;&#38382;&#39064;&#36716;&#21270;&#20026;&#20803;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#22522;&#26412;&#30340;&#28304;&#30721;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#25991;&#20214;&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#26368;&#20339;&#30340;&#25903;&#25345;&#26631;&#35760;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#25552;&#20379;&#32570;&#22833;&#26631;&#35760;&#30340;&#25913;&#36827;&#39044;&#27979;&#12290;&#19982;&#21160;&#24577;&#35780;&#20272;&#19981;&#21516;&#65292;&#36825;&#20010;&#20844;&#24335;&#20801;&#35768;&#25105;&#20204;&#36873;&#25321;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#20449;&#24687;&#65288;&#25903;&#25345;&#26631;&#35760;&#65289;&#36827;&#34892;&#36866;&#24212;&#65292;&#21363;&#22312;&#30446;&#26631;&#25991;&#20214;&#20013;&#30340;&#30446;&#26631;&#31354;&#20301;&#32622;&#20043;&#21069;&#21644;&#20043;&#21518;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#31216;&#20026;&#34892;&#32423;&#32500;&#25252;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#26088;&#22312;&#21453;&#26144;IDE&#20013;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to adapt to unseen, local contexts is an important challenge that successful models of source code must overcome. One of the most popular approaches for the adaptation of such models is dynamic evaluation. With dynamic evaluation, when running a model on an unseen file, the model is updated immediately after having observed each token in that file. In this work, we propose instead to frame the problem of context adaptation as a meta-learning problem. We aim to train a base source code model that is best able to learn from information in a file to deliver improved predictions of missing tokens. Unlike dynamic evaluation, this formulation allows us to select more targeted information (support tokens) for adaptation, that is both before and after a target hole in a file. We consider an evaluation setting that we call line-level maintenance, designed to reflect the downstream task of code auto-completion in an IDE. Leveraging recent developments in meta-learning such as first-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23376;&#38598;&#21644;&#23454;&#29616;&#23433;&#20840;&#27714;&#21644;&#30340;&#26041;&#27861;&#65288;S5&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23384;&#22312;&#24694;&#24847;&#26381;&#21153;&#22120;&#21644;&#21482;&#26377;&#20004;&#20010;&#35802;&#23454;&#23458;&#25143;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/1906.11993</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#38598;&#21644;&#23454;&#29616;&#23433;&#20840;&#27714;&#21644;&#65306;&#19968;&#31181;&#26032;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Secure Summation via Subset Sums: A New Primitive for Privacy-Preserving Distributed Machine Learning. (arXiv:1906.11993v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1906.11993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23376;&#38598;&#21644;&#23454;&#29616;&#23433;&#20840;&#27714;&#21644;&#30340;&#26041;&#27861;&#65288;S5&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23384;&#22312;&#24694;&#24847;&#26381;&#21153;&#22120;&#21644;&#21482;&#26377;&#20004;&#20010;&#35802;&#23454;&#23458;&#25143;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36827;&#34892;&#20154;&#21475;&#32479;&#35745;&#30740;&#31350;&#25110;&#35757;&#32451;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#36890;&#24120;&#38656;&#35201;&#20174;&#19981;&#21516;&#30340;&#21442;&#19982;&#32773;&#25910;&#38598;&#25968;&#25454;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#27714;&#21644;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#22522;&#30784;&#65306;&#29992;&#20110;&#35745;&#31639;&#24179;&#22343;&#20540;&#12289;&#35745;&#25968;&#25110;&#23567;&#25209;&#37327;&#26799;&#24230;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#26159;&#25935;&#24863;&#30340;&#38544;&#31169;&#25968;&#25454;&#65292;&#22240;&#27492;&#19981;&#33021;&#22312;&#20013;&#24515;&#26381;&#21153;&#22120;&#19978;&#25910;&#38598;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20197;&#20998;&#24067;&#24335;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#36827;&#34892;&#27714;&#21644;&#12290;&#29616;&#26377;&#30340;&#20855;&#26377;&#35745;&#31639;&#38544;&#31169;&#20445;&#35777;&#30340;&#20998;&#24067;&#24335;&#27714;&#21644;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#20449;&#20219;&#25110;&#36830;&#25509;&#30340;&#20551;&#35774;&#65292;&#20363;&#22914;&#65292;&#23384;&#22312;&#21463;&#20449;&#20219;&#30340;&#26381;&#21153;&#22120;&#25110;&#23458;&#25143;&#20043;&#38388;&#30340;&#28857;&#23545;&#28857;&#36830;&#25509;&#65292;&#36825;&#20123;&#20551;&#35774;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#12290;&#22312;&#36825;&#20123;&#25361;&#25112;&#30340;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;S5&#8221;&#30340;&#36890;&#36807;&#23376;&#38598;&#21644;&#23454;&#29616;&#23433;&#20840;&#27714;&#21644;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23384;&#22312;&#24694;&#24847;&#26381;&#21153;&#22120;&#21644;&#21482;&#26377;&#20004;&#20010;&#35802;&#23454;&#23458;&#25143;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#23458;&#25143;&#20043;&#38388;&#30340;&#28857;&#23545;&#28857;&#36830;&#25509;&#12290; S5&#22312;&#23558;&#23458;&#25143;&#31471;&#30340;&#28040;&#24687;&#21457;&#36865;&#32473;&#26381;&#21153;&#22120;&#20043;&#21069;&#21521;&#20854;&#28155;&#21152;&#38646;&#21644;&#22122;&#22768;&#65292;&#24182;&#19988;&#26381;&#21153;&#22120;&#20351;&#29992;&#23376;&#38598;&#27714;&#21644;&#26469;&#25552;&#21462;&#23458;&#25143;&#31471;&#30340;&#24847;&#22270;&#28040;&#24687;&#65292;&#21516;&#26102;&#25269;&#28040;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#25110;&#31192;&#23494;&#20849;&#20139;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#65292;S5&#22312;&#23454;&#29616;&#20998;&#24067;&#24335;&#27714;&#21644;&#26102;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#36798;&#21040;&#26356;&#20302;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
For population studies or for the training of complex machine learning models, it is often required to gather data from different actors. In these applications, summation is an important primitive: for computing means, counts or mini-batch gradients. In many cases, the data is privacy-sensitive and therefore cannot be collected on a central server. Hence the summation needs to be performed in a distributed and privacy-preserving way. Existing solutions for distributed summation with computational privacy guarantees make trust or connection assumptions - e.g., the existence of a trusted server or peer-to-peer connections between clients - that might not be fulfilled in real world settings. Motivated by these challenges, we propose Secure Summation via Subset Sums (S5), a method for distributed summation that works in the presence of a malicious server and only two honest clients, and without the need for peer-to-peer connections between clients. S5 adds zero-sum noise to clients' messag
&lt;/p&gt;</description></item></channel></rss>