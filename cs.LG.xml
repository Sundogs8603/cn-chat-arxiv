<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26497;&#20302;&#21151;&#29575;&#19979;&#25805;&#20316;&#30340;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#19968;&#20123;&#23618;&#21482;&#20351;&#29992;&#19968;&#20010;&#20809;&#23376;&#26469;&#24341;&#21457;&#31070;&#32463;&#20803;&#28608;&#27963;&#12290;&#23613;&#31649;&#23384;&#22312;&#26497;&#39640;&#30340;&#22122;&#22768;&#65292;&#20173;&#21487;&#20197;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#20197;&#39640;&#31934;&#24230;&#25191;&#34892;&#30830;&#23450;&#24615;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.15712</link><description>&lt;p&gt;
&#23569;&#37327;&#23376;&#28608;&#27963;&#19979;&#37327;&#23376;&#22122;&#22768;&#21463;&#38480;&#30340;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quantum-noise-limited optical neural networks operating at a few quanta per activation. (arXiv:2307.15712v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26497;&#20302;&#21151;&#29575;&#19979;&#25805;&#20316;&#30340;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#19968;&#20123;&#23618;&#21482;&#20351;&#29992;&#19968;&#20010;&#20809;&#23376;&#26469;&#24341;&#21457;&#31070;&#32463;&#20803;&#28608;&#27963;&#12290;&#23613;&#31649;&#23384;&#22312;&#26497;&#39640;&#30340;&#22122;&#22768;&#65292;&#20173;&#21487;&#20197;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#20197;&#39640;&#31934;&#24230;&#25191;&#34892;&#30830;&#23450;&#24615;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#29289;&#29702;&#31070;&#32463;&#32593;&#32476;&#34987;&#36890;&#24120;&#22312;&#30456;&#23545;&#39640;&#21151;&#29575;&#19979;&#25805;&#20316;&#65292;&#20197;&#20445;&#35777;&#20449;&#22122;&#27604;&#22823;&#20110;10&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26497;&#20302;&#21151;&#29575;&#19979;&#25805;&#20316;&#27169;&#25311;&#31995;&#32479;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65292;&#21363;&#31995;&#32479;&#34892;&#20026;&#21464;&#24471;&#39640;&#24230;&#38543;&#26426;&#19988;&#22122;&#22768;&#19981;&#20877;&#26159;&#20449;&#21495;&#30340;&#23567;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#20013;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20123;&#23618;&#21482;&#20351;&#29992;&#19968;&#20010;&#20809;&#23376;&#26469;&#24341;&#21457;&#31070;&#32463;&#20803;&#28608;&#27963;&#12290;&#22312;&#36825;&#31181;&#26497;&#20302;&#21151;&#29575;&#19979;&#65292;&#31070;&#32463;&#20803;&#28608;&#27963;&#21463;&#21040;&#37327;&#23376;&#22122;&#22768;&#30340;&#20027;&#23548;&#65292;&#36825;&#26159;&#30001;&#20110;&#21333;&#20809;&#23376;&#26816;&#27979;&#24369;&#20809;&#20449;&#21495;&#30340;&#22522;&#26412;&#27010;&#29575;&#24615;&#36136;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;&#22122;&#22768;&#26497;&#22823;&#65288;&#20449;&#22122;&#27604;&#32422;&#20026;1&#65289;&#65292;&#20173;&#28982;&#21487;&#20197;&#35757;&#32451;&#38543;&#26426;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#20197;&#39640;&#31934;&#24230;&#25191;&#34892;&#30830;&#23450;&#24615;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analog physical neural networks, which hold promise for improved energy efficiency and speed compared to digital electronic neural networks, are nevertheless typically operated in a relatively high-power regime so that the signal-to-noise ratio (SNR) is large (&gt;10). What happens if an analog system is instead operated in an ultra-low-power regime, in which the behavior of the system becomes highly stochastic and the noise is no longer a small perturbation on the signal? In this paper, we study this question in the setting of optical neural networks operated in the limit where some layers use only a single photon to cause a neuron activation. Neuron activations in this limit are dominated by quantum noise from the fundamentally probabilistic nature of single-photon detection of weak optical signals. We show that it is possible to train stochastic optical neural networks to perform deterministic image-classification tasks with high accuracy in spite of the extremely high noise (SNR ~ 1) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#20998;&#24067;&#22806;&#30340;&#25968;&#25454;&#24182;&#20174;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#22522;&#20110;&#38598;&#25104;&#30340;OOD&#26816;&#27979;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15710</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Object Detection in the Open World. (arXiv:2307.15710v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#20998;&#24067;&#22806;&#30340;&#25968;&#25454;&#24182;&#20174;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#22522;&#20110;&#38598;&#25104;&#30340;OOD&#26816;&#27979;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#26377;&#19968;&#32452;&#22266;&#23450;&#30340;&#31867;&#21035;&#65292;&#21363;&#23646;&#20110;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#26041;&#27861;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#24212;&#29992;&#26102;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#26410;&#26631;&#35760;&#21644;&#27979;&#35797;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#65292;&#21363;&#23646;&#20110;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25506;&#35752;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#26816;&#27979;&#36825;&#20123;OOD&#26679;&#26412;&#65292;&#22914;&#26524;&#21487;&#20197;&#65292;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#20174;&#20013;&#23398;&#20064;&#65311;&#32771;&#34385;&#21040;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24320;&#25918;&#19990;&#30028;&#21322;&#30417;&#30563;&#26816;&#27979;&#26694;&#26550;&#65288;OWSSD&#65289;&#65292;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;OOD&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#20174;ID&#21644;OOD&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#38598;&#25104;&#30340;OOD&#26816;&#27979;&#22120;&#65292;&#30001;&#20165;&#22312;ID&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;&#33258;&#32534;&#30721;&#22120;&#32593;&#32476;&#32452;&#25104;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;OOD&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches for semi-supervised object detection assume a fixed set of classes present in training and unlabeled datasets, i.e., in-distribution (ID) data. The performance of these techniques significantly degrades when these techniques are deployed in the open-world, due to the fact that the unlabeled and test data may contain objects that were not seen during training, i.e., out-of-distribution (OOD) data. The two key questions that we explore in this paper are: can we detect these OOD samples and if so, can we learn from them? With these considerations in mind, we propose the Open World Semi-supervised Detection framework (OWSSD) that effectively detects OOD data along with a semi-supervised learning pipeline that learns from both ID and OOD data. We introduce an ensemble based OOD detector consisting of lightweight auto-encoder networks trained only on ID data. Through extensive evalulation, we demonstrate that our method performs competitively against state-of-the-art OOD 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35770;&#12289;&#24212;&#29992;&#21644;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#35814;&#32454;&#21644;&#30495;&#23454;&#30340;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15703</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#20174;&#29702;&#35770;&#21040;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Uncertainty in Natural Language Generation: From Theory to Applications. (arXiv:2307.15703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35770;&#12289;&#24212;&#29992;&#21644;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#35814;&#32454;&#21644;&#30495;&#23454;&#30340;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#23853;&#38706;&#22836;&#35282;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#25191;&#34892;&#20256;&#32479;&#20219;&#21153;&#22914;&#25688;&#35201;&#25110;&#32763;&#35793;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;NLG&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#22312;&#21487;&#33021;&#20986;&#38169;&#26102;&#25351;&#31034;&#65292;&#24182;&#25903;&#25345;&#22810;&#31181;&#35266;&#28857;&#12289;&#32972;&#26223;&#21644;&#20889;&#20316;&#39118;&#26684; - &#21453;&#26144;&#22810;&#20803;&#21270;&#20154;&#31867;&#20122;&#32676;&#20307;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#21407;&#21017;&#24615;&#22788;&#29702;&#33021;&#22815;&#24110;&#21161;&#21019;&#24314;&#19982;&#36825;&#20123;&#30446;&#26631;&#26356;&#22909;&#22320;&#23545;&#40784;&#30340;&#31995;&#32479;&#21644;&#35780;&#20272;&#21327;&#35758;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#25152;&#38656;&#30340;&#22522;&#26412;&#29702;&#35770;&#12289;&#26694;&#26550;&#21644;&#35789;&#27719;&#12290;&#28982;&#21518;&#20174;&#35821;&#35328;&#23398;&#30340;&#35282;&#24230;&#25551;&#36848;NLG&#20013;&#30340;&#20027;&#35201;&#19981;&#30830;&#23450;&#24615;&#28304;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#27604;&#27969;&#34892;&#30340;&#38543;&#26426;&#24615;/&#35748;&#35782;&#24615;&#20108;&#20998;&#27861;&#26356;&#35814;&#32454;&#21644;&#30495;&#23454;&#30340;&#20108;&#32500;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances of powerful Language Models have allowed Natural Language Generation (NLG) to emerge as an important technology that can not only perform traditional tasks like summarisation or translation, but also serve as a natural language interface to a variety of applications. As such, it is crucial that NLG systems are trustworthy and reliable, for example by indicating when they are likely to be wrong; and supporting multiple views, backgrounds and writing styles -- reflecting diverse human sub-populations. In this paper, we argue that a principled treatment of uncertainty can assist in creating systems and evaluation protocols better aligned with these goals. We first present the fundamental theory, frameworks and vocabulary required to represent uncertainty. We then characterise the main sources of uncertainty in NLG from a linguistic perspective, and propose a two-dimensional taxonomy that is more informative and faithful than the popular aleatoric/epistemic dichotomy. Final
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MemNet&#30340;&#36890;&#29992;&#36882;&#24402;&#20107;&#20214;&#35760;&#24518;&#26550;&#26500;&#65288;MemNet&#65289;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#23384;&#20648;&#38190;&#20540;&#23545;&#26469;&#25913;&#21892;&#34920;&#31034;&#25928;&#26524;&#12290;MemNet&#20855;&#26377;&#32447;&#24615;&#33258;&#36866;&#24212;&#26144;&#23556;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#26631;&#37327;&#26102;&#38388;&#24207;&#21015;&#12289;&#36923;&#36753;&#36816;&#31639;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15694</link><description>&lt;p&gt;
&#29992;&#20110;&#27969;&#25968;&#25454;&#30340;&#36890;&#29992;&#36882;&#24402;&#20107;&#20214;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Universal Recurrent Event Memories for Streaming Data. (arXiv:2307.15694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MemNet&#30340;&#36890;&#29992;&#36882;&#24402;&#20107;&#20214;&#35760;&#24518;&#26550;&#26500;&#65288;MemNet&#65289;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#23384;&#20648;&#38190;&#20540;&#23545;&#26469;&#25913;&#21892;&#34920;&#31034;&#25928;&#26524;&#12290;MemNet&#20855;&#26377;&#32447;&#24615;&#33258;&#36866;&#24212;&#26144;&#23556;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#26631;&#37327;&#26102;&#38388;&#24207;&#21015;&#12289;&#36923;&#36753;&#36816;&#31639;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20107;&#20214;&#35760;&#24518;&#26550;&#26500;&#65288;MemNet&#65289;&#65292;&#36866;&#29992;&#20110;&#26631;&#37327;&#12289;&#22810;&#21464;&#37327;&#25110;&#31526;&#21495;&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#19982;&#20854;&#20182;&#22806;&#37096;&#31070;&#32463;&#35760;&#24518;&#26550;&#26500;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#23384;&#20648;&#20102;&#38190;&#20540;&#23545;&#65292;&#23558;&#23547;&#22336;&#21644;&#20869;&#23481;&#20449;&#24687;&#20998;&#31163;&#65292;&#20174;&#32780;&#25552;&#39640;&#34920;&#31034;&#25928;&#26524;&#65292;&#31867;&#20284;&#20110;&#25968;&#23383;&#20856;&#20856;&#22411;&#12290;&#27492;&#22806;&#65292;&#38190;&#20540;&#23545;&#36824;&#36991;&#20813;&#20102;&#20869;&#23384;&#28145;&#24230;&#21644;&#20998;&#36776;&#29575;&#20043;&#38388;&#30340;&#25240;&#20013;&#65292;&#36825;&#36866;&#29992;&#20110;&#27169;&#22411;&#29366;&#24577;&#26500;&#24314;&#30340;&#35760;&#24518;&#12290;MemNet&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#65292;&#22312;&#23545;&#36755;&#20837;&#25968;&#25454;&#25191;&#34892;&#38750;&#32447;&#24615;&#25805;&#20316;&#26102;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#33258;&#36866;&#24212;&#26144;&#23556;&#20989;&#25968;&#12290;MemNet&#26550;&#26500;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#26631;&#37327;&#26102;&#38388;&#24207;&#21015;&#12289;&#23383;&#31526;&#20018;&#30340;&#36923;&#36753;&#36816;&#31639;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#12289;&#31526;&#21495;&#25805;&#20316;&#20219;&#21153;&#21644;&#38382;&#39064;&#22238;&#31572;&#31561;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new event memory architecture (MemNet) for recurrent neural networks, which is universal for different types of time series data such as scalar, multivariate or symbolic. Unlike other external neural memory architectures, it stores key-value pairs, which separate the information for addressing and for content to improve the representation, as in the digital archetype. Moreover, the key-value pairs also avoid the compromise between memory depth and resolution that applies to memories constructed by the model state. One of the MemNet key characteristics is that it requires only linear adaptive mapping functions while implementing a nonlinear operation on the input data. MemNet architecture can be applied without modifications to scalar time series, logic operators on strings, and also to natural language processing, providing state-of-the-art results in all application domains such as the chaotic time series, the symbolic operation tasks, and the question-answ
&lt;/p&gt;</description></item><item><title>&#20174;&#39044;&#20808;&#35760;&#24405;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#22312;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#22823;&#37327;&#26469;&#33258;&#27169;&#25311;&#29615;&#22659;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#31995;&#32479;&#21644;&#27169;&#25311;&#29615;&#22659;&#20013;&#25191;&#34892;&#23398;&#20064;&#31574;&#30053;&#30340;&#36873;&#39033;&#65292;&#29992;&#20110;&#25512;&#21160;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15690</link><description>&lt;p&gt;
&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#30828;&#20214;&#19978;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Offline Reinforcement Learning on Real-Robot Hardware. (arXiv:2307.15690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15690
&lt;/p&gt;
&lt;p&gt;
&#20174;&#39044;&#20808;&#35760;&#24405;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#22312;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#22823;&#37327;&#26469;&#33258;&#27169;&#25311;&#29615;&#22659;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#31995;&#32479;&#21644;&#27169;&#25311;&#29615;&#22659;&#20013;&#25191;&#34892;&#23398;&#20064;&#31574;&#30053;&#30340;&#36873;&#39033;&#65292;&#29992;&#20110;&#25512;&#21160;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39044;&#20808;&#35760;&#24405;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#26159;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#65292;&#22240;&#20026;&#22312;&#32447;&#23398;&#20064;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#29305;&#21035;&#26159;&#22312;&#28789;&#24039;&#25805;&#20316;&#39046;&#22495;&#65292;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#30528;&#36890;&#29992;&#24418;&#24335;&#30340;&#38590;&#39064;&#12290;&#28982;&#32780;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#22823;&#22411;&#22810;&#26679;&#25968;&#25454;&#38598;&#30340;&#32467;&#21512;&#26377;&#28508;&#21147;&#22312;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#21462;&#24471;&#31361;&#30772;&#65292;&#31867;&#20284;&#20110;&#36817;&#24180;&#26469;&#22312;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#30340;&#24555;&#36895;&#36827;&#23637;&#12290;&#20026;&#20102;&#21327;&#35843;&#30740;&#31350;&#31038;&#21306;&#30340;&#21162;&#21147;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#65306;i&#65289;&#20174;&#27169;&#25311;&#29615;&#22659;&#20013;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#33021;&#21147;&#24378;&#22823;&#30340;&#20195;&#29702;&#26234;&#33021;&#20307;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#30340;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#65307;ii&#65289;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#31995;&#32479;&#21644;&#27169;&#25311;&#29615;&#22659;&#19978;&#25191;&#34892;&#23398;&#20064;&#31574;&#30053;&#20197;&#20415;&#36827;&#34892;&#39640;&#25928;&#30340;&#35843;&#35797;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20027;&#27969;&#30340;&#24320;&#28304;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#23454;&#39564;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning policies from previously recorded data is a promising direction for real-world robotics tasks, as online learning is often infeasible. Dexterous manipulation in particular remains an open problem in its general form. The combination of offline reinforcement learning with large diverse datasets, however, has the potential to lead to a breakthrough in this challenging domain analogously to the rapid progress made in supervised learning in recent years. To coordinate the efforts of the research community toward tackling this problem, we propose a benchmark including: i) a large collection of data for offline learning from a dexterous manipulation platform on two tasks, obtained with capable RL agents trained in simulation; ii) the option to execute learned policies on a real-world robotic system and a simulation for efficient debugging. We evaluate prominent open-sourced offline reinforcement learning algorithms on the datasets and provide a reproducible experimental setup for of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30417;&#30563;&#24335;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26469;&#20248;&#21270;&#33258;&#28982;&#28798;&#23475;&#20013;&#27773;&#36710;&#32039;&#24613;&#30095;&#25955;&#35745;&#21010;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#27169;&#25311;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15682</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24212;&#24613;&#36867;&#29983;&#36335;&#24452;&#38382;&#39064;&#30340;&#30417;&#30563;&#24335;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A supervised hybrid quantum machine learning solution to the emergency escape routing problem. (arXiv:2307.15682v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30417;&#30563;&#24335;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26469;&#20248;&#21270;&#33258;&#28982;&#28798;&#23475;&#20013;&#27773;&#36710;&#32039;&#24613;&#30095;&#25955;&#35745;&#21010;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#27169;&#25311;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#31649;&#29702;&#23545;&#33258;&#28982;&#28798;&#23475;&#30340;&#21709;&#24212;&#21487;&#20197;&#22823;&#22823;&#20943;&#36731;&#20854;&#30772;&#22351;&#24615;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#30417;&#30563;&#24335;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26469;&#20248;&#21270;&#33258;&#28982;&#28798;&#23475;&#20013;&#27773;&#36710;&#32039;&#24613;&#30095;&#25955;&#35745;&#21010;&#30340;&#28508;&#21147;&#12290;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#22320;&#38663;&#32039;&#24613;&#24773;&#20917;&#65292;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#21160;&#24577;&#35745;&#31639;&#22270;&#65292;&#22320;&#38663;&#30772;&#22351;&#22478;&#24066;&#30340;&#19968;&#37096;&#20998;&#12290;&#23621;&#27665;&#35797;&#22270;&#36890;&#36807;&#21040;&#36798;&#20132;&#36890;&#25317;&#22581;&#21457;&#29983;&#30340;&#20986;&#21475;&#28857;&#26469;&#25764;&#31163;&#22478;&#24066;&#12290;&#35813;&#24773;&#20917;&#34987;&#24314;&#27169;&#20026;&#22312;&#19981;&#30830;&#23450;&#21644;&#21160;&#24577;&#28436;&#21270;&#30340;&#22320;&#22270;&#19978;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#20855;&#20307;&#22478;&#24066;&#22270;&#19978;&#30340;&#20551;&#35774;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#21151;&#33021;&#32447;&#24615;&#35843;&#21046;(FiLM)&#31070;&#32463;&#32593;&#32476;&#65292;&#19982;&#19968;&#20010;&#32463;&#20856;&#30340;FiLM&#32593;&#32476;&#24182;&#34892;&#65292;&#20197;&#27169;&#20223;&#30830;&#23450;&#24615;&#21160;&#24577;&#22270;&#19978;&#30340;Dijkstra&#33410;&#28857;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12290;&#24182;&#34892;&#28155;&#21152;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Managing the response to natural disasters effectively can considerably mitigate their devastating impact. This work explores the potential of using supervised hybrid quantum machine learning to optimize emergency evacuation plans for cars during natural disasters. The study focuses on earthquake emergencies and models the problem as a dynamic computational graph where an earthquake damages an area of a city. The residents seek to evacuate the city by reaching the exit points where traffic congestion occurs. The situation is modeled as a shortest-path problem on an uncertain and dynamically evolving map. We propose a novel hybrid supervised learning approach and test it on hypothetical situations on a concrete city graph. This approach uses a novel quantum feature-wise linear modulation (FiLM) neural network parallel to a classical FiLM network to imitate Dijkstra's node-wise shortest path algorithm on a deterministic dynamic graph. Adding the quantum neural network in parallel increas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#21160;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;&#38544;&#34255;&#29366;&#24577;&#31354;&#38388;&#12290;&#22522;&#20110;&#29305;&#24449;&#20540;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38271;&#26399;&#20381;&#36182;&#24615;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#21021;&#22987;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15679</link><description>&lt;p&gt;
&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#20998;&#26512;&#21644;&#29305;&#24449;&#20540;&#21021;&#22987;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dynamic Analysis and an Eigen Initializer for Recurrent Neural Networks. (arXiv:2307.15679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#21160;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;&#38544;&#34255;&#29366;&#24577;&#31354;&#38388;&#12290;&#22522;&#20110;&#29305;&#24449;&#20540;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38271;&#26399;&#20381;&#36182;&#24615;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#21021;&#22987;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#38382;&#39064;&#65292;&#23398;&#20064;&#38271;&#26399;&#20381;&#36182;&#24615;&#26159;&#20027;&#35201;&#38590;&#28857;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20449;&#24687;&#34928;&#20943;&#30340;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#20998;&#35299;&#30340;&#38544;&#34255;&#29366;&#24577;&#31354;&#38388;&#20998;&#26512;&#26032;&#35270;&#35282;&#12290;&#25105;&#20204;&#20174;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24320;&#22987;&#20998;&#26512;&#65292;&#24182;&#35299;&#37322;&#20102;&#28608;&#27963;&#20989;&#25968;&#20013;&#20449;&#24687;&#20445;&#30041;&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#22522;&#20110;&#29305;&#24449;&#20540;&#20998;&#26512;&#25552;&#20379;&#20102;&#38271;&#26399;&#20381;&#36182;&#24615;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#22238;&#24402;&#20219;&#21153;&#21644;&#20998;&#31867;&#20219;&#21153;&#29305;&#24449;&#20540;&#34892;&#20026;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#23545;&#35757;&#32451;&#33391;&#22909;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#21021;&#22987;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recurrent neural networks, learning long-term dependency is the main difficulty due to the vanishing and exploding gradient problem. Many researchers are dedicated to solving this issue and they proposed many algorithms. Although these algorithms have achieved great success, understanding how the information decays remains an open problem. In this paper, we study the dynamics of the hidden state in recurrent neural networks. We propose a new perspective to analyze the hidden state space based on an eigen decomposition of the weight matrix. We start the analysis by linear state space model and explain the function of preserving information in activation functions. We provide an explanation for long-term dependency based on the eigen analysis. We also point out the different behavior of eigenvalues for regression tasks and classification tasks. From the observations on well-trained recurrent neural networks, we proposed a new initialization method for recurrent neural networks, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23558;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;IT&#30417;&#25511;&#25968;&#25454;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.15678</link><description>&lt;p&gt;
IT&#30417;&#25511;&#26102;&#38388;&#24207;&#21015;&#30340;&#22240;&#26524;&#25512;&#26029;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Case Studies of Causal Discovery from IT Monitoring Time Series. (arXiv:2307.15678v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23558;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;IT&#30417;&#25511;&#25968;&#25454;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25216;&#26415;&#65288;IT&#65289;&#31995;&#32479;&#23545;&#29616;&#20195;&#20225;&#19994;&#33267;&#20851;&#37325;&#35201;&#65292;&#22788;&#29702;&#25968;&#25454;&#23384;&#20648;&#12289;&#36890;&#20449;&#21644;&#27969;&#31243;&#33258;&#21160;&#21270;&#12290;&#30417;&#25511;&#36825;&#20123;&#31995;&#32479;&#23545;&#20110;&#20854;&#27491;&#24120;&#36816;&#34892;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#25910;&#38598;&#35814;&#32454;&#30340;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#12290;&#38543;&#30528;IT&#30417;&#25511;&#31995;&#32479;&#20013;&#23545;&#22240;&#26524;&#25512;&#26029;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20102;&#35299;IT&#31995;&#32479;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26377;&#21161;&#20110;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#24322;&#24120;&#21644;&#20107;&#25925;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#23427;&#36824;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#20998;&#26512;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#22312;&#30340;&#30410;&#22788;&#65292;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#21040;IT&#30417;&#25511;&#25968;&#25454;&#19978;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#20363;&#22914;&#65292;IT&#30417;&#25511;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#19981;&#23545;&#40784;&#30340;&#26102;&#38388;&#24207;&#21015;&#12289;&#20241;&#30496;&#26102;&#38388;&#24207;&#21015;&#12289;&#26102;&#38388;&#25139;&#38169;&#35823;&#21644;&#32570;&#22833;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#21040;&#19981;&#21516;IT&#30417;&#25511;&#25968;&#25454;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information technology (IT) systems are vital for modern businesses, handling data storage, communication, and process automation. Monitoring these systems is crucial for their proper functioning and efficiency, as it allows collecting extensive observational time series data for analysis. The interest in causal discovery is growing in IT monitoring systems as knowing causal relations between different components of the IT system helps in reducing downtime, enhancing system performance and identifying root causes of anomalies and incidents. It also allows proactive prediction of future issues through historical data analysis. Despite its potential benefits, applying causal discovery algorithms on IT monitoring data poses challenges, due to the complexity of the data. For instance, IT monitoring data often contains misaligned time series, sleeping time series, timestamp errors and missing values. This paper presents case studies on applying causal discovery algorithms to different IT mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#24490;&#29615;&#20013;&#20256;&#25773;&#25915;&#20987;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#39046;&#22495;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38450;&#27490;&#32422;30%&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#23545;&#20110;&#38750;&#24120;&#28608;&#28872;&#30340;&#25915;&#20987;&#32780;&#35328;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.15677</link><description>&lt;p&gt;
&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23545;&#25239;&#35757;&#32451;&#19982;&#25915;&#20987;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Adversarial training for tabular data with attack propagation. (arXiv:2307.15677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#24490;&#29615;&#20013;&#20256;&#25773;&#25915;&#20987;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#39046;&#22495;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38450;&#27490;&#32422;30%&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#23545;&#20110;&#38750;&#24120;&#28608;&#28872;&#30340;&#25915;&#20987;&#32780;&#35328;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#26159;&#23433;&#20840;&#20013;&#24515;&#24212;&#29992;&#20013;&#30340;&#37325;&#22823;&#20851;&#27880;&#28857;&#65292;&#24694;&#24847;&#34892;&#20026;&#32773;&#19981;&#26029;&#35797;&#22270;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35823;&#23548;&#20026;&#23558;&#27450;&#35784;&#34892;&#20026;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#21512;&#27861;&#34892;&#20026;&#65292;&#32780;&#31995;&#32479;&#32500;&#25252;&#32773;&#21017;&#35797;&#22270;&#38459;&#27490;&#20182;&#20204;&#12290;&#23545;&#25239;&#24615;&#22320;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#25269;&#25239;&#27492;&#31867;&#25915;&#20987;&#21487;&#20197;&#38450;&#27490;&#19994;&#21153;&#25439;&#22833;&#24182;&#20943;&#36731;&#31995;&#32479;&#32500;&#25252;&#32773;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#22312;&#36825;&#31181;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#36890;&#24120;&#26159;&#34920;&#26684;&#24418;&#24335;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#36827;&#34892;&#22797;&#26434;&#30340;&#29305;&#24449;&#24037;&#31243;&#36716;&#25442;&#65292;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#26377;&#29992;&#20449;&#21495;&#65292;&#32780;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#24418;&#24335;&#65292;&#22312;&#35757;&#32451;&#24490;&#29615;&#20013;&#22312;&#20004;&#20010;&#31354;&#38388;&#20043;&#38388;&#20256;&#25773;&#25915;&#20987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#39046;&#22495;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#22312;&#23454;&#35777;&#19978;&#27979;&#35797;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38754;&#23545;&#20013;&#31561;&#25915;&#20987;&#26102;&#21487;&#20197;&#38450;&#27490;&#32422;30%&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#22312;&#38750;&#24120;&#28608;&#28872;&#30340;&#25915;&#20987;&#19979;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks are a major concern in security-centered applications, where malicious actors continuously try to mislead Machine Learning (ML) models into wrongly classifying fraudulent activity as legitimate, whereas system maintainers try to stop them. Adversarially training ML models that are robust against such attacks can prevent business losses and reduce the work load of system maintainers. In such applications data is often tabular and the space available for attackers to manipulate undergoes complex feature engineering transformations, to provide useful signals for model training, to a space attackers cannot access. Thus, we propose a new form of adversarial training where attacks are propagated between the two spaces in the training loop. We then test this method empirically on a real world dataset in the domain of credit card fraud detection. We show that our method can prevent about 30% performance drops under moderate attacks and is essential under very aggressive att
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#21644;&#31070;&#32463;&#25968;&#25454;&#20013;&#38543;&#26426;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35299;&#30721;&#39045;&#20869;&#31070;&#32463;&#27963;&#21160;&#20013;&#30340;&#31616;&#21333;&#35270;&#35273;&#21050;&#28608;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#22312;4&#21517;&#24739;&#32773;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;75.55&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#27604;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20102;&#32422;3.0&#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#65292;&#25104;&#20026;&#30740;&#31350;&#31070;&#32463;&#27963;&#21160;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2307.15672</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#29992;&#20110;&#20174;&#39045;&#20869;&#31070;&#32463;&#27963;&#21160;&#20013;&#35299;&#30721;&#31616;&#21333;&#30340;&#35270;&#35273;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
Bayesian Time-Series Classifier for Decoding Simple Visual Stimuli from Intracranial Neural Activity. (arXiv:2307.15672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#21644;&#31070;&#32463;&#25968;&#25454;&#20013;&#38543;&#26426;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35299;&#30721;&#39045;&#20869;&#31070;&#32463;&#27963;&#21160;&#20013;&#30340;&#31616;&#21333;&#35270;&#35273;&#21050;&#28608;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#22312;4&#21517;&#24739;&#32773;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;75.55&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#27604;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20102;&#32422;3.0&#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#65292;&#25104;&#20026;&#30740;&#31350;&#31070;&#32463;&#27963;&#21160;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#21644;&#22522;&#30784;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#20102;&#35299;&#22806;&#37096;&#21050;&#28608;&#22914;&#20309;&#32534;&#30721;&#22312;&#20998;&#24067;&#24335;&#31070;&#32463;&#27963;&#21160;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#21644;&#31070;&#32463;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36125;&#21494;&#26031;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65288;BTsC&#65289;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#25968;&#25454;&#35299;&#30721;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#39068;&#33394;&#26469;&#23637;&#31034;&#35813;&#26041;&#27861;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#22312;4&#21517;&#24739;&#32773;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#21487;&#38752;&#30340;&#24179;&#22343;&#24615;&#33021;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;75.55&#65285;&#65292;&#27604;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20102;&#32422;3.0&#20010;&#30334;&#20998;&#28857;&#12290;&#38500;&#20102;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;BTsC&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#65292;&#20351;&#36825;&#31181;&#25216;&#26415;&#25104;&#20026;&#30740;&#31350;&#21508;&#31181;&#20219;&#21153;&#21644;&#31867;&#21035;&#30340;&#31070;&#32463;&#27963;&#21160;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how external stimuli are encoded in distributed neural activity is of significant interest in clinical and basic neuroscience. To address this need, it is essential to develop analytical tools capable of handling limited data and the intrinsic stochasticity present in neural data. In this study, we propose a straightforward Bayesian time series classifier (BTsC) model that tackles these challenges whilst maintaining a high level of interpretability. We demonstrate the classification capabilities of this approach by utilizing neural data to decode colors in a visual task. The model exhibits consistent and reliable average performance of 75.55% on 4 patients' dataset, improving upon state-of-the-art machine learning techniques by about 3.0 percent. In addition to its high classification accuracy, the proposed BTsC model provides interpretable results, making the technique a valuable tool to study neural activity in various tasks and categories. The proposed solution can be 
&lt;/p&gt;</description></item><item><title>CoRe&#20248;&#21270;&#22120;&#26159;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#24179;&#28369;&#25910;&#25947;&#12289;&#20302;&#35745;&#31639;&#38656;&#27714;&#21644;&#36890;&#29992;&#36866;&#29992;&#24615;&#30340;&#29305;&#28857;&#65292;&#22312;&#35757;&#32451;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.15663</link><description>&lt;p&gt;
CoRe&#20248;&#21270;&#22120;&#65306;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20307;&#21270;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
CoRe Optimizer: An All-in-One Solution for Machine Learning. (arXiv:2307.15663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15663
&lt;/p&gt;
&lt;p&gt;
CoRe&#20248;&#21270;&#22120;&#26159;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#24179;&#28369;&#25910;&#25947;&#12289;&#20302;&#35745;&#31639;&#38656;&#27714;&#21644;&#36890;&#29992;&#36866;&#29992;&#24615;&#30340;&#29305;&#28857;&#65292;&#22312;&#35757;&#32451;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#31639;&#27861;&#21450;&#20854;&#36229;&#21442;&#25968;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#20250;&#26174;&#33879;&#24433;&#21709;&#35757;&#32451;&#36895;&#24230;&#21644;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;&#29702;&#24819;&#20248;&#21270;&#22120;&#30340;&#24895;&#26395;&#28165;&#21333;&#21253;&#25324;&#24555;&#36895;&#12289;&#24179;&#28369;&#22320;&#25910;&#25947;&#21040;&#20302;&#35823;&#24046;&#12289;&#20302;&#35745;&#31639;&#38656;&#27714;&#21644;&#36890;&#29992;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#26368;&#36817;&#24341;&#20837;&#30340;&#25345;&#32493;&#24377;&#24615;&#65288;CoRe&#65289;&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;&#26041;&#38754;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#19968;&#38454;&#26799;&#24230;&#20248;&#21270;&#22120;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;CoRe&#20248;&#21270;&#22120;&#36827;&#34892;&#20102;&#19982;&#20854;&#20182;&#20061;&#31181;&#20248;&#21270;&#31639;&#27861;&#30340;&#24191;&#27867;&#24615;&#33021;&#23545;&#27604;&#65292;&#21253;&#25324;Adam&#20248;&#21270;&#22120;&#21644;&#24377;&#24615;&#21453;&#21521;&#20256;&#25773;&#65288;RPROP&#65289;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#36890;&#29992;&#36866;&#29992;&#30340;&#20540;&#12290;CoRe&#20248;&#21270;&#22120;&#22312;&#27599;&#20010;&#30740;&#31350;&#24212;&#29992;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#25110;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#21482;&#38656;&#35201;&#26356;&#25913;&#19968;&#20010;&#36229;&#21442;&#25968;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#23567;&#25209;&#37327;
&lt;/p&gt;
&lt;p&gt;
The optimization algorithm and its hyperparameters can significantly affect the training speed and resulting model accuracy in machine learning applications. The wish list for an ideal optimizer includes fast and smooth convergence to low error, low computational demand, and general applicability. Our recently introduced continual resilient (CoRe) optimizer has shown superior performance compared to other state-of-the-art first-order gradient-based optimizers for training lifelong machine learning potentials. In this work we provide an extensive performance comparison of the CoRe optimizer and nine other optimization algorithms including the Adam optimizer and resilient backpropagation (RPROP) for diverse machine learning tasks. We analyze the influence of different hyperparameters and provide generally applicable values. The CoRe optimizer yields best or competitive performance in every investigated application, while only one hyperparameter needs to be changed depending on mini-batch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#28857;&#20987;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#32954;&#32467;&#33410;&#21644;&#32959;&#22359;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#30149;&#21464;&#28857;&#20987;&#26469;&#22686;&#24378;&#20998;&#21106;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#30340;&#30149;&#21464;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15645</link><description>&lt;p&gt;
&#32954;&#32467;&#33410;&#21644;&#32959;&#22359;&#20998;&#21106;&#30340;&#23610;&#24230;&#24863;&#30693;&#27979;&#35797;&#26102;&#28857;&#20987;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Scale-aware Test-time Click Adaptation for Pulmonary Nodule and Mass Segmentation. (arXiv:2307.15645v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#28857;&#20987;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#32954;&#32467;&#33410;&#21644;&#32959;&#22359;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#30149;&#21464;&#28857;&#20987;&#26469;&#22686;&#24378;&#20998;&#21106;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#30340;&#30149;&#21464;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#32467;&#33410;&#21644;&#32959;&#22359;&#26159;&#32954;&#30284;&#31579;&#26597;&#20013;&#38656;&#35201;&#20180;&#32454;&#22788;&#29702;&#30340;&#20851;&#38190;&#24433;&#20687;&#29305;&#24449;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#19981;&#21516;&#22823;&#23567;&#30340;&#32467;&#33410;&#21644;&#32959;&#22359;&#30340;&#40065;&#26834;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#23610;&#24230;&#24863;&#30693;&#27979;&#35797;&#26102;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23610;&#24230;&#24863;&#30693;&#27979;&#35797;&#26102;&#28857;&#20987;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#22522;&#20110;&#26131;&#33719;&#21462;&#30340;&#30149;&#21464;&#28857;&#20987;&#20316;&#20026;&#27979;&#35797;&#26102;&#30340;&#32447;&#32034;&#65292;&#20197;&#22686;&#24378;&#20998;&#21106;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#30340;&#30149;&#21464;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#32593;&#32476;&#20013;&#12290;&#22312;&#24320;&#28304;&#25968;&#25454;&#38598;&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#19968;&#20123;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#20998;&#21106;&#26041;&#27861;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/SplinterLi/SaTTCA&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulmonary nodules and masses are crucial imaging features in lung cancer screening that require careful management in clinical diagnosis. Despite the success of deep learning-based medical image segmentation, the robust performance on various sizes of lesions of nodule and mass is still challenging. In this paper, we propose a multi-scale neural network with scale-aware test-time adaptation to address this challenge. Specifically, we introduce an adaptive Scale-aware Test-time Click Adaptation method based on effortlessly obtainable lesion clicks as test-time cues to enhance segmentation performance, particularly for large lesions. The proposed method can be seamlessly integrated into existing networks. Extensive experiments on both open-source and in-house datasets consistently demonstrate the effectiveness of the proposed method over some CNN and Transformer-based segmentation methods. Our code is available at https://github.com/SplinterLi/SaTTCA
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36924;&#30495;&#30340;&#29615;&#22659;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24050;&#23384;&#22312;&#30340;&#20195;&#29702;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;</title><link>http://arxiv.org/abs/2307.15644</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#25968;&#25454;&#29983;&#25104;&#35268;&#27169;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36924;&#30495;&#30340;&#29615;&#22659;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24050;&#23384;&#22312;&#30340;&#20195;&#29702;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35821;&#35328;&#24341;&#23548;&#30340;&#35270;&#35273;&#23548;&#33322;&#30740;&#31350;&#20013;&#65292;&#23545;&#20110;&#36941;&#21382;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#21644;&#35757;&#32451;&#21487;&#27867;&#21270;&#20195;&#29702;&#30340;&#30417;&#30563;&#25968;&#37327;&#26377;&#20102;&#26126;&#26174;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;HM3D&#21644;Gibson&#25968;&#25454;&#38598;&#20013;&#30340;1200&#22810;&#20010;&#36924;&#30495;&#30340;&#29615;&#22659;&#65292;&#24182;&#21033;&#29992;&#32593;&#32476;&#19978;&#30340;&#36164;&#28304;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#33539;&#24335;&#20013;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#23545;&#20195;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#24688;&#24403;&#22320;&#24212;&#29992;&#25193;&#22686;&#25968;&#25454;&#26469;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20195;&#29702;&#12290;&#24471;&#30410;&#20110;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#29616;&#26377;&#20195;&#29702;&#30340;&#24615;&#33021;&#21487;&#20197;&#22823;&#24133;&#25552;&#21319;&#65288;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#32477;&#23545;&#20540;&#22686;&#21152;&#20102;11%&#65289;&#65292;&#22312;R2R&#27979;&#35797;&#38598;&#20013;&#21333;&#27425;&#36816;&#34892;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in language-guided visual navigation has demonstrated a significant demand for the diversity of traversable environments and the quantity of supervision for training generalizable agents. To tackle the common data scarcity issue in existing vision-and-language navigation datasets, we propose an effective paradigm for generating large-scale data for learning, which applies 1200+ photo-realistic environments from HM3D and Gibson datasets and synthesizes 4.9 million instruction trajectory pairs using fully-accessible resources on the web. Importantly, we investigate the influence of each component in this paradigm on the agent's performance and study how to adequately apply the augmented data to pre-train and fine-tune an agent. Thanks to our large-scale dataset, the performance of an existing agent can be pushed up (+11% absolute with regard to previous SoTA) to a significantly new best of 80% single-run success rate on the R2R test split by simple imitation learning. The
&lt;/p&gt;</description></item><item><title>TriadNet&#26159;&#19968;&#31181;&#26080;&#37319;&#26679;&#39044;&#27979;&#21306;&#38388;&#30340;&#20998;&#21106;&#26041;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#33041;&#30149;&#21464;&#20307;&#31215;&#65292;&#24182;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.15638</link><description>&lt;p&gt;
TriadNet: &#26080;&#37319;&#26679;&#39044;&#27979;&#21306;&#38388;&#29992;&#20110;&#19977;&#32500;&#33041;MR&#22270;&#20687;&#20013;&#30340;&#30149;&#21464;&#20307;&#31215;
&lt;/p&gt;
&lt;p&gt;
TriadNet: Sampling-free predictive intervals for lesional volume in 3D brain MR images. (arXiv:2307.15638v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15638
&lt;/p&gt;
&lt;p&gt;
TriadNet&#26159;&#19968;&#31181;&#26080;&#37319;&#26679;&#39044;&#27979;&#21306;&#38388;&#30340;&#20998;&#21106;&#26041;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#33041;&#30149;&#21464;&#20307;&#31215;&#65292;&#24182;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30149;&#21464;&#65288;&#22914;&#26775;&#27515;&#25110;&#32959;&#30244;&#65289;&#30340;&#20307;&#31215;&#26159;&#24739;&#32773;&#39044;&#21518;&#30340;&#26377;&#21147;&#25351;&#26631;&#65292;&#24182;&#21487;&#29992;&#20110;&#25351;&#23548;&#27835;&#30103;&#31574;&#30053;&#12290;&#30446;&#21069;&#65292;&#36890;&#24120;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#20998;&#21106;&#26469;&#20272;&#35745;&#30149;&#21464;&#20307;&#31215;&#65292;&#36825;&#26159;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#23558;&#20307;&#31215;&#20998;&#21106;&#24037;&#20855;&#37197;&#22791;&#36866;&#24403;&#30340;&#23450;&#37327;&#39044;&#27979;&#21306;&#38388;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#26377;&#29992;&#24615;&#21644;&#25509;&#21463;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TriadNet&#65292;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;CNN&#26550;&#26500;&#30340;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;&#19968;&#31186;&#38047;&#20869;&#21516;&#26102;&#25552;&#20379;&#30149;&#21464;&#20307;&#31215;&#21644;&#30456;&#20851;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#22312;BraTS 2021&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;MRI&#33041;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#22270;&#20687;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The volume of a brain lesion (e.g. infarct or tumor) is a powerful indicator of patient prognosis and can be used to guide the therapeutic strategy. Lesional volume estimation is usually performed by segmentation with deep convolutional neural networks (CNN), currently the state-of-the-art approach. However, to date, few work has been done to equip volume segmentation tools with adequate quantitative predictive intervals, which can hinder their usefulness and acceptation in clinical practice. In this work, we propose TriadNet, a segmentation approach relying on a multi-head CNN architecture, which provides both the lesion volumes and the associated predictive intervals simultaneously, in less than a second. We demonstrate its superiority over other solutions on BraTS 2021, a large-scale MRI glioblastoma image database.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#36710;&#36947;&#25913;&#21464;&#24847;&#22270;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#38598;&#25104;&#26041;&#27861;&#33021;&#38477;&#20302;&#20998;&#31867;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#32780;LightGBM&#30456;&#27604;XGBoost&#31639;&#27861;&#22312;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#19978;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.15625</link><description>&lt;p&gt;
&#36890;&#36807;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#36827;&#34892;&#36710;&#36947;&#25913;&#21464;&#24847;&#22270;&#35782;&#21035;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Machine Learning Methods for Lane Change Intention Recognition Using Vehicle Trajectory Data. (arXiv:2307.15625v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#36710;&#36947;&#25913;&#21464;&#24847;&#22270;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#38598;&#25104;&#26041;&#27861;&#33021;&#38477;&#20302;&#20998;&#31867;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#32780;LightGBM&#30456;&#27604;XGBoost&#31639;&#27861;&#22312;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#19978;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#26816;&#27979;&#21644;&#39044;&#27979;&#36710;&#36947;&#25913;&#21464;&#36807;&#31243;&#21487;&#20197;&#24110;&#21161;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26356;&#22909;&#22320;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#65292;&#35782;&#21035;&#28508;&#22312;&#30340;&#23433;&#20840;&#38544;&#24739;&#65292;&#24182;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#36710;&#36947;&#25913;&#21464;&#36807;&#31243;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20174;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#35782;&#21035;&#36710;&#36947;&#25913;&#21464;&#24847;&#22270;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#25152;&#25552;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20174;CitySim&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20102;&#24635;&#20849;1023&#20010;&#36710;&#36742;&#36712;&#36857;&#12290;&#23545;&#20110;&#36710;&#36947;&#25913;&#21464;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#23558;II&#22411;&#21644;III&#22411;&#20998;&#31867;&#38169;&#35823;&#30340;&#24433;&#21709;&#38477;&#20302;&#21040;98%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;&#22312;&#19981;&#25439;&#22833;&#35782;&#21035;&#20934;&#30830;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;LightGBM&#30456;&#27604;XGBoost&#31639;&#27861;&#22312;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#19978;&#25552;&#39640;&#20102;6&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately detecting and predicting lane change (LC)processes can help autonomous vehicles better understand their surrounding environment, recognize potential safety hazards, and improve traffic safety. This paper focuses on LC processes and compares different machine learning methods' performance to recognize LC intention from high-dimensionality time series data. To validate the performance of the proposed models, a total number of 1023 vehicle trajectories is extracted from the CitySim dataset. For LC intention recognition issues, the results indicate that with ninety-eight percent of classification accuracy, ensemble methods reduce the impact of Type II and Type III classification errors. Without sacrificing recognition accuracy, the LightGBM demonstrates a sixfold improvement in model training efficiency than the XGBoost algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;PBT-NAS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#21644;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#25910;&#32553;&#25200;&#21160;&#25216;&#26415;&#25913;&#36827;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;PBT-NAS&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15621</link><description>&lt;p&gt;
&#20351;&#29992;&#25910;&#32553;&#25200;&#21160;&#25216;&#26415;&#25913;&#36827;&#31181;&#32676;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26550;&#26500;&#28151;&#21512;&#65292;&#25552;&#39640;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25928;&#26524;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Shrink-Perturb Improves Architecture Mixing during Population Based Training for Neural Architecture Search. (arXiv:2307.15621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;PBT-NAS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#21644;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#25910;&#32553;&#25200;&#21160;&#25216;&#26415;&#25913;&#36827;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;PBT-NAS&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#21516;&#26102;&#35757;&#32451;&#21644;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#26159;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#37096;&#20998;&#35757;&#32451;&#22909;&#30340;&#26435;&#37325;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25628;&#32034;&#65292;&#36825;&#22312;&#31181;&#32676;&#35757;&#32451;&#65288;PBT&#65289;&#31639;&#27861;&#20013;&#24050;&#32463;&#26377;&#25152;&#23637;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PBT-NAS&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#31181;&#32676;&#35757;&#32451;&#65288;PBT&#65289;&#31639;&#27861;&#36866;&#24212;&#21040;NAS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#34920;&#29616;&#24046;&#30340;&#32593;&#32476;&#26367;&#25442;&#20026;&#28151;&#21512;&#34920;&#29616;&#22909;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#25910;&#32553;&#25200;&#21160;&#25216;&#26415;&#32487;&#25215;&#26435;&#37325;&#26469;&#25913;&#36827;&#26550;&#26500;&#12290;&#22312;PBT-NAS&#32467;&#26463;&#21518;&#65292;&#21019;&#24314;&#30340;&#32593;&#32476;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;PBT-NAS&#20855;&#26377;&#39640;&#24230;&#21487;&#24182;&#34892;&#21270;&#21644;&#26377;&#25928;&#24615;&#65292;&#23545;&#20110;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65288;&#22270;&#20687;&#29983;&#25104;&#21644;&#24378;&#21270;&#23398;&#20064;&#65289;&#26469;&#35828;&#65292;PBT-NAS&#30456;&#27604;&#20110;&#22522;&#20934;&#65288;&#38543;&#26426;&#25628;&#32034;&#21644;&#22522;&#20110;&#31361;&#21464;&#30340;PBT&#65289;&#23454;&#29616;&#20102;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we show that simultaneously training and mixing neural networks is a promising way to conduct Neural Architecture Search (NAS). For hyperparameter optimization, reusing the partially trained weights allows for efficient search, as was previously demonstrated by the Population Based Training (PBT) algorithm. We propose PBT-NAS, an adaptation of PBT to NAS where architectures are improved during training by replacing poorly-performing networks in a population with the result of mixing well-performing ones and inheriting the weights using the shrink-perturb technique. After PBT-NAS terminates, the created networks can be directly used without retraining. PBT-NAS is highly parallelizable and effective: on challenging tasks (image generation and reinforcement learning) PBT-NAS achieves superior performance compared to baselines (random search and mutation-based PBT).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#38543;&#26426;&#25968;&#24207;&#21015;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#20998;&#24067;&#30340;&#21069;&#25552;&#19979;&#23545;&#27700;&#21360;&#25991;&#26412;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25913;&#20889;&#25915;&#20987;&#19979;&#20381;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;40-50%&#30340;&#38543;&#26426;&#25200;&#21160;&#19979;&#20173;&#21487;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#27700;&#21360;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.15593</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#38543;&#26426;&#25968;&#24207;&#21015;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#20998;&#24067;&#30340;&#21069;&#25552;&#19979;&#23545;&#27700;&#21360;&#25991;&#26412;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25913;&#20889;&#25915;&#20987;&#19979;&#20381;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;40-50%&#30340;&#38543;&#26426;&#25200;&#21160;&#19979;&#20173;&#21487;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#27700;&#21360;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36825;&#20123;&#27700;&#21360;&#23545;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#25991;&#26412;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#35777;&#29983;&#25104;&#39044;&#31639;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#29992;&#38543;&#26426;&#27700;&#21360;&#23494;&#38053;&#35745;&#31639;&#30340;&#38543;&#26426;&#25968;&#24207;&#21015;&#26144;&#23556;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#26469;&#29983;&#25104;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#12290;&#35201;&#26816;&#27979;&#27700;&#21360;&#25991;&#26412;&#65292;&#21482;&#35201;&#30693;&#36947;&#23494;&#38053;&#30340;&#20219;&#20309;&#19968;&#26041;&#37117;&#21487;&#20197;&#23558;&#25991;&#26412;&#19982;&#38543;&#26426;&#25968;&#24207;&#21015;&#23545;&#40784;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#37319;&#26679;&#26041;&#26696;&#26469;&#23454;&#20363;&#21270;&#27700;&#21360;&#26041;&#27861;&#65306;&#21453;&#21464;&#25442;&#37319;&#26679;&#21644;&#25351;&#25968;&#26368;&#23567;&#37319;&#26679;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27700;&#21360;&#24212;&#29992;&#20110;&#19977;&#20010;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;OPT-1.3B&#12289;LLaMA-7B&#21644;Alpaca-7B&#65292;&#20197;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#23545;&#21508;&#31181;&#25913;&#20889;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;OPT-1.3B&#21644;LLaMA-7B&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25200;&#21160;&#20102;40-50%&#30340;&#35789;&#20803;&#21518;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#65288;$p \leq 0.01$&#65289;&#65292;&#21482;&#38656;&#35201;35&#20010;&#35789;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens via random
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21160;&#24577;&#22270;&#20013;&#30340;k&#20013;&#24515;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#21253;&#25324;&#30830;&#23450;&#24615;&#36882;&#20943;&#30340;&#65288;2+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#21644;&#38543;&#26426;&#22686;&#37327;&#30340;&#65288;4+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#38024;&#23545;&#21152;&#26435;&#22270;&#30340;&#25674;&#38144;&#26356;&#26032;&#26102;&#38388;&#20026;kn^{o(1)}&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31616;&#21270;&#26041;&#27861;&#24471;&#21040;&#20102;&#23545;&#20110;k&#20013;&#24515;&#38382;&#39064;&#30340;&#20840;&#21160;&#24577;&#65288;2+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15557</link><description>&lt;p&gt;
&#22270;&#19978;k&#20013;&#24515;&#38382;&#39064;&#30340;&#21160;&#24577;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic algorithms for k-center on graphs. (arXiv:2307.15557v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21160;&#24577;&#22270;&#20013;&#30340;k&#20013;&#24515;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#21253;&#25324;&#30830;&#23450;&#24615;&#36882;&#20943;&#30340;&#65288;2+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#21644;&#38543;&#26426;&#22686;&#37327;&#30340;&#65288;4+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#38024;&#23545;&#21152;&#26435;&#22270;&#30340;&#25674;&#38144;&#26356;&#26032;&#26102;&#38388;&#20026;kn^{o(1)}&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31616;&#21270;&#26041;&#27861;&#24471;&#21040;&#20102;&#23545;&#20110;k&#20013;&#24515;&#38382;&#39064;&#30340;&#20840;&#21160;&#24577;&#65288;2+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21160;&#24577;&#22270;&#20013;&#30340;k&#20013;&#24515;&#38382;&#39064;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#36873;&#25321;k&#20010;&#20013;&#24515;&#23558;&#36755;&#20837;&#20998;&#20026;k&#20010;&#38598;&#21512;&#65292;&#20351;&#24471;&#20219;&#24847;&#25968;&#25454;&#28857;&#21040;&#26368;&#36817;&#20013;&#24515;&#30340;&#26368;&#22823;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#24050;&#30693;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#35201;&#33719;&#24471;&#20248;&#20110;2&#30340;&#36817;&#20284;&#35299;&#26159;NP&#38590;&#30340;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#36755;&#20837;&#21487;&#20197;&#33258;&#28982;&#22320;&#24314;&#27169;&#20026;&#22270;&#65292;&#20294;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;k&#20013;&#24515;&#38382;&#39064;&#30340;&#20808;&#21069;&#30740;&#31350;&#37117;&#26159;&#22522;&#20110;&#24230;&#37327;&#31354;&#38388;&#30340;&#12290;&#26412;&#25991;&#32473;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#36882;&#20943;&#30340;&#65288;2+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#22686;&#37327;&#30340;&#65288;4+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#65292;&#23545;&#20110;&#21152;&#26435;&#22270;&#65292;&#20004;&#31181;&#31639;&#27861;&#30340;&#25674;&#38144;&#26356;&#26032;&#26102;&#38388;&#20026;kn^{o(1)}&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#32422;&#31616;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#23545;&#20110;k&#20013;&#24515;&#38382;&#39064;&#30340;&#20840;&#21160;&#24577;&#65288;2+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#65292;&#20854;&#26368;&#22351;&#24773;&#20917;&#26356;&#26032;&#26102;&#38388;&#19982;&#32500;&#25252;&#65288;1+k&#36817;&#20284;&#35299;&#30340;&#26368;&#26032;&#19978;&#30028;&#30456;&#24046;&#19981;&#36229;&#36807;k&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we give the first efficient algorithms for the $k$-center problem on dynamic graphs undergoing edge updates. In this problem, the goal is to partition the input into $k$ sets by choosing $k$ centers such that the maximum distance from any data point to the closest center is minimized. It is known that it is NP-hard to get a better than $2$ approximation for this problem.  While in many applications the input may naturally be modeled as a graph, all prior works on $k$-center problem in dynamic settings are on metrics. In this paper, we give a deterministic decremental $(2+\epsilon)$-approximation algorithm and a randomized incremental $(4+\epsilon)$-approximation algorithm, both with amortized update time $kn^{o(1)}$ for weighted graphs. Moreover, we show a reduction that leads to a fully dynamic $(2+\epsilon)$-approximation algorithm for the $k$-center problem, with worst-case update time that is within a factor $k$ of the state-of-the-art upper bound for maintaining $(1+
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#25277;&#35937;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#25277;&#35937;&#30340;&#29992;&#36884;&#21462;&#20915;&#20110;&#20855;&#20307;&#22330;&#26223;&#65292;&#35831;&#27714;&#31616;&#21333;&#30340;&#31895;&#30053;&#25277;&#35937;&#21516;&#26679;&#20250;&#26377;&#20854;&#29992;&#36884;&#65292;&#32780;&#23545;&#20110;&#26356;&#22797;&#26434;</title><link>http://arxiv.org/abs/2307.15546</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#25277;&#35937;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
On the Trade-off Between Efficiency and Precision of Neural Abstraction. (arXiv:2307.15546v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#25277;&#35937;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#25277;&#35937;&#30340;&#29992;&#36884;&#21462;&#20915;&#20110;&#20855;&#20307;&#22330;&#26223;&#65292;&#35831;&#27714;&#31616;&#21333;&#30340;&#31895;&#30053;&#25277;&#35937;&#21516;&#26679;&#20250;&#26377;&#20854;&#29992;&#36884;&#65292;&#32780;&#23545;&#20110;&#26356;&#22797;&#26434;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25277;&#35937;&#26368;&#36817;&#34987;&#24341;&#20837;&#20316;&#20026;&#22797;&#26434;&#38750;&#32447;&#24615;&#21160;&#21147;&#27169;&#22411;&#30340;&#24418;&#24335;&#36817;&#20284;&#12290;&#23427;&#20204;&#21253;&#25324;&#19968;&#20010;&#31070;&#32463;ODE&#21644;&#25277;&#35937;&#31070;&#32463;&#32593;&#32476;&#19982;&#20855;&#20307;&#21160;&#21147;&#27169;&#22411;&#20043;&#38388;&#35823;&#24046;&#30340;&#35777;&#26126;&#19978;&#30028;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#31070;&#32463;&#25277;&#35937;&#20165;&#20197;&#20840;$ReLU$&#28608;&#27963;&#20989;&#25968;&#32452;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#24471;&#21040;&#65292;&#23548;&#33268;&#20855;&#26377;&#20998;&#27573;&#20223;&#23556;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;ODE&#27169;&#22411;&#65292;&#21487;&#20197;&#31561;&#25928;&#22320;&#35299;&#37322;&#20026;&#32447;&#24615;&#28151;&#21512;&#33258;&#21160;&#26426;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25277;&#35937;&#30340;&#25928;&#29992;&#21462;&#20915;&#20110;&#23427;&#30340;&#20351;&#29992;&#65306;&#26576;&#20123;&#24773;&#20917;&#21487;&#33021;&#38656;&#35201;&#23481;&#26131;&#20998;&#26512;&#30340;&#31895;&#30053;&#25277;&#35937;&#65292;&#32780;&#20854;&#20182;&#24773;&#20917;&#21487;&#33021;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#31934;&#32454;&#25277;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#26367;&#20195;&#24418;&#29366;&#30340;&#31070;&#32463;&#25277;&#35937;&#65292;&#21363;&#20998;&#27573;&#24120;&#25968;&#25110;&#38750;&#32447;&#24615;&#38750;&#22810;&#39033;&#24335;&#65288;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;sigmoidal&#28608;&#27963;&#20989;&#25968;&#33719;&#24471;&#65289;&#12290;&#25105;&#20204;&#37319;&#29992;&#27491;&#24335;&#30340;&#24402;&#32435;&#32508;&#21512;&#31243;&#24207;&#26469;&#29983;&#25104;&#31070;&#32463;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural abstractions have been recently introduced as formal approximations of complex, nonlinear dynamical models. They comprise a neural ODE and a certified upper bound on the error between the abstract neural network and the concrete dynamical model. So far neural abstractions have exclusively been obtained as neural networks consisting entirely of $ReLU$ activation functions, resulting in neural ODE models that have piecewise affine dynamics, and which can be equivalently interpreted as linear hybrid automata. In this work, we observe that the utility of an abstraction depends on its use: some scenarios might require coarse abstractions that are easier to analyse, whereas others might require more complex, refined abstractions. We therefore consider neural abstractions of alternative shapes, namely either piecewise constant or nonlinear non-polynomial (specifically, obtained via sigmoidal activations). We employ formal inductive synthesis procedures to generate neural abstractions t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#34987;&#27745;&#26579;&#26679;&#26412;&#20013;&#27880;&#20837;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#65292;&#24403;&#35302;&#21457;&#26102;&#21487;&#20197;&#25233;&#21046;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.15539</link><description>&lt;p&gt;
&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Backdoor Defense with Non-Adversarial Backdoor. (arXiv:2307.15539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#34987;&#27745;&#26579;&#26679;&#26412;&#20013;&#27880;&#20837;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#65292;&#24403;&#35302;&#21457;&#26102;&#21487;&#20197;&#25233;&#21046;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#24182;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24615;&#33021;&#65292;&#20294;&#19968;&#26086;&#28155;&#21152;&#35302;&#21457;&#27169;&#24335;&#65292;&#23601;&#20250;&#25805;&#32437;&#32593;&#32476;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#20294;&#23427;&#20204;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20173;&#28982;&#36828;&#36828;&#33853;&#21518;&#20110;&#24178;&#20928;&#27169;&#22411;&#12290;&#21463;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#38450;&#24481;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#27880;&#20837;&#20102;&#38024;&#23545;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#12290;&#25353;&#29031;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#33324;&#27493;&#39588;&#65292;&#25105;&#20204;&#26816;&#27979;&#19968;&#23567;&#32452;&#21487;&#30097;&#26679;&#26412;&#65292;&#28982;&#21518;&#23545;&#23427;&#20204;&#24212;&#29992;&#27602;&#21270;&#31574;&#30053;&#12290;&#19968;&#26086;&#35302;&#21457;&#65292;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#25233;&#21046;&#20102;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20294;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#38450;&#24481;&#21487;&#20197;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#26399;&#38388;&#36827;&#34892;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26631;&#20934;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#31243;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not affect the network's performance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing defense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags behind a clean model by a large margin. Inspired by the stealthiness and effectiveness of backdoor attack, we propose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data. The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on mul
&lt;/p&gt;</description></item><item><title>Federated Learning&#65288;FL&#65289;&#22312;&#23448;&#26041;&#32479;&#35745;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#25552;&#21319;&#25968;&#25454;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15503</link><description>&lt;p&gt;
Federated Learning&#22312;&#23448;&#26041;&#32479;&#35745;&#20013;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Applicability of Federated Learning to Official Statistics. (arXiv:2307.15503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15503
&lt;/p&gt;
&lt;p&gt;
Federated Learning&#65288;FL&#65289;&#22312;&#23448;&#26041;&#32479;&#35745;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#25552;&#21319;&#25968;&#25454;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;Federated Learning&#65288;FL&#65289;&#22312;&#23448;&#26041;&#32479;&#35745;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;FL&#27169;&#22411;&#24615;&#33021;&#19982;&#38598;&#20013;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#21305;&#37197;&#31243;&#24230;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;FL&#21487;&#20197;&#20445;&#25252;&#25968;&#25454;&#25345;&#26377;&#32773;&#30340;&#38544;&#31169;&#65292;&#20174;&#32780;&#20415;&#20110;&#33719;&#21462;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#24182;&#26368;&#32456;&#25552;&#21319;&#23448;&#26041;&#32479;&#35745;&#25968;&#25454;&#12290;&#36890;&#36807;&#27169;&#25311;&#19977;&#31181;&#19981;&#21516;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#25216;&#26415;&#30340;&#36866;&#29992;&#24615;&#33719;&#24471;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;&#36825;&#20123;&#20351;&#29992;&#26696;&#20363;&#22522;&#20110;&#21307;&#30103;&#20445;&#38505;&#25968;&#25454;&#38598;&#12289;&#32454;&#39063;&#31890;&#29289;&#27745;&#26579;&#25968;&#25454;&#38598;&#21644;&#31227;&#21160;&#26080;&#32447;&#20449;&#21495;&#35206;&#30422;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19982;&#23448;&#26041;&#32479;&#35745;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#32467;&#26524;&#65292;&#21253;&#25324;&#23545;&#27599;&#20010;&#27169;&#25311;&#20013;&#38598;&#20013;&#24335;&#21644;FL&#31639;&#27861;&#24615;&#33021;&#30340;&#27604;&#36739;&#12290;&#22312;&#36825;&#19977;&#20010;&#20351;&#29992;&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;FL&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#19982;&#38598;&#20013;&#24335;&#27169;&#22411;&#22522;&#20934;&#38750;&#24120;&#25509;&#36817;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#21644;&#23427;&#20204;&#23545;&#27169;&#25311;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the potential of Federated Learning (FL) for official statistics and shows how well the performance of FL models can keep up with centralized learning methods. At the same time, its utilization can safeguard the privacy of data holders, thus facilitating access to a broader range of data and ultimately enhancing official statistics. By simulating three different use cases, important insights on the applicability of the technology are gained. The use cases are based on a medical insurance data set, a fine dust pollution data set and a mobile radio coverage data set - all of which are from domains close to official statistics. We provide a detailed analysis of the results, including a comparison of centralized and FL algorithm performances for each simulation. In all three use cases, we were able to train models via FL which reach a performance very close to the centralized model benchmarks. Our key observations and their implications for transferring the simulatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#24352;&#37327;&#21015;&#36710;&#26041;&#27861;&#32467;&#21512;&#22238;&#24402;&#31867;&#22411;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#32500;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#20540;&#36924;&#36817;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#25240;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.15496</link><description>&lt;p&gt;
&#20174;&#36830;&#32493;&#26102;&#38388;&#34920;&#36848;&#21040;&#31163;&#25955;&#21270;&#26041;&#26696;&#65306;&#24352;&#37327;&#21015;&#36710;&#21644;&#40065;&#26834;&#22238;&#24402;&#29992;&#20110;BSDEs&#21644;&#25243;&#29289;&#32447;PDEs
&lt;/p&gt;
&lt;p&gt;
From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs. (arXiv:2307.15496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#24352;&#37327;&#21015;&#36710;&#26041;&#27861;&#32467;&#21512;&#22238;&#24402;&#31867;&#22411;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#32500;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#20540;&#36924;&#36817;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#25240;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#24230;&#20013;&#25968;&#20540;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#26041;&#27861;&#21463;&#21040;&#25152;&#35859;&#32500;&#25968;&#35781;&#21650;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#23581;&#35797;&#20381;&#36182;&#20110;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#21644;&#21464;&#20998;&#34920;&#36848;&#30340;&#32452;&#21512;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20989;&#25968;&#36924;&#36817;&#12290;&#24310;&#32493;&#20043;&#21069;&#30340;&#24037;&#20316;&#65288;Richter&#31561;&#65292;2021&#65289;&#65292;&#25105;&#20204;&#35748;&#20026;&#24352;&#37327;&#21015;&#36710;&#20026;&#25243;&#29289;&#22411;PDE&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26694;&#26550;&#65306;&#36890;&#36807;&#20197;&#36870;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#22238;&#24402;&#31867;&#22411;&#26041;&#27861;&#30340;&#24418;&#24335;&#37325;&#26032;&#34920;&#36848;&#65292;&#32467;&#21512;&#28508;&#22312;&#30340;&#20302;&#31209;&#32467;&#26500;&#65292;&#26377;&#26395;&#23454;&#29616;&#21387;&#32553;&#21644;&#39640;&#25928;&#35745;&#31639;&#30340;&#30446;&#26631;&#12290;&#24378;&#35843;&#36830;&#32493;&#26102;&#38388;&#35266;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36845;&#20195;&#26041;&#26696;&#65292;&#20854;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#26377;&#21033;&#30340;&#25240;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The numerical approximation of partial differential equations (PDEs) poses formidable challenges in high dimensions since classical grid-based methods suffer from the so-called curse of dimensionality. Recent attempts rely on a combination of Monte Carlo methods and variational formulations, using neural networks for function approximation. Extending previous work (Richter et al., 2021), we argue that tensor trains provide an appealing framework for parabolic PDEs: The combination of reformulations in terms of backward stochastic differential equations and regression-type methods holds the promise of leveraging latent low-rank structures, enabling both compression and efficient computation. Emphasizing a continuous-time viewpoint, we develop iterative schemes, which differ in terms of computational efficiency and robustness. We demonstrate both theoretically and numerically that our methods can achieve a favorable trade-off between accuracy and computational efficiency. While previous 
&lt;/p&gt;</description></item><item><title>FeedbackLogs&#26159;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#21453;&#39304;&#35760;&#24405;&#21644;&#26356;&#26032;&#30340;&#34917;&#20805;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#31639;&#27861;&#23457;&#35745;&#21644;&#35760;&#24405;&#21453;&#39304;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2307.15475</link><description>&lt;p&gt;
FeedbackLogs&#65306;&#23558;&#21033;&#30410;&#30456;&#20851;&#32773;&#21453;&#39304;&#35760;&#24405;&#21644;&#32435;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;
&lt;/p&gt;
&lt;p&gt;
FeedbackLogs: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines. (arXiv:2307.15475v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15475
&lt;/p&gt;
&lt;p&gt;
FeedbackLogs&#26159;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#21453;&#39304;&#35760;&#24405;&#21644;&#26356;&#26032;&#30340;&#34917;&#20805;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#31639;&#27861;&#23457;&#35745;&#21644;&#35760;&#24405;&#21453;&#39304;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#24433;&#21709;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#20294;&#20851;&#20110;&#21033;&#30410;&#30456;&#20851;&#32773;&#36755;&#20837;&#30340;&#35760;&#24405;&#21644;&#21033;&#29992;&#26041;&#38754;&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FeedbackLogs&#65292;&#36825;&#26159;&#38024;&#23545;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#25991;&#26723;&#30340;&#34917;&#20805;&#65292;&#29992;&#20110;&#36861;&#36394;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#36755;&#20837;&#12290;&#27599;&#20010;&#26085;&#24535;&#35760;&#24405;&#20102;&#20851;&#20110;&#21453;&#39304;&#25910;&#38598;&#36807;&#31243;&#12289;&#21453;&#39304;&#20869;&#23481;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#21453;&#39304;&#26356;&#26032;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#37325;&#35201;&#32454;&#33410;&#12290;&#26412;&#25991;&#20171;&#32461;&#24182;&#27491;&#24335;&#21270;&#20102;&#25910;&#38598;FeedbackLog&#30340;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#20854;&#20013;FeedbackLogs&#21487;&#20197;&#20316;&#20026;&#31639;&#27861;&#23457;&#35745;&#30340;&#35777;&#25454;&#65292;&#24182;&#29992;&#20316;&#35760;&#24405;&#22522;&#20110;&#21033;&#30410;&#30456;&#20851;&#32773;&#21453;&#39304;&#30340;&#26356;&#26032;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though machine learning (ML) pipelines affect an increasing array of stakeholders, there is little work on how input from stakeholders is recorded and incorporated. We propose FeedbackLogs, addenda to existing documentation of ML pipelines, to track the input of multiple stakeholders. Each log records important details about the feedback collection process, the feedback itself, and how the feedback is used to update the ML pipeline. In this paper, we introduce and formalise a process for collecting a FeedbackLog. We also provide concrete use cases where FeedbackLogs can be employed as evidence for algorithmic auditing and as a tool to record updates based on stakeholder feedback.
&lt;/p&gt;</description></item><item><title>LUCID-GAN&#26159;&#19968;&#20010;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#23450;&#20301;&#19981;&#20844;&#24179;&#24615;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#35268;&#33539;&#38598;&#26469;&#25581;&#31034;&#27169;&#22411;&#30340;&#20869;&#37096;&#36923;&#36753;&#20013;&#28508;&#22312;&#30340;&#19981;&#36947;&#24503;&#20559;&#35265;&#65292;&#25552;&#20379;&#20102;&#23545;&#27495;&#35270;&#26469;&#28304;&#30340;&#39069;&#22806;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.15466</link><description>&lt;p&gt;
LUCID-GAN: &#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#23450;&#20301;&#19981;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
LUCID-GAN: Conditional Generative Models to Locate Unfairness. (arXiv:2307.15466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15466
&lt;/p&gt;
&lt;p&gt;
LUCID-GAN&#26159;&#19968;&#20010;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#23450;&#20301;&#19981;&#20844;&#24179;&#24615;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#35268;&#33539;&#38598;&#26469;&#25581;&#31034;&#27169;&#22411;&#30340;&#20869;&#37096;&#36923;&#36753;&#20013;&#28508;&#22312;&#30340;&#19981;&#36947;&#24503;&#20559;&#35265;&#65292;&#25552;&#20379;&#20102;&#23545;&#27495;&#35270;&#26469;&#28304;&#30340;&#39069;&#22806;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#36755;&#20986;&#30340;&#32479;&#35745;&#24179;&#34913;&#24230;&#25351;&#26631;&#26469;&#26816;&#27979;&#19981;&#36947;&#24503;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20960;&#20010;&#32570;&#28857;&#65292;&#22914;&#21746;&#23398;&#20998;&#27495;&#12289;&#30456;&#20114;&#19981;&#20860;&#23481;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#32570;&#28857;&#20419;&#20351;&#20102;&#23545;&#34917;&#20805;&#24615;&#20559;&#35265;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#23545;&#27495;&#35270;&#26469;&#28304;&#30340;&#39069;&#22806;&#36879;&#26126;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#23545;&#20844;&#24179;&#23450;&#20041;&#21644;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#20808;&#39564;&#20915;&#31574;&#26159;&#19981;&#21487;&#30693;&#30340;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#19968;&#20010;&#26368;&#36817;&#30340;&#25552;&#35758;&#26159;LUCID&#65288;&#36890;&#36807;&#35268;&#33539;&#36870;&#21521;&#35774;&#35745;&#23450;&#20301;&#19981;&#20844;&#24179;&#24615;&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#19978;&#36827;&#34892;&#26799;&#24230;&#19979;&#38477;&#26469;&#29983;&#25104;&#35268;&#33539;&#38598;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22312;&#32473;&#23450;&#39318;&#36873;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#25152;&#24076;&#26395;&#30340;&#36755;&#20837;&#12290;&#36825;&#20123;&#20851;&#20110;&#27169;&#22411;&#26426;&#21046;&#30340;&#20449;&#24687;&#65292;&#21363;&#20026;&#23454;&#29616;&#29305;&#23450;&#36755;&#20986;&#25152;&#24517;&#38656;&#30340;&#29305;&#24449;&#20540;&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#20869;&#37096;&#36923;&#36753;&#20013;&#28508;&#22312;&#30340;&#19981;&#36947;&#24503;&#20559;&#35265;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LUCID-GAN&#65292;&#23427;&#29983;&#25104;c
&lt;/p&gt;
&lt;p&gt;
Most group fairness notions detect unethical biases by computing statistical parity metrics on a model's output. However, this approach suffers from several shortcomings, such as philosophical disagreement, mutual incompatibility, and lack of interpretability. These shortcomings have spurred the research on complementary bias detection methods that offer additional transparency into the sources of discrimination and are agnostic towards an a priori decision on the definition of fairness and choice of protected features. A recent proposal in this direction is LUCID (Locating Unfairness through Canonical Inverse Design), where canonical sets are generated by performing gradient descent on the input space, revealing a model's desired input given a preferred output. This information about the model's mechanisms, i.e., which feature values are essential to obtain specific outputs, allows exposing potential unethical biases in its internal logic. Here, we present LUCID-GAN, which generates c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#22312;&#31616;&#21333;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#31283;&#20581;&#24615;&#20135;&#29983;&#20102;&#25285;&#24551;&#65292;&#36890;&#36807;&#23545;&#20302;&#31070;&#32463;&#20803;&#21644;&#31526;&#21495;&#25277;&#35937;&#30340;&#25506;&#31350;&#65292;&#21457;&#29616;&#21363;&#20351;&#25511;&#21046;&#22120;&#36798;&#21040;&#39640;&#22238;&#25253;&#65292;&#20173;&#20250;&#20135;&#29983;&#22823;&#37327;&#25345;&#20037;&#20302;&#22238;&#25253;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#25163;&#21487;&#20197;&#36731;&#26131;&#21033;&#29992;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#31283;&#20581;&#24615;&#30740;&#31350;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25345;&#20037;&#35299;&#20915;&#26041;&#26696;&#30340;&#23384;&#22312;&#24615;&#20197;&#21450;&#21608;&#26399;&#36712;&#36947;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2307.15456</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21450;&#20854;&#31526;&#21495;&#34920;&#31034;&#30340;&#20196;&#20154;&#25285;&#24551;&#30340;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Worrisome Properties of Neural Network Controllers and Their Symbolic Representations. (arXiv:2307.15456v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#22312;&#31616;&#21333;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#31283;&#20581;&#24615;&#20135;&#29983;&#20102;&#25285;&#24551;&#65292;&#36890;&#36807;&#23545;&#20302;&#31070;&#32463;&#20803;&#21644;&#31526;&#21495;&#25277;&#35937;&#30340;&#25506;&#31350;&#65292;&#21457;&#29616;&#21363;&#20351;&#25511;&#21046;&#22120;&#36798;&#21040;&#39640;&#22238;&#25253;&#65292;&#20173;&#20250;&#20135;&#29983;&#22823;&#37327;&#25345;&#20037;&#20302;&#22238;&#25253;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#25163;&#21487;&#20197;&#36731;&#26131;&#21033;&#29992;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#31283;&#20581;&#24615;&#30740;&#31350;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25345;&#20037;&#35299;&#20915;&#26041;&#26696;&#30340;&#23384;&#22312;&#24615;&#20197;&#21450;&#21608;&#26399;&#36712;&#36947;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#31616;&#21333;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#38382;&#39064;&#20013;&#25511;&#21046;&#22120;&#30340;&#31283;&#20581;&#24615;&#25552;&#20986;&#20102;&#19968;&#20123;&#20851;&#20999;&#12290;&#25105;&#20204;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21450;&#20854;&#20302;&#31070;&#32463;&#20803;&#21644;&#31526;&#21495;&#25277;&#35937;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#36798;&#21040;&#24456;&#39640;&#30340;&#24179;&#22343;&#22238;&#25253;&#20540;&#65292;&#20294;&#20173;&#20250;&#29983;&#25104;&#22823;&#37327;&#25345;&#20037;&#30340;&#20302;&#22238;&#25253;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#19981;&#21487;&#21462;&#30340;&#29305;&#24615;&#65292;&#23481;&#26131;&#34987;&#23545;&#25163;&#21033;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#25511;&#21046;&#22120;&#20250;&#20135;&#29983;&#26356;&#22810;&#25345;&#20037;&#30340;&#19981;&#33391;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#31283;&#20581;&#24615;&#30740;&#31350;&#30340;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#35745;&#31639;&#26426;&#36741;&#21161;&#35777;&#26126;&#26041;&#27861;&#35777;&#26126;&#20102;&#25345;&#20037;&#35299;&#20915;&#26041;&#26696;&#30340;&#23384;&#22312;&#24615;&#65292;&#20197;&#21450;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#21608;&#26399;&#36712;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We raise concerns about controllers' robustness in simple reinforcement learning benchmark problems. We focus on neural network controllers and their low neuron and symbolic abstractions. A typical controller reaching high mean return values still generates an abundance of persistent low-return solutions, which is a highly undesirable property, easily exploitable by an adversary. We find that the simpler controllers admit more persistent bad solutions. We provide an algorithm for a systematic robustness study and prove existence of persistent solutions and, in some cases, periodic orbits, using a computer-assisted proof methodology.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#21355;&#26143;&#19978;&#23398;&#20064;&#28909;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#23567;&#22411;&#21355;&#26143;&#20013;&#28909;&#25511;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#31354;&#38388;&#22788;&#29702;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#33021;&#22815;&#36741;&#21161;&#20256;&#32479;&#28909;&#25511;&#21046;&#31995;&#32479;&#65292;&#20445;&#25345;&#36733;&#33655;&#28201;&#24230;&#22312;&#21487;&#25805;&#20316;&#33539;&#22260;&#20869;&#12290;</title><link>http://arxiv.org/abs/2307.15438</link><description>&lt;p&gt;
&#33258;&#20027;&#36733;&#33655;&#28909;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Autonomous Payload Thermal Control. (arXiv:2307.15438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#21355;&#26143;&#19978;&#23398;&#20064;&#28909;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#23567;&#22411;&#21355;&#26143;&#20013;&#28909;&#25511;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#31354;&#38388;&#22788;&#29702;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#33021;&#22815;&#36741;&#21161;&#20256;&#32479;&#28909;&#25511;&#21046;&#31995;&#32479;&#65292;&#20445;&#25345;&#36733;&#33655;&#28201;&#24230;&#22312;&#21487;&#25805;&#20316;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23567;&#22411;&#21355;&#26143;&#20013;&#65292;&#28909;&#25511;&#21046;&#35774;&#22791;&#12289;&#31185;&#23398;&#20202;&#22120;&#21644;&#30005;&#23376;&#37096;&#20214;&#30340;&#31354;&#38388;&#36739;&#23567;&#12290;&#27492;&#22806;&#65292;&#30005;&#23376;&#35774;&#22791;&#30340;&#36817;&#36317;&#31163;&#20351;&#24471;&#21151;&#32791;&#25955;&#28909;&#22256;&#38590;&#65292;&#23384;&#22312;&#26080;&#27861;&#36866;&#24403;&#25511;&#21046;&#28201;&#24230;&#12289;&#38477;&#20302;&#37096;&#20214;&#23551;&#21629;&#21644;&#20219;&#21153;&#24615;&#33021;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#21033;&#29992;&#21355;&#26143;&#19978;&#36880;&#28176;&#22686;&#21152;&#30340;&#26234;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#23398;&#20064;&#26426;&#36733;&#28909;&#25511;&#21046;&#31574;&#30053;&#12290;&#35813;&#26694;&#26550;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#26410;&#26469;&#23558;&#36816;&#24448;ISS&#24182;&#22312;IMAGIN-e&#20219;&#21153;&#20013;&#36827;&#34892;&#36793;&#32536;&#35745;&#31639;&#30340;&#30495;&#23454;&#31354;&#38388;&#22788;&#29702;&#35745;&#31639;&#26426;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#25511;&#21046;&#36733;&#33655;&#22788;&#29702;&#21151;&#29575;&#65292;&#20197;&#20445;&#25345;&#28201;&#24230;&#22312;&#25805;&#20316;&#33539;&#22260;&#20869;&#65292;&#34917;&#20805;&#20256;&#32479;&#28909;&#25511;&#21046;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In small satellites there is less room for heat control equipment, scientific instruments, and electronic components. Furthermore, the near proximity of the electronics makes power dissipation difficult, with the risk of not being able to control the temperature appropriately, reducing component lifetime and mission performance. To address this challenge, taking advantage of the advent of increasing intelligence on board satellites, a deep reinforcement learning based framework that uses Soft Actor-Critic algorithm is proposed for learning the thermal control policy onboard. The framework is evaluated both in a naive simulated environment and in a real space edge processing computer that will be shipped in the future IMAGIN-e mission and hosted in the ISS. The experiment results show that the proposed framework is able to learn to control the payload processing power to maintain the temperature under operational ranges, complementing traditional thermal control systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#25913;&#36827;&#24046;&#36317;&#24179;&#34913;&#31639;&#27861;&#65288;IGB&#65289;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#19968;&#31181;&#37319;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#39318;&#27425;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#36890;&#36807;&#21160;&#24577;&#20998;&#37197;&#20219;&#21153;&#26435;&#37325;&#26469;&#23454;&#29616;&#21487;&#25913;&#36827;&#24046;&#36317;&#24179;&#34913;&#65292;&#35299;&#20915;&#20102;&#25439;&#22833;&#24179;&#34913;&#20043;&#21518;&#20173;&#28982;&#23384;&#22312;&#30340;&#24615;&#33021;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15429</link><description>&lt;p&gt;
&#21487;&#25913;&#36827;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24046;&#36317;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Improvable Gap Balancing for Multi-Task Learning. (arXiv:2307.15429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#25913;&#36827;&#24046;&#36317;&#24179;&#34913;&#31639;&#27861;&#65288;IGB&#65289;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#19968;&#31181;&#37319;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#39318;&#27425;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#36890;&#36807;&#21160;&#24577;&#20998;&#37197;&#20219;&#21153;&#26435;&#37325;&#26469;&#23454;&#29616;&#21487;&#25913;&#36827;&#24046;&#36317;&#24179;&#34913;&#65292;&#35299;&#20915;&#20102;&#25439;&#22833;&#24179;&#34913;&#20043;&#21518;&#20173;&#28982;&#23384;&#22312;&#30340;&#24615;&#33021;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#65292;&#26799;&#24230;&#24179;&#34913;&#36817;&#26399;&#27604;&#25439;&#22833;&#24179;&#34913;&#26356;&#21560;&#24341;&#30740;&#31350;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#33021;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25439;&#22833;&#24179;&#34913;&#27604;&#26799;&#24230;&#24179;&#34913;&#26356;&#39640;&#25928;&#65292;&#22240;&#27492;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#20173;&#28982;&#20540;&#24471;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#27880;&#24847;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#24573;&#30053;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#21487;&#25913;&#36827;&#24046;&#36317;&#30340;&#20107;&#23454;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#30340;&#21487;&#25913;&#36827;&#24046;&#36317;&#23450;&#20041;&#20026;&#24403;&#21069;&#35757;&#32451;&#36827;&#24230;&#19982;&#26399;&#26395;&#30340;&#26368;&#32456;&#35757;&#32451;&#36827;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;&#22312;&#25439;&#22833;&#24179;&#34913;&#20043;&#21518;&#65292;&#24615;&#33021;&#19981;&#24179;&#34913;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#25439;&#22833;&#24179;&#34913;&#26694;&#26550;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#21487;&#25913;&#36827;&#24046;&#36317;&#24179;&#34913;&#31639;&#27861;&#65288;IGB&#65289;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65306;&#19968;&#31181;&#37319;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#65288;&#39318;&#27425;&#65289;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#29305;&#21035;&#22320;&#65292;&#20004;&#31181;&#31639;&#27861;&#37117;&#36873;&#25321;&#21160;&#24577;&#20998;&#37197;&#20219;&#21153;&#26435;&#37325;&#26469;&#36827;&#34892;&#21487;&#25913;&#36827;&#24046;&#36317;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-task learning (MTL), gradient balancing has recently attracted more research interest than loss balancing since it often leads to better performance. However, loss balancing is much more efficient than gradient balancing, and thus it is still worth further exploration in MTL. Note that prior studies typically ignore that there exist varying improvable gaps across multiple tasks, where the improvable gap per task is defined as the distance between the current training progress and desired final training progress. Therefore, after loss balancing, the performance imbalance still arises in many cases. In this paper, following the loss balancing framework, we propose two novel improvable gap balancing (IGB) algorithms for MTL: one takes a simple heuristic, and the other (for the first time) deploys deep reinforcement learning for MTL. Particularly, instead of directly balancing the losses in MTL, both algorithms choose to dynamically assign task weights for improvable gap balancing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#26684;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#25968;&#25454;&#24402;&#19968;&#21270;&#12289;&#38544;&#31169;&#21644;&#35780;&#20272;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15424</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12289;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#21644;&#24046;&#20998;&#38544;&#31169;&#65306;&#32508;&#36848;&#19982;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis. (arXiv:2307.15424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#26684;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#25968;&#25454;&#24402;&#19968;&#21270;&#12289;&#38544;&#31169;&#21644;&#35780;&#20272;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#26684;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#29305;&#21035;&#27010;&#36848;&#20102;&#22312;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#32972;&#26223;&#19979;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#35814;&#32454;&#35299;&#37322;&#20102;&#21253;&#25324;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#29983;&#25104;&#27169;&#22411;&#22312;&#20869;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;&#35770;&#25991;&#28085;&#30422;&#20102;&#22312;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#38598;&#26102;&#28041;&#21450;&#30340;&#25361;&#25112;&#21644;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#24402;&#19968;&#21270;&#12289;&#38544;&#31169;&#38382;&#39064;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;&#26412;&#32508;&#36848;&#20026;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21450;&#20854;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a comprehensive synthesis of the recent developments in synthetic data generation via deep generative models, focusing on tabular datasets. We specifically outline the importance of synthetic data generation in the context of privacy-sensitive data. Additionally, we highlight the advantages of using deep generative models over other methods and provide a detailed explanation of the underlying concepts, including unsupervised learning, neural networks, and generative models. The paper covers the challenges and considerations involved in using deep generative models for tabular datasets, such as data normalization, privacy concerns, and model evaluation. This review provides a valuable resource for researchers and practitioners interested in synthetic data generation and its applications.
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#22522;&#20934;&#32447;&#22312;&#22810;&#23618;&#27425;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#21462;&#24471;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#20351;&#29992;&#35813;&#22522;&#20934;&#32447;&#24182;&#25193;&#22823;MF-HPO&#22522;&#20934;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15422</link><description>&lt;p&gt;
&#19968;&#20010;Epoch&#23601;&#36275;&#22815;&#36827;&#34892;&#22810;&#23618;&#27425;&#36229;&#21442;&#25968;&#20248;&#21270;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?. (arXiv:2307.15422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15422
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20934;&#32447;&#22312;&#22810;&#23618;&#27425;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#21462;&#24471;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#20351;&#29992;&#35813;&#22522;&#20934;&#32447;&#24182;&#25193;&#22823;MF-HPO&#22522;&#20934;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23545;&#20110;&#24494;&#35843;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#65292;&#22810;&#23618;&#27425;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;MF-HPO&#65289;&#21033;&#29992;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20013;&#38388;&#20934;&#30830;&#24615;&#32423;&#21035;&#65292;&#24182;&#22312;&#23398;&#20064;&#26089;&#26399;&#20002;&#24323;&#20302;&#24615;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#22522;&#20934;&#25968;&#25454;&#19978;&#23558;&#21508;&#31181;&#20195;&#34920;&#24615;&#30340;MF-HPO&#26041;&#27861;&#19982;&#31616;&#21333;&#30340;&#22522;&#20934;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22522;&#20934;&#32447;&#26159;&#22312;&#35757;&#32451;&#20102;&#20165;&#19968;&#20010;Epoch&#21518;&#20002;&#24323;&#38500;Top-K&#20043;&#22806;&#30340;&#25152;&#26377;&#27169;&#22411;&#65292;&#28982;&#21518;&#36827;&#19968;&#27493;&#35757;&#32451;&#20197;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20010;&#22522;&#20934;&#32447;&#19982;&#20854;&#23545;&#24212;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#32780;&#35745;&#31639;&#25104;&#26412;&#20943;&#23569;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#22312;&#20998;&#26512;&#22522;&#20934;&#25968;&#25454;&#30340;&#23398;&#20064;&#26354;&#32447;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#20960;&#20010;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#36825;&#35299;&#37322;&#20102;&#25105;&#20204;&#22522;&#20934;&#32447;&#30340;&#25104;&#21151;&#12290;&#36825;&#34920;&#26126;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#65288;1&#65289;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#20351;&#29992;&#24314;&#35758;&#30340;&#22522;&#20934;&#32447;&#65292;&#24182;&#19988;&#65288;2&#65289;&#25193;&#22823;MF-HPO&#22522;&#20934;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#65292;&#21253;&#25324;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is crucial for fine-tuning machine learning models but can be computationally expensive. To reduce costs, Multi-fidelity HPO (MF-HPO) leverages intermediate accuracy levels in the learning process and discards low-performing models early on. We compared various representative MF-HPO methods against a simple baseline on classical benchmark data. The baseline involved discarding all models except the Top-K after training for only one epoch, followed by further training to select the best model. Surprisingly, this baseline achieved similar results to its counterparts, while requiring an order of magnitude less computation. Upon analyzing the learning curves of the benchmark data, we observed a few dominant learning curves, which explained the success of our baseline. This suggests that researchers should (1) always use the suggested baseline in benchmarks and (2) broaden the diversity of MF-HPO benchmarks to include more complex cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#28508;&#22312;&#30340;&#33258;&#21160;&#21270;&#25307;&#32856;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.15398</link><description>&lt;p&gt;
&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Initial Screening Order Problem. (arXiv:2307.15398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#28508;&#22312;&#30340;&#33258;&#21160;&#21270;&#25307;&#32856;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#36825;&#26159;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23427;&#28041;&#21450;&#19968;&#20010;&#31867;&#20284;&#20154;&#31867;&#30340;&#31579;&#36873;&#32773;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#30340;&#20505;&#36873;&#20154;&#27744;&#20013;&#25214;&#21040;&#21069;k&#20010;&#36866;&#21512;&#30340;&#20505;&#36873;&#20154;&#65292;&#32780;&#19981;&#26159;&#26368;&#22909;&#30340;k&#20010;&#36866;&#21512;&#30340;&#20505;&#36873;&#20154;&#12290;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#34920;&#31034;&#31867;&#20154;&#31579;&#36873;&#32773;&#22312;&#31579;&#36873;&#20043;&#21069;&#22914;&#20309;&#23433;&#25490;&#20505;&#36873;&#20154;&#27744;&#12290;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#30340;&#36873;&#25321;&#23545;&#25152;&#36873;&#30340;k&#20010;&#20505;&#36873;&#20154;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#30007;&#24615;&#20505;&#36873;&#20154;&#22810;&#20110;&#22899;&#24615;&#20505;&#36873;&#20154;&#65289;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#23545;&#21463;&#20445;&#25252;&#30340;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#24179;&#31561;&#30340;&#21162;&#21147;&#12290;&#20854;&#20182;&#20844;&#24179;&#24615;&#32467;&#26524;&#20063;&#22312;&#31867;&#20154;&#31579;&#36873;&#32773;&#19979;&#24471;&#21040;&#35777;&#26126;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#30340;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#20854;&#28508;&#22312;&#33258;&#21160;&#21270;&#30340;&#25307;&#32856;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the initial screening order problem, a crucial step within candidate screening. It involves a human-like screener with an objective to find the first k suitable candidates rather than the best k suitable candidates in a candidate pool given an initial screening order. The initial screening order represents the way in which the human-like screener arranges the candidate pool prior to screening. The choice of initial screening order has considerable effects on the selected set of k candidates. We prove that under an unbalanced candidate pool (e.g., having more male than female candidates), the human-like screener can suffer from uneven efforts that hinder its decision-making over the protected, under-represented group relative to the non-protected, over-represented group. Other fairness results are proven under the human-like screener. This research is based on a collaboration with a large company to better understand its hiring process for potential automation. 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#23567;&#33539;&#25968;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#36827;&#34892;&#22122;&#22768;&#21333;&#21464;&#37327;&#22238;&#24402;&#30340;&#25554;&#20540;&#65292;&#23545;&#20110;$L_1$&#25439;&#22833;&#21644;$ p &lt;2 $&#30340;$L_p$&#25439;&#22833;&#25233;&#21046;&#36807;&#25311;&#21512;&#65292;&#20294;&#23545;&#20110;$ p \geq 2 $&#30340;&#25439;&#22833;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.15396</link><description>&lt;p&gt;
&#22522;&#20110;&#27973;&#23618;&#21333;&#21464;&#37327;ReLU&#32593;&#32476;&#30340;&#22122;&#22768;&#25554;&#20540;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Noisy Interpolation Learning with Shallow Univariate ReLU Networks. (arXiv:2307.15396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15396
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#23567;&#33539;&#25968;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#36827;&#34892;&#22122;&#22768;&#21333;&#21464;&#37327;&#22238;&#24402;&#30340;&#25554;&#20540;&#65292;&#23545;&#20110;$L_1$&#25439;&#22833;&#21644;$ p &lt;2 $&#30340;$L_p$&#25439;&#22833;&#25233;&#21046;&#36807;&#25311;&#21512;&#65292;&#20294;&#23545;&#20110;$ p \geq 2 $&#30340;&#25439;&#22833;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22122;&#22768;&#21333;&#21464;&#37327;&#22238;&#24402;&#20013;&#20351;&#29992;&#26368;&#23567;&#33539;&#25968;&#65288;&#26435;&#37325;&#30340;$\ell_2$&#33539;&#25968;&#65289;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#36827;&#34892;&#25554;&#20540;&#30340;&#28176;&#36817;&#36807;&#25311;&#21512;&#34892;&#20026;&#12290;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;$L_1$&#25439;&#22833;&#21644;$ p &lt;2 $&#30340;&#20219;&#20309;$L_p$&#25439;&#22833;&#65292;&#36807;&#25311;&#21512;&#29616;&#35937;&#20250;&#34987;&#25233;&#21046;&#65292;&#20294;&#23545;&#20110;$ p \geq 2 $&#30340;&#25439;&#22833;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the asymptotic overfitting behavior of interpolation with minimum norm ($\ell_2$ of the weights) two-layer ReLU networks for noisy univariate regression. We show that overfitting is tempered for the $L_1$ loss, and any $L_p$ loss for $p&lt;2$, but catastrophic for $p\geq 2$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#25968;&#25454;&#23545;&#20840;&#27874;&#24418;&#21453;&#28436;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#24378;&#35843;&#27169;&#22411;&#23481;&#37327;&#38656;&#35201;&#26681;&#25454;&#25968;&#25454;&#22823;&#23567;&#36827;&#34892;&#25193;&#23637;&#20197;&#21462;&#24471;&#26368;&#20339;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15388</link><description>&lt;p&gt;
&#20840;&#27874;&#24418;&#21453;&#28436;&#26159;&#21542;&#21463;&#30410;&#20110;&#22823;&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Full Waveform Inversion Benefit from Big Data?. (arXiv:2307.15388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#25968;&#25454;&#23545;&#20840;&#27874;&#24418;&#21453;&#28436;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#24378;&#35843;&#27169;&#22411;&#23481;&#37327;&#38656;&#35201;&#26681;&#25454;&#25968;&#25454;&#22823;&#23567;&#36827;&#34892;&#25193;&#23637;&#20197;&#21462;&#24471;&#26368;&#20339;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#25968;&#25454;&#23545;&#20840;&#27874;&#24418;&#21453;&#28436;&#65288;FWI&#65289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#20247;&#25152;&#21608;&#30693;&#65292;&#22823;&#25968;&#25454;&#21487;&#20197;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;FWI&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#39564;&#35777;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;&#22823;&#22411;&#12289;&#22810;&#32467;&#26500;&#25968;&#25454;&#38598;OpenFWI&#19978;&#35757;&#32451;&#30340;FWI&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;OpenFWI&#30340;10&#20010;2D&#23376;&#38598;&#30340;&#32452;&#21512;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;FWI&#27169;&#22411;&#65292;&#36825;&#20123;&#23376;&#38598;&#24635;&#20849;&#21253;&#21547;&#20102;470K&#20010;&#25968;&#25454;&#23545;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#25552;&#39640;FWI&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#27169;&#22411;&#23481;&#37327;&#38656;&#35201;&#26681;&#25454;&#25968;&#25454;&#22823;&#23567;&#36827;&#34892;&#30456;&#24212;&#30340;&#25193;&#23637;&#20197;&#33719;&#24471;&#26368;&#20339;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the impact of big data on deep learning models for full waveform inversion (FWI). While it is well known that big data can boost the performance of deep learning models in many tasks, its effectiveness has not been validated for FWI. To address this gap, we present an empirical study that investigates how deep learning models in FWI behave when trained on OpenFWI, a collection of large-scale, multi-structural datasets published recently. Particularly, we train and evaluate the FWI models on a combination of 10 2D subsets in OpenFWI that contain 470K data pairs in total. Our experiments demonstrate that larger datasets lead to better performance and generalization of deep learning models for FWI. We further demonstrate that model capacity needs to scale in accordance with data size for optimal improvement.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20849;&#21516;&#27880;&#24847;&#21147;&#22270;&#27719;&#32858;&#65288;CAGPool&#65289;&#30340;&#26032;&#39062;&#21644;&#39640;&#25928;&#30340;&#22270;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#22270;&#23545;&#20043;&#38388;&#30340;&#20132;&#20114;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15377</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#25104;&#23545;&#22270;&#20132;&#20114;&#23398;&#20064;&#30340;&#20849;&#21516;&#27880;&#24847;&#21147;&#22270;&#27719;&#32858;
&lt;/p&gt;
&lt;p&gt;
Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning. (arXiv:2307.15377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20849;&#21516;&#27880;&#24847;&#21147;&#22270;&#27719;&#32858;&#65288;CAGPool&#65289;&#30340;&#26032;&#39062;&#21644;&#39640;&#25928;&#30340;&#22270;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#22270;&#23545;&#20043;&#38388;&#30340;&#20132;&#20114;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#21644;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#29702;&#35299;&#21333;&#19968;&#22270;&#36755;&#20837;&#65292;&#32780;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#38656;&#35201;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#25104;&#23545;&#20998;&#26512;&#65288;&#20363;&#22914;&#22330;&#26223;&#22270;&#21305;&#37197;&#12289;&#20195;&#30721;&#25628;&#32034;&#21644;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65289;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#23558;&#37325;&#28857;&#36716;&#21521;&#23398;&#20064;&#22270;&#23545;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#21363;&#20132;&#20114;&#26159;&#22312;&#33410;&#28857;&#32423;&#21035;&#36827;&#34892;&#32771;&#34385;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;&#24615;&#33021;&#20122;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#22270;&#32423;&#26041;&#27861;&#65292;&#21033;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#22312;&#22270;&#27719;&#32858;&#20013;&#25552;&#21462;&#20132;&#20114;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20849;&#21516;&#27880;&#24847;&#21147;&#22270;&#27719;&#32858;&#65288;CAGPool&#65289;&#65292;&#22312;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#26102;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#23637;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have proven to be effective in processing and learning from graph-structured data. However, previous works mainly focused on understanding single graph inputs while many real-world applications require pair-wise analysis for graph-structured data (e.g., scene graph matching, code searching, and drug-drug interaction prediction). To this end, recent works have shifted their focus to learning the interaction between pairs of graphs. Despite their improved performance, these works were still limited in that the interactions were considered at the node-level, resulting in high computational costs and suboptimal performance. To address this issue, we propose a novel and efficient graph-level approach for extracting interaction representations using co-attention in graph pooling. Our method, Co-Attention Graph Pooling (CAGPool), exhibits competitive performance relative to existing methods in both classification and regression tasks using real-world datasets, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#31454;&#20105;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#25506;&#32034;&#20102;&#28608;&#20809;&#32593;&#32476;&#20316;&#20026;&#20809;&#23376;&#21152;&#36895;&#22120;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#28382;&#21518;&#21644;&#38750;&#28382;&#21518;&#21516;&#27493;&#23454;&#29616;&#20102;&#26080;&#20914;&#31361;&#30340;&#21512;&#20316;&#20915;&#31574;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#20302;&#20914;&#31361;&#29575;&#21644;&#39640;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2307.15373</link><description>&lt;p&gt;
&#28608;&#20809;&#32593;&#32476;&#20013;&#22522;&#20110;&#28382;&#21518;&#21644;&#38750;&#28382;&#21518;&#21516;&#27493;&#30340;&#26080;&#20914;&#31361;&#32852;&#21512;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Conflict-free joint decision by lag and zero-lag synchronization in laser network. (arXiv:2307.15373v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#31454;&#20105;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#25506;&#32034;&#20102;&#28608;&#20809;&#32593;&#32476;&#20316;&#20026;&#20809;&#23376;&#21152;&#36895;&#22120;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#28382;&#21518;&#21644;&#38750;&#28382;&#21518;&#21516;&#27493;&#23454;&#29616;&#20102;&#26080;&#20914;&#31361;&#30340;&#21512;&#20316;&#20915;&#31574;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#20302;&#20914;&#31361;&#29575;&#21644;&#39640;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#30340;&#32456;&#32467;&#21644;&#23545;&#35745;&#31639;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#20809;&#23376;&#21152;&#36895;&#22120;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#36825;&#26159;&#30001;&#20110;&#20809;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#22914;&#39640;&#24102;&#23485;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#22312;&#28608;&#20809;&#29289;&#29702;&#39046;&#22495;&#20013;&#20986;&#29616;&#30340;&#21508;&#31181;&#21516;&#27493;&#29616;&#35937;&#12290;&#36825;&#20123;&#22240;&#32032;&#22312;&#35745;&#31639;&#26426;&#24615;&#33021;&#25509;&#36817;&#26497;&#38480;&#26102;&#21457;&#25381;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28608;&#20809;&#32593;&#32476;&#20316;&#20026;&#20809;&#23376;&#21152;&#36895;&#22120;&#24212;&#29992;&#20110;&#31454;&#20105;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#20914;&#31361;&#36991;&#20813;&#26159;&#26368;&#22823;&#21270;&#29615;&#22659;&#22870;&#21169;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#21322;&#23548;&#20307;&#28608;&#20809;&#22120;&#32593;&#32476;&#20013;&#23454;&#39564;&#39564;&#35777;&#20102;&#21033;&#29992;&#28382;&#21518;&#21644;&#38750;&#28382;&#21518;&#21516;&#27493;&#36827;&#34892;&#21512;&#20316;&#20915;&#31574;&#30340;&#25928;&#26524;&#12290;&#28151;&#27788;&#30340;&#28382;&#21518;&#21516;&#27493;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20915;&#31574;&#65292;&#32780;&#38646;&#24310;&#36831;&#30340;&#21516;&#27493;&#36127;&#36131;&#23454;&#29616;&#30896;&#25758;&#36991;&#20813;&#21151;&#33021;&#12290;&#25105;&#20204;&#22312;&#22522;&#26412;&#30340;&#20914;&#31361;&#29575;&#20302;&#12289;&#22870;&#21169;&#39640;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the end of Moore's Law and the increasing demand for computing, photonic accelerators are garnering considerable attention. This is due to the physical characteristics of light, such as high bandwidth and multiplicity, and the various synchronization phenomena that emerge in the realm of laser physics. These factors come into play as computer performance approaches its limits. In this study, we explore the application of a laser network, acting as a photonic accelerator, to the competitive multi-armed bandit problem. In this context, conflict avoidance is key to maximizing environmental rewards. We experimentally demonstrate cooperative decision-making using zero-lag and lag synchronization within a network of four semiconductor lasers. Lag synchronization of chaos realizes effective decision-making and zero-delay synchronization is responsible for the realization of the collision avoidance function. We experimentally verified a low collision rate and high reward in a fundamental 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;MOB-HSMM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#26102;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#33976;&#39311;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#28165;&#26224;&#35299;&#37322;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#23558;LSTM&#23398;&#20064;&#21040;&#30340;&#39034;&#24207;&#27169;&#24335;&#36716;&#31227;&#21040;MOB&#26641;&#20013;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;MOB&#26641;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;MOB-HSMM&#23558;MOB&#26641;&#19982;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HSMM&#65289;&#25972;&#21512;&#65292;&#23454;&#29616;&#20102;&#28508;&#22312;&#21644;&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.15367</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#23454;&#29616;&#36879;&#26126;&#30340;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Toward Transparent Sequence Models with Model-Based Tree Markov Model. (arXiv:2307.15367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;MOB-HSMM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#26102;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#33976;&#39311;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#28165;&#26224;&#35299;&#37322;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#23558;LSTM&#23398;&#20064;&#21040;&#30340;&#39034;&#24207;&#27169;&#24335;&#36716;&#31227;&#21040;MOB&#26641;&#20013;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;MOB&#26641;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;MOB-HSMM&#23558;MOB&#26641;&#19982;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HSMM&#65289;&#25972;&#21512;&#65292;&#23454;&#29616;&#20102;&#28508;&#22312;&#21644;&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24212;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#12289;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;MOB-HSMM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22266;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#26816;&#27979;&#39640;&#27515;&#20129;&#39118;&#38505;&#20107;&#20214;&#65292;&#24182;&#21457;&#29616;&#19982;&#27515;&#20129;&#39118;&#38505;&#30456;&#20851;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#33976;&#39311;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#28165;&#26224;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LSTM&#23398;&#20064;&#39034;&#24207;&#27169;&#24335;&#65292;&#36827;&#32780;&#23558;&#20854;&#36716;&#31227;&#32473;MOB&#26641;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#65288;MOB&#26641;&#65289;&#30340;&#24615;&#33021;&#12290;&#23558;MOB&#26641;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HSMM&#65289;&#38598;&#25104;&#22312;MOB-HSMM&#20013;&#65292;&#21487;&#20197;&#20351;&#29992;&#21487;&#29992;&#20449;&#24687;&#25581;&#31034;&#28508;&#22312;&#30340;&#21644;&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we address the interpretability issue in complex, black-box Machine Learning models applied to sequence data. We introduce the Model-Based tree Hidden Semi-Markov Model (MOB-HSMM), an inherently interpretable model aimed at detecting high mortality risk events and discovering hidden patterns associated with the mortality risk in Intensive Care Units (ICU). This model leverages knowledge distilled from Deep Neural Networks (DNN) to enhance predictive performance while offering clear explanations. Our experimental results indicate the improved performance of Model-Based trees (MOB trees) via employing LSTM for learning sequential patterns, which are then transferred to MOB trees. Integrating MOB trees with the Hidden Semi-Markov Model (HSMM) in the MOB-HSMM enables uncovering potential and explainable sequences using available information.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#20004;&#20004;&#27604;&#36739;&#65292;&#21487;&#20197;&#20135;&#29983;&#25490;&#24207;&#21644;&#21516;&#26102;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#19988;&#21487;&#20197;&#36873;&#25321;&#21069;k&#20010;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.15361</link><description>&lt;p&gt;
&#30830;&#23450;&#24615;&#29305;&#24449;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Confident Feature Ranking. (arXiv:2307.15361v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15361
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#20004;&#20004;&#27604;&#36739;&#65292;&#21487;&#20197;&#20135;&#29983;&#25490;&#24207;&#21644;&#21516;&#26102;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#19988;&#21487;&#20197;&#36873;&#25321;&#21069;k&#20010;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#35299;&#37322;&#36890;&#24120;&#20381;&#36182;&#20110;&#29305;&#24449;&#30340;&#30456;&#23545;&#39034;&#24207;&#32780;&#19981;&#26159;&#25968;&#20540;&#26412;&#36523;&#65292;&#20063;&#23601;&#26159;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#37325;&#35201;&#24615;&#20540;&#26102;&#20351;&#29992;&#30340;&#26679;&#26412;&#37327;&#36739;&#23567;&#65292;&#25490;&#24207;&#21487;&#33021;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#37325;&#35201;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#31181;&#25490;&#24207;&#21644;&#21516;&#26102;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#22522;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#20004;&#20004;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20445;&#35777;&#39640;&#27010;&#29575;&#21253;&#21547;&#8220;&#30495;&#23454;&#8221;&#65288;&#26080;&#38480;&#26679;&#26412;&#65289;&#25490;&#24207;&#65292;&#24182;&#20801;&#35768;&#36873;&#25321;&#21069;k&#20010;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretation of feature importance values often relies on the relative order of the features rather than on the value itself, referred to as ranking. However, the order may be unstable due to the small sample sizes used in calculating the importance values. We propose that post-hoc importance methods produce a ranking and simultaneous confident intervals for the rankings. Based on pairwise comparisons of the feature importance values, our method is guaranteed to include the ``true'' (infinite sample) ranking with high probability and allows for selecting top-k sets.
&lt;/p&gt;</description></item><item><title>Med-HALT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#65292;&#24182;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.15343</link><description>&lt;p&gt;
Med-HALT:&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#24187;&#35273;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15343
&lt;/p&gt;
&lt;p&gt;
Med-HALT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#65292;&#24182;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#20851;&#27880;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#32972;&#26223;&#19979;&#12290;&#24187;&#35273;&#25351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#20102;&#21512;&#29702;&#20294;&#26410;&#32463;&#39564;&#35777;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23545;&#21307;&#30103;&#24212;&#29992;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;Med-HALT&#65288;&#21307;&#30103;&#39046;&#22495;&#24187;&#35273;&#27979;&#35797;&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24187;&#35273;&#12290;Med-HALT&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#20803;&#21270;&#30340;&#36328;&#22269;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#21307;&#30103;&#26816;&#26597;&#65292;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#12290;Med-HALT&#21253;&#25324;&#20004;&#31867;&#27979;&#35797;&#65306;&#25512;&#29702;&#21644;&#22522;&#20110;&#35760;&#24518;&#30340;&#24187;&#35273;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#20449;&#24687;&#26816;&#32034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#25991;&#26412;Davinci&#65292;GPT-3.5&#65292;LlaMa-2&#65292;MPT&#21644;Falcon&#31561;&#39046;&#20808;&#30340;LLMs&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#35265;&#35299;&#65292;&#20419;&#36827;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Radon&#21464;&#25442;&#21644;&#31526;&#21495;&#32047;&#31215;&#20998;&#24067;&#21464;&#25442;&#30340;&#22270;&#20687;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#34920;&#31034;&#26377;&#31526;&#21495;&#22270;&#20687;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15339</link><description>&lt;p&gt;
Radon&#31526;&#21495;&#32047;&#31215;&#20998;&#24067;&#21464;&#25442;&#21450;&#20854;&#22312;&#26377;&#31526;&#21495;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Radon Signed Cumulative Distribution Transform and its applications in classification of Signed Images. (arXiv:2307.15339v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15339
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Radon&#21464;&#25442;&#21644;&#31526;&#21495;&#32047;&#31215;&#20998;&#24067;&#21464;&#25442;&#30340;&#22270;&#20687;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#34920;&#31034;&#26377;&#31526;&#21495;&#22270;&#20687;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#36755;&#21644;&#26368;&#20248;&#36816;&#36755;&#25968;&#23398;&#30340;&#26032;&#22270;&#20687;&#34920;&#31034;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#22270;&#20687;&#30340;&#33879;&#21517;Radon&#21464;&#25442;&#21644;&#26368;&#36817;&#30340;&#20449;&#21495;&#34920;&#31034;&#26041;&#27861;&#8212;&#8212;&#31526;&#21495;&#32047;&#31215;&#20998;&#24067;&#21464;&#25442;&#30340;&#32452;&#21512;&#12290;&#26032;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#20197;&#21069;&#19982;&#36816;&#36755;&#30456;&#20851;&#30340;&#22270;&#20687;&#34920;&#31034;&#26041;&#27861;&#25512;&#24191;&#21040;&#20219;&#24847;&#20989;&#25968;&#65288;&#22270;&#20687;&#65289;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#26356;&#22810;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#36825;&#31181;&#26032;&#30340;&#21464;&#25442;&#65292;&#20197;&#21450;&#23427;&#30340;&#19968;&#20123;&#25968;&#23398;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#23454;&#38469;&#21644;&#27169;&#25311;&#25968;&#25454;&#19978;&#21010;&#20998;&#22270;&#20687;&#31867;&#21035;&#30340;&#33021;&#21147;&#12290;&#19982;&#29616;&#26377;&#30340;&#36816;&#36755;&#21464;&#25442;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26032;&#30340;&#21464;&#25442;&#26356;&#20934;&#30830;&#22320;&#34920;&#31034;&#20102;&#26377;&#31526;&#21495;&#22270;&#20687;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#22240;&#27492;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;Python&#35821;&#35328;&#20013;&#30340;&#23454;&#29616;&#24050;&#20316;&#20026;PyTransKit&#36719;&#20214;&#21253;&#30340;&#19968;&#37096;&#20998;&#38598;&#25104;&#22312;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
Here we describe a new image representation technique based on the mathematics of transport and optimal transport. The method relies on the combination of the well-known Radon transform for images and a recent signal representation method called the Signed Cumulative Distribution Transform. The newly proposed method generalizes previous transport-related image representation methods to arbitrary functions (images), and thus can be used in more applications. We describe the new transform, and some of its mathematical properties and demonstrate its ability to partition image classes with real and simulated data. In comparison to existing transport transform methods, as well as deep learning-based classification methods, the new transform more accurately represents the information content of signed images, and thus can be used to obtain higher classification accuracies. The implementation of the proposed method in Python language is integrated as a part of the software package PyTransKit,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#26816;&#32034;&#36741;&#21161;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#26469;&#20026;&#26410;&#24067;&#32622;&#30340;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#22270;&#20687;&#29983;&#25104;&#21560;&#24341;&#20154;&#21644;&#36924;&#30495;&#30340;&#33310;&#21488;&#32972;&#26223;&#65292;&#20197;&#25552;&#39640;&#22312;&#32447;&#24191;&#21578;&#30340;&#28857;&#20987;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.15326</link><description>&lt;p&gt;
&#20351;&#29992;&#26816;&#32034;&#36741;&#21161;&#22270;&#20687;&#29983;&#25104;&#20026;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#36827;&#34892;&#22312;&#32447;&#24191;&#21578;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation. (arXiv:2307.15326v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15326
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#26816;&#32034;&#36741;&#21161;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#26469;&#20026;&#26410;&#24067;&#32622;&#30340;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#22270;&#20687;&#29983;&#25104;&#21560;&#24341;&#20154;&#21644;&#36924;&#30495;&#30340;&#33310;&#21488;&#32972;&#26223;&#65292;&#20197;&#25552;&#39640;&#22312;&#32447;&#24191;&#21578;&#30340;&#28857;&#20987;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24191;&#21578;&#36890;&#24120;&#20381;&#36182;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#36890;&#36807;&#30446;&#24405;&#23558;&#20135;&#21697;&#30340;&#22270;&#20687;&#21457;&#36865;&#32473;&#24191;&#21578;&#24179;&#21488;&#12290;&#22312;&#24191;&#21578;&#34892;&#19994;&#20013;&#65292;&#36825;&#26679;&#30340;&#24191;&#21578;&#36890;&#24120;&#34987;&#31216;&#20026;&#21160;&#24577;&#20135;&#21697;&#24191;&#21578;(DPA)&#12290; DPA&#30446;&#24405;&#36890;&#24120;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#20135;&#21697;&#22270;&#20687;&#65288;&#19982;&#21487;&#20197;&#20174;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#36141;&#20080;&#30340;&#20135;&#21697;&#25968;&#37327;&#30456;&#23545;&#24212;&#65289;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#30446;&#24405;&#20013;&#30340;&#25152;&#26377;&#20135;&#21697;&#22270;&#20687;&#22312;&#30452;&#25509;&#37325;&#26032;&#23450;&#20301;&#20026;&#24191;&#21578;&#22270;&#20687;&#26102;&#37117;&#20250;&#21560;&#24341;&#20154;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#36739;&#20302;&#30340;&#28857;&#20987;&#29575;(CTR)&#12290;&#29305;&#21035;&#22320;&#65292;&#21482;&#25918;&#32622;&#22312;&#32431;&#33394;&#32972;&#26223;&#19978;&#30340;&#20135;&#21697;&#21487;&#33021;&#19981;&#22914;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#24067;&#32622;&#30340;&#20135;&#21697;&#21560;&#24341;&#20154;&#21644;&#36924;&#30495;&#12290;&#20026;&#20102;&#35299;&#20915;DPA&#22270;&#20687;&#22312;&#22823;&#35268;&#27169;&#19978;&#30340;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#26469;&#20026;&#26410;&#24067;&#32622;&#20135;&#21697;&#22270;&#20687;&#29983;&#25104;&#33310;&#21488;&#32972;&#26223;&#12290;&#29983;&#25104;&#25972;&#20010;&#24067;&#32622;&#30340;&#32972;&#26223;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#38544;&#21547;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online ads showing e-commerce products typically rely on the product images in a catalog sent to the advertising platform by an e-commerce platform. In the broader ads industry such ads are called dynamic product ads (DPA). It is common for DPA catalogs to be in the scale of millions (corresponding to the scale of products which can be bought from the e-commerce platform). However, not all product images in the catalog may be appealing when directly re-purposed as an ad image, and this may lead to lower click-through rates (CTRs). In particular, products just placed against a solid background may not be as enticing and realistic as a product staged in a natural environment. To address such shortcomings of DPA images at scale, we propose a generative adversarial network (GAN) based approach to generate staged backgrounds for un-staged product images. Generating the entire staged background is a challenging task susceptible to hallucinations. To get around this, we introduce a simpler ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#20013;&#37096;&#20998;&#35266;&#27979;&#25110;&#31895;&#31890;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#20856;&#30340;EDMD&#31639;&#27861;&#19981;&#33021;&#20934;&#30830;&#25552;&#20379;Koopman&#31639;&#23376;&#36817;&#20284;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#31995;&#32479;&#21160;&#24577;&#30340;&#23545;&#31216;&#24615;&#36716;&#31227;&#21040;Koopman&#31639;&#23376;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.15325</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#35266;&#27979;&#37096;&#20998;&#12289;&#31895;&#31890;&#21270;&#21644;&#31561;&#21464;&#24615;&#22312;Koopman&#31639;&#23376;&#29702;&#35770;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Partial observations, coarse graining and equivariance in Koopman operator theory for large-scale dynamical systems. (arXiv:2307.15325v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#20013;&#37096;&#20998;&#35266;&#27979;&#25110;&#31895;&#31890;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#20856;&#30340;EDMD&#31639;&#27861;&#19981;&#33021;&#20934;&#30830;&#25552;&#20379;Koopman&#31639;&#23376;&#36817;&#20284;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#31995;&#32479;&#21160;&#24577;&#30340;&#23545;&#31216;&#24615;&#36716;&#31227;&#21040;Koopman&#31639;&#23376;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#24050;&#32463;&#25104;&#20026;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#12289;&#39044;&#27979;&#21644;&#25511;&#21046;&#22797;&#26434;&#31995;&#32479;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#20174;&#27979;&#37327;&#20013;&#35782;&#21035;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#32447;&#24615;&#20989;&#25968;&#31354;&#38388;&#34920;&#31034;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#31995;&#32479;&#65292;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#37096;&#20998;&#35266;&#27979;&#65288;&#22914;&#23454;&#39564;&#25968;&#25454;&#20013;&#38750;&#24120;&#24120;&#35265;&#30340;&#27979;&#37327;&#65289;&#25110;&#32773;&#20986;&#20110;&#25928;&#29575;&#21407;&#22240;&#21051;&#24847;&#36827;&#34892;&#31895;&#31890;&#21270;&#30340;&#24773;&#20917;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#31181;&#24773;&#20917;&#20013;&#30340;&#22256;&#25200;&#65292;&#21363;&#22914;&#26524;&#25105;&#20204;&#19981;&#20180;&#32454;&#36873;&#25321;&#21487;&#35266;&#27979;&#25968;&#37327;&#65292;&#32463;&#20856;&#30340;EDMD&#31639;&#27861;&#19981;&#33021;&#33258;&#21160;&#25552;&#20379;&#28508;&#22312;&#31995;&#32479;&#30340;Koopman&#31639;&#23376;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31995;&#32479;&#21160;&#24577;&#20013;&#30340;&#23545;&#31216;&#24615;&#21487;&#20197;&#36716;&#31227;&#21040;Koopman&#31639;&#23376;&#20013;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#31616;&#35201;&#35752;&#35770;&#20102;&#19982;&#22495;&#20998;&#35299;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Koopman operator has become an essential tool for data-driven analysis, prediction and control of complex systems, the main reason being the enormous potential of identifying linear function space representations of nonlinear dynamics from measurements. Until now, the situation where for large-scale systems, we (i) only have access to partial observations (i.e., measurements, as is very common for experimental data) or (ii) deliberately perform coarse graining (for efficiency reasons) has not been treated to its full extent. In this paper, we address the pitfall associated with this situation, that the classical EDMD algorithm does not automatically provide a Koopman operator approximation for the underlying system if we do not carefully select the number of observables. Moreover, we show that symmetries in the system dynamics can be carried over to the Koopman operator, which allows us to massively increase the model efficiency. We also briefly draw a connection to domain decompos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#27169;&#25311;&#19982;&#23454;&#38469;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#36328;&#30028;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#36890;&#36807;&#31163;&#32447;&#20195;&#29702;&#20219;&#21153;&#21644;&#27169;&#25311;&#35757;&#32451;&#65292;&#25104;&#21151;&#22320;&#20248;&#21270;&#20102;&#22495;&#38543;&#26426;&#21270;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.15320</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#35270;&#35273;&#27169;&#25311;&#19982;&#23454;&#38469;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#36328;&#30028;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Robust Visual Sim-to-Real Transfer for Robotic Manipulation. (arXiv:2307.15320v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15320
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#27169;&#25311;&#19982;&#23454;&#38469;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#36328;&#30028;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#36890;&#36807;&#31163;&#32447;&#20195;&#29702;&#20219;&#21153;&#21644;&#27169;&#25311;&#35757;&#32451;&#65292;&#25104;&#21151;&#22320;&#20248;&#21270;&#20102;&#22495;&#38543;&#26426;&#21270;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23398;&#20064;&#35270;&#35273;&#21160;&#20316;&#31574;&#30053;&#27604;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#26356;&#23433;&#20840;&#19988;&#26356;&#20415;&#23452;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32463;&#36807;&#27169;&#25311;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#36716;&#31227;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#26102;&#32463;&#24120;&#22833;&#36133;&#12290;&#20026;&#20102;&#24357;&#21512;&#35270;&#35273;&#27169;&#25311;&#19982;&#23454;&#38469;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#22495;&#38543;&#26426;&#21270;(domain randomization, DR)&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#20027;&#35201;&#35780;&#20272;&#20102;DR&#22312;&#23039;&#24577;&#20272;&#35745;&#21644;&#29289;&#20307;&#26816;&#27979;&#31561;&#33073;&#31163;&#21147;&#30340;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35270;&#35273;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#20195;&#29702;&#20219;&#21153;&#65292;&#29992;&#20110;&#36873;&#25321;&#32441;&#29702;&#38543;&#26426;&#21270;&#12289;&#20809;&#29031;&#38543;&#26426;&#21270;&#12289;&#29289;&#20307;&#39068;&#33394;&#21464;&#21270;&#21644;&#30456;&#26426;&#21442;&#25968;&#31561;DR&#21442;&#25968;&#12290;&#29305;&#21035;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DR&#21442;&#25968;&#23545;&#20110;&#31163;&#32447;&#20195;&#29702;&#20219;&#21153;&#21644;&#22312;&#32447;&#31574;&#30053;&#20855;&#26377;&#31867;&#20284;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#31163;&#32447;&#20248;&#21270;&#30340;DR&#21442;&#25968;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#35270;&#35273;&#21160;&#20316;&#31574;&#30053;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#31574;&#30053;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning visuomotor policies in simulation is much safer and cheaper than in the real world. However, due to discrepancies between the simulated and real data, simulator-trained policies often fail when transferred to real robots. One common approach to bridge the visual sim-to-real domain gap is domain randomization (DR). While previous work mainly evaluates DR for disembodied tasks, such as pose estimation and object detection, here we systematically explore visual domain randomization methods and benchmark them on a rich set of challenging robotic manipulation tasks. In particular, we propose an off-line proxy task of cube localization to select DR parameters for texture randomization, lighting randomization, variations of object colors and camera parameters. Notably, we demonstrate that DR parameters have similar impact on our off-line proxy task and on-line policies. We, hence, use off-line optimized DR parameters to train visuomotor policies in simulation and directly apply such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29305;&#24449;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#27604;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#26356;&#21487;&#38752;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#29992;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#26367;&#25442;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#33021;&#22815;&#25552;&#39640;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15317</link><description>&lt;p&gt;
DiffKendall:&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall's Rank Correlation. (arXiv:2307.15317v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29305;&#24449;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#27604;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#26356;&#21487;&#38752;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#29992;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#26367;&#25442;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#33021;&#22815;&#25552;&#39640;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#23558;&#22312;&#22522;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#27169;&#22411;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#26032;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#26032;&#31867;&#21035;&#19978;&#36890;&#36947;&#19978;&#29305;&#24449;&#20540;&#30340;&#20998;&#24067;&#30456;&#23545;&#22343;&#21248;&#65292;&#38590;&#20197;&#30830;&#23450;&#26032;&#20219;&#21153;&#20013;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#12290;&#26631;&#20934;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#22914;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#36127;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#65292;&#26469;&#34913;&#37327;&#20004;&#20010;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#39640;&#20960;&#20309;&#30456;&#20284;&#24230;&#30340;&#29305;&#24449;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#20041;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#29305;&#24449;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#27604;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#29992;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#26367;&#25442;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#33021;&#22815;&#25552;&#39640;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning aims to adapt models trained on the base dataset to novel tasks where the categories are not seen by the model before. This often leads to a relatively uniform distribution of feature values across channels on novel classes, posing challenges in determining channel importance for novel tasks. Standard few-shot learning methods employ geometric similarity metrics such as cosine similarity and negative Euclidean distance to gauge the semantic relatedness between two features. However, features with high geometric similarities may carry distinct semantics, especially in the context of few-shot learning. In this paper, we demonstrate that the importance ranking of feature channels is a more reliable indicator for few-shot learning than geometric similarity metrics. We observe that replacing the geometric similarity metric with Kendall's rank correlation only during inference is able to improve the performance of few-shot learning across a wide range of datasets with diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#36873;&#25321;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#36127;&#33655;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15299</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#36127;&#33655;&#39044;&#27979;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting. (arXiv:2307.15299v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#36873;&#25321;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#36127;&#33655;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#36127;&#33655;&#39044;&#27979;&#22312;&#20247;&#22810;&#39046;&#22495;&#37117;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20934;&#30830;&#25429;&#25417;&#21160;&#21147;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#24577;&#20173;&#28982;&#26159;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;ARIMA&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;ANN&#65292;LSTM&#65292;GRU&#31561;&#65289;&#32463;&#24120;&#34987;&#20351;&#29992;&#65292;&#24182;&#19988;&#36890;&#24120;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#25104;&#21151;&#29575;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#26368;&#36817;&#24320;&#21457;&#30340;Transformer-based&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;Transformer&#27169;&#22411;&#26377;&#26395;&#25913;&#36827;&#36127;&#33655;&#39044;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20854;Attention&#26426;&#21046;&#23398;&#20064;&#21040;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36816;&#29992;&#20102;&#20960;&#31181;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22914;&#24046;&#20998;&#36827;&#21270;&#65292;&#20197;&#23547;&#25214;Transformer-based&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#65292;&#20197;&#20135;&#29983;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#24046;&#20998;&#36827;&#21270;&#20026;&#38750;&#21487;&#24494;&#20998;&#12289;&#22810;&#30446;&#26631;&#25110;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#12289;&#24378;&#20581;&#21644;&#20840;&#23616;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#20854;&#20182;&#27169;&#22411;&#22312;&#36127;&#33655;&#39044;&#27979;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in Load forecasting. Transformer models have the potential to improve Load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer based Ne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21463;&#32422;&#26463;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#38750;&#32447;&#24615;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#38477;&#38454;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#36827;&#34892;&#36924;&#36817;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#27969;&#24418;&#38468;&#36817;&#23545;&#30636;&#24577;&#21160;&#21147;&#23398;&#24314;&#27169;&#26102;&#25152;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15288</link><description>&lt;p&gt;
&#20351;&#29992;&#21463;&#32422;&#26463;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#38750;&#32447;&#24615;&#25237;&#24433;&#65292;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#38477;&#38454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning Nonlinear Projections for Reduced-Order Modeling of Dynamical Systems using Constrained Autoencoders. (arXiv:2307.15288v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15288
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21463;&#32422;&#26463;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#38750;&#32447;&#24615;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#38477;&#38454;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#36827;&#34892;&#36924;&#36817;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#27969;&#24418;&#38468;&#36817;&#23545;&#30636;&#24577;&#21160;&#21147;&#23398;&#24314;&#27169;&#26102;&#25152;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;&#38477;&#38454;&#24314;&#27169;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#20302;&#32500;&#27969;&#24418;&#23545;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#36924;&#36817;&#12290;&#36825;&#26159;&#19968;&#31181;&#22312;&#36807;&#28193;&#21518;&#26465;&#20214;&#20013;&#23545;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20854;&#20013;&#21021;&#22987;&#26465;&#20214;&#21644;&#20854;&#20182;&#24178;&#25200;&#30340;&#25928;&#24212;&#24050;&#32463;&#34928;&#20943;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38656;&#35201;&#23454;&#26102;&#25511;&#21046;&#21644;&#39044;&#27979;&#24212;&#29992;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#30636;&#24577;&#21160;&#21147;&#23398;&#24314;&#27169;&#26469;&#35828;&#65292;&#24555;&#36895;&#21160;&#24577;&#21644;&#38750;&#27491;&#24120;&#25935;&#24863;&#26426;&#21046;&#30340;&#24433;&#21709;&#20351;&#24471;&#38382;&#39064;&#21464;&#24471;&#22797;&#26434;&#12290;&#20026;&#20102;&#24320;&#22987;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30001;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#30340;&#19968;&#31181;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#25237;&#24433;&#31867;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#20351;&#29992;&#21487;&#36870;&#28608;&#27963;&#20989;&#25968;&#21644;&#23545;&#20598;&#26435;&#37325;&#30697;&#38453;&#65292;&#20197;&#30830;&#20445;&#32534;&#30721;&#22120;&#26159;&#35299;&#30721;&#22120;&#30340;&#24038;&#36870;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#20855;&#26377;&#21160;&#24577;&#24863;&#30693;&#24615;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#20419;&#36827;&#23398;&#20064;&#32771;&#34385;&#26012;&#25237;&#24433;&#32420;&#32500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed reduced-order modeling techniques aim to approximate nonlinear dynamical systems on low-dimensional manifolds learned from data. This is an effective approach for modeling dynamics in a post-transient regime where the effects of initial conditions and other disturbances have decayed. However, modeling transient dynamics near an underlying manifold, as needed for real-time control and forecasting applications, is complicated by the effects of fast dynamics and nonnormal sensitivity mechanisms. To begin to address these issues, we introduce a parametric class of nonlinear projections described by constrained autoencoder neural networks in which both the manifold and the projection fibers are learned from data. Our architecture uses invertible activation functions and biorthogonal weight matrices to ensure that the encoder is a left inverse of the decoder. We also introduce new dynamics-aware cost functions that promote learning of oblique projection fibers that account
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;Zonoid&#30340;&#26368;&#20248;&#36924;&#36817;&#21644;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#36924;&#36817;&#20004;&#20010;&#38382;&#39064;&#12290;&#23545;&#20110;Zonoid&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#22312;$d=2,3$&#26102;&#30340;&#23545;&#25968;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;$k \geq 1$&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#21069;&#30340;&#36924;&#36817;&#29575;&#65292;&#24182;&#33021;&#22815;&#22343;&#21248;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.15285</link><description>&lt;p&gt;
Zonoid&#30340;&#26368;&#20248;&#36924;&#36817;&#21644;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks. (arXiv:2307.15285v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;Zonoid&#30340;&#26368;&#20248;&#36924;&#36817;&#21644;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#36924;&#36817;&#20004;&#20010;&#38382;&#39064;&#12290;&#23545;&#20110;Zonoid&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#22312;$d=2,3$&#26102;&#30340;&#23545;&#25968;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;$k \geq 1$&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#21069;&#30340;&#36924;&#36817;&#29575;&#65292;&#24182;&#33021;&#22815;&#22343;&#21248;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#20004;&#20010;&#30456;&#20851;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#30830;&#23450;&#19968;&#20010;&#20219;&#24847;&#30340;&#22312;$\mathbb{R}^{d+1}$&#31354;&#38388;&#20013;&#30340;Zonoid&#21487;&#20197;&#36890;&#36807;$n$&#20010;&#32447;&#27573;&#30340;Hausdorff&#36317;&#31163;&#26469;&#36924;&#36817;&#30340;&#35823;&#24046;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#30830;&#23450;&#27973;&#23618;ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#21464;&#20998;&#31354;&#38388;&#20013;&#30340;&#22343;&#21248;&#33539;&#25968;&#30340;&#26368;&#20248;&#36924;&#36817;&#29575;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#24050;&#32463;&#22312;$d \neq 2, 3$&#26102;&#24471;&#21040;&#35299;&#20915;&#65292;&#20294;&#24403;$d = 2, 3$&#26102;&#65292;&#26368;&#20248;&#19978;&#30028;&#21644;&#26368;&#20248;&#19979;&#30028;&#20043;&#38388;&#20173;&#23384;&#22312;&#19968;&#20010;&#23545;&#25968;&#24046;&#36317;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#24046;&#36317;&#65292;&#23436;&#25104;&#20102;&#25152;&#26377;&#32500;&#24230;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;$k \geq 1$&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#29616;&#26377;&#30340;&#36924;&#36817;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#22343;&#21248;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the following two related problems. The first is to determine to what error an arbitrary zonoid in $\mathbb{R}^{d+1}$ can be approximated in the Hausdorff distance by a sum of $n$ line segments. The second is to determine optimal approximation rates in the uniform norm for shallow ReLU$^k$ neural networks on their variation spaces. The first of these problems has been solved for $d\neq 2,3$, but when $d=2,3$ a logarithmic gap between the best upper and lower bounds remains. We close this gap, which completes the solution in all dimensions. For the second problem, our techniques significantly improve upon existing approximation rates when $k\geq 1$, and enable uniform approximation of both the target function and its derivatives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#20943;&#23569;&#25968;&#37327;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#20687;&#20013;&#24674;&#22797;&#39640;&#36136;&#37327;&#30340;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#65288;FOD&#65289;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;fixel&#20998;&#31867;&#24809;&#32602;&#26469;&#25552;&#39640;&#19979;&#28216;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15273</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20174;&#20943;&#23569;&#25968;&#37327;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#20687;&#20013;&#24674;&#22797;&#39640;&#36136;&#37327;&#30340;FOD
&lt;/p&gt;
&lt;p&gt;
Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture. (arXiv:2307.15273v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#20943;&#23569;&#25968;&#37327;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#20687;&#20013;&#24674;&#22797;&#39640;&#36136;&#37327;&#30340;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#65288;FOD&#65289;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;fixel&#20998;&#31867;&#24809;&#32602;&#26469;&#25552;&#39640;&#19979;&#28216;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#65288;FOD&#65289;&#36827;&#34892;&#37325;&#24314;&#26377;&#21487;&#33021;&#20174;&#20943;&#23569;&#25968;&#37327;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#20687;&#65288;DWI&#65289;&#20013;&#20135;&#29983;&#20934;&#30830;&#30340;FOD&#65292;&#20174;&#32780;&#20943;&#23569;&#24635;&#25104;&#20687;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29699;&#24418;&#21435;&#21367;&#31215;&#32593;&#32476;&#65292;&#21363;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;FOD&#37325;&#24314;&#26550;&#26500;&#65292;&#30830;&#20445;&#32593;&#32476;&#20135;&#29983;&#30340;&#20013;&#38388;&#21644;&#36755;&#20986;&#30340;FOD&#19982;&#36755;&#20837;&#30340;DWI&#20449;&#21495;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#23454;&#26045;&#20102;&#19968;&#39033;fixel&#20998;&#31867;&#24809;&#32602;&#65292;&#40723;&#21169;&#32593;&#32476;&#20135;&#29983;&#33021;&#22815;&#36827;&#19968;&#27493;&#20998;&#21106;&#20026;&#27491;&#30830;&#25968;&#37327;&#30340;fixel&#24182;&#25913;&#21892;&#19979;&#28216;fixel-based&#20998;&#26512;&#30340;FOD&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Fibre orientation distribution (FOD) reconstruction using deep learning has the potential to produce accurate FODs from a reduced number of diffusion-weighted images (DWIs), decreasing total imaging time. Diffusion acquisition invariant representations of the DWI signals are typically used as input to these methods to ensure that they can be applied flexibly to data with different b-vectors and b-values; however, this means the network cannot condition its output directly on the DWI signal. In this work, we propose a spherical deconvolution network, a model-driven deep learning FOD reconstruction architecture, that ensures intermediate and output FODs produced by the network are consistent with the input DWI signals. Furthermore, we implement a fixel classification penalty within our loss function, encouraging the network to produce FODs that can subsequently be segmented into the correct number of fixels and improve downstream fixel-based analysis. Our results show that the model-base
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#35266;&#27979;&#20540;&#30340;&#39044;&#26399;&#27531;&#24046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#31243;&#24207;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24378;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15247</link><description>&lt;p&gt;
&#36825;&#20010;&#27169;&#22411;&#23545;&#27599;&#20010;&#20154;&#37117;&#21487;&#38752;&#21527;&#65311;&#27979;&#35797;&#24378;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Is this model reliable for everyone? Testing for strong calibration. (arXiv:2307.15247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#35266;&#27979;&#20540;&#30340;&#39044;&#26399;&#27531;&#24046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#31243;&#24207;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24378;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#26657;&#20934;&#33391;&#22909;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#23376;&#32676;&#20307;&#65292;&#24179;&#22343;&#39044;&#27979;&#27010;&#29575;&#19982;&#30495;&#23454;&#20107;&#20214;&#29575;&#25509;&#36817;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#36866;&#29992;&#20110;&#24322;&#36136;&#20154;&#32676;&#65292;&#24182;&#28385;&#36275;&#24378;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24378;&#26657;&#20934;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#23457;&#26680;&#26159;&#19968;&#20010;&#24050;&#30693;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#65292;&#30001;&#20110;&#28508;&#22312;&#30340;&#23376;&#32676;&#20307;&#25968;&#37327;&#24222;&#22823;&#12290;&#22240;&#27492;&#65292;&#24120;&#35265;&#20570;&#27861;&#26159;&#21482;&#26681;&#25454;&#23569;&#25968;&#39044;&#23450;&#20041;&#30340;&#23376;&#32676;&#20307;&#35780;&#20272;&#26657;&#20934;&#12290;&#26368;&#36817;&#22312;&#25311;&#21512;&#24230;&#26816;&#39564;&#26041;&#38754;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;&#20110;&#20449;&#21495;&#36739;&#24369;&#25110;&#26657;&#20934;&#19981;&#33391;&#30340;&#23376;&#32676;&#20307;&#36739;&#23567;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#36807;&#24230;&#32454;&#20998;&#25968;&#25454;&#65292;&#35201;&#20040;&#26681;&#26412;&#19981;&#36827;&#34892;&#32454;&#20998;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#36807;&#31243;&#65292;&#22522;&#20110;&#20197;&#19979;&#27934;&#23519;&#65306;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#25353;&#39044;&#26399;&#30340;&#27531;&#24046;&#23545;&#35266;&#27979;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#39044;&#27979;&#20540;&#21644;&#35266;&#23519;&#20540;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#24212;&#35813;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a well-calibrated risk prediction model, the average predicted probability is close to the true event rate for any given subgroup. Such models are reliable across heterogeneous populations and satisfy strong notions of algorithmic fairness. However, the task of auditing a model for strong calibration is well-known to be difficult -- particularly for machine learning (ML) algorithms -- due to the sheer number of potential subgroups. As such, common practice is to only assess calibration with respect to a few predefined subgroups. Recent developments in goodness-of-fit testing offer potential solutions but are not designed for settings with weak signal or where the poorly calibrated subgroup is small, as they either overly subdivide the data or fail to divide the data at all. We introduce a new testing procedure based on the following insight: if we can reorder observations by their expected residuals, there should be a change in the association between the predicted and observed resi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#22312;&#32479;&#35745;&#24322;&#36136;&#23454;&#39564;&#35774;&#35745;&#19979;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#20855;&#26377;&#33391;&#22909;&#28608;&#21169;&#26426;&#21046;&#30340;FL&#23454;&#39564;&#35774;&#32622;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.15245</link><description>&lt;p&gt;
&#12298;&#22312;&#32479;&#35745;&#24322;&#36136;&#23454;&#39564;&#35774;&#35745;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#23454;&#29992;&#37197;&#26041;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design. (arXiv:2307.15245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#22312;&#32479;&#35745;&#24322;&#36136;&#23454;&#39564;&#35774;&#35745;&#19979;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#20855;&#26377;&#33391;&#22909;&#28608;&#21169;&#26426;&#21046;&#30340;FL&#23454;&#39564;&#35774;&#32622;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#19968;&#30452;&#26159;&#30740;&#31350;&#30340;&#28909;&#28857;&#39046;&#22495;&#12290;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#22312;&#22788;&#29702;&#25968;&#25454;&#24322;&#36136;&#24615;&#26102;&#23545;FL&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#35770;&#25991;&#65292;&#20294;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#29366;&#20917;&#26159;&#26410;&#30693;&#30340;&#12290;&#35768;&#22810;&#30740;&#31350;&#37319;&#29992;&#20102;&#19981;&#19968;&#33268;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#19988;&#27809;&#26377;&#23545;FL&#29305;&#23450;&#23454;&#39564;&#21464;&#37327;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#20063;&#27809;&#26377;&#25552;&#20379;&#19968;&#20010;&#26356;&#21487;&#27604;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;FL&#23454;&#39564;&#35774;&#32622;&#30340;&#23454;&#36341;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#23384;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#28151;&#26434;&#21464;&#37327;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#19981;&#19968;&#33268;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;FL&#29305;&#23450;&#23454;&#39564;&#21464;&#37327;&#19982;&#24444;&#27492;&#20043;&#38388;&#21644;&#24615;&#33021;&#32467;&#26524;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#21644;&#24314;&#35758;&#65292;&#20026;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#20855;&#26377;&#33391;&#22909;&#28608;&#21169;&#26426;&#21046;&#30340;FL&#23454;&#39564;&#35774;&#32622;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#21457;&#24067;FedZoo-Bench&#26469;&#24110;&#21161;&#31038;&#21306;&#65292;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has been an area of active research in recent years. There have been numerous studies in FL to make it more successful in the presence of data heterogeneity. However, despite the existence of many publications, the state of progress in the field is unknown. Many of the works use inconsistent experimental settings and there are no comprehensive studies on the effect of FL-specific experimental variables on the results and practical insights for a more comparable and consistent FL experimental setup. Furthermore, the existence of several benchmarks and confounding variables has further complicated the issue of inconsistency and ambiguity. In this work, we present the first comprehensive study on the effect of FL-specific experimental variables in relation to each other and performance results, bringing several insights and recommendations for designing a meaningful and well-incentivized FL experimental setup. We further aid the community by releasing FedZoo-Bench,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#24378;&#31038;&#20250;&#30417;&#30563;&#30340;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.15217</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#24378;&#31038;&#20250;&#30417;&#30563;&#30340;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#30340;&#25216;&#26415;&#12290;RLHF&#24050;&#25104;&#20026;&#24494;&#35843;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26680;&#24515;&#26041;&#27861;&#12290;&#23613;&#31649;&#22914;&#27492;&#21463;&#27426;&#36814;&#65292;&#20294;&#31995;&#32479;&#24615;&#22320;&#31995;&#32479;&#21270;&#20854;&#32570;&#38519;&#30340;&#20844;&#24320;&#24037;&#20316;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#65288;1&#65289;&#35843;&#26597;&#20102;RLHF&#21450;&#30456;&#20851;&#26041;&#27861;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65307;&#65288;2&#65289;&#27010;&#36848;&#20102;&#20102;&#35299;&#12289;&#25913;&#36827;&#21644;&#34917;&#20805;RLHF&#30340;&#23454;&#36341;&#25216;&#26415;&#65307;&#20197;&#21450;&#65288;3&#65289;&#25552;&#20986;&#20102;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#20197;&#25913;&#36827;RLHF&#31995;&#32479;&#30340;&#31038;&#20250;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;RLHF&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#20197;&#22810;&#26041;&#38754;&#26041;&#27861;&#24320;&#21457;&#26356;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26679;&#24335;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#26080;&#28304;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#19982;&#20869;&#23481;&#29305;&#24449;&#38752;&#36817;&#26469;&#20445;&#35777;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15199</link><description>&lt;p&gt;
PromptStyler&#65306;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#28304;&#22495;&#27867;&#21270;&#39118;&#26684;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. (arXiv:2307.15199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26679;&#24335;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#26080;&#28304;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#19982;&#20869;&#23481;&#29305;&#24449;&#38752;&#36817;&#26469;&#20445;&#35777;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#31354;&#38388;&#20013;&#65292;&#25991;&#26412;&#29305;&#24449;&#65288;&#22914;&#8220;&#19968;&#24352;&#29399;&#30340;&#29031;&#29255;&#8221;&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20854;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#29399;&#30340;&#29031;&#29255;&#65289;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#26469;&#21512;&#25104;&#21508;&#31181;&#26679;&#24335;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#22270;&#20687;&#26469;&#22788;&#29702;&#26080;&#28304;&#22495;&#27867;&#21270;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#26679;&#24335;&#35789;&#21521;&#37327;&#20026;&#20266;&#35789;S*&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#29305;&#24449;&#65288;&#22914;&#8220;a S* style of a&#8221;&#65289;&#12290;&#20026;&#20102;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#65288;&#22914;&#8220;a S* style of a [class]&#8221;&#65289;&#22312;&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#31354;&#38388;&#20013;&#38752;&#36817;&#20854;&#23545;&#24212;&#30340;&#20869;&#23481;&#29305;&#24449;&#65288;&#22914;&#8220;[class]&#8221;&#65289;&#12290;&#22312;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#30340;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;PromptStyler&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#22270;&#20687;&#65292;&#24182;&#19988;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20294;&#22312;PACS&#12289;VLCS&#12289;OfficeHome&#21644;DomainNet&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Inspired by this, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. Our method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, although it does not require any images and take
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23567;&#23398;&#20064;&#29575;&#21644;&#26799;&#24230;&#22122;&#22768;&#26159;&#20027;&#35201;&#19981;&#31283;&#23450;&#28304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#21160;&#37327;&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#30340;&#36793;&#38469;&#20215;&#20540;&#26159;&#26377;&#38480;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.15196</link><description>&lt;p&gt;
&#23567;&#23398;&#20064;&#29575;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#21160;&#37327;&#30340;&#36793;&#38469;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
The Marginal Value of Momentum for Small Learning Rate SGD. (arXiv:2307.15196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23567;&#23398;&#20064;&#29575;&#21644;&#26799;&#24230;&#22122;&#22768;&#26159;&#20027;&#35201;&#19981;&#31283;&#23450;&#28304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#21160;&#37327;&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#30340;&#36793;&#38469;&#20215;&#20540;&#26159;&#26377;&#38480;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#30340;&#24378;&#20984;&#29615;&#22659;&#20013;&#65292;&#21160;&#37327;&#24050;&#34987;&#35777;&#26126;&#33021;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#12290;&#22312;&#38543;&#26426;&#20248;&#21270;&#20013;&#65292;&#22914;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#20256;&#35328;&#35748;&#20026;&#21160;&#37327;&#21487;&#20197;&#36890;&#36807;&#20943;&#23567;&#38543;&#26426;&#26799;&#24230;&#26356;&#26032;&#30340;&#26041;&#24046;&#26469;&#24110;&#21161;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#65292;&#20294;&#20043;&#21069;&#30340;&#29702;&#35770;&#20998;&#26512;&#24182;&#27809;&#26377;&#21457;&#29616;&#21160;&#37327;&#21487;&#20197;&#25552;&#20379;&#20219;&#20309;&#21487;&#35777;&#23454;&#30340;&#21152;&#36895;&#12290;&#26412;&#25991;&#30340;&#29702;&#35770;&#32467;&#26524;&#38416;&#26126;&#20102;&#22312;&#23398;&#20064;&#29575;&#36739;&#23567;&#19988;&#26799;&#24230;&#22122;&#22768;&#26159;&#20027;&#35201;&#19981;&#31283;&#23450;&#28304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#21160;&#37327;&#30340;&#20316;&#29992;&#65292;&#34920;&#26126;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#27573;&#20869;&#34920;&#29616;&#30456;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23454;&#38469;&#35757;&#32451;&#20013;&#65292;&#21160;&#37327;&#22312;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#38754;&#30830;&#23454;&#26377;&#23616;&#38480;&#30340;&#30410;&#22788;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#20064;&#29575;&#19981;&#26159;&#24456;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#22312;ImageNet&#19978;&#20174;&#22836;&#35757;&#32451;&#23567;&#33267;&#20013;&#31561;&#25209;&#27425;&#22823;&#23567;&#30340;&#27169;&#22411;&#21644;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning language models on downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15193</link><description>&lt;p&gt;
&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning in Repeated Multi-Unit Pay-As-Bid Auctions. (arXiv:2307.15193v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#30899;&#25490;&#25918;&#20132;&#26131;&#26041;&#26696;&#12289;&#22269;&#20538;&#25293;&#21334;&#21644;&#37319;&#36141;&#25293;&#21334;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#37117;&#28041;&#21450;&#25293;&#21334;&#21516;&#36136;&#30340;&#22810;&#20010;&#21333;&#20301;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22914;&#20309;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#22312;&#27599;&#20010;&#25293;&#21334;&#20013;&#65292;&#22823;&#37327;&#65288;&#30456;&#21516;&#30340;&#65289;&#29289;&#21697;&#23558;&#34987;&#20998;&#37197;&#32473;&#26368;&#39640;&#30340;&#20986;&#20215;&#65292;&#27599;&#20010;&#20013;&#26631;&#20215;&#31561;&#20110;&#20986;&#20215;&#26412;&#36523;&#12290;&#30001;&#20110;&#34892;&#21160;&#31354;&#38388;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#23398;&#20064;&#22914;&#20309;&#22312;&#20184;&#36153;&#25293;&#21334;&#20013;&#20986;&#20215;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20851;&#27880;&#31163;&#32447;&#35774;&#32622;&#65292;&#20854;&#20013;&#25237;&#26631;&#20154;&#36890;&#36807;&#21482;&#33021;&#35775;&#38382;&#20854;&#20182;&#25237;&#26631;&#20154;&#36807;&#21435;&#25552;&#20132;&#30340;&#20986;&#20215;&#26469;&#20248;&#21270;&#20182;&#20204;&#30340;&#20986;&#20215;&#21521;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#26041;&#26696;&#26469;&#33719;&#24471;&#12290;&#25105;&#20204;&#21033;&#29992;DP&#26041;&#26696;&#30340;&#32467;&#26500;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;f-DISTILL&#26694;&#26550;&#65292;&#23558;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#24314;&#27169;&#20026;&#26368;&#23567;&#21270;&#24191;&#20041;f-&#20998;&#27495;&#20989;&#25968;&#12290;&#36890;&#36807;&#22312;&#35789;&#32423;&#19978;&#35745;&#31639;&#25439;&#22833;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#24182;&#20351;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#27169;&#22411;&#20013;&#23398;&#20064;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.15190</link><description>&lt;p&gt;
f-Divergence&#26368;&#23567;&#21270;&#29992;&#20110;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
f-Divergence Minimization for Sequence-Level Knowledge Distillation. (arXiv:2307.15190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;f-DISTILL&#26694;&#26550;&#65292;&#23558;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#24314;&#27169;&#20026;&#26368;&#23567;&#21270;&#24191;&#20041;f-&#20998;&#27495;&#20989;&#25968;&#12290;&#36890;&#36807;&#22312;&#35789;&#32423;&#19978;&#35745;&#31639;&#25439;&#22833;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#24182;&#20351;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#27169;&#22411;&#20013;&#23398;&#20064;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#23558;&#30693;&#35782;&#20174;&#22823;&#27169;&#22411;&#36716;&#31227;&#21040;&#23567;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#30001;&#20110;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#38656;&#27714;&#65292;&#23427;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;f-DISTILL&#26694;&#26550;&#65292;&#23558;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#24314;&#27169;&#20026;&#26368;&#23567;&#21270;&#24191;&#20041;f-&#20998;&#27495;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#25552;&#20986;&#20102;&#22235;&#31181;&#33976;&#39311;&#21464;&#31181;&#65292;&#24182;&#34920;&#26126;&#29616;&#26377;&#30340; SeqKD &#21644; ENGINE &#26041;&#27861;&#26159;&#25105;&#20204;f-DISTILL&#26041;&#27861;&#30340;&#36817;&#20284;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20986;&#20102;&#25105;&#20204;&#30340;f-DISTILL&#30340;&#36880;&#27493;&#20998;&#35299;&#65292;&#23558;&#38590;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#32423;&#20998;&#27495;&#31616;&#21270;&#20026;&#21487;&#20197;&#20197;&#19968;&#31181;&#21487;&#22788;&#29702;&#30340;&#26041;&#24335;&#35745;&#31639;&#30340;&#35789;&#32423;&#25439;&#22833;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;KD&#26041;&#27861;&#65292;&#24182;&#19988;&#25105;&#20204;&#23545;&#31216;&#30340;&#33976;&#39311;&#25439;&#22833;&#21487;&#20197;&#26356;&#22909;&#22320;&#24378;&#36843;&#23398;&#29983;&#20174;&#25945;&#24072;&#20998;&#24067;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an f-DISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our f-DISTILL methods. We further derive step-wise decomposition for our f-DISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.15176</link><description>&lt;p&gt;
RCT&#25298;&#32477;&#25277;&#26679;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15176
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26080;&#20559;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#39640;&#32500;&#21327;&#21464;&#37327;&#30340;&#24773;&#20917;&#65292;&#22914;&#25991;&#26412;&#25968;&#25454;&#12289;&#22522;&#22240;&#32452;&#23398;&#25110;&#34892;&#20026;&#31038;&#20250;&#31185;&#23398;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#30340;&#35843;&#25972;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#25972;&#26041;&#27861;&#30340;&#32463;&#39564;&#35780;&#20272;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#21644;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#32463;&#39564;&#35780;&#20272;&#31574;&#30053;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#35774;&#35745;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#65306;&#23545;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#36827;&#34892;&#23376;&#25277;&#26679;&#65292;&#20197;&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#31216;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#30830;&#20445;&#35266;&#27979;&#25968;&#25454;&#30340;&#22240;&#26524;&#35782;&#21035;&#25104;&#31435;&#65292;&#20174;&#32780;&#21487;&#20197;&#19982;&#22522;&#20934;RCT&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#22411;&#33258;&#21160;&#38656;&#27714;&#21709;&#24212;&#31995;&#32479;&#23384;&#22312;&#30340;&#22240;&#26524;&#24615;&#32593;&#32476;&#25915;&#20987;&#28431;&#27934;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#25915;&#20987;&#31574;&#30053;&#26469;&#27169;&#25311;&#23545;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#24694;&#24847;&#31713;&#25913;&#12290;</title><link>http://arxiv.org/abs/2307.15175</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#22411;&#33258;&#21160;&#38656;&#27714;&#21709;&#24212;&#31995;&#32479;&#19978;&#30340;&#22240;&#26524;&#24615;&#32593;&#32476;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Causative Cyberattacks on Online Learning-based Automated Demand Response Systems. (arXiv:2307.15175v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15175
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#22411;&#33258;&#21160;&#38656;&#27714;&#21709;&#24212;&#31995;&#32479;&#23384;&#22312;&#30340;&#22240;&#26524;&#24615;&#32593;&#32476;&#25915;&#20987;&#28431;&#27934;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#25915;&#20987;&#31574;&#30053;&#26469;&#27169;&#25311;&#23545;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#24694;&#24847;&#31713;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#20844;&#29992;&#20107;&#19994;&#27491;&#22312;&#37319;&#29992;&#33258;&#21160;&#38656;&#27714;&#21709;&#24212;&#65288;ADR&#65289;&#26469;&#26367;&#20195;&#26114;&#36149;&#30340;&#29123;&#29028;&#21457;&#30005;&#26426;&#65292;&#24182;&#22312;&#30005;&#21147;&#38656;&#27714;&#39640;&#23792;&#26399;&#39044;&#38450;&#25317;&#22622;&#12290;&#21516;&#26679;&#65292;&#31532;&#19977;&#26041;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#32858;&#21512;&#22120;&#27491;&#22312;&#21033;&#29992;&#21487;&#25511;&#30340;&#23567;&#35268;&#27169;&#30005;&#21147;&#36127;&#33655;&#20026;&#20844;&#29992;&#20107;&#19994;&#25552;&#20379;&#25353;&#38656;&#30340;&#30005;&#32593;&#25903;&#25345;&#26381;&#21153;&#12290;&#19968;&#20123;&#32858;&#21512;&#22120;&#21644;&#20844;&#29992;&#20107;&#19994;&#24320;&#22987;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26469;&#20102;&#35299;&#30005;&#21147;&#28040;&#36153;&#32773;&#30340;&#33021;&#28304;&#20351;&#29992;&#27169;&#24335;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#26469;&#35774;&#35745;&#26368;&#20339;&#30340;DR&#28608;&#21169;&#25514;&#26045;&#12290;&#36825;&#26679;&#30340;AI&#26694;&#26550;&#20351;&#29992;&#20102;&#20844;&#29992;&#20107;&#19994;/&#32858;&#21512;&#22120;&#21644;DR&#23458;&#25143;&#20043;&#38388;&#30340;&#24320;&#25918;&#36890;&#20449;&#28192;&#36947;&#65292;&#36825;&#20123;&#36890;&#36947;&#23481;&#26131;&#21463;&#21040;\textit{&#22240;&#26524;&#24615;}&#25968;&#25454;&#23436;&#25972;&#24615;&#32593;&#32476;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;AI&#30340;DR&#23398;&#20064;&#30340;&#28431;&#27934;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21033;&#29992;&#20174;&#32445;&#32422;&#22823;&#23398;&#65288;NYU&#65289;&#26657;&#22253;&#24314;&#31569;&#20013;&#25910;&#38598;&#30340;DR&#25968;&#25454;&#36827;&#34892;&#20102;&#20449;&#24687;&#25910;&#38598;&#12290;&#35813;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#24694;&#24847;&#31713;&#25913;&#65288;i&#65289;&#23454;&#26102;DR&#30340;&#21487;&#34892;&#24615;&#21644;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Power utilities are adopting Automated Demand Response (ADR) to replace the costly fuel-fired generators and to preempt congestion during peak electricity demand. Similarly, third-party Demand Response (DR) aggregators are leveraging controllable small-scale electrical loads to provide on-demand grid support services to the utilities. Some aggregators and utilities have started employing Artificial Intelligence (AI) to learn the energy usage patterns of electricity consumers and use this knowledge to design optimal DR incentives. Such AI frameworks use open communication channels between the utility/aggregator and the DR customers, which are vulnerable to \textit{causative} data integrity cyberattacks. This paper explores vulnerabilities of AI-based DR learning and designs a data-driven attack strategy informed by DR data collected from the New York University (NYU) campus buildings. The case study demonstrates the feasibility and effects of maliciously tampering with (i) real-time DR 
&lt;/p&gt;</description></item><item><title>PredictChain&#26159;&#19968;&#20010;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#24066;&#22330;&#65292;&#26088;&#22312;&#35299;&#20915;&#36890;&#36807;&#25552;&#20379;&#21512;&#20316;&#21644;&#25968;&#25454;&#21487;&#35775;&#38382;&#24615;&#30340;&#26426;&#21046;&#26469;&#35757;&#32451;&#21644;&#21033;&#29992;&#39044;&#27979;&#24615;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#25152;&#38754;&#20020;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#35775;&#38382;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15168</link><description>&lt;p&gt;
PredictChain&#65306;&#20026;&#21435;&#20013;&#24515;&#21270;&#21306;&#22359;&#38142;&#24066;&#22330;&#36171;&#33021;&#21512;&#20316;&#21644;&#25968;&#25454;&#21487;&#35775;&#38382;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
PredictChain: Empowering Collaboration and Data Accessibility for AI in a Decentralized Blockchain-based Marketplace. (arXiv:2307.15168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15168
&lt;/p&gt;
&lt;p&gt;
PredictChain&#26159;&#19968;&#20010;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#24066;&#22330;&#65292;&#26088;&#22312;&#35299;&#20915;&#36890;&#36807;&#25552;&#20379;&#21512;&#20316;&#21644;&#25968;&#25454;&#21487;&#35775;&#38382;&#24615;&#30340;&#26426;&#21046;&#26469;&#35757;&#32451;&#21644;&#21033;&#29992;&#39044;&#27979;&#24615;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#25152;&#38754;&#20020;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#35775;&#38382;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#26377;&#38480;&#35775;&#38382;&#32473;&#24076;&#26395;&#35757;&#32451;&#21644;&#21033;&#29992;&#39044;&#27979;&#24615;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20010;&#20154;&#21644;&#22242;&#20307;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#20844;&#24320;&#21487;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#27809;&#26377;&#25176;&#31649;&#65292;&#38656;&#35201;&#26368;&#32456;&#29992;&#25143;&#24314;&#31435;&#33258;&#24049;&#30340;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#12290;&#25110;&#32773;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20165;&#36890;&#36807;&#20184;&#36153;&#30340;&#22522;&#20110;&#20113;&#30340;&#26426;&#21046;&#35775;&#38382;&#65292;&#36825;&#23545;&#20110;&#20844;&#20247;&#30340;&#19968;&#33324;&#20351;&#29992;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#38656;&#35201;&#19968;&#31181;&#26356;&#27969;&#30021;&#30340;&#26041;&#27861;&#26469;&#36319;&#36394;&#36164;&#28304;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20174;&#20854;&#20182;&#20154;&#30340;&#38543;&#21518;&#20351;&#29992;&#20013;&#33719;&#24471;&#36164;&#37329;&#21644;&#20854;&#20182;&#26041;&#38754;&#30340;&#21033;&#30410;&#12290;&#30446;&#21069;&#36824;&#32570;&#20047;&#19968;&#31181;&#26377;&#25928;&#30340;&#26426;&#21046;&#26469;&#20026;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#36129;&#29486;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;PredictChain&#8221;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#24066;&#22330;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#20010;&#24066;&#22330;&#20801;&#35768;&#29992;&#25143;&#19978;&#20256;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#39044;&#27979;&#24615;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35831;&#27714;&#27169;&#22411;&#20351;&#29992;&#24182;&#36861;&#36394;&#36164;&#28304;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limited access to computing resources and training data poses significant challenges for individuals and groups aiming to train and utilize predictive machine learning models. Although numerous publicly available machine learning models exist, they are often unhosted, necessitating end-users to establish their computational infrastructure. Alternatively, these models may only be accessible through paid cloud-based mechanisms, which can prove costly for general public utilization. Moreover, model and data providers require a more streamlined approach to track resource usage and capitalize on subsequent usage by others, both financially and otherwise. An effective mechanism is also lacking to contribute high-quality data for improving model performance. We propose a blockchain-based marketplace called "PredictChain" for predictive machine-learning models to address these issues. This marketplace enables users to upload datasets for training predictive machine learning models, request mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#23884;&#20837;&#34920;&#31034;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#22797;&#26434;&#23545;&#35805;&#20013;&#24773;&#32490;&#34920;&#36798;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#24773;&#32490;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23567;&#26679;&#26412;&#21644;&#19981;&#24179;&#34913;&#30340;&#28151;&#21512;&#30446;&#26631;&#24773;&#32490;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.15164</link><description>&lt;p&gt;
VISU&#21442;&#21152;WASSA 2023&#20849;&#20139;&#20219;&#21153;&#65306;&#21033;&#29992;BERT&#21644;&#22534;&#21472;&#23884;&#20837;&#26816;&#27979;&#23545;&#26032;&#38395;&#25925;&#20107;&#30340;&#24773;&#32490;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings. (arXiv:2307.15164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#23884;&#20837;&#34920;&#31034;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#22797;&#26434;&#23545;&#35805;&#20013;&#24773;&#32490;&#34920;&#36798;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#24773;&#32490;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23567;&#26679;&#26412;&#21644;&#19981;&#24179;&#34913;&#30340;&#28151;&#21512;&#30446;&#26631;&#24773;&#32490;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#31995;&#32479;VISU&#21442;&#21152;&#20102;WASSA 2023&#20849;&#20139;&#20219;&#21153;&#65288;3&#65289;&#65292;&#21363;&#20174;&#23545;&#26032;&#38395;&#25991;&#31456;&#30340;&#21453;&#24212;&#20013;&#20889;&#30340;&#25991;&#31456;&#20013;&#36827;&#34892;&#24773;&#32490;&#20998;&#31867;&#12290;&#20174;&#22797;&#26434;&#23545;&#35805;&#20013;&#26816;&#27979;&#24773;&#32490;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#19978;&#19979;&#25991;/&#39046;&#22495;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#24320;&#21457;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65292;&#20351;&#29992;&#23450;&#21046;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#19982;&#35789;&#23884;&#20837;&#34920;&#31034;&#30340;&#32452;&#21512;&#26469;&#25429;&#25417;&#34920;&#36798;&#30340;&#24773;&#32490;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20102;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#65288;&#21333;&#29420;&#21644;&#22534;&#21472;&#65289;&#19982;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;BiLSTM&#65289;&#21644;Transformer&#27169;&#22411;&#12290;&#22312;&#24773;&#32490;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21344;&#25454;&#20102;&#31532;&#21313;&#21517;&#65292;&#24471;&#20998;&#20026;0.2717&#30340;&#23439;F1-&#20998;&#25968;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#23454;&#26045;&#30340;&#26041;&#27861;&#22312;&#23567;&#26679;&#26412;&#21644;&#19981;&#24179;&#34913;&#30340;&#28151;&#21512;&#30446;&#26631;&#24773;&#32490;&#25968;&#25454;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion Classification from essays written in reaction to news articles. Emotion detection from complex dialogues is challenging and often requires context/domain understanding. Therefore in this research, we have focused on developing deep learning (DL) models using the combination of word embedding representations with tailored prepossessing strategies to capture the nuances of emotions expressed. Our experiments used static and contextual embeddings (individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and Transformer based models. We occupied rank tenth in the emotion detection task by scoring a Macro F1-Score of 0.2717, validating the efficacy of our implemented approaches for small and imbalanced datasets with mixed categories of target emotions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26032;&#22411;&#24863;&#30693;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;R-LPIPS&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;LPIPS&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15157</link><description>&lt;p&gt;
R-LPIPS: &#19968;&#31181;&#20855;&#26377;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24863;&#30693;&#30456;&#20284;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
R-LPIPS: An Adversarially Robust Perceptual Similarity Metric. (arXiv:2307.15157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26032;&#22411;&#24863;&#30693;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;R-LPIPS&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;LPIPS&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#24230;&#24230;&#37327;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#29992;&#20110;&#25429;&#25417;&#22270;&#20687;&#30340;&#22522;&#26412;&#35821;&#20041;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#20986;&#29616;&#20102;&#20808;&#36827;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#20363;&#22914;&#23398;&#20064;&#30340;&#24863;&#30693;&#22270;&#20687;&#22359;&#30456;&#20284;&#24230;&#65288;LPIPS&#65289;&#12290;&#36825;&#20123;&#24230;&#37327;&#21033;&#29992;&#26469;&#33258;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#29305;&#24449;&#65292;&#24182;&#22312;&#30456;&#23545;&#22270;&#20687;&#30456;&#20284;&#24230;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#24863;&#30693;&#23494;&#20999;&#19968;&#33268;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#22312;&#24050;&#32463;&#20247;&#25152;&#21608;&#30693;&#65292;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#65292;&#21363;&#23545;&#20154;&#31867;&#26469;&#35828;&#19981;&#21487;&#35265;&#30340;&#23567;&#25200;&#21160;&#65292;&#34987;&#31934;&#24515;&#35774;&#35745;&#29992;&#26469;&#25925;&#24847;&#35823;&#23548;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;LPIPS&#24230;&#37327;&#26041;&#27861;&#20063;&#23545;&#36825;&#20123;&#23545;&#25239;&#24615;&#31034;&#20363;&#25935;&#24863;&#12290;&#36825;&#31181;&#25935;&#24863;&#24615;&#24341;&#20837;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;LPIPS&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#40065;&#26834;&#30340;&#23398;&#20064;&#24863;&#30693;&#22270;&#20687;&#22359;&#30456;&#20284;&#24230;&#65288;R-LPIPS&#65289;&#24230;&#37327;&#26041;&#27861;&#65292;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#28145;&#24230;&#29305;&#24449;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Similarity metrics have played a significant role in computer vision to capture the underlying semantics of images. In recent years, advanced similarity metrics, such as the Learned Perceptual Image Patch Similarity (LPIPS), have emerged. These metrics leverage deep features extracted from trained neural networks and have demonstrated a remarkable ability to closely align with human perception when evaluating relative image similarity. However, it is now well-known that neural networks are susceptible to adversarial examples, i.e., small perturbations invisible to humans crafted to deliberately mislead the model. Consequently, the LPIPS metric is also sensitive to such adversarial examples. This susceptibility introduces significant security concerns, especially considering the widespread adoption of LPIPS in large-scale applications. In this paper, we propose the Robust Learned Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages adversarially trained deep f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20174;&#19968;&#20010;G-&#26368;&#20248;&#35774;&#35745;&#20013;&#38543;&#26426;&#36873;&#25321;&#33218;&#26469;&#23454;&#29616;&#26368;&#20339;&#33218;&#30340;&#40065;&#26834;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2307.15154</link><description>&lt;p&gt;
A/B&#27979;&#35797;&#21644;&#20855;&#26377;&#38750;&#31283;&#24577;&#40065;&#26834;&#24615;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A/B Testing and Best-arm Identification for Linear Bandits with Robustness to Non-stationarity. (arXiv:2307.15154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20174;&#19968;&#20010;G-&#26368;&#20248;&#35774;&#35745;&#20013;&#38543;&#26426;&#36873;&#25321;&#33218;&#26469;&#23454;&#29616;&#26368;&#20339;&#33218;&#30340;&#40065;&#26834;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21487;&#33021;&#23384;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#32473;&#23450;&#26377;&#38480;&#33218;&#38598;&#21512;X&#65292;&#22266;&#23450;&#39044;&#31639;T&#20197;&#21450;&#19981;&#21487;&#39044;&#27979;&#30340;&#21442;&#25968;&#24207;&#21015;&#952;&#65292;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#20197;&#23613;&#21487;&#33021;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#20339;&#33218;x*&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#22312;&#31283;&#24577;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#38169;&#35823;&#27010;&#29575;&#38543;&#30528;&#39044;&#31639;&#30340;&#22686;&#21152;&#32780;&#25351;&#25968;&#19979;&#38477;&#12290;&#20294;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;A/B/n&#22810;&#21464;&#37327;&#27979;&#35797;&#22330;&#26223;&#20013;&#65292;&#29615;&#22659;&#26159;&#38750;&#31283;&#24577;&#30340;&#65292;&#32780;&#19968;&#20010;&#26399;&#26395;&#31283;&#24577;&#30340;&#31639;&#27861;&#24456;&#23481;&#26131;&#22833;&#36133;&#12290;&#20026;&#20102;&#20855;&#26377;&#40065;&#26834;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22914;&#26524;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20174;X&#30340;&#19968;&#20010;G-&#26368;&#20248;&#35774;&#35745;&#20013;&#20197;&#38543;&#26426;&#21644;&#38750;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#36873;&#25321;&#33218;&#65292;&#37027;&#20040;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#33218;&#30340;&#40065;&#26834;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the fixed-budget best-arm identification (BAI) problem for linear bandits in a potentially non-stationary environment. Given a finite arm set $\mathcal{X}\subset\mathbb{R}^d$, a fixed budget $T$, and an unpredictable sequence of parameters $\left\lbrace\theta_t\right\rbrace_{t=1}^{T}$, an algorithm will aim to correctly identify the best arm $x^* := \arg\max_{x\in\mathcal{X}}x^\top\sum_{t=1}^{T}\theta_t$ with probability as high as possible. Prior work has addressed the stationary setting where $\theta_t = \theta_1$ for all $t$ and demonstrated that the error probability decreases as $\exp(-T /\rho^*)$ for a problem-dependent constant $\rho^*$. But in many real-world $A/B/n$ multivariate testing scenarios that motivate our work, the environment is non-stationary and an algorithm expecting a stationary setting can easily fail. For robust identification, it is well-known that if arms are chosen randomly and non-adaptively from a G-optimal design over $\mathcal{X}$ at each 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;R-Block&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;Dropout&#22359;&#65292;&#36890;&#36807;&#20114;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#24378;&#21046;&#20004;&#20010;&#23376;&#27169;&#22411;&#30340;&#36755;&#20986;&#19968;&#33268;&#24615;&#65292;&#20197;&#35299;&#20915;&#21367;&#31215;&#23618;&#20013;Dropout&#25928;&#26524;&#36739;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15150</link><description>&lt;p&gt;
R-Block: &#29992;&#20110;&#21367;&#31215;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;Dropout&#22359;
&lt;/p&gt;
&lt;p&gt;
R-Block: Regularized Block of Dropout for convolutional networks. (arXiv:2307.15150v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;R-Block&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;Dropout&#22359;&#65292;&#36890;&#36807;&#20114;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#24378;&#21046;&#20004;&#20010;&#23376;&#27169;&#22411;&#30340;&#36755;&#20986;&#19968;&#33268;&#24615;&#65292;&#20197;&#35299;&#20915;&#21367;&#31215;&#23618;&#20013;Dropout&#25928;&#26524;&#36739;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dropout&#20316;&#20026;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#20840;&#36830;&#25509;&#23618;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;&#21367;&#31215;&#23618;&#20013;&#25928;&#26524;&#36739;&#24046;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#26356;&#26377;&#32467;&#26500;&#24615;&#30340;Dropout&#24418;&#24335;&#26469;&#27491;&#21017;&#21270;&#21367;&#31215;&#32593;&#32476;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#32570;&#28857;&#26159;&#24341;&#20837;&#30340;&#38543;&#26426;&#24615;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#24212;&#29992;&#20102;&#19968;&#31181;&#20114;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;R-Block&#65292;&#26469;&#36827;&#34892;&#21367;&#31215;&#23618;&#30340;&#27491;&#21017;&#21270;&#12290;&#23427;&#24378;&#21046;&#20004;&#20010;&#29983;&#25104;&#30340;&#26368;&#22823;&#21270;&#24046;&#24322;&#30340;&#23376;&#27169;&#22411;&#30340;&#36755;&#20986;&#24444;&#27492;&#19968;&#33268;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;R-Block&#36890;&#36807;&#26368;&#23567;&#21270;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#26679;&#26412;&#30340;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;Drop&#21306;&#22495;&#30340;&#23376;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#20043;&#38388;&#30340;&#25439;&#22833;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26500;&#24314;&#36825;&#26679;&#23376;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;R-Block&#27604;&#20854;&#20182;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;Dropout&#21464;&#20307;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#26500;&#24314;&#23376;&#27169;&#22411;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dropout as a regularization technique is widely used in fully connected layers while is less effective in convolutional layers. Therefore more structured forms of dropout have been proposed to regularize convolutional networks. The disadvantage of these methods is that the randomness introduced causes inconsistency between training and inference. In this paper, we apply a mutual learning training strategy for convolutional layer regularization, namely R-Block, which forces two outputs of the generated difference maximizing sub models to be consistent with each other. Concretely, R-Block minimizes the losses between the output distributions of two sub models with different drop regions for each sample in the training dataset. We design two approaches to construct such sub models. Our experiments demonstrate that R-Block achieves better performance than other existing structured dropout variants. We also demonstrate that our approaches to construct sub models outperforms others.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#26356;&#26032;&#23398;&#20064;&#27169;&#22411;&#20197;&#36866;&#24212;&#21487;&#21464;&#22823;&#23567;&#30340;&#26032;&#25968;&#25454;&#22359;&#30340;&#24773;&#26223;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#22312;&#26816;&#27979;&#21464;&#24418;&#25915;&#20987;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#36951;&#24536;&#23398;&#20064;&#65288;LwF&#65289;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;&#31639;&#27861;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2307.15105</link><description>&lt;p&gt;
&#36890;&#36807;&#25345;&#32493;&#22686;&#37327;&#35757;&#32451;&#26816;&#27979;&#21464;&#24418;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Detecting Morphing Attacks via Continual Incremental Training. (arXiv:2307.15105v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#26356;&#26032;&#23398;&#20064;&#27169;&#22411;&#20197;&#36866;&#24212;&#21487;&#21464;&#22823;&#23567;&#30340;&#26032;&#25968;&#25454;&#22359;&#30340;&#24773;&#26223;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#22312;&#26816;&#27979;&#21464;&#24418;&#25915;&#20987;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#36951;&#24536;&#23398;&#20064;&#65288;LwF&#65289;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;&#31639;&#27861;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20256;&#36755;&#21644;&#23384;&#20648;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#27861;&#20351;&#29992;&#21333;&#20010;&#25968;&#25454;&#38598;&#32452;&#21512;&#22810;&#20010;&#25968;&#25454;&#26469;&#28304;&#36827;&#34892;&#25209;&#27425;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#24320;&#21457;&#40065;&#26834;&#24615;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#31181;&#25361;&#25112;&#12290;&#26412;&#25991;&#20551;&#35774;&#26368;&#36817;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#33539;&#24335;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#36890;&#36807;&#22810;&#20010;&#31449;&#28857;&#36827;&#34892;&#22686;&#37327;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#29305;&#27530;&#30340;CL&#26041;&#27861;&#65292;&#21363;&#26080;&#36951;&#24536;&#23398;&#20064;&#65288;LwF&#65289;&#65292;&#26159;&#24615;&#33021;&#26368;&#22909;&#30340;&#31639;&#27861;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scenarios in which restrictions in data transfer and storage limit the possibility to compose a single dataset -- also exploiting different data sources -- to perform a batch-based training procedure, make the development of robust models particularly challenging. We hypothesize that the recent Continual Learning (CL) paradigm may represent an effective solution to enable incremental training, even through multiple sites. Indeed, a basic assumption of CL is that once a model has been trained, old data can no longer be used in successive training iterations and in principle can be deleted. Therefore, in this paper, we investigate the performance of different Continual Learning methods in this scenario, simulating a learning model that is updated every time a new chunk of data, even of variable size, is available. Experimental results reveal that a particular CL method, namely Learning without Forgetting (LwF), is one of the best-performing algorithms. Then, we investigate its usage and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;Nvidia Edge GPU&#35774;&#22791;&#19978;&#21033;&#29992;&#22768;&#38899;&#21644;&#38899;&#39057;&#20998;&#31867;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#20799;&#31461;&#34384;&#24453;&#30340;&#26816;&#27979;&#65292;&#30830;&#20445;&#20102;&#20799;&#31461;&#30340;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2307.15101</link><description>&lt;p&gt;
&#36890;&#36807;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#26426;&#22120;&#23398;&#20064;&#22312;Nvidia Edge GPU&#35774;&#22791;&#19978;&#23454;&#29616;&#22768;&#38899;&#19982;&#38899;&#39057;&#20998;&#31867;&#30340;&#20799;&#31461;&#34384;&#24453;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detection of Children Abuse by Voice and Audio Classification by Short-Time Fourier Transform Machine Learning implemented on Nvidia Edge GPU device. (arXiv:2307.15101v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;Nvidia Edge GPU&#35774;&#22791;&#19978;&#21033;&#29992;&#22768;&#38899;&#21644;&#38899;&#39057;&#20998;&#31867;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#20799;&#31461;&#34384;&#24453;&#30340;&#26816;&#27979;&#65292;&#30830;&#20445;&#20102;&#20799;&#31461;&#30340;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#31119;&#21033;&#38498;&#20013;&#20799;&#31461;&#30340;&#23433;&#20840;&#24050;&#25104;&#20026;&#19968;&#20010;&#26085;&#30410;&#20851;&#27880;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#26412;&#23454;&#39564;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20799;&#31461;&#34384;&#24453;&#30340;&#22330;&#26223;&#26816;&#27979;&#65292;&#20197;&#25552;&#39640;&#20799;&#31461;&#30340;&#23433;&#20840;&#24615;&#12290;&#35813;&#23454;&#39564;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#20799;&#31461;&#30340;&#22768;&#38899;&#36827;&#34892;&#20998;&#31867;&#21644;&#35782;&#21035;&#65292;&#39044;&#27979;&#20799;&#31461;&#24403;&#21069;&#21457;&#20986;&#30340;&#22768;&#38899;&#26159;&#21741;&#27875;&#12289;&#23574;&#21483;&#36824;&#26159;&#31505;&#22768;&#12290;&#22914;&#26524;&#21457;&#29616;&#20799;&#31461;&#27491;&#22312;&#21741;&#27875;&#25110;&#23574;&#21483;&#65292;&#31435;&#21363;&#21521;&#30456;&#20851;&#20154;&#21592;&#21457;&#36865;&#35686;&#25253;&#65292;&#20197;&#20415;&#20182;&#20204;&#33021;&#22815;&#23519;&#35273;&#21040;&#20799;&#31461;&#22312;&#30417;&#25511;&#30450;&#21306;&#20013;&#21487;&#33021;&#32463;&#21382;&#30340;&#24773;&#20917;&#65292;&#24182;&#21450;&#26102;&#20316;&#20986;&#21453;&#24212;&#12290;&#32467;&#21512;&#35270;&#39057;&#22270;&#20687;&#20998;&#31867;&#30340;&#28151;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20799;&#31461;&#34384;&#24453;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#23156;&#20799;&#23460;&#20013;&#20799;&#31461;&#36973;&#21463;&#26292;&#21147;&#34384;&#24453;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#20351;&#24037;&#20316;&#20154;&#21592;&#33021;&#22815;&#21450;&#26102;&#38459;&#27490;&#21363;&#23558;&#21457;&#29983;&#25110;&#21021;&#26399;&#30340;&#20799;&#31461;&#34384;&#24453;&#20107;&#20214;&#12290;&#26412;&#23454;&#39564;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#23436;&#20840;&#26469;&#33258;&#24405;&#38899;&#20013;&#30340;&#22768;&#38899;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
The safety of children in children home has become an increasing social concern, and the purpose of this experiment is to use machine learning applied to detect the scenarios of child abuse to increase the safety of children. This experiment uses machine learning to classify and recognize a child's voice and predict whether the current sound made by the child is crying, screaming or laughing. If a child is found to be crying or screaming, an alert is immediately sent to the relevant personnel so that they can perceive what the child may be experiencing in a surveillance blind spot and respond in a timely manner. Together with a hybrid use of video image classification, the accuracy of child abuse detection can be significantly increased. This greatly reduces the likelihood that a child will receive violent abuse in the nursery and allows personnel to stop an imminent or incipient child abuse incident in time. The datasets collected from this experiment is entirely from sounds recorded 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#23545;&#25554;&#22270;&#36827;&#34892;&#29305;&#24449;&#21521;&#37327;&#30340;&#25552;&#21462;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#32858;&#31867;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25554;&#22270;&#25353;&#29031;&#27675;&#22260;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15099</link><description>&lt;p&gt;
&#20351;&#29992;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#23558;&#25554;&#22270;&#25353;&#29031;&#27675;&#22260;&#36827;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clustering of illustrations by atmosphere using a combination of supervised and unsupervised learning. (arXiv:2307.15099v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#23545;&#25554;&#22270;&#36827;&#34892;&#29305;&#24449;&#21521;&#37327;&#30340;&#25552;&#21462;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#32858;&#31867;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25554;&#22270;&#25353;&#29031;&#27675;&#22260;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21160;&#30011;&#12289;&#28216;&#25103;&#21644;&#21160;&#30011;&#30005;&#24433;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#25554;&#22270;&#30340;&#20998;&#24067;&#36880;&#28176;&#22686;&#21152;&#12290;&#25554;&#22270;&#30340;"&#27675;&#22260;"&#22312;&#29992;&#25143;&#20559;&#22909;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25353;&#29031;&#27675;&#22260;&#23545;&#25554;&#22270;&#36827;&#34892;&#20998;&#31867;&#21487;&#20197;&#20026;&#25512;&#33616;&#21644;&#25628;&#32034;&#25552;&#20379;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#32473;&#38590;&#20197;&#23450;&#20041;&#30340;"&#27675;&#22260;"&#20998;&#37197;&#26126;&#30830;&#30340;&#26631;&#31614;&#24182;&#19988;&#20351;&#29992;&#20256;&#32479;&#30340;&#30417;&#30563;&#20998;&#31867;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#39068;&#33394;&#12289;&#36793;&#32536;&#21644;&#20302;&#23618;&#27425;&#29305;&#24449;&#30456;&#20284;&#30340;&#22270;&#20687;&#20063;&#21487;&#33021;&#27809;&#26377;&#30456;&#20284;&#30340;&#27675;&#22260;&#65292;&#36825;&#20351;&#24471;&#22522;&#20110;&#20302;&#23618;&#27425;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20266;&#26631;&#31614;&#30340;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#20266;&#26631;&#31614;&#36890;&#36807;&#30417;&#30563;&#26041;&#27861;&#33719;&#24471;&#30340;&#29305;&#24449;&#21521;&#37327;&#26377;&#21161;&#20110;&#22788;&#29702;&#27169;&#31946;&#30340;&#27675;&#22260;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#36825;&#20123;&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#32858;&#31867;&#12290;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distribution of illustrations on social media, such as Twitter and Pixiv has increased with the growing popularity of animation, games, and animated movies. The "atmosphere" of illustrations plays an important role in user preferences. Classifying illustrations by atmosphere can be helpful for recommendations and searches. However, assigning clear labels to the elusive "atmosphere" and conventional supervised classification is not always practical. Furthermore, even images with similar colors, edges, and low-level features may not have similar atmospheres, making classification based on low-level features challenging. In this paper, this problem is solved using both supervised and unsupervised learning with pseudo-labels. The feature vectors are obtained using the supervised method with pseudo-labels that contribute to an ambiguous atmosphere. Further, clustering is performed based on these feature vectors. Experimental analyses show that our method outperforms conventional methods
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#21512;&#25104;&#23380;&#24452;&#22768;&#32435;&#65288;SAS&#65289;&#22270;&#20687;&#20013;&#24212;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36827;&#34892;&#30446;&#26631;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;SSL&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#65292;&#25552;&#39640;&#30446;&#26631;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15098</link><description>&lt;p&gt;
&#25552;&#39640;&#21512;&#25104;&#23380;&#24452;&#22768;&#32435;&#30446;&#26631;&#35782;&#21035;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Improved Synthetic Aperture Sonar Target Recognition. (arXiv:2307.15098v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#21512;&#25104;&#23380;&#24452;&#22768;&#32435;&#65288;SAS&#65289;&#22270;&#20687;&#20013;&#24212;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36827;&#34892;&#30446;&#26631;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;SSL&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#65292;&#25552;&#39640;&#30446;&#26631;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21512;&#25104;&#23380;&#24452;&#22768;&#32435;&#65288;SAS&#65289;&#22270;&#20687;&#20013;&#24212;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20197;&#25552;&#39640;&#30446;&#26631;&#35782;&#21035;&#30340;&#24212;&#29992;&#12290;&#27700;&#19979;&#29615;&#22659;&#30340;&#29420;&#29305;&#25361;&#25112;&#20351;&#24471;&#20256;&#32479;&#30340;&#20381;&#36182;&#20110;&#20809;&#23398;&#30456;&#26426;&#22270;&#20687;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#25928;&#26524;&#36739;&#24046;&#12290;SAS&#20973;&#20511;&#20854;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#33021;&#21147;&#25104;&#20026;&#27700;&#19979;&#25104;&#20687;&#30340;&#39318;&#36873;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#39640;&#20998;&#36776;&#29575;SAS&#25968;&#25454;&#23545;&#20110;&#26631;&#27880;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#32780;&#26631;&#27880;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#35299;&#20915;SAS&#25968;&#25454;&#26631;&#27880;&#25361;&#25112;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;MoCov2&#21644;BYOL&#22312;&#20108;&#36827;&#21046;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#19982;&#22791;&#21463;&#35748;&#21487;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;ResNet18&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#31181;SSL&#27169;&#22411;&#37117;&#33021;&#32988;&#36807;&#23436;&#20840;&#26377;&#30417;&#30563;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the application of self-supervised learning (SSL) for improved target recognition in synthetic aperture sonar (SAS) imagery. The unique challenges of underwater environments make traditional computer vision techniques, which rely heavily on optical camera imagery, less effective. SAS, with its ability to generate high-resolution imagery, emerges as a preferred choice for underwater imaging. However, the voluminous high-resolution SAS data presents a significant challenge for labeling; a crucial step for training deep neural networks (DNNs).  SSL, which enables models to learn features in data without the need for labels, is proposed as a potential solution to the data labeling challenge in SAS. The study evaluates the performance of two prominent SSL algorithms, MoCov2 and BYOL, against the well-regarded supervised learning model, ResNet18, for binary image classification tasks. The findings suggest that while both SSL models can outperform a fully supervised model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#32852;&#36328;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#65292;&#29992;&#20110;&#26816;&#27979;&#30005;&#35805;&#23545;&#35805;&#20013;&#30340;&#23458;&#25143;&#35831;&#27714;&#21644;&#25237;&#35785;&#12290;&#22312;ACM Multimedia 2023&#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#30340;&#35831;&#27714;&#23376;&#25361;&#25112;&#20013;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#25237;&#35785;&#21644;&#35831;&#27714;&#31867;&#21035;&#20013;&#36798;&#21040;&#20102;65.41%&#21644;85.87%&#30340;&#19981;&#24179;&#34913;&#24179;&#22343;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.15097</link><description>&lt;p&gt;
&#32423;&#32852;&#36328;&#27169;&#24577;Transformer&#29992;&#20110;&#35831;&#27714;&#21644;&#25237;&#35785;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cascaded Cross-Modal Transformer for Request and Complaint Detection. (arXiv:2307.15097v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#32852;&#36328;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#65292;&#29992;&#20110;&#26816;&#27979;&#30005;&#35805;&#23545;&#35805;&#20013;&#30340;&#23458;&#25143;&#35831;&#27714;&#21644;&#25237;&#35785;&#12290;&#22312;ACM Multimedia 2023&#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#30340;&#35831;&#27714;&#23376;&#25361;&#25112;&#20013;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#25237;&#35785;&#21644;&#35831;&#27714;&#31867;&#21035;&#20013;&#36798;&#21040;&#20102;65.41%&#21644;85.87%&#30340;&#19981;&#24179;&#34913;&#24179;&#22343;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32423;&#32852;&#36328;&#27169;&#24577;Transformer&#65288;CCMT&#65289;&#65292;&#23558;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#26816;&#27979;&#30005;&#35805;&#23545;&#35805;&#20013;&#30340;&#23458;&#25143;&#35831;&#27714;&#21644;&#25237;&#35785;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#36716;&#24405;&#35821;&#38899;&#65292;&#24182;&#23558;&#36716;&#24405;&#20214;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#26469;&#21033;&#29992;&#22810;&#27169;&#24577;&#33539;&#24335;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#29305;&#23450;&#30340;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#19982;Wav2Vec2.0&#38899;&#39057;&#29305;&#24449;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#32423;&#32852;&#20132;&#21449;&#27880;&#24847;&#21147;Transformer&#27169;&#22411;&#20013;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31995;&#32479;&#24212;&#29992;&#20110;ACM Multimedia 2023&#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#30340;&#35831;&#27714;&#23376;&#25361;&#25112;&#20013;&#65292;&#22312;&#25237;&#35785;&#21644;&#35831;&#27714;&#31867;&#21035;&#20013;&#20998;&#21035;&#36798;&#21040;&#20102;65.41%&#21644;85.87%&#30340;&#19981;&#24179;&#34913;&#24179;&#22343;&#21484;&#22238;&#29575;&#65288;UAR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel cascaded cross-modal transformer (CCMT) that combines speech and text transcripts to detect customer requests and complaints in phone conversations. Our approach leverages a multimodal paradigm by transcribing the speech using automatic speech recognition (ASR) models and translating the transcripts into different languages. Subsequently, we combine language-specific BERT-based models with Wav2Vec2.0 audio features in a novel cascaded cross-attention transformer model. We apply our system to the Requests Sub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics Challenge, reaching unweighted average recalls (UAR) of 65.41% and 85.87% for the complaint and request classes, respectively.
&lt;/p&gt;</description></item><item><title>&#27785;&#31215;&#35745;&#31639;&#26159;&#19968;&#20010;&#20855;&#26377;&#38543;&#26426;&#36830;&#25509;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31616;&#21333;&#30340;&#32467;&#26500;&#21644;&#20016;&#23500;&#30340;&#21160;&#21147;&#23398;&#20351;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#33021;&#22815;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#23427;&#30340;&#36328;&#23398;&#31185;&#24212;&#29992;&#21253;&#25324;&#29289;&#29702;&#30828;&#20214;&#23454;&#29616;&#21644;&#29983;&#29289;&#35774;&#22791;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#29702;&#35299;&#22823;&#33041;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.15092</link><description>&lt;p&gt;
&#20851;&#20110;&#27785;&#31215;&#35745;&#31639;&#21450;&#20854;&#36229;&#36234;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#30340;&#36328;&#23398;&#31185;&#24212;&#29992;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning. (arXiv:2307.15092v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15092
&lt;/p&gt;
&lt;p&gt;
&#27785;&#31215;&#35745;&#31639;&#26159;&#19968;&#20010;&#20855;&#26377;&#38543;&#26426;&#36830;&#25509;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31616;&#21333;&#30340;&#32467;&#26500;&#21644;&#20016;&#23500;&#30340;&#21160;&#21147;&#23398;&#20351;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#33021;&#22815;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#23427;&#30340;&#36328;&#23398;&#31185;&#24212;&#29992;&#21253;&#25324;&#29289;&#29702;&#30828;&#20214;&#23454;&#29616;&#21644;&#29983;&#29289;&#35774;&#22791;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#29702;&#35299;&#22823;&#33041;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27785;&#31215;&#35745;&#31639;&#65288;RC&#65289;&#39318;&#20808;&#29992;&#20110;&#26102;&#38388;&#20449;&#21495;&#22788;&#29702;&#65292;&#23427;&#26159;&#19968;&#20010;&#20855;&#26377;&#38543;&#26426;&#36830;&#25509;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#19968;&#26086;&#21021;&#22987;&#21270;&#65292;&#36830;&#25509;&#24378;&#24230;&#23558;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#32467;&#26500;&#23558;RC&#36716;&#21270;&#20026;&#19968;&#20010;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#65292;&#23558;&#20302;&#32500;&#36755;&#20837;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#20013;&#12290;&#35813;&#27169;&#22411;&#30340;&#20016;&#23500;&#21160;&#21147;&#23398;&#12289;&#32447;&#24615;&#21487;&#20998;&#24615;&#21644;&#35760;&#24518;&#23481;&#37327;&#20351;&#24471;&#31616;&#21333;&#30340;&#32447;&#24615;&#35835;&#20986;&#33021;&#22815;&#20026;&#21508;&#31181;&#24212;&#29992;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;RC&#28085;&#30422;&#20102;&#36828;&#36828;&#36229;&#20986;&#26426;&#22120;&#23398;&#20064;&#33539;&#22260;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#24050;&#32463;&#35777;&#26126;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#21487;&#20197;&#22312;&#21508;&#31181;&#29289;&#29702;&#30828;&#20214;&#23454;&#29616;&#21644;&#29983;&#29289;&#35774;&#22791;&#20013;&#23454;&#29616;&#12290;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#26356;&#30701;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#21160;&#21147;&#23398;&#35302;&#21457;&#30340;&#31070;&#32463;&#20803;&#21709;&#24212;&#20063;&#25581;&#31034;&#20102;&#29702;&#35299;&#22823;&#33041;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26426;&#21046;&#20063;&#21033;&#29992;&#31867;&#20284;&#30340;&#21160;&#21147;&#23398;&#36807;&#31243;&#12290;&#34429;&#28982;&#20851;&#20110;RC&#30340;&#25991;&#29486;&#38750;&#24120;&#24222;&#22823;&#19988;&#30862;&#29255;&#21270;&#65292;&#20294;&#25105;&#20204;&#22312;&#36825;&#37324;&#23545;RC&#30340;&#26368;&#26032;&#21457;&#23637;&#36827;&#34892;&#20102;&#32479;&#19968;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir computing (RC), first applied to temporal signal processing, is a recurrent neural network in which neurons are randomly connected. Once initialized, the connection strengths remain unchanged. Such a simple structure turns RC into a non-linear dynamical system that maps low-dimensional inputs into a high-dimensional space. The model's rich dynamics, linear separability, and memory capacity then enable a simple linear readout to generate adequate responses for various applications. RC spans areas far beyond machine learning, since it has been shown that the complex dynamics can be realized in various physical hardware implementations and biological devices. This yields greater flexibility and shorter computation time. Moreover, the neuronal responses triggered by the model's dynamics shed light on understanding brain mechanisms that also exploit similar dynamical processes. While the literature on RC is vast and fragmented, here we conduct a unified review of RC's recent devel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;CNN&#21069;&#21521;&#22788;&#29702;&#20013;&#30340;&#36873;&#25321;&#24615;&#26059;&#36716;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#23398;&#24037;&#20855;&#23545;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#22312;&#25968;&#25454;&#22788;&#29702;&#27169;&#24335;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15090</link><description>&lt;p&gt;
&#29702;&#35299;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Understanding Forward Process of Convolutional Neural Network. (arXiv:2307.15090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;CNN&#21069;&#21521;&#22788;&#29702;&#20013;&#30340;&#36873;&#25321;&#24615;&#26059;&#36716;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#23398;&#24037;&#20855;&#23545;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#22312;&#25968;&#25454;&#22788;&#29702;&#27169;&#24335;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;CNN&#21069;&#21521;&#22788;&#29702;&#20013;&#30340;&#36873;&#25321;&#24615;&#26059;&#36716;&#12290;&#23427;&#38416;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#20316;&#20026;&#19968;&#20010;&#20998;&#26512;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#26059;&#36716;&#26041;&#38754;&#32479;&#19968;&#37327;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#23450;&#20041;&#30340;&#26041;&#27861;&#35770;&#21453;&#26144;&#20102;&#36827;&#31243;&#32593;&#32476;&#26681;&#25454;&#32479;&#35745;&#25351;&#26631;&#21306;&#20998;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#32467;&#26500;&#21270;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#29702;&#35299;&#25110;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#25581;&#31034;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#22312;&#25968;&#25454;&#22788;&#29702;&#27169;&#24335;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reveal the selective rotation in the CNNs' forward processing. It elucidates the activation function as a discerning mechanism that unifies and quantizes the rotational aspects of the input data. Experiments show how this defined methodology reflects the progress network distinguish inputs based on statistical indicators, which can be comprehended or analyzed by applying structured mathematical tools. Our findings also unveil the consistency between artificial neural networks and the human brain in their data processing pattern.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#20449;&#24687;&#25552;&#21319;&#30340;&#26041;&#27861;&#36827;&#34892;&#23376;&#32676;&#21457;&#29616;&#12290;&#20855;&#20307;&#38024;&#23545;&#32954;&#30284;&#27835;&#30103;&#65292;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#30340;&#21516;&#26102;&#20943;&#23569;&#21103;&#20316;&#29992;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#38750;&#24120;&#37325;&#35201;&#65292;&#20020;&#24202;&#25351;&#21335;&#34429;&#28982;&#25552;&#20379;&#20102;&#27835;&#30103;&#24314;&#35758;&#65292;&#20294;&#20173;&#26410;&#23558;&#27835;&#30103;&#32467;&#26524;&#32435;&#20837;&#32771;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15089</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#25552;&#21319;&#23376;&#32676;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Information Gained Subgroup Discovery in Datasets. (arXiv:2307.15089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#20449;&#24687;&#25552;&#21319;&#30340;&#26041;&#27861;&#36827;&#34892;&#23376;&#32676;&#21457;&#29616;&#12290;&#20855;&#20307;&#38024;&#23545;&#32954;&#30284;&#27835;&#30103;&#65292;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#30340;&#21516;&#26102;&#20943;&#23569;&#21103;&#20316;&#29992;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#38750;&#24120;&#37325;&#35201;&#65292;&#20020;&#24202;&#25351;&#21335;&#34429;&#28982;&#25552;&#20379;&#20102;&#27835;&#30103;&#24314;&#35758;&#65292;&#20294;&#20173;&#26410;&#23558;&#27835;&#30103;&#32467;&#26524;&#32435;&#20837;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#30284;&#30151;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#39044;&#35745;2023&#24180;&#23558;&#26377;&#36229;&#36807;238,340&#20363;&#26032;&#30340;&#32954;&#30284;&#24739;&#32773;&#65292;&#20854;&#20013;&#26377;&#36229;&#36807;127,070&#20363;&#27515;&#20129;&#12290;&#36873;&#25321;&#27491;&#30830;&#30340;&#27835;&#30103;&#26041;&#26696;&#26159;&#25552;&#39640;&#23384;&#27963;&#29575;&#21644;&#25913;&#21892;&#24739;&#32773;&#29983;&#27963;&#36136;&#37327;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#30284;&#30151;&#27835;&#30103;&#21487;&#33021;&#24341;&#21457;&#21103;&#20316;&#29992;&#65292;&#36825;&#20123;&#27602;&#21103;&#21453;&#24212;&#20250;&#24341;&#36215;&#19981;&#21516;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#24433;&#21709;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#30340;&#21516;&#26102;&#20943;&#23569;&#27835;&#30103;&#21103;&#20316;&#29992;&#26159;&#20020;&#24202;&#35282;&#24230;&#35201;&#36861;&#27714;&#30340;&#37325;&#35201;&#30446;&#26631;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20020;&#24202;&#25351;&#21335;&#21253;&#25324;&#30284;&#30151;&#27835;&#30103;&#24314;&#35758;&#30340;&#19968;&#33324;&#30693;&#35782;&#65292;&#20197;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#12290;&#23613;&#31649;&#20182;&#20204;&#26681;&#25454;&#30284;&#30151;&#30142;&#30149;&#26041;&#38754;&#21644;&#20010;&#20307;&#24739;&#32773;&#29305;&#24449;&#25552;&#20379;&#27835;&#30103;&#24314;&#35758;&#65292;&#20294;&#24182;&#26410;&#25552;&#20379;&#22522;&#20110;&#27835;&#30103;&#32467;&#26524;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#23545;&#20020;&#24202;&#25351;&#21335;&#19982;&#27835;&#30103;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is the leading cause of cancer death. More than 238,340 new cases of lung cancer patients are expected in 2023, with an estimation of more than 127,070 deaths. Choosing the correct treatment is an important element to enhance the probability of survival and to improve patient's quality of life. Cancer treatments might provoke secondary effects. These toxicities cause different health problems that impact the patient's quality of life. Hence, reducing treatments toxicities while maintaining or improving their effectivenes is an important goal that aims to be pursued from the clinical perspective. On the other hand, clinical guidelines include general knowledge about cancer treatment recommendations to assist clinicians. Although they provide treatment recommendations based on cancer disease aspects and individual patient features, a statistical analysis taking into account treatment outcomes is not provided here. Therefore, the comparison between clinical guidelines with tre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#20844;&#24179;&#24615;&#30340;&#26102;&#21464;&#23450;&#20215;&#30005;&#36153;&#12290;&#36890;&#36807;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#27861;&#25429;&#25417;&#28040;&#36153;&#32773;&#20215;&#26684;&#21709;&#24212;&#34892;&#20026;&#65292;&#24182;&#23558;&#20043;&#23884;&#20837;&#21040;&#23450;&#20215;&#26041;&#26696;&#20248;&#21270;&#20013;&#65292;&#23454;&#29616;&#20102;&#20445;&#25252;&#20302;&#25910;&#20837;&#28040;&#36153;&#32773;&#20813;&#21463;&#20215;&#26684;&#27874;&#21160;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#26377;&#25928;&#28608;&#21169;&#20102;&#33021;&#28304;&#35843;&#25972;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.15088</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#27491;&#24615;&#30340;&#26102;&#21464;&#23450;&#20215;&#30005;&#36153;&#35774;&#35745;&#65306;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#21644;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Equitable Time-Varying Pricing Tariff Design: A Joint Learning and Optimization Approach. (arXiv:2307.15088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#20844;&#24179;&#24615;&#30340;&#26102;&#21464;&#23450;&#20215;&#30005;&#36153;&#12290;&#36890;&#36807;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#27861;&#25429;&#25417;&#28040;&#36153;&#32773;&#20215;&#26684;&#21709;&#24212;&#34892;&#20026;&#65292;&#24182;&#23558;&#20043;&#23884;&#20837;&#21040;&#23450;&#20215;&#26041;&#26696;&#20248;&#21270;&#20013;&#65292;&#23454;&#29616;&#20102;&#20445;&#25252;&#20302;&#25910;&#20837;&#28040;&#36153;&#32773;&#20813;&#21463;&#20215;&#26684;&#27874;&#21160;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#26377;&#25928;&#28608;&#21169;&#20102;&#33021;&#28304;&#35843;&#25972;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#21464;&#23450;&#20215;&#30005;&#36153;&#28608;&#21169;&#28040;&#36153;&#32773;&#35843;&#25972;&#30005;&#21147;&#38656;&#27714;&#20197;&#38477;&#20302;&#25104;&#26412;&#65292;&#20294;&#21487;&#33021;&#22686;&#21152;&#23545;&#21453;&#24212;&#33021;&#21147;&#26377;&#38480;&#30340;&#28040;&#36153;&#32773;&#30340;&#33021;&#28304;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#33021;&#28304;&#20844;&#21496;&#22312;&#35774;&#35745;&#36825;&#20123;&#23450;&#20215;&#26041;&#26696;&#26102;&#24517;&#39035;&#24179;&#34913;&#21487;&#36127;&#25285;&#24615;&#21644;&#28608;&#21169;&#25928;&#26524;&#65292;&#24182;&#32771;&#34385;&#28040;&#36153;&#32773;&#30340;&#21453;&#24212;&#26399;&#26395;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#23398;&#20064;&#21644;&#20248;&#21270;&#26041;&#27861;&#30340;&#20844;&#24179;&#26102;&#21464;&#23450;&#20215;&#30005;&#36153;&#35774;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21382;&#21490;&#20215;&#26684;&#21644;&#38656;&#27714;&#21709;&#24212;&#25968;&#25454;&#32534;&#30721;&#20026;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#20197;&#25429;&#25417;&#39640;&#32500;&#21644;&#38750;&#32447;&#24615;&#30340;&#28040;&#36153;&#32773;&#20215;&#26684;&#21709;&#24212;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;RNN&#23884;&#20837;&#21040;&#23450;&#20215;&#26041;&#26696;&#20248;&#21270;&#20013;&#65292;&#26500;&#24314;&#19968;&#20010;&#30446;&#26631;&#20026;&#20108;&#27425;&#22411;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#23454;&#29616;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#35745;&#31639;&#12290;&#20351;&#29992;&#30495;&#23454;&#28040;&#36153;&#32773;&#25968;&#25454;&#36827;&#34892;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20844;&#24179;&#23450;&#20215;&#26041;&#26696;&#22312;&#20445;&#25252;&#20302;&#25910;&#20837;&#28040;&#36153;&#32773;&#20813;&#21463;&#20215;&#26684;&#27874;&#21160;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#28608;&#21169;&#20102;&#33021;&#28304;&#35843;&#25972;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-varying pricing tariffs incentivize consumers to shift their electricity demand and reduce costs, but may increase the energy burden for consumers with limited response capability. The utility must thus balance affordability and response incentives when designing these tariffs by considering consumers' response expectations. This paper proposes a joint learning-based identification and optimization method to design equitable time-varying tariffs. Our proposed method encodes historical prices and demand response data into a recurrent neural network (RNN) to capture high-dimensional and non-linear consumer price response behaviors. We then embed the RNN into the tariff design optimization, formulating a non-linear optimization problem with a quadratic objective. We propose a gradient-based solution method that achieves fast and scalable computation. Simulation using real-world consumer data shows that our equitable tariffs protect low-income consumers from price surges while effecti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24739;&#32773;&#30340;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#20197;&#25551;&#36848;&#22522;&#20110;BCG&#30340;&#33152;&#33009;&#30284;&#27835;&#30103;&#30340;&#20020;&#24202;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2307.15084</link><description>&lt;p&gt;
&#20351;&#29992;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#23545;&#22522;&#20110;BCG&#30340;&#33152;&#33009;&#30284;&#27835;&#30103;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Mathematical Modeling of BCG-based Bladder Cancer Treatment Using Socio-Demographics. (arXiv:2307.15084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24739;&#32773;&#30340;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#20197;&#25551;&#36848;&#22522;&#20110;BCG&#30340;&#33152;&#33009;&#30284;&#27835;&#30103;&#30340;&#20020;&#24202;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#19990;&#30028;&#19978;&#26368;&#24120;&#35265;&#30340;&#30142;&#30149;&#20043;&#19968;&#65292;&#27599;&#24180;&#37117;&#26377;&#25968;&#30334;&#19975;&#26032;&#24739;&#32773;&#12290;&#33152;&#33009;&#30284;&#26159;&#19968;&#31181;&#26368;&#26222;&#36941;&#30340;&#30284;&#30151;&#31867;&#22411;&#65292;&#24433;&#21709;&#25152;&#26377;&#20154;&#65292;&#24182;&#27809;&#26377;&#26126;&#26174;&#30340;&#20856;&#22411;&#24739;&#32773;&#12290;&#30446;&#21069;&#65292;BCG&#20813;&#30123;&#27835;&#30103;&#26159;&#33152;&#33009;&#30284;&#30340;&#26631;&#20934;&#27835;&#30103;&#26041;&#27861;&#65292;&#25152;&#26377;&#24739;&#32773;&#37117;&#20250;&#36827;&#34892;&#27599;&#21608;&#20363;&#34892;&#30340;BCG&#27835;&#30103;&#12290;&#30001;&#20110;&#20813;&#30123;&#31995;&#32479;&#12289;&#27835;&#30103;&#21644;&#30284;&#32454;&#32990;&#20043;&#38388;&#30340;&#29983;&#29289;&#21644;&#20020;&#24202;&#22797;&#26434;&#24615;&#65292;BCG&#27835;&#30103;&#30340;&#20020;&#24202;&#32467;&#26524;&#22312;&#24739;&#32773;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24739;&#32773;&#30340;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#20197;&#25551;&#36848;&#19982;&#22522;&#20110;BCG&#30340;&#27835;&#30103;&#30456;&#20851;&#30340;&#20020;&#24202;&#21160;&#24577;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#25104;&#29087;&#30340;BCG&#27835;&#30103;&#27169;&#22411;&#65292;&#24182;&#25972;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#65292;&#20197;&#22312;&#27169;&#22411;&#20869;&#37096;&#21363;&#26102;&#35843;&#25972;&#21644;&#37325;&#26032;&#37197;&#32622;&#20851;&#38190;&#21442;&#25968;&#65292;&#20174;&#32780;&#20419;&#36827;&#20854;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer is one of the most widespread diseases around the world with millions of new patients each year. Bladder cancer is one of the most prevalent types of cancer affecting all individuals alike with no obvious prototypical patient. The current standard treatment for BC follows a routine weekly Bacillus Calmette-Guerin (BCG) immunotherapy-based therapy protocol which is applied to all patients alike. The clinical outcomes associated with BCG treatment vary significantly among patients due to the biological and clinical complexity of the interaction between the immune system, treatments, and cancer cells. In this study, we take advantage of the patient's socio-demographics to offer a personalized mathematical model that describes the clinical dynamics associated with BCG-based treatment. To this end, we adopt a well-established BCG treatment model and integrate a machine learning component to temporally adjust and reconfigure key parameters within the model thus promoting its personali
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#32451;&#20064;&#30340;&#20195;&#34920;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#24230;&#65292;&#20197;&#21450;&#24341;&#20837;&#26032;&#39062;&#30340;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#31639;&#27861;&#20013;&#23545;&#32451;&#20064;&#29305;&#24449;&#24314;&#27169;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2307.15076</link><description>&lt;p&gt;
&#22522;&#20110;&#32451;&#20064;&#20195;&#34920;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#24230;&#30340;&#30693;&#35782;&#22270;&#22686;&#24378;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Enhanced Intelligent Tutoring System Based on Exercise Representativeness and Informativeness. (arXiv:2307.15076v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#32451;&#20064;&#30340;&#20195;&#34920;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#24230;&#65292;&#20197;&#21450;&#24341;&#20837;&#26032;&#39062;&#30340;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#31639;&#27861;&#20013;&#23545;&#32451;&#20064;&#29305;&#24449;&#24314;&#27169;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31639;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#20165;&#32771;&#34385;&#20102;&#21333;&#19968;&#20851;&#31995;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#27809;&#26377;&#26377;&#25928;&#22320;&#23545;&#32451;&#20064;&#30340;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#32451;&#20064;&#20195;&#34920;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#24230;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21363;&#30693;&#35782;&#22270;&#35889;&#32451;&#20064;&#20195;&#34920;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#24230;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30001;&#22235;&#20010;&#22797;&#26434;&#30340;&#32452;&#20214;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#8212;&#8212;&#31070;&#32463;&#27880;&#24847;&#21147;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#32452;&#25104;&#12290;&#36825;&#20123;&#32452;&#20214;&#21253;&#25324;&#20449;&#24687;&#20016;&#23500;&#24615;&#32452;&#20214;&#12289;&#32451;&#20064;&#34920;&#31034;&#32452;&#20214;&#12289;&#30693;&#35782;&#37325;&#35201;&#24615;&#32452;&#20214;&#21644;&#32451;&#20064;&#20195;&#34920;&#24615;&#32452;&#20214;&#12290;&#20449;&#24687;&#20016;&#23500;&#24615;&#32452;&#20214;&#35780;&#20272;&#27599;&#20010;&#38382;&#39064;&#30340;&#20449;&#24687;&#20215;&#20540;&#65292;&#24182;&#30830;&#23450;&#20855;&#26377;&#26368;&#39640;&#32451;&#20064;&#20449;&#24687;&#20016;&#23500;&#24230;&#30340;&#20505;&#36873;&#38382;&#39064;&#38598;&#12290;&#27492;&#22806;&#65292;&#25216;&#33021;&#23884;&#20837;&#20063;&#34987;&#24341;&#20837;&#29992;&#20110;&#35780;&#20272;&#23398;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Presently, knowledge graph-based recommendation algorithms have garnered considerable attention among researchers. However, these algorithms solely consider knowledge graphs with single relationships and do not effectively model exercise-rich features, such as exercise representativeness and informativeness. Consequently, this paper proposes a framework, namely the Knowledge-Graph-Exercise Representativeness and Informativeness Framework, to address these two issues. The framework consists of four intricate components and a novel cognitive diagnosis model called the Neural Attentive cognitive diagnosis model. These components encompass the informativeness component, exercise representation component, knowledge importance component, and exercise representativeness component. The informativeness component evaluates the informational value of each question and identifies the candidate question set that exhibits the highest exercise informativeness. Furthermore, the skill embeddings are em
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ISAC-NET&#30340;&#27169;&#22411;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#34987;&#21160;&#24863;&#30693;&#19982;&#36890;&#20449;&#20449;&#21495;&#26816;&#27979;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21516;&#26102;&#33719;&#24471;&#34987;&#21160;&#24863;&#30693;&#32467;&#26524;&#21644;&#36890;&#20449;&#35299;&#35843;&#31526;&#21495;&#26469;&#35299;&#20915;&#34987;&#21160;&#24863;&#30693;&#22312;&#36890;&#20449;&#35299;&#35843;&#38169;&#35823;&#26465;&#20214;&#19979;&#30340;&#39640;&#24863;&#30693;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15074</link><description>&lt;p&gt;
ISAC-NET: &#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#38598;&#25104;&#34987;&#21160;&#24863;&#30693;&#21644;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
ISAC-NET: Model-driven Deep Learning for Integrated Passive Sensing and Communication. (arXiv:2307.15074v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ISAC-NET&#30340;&#27169;&#22411;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#34987;&#21160;&#24863;&#30693;&#19982;&#36890;&#20449;&#20449;&#21495;&#26816;&#27979;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21516;&#26102;&#33719;&#24471;&#34987;&#21160;&#24863;&#30693;&#32467;&#26524;&#21644;&#36890;&#20449;&#35299;&#35843;&#31526;&#21495;&#26469;&#35299;&#20915;&#34987;&#21160;&#24863;&#30693;&#22312;&#36890;&#20449;&#35299;&#35843;&#38169;&#35823;&#26465;&#20214;&#19979;&#30340;&#39640;&#24863;&#30693;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#23545;&#24863;&#30693;&#33021;&#21147;&#24040;&#22823;&#38656;&#27714;&#30340;&#26080;&#32447;&#36890;&#20449;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#25216;&#26415;&#24212;&#36816;&#32780;&#29983;&#65292;&#20854;&#20013;&#34987;&#21160;&#24863;&#30693;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34987;&#21160;&#24863;&#30693;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#36890;&#20449;&#35299;&#35843;&#38169;&#35823;&#30340;&#26465;&#20214;&#19979;&#22914;&#20309;&#23454;&#29616;&#39640;&#24863;&#30693;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ISAC-NET&#30340;ISAC&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#23558;&#34987;&#21160;&#24863;&#30693;&#19982;&#36890;&#20449;&#20449;&#21495;&#26816;&#27979;&#30456;&#32467;&#21512;&#12290;&#19982;&#29616;&#26377;&#30340;&#39318;&#20808;&#35299;&#35843;&#20256;&#36755;&#30340;&#31526;&#21495;&#65292;&#28982;&#21518;&#20174;&#35299;&#35843;&#30340;&#31526;&#21495;&#20013;&#33719;&#24471;&#34987;&#21160;&#24863;&#30693;&#32467;&#26524;&#30340;&#34987;&#21160;&#24863;&#30693;&#31639;&#27861;&#19981;&#21516;&#65292;ISAC-NET&#21516;&#26102;&#33719;&#24471;&#34987;&#21160;&#24863;&#30693;&#32467;&#26524;&#21644;&#36890;&#20449;&#35299;&#35843;&#31526;&#21495;&#12290;&#19981;&#21516;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;DL&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22359;&#22788;&#29702;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#23558;ISAC-NET&#21010;&#20998;&#20026;&#34987;&#21160;&#24863;&#30693;&#27169;&#22359;&#12289;&#20449;&#21495;&#26816;&#27979;&#27169;&#22359;&#21644;&#20449;&#36947;&#37325;&#26500;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in wireless communication with the enormous demands of sensing ability have given rise to the integrated sensing and communication (ISAC) technology, among which passive sensing plays an important role. The main challenge of passive sensing is how to achieve high sensing performance in the condition of communication demodulation errors. In this paper, we propose an ISAC network (ISAC-NET) that combines passive sensing with communication signal detection by using model-driven deep learning (DL). Dissimilar to existing passive sensing algorithms that first demodulate the transmitted symbols and then obtain passive sensing results from the demodulated symbols, ISAC-NET obtains passive sensing results and communication demodulated symbols simultaneously. Different from the data-driven DL method, we adopt the block-by-block signal processing method that divides the ISAC-NET into the passive sensing module, signal detection module and channel reconstruction module. From the s
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#20449;&#24687;&#30340;&#20808;&#39564;&#20998;&#24067;&#27169;&#22411;Q-SAVI&#65292;&#33021;&#22815;&#35299;&#20915;&#33647;&#29289;&#21457;&#29616;&#20013;&#26631;&#31614;&#25968;&#25454;&#31232;&#32570;&#21644;&#29305;&#24449;&#36716;&#31227;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36879;&#26126;&#19988;&#27010;&#29575;&#19978;&#21512;&#29702;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#24314;&#27169;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.15073</link><description>&lt;p&gt;
&#22312;&#29305;&#24449;&#36716;&#31227;&#26465;&#20214;&#19979;&#21033;&#29992;&#22522;&#20110;&#39046;&#22495;&#20449;&#24687;&#30340;&#20808;&#39564;&#20998;&#24067;&#36827;&#34892;&#33647;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Drug Discovery under Covariate Shift with Domain-Informed Prior Distributions over Functions. (arXiv:2307.15073v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15073
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#20449;&#24687;&#30340;&#20808;&#39564;&#20998;&#24067;&#27169;&#22411;Q-SAVI&#65292;&#33021;&#22815;&#35299;&#20915;&#33647;&#29289;&#21457;&#29616;&#20013;&#26631;&#31614;&#25968;&#25454;&#31232;&#32570;&#21644;&#29305;&#24449;&#36716;&#31227;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36879;&#26126;&#19988;&#27010;&#29575;&#19978;&#21512;&#29702;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#24314;&#27169;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#36895;&#21457;&#29616;&#26032;&#22411;&#21644;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#33647;&#29289;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#21046;&#33647;&#38382;&#39064;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20854;&#20013;&#25198;&#28436;&#30528;&#24840;&#21457;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#33647;&#29289;&#21457;&#29616;&#20219;&#21153;&#36890;&#24120;&#29305;&#28857;&#26159;&#26631;&#31614;&#25968;&#25454;&#31232;&#32570;&#21644;&#26174;&#33879;&#30340;&#29305;&#24449;&#36716;&#31227;&#65292;&#36825;&#23545;&#20110;&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Q-SAVI&#65292;&#36825;&#26159;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26174;&#24335;&#20808;&#39564;&#30693;&#35782;&#32534;&#30721;&#20026;&#20989;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#36879;&#26126;&#19988;&#27010;&#29575;&#19978;&#21512;&#29702;&#30340;&#26041;&#24335;&#26469;&#32534;&#30721;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#20559;&#22909;&#12290;&#22522;&#20110;&#19968;&#20010;&#26032;&#39062;&#30340;&#65292;&#39640;&#26631;&#20934;&#30340;&#29983;&#29289;&#27963;&#24615;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#22312;&#22806;&#25512;&#27169;&#24335;&#19979;&#33021;&#22815;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35825;&#23548;&#25968;&#25454;&#36716;&#31227;&#24182;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#29615;&#22659;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;Q-SAVI&#21487;&#20197;&#22312;&#29305;&#24449;&#36716;&#31227;&#26465;&#20214;&#19979;&#26356;&#22909;&#22320;&#36827;&#34892;&#33647;&#29289;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accelerating the discovery of novel and more effective therapeutics is an important pharmaceutical problem in which deep learning is playing an increasingly significant role. However, real-world drug discovery tasks are often characterized by a scarcity of labeled data and significant covariate shift$\unicode{x2013}\unicode{x2013}$a setting that poses a challenge to standard deep learning methods. In this paper, we present Q-SAVI, a probabilistic model able to address these challenges by encoding explicit prior knowledge of the data-generating process into a prior distribution over functions, presenting researchers with a transparent and probabilistically principled way to encode data-driven modeling preferences. Building on a novel, gold-standard bioactivity dataset that facilitates a meaningful comparison of models in an extrapolative regime, we explore different approaches to induce data shift and construct a challenging evaluation setup. We then demonstrate that using Q-SAVI to int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#21335;&#38750;&#30123;&#33495;&#29369;&#35947;&#30456;&#20851;&#25512;&#25991;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65292;&#26816;&#27979;&#20986;COVID-19&#30123;&#33495;&#29369;&#35947;&#24773;&#32490;&#30340;&#23384;&#22312;&#24182;&#23545;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#12290;&#36825;&#23545;&#20110;&#24212;&#23545;&#30123;&#33495;&#29369;&#35947;&#23545;&#20844;&#20849;&#21355;&#29983;&#24037;&#20316;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.15072</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#21335;&#38750;Twitter&#25968;&#25454;&#20013;&#26816;&#27979;COVID-19&#30123;&#33495;&#29369;&#35947;&#24773;&#32490;&#30340;&#23384;&#22312;
&lt;/p&gt;
&lt;p&gt;
Detecting the Presence of COVID-19 Vaccination Hesitancy from South African Twitter Data Using Machine Learning. (arXiv:2307.15072v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#21335;&#38750;&#30123;&#33495;&#29369;&#35947;&#30456;&#20851;&#25512;&#25991;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65292;&#26816;&#27979;&#20986;COVID-19&#30123;&#33495;&#29369;&#35947;&#24773;&#32490;&#30340;&#23384;&#22312;&#24182;&#23545;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#12290;&#36825;&#23545;&#20110;&#24212;&#23545;&#30123;&#33495;&#29369;&#35947;&#23545;&#20844;&#20849;&#21355;&#29983;&#24037;&#20316;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#20851;&#20110;&#21335;&#38750;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#31038;&#20132;&#23186;&#20307;&#30740;&#31350;&#38750;&#24120;&#23569;&#65292;&#20351;&#29992;&#25163;&#21160;&#26631;&#27880;&#26041;&#27861;&#26356;&#26159;&#20964;&#27611;&#40607;&#35282;&#12290;&#30123;&#33495;&#25509;&#31181;&#26159;&#23545;&#25239;&#30123;&#24773;&#30340;&#20027;&#35201;&#25163;&#27573;&#65292;&#20294;&#30123;&#33495;&#29369;&#35947;&#21361;&#21450;&#20844;&#20849;&#21355;&#29983;&#24037;&#20316;&#12290;&#26412;&#30740;&#31350;&#23545;&#19982;&#30123;&#33495;&#29369;&#35947;&#26377;&#20851;&#30340;&#21335;&#38750;&#25512;&#25991;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#26088;&#22312;&#35757;&#32451;AI&#23186;&#20171;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#20998;&#31867;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#26102;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#26469;&#33258;&#21335;&#38750;&#30340;3&#19975;&#26465;&#25512;&#25991;&#65292;&#24182;&#23558;&#20854;&#25163;&#21160;&#26631;&#27880;&#20026;&#19977;&#20010;&#24773;&#24863;&#31867;&#21035;&#20043;&#19968;&#65306;&#31215;&#26497;&#12289;&#28040;&#26497;&#12289;&#20013;&#24615;&#12290;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;LSTM&#12289;&#21452;&#21521;LSTM&#12289;SVM&#12289;BERT-base-cased&#21644;RoBERTa-base&#27169;&#22411;&#65292;&#36890;&#36807;WandB&#24179;&#21488;&#31934;&#24515;&#36873;&#25321;&#21644;&#35843;&#25972;&#20102;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65306;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Very few social media studies have been done on South African user-generated content during the COVID-19 pandemic and even fewer using hand-labelling over automated methods. Vaccination is a major tool in the fight against the pandemic, but vaccine hesitancy jeopardizes any public health effort. In this study, sentiment analysis on South African tweets related to vaccine hesitancy was performed, with the aim of training AI-mediated classification models and assessing their reliability in categorizing UGC. A dataset of 30000 tweets from South Africa were extracted and hand-labelled into one of three sentiment classes: positive, negative, neutral. The machine learning models used were LSTM, bi-LSTM, SVM, BERT-base-cased and the RoBERTa-base models, whereby their hyperparameters were carefully chosen and tuned using the WandB platform. We used two different approaches when we pre-processed our data for comparison: one was semantics-based, while the other was corpus-based. The pre-processi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#30340;&#38598;&#21512;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#26465;&#20214;&#37319;&#26679;&#25581;&#31034;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#27700;&#21360;&#65292;&#24182;&#35777;&#26126;&#35813;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#26816;&#27979;&#38750;&#27861;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.15067</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#25216;&#26415;&#36827;&#34892;&#38598;&#21512;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Set-Membership Inference Attacks using Data Watermarking. (arXiv:2307.15067v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#30340;&#38598;&#21512;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#26465;&#20214;&#37319;&#26679;&#25581;&#31034;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#27700;&#21360;&#65292;&#24182;&#35777;&#26126;&#35813;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#26816;&#27979;&#38750;&#27861;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#27700;&#21360;&#25216;&#26415;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#30340;&#38598;&#21512;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#29983;&#25104;&#27169;&#22411;&#20013;&#36827;&#34892;&#26465;&#20214;&#37319;&#26679;&#65292;&#20197;&#25581;&#31034;&#27880;&#20837;&#21040;&#35757;&#32451;&#25968;&#25454;&#37096;&#20998;&#30340;&#27700;&#21360;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27700;&#21360;&#25216;&#26415;&#26159;&#19968;&#31181;&#26816;&#27979;&#38750;&#21327;&#21830;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a set-membership inference attack for generative models using deep image watermarking techniques. In particular, we demonstrate how conditional sampling from a generative model can reveal the watermark that was injected into parts of the training data. Our empirical results demonstrate that the proposed watermarking technique is a principled approach for detecting the non-consensual use of image data in training generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24809;&#32602;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#32422;&#26463;&#30340;&#33258;&#28982;&#31995;&#32479;&#38598;&#25104;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14940</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#36866;&#24212;&#24809;&#32602;&#26041;&#27861;&#29992;&#20110;&#23558;&#20808;&#39564;&#30693;&#35782;&#32422;&#26463;&#38598;&#25104;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;
&lt;/p&gt;
&lt;p&gt;
A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs. (arXiv:2307.14940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24809;&#32602;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#32422;&#26463;&#30340;&#33258;&#28982;&#31995;&#32479;&#38598;&#25104;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(Neural ODEs)&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#33258;&#28982;&#31995;&#32479;&#30340;&#36830;&#32493;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#20934;&#30830;&#32780;&#26377;&#24847;&#20041;&#30340;&#39044;&#27979;&#65292;&#27169;&#22411;&#24517;&#39035;&#36981;&#24490;&#31649;&#29702;&#36825;&#20123;&#31995;&#32479;&#30340;&#22522;&#26412;&#35268;&#24459;&#25110;&#23450;&#24459;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24809;&#32602;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#32422;&#26463;&#30340;&#33258;&#28982;&#31995;&#32479;&#38598;&#25104;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#12290;&#25152;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#24809;&#32602;&#20989;&#25968;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#24809;&#32602;&#21442;&#25968;&#12290;&#24341;&#20837;&#20808;&#39564;&#30693;&#35782;&#26377;&#21161;&#20110;&#22686;&#21152;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#19977;&#20010;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#32422;&#26463;&#30340;&#33258;&#28982;&#31995;&#32479;(&#20154;&#21475;&#22686;&#38271;&#65292;&#21270;&#23398;&#21453;&#24212;&#28436;&#21270;&#21644;&#38459;&#23612;&#35856;&#25391;&#36816;&#21160;)&#26469;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#24809;&#32602;&#31639;&#27861;&#22312;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#20854;&#20182;&#24809;&#32602;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26041;&#27861;&#21644;&#21407;&#22987;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuous dynamics of natural systems has been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs). However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems. In this work, we propose a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. The proposed self-adaptive penalty function can dynamically adjust the penalty parameters. The explicit introduction of prior knowledge helps to increase the interpretability of Neural ODE -based models. We validate the proposed approach by modelling three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion. The numerical experiments and a comparison with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE, demonstrate the effectiveness of the proposed self-adaptive penalty algorithm for Neural
&lt;/p&gt;</description></item><item><title>Desbordante&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#39564;&#35777;&#22797;&#26434;&#32479;&#35745;&#20449;&#24687;&#26469;&#24110;&#21161;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#23478;&#36827;&#34892;&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#19982;&#29616;&#26377;&#24037;&#20855;&#30340;&#36866;&#24403;&#38598;&#25104;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#24037;&#19994;&#32423;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#25552;&#20379;&#25551;&#36848;&#24615;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#27169;&#24335;&#32570;&#22833;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2307.14935</link><description>&lt;p&gt;
&#20351;&#29992;Desbordante&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65306;&#19968;&#39033;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Solving Data Quality Problems with Desbordante: a Demo. (arXiv:2307.14935v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14935
&lt;/p&gt;
&lt;p&gt;
Desbordante&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#39564;&#35777;&#22797;&#26434;&#32479;&#35745;&#20449;&#24687;&#26469;&#24110;&#21161;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#23478;&#36827;&#34892;&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#19982;&#29616;&#26377;&#24037;&#20855;&#30340;&#36866;&#24403;&#38598;&#25104;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#24037;&#19994;&#32423;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#25552;&#20379;&#25551;&#36848;&#24615;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#27169;&#24335;&#32570;&#22833;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#26159;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#34892;&#19994;&#20013;&#30340;&#37325;&#35201;&#36807;&#31243;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#21457;&#29616;&#21644;&#39564;&#35777;&#22797;&#26434;&#32479;&#35745;&#20449;&#24687;&#65292;&#21253;&#25324;&#20989;&#25968;&#20381;&#36182;&#12289;&#25968;&#25454;&#32422;&#26463;&#12289;&#20851;&#32852;&#35268;&#21017;&#31561;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#22797;&#26434;&#32479;&#35745;&#30340;&#25968;&#25454;&#27010;&#35201;&#20998;&#26512;&#31995;&#32479;&#22312;&#19982;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#23478;&#20351;&#29992;&#30340;&#24037;&#20855;&#36827;&#34892;&#36866;&#24403;&#38598;&#25104;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#22312;&#34892;&#19994;&#20013;&#23545;&#36825;&#20123;&#24037;&#20855;&#37319;&#29992;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#31995;&#32479;&#24182;&#19981;&#32771;&#34385;&#24037;&#19994;&#32423;&#24037;&#20316;&#36127;&#36733;&#12290;&#26368;&#21518;&#65292;&#23427;&#20204;&#19981;&#26088;&#22312;&#25552;&#20379;&#25551;&#36848;&#24615;&#30340;&#35299;&#37322;&#65292;&#21363;&#20026;&#20160;&#20040;&#25214;&#19981;&#21040;&#32473;&#23450;&#30340;&#27169;&#24335;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22240;&#20026;&#20102;&#35299;&#29305;&#23450;&#27169;&#24335;&#32570;&#22833;&#30340;&#26681;&#26412;&#21407;&#22240;&#23545;&#22522;&#20110;&#25968;&#25454;&#30340;&#26126;&#26234;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#24335;&#23454;&#38469;&#19978;&#26159;&#28040;&#22833;&#22312;&#31354;&#20013;&#20013;&#65306;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#30456;&#23545;&#26377;&#38480;&#65292;&#24456;&#23569;&#34987;&#24191;&#22823;&#20844;&#20247;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data profiling is an essential process in modern data-driven industries. One of its critical components is the discovery and validation of complex statistics, including functional dependencies, data constraints, association rules, and others.  However, most existing data profiling systems that focus on complex statistics do not provide proper integration with the tools used by contemporary data scientists. This creates a significant barrier to the adoption of these tools in the industry. Moreover, existing systems were not created with industrial-grade workloads in mind. Finally, they do not aim to provide descriptive explanations, i.e. why a given pattern is not found. It is a significant issue as it is essential to understand the underlying reasons for a specific pattern's absence to make informed decisions based on the data.  Because of that, these patterns are effectively rest in thin air: their application scope is rather limited, they are rarely used by the broader public. At the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;(MVMR-FS)&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#24230;&#37327;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#20887;&#20313;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#20934;&#21017;(MVMR)&#65292;&#20197;&#36741;&#21161;&#29305;&#24449;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2307.14643</link><description>&lt;p&gt;
MVMR-FS&#65306;&#22522;&#20110;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy. (arXiv:2307.14643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;(MVMR-FS)&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#24230;&#37327;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#20887;&#20313;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#20934;&#21017;(MVMR)&#65292;&#20197;&#36741;&#21161;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20934;&#30830;&#24230;&#37327;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#21644;&#20887;&#20313;&#26159;&#29305;&#24449;&#36873;&#25321;&#39046;&#22495;&#30340;&#19968;&#20010;&#21476;&#32769;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#36807;&#28388;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24230;&#37327;&#36830;&#32493;&#25968;&#25454;&#30340;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#25351;&#23450;&#29305;&#24449;&#25968;&#37327;&#65292;&#36825;&#22312;&#27809;&#26377;&#19987;&#23478;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#30340;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#31616;&#31216;&#20026;MVMR-FS&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26680;&#23494;&#24230;&#20272;&#35745;&#65292;&#20197;&#25429;&#25417;&#23427;&#20204;&#22312;&#31867;&#38388;&#21644;&#25972;&#20307;&#20998;&#24067;&#20013;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#31867;&#38388;&#24046;&#24322;&#21644;&#26368;&#23567;&#20887;&#20313;&#65288;MVMR&#65289;&#30340;&#20934;&#21017;&#65292;&#20854;&#20013;&#21033;&#29992;&#31867;&#38388;&#27010;&#29575;&#20998;&#24067;&#26469;&#21453;&#26144;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#25972;&#20307;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#37327;&#21270;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to accurately measure the relevance and redundancy of features is an age-old challenge in the field of feature selection. However, existing filter-based feature selection methods cannot directly measure redundancy for continuous data. In addition, most methods rely on manually specifying the number of features, which may introduce errors in the absence of expert knowledge. In this paper, we propose a non-parametric feature selection algorithm based on maximum inter-class variation and minimum redundancy, abbreviated as MVMR-FS. We first introduce supervised and unsupervised kernel density estimation on the features to capture their similarities and differences in inter-class and overall distributions. Subsequently, we present the criteria for maximum inter-class variation and minimum redundancy (MVMR), wherein the inter-class probability distributions are employed to reflect feature relevance and the distances between overall probability distributions are used to quantify redundanc
&lt;/p&gt;</description></item><item><title>ARB&#26159;&#19968;&#20010;&#26032;&#22411;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#29983;&#29289;&#12289;&#21270;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#39640;&#32423;&#25512;&#29702;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#24471;&#20998;&#36828;&#20302;&#20110;50%&#65292;&#20026;&#20102;&#25552;&#39640;&#35780;&#20272;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13692</link><description>&lt;p&gt;
ARB&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#32423;&#25512;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ARB: Advanced Reasoning Benchmark for Large Language Models. (arXiv:2307.13692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13692
&lt;/p&gt;
&lt;p&gt;
ARB&#26159;&#19968;&#20010;&#26032;&#22411;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#29983;&#29289;&#12289;&#21270;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#39640;&#32423;&#25512;&#29702;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#24471;&#20998;&#36828;&#20302;&#20110;50%&#65292;&#20026;&#20102;&#25552;&#39640;&#35780;&#20272;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#23450;&#37327;&#25512;&#29702;&#21644;&#30693;&#35782;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#36824;&#27809;&#26377;&#36798;&#21040;&#19987;&#23478;&#27700;&#24179;&#65292;&#20294;&#35768;&#22810;&#36825;&#20123;&#22522;&#20934;&#38543;&#30528;LLMs&#33719;&#24471;&#36234;&#26469;&#36234;&#39640;&#30340;&#20998;&#25968;&#32780;&#22833;&#21435;&#20102;&#25928;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ARB&#65292;&#19968;&#20010;&#30001;&#22810;&#20010;&#39046;&#22495;&#30340;&#39640;&#32423;&#25512;&#29702;&#38382;&#39064;&#32452;&#25104;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;ARB&#25552;&#20379;&#27604;&#20197;&#21069;&#30340;&#22522;&#20934;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#29983;&#29289;&#12289;&#21270;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;ARB&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#32452;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#21644;&#29289;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#39640;&#32423;&#31526;&#21495;&#25512;&#29702;&#21644;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#36817;&#30340;&#27169;&#22411;&#65292;&#22914;GPT-4&#21644;Claude&#22312;ARB&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#24403;&#21069;&#27169;&#22411;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#24471;&#20998;&#36828;&#20302;&#20110;50%&#12290;&#20026;&#20102;&#25913;&#36827;&#33258;&#21160;&#21644;&#36741;&#21161;&#35780;&#20272;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20801;&#35768;GPT-4&#23545;&#20854;&#33258;&#36523;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct 
&lt;/p&gt;</description></item><item><title>Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13494</link><description>&lt;p&gt;
Duet: &#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13494
&lt;/p&gt;
&lt;p&gt;
Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30001;&#20110;&#22312;&#22788;&#29702;&#33539;&#22260;&#26597;&#35810;&#26102;&#20351;&#29992;&#30340;&#37319;&#26679;&#26041;&#27861;&#32780;&#23548;&#33268;&#20272;&#35745;&#25104;&#26412;&#36739;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#37319;&#26679;&#26041;&#27861;&#20063;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#65292;&#22240;&#27492;&#26469;&#33258;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#30340;&#30417;&#30563;&#20449;&#21495;&#24456;&#38590;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#22522;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#30830;&#23450;&#24615;&#24314;&#27169;&#26041;&#27861;&#65288;Duet&#65289;&#29992;&#20110;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;Duet&#21487;&#20197;&#20197;&#26356;&#20302;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#30452;&#25509;&#20272;&#35745;&#33539;&#22260;&#26597;&#35810;&#30340;&#22522;&#25968;&#65292;&#24182;&#19988;&#20197;&#21487;&#21306;&#20998;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#30001;&#20110;&#27492;&#26041;&#27861;&#30340;&#39044;&#27979;&#36807;&#31243;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20272;&#35745;&#35823;&#24046;&#36739;&#22823;&#30340;&#26597;&#35810;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#25193;&#23637;&#20102;split conformal prediction&#25216;&#26415;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#20005;&#37325;&#24615;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#20102;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.13124</link><description>&lt;p&gt;
&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#30340;&#31526;&#21512;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction for frequency-severity modeling. (arXiv:2307.13124v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13124
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#25193;&#23637;&#20102;split conformal prediction&#25216;&#26415;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#20005;&#37325;&#24615;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#20102;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#23558;&#20998;&#21106;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#25193;&#23637;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#24403;&#22522;&#30784;&#20005;&#37325;&#24615;&#27169;&#22411;&#26159;&#38543;&#26426;&#26862;&#26519;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20004;&#38454;&#27573;&#20998;&#21106;&#31526;&#21512;&#24615;&#39044;&#27979;&#36807;&#31243;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#30149;&#20154;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;</title><link>http://arxiv.org/abs/2307.11465</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#25972;&#20307;&#29983;&#23384;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach for Overall Survival Analysis with Missing Values. (arXiv:2307.11465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11465
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#30149;&#20154;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#24212;&#29992;&#20110;&#32954;&#30284;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#23545;&#20110;&#30149;&#20154;&#29366;&#24577;&#30340;&#25972;&#20307;&#29983;&#23384;&#65288;OS&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#25351;&#26631;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#29983;&#23384;&#27010;&#29575;&#19981;&#21516;&#30340;&#20122;&#32452;&#65292;&#20174;&#32780;&#23454;&#29616;&#20010;&#20307;&#21270;&#27835;&#30103;&#21644;&#25913;&#21892;&#25972;&#20307;&#29983;&#23384;&#29575;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#65292;&#38656;&#35201;&#32771;&#34385;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#27599;&#20010;&#30149;&#20154;&#30340;&#21487;&#29992;&#20449;&#24687;&#65292;&#21033;&#29992;&#26410;&#34987;&#23457;&#26597;&#30340;&#65288;&#21363;&#27515;&#20129;&#65289;&#21644;&#34987;&#23457;&#26597;&#30340;&#65288;&#21363;&#24184;&#23384;&#32773;&#65289;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#20063;&#35201;&#32771;&#34385;&#21040;&#27515;&#20129;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;&#19981;&#23436;&#25972;&#25968;&#25454;&#22788;&#29702;&#26159;&#21307;&#23398;&#39046;&#22495;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#25554;&#34917;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#20010;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30340;&#30149;&#20154;&#21450;&#20854;&#21487;&#29992;&#29305;&#24449;&#20013;&#26377;&#25928;&#23398;&#20064;&#65292;&#39044;&#27979;NSCLC&#30149;&#20154;&#30340;OS&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most challenging fields where Artificial Intelligence (AI) can be applied is lung cancer research, specifically non-small cell lung cancer (NSCLC). In particular, overall survival (OS) is a vital indicator of patient status, helping to identify subgroups with diverse survival probabilities, enabling tailored treatment and improved OS rates. In this analysis, there are two challenges to take into account. First, few studies effectively exploit the information available from each patient, leveraging both uncensored (i.e., dead) and censored (i.e., survivors) patients, considering also the death times. Second, the handling of incomplete data is a common issue in the medical field. This problem is typically tackled through the use of imputation methods. Our objective is to present an AI model able to overcome these limits, effectively learning from both censored and uncensored patients and their available features, for the prediction of OS for NSCLC patients. We present a novel 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07686</link><description>&lt;p&gt;
&#21019;&#24314;&#19968;&#20010;&#25903;&#25345;OpenMP Fortran&#21644;C++&#20195;&#30721;&#30456;&#20114;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#65292;&#25105;&#20204;&#30830;&#20445;&#20102;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#65288;CodeBLEU&#65289;&#21644;&#23450;&#24615;&#65288;&#20154;&#24037;&#35780;&#20272;&#65289;&#26041;&#27861;&#35780;&#20272;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#32534;&#30721;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;5.1&#20493;&#65292;&#23545;&#20110;&#20855;&#26377;&#19968;&#23450;&#32534;&#30721;&#29087;&#24713;&#24230;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;9.9&#20493;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#20195;&#30721;&#32763;&#35793;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#29305;&#24449;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#38024;&#23545;&#31232;&#30095;&#34920;&#38754;&#21387;&#21147;&#20256;&#24863;&#22120;&#19979;&#30340;&#24490;&#29615;&#27668;&#32568;&#27969;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#38459;&#21147;&#21644;&#20943;&#23567;&#21319;&#21147;&#27874;&#21160;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#23558;&#20256;&#24863;&#22120;&#20449;&#21495;&#25552;&#21462;&#20026;&#21160;&#24577;&#29305;&#24449;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#24182;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#21160;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#24471;&#25511;&#21046;&#24615;&#33021;&#33021;&#22815;&#22312;&#19981;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#31232;&#30095;&#20256;&#24863;&#22120;&#24863;&#30693;&#27969;&#21160;&#12290;&#35813;&#26041;&#27861;&#22312;&#38459;&#21147;&#31995;&#25968;&#21644;&#21319;&#21147;&#31995;&#25968;&#30340;&#25913;&#21892;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01995</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#29305;&#24449;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#31232;&#30095;&#34920;&#38754;&#21387;&#21147;&#20256;&#24863;&#22120;&#19979;&#30340;&#24490;&#29615;&#27668;&#32568;&#27969;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dynamic Feature-based Deep Reinforcement Learning for Flow Control of Circular Cylinder with Sparse Surface Pressure Sensing. (arXiv:2307.01995v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#29305;&#24449;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#38024;&#23545;&#31232;&#30095;&#34920;&#38754;&#21387;&#21147;&#20256;&#24863;&#22120;&#19979;&#30340;&#24490;&#29615;&#27668;&#32568;&#27969;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#38459;&#21147;&#21644;&#20943;&#23567;&#21319;&#21147;&#27874;&#21160;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#23558;&#20256;&#24863;&#22120;&#20449;&#21495;&#25552;&#21462;&#20026;&#21160;&#24577;&#29305;&#24449;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#24182;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#21160;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#24471;&#25511;&#21046;&#24615;&#33021;&#33021;&#22815;&#22312;&#19981;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#31232;&#30095;&#20256;&#24863;&#22120;&#24863;&#30693;&#27969;&#21160;&#12290;&#35813;&#26041;&#27861;&#22312;&#38459;&#21147;&#31995;&#25968;&#21644;&#21319;&#21147;&#31995;&#25968;&#30340;&#25913;&#21892;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#31639;&#27861;&#65292;&#38024;&#23545;&#31232;&#30095;&#20256;&#24863;&#22120;&#20449;&#24687;&#30340;&#23553;&#38381;&#29615;&#27668;&#32568;&#23614;&#27969;&#25511;&#21046;&#65292;&#20197;&#38477;&#20302;&#38459;&#21147;&#21644;&#20943;&#23567;&#21319;&#21147;&#30340;&#27874;&#21160;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#36215;&#22987;&#28857;&#65292;&#24182;&#36890;&#36807;&#23558;&#20256;&#24863;&#22120;&#20449;&#21495;&#25552;&#21462;&#20026;&#21160;&#24577;&#29305;&#24449;&#65288;DF&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;DRL&#24615;&#33021;&#65292;&#20174;&#32780;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#21160;&#29366;&#24577;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#22522;&#20110;&#21160;&#24577;&#29305;&#24449;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DF-DRL&#65289;&#22312;&#27809;&#26377;&#21160;&#24577;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#23398;&#20064;&#20102;&#19968;&#20010;&#22312;&#31995;&#32479;&#20013;&#30340;&#21453;&#39304;&#25511;&#21046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#30452;&#25509;&#20256;&#24863;&#22120;&#21453;&#39304;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;DF-DRL&#27169;&#22411;&#30340;&#38459;&#21147;&#31995;&#25968;&#20943;&#23567;&#20102;25%&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20165;&#20351;&#29992;&#19968;&#20010;&#34920;&#38754;&#21387;&#21147;&#20256;&#24863;&#22120;&#65292;DF-DRL&#21487;&#20197;&#23558;&#38459;&#21147;&#31995;&#25968;&#38477;&#20302;&#21040;Re = 100&#26102;&#32422;8%&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#33879;&#20943;&#36731;&#20102;&#21319;&#21147;&#31995;&#25968;&#30340;&#27874;&#21160;&#12290;&#22240;&#27492;&#65292;DF-DRL&#20801;&#35768;&#22312;&#19981;&#38477;&#20302;&#25511;&#21046;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#31232;&#30095;&#20256;&#24863;&#22120;&#23545;&#27969;&#21160;&#36827;&#34892;&#24863;&#30693;&#12290;&#27492;&#26041;&#27861;&#36824;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a self-learning algorithm for closed-loop cylinder wake control targeting lower drag and lower lift fluctuations with the additional challenge of sparse sensor information, taking deep reinforcement learning as the starting point. DRL performance is significantly improved by lifting the sensor signals to dynamic features (DF), which predict future flow states. The resulting dynamic feature-based DRL (DF-DRL) automatically learns a feedback control in the plant without a dynamic model. Results show that the drag coefficient of the DF-DRL model is 25% less than the vanilla model based on direct sensor feedback. More importantly, using only one surface pressure sensor, DF-DRL can reduce the drag coefficient to a state-of-the-art performance of about 8% at Re = 100 and significantly mitigate lift coefficient fluctuations. Hence, DF-DRL allows the deployment of sparse sensing of the flow without degrading the control performance. This method also shows good robustness in
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#27969;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#21487;&#25511;&#21512;&#25104;&#35299;&#21078;&#23398;&#34394;&#25311;&#20154;&#32676;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#20851;&#21327;&#21464;&#37327;&#65292;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#30340;&#30446;&#26631;&#20154;&#32676;/&#29305;&#24449;&#21512;&#25104;&#34394;&#25311;&#20154;&#32676;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14680</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21487;&#25511;&#21512;&#25104;&#35299;&#21078;&#23398;&#34394;&#25311;&#20154;&#32676;&#30340;&#26465;&#20214;&#27969;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy. (arXiv:2306.14680v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14680
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#27969;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#21487;&#25511;&#21512;&#25104;&#35299;&#21078;&#23398;&#34394;&#25311;&#20154;&#32676;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#20851;&#21327;&#21464;&#37327;&#65292;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#30340;&#30446;&#26631;&#20154;&#32676;/&#29305;&#24449;&#21512;&#25104;&#34394;&#25311;&#20154;&#32676;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#20154;&#32676;&#30340;&#29983;&#25104;&#23545;&#20110;&#36827;&#34892;&#21307;&#30103;&#35774;&#22791;&#30340;&#35745;&#31639;&#26426;&#27169;&#25311;&#35797;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#24120;&#65292;&#29983;&#25104;&#30340;&#34394;&#25311;&#20154;&#32676;&#24212;&#35813;&#25429;&#25417;&#21040;&#36275;&#22815;&#30340;&#21464;&#24322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#20449;&#24230;&#65292;&#24182;&#21453;&#26144;&#30495;&#23454;&#20154;&#32676;&#20013;&#35266;&#23519;&#21040;&#30340;&#24739;&#32773;&#30340;&#29305;&#23450;&#29305;&#24449;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#29305;&#24449;&#12290;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#65292;&#24076;&#26395;&#20197;&#8220;&#21487;&#25511;&#8221;&#30340;&#26041;&#24335;&#21512;&#25104;&#34394;&#25311;&#20154;&#32676;&#65292;&#21363;&#20351;&#29992;&#30456;&#20851;&#21327;&#21464;&#37327;&#20197;&#26465;&#20214;&#21512;&#25104;&#34394;&#25311;&#20154;&#32676;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#30340;&#30446;&#26631;&#20154;&#32676;/&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(cVAE)&#19982;&#27491;&#21017;&#21270;&#27969;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#23398;&#20064;&#21040;&#30340;&#36817;&#20284;&#21518;&#39564;&#30340;&#28789;&#27963;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#21512;&#25104;&#35299;&#21078;&#32467;&#26500;&#34394;&#25311;&#20154;&#32676;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;2360&#21517;&#24739;&#32773;&#20013;&#33719;&#21462;&#30340;&#24515;&#33039;&#24038;&#23460;&#25968;&#25454;&#38598;&#21644;&#30456;&#20851;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#26465;&#20214;&#27969;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of virtual populations (VPs) of anatomy is essential for conducting in silico trials of medical devices. Typically, the generated VP should capture sufficient variability while remaining plausible and should reflect the specific characteristics and demographics of the patients observed in real populations. In several applications, it is desirable to synthesise virtual populations in a \textit{controlled} manner, where relevant covariates are used to conditionally synthesise virtual populations that fit a specific target population/characteristics. We propose to equip a conditional variational autoencoder (cVAE) with normalising flows to boost the flexibility and complexity of the approximate posterior learnt, leading to enhanced flexibility for controllable synthesis of VPs of anatomical structures. We demonstrate the performance of our conditional flow VAE using a data set of cardiac left ventricles acquired from 2360 patients, with associated demographic information an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857; Forney &#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#28040;&#24687;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#65292;&#24182;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#12290;</title><link>http://arxiv.org/abs/2306.05965</link><description>&lt;p&gt;
&#22312;&#22240;&#23376;&#22270;&#20013;&#33258;&#21160;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857; Forney &#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#28040;&#24687;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#65292;&#24182;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#65292;&#36125;&#21494;&#26031;&#29366;&#24577;&#21644;&#21442;&#25968;&#20272;&#35745;&#24050;&#32463;&#34987;&#26377;&#25928;&#33258;&#21160;&#21270;&#65292;&#20294;&#23545;&#20110;&#27169;&#22411;&#27604;&#36739;&#23578;&#26410;&#22914;&#27492;&#65292;&#22240;&#27492;&#20173;&#38656;&#35201;&#23481;&#26131;&#20986;&#38169;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#25512;&#23548;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#27604;&#36739;&#32463;&#24120;&#34987;&#24573;&#35270;&#21644;&#24573;&#30053;&#65292;&#23613;&#31649;&#23427;&#24456;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;Forney&#26679;&#24335;&#30340;&#22240;&#23376;&#22270;&#19978;&#20351;&#29992;&#33258;&#23450;&#20041;&#28151;&#21512;&#33410;&#28857;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#26469;&#39640;&#25928;&#22320;&#33258;&#21160;&#21270;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#36873;&#25321;&#21644;&#32452;&#21512;&#12290;&#36827;&#32780;&#21487;&#20351;&#29992;&#32553;&#25918;&#22240;&#23376;&#21516;&#26102;&#25191;&#34892;&#21442;&#25968;&#21644;&#29366;&#24577;&#25512;&#26029;&#20197;&#21450;&#27169;&#22411;&#27604;&#36739;&#12290;&#36825;&#31181;&#26041;&#27861;&#32553;&#30701;&#20102;&#27169;&#22411;&#35774;&#35745;&#21608;&#26399;&#65292;&#21516;&#26102;&#20801;&#35768;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#20998;&#23618;&#21644;&#26102;&#38388;&#27169;&#22411;&#20808;&#39564;&#65292;&#20197;&#36866;&#24212;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#21464;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian state and parameter estimation have been automated effectively in the literature, however, this has not yet been the case for model comparison, which therefore still requires error-prone and time-consuming manual derivations. As a result, model comparison is often overlooked and ignored, despite its importance. This paper efficiently automates Bayesian model averaging, selection, and combination by message passing on a Forney-style factor graph with a custom mixture node. Parameter and state inference, and model comparison can then be executed simultaneously using message passing with scale factors. This approach shortens the model design cycle and allows for the straightforward extension to hierarchical and temporal model priors to accommodate for modeling complicated time-varying processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SACSoN&#30340;&#33258;&#20027;&#23548;&#33322;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20154;&#31867;&#21344;&#29992;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35270;&#35273;&#29702;&#35299;&#21644;&#23398;&#20064;&#65292;&#33258;&#20027;&#25910;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25968;&#25454;&#38598;&#25299;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.01874</link><description>&lt;p&gt;
SACSoN&#65306;&#38754;&#21521;&#31038;&#20132;&#23548;&#33322;&#30340;&#21487;&#25193;&#23637;&#33258;&#20027;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SACSoN: Scalable Autonomous Data Collection for Social Navigation. (arXiv:2306.01874v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SACSoN&#30340;&#33258;&#20027;&#23548;&#33322;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20154;&#31867;&#21344;&#29992;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35270;&#35273;&#29702;&#35299;&#21644;&#23398;&#20064;&#65292;&#33258;&#20027;&#25910;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25968;&#25454;&#38598;&#25299;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20026;&#26500;&#24314;&#31526;&#21512;&#31038;&#20132;&#35268;&#33539;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#36229;&#36234;&#20102;&#23545;&#20154;&#31867;&#34892;&#20026;&#30340;&#31616;&#21333;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#29702;&#35299;&#36807;&#21435;&#32463;&#39564;&#20013;&#30340;&#20154;&#31867;&#20132;&#20114;&#65292;&#23398;&#20064;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#31038;&#20132;&#23548;&#33322;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#20154;&#31867;&#21344;&#29992;&#30340;&#29615;&#22659;&#20013;&#25910;&#38598;&#23548;&#33322;&#25968;&#25454;&#21487;&#33021;&#38656;&#35201;&#36828;&#31243;&#25805;&#20316;&#25110;&#25345;&#32493;&#30417;&#35270;&#65292;&#20351;&#24471;&#36825;&#20010;&#36807;&#31243;&#38590;&#20197;&#25193;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#35270;&#35273;&#23548;&#33322;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;SACSoN&#65292;&#21487;&#20197;&#33258;&#20027;&#23548;&#33322;&#20110;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#34892;&#20154;&#21608;&#22260;&#65292;&#24182;&#40723;&#21169;&#20016;&#23500;&#30340;&#20132;&#20114;&#12290;SACSoN&#20351;&#29992;&#35270;&#35273;&#35266;&#23519;&#26469;&#35266;&#23519;&#21644;&#22238;&#24212;&#20854;&#38468;&#36817;&#30340;&#20154;&#31867;&#12290;&#23427;&#23558;&#36825;&#31181;&#35270;&#35273;&#29702;&#35299;&#19982;&#25345;&#32493;&#30340;&#23398;&#20064;&#21644;&#33258;&#20027;&#30896;&#25758;&#24674;&#22797;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20154;&#25805;&#20316;&#21592;&#30340;&#21442;&#19982;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25193;&#23637;&#20102;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#31995;&#32479;&#26469;&#25910;&#38598;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Machine learning provides a powerful tool for building socially compliant robotic systems that go beyond simple predictive models of human behavior. By observing and understanding human interactions from past experiences, learning can enable effective social navigation behaviors directly from data. However, collecting navigation data in human-occupied environments may require teleoperation or continuous monitoring, making the process prohibitively expensive to scale. In this paper, we present a scalable data collection system for vision-based navigation, SACSoN, that can autonomously navigate around pedestrians in challenging real-world environments while encouraging rich interactions. SACSoN uses visual observations to observe and react to humans in its vicinity. It couples this visual understanding with continual learning and an autonomous collision recovery system that limits the involvement of a human operator, allowing for better dataset scaling. We use a this system to collect th
&lt;/p&gt;</description></item><item><title>FACT&#26159;&#19968;&#31181;&#32852;&#37030;&#25932;&#23545;&#20132;&#21449;&#35757;&#32451;&#31639;&#27861;&#65292;&#21033;&#29992;&#28304;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38544;&#24335;&#39046;&#22495;&#24046;&#24322;&#26469;&#35782;&#21035;&#30446;&#26631;&#39046;&#22495;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FACT&#22312;&#22810;&#28304;&#21333;&#30446;&#26631;&#22522;&#20934;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#19988;&#20248;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.00607</link><description>&lt;p&gt;
FACT: &#32852;&#37030;&#25932;&#23545;&#20132;&#21449;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FACT: Federated Adversarial Cross Training. (arXiv:2306.00607v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00607
&lt;/p&gt;
&lt;p&gt;
FACT&#26159;&#19968;&#31181;&#32852;&#37030;&#25932;&#23545;&#20132;&#21449;&#35757;&#32451;&#31639;&#27861;&#65292;&#21033;&#29992;&#28304;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38544;&#24335;&#39046;&#22495;&#24046;&#24322;&#26469;&#35782;&#21035;&#30446;&#26631;&#39046;&#22495;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FACT&#22312;&#22810;&#28304;&#21333;&#30446;&#26631;&#22522;&#20934;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#19988;&#20248;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21487;&#20197;&#20419;&#36827;&#20998;&#24067;&#24335;&#27169;&#22411;&#24320;&#21457;&#65292;&#20197;&#32858;&#21512;&#22810;&#20010;&#26426;&#23494;&#25968;&#25454;&#28304;&#12290;&#23458;&#25143;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#36755;&#21487;&#33021;&#21463;&#21040;&#20998;&#24067;&#24046;&#24322;&#30340;&#24433;&#21709;&#65292;&#21363;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223;&#26159;&#23558;&#32852;&#37030;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#23458;&#25143;&#31471;&#65292;&#32780;&#35813;&#23458;&#25143;&#31471;&#26080;&#27861;&#35775;&#38382;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#25932;&#23545;&#20132;&#21449;&#35757;&#32451;&#65288;FACT&#65289;&#65292;&#21033;&#29992;&#28304;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38544;&#24335;&#39046;&#22495;&#24046;&#24322;&#26469;&#35782;&#21035;&#30446;&#26631;&#39046;&#22495;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#22312;FL&#30340;&#27599;&#19968;&#36718;&#20013;&#65292;FACT&#20132;&#21449;&#21021;&#22987;&#21270;&#19968;&#23545;&#28304;&#23458;&#25143;&#31471;&#65292;&#29983;&#25104;&#19987;&#29992;&#20110;&#39046;&#22495;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#30452;&#25509;&#23545;&#25163;&#26469;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;FACT&#22312;&#19977;&#20010;&#27969;&#34892;&#30340;&#22810;&#28304;&#21333;&#30446;&#26631;&#22522;&#20934;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#12289;&#38750;&#32852;&#37030;&#21644;&#26080;&#28304;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#65292;&#24182;&#19988;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) facilitates distributed model development to aggregate multiple confidential data sources. The information transfer among clients can be compromised by distributional differences, i.e., by non-i.i.d. data. A particularly challenging scenario is the federated model adaptation to a target client without access to annotated data. We propose Federated Adversarial Cross Training (FACT), which uses the implicit domain differences between source clients to identify domain shifts in the target domain. In each round of FL, FACT cross initializes a pair of source clients to generate domain specialized representations which are then used as a direct adversary to learn a domain invariant data representation. We empirically show that FACT outperforms state-of-the-art federated, non-federated and source-free domain adaptation models on three popular multi-source-single-target benchmarks, and state-of-the-art Unsupervised Domain Adaptation (UDA) models on single-source-single-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Hugging Face&#19978;1,417&#20010;ML&#27169;&#22411;&#21450;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30899;&#36275;&#36857;&#27979;&#37327;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#26377;&#20851;&#22914;&#20309;&#25253;&#21578;&#21644;&#20248;&#21270;ML&#27169;&#22411;&#30340;&#30899;&#25928;&#29575;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.11164</link><description>&lt;p&gt;
&#25506;&#32034;&#25265;&#25265;&#33080;ML&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#65306;&#19968;&#39033;&#23384;&#20648;&#24211;&#25366;&#25496;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study. (arXiv:2305.11164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Hugging Face&#19978;1,417&#20010;ML&#27169;&#22411;&#21450;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30899;&#36275;&#36857;&#27979;&#37327;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#26377;&#20851;&#22914;&#20309;&#25253;&#21578;&#21644;&#20248;&#21270;ML&#27169;&#22411;&#30340;&#30899;&#25928;&#29575;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#31995;&#32479;&#30340;&#23835;&#36215;&#21152;&#21095;&#20102;&#23427;&#20204;&#30340;&#30899;&#36275;&#36857;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#22686;&#21152;&#30340;&#33021;&#21147;&#21644;&#27169;&#22411;&#22823;&#23567;&#25152;&#33268;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;ML&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#22914;&#20309;&#23454;&#38469;&#27979;&#37327;&#12289;&#25253;&#21578;&#21644;&#35780;&#20272;&#30340;&#35748;&#35782;&#30456;&#23545;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26412;&#35770;&#25991;&#26088;&#22312;&#20998;&#26512;&#22312;Hugging Face&#19978;1,417&#20010;ML&#27169;&#22411;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30899;&#36275;&#36857;&#27979;&#37327;&#24773;&#20917;&#65292;Hugging Face&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#39044;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#23384;&#20648;&#24211;&#12290;&#30446;&#26631;&#26159;&#25552;&#20379;&#26377;&#20851;&#22914;&#20309;&#25253;&#21578;&#21644;&#20248;&#21270;ML&#27169;&#22411;&#30340;&#30899;&#25928;&#29575;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;Hugging Face Hub API&#19978;&#26377;&#20851;&#30899;&#25490;&#25918;&#30340;&#31532;&#19968;&#39033;&#23384;&#20648;&#24211;&#25366;&#25496;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;(1) ML&#27169;&#22411;&#30340;&#21019;&#24314;&#32773;&#22914;&#20309;&#22312;Hugging Face Hub&#19978;&#27979;&#37327;&#21644;&#25253;&#21578;&#30899;&#25490;&#25918;&#65311;(2) &#21738;&#20123;&#26041;&#38754;&#24433;&#21709;&#20102;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#30899;&#25490;&#25918;&#65311;&#35813;&#30740;&#31350;&#24471;&#20986;&#20102;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#12290;&#20854;&#20013;&#21253;&#25324;&#30899;&#25490;&#25918;&#25253;&#21578;&#27169;&#24335;&#27604;&#20363;&#30340;&#36880;&#27493;&#19979;&#38477;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of machine learning (ML) systems has exacerbated their carbon footprint due to increased capabilities and model sizes. However, there is scarce knowledge on how the carbon footprint of ML models is actually measured, reported, and evaluated. In light of this, the paper aims to analyze the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face, which is the most popular repository for pretrained ML models. The goal is to provide insights and recommendations on how to report and optimize the carbon efficiency of ML models. The study includes the first repository mining study on the Hugging Face Hub API on carbon emissions. This study seeks to answer two research questions: (1) how do ML model creators measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects impact the carbon emissions of training ML models? The study yielded several key findings. These include a decreasing proportion of carbon emissions-reporting mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#19968;&#31867;&#30001;&#31649;&#36947;&#22270;&#25551;&#36848;&#30340;&#19977;&#32500;&#27969;&#24418;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#39640;&#20934;&#30830;&#24615;&#30340;GNN&#27169;&#22411;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;GNN&#22312;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.05966</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#19977;&#32500;&#25299;&#25169;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks and 3-Dimensional Topology. (arXiv:2305.05966v1 [math.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#19968;&#31867;&#30001;&#31649;&#36947;&#22270;&#25551;&#36848;&#30340;&#19977;&#32500;&#27969;&#24418;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#39640;&#20934;&#30830;&#24615;&#30340;GNN&#27169;&#22411;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;GNN&#22312;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#26576;&#20123;&#31616;&#21333;&#24773;&#22659;&#20013;&#27979;&#35797;&#20102;&#23558;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21040;&#20302;&#32500;&#25299;&#25169;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#30001;&#31649;&#36947;&#22270;&#25551;&#36848;&#30340;&#19977;&#32500;&#27969;&#24418;&#31867;&#21035;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#35299;&#20915;&#21028;&#26029;&#19968;&#23545;&#22270;&#26159;&#21542;&#25552;&#20379;&#21516;&#32986;&#19977;&#32500;&#27969;&#24418;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;GNN&#65292;&#20197;&#39640;&#20934;&#30830;&#24615;&#25552;&#20379;&#36825;&#31181;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#31572;&#26696;&#20026;&#32943;&#23450;&#65292;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;GNN&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#65292;&#25214;&#21040;&#23558;&#19968;&#23545;&#22270;&#30456;&#20851;&#32852;&#30340;Neumann&#31227;&#21160;&#24207;&#21015;&#12290;&#36825;&#20010;&#24773;&#22659;&#21487;&#20197;&#20316;&#20026;&#35299;&#20915;&#19968;&#23545;Kirby&#22270;&#26159;&#21542;&#25552;&#20379;&#21516;&#26500;&#19977;&#32500;&#25110;&#22235;&#32500;&#27969;&#24418;&#38382;&#39064;&#30340;&#29609;&#20855;&#27169;&#22411;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We test the efficiency of applying Geometric Deep Learning to the problems in low-dimensional topology in a certain simple setting. Specifically, we consider the class of 3-manifolds described by plumbing graphs and use Graph Neural Networks (GNN) for the problem of deciding whether a pair of graphs give homeomorphic 3-manifolds. We use supervised learning to train a GNN that provides the answer to such a question with high accuracy. Moreover, we consider reinforcement learning by a GNN to find a sequence of Neumann moves that relates the pair of graphs if the answer is positive. The setting can be understood as a toy model of the problem of deciding whether a pair of Kirby diagrams give diffeomorphic 3- or 4-manifolds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;CUQB&#65292;&#26469;&#35299;&#20915;&#22797;&#21512;&#20989;&#25968;&#65288;&#28151;&#21512;&#27169;&#22411;&#65289;&#30340;&#39640;&#25928;&#32422;&#26463;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#22343;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#36827;&#34892;&#20102;&#26368;&#20248;&#25511;&#21046;&#30340;&#27969;&#20307;&#27969;&#37327;&#21644;&#25299;&#25169;&#32467;&#26500;&#20248;&#21270;&#65292;&#21518;&#32773;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35774;&#35745;&#24378;2&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.03824</link><description>&lt;p&gt;
&#26080;&#36951;&#25022;&#30340;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#24102;&#26377;&#22122;&#22768;&#21644;&#26114;&#36149;&#28151;&#21512;&#27169;&#22411;&#30340;&#24046;&#20998;&#20998;&#20301;&#25968;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
No-Regret Constrained Bayesian Optimization of Noisy and Expensive Hybrid Models using Differentiable Quantile Function Approximations. (arXiv:2305.03824v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;CUQB&#65292;&#26469;&#35299;&#20915;&#22797;&#21512;&#20989;&#25968;&#65288;&#28151;&#21512;&#27169;&#22411;&#65289;&#30340;&#39640;&#25928;&#32422;&#26463;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#22343;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#36827;&#34892;&#20102;&#26368;&#20248;&#25511;&#21046;&#30340;&#27969;&#20307;&#27969;&#37327;&#21644;&#25299;&#25169;&#32467;&#26500;&#20248;&#21270;&#65292;&#21518;&#32773;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35774;&#35745;&#24378;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#21512;&#20989;&#25968;&#65288;&#28151;&#21512;&#27169;&#22411;&#65289;&#30340;&#39640;&#25928;&#32422;&#26463;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#30340;&#36755;&#20837;&#26159;&#20855;&#26377;&#30690;&#37327;&#20540;&#36755;&#20986;&#21644;&#26377;&#22122;&#22768;&#35266;&#27979;&#30340;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#65292;&#36825;&#22312;&#23454;&#38469;&#30340;&#31185;&#23398;&#12289;&#24037;&#31243;&#12289;&#21046;&#36896;&#21644;&#25511;&#21046;&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;Constrained Upper Quantile Bound&#65288;CUQB&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#31181;&#38382;&#39064;&#65292;&#30452;&#25509;&#21033;&#29992;&#20102;&#25105;&#20204;&#23637;&#31034;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#20989;&#25968;&#30340;&#22797;&#21512;&#32467;&#26500;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#12290;CUQB&#30340;&#27010;&#24565;&#31616;&#21333;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#26041;&#27861;&#25152;&#20351;&#29992;&#30340;&#32422;&#26463;&#36924;&#36817;&#12290;&#34429;&#28982;CUQB&#30340;&#25910;&#36141;&#20989;&#25968;&#19981;&#22312;&#23553;&#38381;&#24418;&#24335;&#20013;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#38543;&#26426;&#36924;&#36817;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24471;&#20986;&#20102;&#23545;&#20110;&#32047;&#31215;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#30340;&#30028;&#38480;&#12290;&#30001;&#20110;&#22312;&#26576;&#20123;&#35268;&#21017;&#20551;&#35774;&#19979;&#36825;&#20123;&#30028;&#38480;&#23545;&#36845;&#20195;&#27425;&#25968;&#20855;&#26377;&#27425;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26080;&#36951;&#25022;&#24182;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#23637;&#31034;&#20102;CUQB&#30340;&#21151;&#25928;&#65292;&#21253;&#25324;&#26725;&#26550;&#25299;&#25169; - &#22312;&#20854;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#30340;&#32467;&#26500;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35774;&#35745;&#24378;2&#20493; - &#20197;&#21450;&#27969;&#20307;&#27969;&#37327;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#23569;&#20102;3&#20493;&#30340;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the problem of efficient constrained global optimization of composite functions (hybrid models) whose input is an expensive black-box function with vector-valued outputs and noisy observations, which often arises in real-world science, engineering, manufacturing, and control applications. We propose a novel algorithm, Constrained Upper Quantile Bound (CUQB), to solve such problems that directly exploits the composite structure of the objective and constraint functions that we show leads substantially improved sampling efficiency. CUQB is conceptually simple and avoids the constraint approximations used by previous methods. Although the CUQB acquisition function is not available in closed form, we propose a novel differentiable stochastic approximation that enables it to be efficiently maximized. We further derive bounds on the cumulative regret and constraint violation. Since these bounds depend sublinearly on the number of iterations under some regularity assum
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#39046;&#22495;&#23545;&#27604;&#23398;&#20064;&#65288;MDCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21407;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#26469;&#33258;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#19981;&#20805;&#20998;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02757</link><description>&lt;p&gt;
&#19981;&#20805;&#20998;&#26631;&#27880;&#19979;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Domain Learning From Insufficient Annotations. (arXiv:2305.02757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02757
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#39046;&#22495;&#23545;&#27604;&#23398;&#20064;&#65288;MDCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21407;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#26469;&#33258;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#19981;&#20805;&#20998;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#23398;&#20064;(MDL)&#25351;&#21516;&#26102;&#22312;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#25110;&#19968;&#32452;&#27169;&#22411;&#12290;&#20256;&#32479;&#26041;&#27861;&#24378;&#35843;&#22495;&#20849;&#20139;&#20449;&#24687;&#30340;&#25552;&#21462;&#21644;&#22495;&#31169;&#26377;&#20449;&#24687;&#30340;&#20445;&#30041;&#65292;&#36981;&#24490;&#20849;&#20139;-&#31169;&#26377;&#26550;&#26500;(SP&#27169;&#22411;)&#65292;&#36825;&#27604;&#21333;&#39046;&#22495;&#23398;&#20064;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#20010;&#39046;&#22495;&#20013;&#26377;&#38480;&#30340;&#24050;&#27880;&#37322;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#20005;&#37325;&#38459;&#30861;&#20102;&#20256;&#32479;&#30417;&#30563;MDL&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#39046;&#22495;&#23545;&#27604;&#23398;&#20064;(MDCL)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#26469;&#33258;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#19981;&#20805;&#20998;&#27880;&#37322;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MDCL&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#22495;&#38388;&#35821;&#20041;&#23545;&#40784;&#21644;&#22495;&#20869;&#23545;&#27604;&#12290;&#21069;&#32773;&#26088;&#22312;&#23558;&#19981;&#21516;&#39046;&#22495;&#20013;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#30340;&#24050;&#26631;&#27880;&#23454;&#20363;&#22312;&#20849;&#20139;&#30340;&#38544;&#31354;&#38388;&#20013;&#23545;&#40784;&#65292;&#32780;&#21518;&#32773;&#26088;&#22312;&#22312;&#27599;&#20010;&#39046;&#22495;&#20869;&#26368;&#22823;&#21270;&#20998;&#31163;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22810;&#39046;&#22495;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;MDCL&#26041;&#27861;&#22312;&#21508;&#31181;&#27880;&#37322;&#26041;&#26696;&#19979;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;MDL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-domain learning (MDL) refers to simultaneously constructing a model or a set of models on datasets collected from different domains. Conventional approaches emphasize domain-shared information extraction and domain-private information preservation, following the shared-private framework (SP models), which offers significant advantages over single-domain learning. However, the limited availability of annotated data in each domain considerably hinders the effectiveness of conventional supervised MDL approaches in real-world applications. In this paper, we introduce a novel method called multi-domain contrastive learning (MDCL) to alleviate the impact of insufficient annotations by capturing both semantic and structural information from both labeled and unlabeled data.Specifically, MDCL comprises two modules: inter-domain semantic alignment and intra-domain contrast. The former aims to align annotated instances of the same semantic category from distinct domains within a shared hidd
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EXPECTED&#30340;&#25361;&#25112;&#65292;&#35299;&#20915;&#27169;&#22411;&#35843;&#25972;&#38382;&#39064;&#65292;&#27169;&#22411;&#25552;&#20379;&#32773;&#21487;&#20197;&#36890;&#36807;&#26469;&#33258;&#26412;&#22320;&#29992;&#25143;&#30340;&#21453;&#39304;&#22810;&#27425;&#35775;&#38382;&#20505;&#36873;&#27169;&#22411;&#30340;&#25805;&#20316;&#24615;&#33021;&#65292;&#20174;&#32780;&#20248;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#30446;&#26631;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.14831</link><description>&lt;p&gt;
&#20174;&#38480;&#21046;&#24615;&#21453;&#39304;&#20013;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Earning Extra Performance from Restrictive Feedbacks. (arXiv:2304.14831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EXPECTED&#30340;&#25361;&#25112;&#65292;&#35299;&#20915;&#27169;&#22411;&#35843;&#25972;&#38382;&#39064;&#65292;&#27169;&#22411;&#25552;&#20379;&#32773;&#21487;&#20197;&#36890;&#36807;&#26469;&#33258;&#26412;&#22320;&#29992;&#25143;&#30340;&#21453;&#39304;&#22810;&#27425;&#35775;&#38382;&#20505;&#36873;&#27169;&#22411;&#30340;&#25805;&#20316;&#24615;&#33021;&#65292;&#20174;&#32780;&#20248;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#30446;&#26631;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#38754;&#20020;&#36825;&#26679;&#19968;&#31181;&#24773;&#20917;&#65306;&#27169;&#22411;&#25552;&#20379;&#32773;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#20808;&#21069;&#35757;&#32451;&#30340;&#27169;&#22411;&#20197;&#28385;&#36275;&#26412;&#22320;&#29992;&#25143;&#30340;&#29305;&#23450;&#38656;&#27714;&#12290;&#22914;&#26524;&#21487;&#20197;&#23558;&#30446;&#26631;&#25968;&#25454;&#20256;&#36882;&#32473;&#27169;&#22411;&#65292;&#37027;&#20040;&#36825;&#20010;&#38382;&#39064;&#23601;&#36716;&#21270;&#20026;&#26631;&#20934;&#30340;&#27169;&#22411;&#35843;&#25972;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#30446;&#26631;&#25968;&#25454;&#24182;&#19981;&#20849;&#20139;&#32473;&#27169;&#22411;&#25552;&#20379;&#32773;&#65292;&#32780;&#21482;&#26159;&#19968;&#20123;&#20851;&#20110;&#27169;&#22411;&#30340;&#35780;&#20272;&#21487;&#20379;&#35775;&#38382;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EXPECTED&#65288;Earning eXtra PerformancE from restriCTive feEDdbacks&#65289;&#30340;&#25361;&#25112;&#65292;&#27491;&#24335;&#25551;&#36848;&#20102;&#36825;&#31181;&#24418;&#24335;&#30340;&#27169;&#22411;&#35843;&#25972;&#38382;&#39064;&#65292;&#20801;&#35768;&#27169;&#22411;&#25552;&#20379;&#32773;&#36890;&#36807;&#26469;&#33258;&#26412;&#22320;&#29992;&#25143;&#65288;&#25110;&#19968;&#32452;&#29992;&#25143;&#65289;&#30340;&#21453;&#39304;&#22810;&#27425;&#35775;&#38382;&#20505;&#36873;&#27169;&#22411;&#30340;&#25805;&#20316;&#24615;&#33021;&#12290;&#27169;&#22411;&#25552;&#20379;&#32773;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#21033;&#29992;&#21453;&#39304;&#26368;&#32456;&#21521;&#26412;&#22320;&#29992;&#25143;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#30340;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#19981;&#21516;&#65292;EXPECTED&#22312;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27169;&#22411;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning applications encounter a situation where model providers are required to further refine the previously trained model so as to gratify the specific need of local users. This problem is reduced to the standard model tuning paradigm if the target data is permissibly fed to the model. However, it is rather difficult in a wide range of practical cases where target data is not shared with model providers but commonly some evaluations about the model are accessible. In this paper, we formally set up a challenge named \emph{Earning eXtra PerformancE from restriCTive feEDdbacks} (EXPECTED) to describe this form of model tuning problems. Concretely, EXPECTED admits a model provider to access the operational performance of the candidate model multiple times via feedback from a local user (or a group of users). The goal of the model provider is to eventually deliver a satisfactory model to the local user(s) by utilizing the feedbacks. Unlike existing model tuning methods wher
&lt;/p&gt;</description></item><item><title>VISAR&#26159;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#25552;&#21319;&#20889;&#20316;&#20307;&#39564;&#21644;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#22312;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#38543;&#26102;&#24110;&#21161;&#20316;&#32773;&#26500;&#24605;&#21644;&#20462;&#25913;&#30446;&#26631;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#32534;&#31243;&#26469;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#25512;&#33616;&#26469;&#22686;&#21152;&#35828;&#26381;&#21147;&#12290;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2304.07810</link><description>&lt;p&gt;
VISAR&#65306;&#19968;&#31181;&#24102;&#26377;&#21487;&#35270;&#21270;&#32534;&#31243;&#21644;&#24555;&#36895;&#33609;&#26696;&#21407;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#35770;&#35777;&#20889;&#20316;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping. (arXiv:2304.07810v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07810
&lt;/p&gt;
&lt;p&gt;
VISAR&#26159;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#25552;&#21319;&#20889;&#20316;&#20307;&#39564;&#21644;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#22312;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#38543;&#26102;&#24110;&#21161;&#20316;&#32773;&#26500;&#24605;&#21644;&#20462;&#25913;&#30446;&#26631;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#32534;&#31243;&#26469;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#25512;&#33616;&#26469;&#22686;&#21152;&#35828;&#26381;&#21147;&#12290;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36777;&#35770;&#20889;&#20316;&#20013;&#65292;&#20316;&#32773;&#24517;&#39035;&#26500;&#24605;&#20998;&#23618;&#20889;&#20316;&#30446;&#26631;&#65292;&#30830;&#20445;&#20854;&#35770;&#28857;&#30340;&#35828;&#26381;&#21147;&#65292;&#24182;&#36890;&#36807;&#36215;&#33609;&#26469;&#20462;&#35746;&#21644;&#32452;&#32455;&#20182;&#20204;&#30340;&#35745;&#21010;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#65288;&#20363;&#22914;ChatGPT&#65289;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#24573;&#30053;&#20102;&#38544;&#21547;&#30340;&#20889;&#20316;&#19978;&#19979;&#25991;&#21644;&#29992;&#25143;&#24847;&#22270;&#65292;&#32570;&#20047;&#29992;&#25143;&#25511;&#21046;&#21644;&#33258;&#20027;&#26435;&#65292;&#24182;&#19988;&#25552;&#20379;&#26377;&#38480;&#30340;&#24110;&#21161;&#26469;&#36827;&#34892;&#24847;&#20041;&#26500;&#24314;&#21644;&#20462;&#35746;&#20889;&#20316;&#35745;&#21010;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VISAR&#65292;&#19968;&#31181;AI&#25903;&#25345;&#30340;&#20889;&#20316;&#21161;&#25163;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#22312;&#20854;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#26500;&#24605;&#21644;&#20462;&#35746;&#20998;&#23618;&#30446;&#26631;&#65292;&#36890;&#36807;&#21516;&#27493;&#25991;&#26412;&#32534;&#36753;&#21644;&#21487;&#35270;&#21270;&#32534;&#31243;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#35770;&#35777;&#28779;&#33457;&#25512;&#33616;&#22686;&#24378;&#35828;&#26381;&#21147;&#12290;VISAR&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#25506;&#32034;&#12289;&#23454;&#39564;&#21644;&#39564;&#35777;&#20182;&#20204;&#30340;&#20889;&#20316;&#35745;&#21010;&#12290;&#19968;&#20010;&#21463;&#25511;&#23454;&#39564;&#23460;&#30740;&#31350;&#35777;&#23454;&#65292;VISAR&#21487;&#20197;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#29992;&#25143;&#30340;&#20889;&#20316;&#20307;&#39564;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22522;&#20110;ANN&#30340;&#22343;&#34913;&#22120;&#21450;&#20854;&#21487;&#35757;&#32451;&#30340;FPGA&#23454;&#29616;&#65292;&#36890;&#36807;&#33258;&#23450;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2304.06987</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;ANN&#30340;&#22343;&#34913;&#22120;&#21450;&#20854;&#35757;&#32451;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Unsupervised ANN-Based Equalizer and Its Trainable FPGA Implementation. (arXiv:2304.06987v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22522;&#20110;ANN&#30340;&#22343;&#34913;&#22120;&#21450;&#20854;&#21487;&#35757;&#32451;&#30340;FPGA&#23454;&#29616;&#65292;&#36890;&#36807;&#33258;&#23450;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#20449;&#24037;&#31243;&#24072;&#24378;&#35843;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#21450;&#20854;&#32452;&#20214;&#30340;&#28789;&#27963;&#24615;&#21644;&#33258;&#20027;&#24615;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26080;&#30417;&#30563;&#35757;&#32451;&#20855;&#26377;&#29305;&#27530;&#30340;&#24847;&#20041;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#19981;&#20256;&#36755;&#23548;&#39057;&#31526;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ANN&#30340;&#26080;&#30417;&#30563;&#22343;&#34913;&#22120;&#21450;&#20854;&#21487;&#35757;&#32451;&#30340;&#29616;&#22330;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#23454;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#23450;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#35753;ANN&#36866;&#24212;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#65292;&#25509;&#36817;&#30417;&#30563;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#23454;&#38469;&#30340;&#36890;&#20449;&#31995;&#32479;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;FPGA&#23454;&#29616;&#65292;&#20854;&#21534;&#21520;&#37327;&#36798;&#21040;Gbit/s&#32423;&#21035;&#65292;&#36828;&#36828;&#36229;&#20986;&#39640;&#24615;&#33021;GPU&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, communication engineers put strong emphasis on artificial neural network (ANN)-based algorithms with the aim of increasing the flexibility and autonomy of the system and its components. In this context, unsupervised training is of special interest as it enables adaptation without the overhead of transmitting pilot symbols. In this work, we present a novel ANN-based, unsupervised equalizer and its trainable field programmable gate array (FPGA) implementation. We demonstrate that our custom loss function allows the ANN to adapt for varying channel conditions, approaching the performance of a supervised baseline. Furthermore, as a first step towards a practical communication system, we design an efficient FPGA implementation of our proposed algorithm, which achieves a throughput in the order of Gbit/s, outperforming a high-performance GPU by a large margin.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#32593;&#26684;&#32454;&#21270;&#30340;&#32676;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32593;&#26684;&#24314;&#27169;&#20026;&#19968;&#32452;&#31616;&#21333;&#21327;&#20316;&#30340;&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#22312;&#30456;&#37051;&#32593;&#26684;&#20803;&#32032;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#22797;&#26434;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#34987;&#35777;&#23454;&#21487;&#20197;&#23398;&#20064;&#21487;&#38752;&#12289;&#21487;&#25193;&#23637;&#30340;&#32593;&#26684;&#32454;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.00818</link><description>&lt;p&gt;
&#36866;&#24212;&#32593;&#26684;&#32454;&#21270;&#30340;&#32676;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Swarm Reinforcement Learning For Adaptive Mesh Refinement. (arXiv:2304.00818v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00818
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#32593;&#26684;&#32454;&#21270;&#30340;&#32676;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32593;&#26684;&#24314;&#27169;&#20026;&#19968;&#32452;&#31616;&#21333;&#21327;&#20316;&#30340;&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#22312;&#30456;&#37051;&#32593;&#26684;&#20803;&#32032;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#22797;&#26434;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#34987;&#35777;&#23454;&#21487;&#20197;&#23398;&#20064;&#21487;&#38752;&#12289;&#21487;&#25193;&#23637;&#30340;&#32593;&#26684;&#32454;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#20803;&#26041;&#27861;&#26159;&#24037;&#31243;&#23398;&#20013;&#19968;&#31181;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#33258;&#36866;&#24212;&#32593;&#26684;&#32454;&#21270;&#65288;AMR&#65289;&#36890;&#36807;&#21160;&#24577;&#32454;&#21270;&#32593;&#26684;&#21306;&#22495;&#65292;&#22312;&#35745;&#31639;&#36895;&#24230;&#21644;&#27169;&#25311;&#31934;&#24230;&#20043;&#38388;&#21462;&#24471;&#26377;&#21033;&#30340;&#24179;&#34913;&#12290;&#20256;&#32479;&#30340;AMR&#26041;&#27861;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#25110;&#26114;&#36149;&#30340;&#35823;&#24046;&#20272;&#35745;&#22120;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#22797;&#26434;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#23398;&#20064;&#22411;AMR&#26041;&#27861;&#35797;&#22270;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#21482;&#33021;&#24212;&#29992;&#20110;&#31616;&#21333;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23558;AMR&#34920;&#36848;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#32676;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#32593;&#26684;&#34987;&#24314;&#27169;&#20026;&#19968;&#32452;&#31616;&#21333;&#21327;&#20316;&#30340;&#20195;&#29702;&#65292;&#21487;&#20197;&#20998;&#35010;&#20026;&#22810;&#20010;&#26032;&#20195;&#29702;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20351;&#29992;&#31354;&#38388;&#22870;&#21169;&#20844;&#24335;&#21270;&#31616;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#22312;&#30456;&#37051;&#32593;&#26684;&#20803;&#32032;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#33258;&#36866;&#24212;&#32676;&#20307;&#32593;&#26684;&#32454;&#21270;&#65288;ASMR&#65289;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#23427;&#21487;&#20197;&#23398;&#20064;&#21487;&#38752;&#12289;&#21487;&#25193;&#23637;&#30340;&#32593;&#26684;&#32454;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Finite Element Method, an important technique in engineering, is aided by Adaptive Mesh Refinement (AMR), which dynamically refines mesh regions to allow for a favorable trade-off between computational speed and simulation accuracy. Classical methods for AMR depend on task-specific heuristics or expensive error estimators, hindering their use for complex simulations. Recent learned AMR methods tackle these problems, but so far scale only to simple toy examples. We formulate AMR as a novel Adaptive Swarm Markov Decision Process in which a mesh is modeled as a system of simple collaborating agents that may split into multiple new agents. This framework allows for a spatial reward formulation that simplifies the credit assignment problem, which we combine with Message Passing Networks to propagate information between neighboring mesh elements. We experimentally validate the effectiveness of our approach, Adaptive Swarm Mesh Refinement (ASMR), showing that it learns reliable, scalable,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#25163;&#21160;&#35843;&#25972;&#21644;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#28155;&#21152;&#33258;&#23450;&#20041;&#23618;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#21487;&#20197;&#25913;&#21892;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07189</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25104;&#20687;&#20013;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Optimizing Convolutional Neural Networks for Chronic Obstructive Pulmonary Disease Detection in Clinical Computed Tomography Imaging. (arXiv:2303.07189v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#25163;&#21160;&#35843;&#25972;&#21644;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#28155;&#21152;&#33258;&#23450;&#20041;&#23618;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#21487;&#20197;&#25913;&#21892;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#36890;&#36807;&#25506;&#32034;&#25163;&#21160;&#35843;&#25972;&#21644;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#32954;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#26816;&#27979;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#65288;COPD&#65289;&#30340;&#23384;&#22312;&#65292;&#26469;&#20248;&#21270;&#20108;&#36827;&#21046;COPD&#30340;&#26816;&#27979;&#12290;&#26041;&#27861;&#65306;&#22238;&#39038;&#24615;&#36873;&#25321;&#20102;78&#21517;&#21463;&#35797;&#32773;&#65288;43&#21517;COPD&#24739;&#32773;&#65307;35&#21517;&#20581;&#24247;&#23545;&#29031;&#32452;&#65289;&#30340;7,194&#20010;CT&#22270;&#20687;&#65288;3,597&#20010;COPD&#65307;3,597&#20010;&#20581;&#24247;&#23545;&#29031;&#32452;&#65289;&#65288;2018&#24180;10&#26376;&#33267;2019&#24180;12&#26376;&#65289;&#12290;&#23545;&#27599;&#20010;&#22270;&#20687;&#65292;&#23558;&#24378;&#24230;&#20540;&#25163;&#21160;&#35009;&#21098;&#21040;&#32954;&#27668;&#32959;&#31383;&#21475;&#35774;&#32622;&#21644;&#22522;&#20934;&#30340;&#8220;&#20840;&#33539;&#22260;&#8221;&#31383;&#21475;&#35774;&#32622;&#12290;&#31867;&#24179;&#34913;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#21253;&#21547;&#20102;3,392&#12289;1,114&#21644;2,688&#20010;&#22270;&#20687;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;CNN&#26550;&#26500;&#26469;&#20248;&#21270;&#32593;&#32476;&#20027;&#24178;&#12290;&#27492;&#22806;&#65292;&#36824;&#36890;&#36807;&#21521;&#27169;&#22411;&#28155;&#21152;&#33258;&#23450;&#20041;&#23618;&#26469;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#12290;&#26681;&#25454;&#21463;&#35797;&#32773;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#65288;ROC&#65289;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#30340;&#22270;&#20687;&#27700;&#24179;&#65292;&#35745;&#31639;&#20986;P&#20540;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To optimize the binary detection of Chronic Obstructive Pulmonary Disease (COPD) based on emphysema presence in the lung with convolutional neural networks (CNN) by exploring manually adjusted versus automated window-setting optimization (WSO) on computed tomography (CT) images.  Methods: 7,194 CT images (3,597 with COPD; 3,597 healthy controls) from 78 subjects (43 with COPD; 35 healthy controls) were selected retrospectively (10.2018-12.2019) and preprocessed. For each image, intensity values were manually clipped to the emphysema window setting and a baseline 'full-range' window setting. Class-balanced train, validation, and test sets contained 3,392, 1,114, and 2,688 images. The network backbone was optimized by comparing various CNN architectures. Furthermore, automated WSO was implemented by adding a customized layer to the model. The image-level area under the Receiver Operating Characteristics curve (AUC) [lower, upper limit 95% confidence] and P-values calculated from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Expert-Free Online Transfer Learning (EF-OnTL)&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#26080;&#19987;&#23478;&#30340;&#23454;&#26102;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36801;&#31227;&#28304;&#26234;&#33021;&#20307;&#21644;&#35201;&#36716;&#31227;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#38656;&#35201;&#23545;&#19987;&#23478;&#26234;&#33021;&#20307;&#20219;&#21153;&#26377;&#33391;&#22909;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.01170</link><description>&lt;p&gt;
&#26080;&#19987;&#23478;&#22312;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning. (arXiv:2303.01170v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Expert-Free Online Transfer Learning (EF-OnTL)&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#26080;&#19987;&#23478;&#30340;&#23454;&#26102;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#36801;&#31227;&#28304;&#26234;&#33021;&#20307;&#21644;&#35201;&#36716;&#31227;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#38656;&#35201;&#23545;&#19987;&#23478;&#26234;&#33021;&#20307;&#20219;&#21153;&#26377;&#33391;&#22909;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#19987;&#23478;&#26234;&#33021;&#20307;&#36716;&#31227;&#21040;&#26032;&#25163;&#26234;&#33021;&#20307;&#26469;&#35299;&#20915;&#35757;&#32451;&#38382;&#39064;&#65292;&#22914;&#25506;&#32034;&#25104;&#26412;&#12289;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#25910;&#25947;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36801;&#31227;&#38656;&#35201;&#26032;&#25163;&#26234;&#33021;&#20307;&#23545;&#19987;&#23478;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#26377;&#33391;&#22909;&#30340;&#29702;&#35299;&#25165;&#33021;&#26377;&#25928;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#19987;&#23478;&#22312;&#32447;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65288;EF-OnTL&#65289;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#26080;&#19987;&#23478;&#30340;&#23454;&#26102;&#36801;&#31227;&#23398;&#20064;&#12290;&#22312;&#27599;&#19968;&#27425;&#36801;&#31227;&#27493;&#39588;&#20013;&#65292;&#26681;&#25454;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#26469;&#21160;&#24577;&#36873;&#25321;&#36801;&#31227;&#28304;&#26234;&#33021;&#20307;&#21644;&#35201;&#36716;&#31227;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SARS-RND&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#23545;RND&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#20174;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#12289;&#34892;&#21160;&#12289;&#22870;&#21169;&#21644;&#19979;&#19968;&#29366;&#24577;&#20013;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning in Reinforcement Learning (RL) has been widely studied to overcome training issues of Deep-RL, i.e., exploration cost, data availability and convergence time, by introducing a way to enhance training phase with external knowledge. Generally, knowledge is transferred from expert-agents to novices. While this fixes the issue for a novice agent, a good understanding of the task on expert agent is required for such transfer to be effective. As an alternative, in this paper we propose Expert-Free Online Transfer Learning (EF-OnTL), an algorithm that enables expert-free real-time dynamic transfer learning in multi-agent system. No dedicated expert exists, and transfer source agent and knowledge to be transferred are dynamically selected at each transfer step based on agents' performance and uncertainty. To improve uncertainty estimation, we also propose State Action Reward Next-State Random Network Distillation (sars-RND), an extension of RND that estimates uncertainty from
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12247</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#65306;&#19968;&#31181;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25152;&#38656;&#30340;&#20132;&#20114;&#22914;&#20309;&#36827;&#34892;&#37327;&#21270;&#65311;&#26368;&#36866;&#21512;&#25429;&#25417;&#36825;&#20123;&#20132;&#20114;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#34913;&#37327;&#26631;&#20934;&#31216;&#20026;&#22810;&#27169;&#24577;&#20998;&#24067;&#65288;&#25110;&#31616;&#31216;PID&#65289;&#30340;PID&#32479;&#35745;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;PID&#20272;&#35745;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;PID&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#38750;&#31283;&#24577;&#36172;&#21338;&#26426;&#23450;&#20041;&#27495;&#20041;&#30340;&#26032;&#23450;&#20041;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#23450;&#20041;&#20013;&#30340;&#38382;&#39064;&#24182;&#20462;&#22797;&#20102;&#20195;&#29702;&#35774;&#35745;&#20013;&#25506;&#32034;&#36807;&#24230;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2302.12202</link><description>&lt;p&gt;
&#38750;&#31283;&#24577;&#36172;&#21338;&#26426;&#30340;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
A Definition of Non-Stationary Bandits. (arXiv:2302.12202v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12202
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#38750;&#31283;&#24577;&#36172;&#21338;&#26426;&#23450;&#20041;&#27495;&#20041;&#30340;&#26032;&#23450;&#20041;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#23450;&#20041;&#20013;&#30340;&#38382;&#39064;&#24182;&#20462;&#22797;&#20102;&#20195;&#29702;&#35774;&#35745;&#20013;&#25506;&#32034;&#36807;&#24230;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38750;&#31283;&#24577;&#36172;&#21338;&#26426;&#23398;&#20064;&#30340;&#35838;&#39064;&#21560;&#24341;&#20102;&#26368;&#36817;&#30340;&#24456;&#22810;&#20851;&#27880;&#65292;&#20294;&#25105;&#20204;&#36824;&#27809;&#26377;&#25214;&#21040;&#19968;&#20010;&#33021;&#22815;&#21306;&#20998;&#38750;&#31283;&#24577;&#36172;&#21338;&#26426;&#21644;&#31283;&#24577;&#36172;&#21338;&#26426;&#30340;&#24418;&#24335;&#23450;&#20041;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#38750;&#31283;&#24577;&#36172;&#21338;&#26426;&#23450;&#20041;&#20026;&#22870;&#21169;&#20998;&#24067;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36172;&#21338;&#26426;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#23450;&#20041;&#22312;&#23558;&#21516;&#19968;&#20010;&#36172;&#21338;&#26426;&#21516;&#26102;&#20998;&#31867;&#20026;&#31283;&#24577;&#21644;&#38750;&#31283;&#24577;&#26102;&#23384;&#22312;&#27495;&#20041;&#65307;&#36825;&#31181;&#27495;&#20041;&#28304;&#20110;&#35813;&#23450;&#20041;&#23545;&#28508;&#22312;&#22870;&#21169;&#20998;&#24067;&#24207;&#21015;&#30340;&#20381;&#36182;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#23450;&#20041;&#20063;&#23548;&#33268;&#20102;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36951;&#25022;&#27010;&#24565;&#65306;&#21160;&#24577;&#36951;&#25022;&#21644;&#24369;&#36951;&#25022;&#12290;&#36825;&#20123;&#27010;&#24565;&#22312;&#19968;&#20123;&#36172;&#21338;&#26426;&#20013;&#24182;&#19981;&#33021;&#20934;&#30830;&#22320;&#21453;&#26144;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#38750;&#31283;&#24577;&#36172;&#21338;&#26426;&#30340;&#23450;&#20041;&#36824;&#23548;&#33268;&#20102;&#25506;&#32034;&#36807;&#24230;&#30340;&#20195;&#29702;&#35774;&#35745;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#38750;&#31283;&#24577;&#36172;&#21338;&#26426;&#30340;&#24418;&#24335;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the subject of non-stationary bandit learning having attracted much recent attention, we have yet to identify a formal definition of non-stationarity that can consistently distinguish non-stationary bandits from stationary ones. Prior work has characterized non-stationary bandits as bandits for which the reward distribution changes over time. We demonstrate that this definition can ambiguously classify the same bandit as both stationary and non-stationary; this ambiguity arises in the existing definition's dependence on the latent sequence of reward distributions. Moreover, the definition has given rise to two widely used notions of regret: the dynamic regret and the weak regret. These notions are not indicative of qualitative agent performance in some bandits. Additionally, this definition of non-stationary bandits has led to the design of agents that explore excessively. We introduce a formal definition of non-stationary bandits that resolves these issues. Our new definition 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24037;&#31243;&#35774;&#35745;&#39046;&#22495;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65288;MMML&#65289;&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#34920;&#31034;&#12289;&#34701;&#21512;&#12289;&#23545;&#40784;&#12289;&#36716;&#25442;&#21644;&#20849;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#20197;&#21450;&#19982;&#24037;&#31243;&#35774;&#35745;&#30456;&#20851;&#30340;&#21069;&#27839;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#65292;&#21253;&#25324;&#26500;&#24314;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#65292;&#20197;&#20419;&#36827;&#31639;&#27861;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2302.10909</link><description>&lt;p&gt;
&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65306;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Machine Learning in Engineering Design: A Review and Future Directions. (arXiv:2302.10909v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24037;&#31243;&#35774;&#35745;&#39046;&#22495;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65288;MMML&#65289;&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#34920;&#31034;&#12289;&#34701;&#21512;&#12289;&#23545;&#40784;&#12289;&#36716;&#25442;&#21644;&#20849;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#20197;&#21450;&#19982;&#24037;&#31243;&#35774;&#35745;&#30456;&#20851;&#30340;&#21069;&#27839;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#65292;&#21253;&#25324;&#26500;&#24314;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#65292;&#20197;&#20419;&#36827;&#31639;&#27861;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65288;MMML&#65289;&#39046;&#22495;&#20013;&#65292;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#34701;&#21512;&#26377;&#28508;&#21147;&#37325;&#22609;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#24037;&#31243;&#35774;&#35745;&#39046;&#22495;&#20013;MMML&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#32508;&#36848;&#39318;&#20808;&#28145;&#20837;&#25506;&#35752;&#20102;MMML&#30340;&#20116;&#20010;&#22522;&#26412;&#27010;&#24565;&#65306;&#22810;&#27169;&#24577;&#20449;&#24687;&#34920;&#31034;&#12289;&#34701;&#21512;&#12289;&#23545;&#40784;&#12289;&#36716;&#25442;&#21644;&#20849;&#23398;&#20064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MMML&#30340;&#21069;&#27839;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#19982;&#24037;&#31243;&#35774;&#35745;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#22914;&#36328;&#27169;&#24577;&#32508;&#21512;&#12289;&#22810;&#27169;&#24577;&#39044;&#27979;&#21644;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#12290;&#36890;&#36807;&#36825;&#20010;&#32508;&#36848;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#37319;&#29992;MMML&#25152;&#38754;&#20020;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#20026;&#20102;&#25512;&#21160;MMML&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#25345;&#32493;&#28436;&#36827;&#65292;&#25105;&#20204;&#25552;&#20513;&#38598;&#20013;&#21162;&#21147;&#26500;&#24314;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#65292;&#20197;&#20419;&#36827;&#31639;&#27861;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing field of multi-modal machine learning (MMML), the convergence of multiple data modalities has the potential to reshape various applications. This paper presents a comprehensive overview of the current state, advancements, and challenges of MMML within the sphere of engineering design. The review begins with a deep dive into five fundamental concepts of MMML:multi-modal information representation, fusion, alignment, translation, and co-learning. Following this, we explore the cutting-edge applications of MMML, placing a particular emphasis on tasks pertinent to engineering design, such as cross-modal synthesis, multi-modal prediction, and cross-modal information retrieval. Through this comprehensive overview, we highlight the inherent challenges in adopting MMML in engineering design, and proffer potential directions for future research. To spur on the continued evolution of MMML in engineering design, we advocate for concentrated efforts to construct extensive 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;INFUSE&#65292;&#29992;&#20110;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#20915;&#31574;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#21019;&#24314;&#28151;&#21512;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#20116;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#20998;&#31867;&#21644;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2302.09394</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks based Meta-Learning for Network Intrusion Detection. (arXiv:2302.09394v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;INFUSE&#65292;&#29992;&#20110;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#20915;&#31574;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#21019;&#24314;&#28151;&#21512;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#20116;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#20998;&#31867;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#32452;&#20214;&#30340;&#25968;&#23383;&#21270;&#20197;&#21450;&#26412;&#22320;&#32593;&#32476;&#20043;&#38388;&#30340;&#20114;&#36830;&#24615;&#22686;&#21152;&#20102;&#32593;&#32476;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#35774;&#35745;&#19968;&#20010;&#30830;&#20445;&#24037;&#19994;&#29983;&#24577;&#31995;&#32479;&#23433;&#20840;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#32593;&#32476;&#27969;&#37327;&#21253;&#21547;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#21253;&#25324;&#32454;&#24494;&#21464;&#21270;&#30340;&#26032;&#22411;&#21644;&#36827;&#21270;&#22411;&#25915;&#20987;&#12290;&#29992;&#20110;&#26500;&#24314;&#35745;&#31639;&#26426;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#30340;&#25968;&#25454;&#20855;&#26377;&#20559;&#26012;&#30340;&#31867;&#20998;&#24067;&#21644;&#26377;&#38480;&#30340;&#25915;&#20987;&#31867;&#22411;&#34920;&#31034;&#65292;&#36825;&#19982;&#30495;&#23454;&#30340;&#32593;&#32476;&#27969;&#37327;&#19981;&#21516;&#12290;&#36825;&#20123;&#38480;&#21046;&#23548;&#33268;&#20102;&#25968;&#25454;&#38598;&#28418;&#31227;&#65292;&#36127;&#38754;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#38477;&#20302;&#20102;&#23545;&#26032;&#22411;&#25915;&#20987;&#30340;&#26816;&#27979;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65307;INformation FUsion and Stacking Ensemble (INFUSE)&#29992;&#20110;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#25972;&#21512;&#20915;&#31574;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#21019;&#24314;&#28151;&#21512;&#29305;&#24449;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20116;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The digitization of different components of industry and inter-connectivity among indigenous networks have increased the risk of network attacks. Designing an intrusion detection system to ensure security of the industrial ecosystem is difficult as network traffic encompasses various attack types, including new and evolving ones with minor changes. The data used to construct a predictive model for computer networks has a skewed class distribution and limited representation of attack types, which differ from real network traffic. These limitations result in dataset shift, negatively impacting the machine learning models' predictive abilities and reducing the detection rate against novel attacks. To address the challenges, we propose a novel deep neural network based Meta-Learning framework; INformation FUsion and Stacking Ensemble (INFUSE) for network intrusion detection. First, a hybrid feature space is created by integrating decision and feature spaces. Five different classifiers are 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35780;&#20272;&#21453;&#20107;&#23454;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#24182;&#20272;&#35745;&#21160;&#24577;&#22788;&#29702;&#25928;&#24212;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;$Z$-&#20272;&#35745;&#26041;&#27861;&#31283;&#23450;&#24773;&#33410;&#21464;&#21270;&#30340;&#20272;&#35745;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2302.08854</link><description>&lt;p&gt;
&#21518;&#26399;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Post-Episodic Reinforcement Learning Inference. (arXiv:2302.08854v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08854
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35780;&#20272;&#21453;&#20107;&#23454;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#24182;&#20272;&#35745;&#21160;&#24577;&#22788;&#29702;&#25928;&#24212;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;$Z$-&#20272;&#35745;&#26041;&#27861;&#31283;&#23450;&#24773;&#33410;&#21464;&#21270;&#30340;&#20272;&#35745;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#20272;&#35745;&#21644;&#25512;&#26029;&#65307;&#21363;&#22312;&#27599;&#20010;&#26102;&#26399;&#65288;&#20063;&#31216;&#20026;&#24773;&#33410;&#65289;&#20197;&#39034;&#24207;&#26041;&#24335;&#19982;&#21333;&#20010;&#21463;&#35797;&#21333;&#20803;&#22810;&#27425;&#20132;&#20114;&#30340;&#33258;&#36866;&#24212;&#35797;&#39564;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#25910;&#38598;&#25968;&#25454;&#21518;&#33021;&#22815;&#35780;&#20272;&#21453;&#20107;&#23454;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#24182;&#20272;&#35745;&#32467;&#26500;&#21442;&#25968;&#65292;&#22914;&#21160;&#24577;&#22788;&#29702;&#25928;&#24212;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20449;&#29992;&#20998;&#37197;&#65288;&#20363;&#22914;&#65292;&#31532;&#19968;&#20010;&#26102;&#26399;&#30340;&#34892;&#21160;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#65289;&#12290;&#36825;&#20123;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#21487;&#20197;&#26500;&#25104;&#30697;&#26041;&#31243;&#30340;&#35299;&#65292;&#20294;&#19981;&#26159;&#24635;&#20307;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#22120;&#65292;&#22312;&#38745;&#24577;&#25968;&#25454;&#24773;&#20917;&#19979;&#23548;&#33268;&#20102;$Z$-&#20272;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20272;&#35745;&#37327;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#25910;&#38598;&#30340;&#24773;&#20917;&#19979;&#19981;&#33021;&#28176;&#36817;&#27491;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;$Z$-&#20272;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#26469;&#31283;&#23450;&#24773;&#33410;&#21464;&#21270;&#30340;&#20272;&#35745;&#26041;&#24046;&#65292;&#36825;&#26159;&#30001;&#38750;...
&lt;/p&gt;
&lt;p&gt;
We consider estimation and inference with data collected from episodic reinforcement learning (RL) algorithms; i.e. adaptive experimentation algorithms that at each period (aka episode) interact multiple times in a sequential manner with a single treated unit. Our goal is to be able to evaluate counterfactual adaptive policies after data collection and to estimate structural parameters such as dynamic treatment effects, which can be used for credit assignment (e.g. what was the effect of the first period action on the final outcome). Such parameters of interest can be framed as solutions to moment equations, but not minimizers of a population loss function, leading to $Z$-estimation approaches in the case of static data. However, such estimators fail to be asymptotically normal in the case of adaptive data collection. We propose a re-weighted $Z$-estimation approach with carefully designed adaptive weights to stabilize the episode-varying estimation variance, which results from the non
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#28151;&#21512;&#20132;&#36890;&#20013;&#30340;&#33258;&#21160;&#21270;&#20132;&#21449;&#21475;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#20154;&#39550;&#39542;&#21592;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35780;&#20272;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.12717</link><description>&lt;p&gt;
&#28151;&#21512;&#20132;&#36890;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33258;&#21160;&#21270;&#20132;&#21449;&#21475;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Automatic Intersection Management in Mixed Traffic Using Reinforcement Learning and Graph Neural Networks. (arXiv:2301.12717v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#28151;&#21512;&#20132;&#36890;&#20013;&#30340;&#33258;&#21160;&#21270;&#20132;&#21449;&#21475;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#20154;&#39550;&#39542;&#21592;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35780;&#20272;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#30340;&#33258;&#21160;&#39550;&#39542;&#26377;&#28508;&#21147;&#26174;&#33879;&#25552;&#39640;&#22478;&#24066;&#20132;&#36890;&#25928;&#29575;&#65292;&#20363;&#22914;&#36890;&#36807;&#20943;&#36731;&#36974;&#25377;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;&#21512;&#20316;&#34892;&#20026;&#35268;&#21010;&#21487;&#20197;&#29992;&#20110;&#32852;&#21512;&#20248;&#21270;&#22810;&#36742;&#36710;&#36742;&#30340;&#36816;&#21160;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#33258;&#21160;&#20132;&#21449;&#21475;&#31649;&#29702;&#26041;&#27861;&#21482;&#32771;&#34385;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#20132;&#36890;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#28151;&#21512;&#20132;&#36890;&#65292;&#21363;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#39550;&#39542;&#36710;&#36742;&#30340;&#21516;&#26102;&#36947;&#36335;&#20351;&#29992;&#65292;&#23558;&#26222;&#36941;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#22270;&#30340;&#22330;&#26223;&#34920;&#31034;&#36827;&#34892;&#21327;&#21516;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#36825;&#20123;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36866;&#29992;&#20110;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#20132;&#36890;&#12290;&#22330;&#26223;&#34920;&#31034;&#25193;&#23637;&#21040;&#28151;&#21512;&#20132;&#36890;&#65292;&#24182;&#32771;&#34385;&#20154;&#39550;&#39542;&#21592;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#22522;&#20110;&#20223;&#30495;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#35843;&#25972;&#30340;&#22122;&#22768;&#36807;&#31243;&#27169;&#25311;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connected automated driving has the potential to significantly improve urban traffic efficiency, e.g., by alleviating issues due to occlusion. Cooperative behavior planning can be employed to jointly optimize the motion of multiple vehicles. Most existing approaches to automatic intersection management, however, only consider fully automated traffic. In practice, mixed traffic, i.e., the simultaneous road usage by automated and human-driven vehicles, will be prevalent. The present work proposes to leverage reinforcement learning and a graph-based scene representation for cooperative multi-agent planning. We build upon our previous works that showed the applicability of such machine learning methods to fully automated traffic. The scene representation is extended for mixed traffic and considers uncertainty in the human drivers' intentions. In the simulation-based evaluation, we model measurement uncertainties through noise processes that are tuned using real-world data. The paper evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DiME&#30340;&#20449;&#24687;&#29702;&#35770;&#37327;&#65292;&#21487;&#20197;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#36991;&#20813;&#20102;&#24179;&#20961;&#35299;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.08164</link><description>&lt;p&gt;
DiME&#65306;&#36890;&#36807;&#29109;&#30697;&#38453;&#30340;&#24046;&#24322;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
DiME: Maximizing Mutual Information by a Difference of Matrix-Based Entropies. (arXiv:2301.08164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DiME&#30340;&#20449;&#24687;&#29702;&#35770;&#37327;&#65292;&#21487;&#20197;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#36991;&#20813;&#20102;&#24179;&#20961;&#35299;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20449;&#24687;&#29702;&#35770;&#37327;&#65292;&#20855;&#26377;&#19982;&#30456;&#20114;&#20449;&#24687;&#31867;&#20284;&#30340;&#24615;&#36136;&#65292;&#24182;&#21487;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#28508;&#22312;&#20998;&#24067;&#36827;&#34892;&#26126;&#30830;&#20551;&#35774;&#12290;&#35813;&#25968;&#37327;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#30697;&#38453;&#30340;&#29109;&#65292;&#35813;&#29109;&#21033;&#29992;&#35268;&#33539;&#21270; Gram &#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#26469;&#35745;&#31639;&#20877;&#29983;&#26680; Hilbert &#31354;&#38388;&#20013;&#26410;&#38598;&#20013;&#21327;&#26041;&#24046;&#36816;&#31639;&#31526;&#30340;&#29305;&#24449;&#20540;&#30340;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#24046;&#24322;&#30697;&#38453;&#29109;&#65288;DiME&#65289;&#23545;&#20110;&#28041;&#21450;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30456;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#38750;&#24120;&#36866;&#29992;&#12290;&#34429;&#28982;&#35768;&#22810;&#27492;&#31867;&#20219;&#21153;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#24179;&#20961;&#35299;&#65292;&#20294; DiME &#33258;&#28982;&#20250;&#23545;&#36825;&#26679;&#30340;&#32467;&#26524;&#36827;&#34892;&#24809;&#32602;&#12290;&#25105;&#20204;&#23558; DiME &#19982;&#22810;&#20010;&#30456;&#20114;&#20449;&#24687;&#22522;&#20934;&#20272;&#35745;&#22120;&#22312;&#19968;&#20010;&#29609;&#20855;&#39640;&#26031;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102; DiME &#30340;&#29992;&#20363;&#31034;&#20363;&#65292;&#20363;&#22914;&#28508;&#22312;&#22240;&#23376;&#20998;&#35299;&#21644;&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013; DiME &#34987;&#29992;&#20110;&#23398;&#20064;&#35270;&#22270;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#20855;&#26377;&#39640;&#20114;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an information-theoretic quantity with similar properties to mutual information that can be estimated from data without making explicit assumptions on the underlying distribution. This quantity is based on a recently proposed matrix-based entropy that uses the eigenvalues of a normalized Gram matrix to compute an estimate of the eigenvalues of an uncentered covariance operator in a reproducing kernel Hilbert space. We show that a difference of matrix-based entropies (DiME) is well suited for problems involving the maximization of mutual information between random variables. While many methods for such tasks can lead to trivial solutions, DiME naturally penalizes such outcomes. We compare DiME to several baseline estimators of mutual information on a toy Gaussian dataset. We provide examples of use cases for DiME, such as latent factor disentanglement and a multiview representation learning problem where DiME is used to learn a shared representation among views with high mu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#27668;&#20505;&#25253;&#21578;&#20013;&#22238;&#31572;&#27668;&#20505;&#38382;&#21367;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#27668;&#20505;&#38382;&#21367;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#32467;&#26500;&#35757;&#32451;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#21644;&#20154;&#31867;&#35797;&#39564;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#27668;&#20505;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#27668;&#20505;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2301.04253</link><description>&lt;p&gt;
&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#27668;&#20505;&#25253;&#21578;&#20013;&#22238;&#31572;&#27668;&#20505;&#38382;&#21367;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Answering Climate Questionnaires from Unstructured Climate Reports. (arXiv:2301.04253v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#27668;&#20505;&#25253;&#21578;&#20013;&#22238;&#31572;&#27668;&#20505;&#38382;&#21367;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#27668;&#20505;&#38382;&#21367;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#32467;&#26500;&#35757;&#32451;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#21644;&#20154;&#31867;&#35797;&#39564;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#27668;&#20505;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#27668;&#20505;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27668;&#20505;&#21464;&#21270;&#38382;&#39064;&#32039;&#36843;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23545;&#20854;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#34892;&#21160;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#38656;&#35201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#24222;&#22823;&#19988;&#24555;&#36895;&#22686;&#38271;&#30340;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#27668;&#20505;&#25253;&#21578;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#24418;&#24335;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#27668;&#20505;&#38382;&#21367;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#20854;&#29616;&#26377;&#32467;&#26500;&#26469;&#35757;&#32451;&#33258;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#19981;&#21516;&#32452;&#32455;&#31867;&#22411;&#30340;&#27668;&#20505;&#25259;&#38706;&#36827;&#34892;&#27867;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#31867;&#35797;&#39564;&#20013;&#24110;&#21161;&#23558;&#38750;&#32467;&#26500;&#21270;&#27668;&#20505;&#25991;&#26723;&#20013;&#30340;&#25991;&#26412;&#19982;&#21322;&#32467;&#26500;&#21270;&#38382;&#21367;&#23545;&#40784;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#25903;&#25345;&#27668;&#20505;&#39046;&#22495;&#36827;&#19968;&#27493;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29616;&#26377;&#27668;&#20505;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#21644;&#27604;&#36739;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The topic of Climate Change (CC) has received limited attention in NLP despite its urgency. Activists and policymakers need NLP tools to effectively process the vast and rapidly growing unstructured textual climate reports into structured form. To tackle this challenge we introduce two new large-scale climate questionnaire datasets and use their existing structure to train self-supervised models. We conduct experiments to show that these models can learn to generalize to climate disclosures of different organizations types than seen during training. We then use these models to help align texts from unstructured climate documents to the semi-structured questionnaires in a human pilot study. Finally, to support further NLP research in the climate domain we introduce a benchmark of existing climate text classification datasets to better evaluate and compare existing models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#26377;&#20559;&#25968;&#25454;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#19968;&#33268;&#33539;&#22260;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#22312;&#30446;&#26631;&#20154;&#32676;&#19978;&#26500;&#24314;&#20102;&#21487;&#35777;&#26126;&#20844;&#24179;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.10839</link><description>&lt;p&gt;
&#20844;&#24179;&#39044;&#27979;&#24314;&#27169;&#30340;&#19968;&#31181;&#19968;&#33268;&#33539;&#22260;&#36924;&#36817;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Consistent Range Approximation for Fair Predictive Modeling. (arXiv:2212.10839v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#26377;&#20559;&#25968;&#25454;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#19968;&#33268;&#33539;&#22260;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#22312;&#30446;&#26631;&#20154;&#32676;&#19978;&#26500;&#24314;&#20102;&#21487;&#35777;&#26126;&#20844;&#24179;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#26377;&#20559;&#25968;&#25454;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#23427;&#20511;&#37492;&#20102;&#23545;&#19981;&#23436;&#25972;&#21644;&#19981;&#19968;&#33268;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#22238;&#31572;&#65292;&#20197;&#24418;&#24335;&#21270;&#20844;&#24179;&#26597;&#35810;&#22312;&#30446;&#26631;&#20154;&#32676;&#19978;&#30340;&#19968;&#33268;&#33539;&#22260;&#36924;&#36817;&#65288;CRA&#65289;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#21644;&#26377;&#20559;&#25968;&#25454;&#30340;&#32972;&#26223;&#30693;&#35782;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#30446;&#26631;&#20154;&#32676;&#32479;&#35745;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#20844;&#24179;&#26597;&#35810;&#30340;&#31572;&#26696;&#33539;&#22260;&#12290;&#36890;&#36807;CRA&#65292;&#35813;&#26694;&#26550;&#26500;&#24314;&#30340;&#39044;&#27979;&#27169;&#22411;&#21487;&#20197;&#22312;&#30446;&#26631;&#20154;&#32676;&#19978;&#33719;&#24471;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;&#65292;&#32780;&#19981;&#21463;&#35757;&#32451;&#36807;&#31243;&#20013;&#22806;&#37096;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#38480;&#21046;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel framework for certifying the fairness of predictive models trained on biased data. It draws from query answering for incomplete and inconsistent databases to formulate the problem of consistent range approximation (CRA) of fairness queries for a predictive model on a target population. The framework employs background knowledge of the data collection process and biased data, working with or without limited statistics about the target population, to compute a range of answers for fairness queries. Using CRA, the framework builds predictive models that are certifiably fair on the target population, regardless of the availability of external data during training. The framework's efficacy is demonstrated through evaluations on real data, showing substantial improvement over existing state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#24179;&#28369;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;Shapley&#26354;&#32447;&#20316;&#20026;&#23616;&#37096;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#22312;&#29305;&#24449;&#30340;&#29420;&#31435;&#21644;&#20381;&#36182;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#20026;&#20272;&#35745;&#30340;Shapley&#26354;&#32447;&#26500;&#24314;&#20102;&#32622;&#20449;&#21306;&#38388;&#24182;&#36827;&#34892;&#20102;&#25512;&#26029;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#28176;&#36817;&#32467;&#26524;&#12290;&#24212;&#29992;&#20013;&#20998;&#26512;&#20102;&#21738;&#20123;&#23646;&#24615;&#39537;&#21160;&#36710;&#36742;&#20215;&#26684;&#12290;</title><link>http://arxiv.org/abs/2211.13289</link><description>&lt;p&gt;
Shapley&#26354;&#32447;&#65306;&#19968;&#31181;&#24179;&#28369;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Shapley Curves: A Smoothing Perspective. (arXiv:2211.13289v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#24179;&#28369;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;Shapley&#26354;&#32447;&#20316;&#20026;&#23616;&#37096;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#22312;&#29305;&#24449;&#30340;&#29420;&#31435;&#21644;&#20381;&#36182;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#20026;&#20272;&#35745;&#30340;Shapley&#26354;&#32447;&#26500;&#24314;&#20102;&#32622;&#20449;&#21306;&#38388;&#24182;&#36827;&#34892;&#20102;&#25512;&#26029;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#28176;&#36817;&#32467;&#26524;&#12290;&#24212;&#29992;&#20013;&#20998;&#26512;&#20102;&#21738;&#20123;&#23646;&#24615;&#39537;&#21160;&#36710;&#36742;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#33258;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#65292;Shapley&#20540;&#24050;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#24230;&#37327;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23545;Shapley&#20540;&#30340;&#32479;&#35745;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#20197;&#38750;&#21442;&#25968;(&#25110;&#24179;&#28369;)&#30340;&#35282;&#24230;&#65292;&#24341;&#20837;Shapley&#26354;&#32447;&#20316;&#20026;&#23616;&#37096;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#22312;&#29305;&#24449;&#29420;&#31435;&#21644;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#37117;&#24471;&#20986;&#20102;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#24182;&#23545;&#20272;&#35745;&#30340;Shapley&#26354;&#32447;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37326;&#34542;&#24341;&#23548;&#31243;&#24207;&#29256;&#26412;&#65292;&#19987;&#38376;&#35843;&#25972;&#20197;&#33719;&#24471;Shapley&#26354;&#32447;&#30340;&#33391;&#22909;&#26377;&#38480;&#26679;&#26412;&#35206;&#30422;&#12290;&#28176;&#36817;&#32467;&#26524;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#12290;&#22312;&#23454;&#35777;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21738;&#20123;&#23646;&#24615;&#39537;&#21160;&#20102;&#36710;&#36742;&#30340;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originating from cooperative game theory, Shapley values have become one of the most widely used measures for variable importance in applied Machine Learning. However, the statistical understanding of Shapley values is still limited. In this paper, we take a nonparametric (or smoothing) perspective by introducing Shapley curves as a local measure of variable importance. We propose two estimation strategies and derive the consistency and asymptotic normality both under independence and dependence among the features. This allows us to construct confidence intervals and conduct inference on the estimated Shapley curves. We propose a novel version of the wild bootstrap procedure, specifically adjusted to give good finite sample coverage of the Shapley curves. The asymptotic results are validated in extensive experiments. In an empirical application, we analyze which attributes drive the prices of vehicles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#38750;&#32447;&#24615;&#29305;&#24449;&#21521;&#37327;&#31639;&#27861;&#65288;WDA-nepv&#65289;&#30340;Wasserstein&#21028;&#21035;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#19981;&#21516;&#31867;&#21035;&#30340;&#31163;&#25955;&#24230;&#65292;&#24182;&#26368;&#23567;&#21270;&#30456;&#21516;&#31867;&#21035;&#30340;&#31163;&#25955;&#24230;&#65292;&#20805;&#20998;&#21033;&#29992;WDA&#30340;&#21452;&#23618;&#20248;&#21270;&#32467;&#26500;&#65292;&#24182;&#22312;&#33258;&#27965;&#22330;&#26694;&#26550;&#19979;&#39640;&#25928;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2211.11891</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#23618;&#38750;&#32447;&#24615;&#29305;&#24449;&#21521;&#37327;&#31639;&#27861;&#30340;Wasserstein&#21028;&#21035;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Bi-level Nonlinear Eigenvector Algorithm for Wasserstein Discriminant Analysis. (arXiv:2211.11891v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#38750;&#32447;&#24615;&#29305;&#24449;&#21521;&#37327;&#31639;&#27861;&#65288;WDA-nepv&#65289;&#30340;Wasserstein&#21028;&#21035;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#19981;&#21516;&#31867;&#21035;&#30340;&#31163;&#25955;&#24230;&#65292;&#24182;&#26368;&#23567;&#21270;&#30456;&#21516;&#31867;&#21035;&#30340;&#31163;&#25955;&#24230;&#65292;&#20805;&#20998;&#21033;&#29992;WDA&#30340;&#21452;&#23618;&#20248;&#21270;&#32467;&#26500;&#65292;&#24182;&#22312;&#33258;&#27965;&#22330;&#26694;&#26550;&#19979;&#39640;&#25928;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#21516;&#32463;&#20856;&#30340;Fisher&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;Wasserstein&#21028;&#21035;&#20998;&#26512;&#65288;WDA&#65289;&#26159;&#19968;&#31181;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26469;&#23547;&#27714;&#19968;&#20010;&#25237;&#24433;&#30697;&#38453;&#65292;&#26368;&#22823;&#21270;&#19981;&#21516;&#25968;&#25454;&#31867;&#21035;&#30340;&#31163;&#25955;&#24230;&#65292;&#24182;&#26368;&#23567;&#21270;&#30456;&#21516;&#25968;&#25454;&#31867;&#21035;&#30340;&#31163;&#25955;&#24230;&#12290;&#19982;LDA&#19981;&#21516;&#65292;WDA&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#21487;&#20197;&#32771;&#34385;&#25968;&#25454;&#31867;&#21035;&#20043;&#38388;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#30456;&#20114;&#20851;&#32852;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#38750;&#32447;&#24615;&#29305;&#24449;&#21521;&#37327;&#31639;&#27861;&#65288;WDA-nepv&#65289;&#26469;&#20805;&#20998;&#21033;&#29992;WDA&#30340;&#21452;&#23618;&#20248;&#21270;&#32467;&#26500;&#12290;WDA-nepv&#30340;&#20869;&#37096;&#23618;&#29992;&#20110;&#35745;&#31639;&#26368;&#20248;&#20256;&#36755;&#30697;&#38453;&#65292;&#24182;&#34987;&#26500;&#36896;&#20026;&#19968;&#20010;&#20381;&#36182;&#20110;&#29305;&#24449;&#21521;&#37327;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#20540;&#38382;&#39064;&#65288;NEPv&#65289;&#65292;&#21516;&#26102;&#65292;&#22806;&#37096;&#23618;&#29992;&#20110;&#36861;&#36394;&#27604;&#29575;&#20248;&#21270;&#65292;&#24182;&#34987;&#26500;&#36896;&#20026;&#21478;&#19968;&#20010;NEPv&#38382;&#39064;&#12290;&#36825;&#20004;&#20010;NEPv&#38382;&#39064;&#21487;&#20197;&#22312;&#33258;&#27965;&#22330;&#65288;SCF&#65289;&#26694;&#26550;&#19979;&#39640;&#25928;&#35745;&#31639;&#12290;WDA-nepv&#26159;&#21487;&#23548;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much like the classical Fisher linear discriminant analysis (LDA), the recently proposed Wasserstein discriminant analysis (WDA) is a linear dimensionality reduction method that seeks a projection matrix to maximize the dispersion of different data classes and minimize the dispersion of same data classes via a bi-level optimization. In contrast to LDA, WDA can account for both global and local interconnections between data classes by using the underlying principles of optimal transport. In this paper, a bi-level nonlinear eigenvector algorithm (WDA-nepv) is presented to fully exploit the structures of the bi-level optimization of WDA. The inner level of WDA-nepv for computing the optimal transport matrices is formulated as an eigenvector-dependent nonlinear eigenvalue problem (NEPv), and meanwhile, the outer level for trace ratio optimizations is formulated as another NEPv. Both NEPvs can be computed efficiently under the self-consistent field (SCF) framework. WDA-nepv is derivative-fr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Refoqus&#30340;&#36164;&#28304;&#33410;&#32422;&#30340;&#37327;&#23376;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#21516;&#26102;&#38543;&#26426;&#37319;&#26679;&#25968;&#25454;&#38598;&#21644;&#27979;&#37327;&#25805;&#20316;&#65292;&#33021;&#22815;&#20445;&#23384;&#22823;&#37327;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2211.04965</link><description>&lt;p&gt;
&#36164;&#28304;&#33410;&#32422;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Resource frugal optimizer for quantum machine learning. (arXiv:2211.04965v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Refoqus&#30340;&#36164;&#28304;&#33410;&#32422;&#30340;&#37327;&#23376;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#21516;&#26102;&#38543;&#26426;&#37319;&#26679;&#25968;&#25454;&#38598;&#21644;&#27979;&#37327;&#25805;&#20316;&#65292;&#33021;&#22815;&#20445;&#23384;&#22823;&#37327;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#22686;&#24378;&#30340;&#25968;&#25454;&#31185;&#23398;&#65292;&#20063;&#31216;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#65292;&#20316;&#20026;&#36817;&#26399;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#21464;&#20998;QML&#31639;&#27861;&#22312;&#28041;&#21450;&#37327;&#23376;&#25968;&#25454;&#26102;&#26377;&#33021;&#21147;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#31639;&#27861;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#38656;&#35201;&#23450;&#21046;&#30340;&#20248;&#21270;&#31243;&#24207;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;QML&#24212;&#29992;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#37319;&#26679;&#27425;&#25968;&#65292;&#22240;&#20026;&#28041;&#21450;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20513;&#23545;&#25968;&#25454;&#38598;&#21644;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#30340;&#27979;&#37327;&#25805;&#20316;&#36827;&#34892;&#21516;&#26102;&#38543;&#26426;&#37319;&#26679;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#39640;&#24230;&#36890;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#20102;&#35768;&#22810;QML&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314;&#20854;&#26799;&#24230;&#30340;&#26080;&#20559;&#20272;&#35745;&#22120;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;Refoqus&#65288;&#36164;&#28304;&#33410;&#32422;&#30340;&#37327;&#23376;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#65289;&#30340;&#33410;&#32422;&#37319;&#26679;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;Refoqus&#33021;&#22815;&#33410;&#30465;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum-enhanced data science, also known as quantum machine learning (QML), is of growing interest as an application of near-term quantum computers. Variational QML algorithms have the potential to solve practical problems on real hardware, particularly when involving quantum data. However, training these algorithms can be challenging and calls for tailored optimization procedures. Specifically, QML applications can require a large shot-count overhead due to the large datasets involved. In this work, we advocate for simultaneous random sampling over both the dataset as well as the measurement operators that define the loss function. We consider a highly general loss function that encompasses many QML applications, and we show how to construct an unbiased estimator of its gradient. This allows us to propose a shot-frugal gradient descent optimizer called Refoqus (REsource Frugal Optimizer for QUantum Stochastic gradient descent). Our numerics indicate that Refoqus can save several orde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27880;&#24847;&#21147;&#31070;&#32463;&#31639;&#23376;&#65288;HANO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#20809;&#35889;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.10890</link><description>&lt;p&gt;
&#32531;&#35299;&#20998;&#23618;&#27880;&#24847;&#21147;&#22810;&#23610;&#24230;&#31639;&#23376;&#23398;&#20064;&#20013;&#30340;&#20809;&#35889;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating spectral bias for the multiscale operator learning with hierarchical attention. (arXiv:2210.10890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27880;&#24847;&#21147;&#31070;&#32463;&#31639;&#23376;&#65288;HANO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#20809;&#35889;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26080;&#38480;&#32500;&#21442;&#25968;&#21644;&#35299;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#30340;&#22810;&#23610;&#24230;PDE&#65292;&#22914;&#27833;&#34255;&#24314;&#27169;&#21644;&#28237;&#27969;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#36825;&#31181;PDE&#65292;&#23545;&#20302;&#39057;&#20998;&#37327;&#23384;&#22312;&#20809;&#35889;&#20559;&#24046;&#26159;&#29616;&#26377;&#31070;&#32463;&#31639;&#23376;&#30340;&#19968;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#23618;&#27425;&#30697;&#38453;&#26041;&#27861;&#21551;&#21457;&#30340;&#20998;&#23618;&#27880;&#24847;&#21147;&#31070;&#32463;&#31639;&#23376;&#65288;HANO&#65289;&#12290;HANO&#20855;&#26377;&#33258;&#36866;&#24212;&#23610;&#24230;&#20132;&#20114;&#33539;&#22260;&#21644;&#23618;&#27425;&#32467;&#26500;&#19978;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#23454;&#29616;&#21487;&#25511;&#32447;&#24615;&#25104;&#26412;&#30340;&#23884;&#22871;&#29305;&#24449;&#35745;&#31639;&#21644;&#22810;&#23610;&#24230;&#35299;&#31354;&#38388;&#30340;&#32534;&#30721;/&#35299;&#30721;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#32463;&#39564;H^1&#25439;&#22833;&#20989;&#25968;&#26469;&#22686;&#24378;&#23545;&#39640;&#39057;&#20998;&#37327;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;HANO&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators have emerged as a powerful tool for learning the mapping between infinite-dimensional parameter and solution spaces of partial differential equations (PDEs). In this work, we focus on multiscale PDEs that have important applications such as reservoir modeling and turbulence prediction. We demonstrate that for such PDEs, the spectral bias towards low-frequency components presents a significant challenge for existing neural operators. To address this challenge, we propose a hierarchical attention neural operator (HANO) inspired by the hierarchical matrix approach. HANO features a scale-adaptive interaction range and self-attentions over a hierarchy of levels, enabling nested feature computation with controllable linear cost and encoding/decoding of multiscale solution space. We also incorporate an empirical $H^1$ loss function to enhance the learning of high-frequency components. Our numerical experiments demonstrate that HANO outperforms state-of-the-art (SOTA) methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#31995;&#32479;&#20013;&#30340;&#31070;&#32463;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#27010;&#29575;1&#22312;&#25351;&#23450;&#30340;&#31283;&#23450;&#21306;&#22495;&#20869;&#23454;&#29616;&#31995;&#32479;&#31283;&#23450;&#65292;&#24182;&#24341;&#20837;&#20102;&#31283;&#23450;&#25490;&#24207;&#36229;&#32423;&#38789;(sRSMs)&#30340;&#27010;&#24565;&#26469;&#20811;&#26381;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05304</link><description>&lt;p&gt;
&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#31995;&#32479;&#30340;&#21487;&#35777;&#26126;&#31283;&#23450;&#31070;&#32463;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Provably Stabilizing Neural Controllers for Discrete-Time Stochastic Systems. (arXiv:2210.05304v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#31995;&#32479;&#20013;&#30340;&#31070;&#32463;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#27010;&#29575;1&#22312;&#25351;&#23450;&#30340;&#31283;&#23450;&#21306;&#22495;&#20869;&#23454;&#29616;&#31995;&#32479;&#31283;&#23450;&#65292;&#24182;&#24341;&#20837;&#20102;&#31283;&#23450;&#25490;&#24207;&#36229;&#32423;&#38789;(sRSMs)&#30340;&#27010;&#24565;&#26469;&#20811;&#26381;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#31995;&#32479;&#20013;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#35813;&#31574;&#30053;&#20445;&#35777;&#31995;&#32479;&#20197;&#27010;&#29575;1&#22312;&#26576;&#20010;&#25351;&#23450;&#30340;&#31283;&#23450;&#21306;&#22495;&#20869;&#31283;&#23450;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#30340;&#21019;&#26032;&#27010;&#24565;&#8212;&#8212;&#31283;&#23450;&#25490;&#24207;&#36229;&#32423;&#38789;(sRSMs)&#12290;&#25105;&#20204;&#30340;sRSMs&#20811;&#26381;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#36827;&#20837;&#31283;&#23450;&#21306;&#22495;&#21518;&#26080;&#27861;&#31163;&#24320;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#23398;&#20064;&#19968;&#20010;&#25511;&#21046;&#31574;&#30053;&#21644;&#19968;&#20010;&#27491;&#24335;&#35777;&#26126;&#27010;&#29575;1&#31283;&#23450;&#24615;&#30340;sRSM&#65292;&#20004;&#32773;&#37117;&#20197;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#35813;&#36807;&#31243;&#21487;&#20197;&#36866;&#24212;&#20110;&#22312;&#32473;&#23450;&#30340;Lipschitz&#36830;&#32493;&#25511;&#21046;&#31574;&#30053;&#19979;&#65292;&#39564;&#35777;&#38543;&#26426;&#31995;&#32479;&#20197;&#27010;&#29575;1&#22312;&#26576;&#20010;&#31283;&#23450;&#21306;&#22495;&#20869;&#31283;&#23450;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23398;&#20064;&#36807;&#31243;&#33021;&#22815;&#25104;&#21151;&#22320;&#23398;&#20064;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#31070;&#32463;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning control policies in discrete-time stochastic systems which guarantee that the system stabilizes within some specified stabilization region with probability~$1$. Our approach is based on the novel notion of stabilizing ranking supermartingales (sRSMs) that we introduce in this work. Our sRSMs overcome the limitation of methods proposed in previous works whose applicability is restricted to systems in which the stabilizing region cannot be left once entered under any control policy. We present a learning procedure that learns a control policy together with an sRSM that formally certifies probability~$1$ stability, both learned as neural networks. We show that this procedure can also be adapted to formally verifying that, under a given Lipschitz continuous control policy, the stochastic system stabilizes within some stabilizing region with probability~$1$. Our experimental evaluation shows that our learning procedure can successfully learn provably stab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Passau-SFCH&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#29992;&#20110;&#33258;&#21457;&#24189;&#40664;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#30340;&#20998;&#26512;&#21644;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#24189;&#40664;&#20197;&#21450;&#24189;&#40664;&#24773;&#24863;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2209.14272</link><description>&lt;p&gt;
&#36808;&#21521;&#22810;&#27169;&#24577;&#39044;&#27979;&#33258;&#21457;&#24189;&#40664;&#65306;&#19968;&#20221;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21644;&#21021;&#27493;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results. (arXiv:2209.14272v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Passau-SFCH&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#29992;&#20110;&#33258;&#21457;&#24189;&#40664;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#30340;&#20998;&#26512;&#21644;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#24189;&#40664;&#20197;&#21450;&#24189;&#40664;&#24773;&#24863;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#24773;&#24863;&#21644;&#35748;&#30693;&#30340;&#37325;&#35201;&#20803;&#32032;&#12290;&#20854;&#33258;&#21160;&#29702;&#35299;&#21487;&#20197;&#20419;&#36827;&#26356;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20154;&#24615;&#21270;&#12290;&#30446;&#21069;&#30340;&#24189;&#40664;&#26816;&#27979;&#26041;&#27861;&#20165;&#22522;&#20110;&#31574;&#21010;&#25968;&#25454;&#65292;&#19981;&#33021;&#28385;&#36275;&#8220;&#29616;&#23454;&#19990;&#30028;&#8221;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;Passau-Spontaneous Football Coach Humour&#65288;Passau-SFCH&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#32570;&#38519;&#12290;Passau-SFCH&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26681;&#25454;Martin&#30340;&#24189;&#40664;&#39118;&#26684;&#38382;&#21367;&#25552;&#20986;&#30340;&#24189;&#40664;&#23384;&#22312;&#21450;&#20854;&#32500;&#24230;&#65288;&#24773;&#24863;&#21644;&#26041;&#21521;&#65289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#19987;&#23478;&#35774;&#35745;&#30340;&#29305;&#24449;&#12290;&#20998;&#26512;&#20102;&#33258;&#21457;&#24189;&#40664;&#35782;&#21035;&#30340;&#27599;&#31181;&#27169;&#24577;&#65288;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#24189;&#40664;&#21450;&#20854;&#24773;&#24863;&#30340;&#33258;&#21160;&#20998;&#26512;&#65292;&#22810;&#27169;&#24577;&#32852;&#21512;&#20351;&#29992;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humour is a substantial element of human affect and cognition. Its automatic understanding can facilitate a more naturalistic human-device interaction and the humanisation of artificial intelligence. Current methods of humour detection are solely based on staged data making them inadequate for 'real-world' applications. We address this deficiency by introducing the novel Passau-Spontaneous Football Coach Humour (Passau-SFCH) dataset, comprising of about 11 hours of recordings. The Passau-SFCH dataset is annotated for the presence of humour and its dimensions (sentiment and direction) as proposed in Martin's Humor Style Questionnaire. We conduct a series of experiments, employing pretrained Transformers, convolutional neural networks, and expert-designed features. The performance of each modality (text, audio, video) for spontaneous humour recognition is analysed and their complementarity is investigated. Our findings suggest that for the automatic analysis of humour and its sentiment, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SynthA1c&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#25968;&#25454;&#26469;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#39118;&#38505;&#65292;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#34880;&#28082;&#23454;&#39564;&#23460;&#27979;&#37327;&#65292;&#20854;&#25935;&#24863;&#24615;&#39640;&#36798;87.6%&#12290;</title><link>http://arxiv.org/abs/2209.10043</link><description>&lt;p&gt;
SynthA1c:&#38024;&#23545;&#31958;&#23615;&#30149;&#39118;&#38505;&#20998;&#23618;&#30340;&#20020;&#24202;&#35299;&#37322;&#22411;&#24739;&#32773;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SynthA1c: Towards Clinically Interpretable Patient Representations for Diabetes Risk Stratification. (arXiv:2209.10043v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SynthA1c&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#25968;&#25454;&#26469;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#39118;&#38505;&#65292;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#34880;&#28082;&#23454;&#39564;&#23460;&#27979;&#37327;&#65292;&#20854;&#25935;&#24863;&#24615;&#39640;&#36798;87.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#35786;&#26029;2&#22411;&#31958;&#23615;&#30149;(T2DM)&#23545;&#20110;&#21551;&#21160;&#21450;&#26102;&#30340;&#27835;&#30103;&#24178;&#39044;&#21644;&#29983;&#27963;&#26041;&#24335;&#25913;&#21464;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#20020;&#24202;&#35786;&#25152;&#35775;&#38382;&#26102;&#38388;&#30340;&#32553;&#30701;&#21644;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#21033;&#29992;&#24739;&#32773;&#22270;&#20687;&#25968;&#25454;&#26426;&#20250;&#24615;&#22320;&#36890;&#36807;&#21307;&#29983;&#23545;T2DM&#36827;&#34892;&#39069;&#22806;&#35786;&#26029;&#24037;&#20316;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22270;&#20687;&#34893;&#29983;&#30340;&#34920;&#22411;&#25968;&#25454;&#22312;&#34920;&#26684;&#23398;&#20064;&#20998;&#31867;&#22120;&#27169;&#22411;&#20013;&#39044;&#27979;T2DM&#39118;&#38505;&#65292;&#20197;&#33258;&#21160;&#22320;&#30830;&#23450;&#39640;&#39118;&#38505;&#24739;&#32773;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#34880;&#28082;&#23454;&#39564;&#23460;&#27979;&#35797;&#12290;&#19982;&#20256;&#32479;&#30340;&#20108;&#20998;&#31867;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#20915;&#31574;&#26641;&#27169;&#22411;&#23558;&#24739;&#32773;&#25968;&#25454;&#34920;&#31034;&#20026;&#8220;SynthA1c&#8221;&#28508;&#22312;&#21464;&#37327;&#65292;&#36825;&#20123;&#21464;&#37327;&#27169;&#25311;&#20102;&#34880;&#32418;&#34507;&#30333;A1c&#30340;&#32463;&#39564;&#23454;&#39564;&#23460;&#27979;&#37327;&#32467;&#26524;&#65292;&#20854;&#25935;&#24863;&#24615;&#39640;&#36798;87.6%&#12290;&#20026;&#20102;&#35780;&#20272;SynthA1c&#27169;&#22411;&#22312;&#20854;&#20182;&#24739;&#32773;&#32676;&#20307;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25512;&#24191;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of Type 2 Diabetes Mellitus (T2DM) is crucial to enable timely therapeutic interventions and lifestyle modifications. As the time available for clinical office visits shortens and medical imaging data become more widely available, patient image data could be used to opportunistically identify patients for additional T2DM diagnostic workup by physicians. We investigated whether image-derived phenotypic data could be leveraged in tabular learning classifier models to predict T2DM risk in an automated fashion to flag high-risk patients without the need for additional blood laboratory measurements. In contrast to traditional binary classifiers, we leverage neural networks and decision tree models to represent patient data as 'SynthA1c' latent variables, which mimic blood hemoglobin A1c empirical lab measurements, that achieve sensitivities as high as 87.6%. To evaluate how SynthA1c models may generalize to other patient populations, we introduce a novel generalizable metric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31867;&#22411;&#25512;&#26029;&#31995;&#32479;Type4Py&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Type4Py&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#31867;&#22411;&#25512;&#26029;&#26102;&#33021;&#22815;&#25552;&#20379;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09189</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31867;&#22411;&#25512;&#26029;&#31995;&#32479;&#30340;&#36328;&#22495;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Evaluation of a Deep Learning-Based Type Inference System. (arXiv:2208.09189v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31867;&#22411;&#25512;&#26029;&#31995;&#32479;Type4Py&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Type4Py&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#31867;&#22411;&#25512;&#26029;&#26102;&#33021;&#22815;&#25552;&#20379;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36873;&#31867;&#22411;&#27880;&#37322;&#20801;&#35768;&#22312;&#21160;&#24577;&#32534;&#31243;&#35821;&#35328;&#20013;&#22686;&#21152;&#38745;&#24577;&#31867;&#22411;&#29305;&#24615;&#65292;&#20363;&#22914;&#26356;&#22909;&#30340;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDE&#65289;&#25903;&#25345;&#12289;&#26356;&#31934;&#30830;&#30340;&#31243;&#24207;&#20998;&#26512;&#20197;&#21450;&#31867;&#22411;&#30456;&#20851;&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#26089;&#26399;&#26816;&#27979;&#21644;&#39044;&#38450;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31867;&#22411;&#25512;&#26029;&#20026;&#33258;&#21160;&#21270;&#27492;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#20351;&#29992;&#21462;&#20915;&#20110;&#23427;&#20204;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#32463;&#24120;&#34987;&#24212;&#29992;&#20110;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#12290;&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#24191;&#27867;&#30340;&#36328;&#39046;&#22495;&#23454;&#39564;&#65292;&#23558;Type4Py&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31867;&#22411;&#25512;&#26029;&#31995;&#32479;&#30340;&#20195;&#34920;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#35299;&#20915;&#20197;&#19979;&#38382;&#39064;&#65306;&#31867;&#19981;&#24179;&#34913;&#12289;&#35789;&#27719;&#34920;&#22806;&#21333;&#35789;&#12289;&#25968;&#25454;&#38598;&#36716;&#31227;&#21644;&#26410;&#30693;&#31867;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#26679;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;ManyTypes4Py&#21644;CrossDomainTypes4Py&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20171;&#32461;&#20102;&#21518;&#32773;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#31867;&#22411;&#25512;&#26029;&#31995;&#32479;&#65292;&#22312;&#35813;&#24773;&#20917;&#19979;&#31243;&#24207;&#39046;&#22495;&#19982;&#35757;&#32451;&#38598;&#20013;&#30340;&#39046;&#22495;&#19981;&#21516;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;Type4Py&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#20026;&#19981;&#21516;&#39046;&#22495;&#30340;&#31867;&#22411;&#25512;&#26029;&#25552;&#20379;&#20102;&#21487;&#20280;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optional type annotations allow for enriching dynamic programming languages with static typing features like better Integrated Development Environment (IDE) support, more precise program analysis, and early detection and prevention of type-related runtime errors. Machine learning-based type inference promises interesting results for automating this task. However, the practical usage of such systems depends on their ability to generalize across different domains, as they are often applied outside their training domain. In this work, we investigate Type4Py as a representative of state-of-the-art deep learning-based type inference systems, by conducting extensive cross-domain experiments. Thereby, we address the following problems: class imbalances, out-of-vocabulary words, dataset shifts, and unknown classes. To perform such experiments, we use the datasets ManyTypes4Py and CrossDomainTypes4Py. The latter we introduce in this paper. Our dataset enables the evaluation of type inference sy
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#23545;&#40784;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.07734</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#20010;&#36229;&#21442;&#25968;&#65306;&#31934;&#24515;&#31579;&#36873;&#30340;&#33258;&#30417;&#30563;&#23545;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#25104;&#21151;&#20135;&#29983;&#20102;&#24187;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07734
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#23545;&#40784;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#21019;&#24314;&#30417;&#30563;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#24040;&#22823;&#25104;&#26412;&#12290;&#23545;&#20110;&#26631;&#35760;&#24322;&#24120;&#31232;&#32570;&#25110;&#20960;&#20046;&#19981;&#23384;&#22312;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65288;&#22914;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;SSL&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#12290;&#36807;&#21435;&#24050;&#32463;&#20351;&#29992;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#26469;&#36827;&#34892;&#22522;&#20110;SSL&#30340;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#30340;&#31867;&#22411;&#23545;&#20934;&#30830;&#24615;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#26816;&#27979;&#27169;&#22411;&#21644;420&#20010;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25968;&#23383;&#21644;&#21487;&#35270;&#35777;&#25454;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;SSAD&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#32780;&#22312;&#32570;&#20047;&#23545;&#40784;&#30340;&#24773;&#20917;&#19979;&#65292;SSL&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#20851;&#20110;&#22270;&#20687;&#22411;SSAD&#30340;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has emerged as a promising alternative to create supervisory signals to real-world problems, avoiding the extensive cost of manual labeling. SSL is particularly attractive for unsupervised tasks such as anomaly detection (AD), where labeled anomalies are rare or often nonexistent. A large catalog of augmentation functions has been used for SSL-based AD (SSAD) on image data, and recent works have reported that the type of augmentation has a significant impact on accuracy. Motivated by those, this work sets out to put image-based SSAD under a larger lens and investigate the role of data augmentation in SSAD. Through extensive experiments on 3 different detector models and across 420 AD tasks, we provide comprehensive numerical and visual evidences that the alignment between data augmentation and anomaly-generating mechanism is the key to the success of SSAD, and in the lack thereof, SSL may even impair accuracy. To the best of our knowledge, this is the fir
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#20445;&#30495;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#24265;&#20215;&#30340;&#20302;&#20445;&#30495;&#24230;&#25968;&#25454;&#21644;&#26114;&#36149;&#30340;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#38656;&#35201;&#26377;&#25928;&#30456;&#20851;&#24615;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.05606</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#21450;&#20854;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity wavelet neural operator with application to uncertainty quantification. (arXiv:2208.05606v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05606
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#20445;&#30495;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#24265;&#20215;&#30340;&#20302;&#20445;&#30495;&#24230;&#25968;&#25454;&#21644;&#26114;&#36149;&#30340;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#38656;&#35201;&#26377;&#25928;&#30456;&#20851;&#24615;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#31639;&#23398;&#20064;&#26694;&#26550;&#22240;&#20854;&#22312;&#20004;&#20010;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#23398;&#20064;&#38750;&#32447;&#24615;&#26144;&#23556;&#30340;&#33021;&#21147;&#20197;&#21450;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#32780;&#22312;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#36817;&#25104;&#20026;&#19968;&#20010;&#26356;&#20026;&#37325;&#35201;&#30340;&#39046;&#22495;&#12290;&#23613;&#31649;&#36825;&#20123;&#26694;&#26550;&#22312;&#24314;&#27169;&#22797;&#26434;&#29616;&#35937;&#26041;&#38754;&#38750;&#24120;&#26377;&#33021;&#21147;&#65292;&#20294;&#20026;&#20102;&#25104;&#21151;&#35757;&#32451;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#19981;&#21487;&#29992;&#25110;&#36807;&#20110;&#26114;&#36149;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#23398;&#20064;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#27169;&#22411;&#20351;&#29992;&#22823;&#37327;&#24265;&#20215;&#30340;&#20302;&#20445;&#30495;&#24230;&#25968;&#25454;&#21644;&#23569;&#37327;&#26114;&#36149;&#30340;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#22810;&#20445;&#30495;&#24230;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#35299;&#20915;&#38656;&#35201;&#26377;&#25928;&#30456;&#20851;&#24615;&#23398;&#20064;&#30340;&#19981;&#21516;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#24320;&#21457;&#27169;&#22411;&#30340;&#20986;&#33394;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Operator learning frameworks, because of their ability to learn nonlinear maps between two infinite dimensional functional spaces and utilization of neural networks in doing so, have recently emerged as one of the more pertinent areas in the field of applied machine learning. Although these frameworks are extremely capable when it comes to modeling complex phenomena, they require an extensive amount of data for successful training which is often not available or is too expensive. However, this issue can be alleviated with the use of multi-fidelity learning, where a model is trained by making use of a large amount of inexpensive low-fidelity data along with a small amount of expensive high-fidelity data. To this end, we develop a new framework based on the wavelet neural operator which is capable of learning from a multi-fidelity dataset. The developed model's excellent learning capabilities are demonstrated by solving different problems which require effective correlation learning betw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#24674;&#22797;&#20934;&#30830;&#24230;&#36798;&#21040;90-97%&#12290;&#36825;&#19968;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#27969;&#31243;&#27169;&#22411;&#21644;&#38543;&#26426;&#24050;&#30693;&#36712;&#36857;&#30340;&#21512;&#35268;&#24615;&#65292;&#24182;&#24674;&#22797;&#22312;&#35813;&#38543;&#26426;&#36712;&#36857;&#20013;&#30340;&#26368;&#20339;&#23545;&#40784;&#20316;&#20026;&#30495;&#23454;&#36712;&#36857;&#12290;&#23545;&#27604;&#20854;&#20182;&#36712;&#36857;&#24674;&#22797;&#36873;&#39033;&#65292;&#20351;&#29992;&#20102;&#20135;&#21697;&#22810;&#22270;&#26469;&#20998;&#26512;&#25104;&#26412;&#27169;&#22411;&#23545;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#31639;&#27861;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#12289;&#38169;&#35823;&#25490;&#26597;&#21644;&#31995;&#32479;&#24615;&#33021;&#25913;&#36827;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2206.12672</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Trace Recovery from Stochastically Known Logs. (arXiv:2206.12672v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#24674;&#22797;&#20934;&#30830;&#24230;&#36798;&#21040;90-97%&#12290;&#36825;&#19968;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#27969;&#31243;&#27169;&#22411;&#21644;&#38543;&#26426;&#24050;&#30693;&#36712;&#36857;&#30340;&#21512;&#35268;&#24615;&#65292;&#24182;&#24674;&#22797;&#22312;&#35813;&#38543;&#26426;&#36712;&#36857;&#20013;&#30340;&#26368;&#20339;&#23545;&#40784;&#20316;&#20026;&#30495;&#23454;&#36712;&#36857;&#12290;&#23545;&#27604;&#20854;&#20182;&#36712;&#36857;&#24674;&#22797;&#36873;&#39033;&#65292;&#20351;&#29992;&#20102;&#20135;&#21697;&#22810;&#22270;&#26469;&#20998;&#26512;&#25104;&#26412;&#27169;&#22411;&#23545;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#31639;&#27861;&#23545;&#20110;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#12289;&#38169;&#35823;&#25490;&#26597;&#21644;&#31995;&#32479;&#24615;&#33021;&#25913;&#36827;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#36712;&#36857;&#30340;&#31639;&#27861;&#12290;&#38543;&#30528;&#20256;&#24863;&#22120;&#25968;&#37327;&#30340;&#22686;&#21152;&#21644;&#29983;&#25104;&#19981;&#30830;&#23450;&#25968;&#25454;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#35774;&#32622;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35745;&#31639;&#27969;&#31243;&#27169;&#22411;&#19982;&#38543;&#26426;&#24050;&#30693;&#36712;&#36857;&#20043;&#38388;&#30340;&#21512;&#35268;&#24615;&#65292;&#24182;&#22312;&#36825;&#20010;&#38543;&#26426;&#36712;&#36857;&#20013;&#24674;&#22797;&#26368;&#20339;&#23545;&#40784;&#20316;&#20026;&#30495;&#23454;&#36712;&#36857;&#12290;&#35770;&#25991;&#23545;&#19981;&#21516;&#25104;&#26412;&#27169;&#22411;&#23545;&#36712;&#36857;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#21033;&#29992;&#20135;&#21697;&#22810;&#22270;&#26469;&#27604;&#36739;&#26367;&#20195;&#36712;&#36857;&#24674;&#22797;&#36873;&#39033;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#24179;&#22343;&#24674;&#22797;&#20934;&#30830;&#24230;&#36798;&#21040;90-97%&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#24120;&#35265;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36873;&#25321;&#27599;&#20010;&#19981;&#30830;&#23450;&#27963;&#21160;&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20174;&#38543;&#26426;&#24050;&#30693;&#26085;&#24535;&#20013;&#24674;&#22797;&#27491;&#30830;&#36712;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#26159;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#12289;&#38169;&#35823;&#25490;&#26597;&#21644;&#25913;&#36827;&#31995;&#32479;&#24615;&#33021;&#30340;&#26377;&#21147;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose an algorithm for trace recovery from stochastically known logs, a setting that is becoming more common with the increasing number of sensors and predictive models that generate uncertain data. The suggested approach calculates the conformance between a process model and a stochastically known trace and recovers the best alignment within this stochastic trace as the true trace. The paper offers an analysis of the impact of various cost models on trace recovery accuracy and makes use of a product multi-graph to compare alternative trace recovery options. The average accuracy of our approach, evaluated using two publicly available datasets, is impressive, with an average recovery accuracy score of 90-97%, significantly improving a common heuristic that chooses the most likely value for each uncertain activity. We believe that the effectiveness of the proposed algorithm in recovering correct traces from stochastically known logs may be a powerful aid for developing 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#32780;&#23433;&#20840;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#23433;&#20840;&#23618;&#65292;&#20351;&#24471;&#29983;&#25104;&#36830;&#32493;&#31574;&#30053;&#30340;&#27010;&#29575;&#23494;&#24230;/&#26799;&#24230;&#25104;&#20026;&#38381;&#21512;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#31471;&#21040;&#31471;&#30340;&#29983;&#25104;&#23545;&#25239;&#35757;&#32451;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;&#37319;&#29992;&#23545;&#25239;&#21487;&#36798;&#24615;&#20998;&#26512;&#21644;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#24615;&#31561;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#26029;&#34892;&#21160;&#37051;&#22495;&#30340;&#23433;&#20840;&#24615;&#26469;&#30830;&#23450;&#19968;&#32452;&#23433;&#20840;&#34892;&#21160;&#12290;&#22312;&#23454;&#38469;&#39550;&#39542;&#21592;&#20132;&#20114;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2203.01696</link><description>&lt;p&gt;
&#23433;&#20840;&#21487;&#38752;&#30340;&#23545;&#25239;&#29983;&#25104;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fail-Safe Adversarial Generative Imitation Learning. (arXiv:2203.01696v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#32780;&#23433;&#20840;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#23433;&#20840;&#23618;&#65292;&#20351;&#24471;&#29983;&#25104;&#36830;&#32493;&#31574;&#30053;&#30340;&#27010;&#29575;&#23494;&#24230;/&#26799;&#24230;&#25104;&#20026;&#38381;&#21512;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#31471;&#21040;&#31471;&#30340;&#29983;&#25104;&#23545;&#25239;&#35757;&#32451;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;&#37319;&#29992;&#23545;&#25239;&#21487;&#36798;&#24615;&#20998;&#26512;&#21644;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#24615;&#31561;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#26029;&#34892;&#21160;&#37051;&#22495;&#30340;&#23433;&#20840;&#24615;&#26469;&#30830;&#23450;&#19968;&#32452;&#23433;&#20840;&#34892;&#21160;&#12290;&#22312;&#23454;&#38469;&#39550;&#39542;&#21592;&#20132;&#20114;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#28789;&#27963;&#32780;&#23433;&#20840;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#21644;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#23433;&#20840;&#23618;&#65292;&#35813;&#23618;&#33021;&#22815;&#20351;&#23433;&#20840;&#29983;&#25104;&#36830;&#32493;&#31574;&#30053;&#30340;&#27010;&#29575;&#23494;&#24230;/&#26799;&#24230;&#25104;&#20026;&#38381;&#21512;&#24418;&#24335;&#65292;&#24182;&#25552;&#20379;&#31471;&#21040;&#31471;&#30340;&#29983;&#25104;&#23545;&#25239;&#35757;&#32451;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;&#23433;&#20840;&#23618;&#23558;&#25152;&#26377;&#34892;&#21160;&#26144;&#23556;&#21040;&#19968;&#32452;&#23433;&#20840;&#34892;&#21160;&#65292;&#24182;&#20351;&#29992;&#21464;&#37327;&#36716;&#25442;&#20844;&#24335;&#21644;&#24230;&#37327;&#30340;&#21487;&#21152;&#24615;&#26469;&#35745;&#31639;&#23494;&#24230;&#12290;&#36890;&#36807;&#23545;&#22238;&#36864;&#25805;&#20316;&#30340;&#23545;&#25239;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#39318;&#20808;&#26816;&#26597;&#26377;&#38480;&#26679;&#26412;&#30340;&#34892;&#21160;&#23433;&#20840;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#24615;&#31561;&#26041;&#27861;&#26469;&#25512;&#26029;&#36825;&#20123;&#34892;&#21160;&#37051;&#22495;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#34920;&#26126;&#19982;&#20165;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#23433;&#20840;&#23618;&#65288;&#26368;&#22810;&#20108;&#27425;&#35823;&#24046;&#65289;&#30456;&#27604;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#23433;&#20840;&#23618;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#65288;&#27169;&#20223;&#35823;&#24046;&#19982;&#26102;&#38388;&#24207;&#21015;&#32447;&#24615;&#30456;&#20851;&#65289;&#12290;&#22312;&#23454;&#38469;&#39550;&#39542;&#21592;&#20132;&#20114;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For flexible yet safe imitation learning (IL), we propose theory and a modular method, with a safety layer that enables a closed-form probability density/gradient of the safe generative continuous policy, end-to-end generative adversarial training, and worst-case safety guarantees. The safety layer maps all actions into a set of safe actions, and uses the change-of-variables formula plus additivity of measures for the density. The set of safe actions is inferred by first checking safety of a finite sample of actions via adversarial reachability analysis of fallback maneuvers, and then concluding on the safety of these actions' neighborhoods using, e.g., Lipschitz continuity. We provide theoretical analysis showing the robustness advantage of using the safety layer already during training (imitation error linear in the horizon) compared to only using it at test time (up to quadratic error). In an experiment on real-world driver interaction data, we empirically demonstrate tractability, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#26410;&#30693;&#21040;&#36798;&#21644;&#26381;&#21153;&#29575;&#30340;&#25490;&#38431;&#31995;&#32479;&#20013;&#30340;&#20837;&#22330;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#21040;&#36798;&#26102;&#38388;&#21644;&#31995;&#32479;&#29366;&#24577;&#65292;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#35843;&#24230;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#35843;&#24230;&#21592;&#30340;&#38271;&#26399;&#24179;&#22343;&#22238;&#25253;&#12290;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#27492;&#38382;&#39064;&#65292;&#22240;&#20026;&#35843;&#24230;&#21592;&#26080;&#27861;&#35266;&#23519;&#21040;&#26381;&#21153;&#26102;&#38388;&#21644;&#31163;&#24320;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2202.02419</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#26410;&#30693;&#26381;&#21153;&#29575;&#30340;&#25490;&#38431;&#31995;&#32479;&#20013;&#19968;&#32452;&#31163;&#25955;&#30340;&#26368;&#20248;&#20998;&#37197;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Learning a Discrete Set of Optimal Allocation Rules in a Queueing System with Unknown Service Rate. (arXiv:2202.02419v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02419
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#26410;&#30693;&#21040;&#36798;&#21644;&#26381;&#21153;&#29575;&#30340;&#25490;&#38431;&#31995;&#32479;&#20013;&#30340;&#20837;&#22330;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#21040;&#36798;&#26102;&#38388;&#21644;&#31995;&#32479;&#29366;&#24577;&#65292;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#35843;&#24230;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#35843;&#24230;&#21592;&#30340;&#38271;&#26399;&#24179;&#22343;&#22238;&#25253;&#12290;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#27492;&#38382;&#39064;&#65292;&#22240;&#20026;&#35843;&#24230;&#21592;&#26080;&#27861;&#35266;&#23519;&#21040;&#26381;&#21153;&#26102;&#38388;&#21644;&#31163;&#24320;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#36890;&#20449;&#32593;&#32476;&#12289;&#21628;&#21483;&#20013;&#24515;&#20197;&#21450;&#35774;&#35745;&#29983;&#20135;&#31995;&#32479;&#12289;&#28040;&#24687;&#31995;&#32479;&#21644;&#22522;&#20110;&#24212;&#29992;&#30340;&#20572;&#36710;&#31995;&#32479;&#31561;&#29616;&#20195;&#24212;&#29992;&#39046;&#22495;&#20043;&#22806;&#30340;Erlang-B&#38459;&#22622;&#27169;&#22411;&#20013;&#65292;&#32771;&#34385;&#21040;&#21040;&#36798;&#21644;&#26381;&#21153;&#29575;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#23545;&#35813;&#31995;&#32479;&#30340;&#20837;&#22330;&#25511;&#21046;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#22312;&#27599;&#20010;&#20316;&#19994;&#21040;&#36798;&#26102;&#65292;&#35843;&#24230;&#21592;&#20915;&#23450;&#23558;&#20316;&#19994;&#20998;&#37197;&#32473;&#19968;&#20010;&#21487;&#29992;&#30340;&#26381;&#21153;&#22120;&#25110;&#32773;&#38459;&#22622;&#23427;&#12290;&#27599;&#20010;&#24050;&#26381;&#21153;&#30340;&#20316;&#19994;&#20026;&#35843;&#24230;&#21592;&#24102;&#26469;&#20102;&#22266;&#23450;&#30340;&#22238;&#25253;&#65292;&#20294;&#20063;&#23548;&#33268;&#20102;&#27599;&#21333;&#20301;&#26381;&#21153;&#26102;&#38388;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#31181;&#35843;&#24230;&#31574;&#30053;&#65292;&#22522;&#20110;&#20165;&#35266;&#23519;&#21040;&#21040;&#36798;&#26102;&#38388;&#21644;&#27599;&#27425;&#21040;&#36798;&#26102;&#31995;&#32479;&#29366;&#24577;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#35843;&#24230;&#21592;&#30340;&#38271;&#26399;&#24179;&#22343;&#22238;&#25253;&#65292;&#36825;&#21453;&#26144;&#20102;&#23545;&#36825;&#31181;&#31995;&#32479;&#30340;&#29616;&#23454;&#37319;&#26679;&#12290;&#20851;&#38190;&#26159;&#65292;&#35843;&#24230;&#21592;&#26082;&#19981;&#35266;&#23519;&#26381;&#21153;&#26102;&#38388;&#20063;&#19981;&#35266;&#23519;&#31163;&#24320;&#26102;&#38388;&#65292;&#22240;&#27492;&#19981;&#33021;&#24212;&#29992;&#20351;&#29992;&#22870;&#21169;&#20449;&#21495;&#30340;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#22522;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Motivated by the wide range of modern applications of the Erlang-B blocking model beyond communication networks and call centers to sizing and pricing in design production systems, messaging systems, and app-based parking systems, we study admission control for such a system but with unknown arrival and service rates. In our model, at every job arrival, a dispatcher decides to assign the job to an available server or block it. Every served job yields a fixed reward for the dispatcher, but it also results in a cost per unit time of service. Our goal is to design a dispatching policy that maximizes the long-term average reward for the dispatcher based on observing only the arrival times and the state of the system at each arrival that reflects a realistic sampling of such systems. Critically, the dispatcher observes neither the service times nor departure times so that standard reinforcement learning-based approaches that use reward signals do not apply. Hence, we develop our learning-ba
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#21160;&#24577;&#27495;&#35270;&#24615;&#23450;&#20215;&#38382;&#39064;&#65292;&#38024;&#23545;&#22312;&#32447;&#38646;&#21806;&#20013;&#23384;&#22312;&#30340;&#20215;&#26684;&#27495;&#35270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#26368;&#22823;&#21270;&#25910;&#20837;&#30340;&#21516;&#26102;&#30830;&#20445;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.08221</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#21442;&#25968;&#38656;&#27714;&#27169;&#22411;&#30340;&#20844;&#24179;&#24863;&#30693;&#22312;&#32447;&#20215;&#26684;&#27495;&#35270;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Online Price Discrimination with Nonparametric Demand Models. (arXiv:2111.08221v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#21160;&#24577;&#27495;&#35270;&#24615;&#23450;&#20215;&#38382;&#39064;&#65292;&#38024;&#23545;&#22312;&#32447;&#38646;&#21806;&#20013;&#23384;&#22312;&#30340;&#20215;&#26684;&#27495;&#35270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#26368;&#22823;&#21270;&#25910;&#20837;&#30340;&#21516;&#26102;&#30830;&#20445;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#26684;&#27495;&#35270;&#26159;&#22312;&#32593;&#19978;&#38646;&#21806;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#31181;&#31574;&#30053;&#65292;&#25351;&#30340;&#26159;&#20026;&#19981;&#21516;&#30340;&#23458;&#25143;&#32676;&#20307;&#35774;&#23450;&#19981;&#21516;&#30340;&#20215;&#26684;&#12290;&#23613;&#31649;&#23427;&#26377;&#21161;&#20110;&#25552;&#39640;&#32593;&#19978;&#38646;&#21806;&#21830;&#30340;&#25910;&#20837;&#65292;&#20294;&#21487;&#33021;&#24341;&#21457;&#20851;&#20110;&#20844;&#24179;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#65292;&#29978;&#33267;&#36829;&#21453;&#35268;&#23450;&#21644;&#27861;&#24459;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#21160;&#24577;&#27495;&#35270;&#24615;&#23450;&#20215;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26377;&#38480;&#38144;&#21806;&#21608;&#26399;&#38271;&#24230;&#20026;T&#30340;&#21333;&#19968;&#20135;&#21697;&#65292;&#26377;&#20004;&#32452;&#23458;&#25143;&#12290;&#27599;&#32452;&#23458;&#25143;&#37117;&#26377;&#20854;&#26410;&#30693;&#30340;&#38656;&#27714;&#20989;&#25968;&#38656;&#35201;&#23398;&#20064;&#12290;&#23545;&#20110;&#27599;&#20010;&#38144;&#21806;&#21608;&#26399;&#65292;&#21334;&#23478;&#30830;&#23450;&#27599;&#32452;&#30340;&#20215;&#26684;&#24182;&#35266;&#23519;&#20854;&#36141;&#20080;&#34892;&#20026;&#12290;&#34429;&#28982;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#26368;&#22823;&#21270;&#25910;&#20837;&#65292;&#20294;&#22312;&#21160;&#24577;&#23450;&#20215;&#25991;&#29486;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30830;&#20445;&#19981;&#21516;&#23458;&#25143;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;Cohen&#31561;&#20154;&#65288;2022&#65289;&#30340;&#20844;&#24179;&#27010;&#24565;&#12290;&#23545;&#20110;&#20215;&#26684;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21160;&#24577;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Price discrimination, which refers to the strategy of setting different prices for different customer groups, has been widely used in online retailing. Although it helps boost the collected revenue for online retailers, it might create serious concerns about fairness, which even violates the regulation and laws. This paper studies the problem of dynamic discriminatory pricing under fairness constraints. In particular, we consider a finite selling horizon of length $T$ for a single product with two groups of customers. Each group of customers has its unknown demand function that needs to be learned. For each selling period, the seller determines the price for each group and observes their purchase behavior. While existing literature mainly focuses on maximizing revenue, ensuring fairness among different customers has not been fully explored in the dynamic pricing literature. This work adopts the fairness notion from Cohen et al. (2022). For price fairness, we propose an optimal dynamic 
&lt;/p&gt;</description></item><item><title>COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#19990;&#30028;&#21508;&#22320;&#30340;&#32452;&#32455;&#22312;&#27807;&#36890;&#19978;&#21464;&#24471;&#26356;&#21152;&#23396;&#31435;&#65292;&#27169;&#22359;&#21270;&#22686;&#21152;&#65292;&#21592;&#24037;&#24320;&#22987;&#26356;&#21152;&#28789;&#27963;&#22320;&#36827;&#34892;&#27807;&#36890;&#12290;</title><link>http://arxiv.org/abs/2104.00641</link><description>&lt;p&gt;
&#21160;&#24577;&#27807;&#36890;&#32593;&#32476;&#65306;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#32452;&#32455;&#20869;&#37096;&#27807;&#36890;&#32593;&#32476;&#20013;&#30340;&#27169;&#22359;&#21270;&#22686;&#21152;
&lt;/p&gt;
&lt;p&gt;
Dynamic Silos: Increased Modularity in Intra-organizational Communication Networks during the Covid-19 Pandemic. (arXiv:2104.00641v6 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.00641
&lt;/p&gt;
&lt;p&gt;
COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#19990;&#30028;&#21508;&#22320;&#30340;&#32452;&#32455;&#22312;&#27807;&#36890;&#19978;&#21464;&#24471;&#26356;&#21152;&#23396;&#31435;&#65292;&#27169;&#22359;&#21270;&#22686;&#21152;&#65292;&#21592;&#24037;&#24320;&#22987;&#26356;&#21152;&#28789;&#27963;&#22320;&#36827;&#34892;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#65292;&#30456;&#20851;&#30340;&#23621;&#23478;&#21150;&#20844;&#25919;&#31574;&#21644;&#36828;&#31243;&#24037;&#20316;&#30340;&#20852;&#36215;&#65292;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#19990;&#30028;&#21508;&#22320;&#30340;&#24037;&#20316;&#22330;&#25152;&#27807;&#36890;&#26041;&#24335;&#12290;&#20026;&#20102;&#20102;&#35299;&#36825;&#20123;&#21464;&#21270;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26469;&#33258;&#20840;&#29699;4361&#20010;&#32452;&#32455;&#30340;3600&#20159;&#23553;&#30005;&#23376;&#37038;&#20214;&#30340;&#32858;&#21512;&#12289;&#21311;&#21517;&#21270;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#27604;&#36739;&#27599;&#26376;&#21644;&#24180;&#24230;&#30340;&#25351;&#26631;&#65292;&#25105;&#20204;&#23545;COVID-19&#20043;&#21069;&#21644;&#20043;&#21518;24&#20010;&#26376;&#20869;&#32593;&#32476;&#31038;&#32676;&#32467;&#26500;&#30340;&#21464;&#21270;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21333;&#20010;&#20840;&#29699;&#32452;&#32455;&#20869;&#22810;&#31181;&#27807;&#36890;&#23186;&#20307;&#65288;&#30005;&#23376;&#37038;&#20214;&#12289;&#21363;&#26102;&#28040;&#24687;&#12289;&#35270;&#39057;&#36890;&#35805;&#21644;&#26085;&#21382;&#36719;&#20214;&#65289;&#30340;&#21464;&#21270;&#65292;&#24182;&#23558;&#20854;&#19982;&#30001;&#20110;&#32452;&#32455;&#32467;&#26500;&#25913;&#21464;&#32780;&#25512;&#21160;&#30340;&#27807;&#36890;&#21464;&#21270;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;2020&#24180;&#65292;&#19990;&#30028;&#21508;&#22320;&#30340;&#32452;&#32455;&#27604;2019&#24180;&#26356;&#21152;&#23396;&#31435;&#65292;&#34920;&#29616;&#20026;&#22686;&#21152;&#30340;&#27169;&#22359;&#21270;&#12290;&#36825;&#19968;&#36716;&#21464;&#19982;&#27169;&#22359;&#20869;&#31283;&#23450;&#24615;&#30340;&#38477;&#20302;&#21516;&#26102;&#21457;&#29983;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20849;&#21516;&#34920;&#26126;&#65292;COVID-19&#29190;&#21457;&#21518;&#65292;&#21592;&#24037;&#24320;&#22987;&#26356;&#21152;&#28789;&#27963;&#22320;&#36827;&#34892;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;
Workplace communications around the world were drastically altered by Covid-19, related work-from-home orders, and the rise of remote work. To understand these shifts, we analyzed aggregated, anonymized metadata from over 360 billion emails within 4,361 organizations worldwide. By comparing month-to-month and year-over-year metrics, we examined changes in network community structures over 24 months before and after Covid-19. We also examined shifts across multiple communication media (email, instant messages, video calls, and calendaring software) within a single global organization, and compared them to communications shifts that were driven by changes in formal organizational structure. We found that, in 2020, organizations around the world became more siloed than in 2019, evidenced by increased modularity. This shift was concurrent with decreased stability within silos. Collectively, our analyses indicate that following the onset of Covid-19, employees began to shift more dynamicall
&lt;/p&gt;</description></item></channel></rss>