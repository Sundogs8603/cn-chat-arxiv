<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#38160;&#24230;&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20986;&#26089;&#26399;&#38160;&#24230;&#38477;&#20302;&#12289;&#36880;&#28176;&#22686;&#21152;&#38160;&#21270;&#21644;&#31283;&#23450;&#36793;&#30028;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#22686;&#22823;&#23398;&#20064;&#29575;&#26102;&#65292;&#31283;&#23450;&#36793;&#30028;&#27969;&#24418;&#19978;&#21457;&#29983;&#20493;&#22686;&#28151;&#27788;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2311.02076</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26222;&#36866;&#38160;&#24230;&#21160;&#21147;&#23398;&#65306;&#22266;&#23450;&#28857;&#20998;&#26512;&#12289;&#31283;&#23450;&#36793;&#30028;&#21644;&#28151;&#27788;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos. (arXiv:2311.02076v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#38160;&#24230;&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20986;&#26089;&#26399;&#38160;&#24230;&#38477;&#20302;&#12289;&#36880;&#28176;&#22686;&#21152;&#38160;&#21270;&#21644;&#31283;&#23450;&#36793;&#30028;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#22686;&#22823;&#23398;&#20064;&#29575;&#26102;&#65292;&#31283;&#23450;&#36793;&#30028;&#27969;&#24418;&#19978;&#21457;&#29983;&#20493;&#22686;&#28151;&#27788;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#20013;&#65292;&#25439;&#22833;&#20989;&#25968;&#28023;&#26862;&#30697;&#38453;&#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#65288;&#38160;&#24230;&#65289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23637;&#31034;&#20986;&#21508;&#31181;&#31283;&#20581;&#30340;&#29616;&#35937;&#12290;&#36825;&#21253;&#25324;&#26089;&#26399;&#26102;&#38388;&#38454;&#27573;&#65292;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#38160;&#24230;&#21487;&#33021;&#20943;&#23567;&#65288;&#38477;&#20302;&#38160;&#24230;&#65289;&#65292;&#20197;&#21450;&#21518;&#26399;&#34892;&#20026;&#65292;&#22914;&#36880;&#28176;&#22686;&#21152;&#30340;&#38160;&#21270;&#21644;&#31283;&#23450;&#36793;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;2&#23618;&#32447;&#24615;&#32593;&#32476;&#65288;UV&#27169;&#22411;&#65289;&#65292;&#22312;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#19978;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#35266;&#23519;&#21040;&#30340;&#25152;&#26377;&#20851;&#38190;&#38160;&#24230;&#29616;&#35937;&#12290;&#36890;&#36807;&#20998;&#26512;&#20989;&#25968;&#31354;&#38388;&#20013;&#21160;&#21147;&#23398;&#22266;&#23450;&#28857;&#30340;&#32467;&#26500;&#21644;&#20989;&#25968;&#26356;&#26032;&#30340;&#21521;&#37327;&#22330;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#38160;&#24230;&#36235;&#21183;&#32972;&#21518;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#65306;(i)&#26089;&#26399;&#38160;&#24230;&#38477;&#20302;&#21644;&#36880;&#28176;&#22686;&#21152;&#38160;&#21270;&#30340;&#26426;&#21046;&#65292;(ii)&#31283;&#23450;&#36793;&#30028;&#25152;&#38656;&#30340;&#26465;&#20214;&#65292;&#20197;&#21450; (iii)&#24403;&#23398;&#20064;&#29575;&#22686;&#21152;&#26102;&#65292;&#31283;&#23450;&#36793;&#30028;&#27969;&#24418;&#19978;&#30340;&#20493;&#22686;&#28151;&#27788;&#36335;&#24452;.
&lt;/p&gt;
&lt;p&gt;
In gradient descent dynamics of neural networks, the top eigenvalue of the Hessian of the loss (sharpness) displays a variety of robust phenomena throughout training. This includes early time regimes where the sharpness may decrease during early periods of training (sharpness reduction), and later time behavior such as progressive sharpening and edge of stability. We demonstrate that a simple $2$-layer linear network (UV model) trained on a single training example exhibits all of the essential sharpness phenomenology observed in real-world scenarios. By analyzing the structure of dynamical fixed points in function space and the vector field of function updates, we uncover the underlying mechanisms behind these sharpness trends. Our analysis reveals (i) the mechanism behind early sharpness reduction and progressive sharpening, (ii) the required conditions for edge of stability, and (iii) a period-doubling route to chaos on the edge of stability manifold as learning rate is increased. Fi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#22320;&#38754;&#35266;&#27979;&#20013;&#39640;&#25928;&#20272;&#35745;&#29289;&#31181;&#30340;&#22320;&#29702;&#20998;&#24067;&#33539;&#22260;&#12290;&#36890;&#36807;&#24314;&#27169;&#26410;&#27979;&#32472;&#29289;&#31181;&#30340;&#33539;&#22260;&#20026;&#19981;&#21516;&#29289;&#31181;&#30340;&#20272;&#35745;&#33539;&#22260;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#20110;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#35266;&#27979;&#25968;&#25454;&#30340;&#27169;&#22411;&#29983;&#25104;&#20505;&#36873;&#33539;&#22260;&#38598;&#21512;&#65292;&#28982;&#21518;&#36890;&#36807;&#26032;&#30340;&#20027;&#21160;&#26597;&#35810;&#26041;&#27861;&#36873;&#25321;&#22320;&#29702;&#20301;&#32622;&#36827;&#34892;&#35775;&#38382;&#20197;&#20943;&#23569;&#23545;&#26410;&#27979;&#32472;&#29289;&#31181;&#33539;&#22260;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25509;&#36817;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.02061</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#29289;&#31181;&#20998;&#24067;&#33539;&#22260;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Active Learning-Based Species Range Estimation. (arXiv:2311.02061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02061
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#22320;&#38754;&#35266;&#27979;&#20013;&#39640;&#25928;&#20272;&#35745;&#29289;&#31181;&#30340;&#22320;&#29702;&#20998;&#24067;&#33539;&#22260;&#12290;&#36890;&#36807;&#24314;&#27169;&#26410;&#27979;&#32472;&#29289;&#31181;&#30340;&#33539;&#22260;&#20026;&#19981;&#21516;&#29289;&#31181;&#30340;&#20272;&#35745;&#33539;&#22260;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#20110;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#35266;&#27979;&#25968;&#25454;&#30340;&#27169;&#22411;&#29983;&#25104;&#20505;&#36873;&#33539;&#22260;&#38598;&#21512;&#65292;&#28982;&#21518;&#36890;&#36807;&#26032;&#30340;&#20027;&#21160;&#26597;&#35810;&#26041;&#27861;&#36873;&#25321;&#22320;&#29702;&#20301;&#32622;&#36827;&#34892;&#35775;&#38382;&#20197;&#20943;&#23569;&#23545;&#26410;&#27979;&#32472;&#29289;&#31181;&#33539;&#22260;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25509;&#36817;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#22320;&#38754;&#35266;&#27979;&#20013;&#20272;&#35745;&#29289;&#31181;&#30340;&#22320;&#29702;&#20998;&#24067;&#33539;&#22260;&#12290;&#25105;&#20204;&#23558;&#26410;&#27979;&#32472;&#29289;&#31181;&#30340;&#33539;&#22260;&#24314;&#27169;&#20026;&#20174;&#19968;&#32452;&#19981;&#21516;&#29289;&#31181;&#30340;&#20272;&#35745;&#33539;&#22260;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#22312;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#31038;&#21306;&#25910;&#38598;&#30340;&#35266;&#27979;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#36825;&#20010;&#20505;&#36873;&#33539;&#22260;&#38598;&#21512;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#26597;&#35810;&#26041;&#27861;&#65292;&#39034;&#24207;&#36873;&#25321;&#26368;&#33021;&#20943;&#23569;&#23545;&#26410;&#27979;&#32472;&#29289;&#31181;&#33539;&#22260;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#22320;&#29702;&#20301;&#32622;&#36827;&#34892;&#35775;&#38382;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#21253;&#21547;&#19968;&#21315;&#31181;&#29289;&#31181;&#30340;&#19987;&#23478;&#25512;&#23548;&#33539;&#22260;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#23558;&#20854;&#19982;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#20165;&#20351;&#29992;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#26102;&#21487;&#20197;&#25509;&#36817;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new active learning approach for efficiently estimating the geographic range of a species from a limited number of on the ground observations. We model the range of an unmapped species of interest as the weighted combination of estimated ranges obtained from a set of different species. We show that it is possible to generate this candidate set of ranges by using models that have been trained on large weakly supervised community collected observation data. From this, we develop a new active querying approach that sequentially selects geographic locations to visit that best reduce our uncertainty over an unmapped species' range. We conduct a detailed evaluation of our approach and compare it to existing active learning methods using an evaluation dataset containing expert-derived ranges for one thousand species. Our results demonstrate that our method outperforms alternative active learning methods and approaches the performance of end-to-end trained models, even when only u
&lt;/p&gt;</description></item><item><title>LOTUS&#26159;&#19968;&#31181;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#25216;&#33021;&#24211;&#65292;&#24182;&#20351;&#29992;&#20803;&#25511;&#21046;&#22120;&#28789;&#27963;&#32452;&#21512;&#25216;&#33021;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.02058</link><description>&lt;p&gt;
LOTUS&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery. (arXiv:2311.02058v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02058
&lt;/p&gt;
&lt;p&gt;
LOTUS&#26159;&#19968;&#31181;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#25216;&#33021;&#24211;&#65292;&#24182;&#20351;&#29992;&#20803;&#25511;&#21046;&#22120;&#28789;&#27963;&#32452;&#21512;&#25216;&#33021;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LOTUS&#30340;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#20351;&#24471;&#29289;&#29702;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#32780;&#39640;&#25928;&#22320;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;LOTUS&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#19968;&#31995;&#21015;&#26032;&#20219;&#21153;&#30340;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#26500;&#24314;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#25216;&#33021;&#24211;&#12290;LOTUS&#39318;&#20808;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#25345;&#32493;&#25216;&#33021;&#21457;&#29616;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#20174;&#26410;&#20998;&#27573;&#30340;&#28436;&#31034;&#20013;&#25552;&#21462;&#37325;&#22797;&#20986;&#29616;&#30340;&#25216;&#33021;&#27169;&#24335;&#12290;&#25345;&#32493;&#25216;&#33021;&#21457;&#29616;&#26356;&#26032;&#29616;&#26377;&#25216;&#33021;&#20197;&#36991;&#20813;&#23545;&#20197;&#21069;&#20219;&#21153;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#28155;&#21152;&#26032;&#25216;&#33021;&#20197;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;LOTUS&#35757;&#32451;&#19968;&#20010;&#20803;&#25511;&#21046;&#22120;&#65292;&#22312;&#32456;&#36523;&#23398;&#20064;&#36807;&#31243;&#20013;&#28789;&#27963;&#22320;&#32452;&#21512;&#21508;&#31181;&#25216;&#33021;&#26469;&#35299;&#20915;&#22522;&#20110;&#35270;&#35273;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;LOTUS&#22312;&#25104;&#21151;&#29575;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#22522;&#32447;&#26041;&#27861;11&#65285;&#20197;&#19978;&#65292;&#26174;&#31034;&#20102;&#20854;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce LOTUS, a continual imitation learning algorithm that empowers a physical robot to continuously and efficiently learn to solve new manipulation tasks throughout its lifespan. The core idea behind LOTUS is constructing an ever-growing skill library from a sequence of new tasks with a small number of human demonstrations. LOTUS starts with a continual skill discovery process using an open-vocabulary vision model, which extracts skills as recurring patterns presented in unsegmented demonstrations. Continual skill discovery updates existing skills to avoid catastrophic forgetting of previous tasks and adds new skills to solve novel tasks. LOTUS trains a meta-controller that flexibly composes various skills to tackle vision-based manipulation tasks in the lifelong learning process. Our comprehensive experiments show that LOTUS outperforms state-of-the-art baselines by over 11% in success rate, showing its superior knowledge transfer ability compared to prior methods. More result
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22522;&#20110;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#20135;&#29983;&#25152;&#38656;&#30340;&#37327;&#23376;&#25805;&#20316;&#65292;&#32780;&#19988;&#33021;&#22815;&#32469;&#36807;&#32463;&#20856;&#27169;&#25311;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#25351;&#25968;&#32423;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#32416;&#32544;&#29983;&#25104;&#21644;&#37193;&#32534;&#35793;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#24182;&#25903;&#25345;&#25193;&#23637;&#21151;&#33021;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#37327;&#23376;&#35774;&#22791;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2311.02041</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Quantum circuit synthesis with diffusion models. (arXiv:2311.02041v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02041
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22522;&#20110;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#20135;&#29983;&#25152;&#38656;&#30340;&#37327;&#23376;&#25805;&#20316;&#65292;&#32780;&#19988;&#33021;&#22815;&#32469;&#36807;&#32463;&#20856;&#27169;&#25311;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#25351;&#25968;&#32423;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#32416;&#32544;&#29983;&#25104;&#21644;&#37193;&#32534;&#35793;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#24182;&#25903;&#25345;&#25193;&#23637;&#21151;&#33021;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#37327;&#23376;&#35774;&#22791;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26368;&#36817;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#25152;&#25215;&#35834;&#30340;&#20248;&#21183;&#20381;&#36182;&#20110;&#23558;&#37327;&#23376;&#25805;&#20316;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#29289;&#29702;&#23454;&#29616;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#20197;&#20419;&#36827;&#36825;&#31181;&#36716;&#21270;&#12290;&#36890;&#36807;&#25991;&#26412;&#26465;&#20214;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#22312;&#22522;&#20110;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#20135;&#29983;&#25152;&#38656;&#30340;&#37327;&#23376;&#25805;&#20316;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DMs&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#32463;&#20856;&#27169;&#25311;&#37327;&#23376;&#21160;&#21147;&#23398;&#20013;&#22266;&#26377;&#30340;&#25351;&#25968;&#32423;&#24320;&#38144;&#65292;&#36825;&#26159;&#20808;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#19968;&#30452;&#23384;&#22312;&#30340;&#29942;&#39048;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#65306;&#32416;&#32544;&#29983;&#25104;&#21644;&#37193;&#32534;&#35793;&#12290;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#26032;&#30005;&#36335;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25903;&#25345;&#20856;&#22411;&#30340;DM&#25193;&#23637;&#65292;&#20363;&#22914;&#25513;&#30721;&#21644;&#32534;&#36753;&#65292;&#20197;&#20351;&#30005;&#36335;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#37327;&#23376;&#35774;&#22791;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#37327;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing has recently emerged as a transformative technology. Yet, its promised advantages rely on efficiently translating quantum operations into viable physical realizations. In this work, we use generative machine learning models, specifically denoising diffusion models (DMs), to facilitate this transformation. Leveraging text-conditioning, we steer the model to produce desired quantum operations within gate-based quantum circuits. Notably, DMs allow to sidestep during training the exponential overhead inherent in the classical simulation of quantum dynamics -- a consistent bottleneck in preceding ML techniques. We demonstrate the model's capabilities across two tasks: entanglement generation and unitary compilation. The model excels at generating new circuits and supports typical DM extensions such as masking and editing to, for instance, align the circuit generation to the constraints of the targeted quantum device. Given their flexibility and generalization abilities, we
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#34955;&#35013;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#19988;&#24191;&#27867;&#36866;&#29992;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22312;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#19979;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.02019</link><description>&lt;p&gt;
&#20351;&#29992;&#34955;&#35013;&#21518;&#39564;&#36827;&#34892;&#21487;&#37325;&#29616;&#30340;&#21442;&#25968;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Reproducible Parameter Inference Using Bagged Posteriors. (arXiv:2311.02019v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02019
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#34955;&#35013;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#19988;&#24191;&#27867;&#36866;&#29992;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22312;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#19979;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#19979;&#65292;&#24050;&#30693;&#36125;&#21494;&#26031;&#21518;&#39564;&#36890;&#24120;&#19981;&#33021;&#27491;&#30830;&#37327;&#21270;&#20851;&#20110;&#30495;&#23454;&#25110;&#20266;&#30495;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#38169;&#35823;&#35268;&#33539;&#20250;&#23548;&#33268;&#22312;&#29420;&#31435;&#25968;&#25454;&#38598;&#19978;&#21516;&#19968;&#27169;&#22411;&#20135;&#29983;&#30456;&#20114;&#30683;&#30462;&#30340;&#21518;&#39564;&#32467;&#26524;&#65292;&#20174;&#32780;&#32570;&#20047;&#21487;&#37325;&#29616;&#24615;&#12290;&#20026;&#20102;&#23450;&#20041;&#22312;&#38169;&#35823;&#35268;&#33539;&#19979;&#21487;&#37325;&#29616;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26631;&#20934;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#29420;&#31435;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#20004;&#20010;&#32622;&#20449;&#21306;&#38388;&#20855;&#26377;&#38750;&#31354;&#20132;&#38598;&#30340;&#27010;&#29575;&#65292;&#24182;&#20026;&#20219;&#20309;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#24314;&#31435;&#20102;&#35813;&#20132;&#38598;&#27010;&#29575;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26631;&#20934;&#21518;&#39564;&#30340;&#21487;&#20449;&#21306;&#38388;&#22312;&#39640;&#32500;&#35774;&#32622;&#19979;&#65288;&#21363;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#26102;&#30340;&#32500;&#24230;&#65289;&#21487;&#20197;&#20005;&#37325;&#36829;&#21453;&#36825;&#20010;&#19979;&#30028;&#65292;&#34920;&#26126;&#22312;&#38169;&#35823;&#35268;&#33539;&#19979;&#23427;&#19981;&#26159;&#20869;&#37096;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#25552;&#39640;&#26131;&#20110;&#20351;&#29992;&#19988;&#24191;&#27867;&#36866;&#29992;&#30340;&#21487;&#37325;&#29616;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#24212;&#29992;&#34955;&#35013;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under model misspecification, it is known that Bayesian posteriors often do not properly quantify uncertainty about true or pseudo-true parameters. Even more fundamentally, misspecification leads to a lack of reproducibility in the sense that the same model will yield contradictory posteriors on independent data sets from the true distribution. To define a criterion for reproducible uncertainty quantification under misspecification, we consider the probability that two confidence sets constructed from independent data sets have nonempty overlap, and we establish a lower bound on this overlap probability that holds for any valid confidence sets. We prove that credible sets from the standard posterior can strongly violate this bound, particularly in high-dimensional settings (i.e., with dimension increasing with sample size), indicating that it is not internally coherent under misspecification. To improve reproducibility in an easy-to-use and widely applicable way, we propose to apply ba
&lt;/p&gt;</description></item><item><title>DeliverAI&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#36335;&#24452;&#20849;&#20139;&#32593;&#32476;&#65292;&#29992;&#20110;&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#25104;&#26412;&#24182;&#25552;&#39640;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#12290;</title><link>http://arxiv.org/abs/2311.02017</link><description>&lt;p&gt;
DeliverAI: &#24378;&#21270;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20998;&#24067;&#24335;&#36335;&#24452;&#20849;&#20139;&#32593;&#32476;&#29992;&#20110;&#39135;&#21697;&#37197;&#36865;
&lt;/p&gt;
&lt;p&gt;
DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries. (arXiv:2311.02017v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02017
&lt;/p&gt;
&lt;p&gt;
DeliverAI&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#36335;&#24452;&#20849;&#20139;&#32593;&#32476;&#65292;&#29992;&#20110;&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#25104;&#26412;&#24182;&#25552;&#39640;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#20174;&#29983;&#20135;&#32773;&#21040;&#28040;&#36153;&#32773;&#30340;&#29289;&#21697;&#37197;&#36865;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#27969;&#34892;&#30149;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#22686;&#38271;&#12290;&#20122;&#39532;&#36874;&#29983;&#40092;&#12289;Shopify&#12289;UberEats&#12289;InstaCart&#21644;DoorDash&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#28040;&#36153;&#21697;&#25110;&#39135;&#21697;&#37197;&#36865;&#19994;&#21153;&#27169;&#24335;&#12290;&#29616;&#26377;&#30340;&#39135;&#21697;&#37197;&#36865;&#26041;&#27861;&#23384;&#22312;&#32570;&#38519;&#65292;&#22240;&#20026;&#27599;&#27425;&#37197;&#36865;&#37117;&#26159;&#22312;&#26368;&#30701;&#26102;&#38388;&#36335;&#24452;&#19978;&#20174;&#29983;&#20135;&#32773;&#30452;&#25509;&#21040;&#28040;&#36153;&#32773;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#24403;&#21069;&#27169;&#22411;&#19979;&#65292;&#26377;&#24456;&#22823;&#30340;&#20943;&#23569;&#37197;&#36865;&#25104;&#26412;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#39135;&#21697;&#37197;&#36865;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#21644;&#37197;&#36865;&#25104;&#26412;&#37117;&#38656;&#35201;&#36827;&#34892;&#20248;&#21270;&#12290;&#21463;&#20986;&#31199;&#36710;&#34892;&#19994;&#20013;&#25340;&#36710;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeliverAI - &#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36335;&#24452;&#20849;&#20139;&#31639;&#27861;&#12290;&#19982;&#20197;&#21069;&#30340;&#36335;&#24452;&#20849;&#20139;&#23581;&#35797;&#19981;&#21516;&#65292;DeliverAI&#21487;&#20197;&#25552;&#20379;&#23454;&#26102;&#12289;&#26102;&#38388;&#39640;&#25928;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Delivery of items from the producer to the consumer has experienced significant growth over the past decade and has been greatly fueled by the recent pandemic. Amazon Fresh, Shopify, UberEats, InstaCart, and DoorDash are rapidly growing and are sharing the same business model of consumer items or food delivery. Existing food delivery methods are sub-optimal because each delivery is individually optimized to go directly from the producer to the consumer via the shortest time path. We observe a significant scope for reducing the costs associated with completing deliveries under the current model. We model our food delivery problem as a multi-objective optimization, where consumer satisfaction and delivery costs, both, need to be optimized. Taking inspiration from the success of ride-sharing in the taxi industry, we propose DeliverAI - a reinforcement learning-based path-sharing algorithm. Unlike previous attempts for path-sharing, DeliverAI can provide real-time, time-efficient decision-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;SMORe&#65292;&#23427;&#23558;&#21344;&#26377;&#21305;&#37197;&#30340;&#35270;&#35282;&#19982;&#28151;&#21512;&#20998;&#24067;&#21305;&#37197;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#23398;&#20064;&#37492;&#21035;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;GCRL&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.02013</link><description>&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#35780;&#20998;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Score Models for Offline Goal-Conditioned Reinforcement Learning. (arXiv:2311.02013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;SMORe&#65292;&#23427;&#23558;&#21344;&#26377;&#21305;&#37197;&#30340;&#35270;&#35282;&#19982;&#28151;&#21512;&#20998;&#24067;&#21305;&#37197;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#23398;&#20064;&#37492;&#21035;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;GCRL&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#30340;&#20219;&#21153;&#26159;&#20351;&#29992;&#31232;&#30095;&#22870;&#21169;&#20989;&#25968;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#22312;&#29615;&#22659;&#20013;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;&#12290;&#31163;&#32447;GCRL&#23545;&#20110;&#24320;&#21457;&#33021;&#22815;&#21033;&#29992;&#39044;&#20808;&#23384;&#22312;&#30340;&#25968;&#25454;&#38598;&#23398;&#20064;&#22810;&#26679;&#21270;&#21644;&#21487;&#22797;&#29992;&#25216;&#33021;&#30340;&#36890;&#29992;&#22411;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#26080;&#38656;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#29616;&#20195;GCRL&#26041;&#27861;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#24448;&#24448;&#19981;&#22826;&#29702;&#24819;&#12290;GCRL&#30340;&#21478;&#19968;&#31181;&#35266;&#28857;&#26159;&#20248;&#21270;&#21344;&#26377;&#21305;&#37197;&#65292;&#20294;&#38656;&#35201;&#23398;&#20064;&#37492;&#21035;&#22120;&#65292;&#38543;&#21518;&#35813;&#37492;&#21035;&#22120;&#20316;&#20026;&#19979;&#28216;&#24378;&#21270;&#23398;&#20064;&#30340;&#20266;&#22870;&#21169;&#12290;&#23398;&#20064;&#21040;&#30340;&#37492;&#21035;&#22120;&#30340;&#19981;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#36127;&#38754;&#24433;&#21709;&#65292;&#36827;&#32780;&#24433;&#21709;&#29983;&#25104;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GCRL&#26041;&#27861;&#65292;&#22522;&#20110;&#28151;&#21512;&#20998;&#24067;&#21305;&#37197;&#30340;&#26032;&#35270;&#35282;&#65292;&#37319;&#29992;&#26080;&#37492;&#21035;&#22120;&#30340;&#26041;&#27861;&#65306;SMORe&#12290;&#20851;&#38190;&#27934;&#35265;&#26159;&#23558;GCRL&#30340;&#21344;&#26377;&#21305;&#37197;&#35270;&#35282;&#19982;&#19968;&#20010;&#26377;&#25928;&#30340;&#32858;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#26080;&#37492;&#21035;&#22120;&#26041;&#27861;&#65306;SMORe&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a conve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#35270;&#35282;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#39640;&#20998;&#36776;&#29575;ODE&#65292;&#24182;&#36890;&#36807;&#23558;Nesterov&#21152;&#36895;&#26799;&#24230;&#27861;&#24212;&#29992;&#20110;&#26799;&#24230;&#33539;&#25968;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;Nesterov&#30340;&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#23545;&#39640;&#20998;&#36776;&#29575;ODE&#30340;&#21512;&#36866;&#31163;&#25955;&#12290;</title><link>http://arxiv.org/abs/2311.02002</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#20998;&#36776;&#29575;ODE&#30340;&#19968;&#31181;&#21464;&#20998;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Variational Perspective on High-Resolution ODEs. (arXiv:2311.02002v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02002
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#35270;&#35282;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#39640;&#20998;&#36776;&#29575;ODE&#65292;&#24182;&#36890;&#36807;&#23558;Nesterov&#21152;&#36895;&#26799;&#24230;&#27861;&#24212;&#29992;&#20110;&#26799;&#24230;&#33539;&#25968;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;Nesterov&#30340;&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#23545;&#39640;&#20998;&#36776;&#29575;ODE&#30340;&#21512;&#36866;&#31163;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20809;&#28369;&#20984;&#20989;&#25968;&#30340;&#38750;&#32422;&#26463;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21464;&#20998;&#35270;&#35282;&#65292;&#21033;&#29992;&#24378;&#21046;&#24615;&#30340;Euler-Lagrange&#26041;&#31243;&#26469;&#30740;&#31350;&#39640;&#20998;&#36776;&#29575;ODE&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20351;&#29992;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#27861;&#36827;&#34892;&#26799;&#24230;&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Nesterov&#30340;&#26041;&#27861;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#23545;&#19968;&#20010;&#21512;&#36866;&#36873;&#25321;&#30340;&#39640;&#20998;&#36776;&#29575;ODE&#36827;&#34892;&#21305;&#37197;&#29575;&#31163;&#25955;&#21270;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#36825;&#31181;&#26032;&#21464;&#20998;&#35270;&#35282;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22122;&#22768;&#26799;&#24230;&#30340;&#38543;&#26426;&#26041;&#27861;&#12290;&#22810;&#20010;&#25968;&#20540;&#23454;&#39564;&#27604;&#36739;&#21644;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#38543;&#26426;&#31639;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider unconstrained minimization of smooth convex functions. We propose a novel variational perspective using forced Euler-Lagrange equation that allows for studying high-resolution ODEs. Through this, we obtain a faster convergence rate for gradient norm minimization using Nesterov's accelerated gradient method. Additionally, we show that Nesterov's method can be interpreted as a rate-matching discretization of an appropriately chosen high-resolution ODE. Finally, using the results from the new variational perspective, we propose a stochastic method for noisy gradients. Several numerical experiments compare and illustrate our stochastic algorithm with state of the art methods.
&lt;/p&gt;</description></item><item><title>Adam&#31639;&#27861;&#22312;&#38750;&#20984;&#24179;&#28369;&#38543;&#26426;&#20248;&#21270;&#20013;&#65292;&#32463;&#36807;&#28145;&#20837;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#22312;&#22352;&#26631;-wise&#8220;&#20223;&#23556;&#8221;&#26041;&#24046;&#22122;&#22768;&#19979;&#65292;Adam&#21487;&#20197;&#20197;&#39640;&#27010;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#26080;&#38656;&#20219;&#20309;&#26377;&#30028;&#26799;&#24230;&#20551;&#35774;&#21644;&#38382;&#39064;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2311.02000</link><description>&lt;p&gt;
Adam&#31639;&#27861;&#22312;&#26080;&#30028;&#26799;&#24230;&#21644;&#20223;&#23556;&#26041;&#24046;&#22122;&#22768;&#19979;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
High Probability Convergence of Adam Under Unbounded Gradients and Affine Variance Noise. (arXiv:2311.02000v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02000
&lt;/p&gt;
&lt;p&gt;
Adam&#31639;&#27861;&#22312;&#38750;&#20984;&#24179;&#28369;&#38543;&#26426;&#20248;&#21270;&#20013;&#65292;&#32463;&#36807;&#28145;&#20837;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#22312;&#22352;&#26631;-wise&#8220;&#20223;&#23556;&#8221;&#26041;&#24046;&#22122;&#22768;&#19979;&#65292;Adam&#21487;&#20197;&#20197;&#39640;&#27010;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#26080;&#38656;&#20219;&#20309;&#26377;&#30028;&#26799;&#24230;&#20551;&#35774;&#21644;&#38382;&#39064;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20984;&#24179;&#28369;&#38543;&#26426;&#20248;&#21270;&#20013;&#65292;&#33258;&#36866;&#24212;&#30697;&#27861;&#65288;Adam&#65289;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#23613;&#31649;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20854;&#29702;&#35770;&#24615;&#36136;&#20173;&#28982;&#26377;&#38480;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20174;&#26399;&#26395;&#35282;&#24230;&#32771;&#34385;&#20102;Adam&#30340;&#25910;&#25947;&#24615;&#65292;&#24120;&#24120;&#38656;&#35201;&#24378;&#20551;&#35774;&#65292;&#27604;&#22914;&#22343;&#21248;&#38543;&#26426;&#26377;&#30028;&#26799;&#24230;&#25110;&#32773;&#20808;&#39564;&#30340;&#38382;&#39064;&#30456;&#20851;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32467;&#26524;&#22312;&#23454;&#38469;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#22352;&#26631;-wise&#8220;&#20223;&#23556;&#8221;&#26041;&#24046;&#22122;&#22768;&#19979;&#65292;Adam&#21487;&#20197;&#20197;&#39640;&#27010;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#20026;$\mathcal{O}\left({\rm poly}(\log T)/\sqrt{T}\right)$&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#26377;&#30028;&#26799;&#24230;&#20551;&#35774;&#21644;&#20219;&#20309;&#38382;&#39064;&#30456;&#20851;&#30340;&#30693;&#35782;&#26469;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;Adam&#38480;&#21046;&#20102;&#20854;&#26799;&#24230;&#30340;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the convergence of the Adaptive Moment Estimation (Adam) algorithm under unconstrained non-convex smooth stochastic optimizations. Despite the widespread usage in machine learning areas, its theoretical properties remain limited. Prior researches primarily investigated Adam's convergence from an expectation view, often necessitating strong assumptions like uniformly stochastic bounded gradients or problem-dependent knowledge in prior. As a result, the applicability of these findings in practical real-world scenarios has been constrained. To overcome these limitations, we provide a deep analysis and show that Adam could converge to the stationary point in high probability with a rate of $\mathcal{O}\left({\rm poly}(\log T)/\sqrt{T}\right)$ under coordinate-wise "affine" variance noise, not requiring any bounded gradient assumption and any problem-dependent knowledge in prior to tune hyper-parameters. Additionally, it is revealed that Adam confines its gradients' 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#22522;&#20110;CNN&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21457;&#29616;&#20351;&#29992;&#22522;&#20110;DenseNet201&#30340;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#22278;&#38181;&#35282;&#33180;&#30149;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;89.14%&#12290;</title><link>http://arxiv.org/abs/2311.01996</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22278;&#38181;&#35282;&#33180;&#30149;
&lt;/p&gt;
&lt;p&gt;
Detection of keratoconus Diseases using deep Learning. (arXiv:2311.01996v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#22522;&#20110;CNN&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21457;&#29616;&#20351;&#29992;&#22522;&#20110;DenseNet201&#30340;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#22278;&#38181;&#35282;&#33180;&#30149;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;89.14%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22278;&#38181;&#35282;&#33180;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#30524;&#35282;&#33180;&#30142;&#30149;&#65292;&#26089;&#26399;&#35786;&#26029;&#22256;&#38590;&#65292;&#21487;&#33021;&#23548;&#33268;&#22833;&#26126;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20043;&#19968;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#20934;&#30830;&#21450;&#26102;&#35786;&#26029;&#22278;&#38181;&#35282;&#33180;&#30149;&#30340;&#26377;&#24076;&#26395;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#35780;&#20272;&#19981;&#21516;D-CNN&#27169;&#22411;&#22312;&#35782;&#21035;&#22278;&#38181;&#35282;&#33180;&#30456;&#20851;&#30142;&#30149;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#19981;&#21516;&#22522;&#20110;CNN&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;DenseNet201&#65292;InceptionV3&#65292;MobileNetV2&#65292;VGG19&#65292;Xception&#65289;&#12290;&#22312;&#20840;&#38754;&#30340;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;&#22522;&#20110;DenseNet201&#30340;&#27169;&#22411;&#22312;&#22278;&#38181;&#35282;&#33180;&#30149;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#22312;&#19977;&#31181;&#20851;&#38190;&#31867;&#21035;&#65306;&#22278;&#38181;&#35282;&#33180;&#65292;&#27491;&#24120;&#21644;&#30097;&#20284;&#20013;&#30340;&#36798;&#21040;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;89.14%&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most serious corneal disorders, keratoconus is difficult to diagnose in its early stages and can result in blindness. This illness, which often appears in the second decade of life, affects people of all sexes and races. Convolutional neural networks (CNNs), one of the deep learning approaches, have recently come to light as particularly promising tools for the accurate and timely diagnosis of keratoconus. The purpose of this study was to evaluate how well different D-CNN models identified keratoconus-related diseases. To be more precise, we compared five different CNN-based deep learning architectures (DenseNet201, InceptionV3, MobileNetV2, VGG19, Xception). In our comprehensive experimental analysis, the DenseNet201-based model performed very well in keratoconus disease identification in our extensive experimental research. This model outperformed its D-CNN equivalents, with an astounding accuracy rate of 89.14% in three crucial classes: Keratoconus, Normal, and Suspect. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#33719;&#21462;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#31232;&#30095;&#30340;&#35268;&#21017;&#38598;&#21512;&#26469;&#21516;&#26102;&#35299;&#20915;&#35268;&#21017;&#38598;&#30340;&#31232;&#30095;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#20445;&#35777;&#27867;&#21270;&#24615;&#33021;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.01994</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#33719;&#21462;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Obtaining Explainable Classification Models using Distributionally Robust Optimization. (arXiv:2311.01994v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#33719;&#21462;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#31232;&#30095;&#30340;&#35268;&#21017;&#38598;&#21512;&#26469;&#21516;&#26102;&#35299;&#20915;&#35268;&#21017;&#38598;&#30340;&#31232;&#30095;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#20445;&#35777;&#27867;&#21270;&#24615;&#33021;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20154;&#31867;&#29992;&#25143;&#26469;&#35828;&#65292;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#29702;&#35299;&#25552;&#35758;&#20998;&#31867;&#22120;&#22914;&#20309;&#26681;&#25454;&#29305;&#24449;&#20540;&#32473;&#25968;&#25454;&#20998;&#37197;&#26631;&#31614;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20351;&#29992;&#29305;&#24449;&#20540;&#35268;&#21017;&#38598;&#26500;&#24314;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#38750;&#32447;&#24615;&#20381;&#36182;&#21644;&#20132;&#20114;&#20316;&#29992;&#12290;&#35268;&#21017;&#38598;&#30340;&#31232;&#30095;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#26435;&#34913;&#12290;&#20351;&#29992;&#29616;&#26377;&#26041;&#27861;&#26469;&#25214;&#21040;&#21512;&#36866;&#30340;&#31232;&#30095;&#24230;&#36873;&#25321;&#65288;&#20363;&#22914;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#65289;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#26469;&#23398;&#20064;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#31454;&#20105;&#22240;&#32032;&#30340;&#35268;&#21017;&#38598;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26469;&#30830;&#20445;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#35813;&#20844;&#24335;&#21033;&#29992;&#21015;&#29983;&#25104;&#26377;&#25928;&#22320;&#25628;&#32034;&#35268;&#21017;&#38598;&#21512;&#30340;&#31354;&#38388;&#24182;&#26500;&#24314;&#31232;&#30095;&#30340;&#35268;&#21017;&#38598;&#21512;&#65292;&#19982;&#38543;&#26426;&#26862;&#26519;&#25110;Boosting&#21450;&#20854;&#21464;&#20307;&#31561;&#25216;&#26415;&#30456;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35770;&#32467;&#26524;&#26469;&#25512;&#21160;&#36825;&#19968;&#20844;&#24335;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model explainability is crucial for human users to be able to interpret how a proposed classifier assigns labels to data based on its feature values. We study generalized linear models constructed using sets of feature value rules, which can capture nonlinear dependencies and interactions. An inherent trade-off exists between rule set sparsity and its prediction accuracy. It is computationally expensive to find the right choice of sparsity -- e.g., via cross-validation -- with existing methods. We propose a new formulation to learn an ensemble of rule sets that simultaneously addresses these competing factors. Good generalization is ensured while keeping computational costs low by utilizing distributionally robust optimization. The formulation utilizes column generation to efficiently search the space of rule sets and constructs a sparse ensemble of rule sets, in contrast with techniques like random forests or boosting and their variants. We present theoretical results that motivate an
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#30452;&#25509;&#20559;&#22909;&#36807;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20559;&#22909;&#30340;&#24207;&#32467;&#26500;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#35777;&#26368;&#20248;&#31574;&#30053;&#23384;&#22312;&#30340;&#26465;&#20214;&#12290;&#36825;&#20010;&#30740;&#31350;&#32553;&#23567;&#20102;&#23398;&#20064;&#20559;&#22909;&#21453;&#39304;&#31639;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#31574;&#30053;&#23384;&#22312;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2311.01990</link><description>&lt;p&gt;
&#26465;&#20214;&#23545;&#20559;&#22909;&#20851;&#31995;&#36827;&#34892;&#38480;&#21046;&#20197;&#20445;&#35777;&#26368;&#20248;&#31574;&#30053;&#30340;&#23384;&#22312;
&lt;/p&gt;
&lt;p&gt;
Conditions on Preference Relations that Guarantee the Existence of Optimal Policies. (arXiv:2311.01990v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01990
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#30452;&#25509;&#20559;&#22909;&#36807;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20559;&#22909;&#30340;&#24207;&#32467;&#26500;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#35777;&#26368;&#20248;&#31574;&#30053;&#23384;&#22312;&#30340;&#26465;&#20214;&#12290;&#36825;&#20010;&#30740;&#31350;&#32553;&#23567;&#20102;&#23398;&#20064;&#20559;&#22909;&#21453;&#39304;&#31639;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#31574;&#30053;&#23384;&#22312;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20559;&#22909;&#21453;&#39304; (LfPF) &#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26576;&#20123;&#31867;&#22411;&#30340;&#20132;&#20114;&#24335;&#23398;&#20064;&#20195;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;LfPF&#31639;&#27861;&#30340;&#29702;&#35770;&#21644;&#24212;&#29992;&#20043;&#38388;&#23384;&#22312;&#30528;&#23454;&#36136;&#24615;&#30340;&#24046;&#36317;&#12290;&#30446;&#21069;&#20445;&#35777;&#22312;LfPF&#38382;&#39064;&#20013;&#23384;&#22312;&#26368;&#20248;&#31574;&#30053;&#30340;&#32467;&#26524;&#20551;&#35774;&#20559;&#22909;&#21644;&#36716;&#31227;&#21160;&#24577;&#37117;&#30001;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30830;&#23450;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#30452;&#25509;&#20559;&#22909;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#12289;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#20998;&#26512;LfPF&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#20559;&#22909;&#30340;&#24207;&#32467;&#26500;&#24314;&#31435;&#20102;&#20445;&#35777;&#26368;&#20248;&#31574;&#30053;&#23384;&#22312;&#30340;&#26465;&#20214;&#12290;&#21033;&#29992;&#20911;&#183;&#35834;&#20381;&#26364;-&#25705;&#26681;&#26031;&#29305;&#24681;&#26399;&#26395;&#25928;&#29992;&#23450;&#29702;&#65292;&#25105;&#20204;&#34920;&#26126;&#30452;&#25509;&#20559;&#22909;&#36807;&#31243;&#25512;&#24191;&#20102;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#32553;&#23567;&#20102;LfPF&#31639;&#27861;&#22312;&#32463;&#39564;&#25104;&#21151;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#31574;&#30053;&#23384;&#22312;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Preferential Feedback (LfPF) plays an essential role in training Large Language Models, as well as certain types of interactive learning agents. However, a substantial gap exists between the theory and application of LfPF algorithms. Current results guaranteeing the existence of optimal policies in LfPF problems assume that both the preferences and transition dynamics are determined by a Markov Decision Process. We introduce the Direct Preference Process, a new framework for analyzing LfPF problems in partially-observable, non-Markovian environments. Within this framework, we establish conditions that guarantee the existence of optimal policies by considering the ordinal structure of the preferences. Using the von Neumann-Morgenstern Expected Utility Theorem, we show that the Direct Preference Process generalizes the standard reinforcement learning problem. Our findings narrow the gap between the empirical success and theoretical understanding of LfPF algorithms and provi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#26465;&#20214;&#19979;&#20648;&#23618;&#30456;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20805;&#20998;&#20445;&#30041;&#26465;&#20214;&#25968;&#25454;&#65292;&#29983;&#25104;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#20648;&#23618;&#30456;&#12290;&#23427;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;GANs&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01968</link><description>&lt;p&gt;
&#26465;&#20214;&#20648;&#23618;&#30456;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Model for Conditional Reservoir Facies Generation. (arXiv:2311.01968v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#26465;&#20214;&#19979;&#20648;&#23618;&#30456;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20805;&#20998;&#20445;&#30041;&#26465;&#20214;&#25968;&#25454;&#65292;&#29983;&#25104;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#20648;&#23618;&#30456;&#12290;&#23427;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;GANs&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27833;&#27668;&#39046;&#22495;&#30340;&#30000;&#22320;&#24320;&#21457;&#21644;&#20648;&#23618;&#31649;&#29702;&#20013;&#65292;&#22522;&#20110;&#26377;&#38480;&#27979;&#37327;&#25968;&#25454;&#21019;&#24314;&#20934;&#30830;&#19988;&#22320;&#36136;&#30495;&#23454;&#30340;&#20648;&#23618;&#30456;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20004;&#28857;&#22320;&#36136;&#32479;&#35745;&#26041;&#27861;&#34429;&#28982;&#22522;&#30784;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#22320;&#36136;&#27169;&#24335;&#12290;&#22810;&#28857;&#32479;&#35745;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#38543;&#30528;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#20852;&#36215;&#21644;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#20154;&#20204;&#24320;&#22987;&#20542;&#21521;&#20110;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#20648;&#23618;&#30456;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#30456;&#36739;&#20110;GANs&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#26465;&#20214;&#19979;&#30340;&#20648;&#23618;&#30456;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#20135;&#29983;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#20648;&#23618;&#30456;&#65292;&#20005;&#26684;&#20445;&#30041;&#20102;&#26465;&#20214;&#25968;&#25454;&#12290;&#23427;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;GANs&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating accurate and geologically realistic reservoir facies based on limited measurements is crucial for field development and reservoir management, especially in the oil and gas sector. Traditional two-point geostatistics, while foundational, often struggle to capture complex geological patterns. Multi-point statistics offers more flexibility, but comes with its own challenges. With the rise of Generative Adversarial Networks (GANs) and their success in various fields, there has been a shift towards using them for facies generation. However, recent advances in the computer vision domain have shown the superiority of diffusion models over GANs. Motivated by this, a novel Latent Diffusion Model is proposed, which is specifically designed for conditional generation of reservoir facies. The proposed model produces high-fidelity facies realizations that rigorously preserve conditioning data. It significantly outperforms a GAN-based alternative.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#32467;&#26500;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#23545;&#25552;&#31034;&#30340;&#35821;&#35328;&#23646;&#24615;&#26377;&#36739;&#39640;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01967</link><description>&lt;p&gt;
&#25552;&#31034;&#30340;&#35821;&#35328;&#65306;&#20160;&#20040;&#35821;&#35328;&#23646;&#24615;&#20351;&#24471;&#25552;&#31034;&#25104;&#21151;&#65311;
&lt;/p&gt;
&lt;p&gt;
The language of prompting: What linguistic properties make a prompt successful?. (arXiv:2311.01967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#32467;&#26500;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#23545;&#25552;&#31034;&#30340;&#35821;&#35328;&#23646;&#24615;&#26377;&#36739;&#39640;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#19968;&#20195;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;&#26469;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24615;&#33021;&#23545;&#25552;&#31034;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65292;&#20154;&#20204;&#20184;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#36827;&#34892;&#20247;&#21253;&#25552;&#31034;&#25110;&#35774;&#35745;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#23545;&#25552;&#31034;&#30340;&#35821;&#35328;&#23646;&#24615;&#19982;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#20041;&#19978;&#31561;&#25928;&#20294;&#22312;&#35821;&#35328;&#32467;&#26500;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#35821;&#27861;&#23646;&#24615;&#65288;&#22914;&#24773;&#24577;&#12289;&#26102;&#24577;&#12289;&#35821;&#24577;&#21644;&#35821;&#27668;&#65289;&#20197;&#21450;&#36890;&#36807;&#20351;&#29992;&#21516;&#20041;&#35789;&#24341;&#20837;&#35789;&#27719;-&#35821;&#20041;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#24120;&#35265;&#20551;&#35774;&#30456;&#30683;&#30462;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#22256;&#24785;&#24230;&#30340;&#25552;&#31034;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#36825;&#20123;&#25552;&#31034;&#21453;&#26144;&#20102;&#39044;&#35757;&#32451;&#25110;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#20351;&#29992;&#12290;&#25552;&#31034;&#22312;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#20043;&#38388;&#36716;&#31227;&#25928;&#26524;&#19981;&#20339;&#65292;&#24615;&#33021;&#26377;&#25152;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on lower perplexity prompts that reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30697;&#38453;&#20056;&#31215;&#20803;&#32032;&#36716;&#25442;&#30340;&#20302;&#31209;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#28385;&#36275;&#27604;$n^{2-o(1)}$&#26102;&#38388;&#22797;&#26434;&#24230;&#26356;&#22909;&#30340;&#30456;&#23545;&#35823;&#24046;&#20302;&#31209;&#36924;&#36817;&#31639;&#27861;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#20851;&#20110;&#35813;&#38382;&#39064;&#30340;&#26465;&#20214;&#26102;&#38388;&#22797;&#26434;&#24230;&#22256;&#38590;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01960</link><description>&lt;p&gt;
&#30697;&#38453;&#20056;&#31215;&#20803;&#32032;&#36716;&#25442;&#30340;&#20302;&#31209;&#36924;&#36817;&#30340;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products. (arXiv:2311.01960v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30697;&#38453;&#20056;&#31215;&#20803;&#32032;&#36716;&#25442;&#30340;&#20302;&#31209;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#28385;&#36275;&#27604;$n^{2-o(1)}$&#26102;&#38388;&#22797;&#26434;&#24230;&#26356;&#22909;&#30340;&#30456;&#23545;&#35823;&#24046;&#20302;&#31209;&#36924;&#36817;&#31639;&#27861;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#20851;&#20110;&#35813;&#38382;&#39064;&#30340;&#26465;&#20214;&#26102;&#38388;&#22797;&#26434;&#24230;&#22256;&#38590;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24555;&#36895;&#31639;&#27861;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#20803;&#32032;&#36716;&#25442;&#30340;&#20302;&#31209;&#36924;&#36817;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#24076;&#26395;&#25214;&#21040;&#19968;&#20010;&#22909;&#30340;&#31209;&#20026;$k$&#30340;&#36924;&#36817;&#20540;$f(U \cdot V)$&#65292;&#20854;&#20013;$U, V^\top \in \mathbb{R}^{n \times r}$&#26159;&#32473;&#23450;&#30340;&#65292;$r = O(\log(n))$&#65292;$f(x)$&#26159;&#19968;&#20010;&#19968;&#33324;&#30340;&#26631;&#37327;&#20989;&#25968;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#22312;&#20122;&#32447;&#24615;&#30340;&#20302;&#31209;&#36924;&#36817;&#20013;&#24050;&#32463;&#23637;&#31034;&#20102;&#22914;&#26524;&#28385;&#36275;&#26465;&#20214;(1)$U = V^\top$&#21644;&#26465;&#20214;(2)$f(x)$&#26159;&#19968;&#20010;PSD&#26680;&#20989;&#25968;&#65292;&#37027;&#20040;&#23384;&#22312;&#19968;&#20010;$O(nk^{\omega-1})$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#24120;&#31995;&#25968;&#30456;&#23545;&#35823;&#24046;&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#20013;$\omega \approx 2.376$&#26159;&#30697;&#38453;&#20056;&#27861;&#30340;&#25351;&#25968;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#35813;&#38382;&#39064;&#30340;&#39318;&#20010;&#26465;&#20214;&#26102;&#38388;&#22797;&#26434;&#24230;&#22256;&#38590;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#26465;&#20214;(1)&#21644;&#26465;&#20214;(2)&#23545;&#20110;&#22312;&#24191;&#27867;&#31867;&#21035;&#30340;&#20989;&#25968;&#20013;&#33719;&#24471;&#27604;$n^{2-o(1)}$&#26102;&#38388;&#22797;&#26434;&#24230;&#26356;&#22909;&#30340;&#30456;&#23545;&#35823;&#24046;&#20302;&#31209;&#36924;&#36817;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#26032;&#39062;&#30340;&#35268;&#32422;&#20174;&#24378;&#25351;&#25968;&#26102;&#38388;&#20551;&#35774;&#65288;SETH&#65289;&#20013;&#38477;&#20302;&#20102;&#23545;&#20110;&#24179;&#22374;spar&#30340;&#26464;&#26438;&#24471;&#20998;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by fast algorithms in natural language processing, we study low rank approximation in the entrywise transformed setting where we want to find a good rank $k$ approximation to $f(U \cdot V)$, where $U, V^\top \in \mathbb{R}^{n \times r}$ are given, $r = O(\log(n))$, and $f(x)$ is a general scalar function. Previous work in sublinear low rank approximation has shown that if both (1) $U = V^\top$ and (2) $f(x)$ is a PSD kernel function, then there is an $O(nk^{\omega-1})$ time constant relative error approximation algorithm, where $\omega \approx 2.376$ is the exponent of matrix multiplication. We give the first conditional time hardness results for this problem, demonstrating that both conditions (1) and (2) are in fact necessary for getting better than $n^{2-o(1)}$ time for a relative error low rank approximation for a wide class of functions. We give novel reductions from the Strong Exponential Time Hypothesis (SETH) that rely on lower bounding the leverage scores of flat spar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#24341;&#20837;&#20048;&#35266;&#26356;&#26032;&#65292;&#20197;&#32531;&#35299;&#21512;&#20316;&#20219;&#21153;&#20013;&#30340;&#30456;&#23545;&#36807;&#24230;&#27010;&#25324;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27844;&#28431;&#21270;&#32447;&#24615;&#25972;&#27969;&#20989;&#25968;&#26469;&#37325;&#22609;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#23545;&#28508;&#22312;&#30001;&#20854;&#20182;&#20195;&#29702;&#24341;&#36215;&#30340;&#20302;&#22238;&#25253;&#20010;&#21035;&#21160;&#20316;&#30340;&#20048;&#35266;&#24577;&#24230;&#12290;</title><link>http://arxiv.org/abs/2311.01953</link><description>&lt;p&gt;
&#20048;&#35266;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#22312;&#21512;&#20316;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimistic Multi-Agent Policy Gradient for Cooperative Tasks. (arXiv:2311.01953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#24341;&#20837;&#20048;&#35266;&#26356;&#26032;&#65292;&#20197;&#32531;&#35299;&#21512;&#20316;&#20219;&#21153;&#20013;&#30340;&#30456;&#23545;&#36807;&#24230;&#27010;&#25324;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27844;&#28431;&#21270;&#32447;&#24615;&#25972;&#27969;&#20989;&#25968;&#26469;&#37325;&#22609;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#23545;&#28508;&#22312;&#30001;&#20854;&#20182;&#20195;&#29702;&#24341;&#36215;&#30340;&#20302;&#22238;&#25253;&#20010;&#21035;&#21160;&#20316;&#30340;&#20048;&#35266;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#27425;&#20248;&#34892;&#20026;&#65292;&#23548;&#33268;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#27425;&#20248;&#32852;&#21512;&#31574;&#30053;&#65292;&#20986;&#29616;&#20102;&#30456;&#23545;&#36807;&#24230;&#27010;&#25324;&#65288;RO&#65289;&#38382;&#39064;&#12290;&#26089;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#20048;&#35266;&#20027;&#20041;&#21487;&#20197;&#32531;&#35299;&#20351;&#29992;&#34920;&#26684;&#21270;Q&#23398;&#20064;&#26102;&#30340;RO&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#20219;&#21153;&#26469;&#35828;&#65292;&#21033;&#29992;&#20989;&#25968;&#36924;&#36817;&#20048;&#35266;&#20027;&#20041;&#21487;&#33021;&#21152;&#21095;&#36807;&#20272;&#35745;&#65292;&#20174;&#32780;&#22833;&#36133;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26368;&#36817;&#30340;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#65288;MAPG&#65289;&#26041;&#27861;&#22312;&#35768;&#22810;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#20005;&#37325;&#30340;RO&#24773;&#20917;&#19979;&#21487;&#33021;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#32780;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20197;&#22312;MAPG&#26041;&#27861;&#20013;&#23454;&#29616;&#20048;&#35266;&#26356;&#26032;&#24182;&#32531;&#35299;RO&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#27844;&#28431;&#21270;&#32447;&#24615;&#25972;&#27969;&#20989;&#25968;&#65292;&#20854;&#20013;&#19968;&#20010;&#36229;&#21442;&#25968;&#36873;&#25321;&#20048;&#35266;&#31243;&#24230;&#20197;&#22312;&#26356;&#26032;&#31574;&#30053;&#26102;&#37325;&#26032;&#22609;&#36896;&#20248;&#21183;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21487;&#33021;&#30001;&#20854;&#20182;&#20195;&#29702;&#24341;&#36215;&#30340;&#22238;&#25253;&#36739;&#20302;&#30340;&#20010;&#21035;&#21160;&#20316;&#20445;&#25345;&#20048;&#35266;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Relative overgeneralization} (RO) occurs in cooperative multi-agent learning tasks when agents converge towards a suboptimal joint policy due to overfitting to suboptimal behavior of other agents. In early work, optimism has been shown to mitigate the \textit{RO} problem when using tabular Q-learning. However, with function approximation optimism can amplify overestimation and thus fail on complex tasks. On the other hand, recent deep multi-agent policy gradient (MAPG) methods have succeeded in many complex tasks but may fail with severe \textit{RO}. We propose a general, yet simple, framework to enable optimistic updates in MAPG methods and alleviate the RO problem. Specifically, we employ a \textit{Leaky ReLU} function where a single hyperparameter selects the degree of optimism to reshape the advantages when updating the policy. Intuitively, our method remains optimistic toward individual actions with lower returns which are potentially caused by other agents' sub-optimal be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ForecastPFN&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#19988;&#26356;&#24555;&#22320;&#36827;&#34892;&#38646;&#26679;&#26412;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2311.01933</link><description>&lt;p&gt;
ForecastPFN&#65306;&#21512;&#25104;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ForecastPFN: Synthetically-Trained Zero-Shot Forecasting. (arXiv:2311.01933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ForecastPFN&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#19988;&#26356;&#24555;&#22320;&#36827;&#34892;&#38646;&#26679;&#26412;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32477;&#22823;&#22810;&#25968;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#39044;&#27979;&#24212;&#29992;&#21482;&#26377;&#38750;&#24120;&#23569;&#30340;&#21021;&#22987;&#35266;&#27979;&#25968;&#25454;&#65292;&#26377;&#26102;&#21482;&#26377;40&#20010;&#25110;&#26356;&#23569;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#39044;&#27979;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#30095;&#30340;&#21830;&#19994;&#24212;&#29992;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#34429;&#28982;&#26368;&#36817;&#22312;&#38750;&#24120;&#26377;&#38480;&#30340;&#21021;&#22987;&#25968;&#25454;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#19968;&#20123;&#30740;&#31350;&#65288;&#25152;&#35859;&#30340;&#8220;&#38646;&#26679;&#26412;&#8221;&#39044;&#27979;&#65289;&#65292;&#20294;&#20854;&#24615;&#33021;&#22240;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#32780;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;ForecastPFN&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#27169;&#22411;&#65292;&#20165;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#12290;ForecastPFN&#26159;&#19968;&#20010;&#32463;&#36807;&#25968;&#25454;&#25311;&#21512;&#30340;&#32593;&#32476;&#65292;&#35757;&#32451;&#29992;&#20110;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#21487;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ForecastPFN&#36827;&#34892;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#27604;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#39044;&#27979;&#26041;&#27861;&#26356;&#20934;&#30830;&#21644;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vast majority of time-series forecasting approaches require a substantial training dataset. However, many real-life forecasting applications have very little initial observations, sometimes just 40 or fewer. Thus, the applicability of most forecasting methods is restricted in data-sparse commercial applications. While there is recent work in the setting of very limited initial data (so-called `zero-shot' forecasting), its performance is inconsistent depending on the data used for pretraining. In this work, we take a different approach and devise ForecastPFN, the first zero-shot forecasting model trained purely on a novel synthetic data distribution. ForecastPFN is a prior-data fitted network, trained to approximate Bayesian inference, which can make predictions on a new time series dataset in a single forward pass. Through extensive experiments, we show that zero-shot predictions made by ForecastPFN are more accurate and faster compared to state-of-the-art forecasting methods, even
&lt;/p&gt;</description></item><item><title>GateLoop&#26159;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#24207;&#21015;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#32473;Attention&#12290;</title><link>http://arxiv.org/abs/2311.01927</link><description>&lt;p&gt;
GateLoop: &#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01927
&lt;/p&gt;
&lt;p&gt;
GateLoop&#26159;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#24207;&#21015;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#32473;Attention&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#36882;&#24402;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#38271;&#24207;&#21015;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#29616;&#26377;&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20854;&#28508;&#21147;&#12290;&#22312;&#36825;&#19968;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GateLoop&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#30784;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#25511;&#21046;&#30340;&#29366;&#24577;&#36716;&#25442;&#26469;&#25512;&#24191;&#32447;&#24615;&#36882;&#24402;&#27169;&#22411;&#65292;&#22914;S4&#12289;S5&#12289;LRU&#21644;RetNet&#12290;&#21033;&#29992;&#36825;&#19968;&#29702;&#35770;&#36827;&#27493;&#65292;GateLoop&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#22312;&#23454;&#35777;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20302;&#25104;&#26412;&#30340;$O(l)$&#36882;&#24402;&#27169;&#24335;&#21644;&#39640;&#24230;&#20248;&#21270;&#30340;&#20851;&#32852;&#25195;&#25551;&#23454;&#29616;&#30340;&#39640;&#25928;$O(l \log_{2} l)$&#24182;&#34892;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;$O(l^2)$&#30340;&#20195;&#29702;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23545;Transformer&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#26550;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#21521;Attention&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;&#32780;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#26080;&#20851;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on da
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25991;&#31456;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#23398;&#20013;&#30340;&#24212;&#29992;&#21644;&#24847;&#20041;&#12290;LLMs&#22312;&#30693;&#35782;&#26816;&#32034;&#12289;&#30740;&#31350;&#25903;&#25345;&#12289;&#20020;&#24202;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#35786;&#26029;&#36741;&#21161;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22810;&#27169;&#24577;LLMs&#21487;&#20197;&#22788;&#29702;&#21307;&#23398;&#24433;&#20687;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31561;&#22810;&#26679;&#21270;&#25968;&#25454;&#31867;&#22411;&#20197;&#22686;&#24378;&#35786;&#26029;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01918</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38416;&#26126;&#20102;&#20154;&#24037;&#21307;&#30103;&#21161;&#25163;&#30340;&#36827;&#23637;&#36335;&#24452;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review. (arXiv:2311.01918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25991;&#31456;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#23398;&#20013;&#30340;&#24212;&#29992;&#21644;&#24847;&#20041;&#12290;LLMs&#22312;&#30693;&#35782;&#26816;&#32034;&#12289;&#30740;&#31350;&#25903;&#25345;&#12289;&#20020;&#24202;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#35786;&#26029;&#36741;&#21161;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22810;&#27169;&#24577;LLMs&#21487;&#20197;&#22788;&#29702;&#21307;&#23398;&#24433;&#20687;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31561;&#22810;&#26679;&#21270;&#25968;&#25454;&#31867;&#22411;&#20197;&#22686;&#24378;&#35786;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#27169;&#25311;&#20154;&#31867;&#32423;&#21035;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#22686;&#24378;&#21307;&#30103;&#21508;&#20010;&#26041;&#38754;&#30340;&#37325;&#35201;&#20852;&#36259;&#65292;&#33539;&#22260;&#20174;&#21307;&#23398;&#25945;&#32946;&#21040;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#28041;&#21450;&#22810;&#26041;&#38754;&#30340;&#25968;&#25454;&#27169;&#24577;&#21644;&#24494;&#22937;&#30340;&#25512;&#29702;&#25216;&#33021;&#65292;&#36825;&#32473;LLMs&#30340;&#25972;&#21512;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#22312;&#21307;&#23398;&#20013;&#30340;&#24212;&#29992;&#21644;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#32771;&#23519;&#20102;&#36890;&#29992;&#22411;&#21644;&#19987;&#38376;&#21270;LLMs&#30340;&#22522;&#26412;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#30693;&#35782;&#26816;&#32034;&#12289;&#30740;&#31350;&#25903;&#25345;&#12289;&#20020;&#24202;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#35786;&#26029;&#36741;&#21161;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#37492;&#20110;&#21307;&#23398;&#30340;&#22266;&#26377;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#35813;&#32508;&#36848;&#36827;&#19968;&#27493;&#20851;&#27880;&#22810;&#27169;&#24577;LLMs&#65292;&#30740;&#31350;&#20854;&#22788;&#29702;&#21307;&#23398;&#24433;&#20687;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31561;&#22810;&#26679;&#21270;&#25968;&#25454;&#31867;&#22411;&#20197;&#22686;&#24378;&#35786;&#26029;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of artificial intelligence, large language models (LLMs) have shown promising capabilities in mimicking human-level language comprehension and reasoning. This has sparked significant interest in applying LLMs to enhance various aspects of healthcare, ranging from medical education to clinical decision support. However, medicine involves multifaceted data modalities and nuanced reasoning skills, presenting challenges for integrating LLMs. This paper provides a comprehensive review on the applications and implications of LLMs in medicine. It begins by examining the fundamental applications of general-purpose and specialized LLMs, demonstrating their utilities in knowledge retrieval, research support, clinical workflow automation, and diagnostic assistance. Recognizing the inherent multimodality of medicine, the review then focuses on multimodal LLMs, investigating their ability to process diverse data types like medical imaging and EHRs to augment diagnostic ac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29616;&#23454;&#26465;&#20214;&#38543;&#26426;&#21270;&#29615;&#22659;&#20013;&#23545;&#22240;&#26524;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#26041;&#24046;&#22240;&#26524;&#35823;&#24046;&#20272;&#35745;&#22120;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#24341;&#20837;&#30340;&#26041;&#24046;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#24471;&#21040;&#25913;&#36827;&#65292;&#24182;&#23637;&#31034;&#20102;&#25509;&#36817;RCT&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01902</link><description>&lt;p&gt;
&#39640;&#31934;&#24230;&#30340;&#26465;&#20214;&#38543;&#26426;&#21270;&#22240;&#26524;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
High Precision Causal Model Evaluation with Conditional Randomization. (arXiv:2311.01902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01902
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29616;&#23454;&#26465;&#20214;&#38543;&#26426;&#21270;&#29615;&#22659;&#20013;&#23545;&#22240;&#26524;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#26041;&#24046;&#22240;&#26524;&#35823;&#24046;&#20272;&#35745;&#22120;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#24341;&#20837;&#30340;&#26041;&#24046;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#24471;&#21040;&#25913;&#36827;&#65292;&#24182;&#23637;&#31034;&#20102;&#25509;&#36817;RCT&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#27169;&#22411;&#35780;&#20272;&#30340;&#40644;&#37329;&#26631;&#20934;&#26159;&#23558;&#27169;&#22411;&#39044;&#27979;&#19982;&#26469;&#33258;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCT&#65289;&#30340;&#30495;&#23454;&#25928;&#24212;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#36827;&#34892;RCT&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#25110;&#36947;&#24503;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#36870;&#27010;&#29575;&#21152;&#26435;&#65288;IPW&#65289;&#30340;&#26465;&#20214;&#38543;&#26426;&#21270;&#23454;&#39564;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#29616;&#23454;&#30340;&#26041;&#27861;&#65292;&#20294;&#21487;&#33021;&#21463;&#21040;&#20272;&#35745;&#26041;&#24046;&#36807;&#22823;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#24182;&#22686;&#24378;&#22312;&#26465;&#20214;&#38543;&#26426;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#26524;&#27169;&#22411;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#26041;&#24046;&#22240;&#26524;&#35823;&#24046;&#20272;&#35745;&#22120;&#65292;&#31216;&#20043;&#20026;&#8220;&#23545;&#27604;&#20272;&#35745;&#22120;&#8221;&#12290;&#36890;&#36807;&#23558;&#30456;&#21516;&#30340;IPW&#20272;&#35745;&#22120;&#24212;&#29992;&#20110;&#27169;&#22411;&#21644;&#30495;&#23454;&#23454;&#39564;&#25928;&#24212;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#30001;&#20110;IPW&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#28176;&#36817;&#26041;&#24046;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#25913;&#36827;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#23454;&#29616;&#25509;&#36817;RCT&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#22312;&#26465;&#20214;&#38543;&#26426;&#21270;&#29615;&#22659;&#20013;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#25552;&#20379;&#20102;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The gold standard for causal model evaluation involves comparing model predictions with true effects estimated from randomized controlled trials (RCT). However, RCTs are not always feasible or ethical to perform. In contrast, conditionally randomized experiments based on inverse probability weighting (IPW) offer a more realistic approach but may suffer from high estimation variance. To tackle this challenge and enhance causal model evaluation in real-world conditional randomization settings, we introduce a novel low-variance estimator for causal error, dubbed as the pairs estimator. By applying the same IPW estimator to both the model and true experimental effects, our estimator effectively cancels out the variance due to IPW and achieves a smaller asymptotic variance. Empirical studies demonstrate the improved of our estimator, highlighting its potential on achieving near-RCT performance. Our method offers a simple yet powerful solution to evaluate causal inference models in condition
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#38750;&#21442;&#25968;&#20284;&#28982;&#27604;&#20272;&#35745;&#65288;OLRE&#65289;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20272;&#35745;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20043;&#38388;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26680;&#26041;&#27861;&#21644;&#20989;&#25968;&#26368;&#23567;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#22312;&#32447;&#26356;&#26032;&#65292;&#21516;&#26102;&#20855;&#26377;&#23545;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#24418;&#24335;&#26080;&#30693;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2311.01900</link><description>&lt;p&gt;
&#22312;&#32447;&#38750;&#21442;&#25968;&#20284;&#28982;&#27604;&#20272;&#35745;&#30340;&#30382;&#23572;&#36874;&#25955;&#24230;&#20989;&#25968;&#26368;&#23567;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online non-parametric likelihood-ratio estimation by Pearson-divergence functional minimization. (arXiv:2311.01900v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#38750;&#21442;&#25968;&#20284;&#28982;&#27604;&#20272;&#35745;&#65288;OLRE&#65289;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20272;&#35745;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20043;&#38388;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26680;&#26041;&#27861;&#21644;&#20989;&#25968;&#26368;&#23567;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#22312;&#32447;&#26356;&#26032;&#65292;&#21516;&#26102;&#20855;&#26377;&#23545;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#24418;&#24335;&#26080;&#30693;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21487;&#29992;&#25968;&#25454;&#37327;&#21270;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;p&#21644;q&#20043;&#38388;&#30340;&#24046;&#24322;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#20284;&#28982;&#27604;&#20272;&#35745;&#65288;LRE&#65289;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#32447;&#38750;&#21442;&#25968;&#20284;&#28982;&#27604;&#20272;&#35745;&#65288;OLRE&#65289;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#38543;&#26102;&#38388;&#35266;&#23519;&#21040;&#30340;i.i.d&#35266;&#27979;&#20540;&#65288;$x_t \sim p, x'_t \sim q$&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#38750;&#21442;&#25968;&#24615;&#36136;&#20855;&#26377;&#23545;$p$&#21644;$q$&#30340;&#24418;&#24335;&#26080;&#30693;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#26680;&#26041;&#27861;&#21644;&#20989;&#25968;&#26368;&#23567;&#21270;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#22312;&#32447;&#26356;&#26032;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;OLRE&#26041;&#27861;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#21512;&#25104;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying the difference between two probability density functions, $p$ and $q$, using available data, is a fundamental problem in Statistics and Machine Learning. A usual approach for addressing this problem is the likelihood-ratio estimation (LRE) between $p$ and $q$, which -- to our best knowledge -- has been investigated mainly for the offline case. This paper contributes by introducing a new framework for online non-parametric LRE (OLRE) for the setting where pairs of iid observations $(x_t \sim p, x'_t \sim q)$ are observed over time. The non-parametric nature of our approach has the advantage of being agnostic to the forms of $p$ and $q$. Moreover, we capitalize on the recent advances in Kernel Methods and functional minimization to develop an estimator that can be efficiently updated online. We provide theoretical guarantees for the performance of the OLRE method along with empirical validation in synthetic experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#29992;&#20110;&#31232;&#30095;&#32534;&#30721;&#21442;&#25968;&#30340;&#23398;&#20064;&#65292;&#36890;&#36807;&#38750;&#24179;&#20961;&#30340;&#21518;&#39564;&#36924;&#36817;&#21644;&#35299;&#26512;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#26631;&#20934;&#31232;&#30095;&#32534;&#30721;&#30340;&#23398;&#20064;&#65292;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01888</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#29109;&#30340;ELBO&#23398;&#20064;&#31232;&#30095;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Learning Sparse Codes with Entropy-Based ELBOs. (arXiv:2311.01888v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#29992;&#20110;&#31232;&#30095;&#32534;&#30721;&#21442;&#25968;&#30340;&#23398;&#20064;&#65292;&#36890;&#36807;&#38750;&#24179;&#20961;&#30340;&#21518;&#39564;&#36924;&#36817;&#21644;&#35299;&#26512;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#26631;&#20934;&#31232;&#30095;&#32534;&#30721;&#30340;&#23398;&#20064;&#65292;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#27010;&#29575;&#31232;&#30095;&#32534;&#30721;&#20551;&#35774;&#25289;&#26222;&#25289;&#26031;&#20808;&#39564;&#12289;&#20174;&#28508;&#22312;&#21040;&#21487;&#35266;&#27979;&#30340;&#32447;&#24615;&#26144;&#23556;&#20197;&#21450;&#39640;&#26031;&#21487;&#35266;&#27979;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23548;&#20986;&#20102;&#19968;&#20010;&#20165;&#22522;&#20110;&#29109;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#29992;&#20110;&#26631;&#20934;&#31232;&#30095;&#32534;&#30721;&#30340;&#21442;&#25968;&#12290;&#36825;&#20010;&#26032;&#30340;&#21464;&#20998;&#30446;&#26631;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;A&#65289;&#19982;MAP&#36924;&#36817;&#19981;&#21516;&#65292;&#23427;&#20351;&#29992;&#20102;&#27010;&#29575;&#25512;&#29702;&#30340;&#38750;&#24179;&#20961;&#21518;&#39564;&#36924;&#36817;&#65307;&#65288;B&#65289;&#19982;&#20197;&#21069;&#30340;&#38750;&#24179;&#20961;&#36924;&#36817;&#19981;&#21516;&#65292;&#36825;&#20010;&#26032;&#30340;&#30446;&#26631;&#26159;&#23436;&#20840;&#35299;&#26512;&#30340;&#65307;&#65288;C&#65289;&#35813;&#30446;&#26631;&#20801;&#35768;&#19968;&#31181;&#26032;&#30340;&#21407;&#21017;&#24615;&#30340;&#36864;&#28779;&#24418;&#24335;&#12290;&#30446;&#26631;&#30340;&#23548;&#20986;&#39318;&#20808;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;ELBO&#30446;&#26631;&#25910;&#25947;&#21040;&#29109;&#30340;&#21644;&#65292;&#36825;&#19982;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#36817;&#31867;&#20284;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ELBO&#31561;&#20110;&#29109;&#30340;&#26465;&#20214;&#20855;&#26377;&#35299;&#26512;&#35299;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#23436;&#20840;&#35299;&#26512;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#23398;&#20064;&#36924;&#30495;&#24615;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard probabilistic sparse coding assumes a Laplace prior, a linear mapping from latents to observables, and Gaussian observable distributions. We here derive a solely entropy-based learning objective for the parameters of standard sparse coding. The novel variational objective has the following features: (A) unlike MAP approximations, it uses non-trivial posterior approximations for probabilistic inference; (B) unlike for previous non-trivial approximations, the novel objective is fully analytical; and (C) the objective allows for a novel principled form of annealing. The objective is derived by first showing that the standard ELBO objective converges to a sum of entropies, which matches similar recent results for generative models with Gaussian priors. The conditions under which the ELBO becomes equal to entropies are then shown to have analytical solutions, which leads to the fully analytical objective. Numerical experiments are used to demonstrate the feasibility of learning wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#29109;&#26368;&#22823;&#21270;&#30340;&#26041;&#24335;&#22312;&#27169;&#25311;&#35757;&#32451;&#20013;&#35843;&#25972;&#21160;&#21147;&#23398;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#65292;&#26080;&#38656;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#33021;&#22815;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#21644;&#20195;&#29702;&#30340;&#39640;&#25104;&#21151;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.01885</link><description>&lt;p&gt;
&#36890;&#36807;&#29109;&#26368;&#22823;&#21270;&#36827;&#34892;&#39046;&#22495;&#38543;&#26426;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Randomization via Entropy Maximization. (arXiv:2311.01885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#29109;&#26368;&#22823;&#21270;&#30340;&#26041;&#24335;&#22312;&#27169;&#25311;&#35757;&#32451;&#20013;&#35843;&#25972;&#21160;&#21147;&#23398;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#65292;&#26080;&#38656;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#33021;&#22815;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#21644;&#20195;&#29702;&#30340;&#39640;&#25104;&#21151;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#25913;&#21464;&#27169;&#25311;&#20013;&#30340;&#21160;&#21147;&#23398;&#21442;&#25968;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20811;&#26381;&#29616;&#23454;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#38543;&#26426;&#21270;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#21160;&#21147;&#23398;&#21442;&#25968;&#30340;&#25277;&#26679;&#20998;&#24067;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#39640;&#21464;&#24322;&#23545;&#20110;&#35268;&#33539;&#20195;&#29702;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36807;&#24230;&#38543;&#26426;&#21270;&#20250;&#23548;&#33268;&#36807;&#20110;&#20445;&#23432;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#65292;&#21363;&#22312;&#27169;&#25311;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#21160;&#35843;&#25972;&#21160;&#21147;&#23398;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;&#29109;&#26368;&#22823;&#21270;&#23454;&#29616;&#39046;&#22495;&#38543;&#26426;&#21270;&#30340;&#26041;&#27861;&#65288;DORAEMON&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#65292;&#30452;&#25509;&#26368;&#22823;&#21270;&#35757;&#32451;&#20998;&#24067;&#30340;&#29109;&#21516;&#26102;&#20445;&#30041;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;DORAEMON&#38543;&#30528;&#24403;&#21069;&#31574;&#30053;&#25104;&#21151;&#27010;&#29575;&#36275;&#22815;&#39640;&#65292;&#36880;&#28176;&#22686;&#21152;&#26679;&#26412;&#21160;&#21147;&#23398;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Varying dynamics parameters in simulation is a popular Domain Randomization (DR) approach for overcoming the reality gap in Reinforcement Learning (RL). Nevertheless, DR heavily hinges on the choice of the sampling distribution of the dynamics parameters, since high variability is crucial to regularize the agent's behavior but notoriously leads to overly conservative policies when randomizing excessively. In this paper, we propose a novel approach to address sim-to-real transfer, which automatically shapes dynamics distributions during training in simulation without requiring real-world data. We introduce DOmain RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization problem that directly maximizes the entropy of the training distribution while retaining generalization capabilities. In achieving this, DORAEMON gradually increases the diversity of sampled dynamics parameters as long as the probability of success of the current policy is sufficiently high. We empiri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22686;&#24378;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#65288;FDA&#65289;&#65292;&#24182;&#36890;&#36807;&#19982;&#24120;&#35265;FDA&#22238;&#24402;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#35777;&#26126;&#20102;SNNs&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01875</link><description>&lt;p&gt;
&#29992;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#65306;&#20248;&#21183;&#21644;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Functional Data Analysis with Sequential Neural Networks: Advantages and Comparative Study. (arXiv:2311.01875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22686;&#24378;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#65288;FDA&#65289;&#65292;&#24182;&#36890;&#36807;&#19982;&#24120;&#35265;FDA&#22238;&#24402;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#35777;&#26126;&#20102;SNNs&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#65288;FDA&#65289;&#26159;&#19968;&#31181;&#22788;&#29702;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#29305;&#24449;&#30340;&#32479;&#35745;&#39046;&#22495;&#12290;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#65292;&#36825;&#26159;&#21151;&#33021;&#25968;&#25454;&#30340;&#19968;&#20010;&#22522;&#26412;&#29305;&#24449;&#12290;&#23613;&#31649;SNNs&#22312;&#24314;&#27169;&#21151;&#33021;&#25968;&#25454;&#26041;&#38754;&#38750;&#24120;&#28789;&#27963;&#65292;&#20294;&#22312;FDA&#31038;&#21306;&#20013;&#20351;&#29992;&#19981;&#22810;&#12290;SNNs&#30340;&#19968;&#20010;&#26174;&#33879;&#20248;&#21183;&#26159;&#26131;&#20110;&#23454;&#26045;&#65292;&#21487;&#20197;&#35753;&#24191;&#22823;&#38750;&#23398;&#26415;&#30028;&#30340;&#20154;&#32676;&#33021;&#22815;&#20351;&#29992;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;FDA&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38750;&#19987;&#19994;&#20154;&#22763;&#26469;&#35828;&#65292;&#22240;&#20026;&#23427;&#20204;&#36807;&#20110;&#22797;&#26434;&#12290;&#22522;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#35758;&#22312;FDA&#24212;&#29992;&#20013;&#21033;&#29992;SNNs&#65292;&#24182;&#36890;&#36807;&#19982;&#24120;&#35265;FDA&#22238;&#24402;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;SNNs&#30340;&#26550;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;FDA&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional Data Analysis (FDA) is a statistical domain developed to handle functional data characterized by high dimensionality and complex data structures. Sequential Neural Networks (SNNs) are specialized neural networks capable of processing sequence data, a fundamental aspect of functional data. Despite their great flexibility in modeling functional data, SNNs have been inadequately employed in the FDA community. One notable advantage of SNNs is the ease of implementation, making them accessible to a broad audience beyond academia. Conversely, FDA-based methodologies present challenges, particularly for practitioners outside the field, due to their intricate complexity. In light of this, we propose utilizing SNNs in FDA applications and demonstrate their effectiveness through comparative analyses against popular FDA regression models based on numerical experiments and real-world data analysis. SNN architectures allow us to surpass the limitations of traditional FDA methods, offerin
&lt;/p&gt;</description></item><item><title>SortNet&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27604;&#36739;&#22120;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#25490;&#24207;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#26500;&#24314;&#35757;&#32451;&#38598;&#65292;&#26681;&#25454;&#25104;&#23545;&#39033;&#30446;&#20043;&#38388;&#30340;&#25490;&#24207;&#31034;&#20363;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2311.01864</link><description>&lt;p&gt;
SortNet: &#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25490;&#24207;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
SortNet: Learning To Rank By a Neural-Based Sorting Algorithm. (arXiv:2311.01864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01864
&lt;/p&gt;
&lt;p&gt;
SortNet&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27604;&#36739;&#22120;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#25490;&#24207;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#26500;&#24314;&#35757;&#32451;&#38598;&#65292;&#26681;&#25454;&#25104;&#23545;&#39033;&#30446;&#20043;&#38388;&#30340;&#25490;&#24207;&#31034;&#20363;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#30456;&#20851;&#24615;&#25490;&#21517;&#30340;&#38382;&#39064;&#65292;&#21363;&#26681;&#25454;&#32473;&#23450;&#30340;&#26631;&#20934;&#23545;&#19968;&#32452;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#12290;&#30001;&#20110;&#29992;&#25143;&#21487;&#33021;&#20559;&#22909;&#19981;&#21516;&#30340;&#30456;&#20851;&#24615;&#26631;&#20934;&#65292;&#22240;&#27492;&#25490;&#24207;&#31639;&#27861;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#36827;&#34892;&#35843;&#25972;&#12290;&#23398;&#20064;&#25490;&#24207;&#30340;&#20219;&#21153;&#22312;&#25991;&#29486;&#20013;&#23384;&#22312;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;1&#65289;&#36890;&#36807;&#31034;&#20363;&#23398;&#20064;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#35780;&#20272;&#27599;&#20010;&#23545;&#35937;&#30340;&#23646;&#24615;&#65292;&#29983;&#25104;&#21487;&#29992;&#20110;&#23545;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#30340;&#32477;&#23545;&#30456;&#20851;&#24615;&#20540;&#65307;2&#65289;&#19968;&#31181;&#25104;&#23545;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#35937;&#23545;&#26469;&#23398;&#20064;&#8220;&#20559;&#22909;&#20989;&#25968;&#8221;&#65292;&#23450;&#20041;&#21738;&#19968;&#20010;&#23545;&#35937;&#24212;&#35813;&#39318;&#20808;&#25490;&#21517;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SortNet&#65292;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27604;&#36739;&#22120;&#26469;&#23545;&#23545;&#35937;&#36827;&#34892;&#33258;&#36866;&#24212;&#25490;&#24207;&#30340;&#31639;&#27861;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38598;&#25552;&#20379;&#20102;&#23545;&#20110;&#25104;&#23545;&#39033;&#30446;&#20043;&#38388;&#25152;&#38656;&#25490;&#24207;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#26500;&#24314;&#65292;&#27599;&#27425;&#36845;&#20195;&#37117;&#20250;&#28155;&#21152;&#26368;&#20855;&#20449;&#24687;&#24615;&#30340;&#35757;&#32451;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#27604;&#36739;&#22120;&#37319;&#29992;&#20102;&#36830;&#25509;&#20027;&#20041;&#20307;&#31995;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of relevance ranking consists of sorting a set of objects with respect to a given criterion. Since users may prefer different relevance criteria, the ranking algorithms should be adaptable to the user needs. Two main approaches exist in literature for the task of learning to rank: 1) a score function, learned by examples, which evaluates the properties of each object yielding an absolute relevance value that can be used to order the objects or 2) a pairwise approach, where a "preference function" is learned using pairs of objects to define which one has to be ranked first. In this paper, we present SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. The neural network training set provides examples of the desired ordering between pairs of items and it is constructed by an iterative procedure which, at each iteration, adds the most informative training examples. Moreover, the comparator adopts a connectionist architecture that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23615;&#28082;&#21442;&#25968;&#30340;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;COVID-19&#31579;&#26597;&#12290;&#36890;&#36807;&#36716;&#25442;&#22810;&#20010;&#39068;&#33394;&#31354;&#38388;&#24182;&#37319;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01854</link><description>&lt;p&gt;
&#22522;&#20110;&#23615;&#28082;&#21442;&#25968;&#30340;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;COVID-19&#31579;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Ensemble Machine Learning Approach for Screening Covid-19 based on Urine Parameters. (arXiv:2311.01854v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23615;&#28082;&#21442;&#25968;&#30340;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;COVID-19&#31579;&#26597;&#12290;&#36890;&#36807;&#36716;&#25442;&#22810;&#20010;&#39068;&#33394;&#31354;&#38388;&#24182;&#37319;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30340;&#24555;&#36895;&#20256;&#25773;&#21644;&#26032;&#21464;&#31181;&#30340;&#20986;&#29616;&#20984;&#26174;&#20102;&#26377;&#25928;&#31579;&#26597;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;&#21450;&#26102;&#35786;&#26029;&#21644;&#38543;&#21518;&#30340;&#24863;&#26579;&#32773;&#38548;&#31163;&#21487;&#20197;&#38450;&#27490;&#30149;&#27602;&#22312;&#31038;&#20250;&#20013;&#36827;&#19968;&#27493;&#20256;&#25773;&#12290;&#34429;&#28982;PCR&#27979;&#35797;&#26159;COVID-19&#35786;&#26029;&#30340;&#37329;&#26631;&#20934;&#65292;&#20294;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#38271;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23615;&#28082;&#35797;&#32440;&#26159;&#19968;&#31181;&#24265;&#20215;&#12289;&#38750;&#20405;&#20837;&#24615;&#19988;&#24555;&#36895;&#33719;&#21462;&#30340;&#31579;&#26597;&#26041;&#27861;&#65292;&#21487;&#25552;&#20379;&#20851;&#20110;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#23615;&#28082;&#35797;&#32440;&#21442;&#25968;&#30340;RGB&#65288;&#32418;&#32511;&#34013;&#65289;&#33394;&#24425;&#31354;&#38388;&#26469;&#26816;&#27979;&#20010;&#20307;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#23558;RGB&#31354;&#38388;&#36716;&#25442;&#20026;&#20854;&#20182;10&#20010;&#39068;&#33394;&#31354;&#38388;&#12290;&#32463;&#36807;&#35780;&#20272;&#22235;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#38598;&#25104;&#27169;&#22411;&#12290;&#34429;&#28982;&#26368;&#21021;&#30340;&#32467;&#26524;&#24182;&#19981;&#24378;&#65292;&#20294;&#25105;&#20204;&#33021;&#22815;&#25913;&#36827;&#27169;&#22411;&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid spread of COVID-19 and the emergence of new variants underscore the importance of effective screening measures. Rapid diagnosis and subsequent quarantine of infected individuals can prevent further spread of the virus in society. While PCR tests are the gold standard for COVID-19 diagnosis, they are costly and time-consuming. In contrast, urine test strips are an inexpensive, non-invasive, and rapidly obtainable screening method that can provide important information about a patient's health status. In this study, we collected a new dataset and used the RGB (Red Green Blue) color space of urine test strips parameters to detect the health status of individuals. To improve the accuracy of our model, we converted the RGB space to 10 additional color spaces. After evaluating four different machine learning models, we proposed a new ensemble model based on a multi-layer perceptron neural network. Although the initial results were not strong, we were able to improve the model's scr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#33410;&#28857;&#23646;&#24615;&#30340;&#22810;&#20851;&#31995;&#22270;&#30340;&#32852;&#21512;&#38477;&#32500;&#25216;&#26415;SpectralMix&#65292;&#36890;&#36807;&#25972;&#21512;&#23646;&#24615;&#12289;&#19981;&#21516;&#31867;&#22411;&#20851;&#31995;&#21644;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2311.01840</link><description>&lt;p&gt;
&#24102;&#23646;&#24615;&#30340;&#22810;&#20851;&#31995;&#22270;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spectral Clustering of Attributed Multi-relational Graphs. (arXiv:2311.01840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#33410;&#28857;&#23646;&#24615;&#30340;&#22810;&#20851;&#31995;&#22270;&#30340;&#32852;&#21512;&#38477;&#32500;&#25216;&#26415;SpectralMix&#65292;&#36890;&#36807;&#25972;&#21512;&#23646;&#24615;&#12289;&#19981;&#21516;&#31867;&#22411;&#20851;&#31995;&#21644;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26088;&#22312;&#21457;&#29616;&#33410;&#28857;&#30340;&#33258;&#28982;&#20998;&#32452;&#65292;&#20351;&#24471;&#30456;&#20284;&#30340;&#33410;&#28857;&#34987;&#20998;&#37197;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#32858;&#31867;&#20013;&#12290;&#35768;&#22810;&#19981;&#21516;&#30340;&#31639;&#27861;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#65306;&#23545;&#20110;&#31616;&#21333;&#22270;&#65292;&#23545;&#20110;&#24102;&#26377;&#33410;&#28857;&#23646;&#24615;&#30340;&#22270;&#65292;&#20197;&#21450;&#23545;&#20110;&#36793;&#34920;&#31034;&#33410;&#28857;&#20043;&#38388;&#19981;&#21516;&#31867;&#22411;&#20851;&#31995;&#30340;&#22270;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#39046;&#22495;&#20013;&#30340;&#22797;&#26434;&#25968;&#25454;&#21487;&#20197;&#34920;&#31034;&#20026;&#26082;&#20855;&#26377;&#23646;&#24615;&#21448;&#20855;&#26377;&#22810;&#20851;&#31995;&#30340;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpectralMix&#65292;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#20998;&#31867;&#33410;&#28857;&#23646;&#24615;&#30340;&#22810;&#20851;&#31995;&#22270;&#30340;&#32852;&#21512;&#38477;&#32500;&#25216;&#26415;&#12290;SpectralMix&#25972;&#21512;&#20102;&#26469;&#33258;&#23646;&#24615;&#12289;&#19981;&#21516;&#31867;&#22411;&#20851;&#31995;&#21644;&#22270;&#32467;&#26500;&#30340;&#25152;&#26377;&#21487;&#29992;&#20449;&#24687;&#65292;&#20197;&#20415;&#23545;&#32858;&#31867;&#32467;&#26524;&#36827;&#34892;&#21512;&#29702;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25512;&#24191;&#20102;&#29616;&#26377;&#25216;&#26415;&#65306;&#24403;&#20165;&#24212;&#29992;&#20110;&#21333;&#20010;&#22270;&#26102;&#65292;&#23427;&#20250;&#21464;&#20026;&#35889;&#23884;&#20837;&#21644;&#32858;&#31867;&#65307;&#24403;&#24212;&#29992;&#20110;&#20998;&#31867;&#25968;&#25454;&#26102;&#65292;&#23427;&#23558;&#21464;&#20026;&#22343;&#19968;&#24615;&#20998;&#26512;&#12290;&#23454;&#39564;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32858;&#31867;&#20998;&#26512;&#65292;&#39564;&#35777;&#20102;SpectralMix&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering aims at discovering a natural grouping of the nodes such that similar nodes are assigned to a common cluster. Many different algorithms have been proposed in the literature: for simple graphs, for graphs with attributes associated to nodes, and for graphs where edges represent different types of relations among nodes. However, complex data in many domains can be represented as both attributed and multi-relational networks.  In this paper, we propose SpectralMix, a joint dimensionality reduction technique for multi-relational graphs with categorical node attributes. SpectralMix integrates all information available from the attributes, the different types of relations, and the graph structure to enable a sound interpretation of the clustering results. Moreover, it generalizes existing techniques: it reduces to spectral embedding and clustering when only applied to a single graph and to homogeneity analysis when applied to categorical data. Experiments conducted on severa
&lt;/p&gt;</description></item><item><title>Mix-ME&#26159;&#19968;&#31181;&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26469;&#33258;&#19981;&#21516;&#22242;&#38431;&#30340;&#26234;&#33021;&#20307;&#24418;&#25104;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#36866;&#24212;&#19981;&#21516;&#29615;&#22659;&#21644;&#38656;&#27714;&#25552;&#20379;&#20102;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01829</link><description>&lt;p&gt;
Mix-ME: &#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mix-ME: Quality-Diversity for Multi-Agent Learning. (arXiv:2311.01829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01829
&lt;/p&gt;
&lt;p&gt;
Mix-ME&#26159;&#19968;&#31181;&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26469;&#33258;&#19981;&#21516;&#22242;&#38431;&#30340;&#26234;&#33021;&#20307;&#24418;&#25104;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#36866;&#24212;&#19981;&#21516;&#29615;&#22659;&#21644;&#38656;&#27714;&#25552;&#20379;&#20102;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#20013;&#65292;&#22914;&#33258;&#36866;&#24212;&#26426;&#22120;&#20154;&#65292;&#20165;&#23454;&#29616;&#21333;&#19968;&#30340;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#26159;&#19981;&#22815;&#30340;&#12290;&#30456;&#21453;&#65292;&#36890;&#24120;&#38656;&#35201;&#19968;&#32452;&#20855;&#26377;&#22810;&#26679;&#24615;&#19988;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#29615;&#22659;&#21644;&#38656;&#27714;&#12290;&#36825;&#26159;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#21457;&#29616;&#19968;&#32452;&#20855;&#26377;&#29420;&#29305;&#29305;&#24449;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;QD&#26041;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#23398;&#65292;&#23588;&#20854;&#26159;&#22312;&#21457;&#29616;&#36866;&#24212;&#24615;&#25439;&#20260;&#30340;&#34892;&#21160;&#25511;&#21046;&#22120;&#26041;&#38754;&#65292;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#26159;&#22810;&#26234;&#33021;&#20307;&#30340;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mix-ME&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#26032;&#22411;MAP-Elites&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#26469;&#33258;&#19981;&#21516;&#22242;&#38431;&#30340;&#26234;&#33021;&#20307;&#28151;&#21512;&#22312;&#19968;&#36215;&#24418;&#25104;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22810;&#26234;&#33021;&#20307;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world systems, such as adaptive robotics, achieving a single, optimised solution may be insufficient. Instead, a diverse set of high-performing solutions is often required to adapt to varying contexts and requirements. This is the realm of Quality-Diversity (QD), which aims to discover a collection of high-performing solutions, each with their own unique characteristics. QD methods have recently seen success in many domains, including robotics, where they have been used to discover damage-adaptive locomotion controllers. However, most existing work has focused on single-agent settings, despite many tasks of interest being multi-agent. To this end, we introduce Mix-ME, a novel multi-agent variant of the popular MAP-Elites algorithm that forms new solutions using a crossover-like operator by mixing together agents from different teams. We evaluate the proposed methods on a variety of partially observable continuous control tasks. Our evaluation shows that these multi-agent v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#36739;&#22823;&#35268;&#27169;&#20248;&#21270;&#38382;&#39064;&#30340;&#24555;&#36895;&#36895;&#20889;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#20984;&#25110;&#38750;&#20984;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;&#30456;&#27604;&#24050;&#26377;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22788;&#29702;&#36890;&#29992;&#30340;Frechet&#23376;&#24494;&#20998;&#27491;&#21017;&#21270;&#20989;&#25968;&#24182;&#25552;&#20379;&#20102;&#19968;&#33324;&#30340;&#36817;&#20284;&#35823;&#24046;&#29702;&#35770;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#35299;&#20915;&#36895;&#20889;&#30340;&#31232;&#30095;&#20984;&#25110;&#38750;&#20984;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#36824;&#24471;&#21040;&#20102;&#31232;&#30095;&#20449;&#21495;&#20272;&#35745;&#30340;&#26497;&#23567;&#26497;&#22823;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.01806</link><description>&lt;p&gt;
&#21033;&#29992;&#23646;&#24615;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#30340;&#36895;&#20889;&#31639;&#27861;&#21644;&#38160;&#21033;&#20445;&#35777;&#30340;&#20984;&#21644;&#38750;&#20984;&#27491;&#21017;&#21270;&#12290; &#65288;arXiv&#65306;2311.01806v1 [math.OC]&#65289;
&lt;/p&gt;
&lt;p&gt;
Sketching for Convex and Nonconvex Regularized Least Squares with Sharp Guarantees. (arXiv:2311.01806v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#36739;&#22823;&#35268;&#27169;&#20248;&#21270;&#38382;&#39064;&#30340;&#24555;&#36895;&#36895;&#20889;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#20984;&#25110;&#38750;&#20984;&#27491;&#21017;&#21270;&#20989;&#25968;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;&#30456;&#27604;&#24050;&#26377;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22788;&#29702;&#36890;&#29992;&#30340;Frechet&#23376;&#24494;&#20998;&#27491;&#21017;&#21270;&#20989;&#25968;&#24182;&#25552;&#20379;&#20102;&#19968;&#33324;&#30340;&#36817;&#20284;&#35823;&#24046;&#29702;&#35770;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#35299;&#20915;&#36895;&#20889;&#30340;&#31232;&#30095;&#20984;&#25110;&#38750;&#20984;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#36824;&#24471;&#21040;&#20102;&#31232;&#30095;&#20449;&#21495;&#20272;&#35745;&#30340;&#26497;&#23567;&#26497;&#22823;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#31639;&#27861;&#23545;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#20248;&#21270;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36895;&#20889;&#31639;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20984;&#25110;&#38750;&#20984;&#27491;&#21017;&#21270;&#20989;&#25968;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#65292;&#21363;&#36895;&#20889;&#27491;&#21017;&#21270;&#20248;&#21270;&#65288;SRO&#65289;&#12290;&#25105;&#20204;&#30340;SRO&#31639;&#27861;&#39318;&#20808;&#29983;&#25104;&#21407;&#22987;&#25968;&#25454;&#30697;&#38453;&#30340;&#36895;&#20889;&#65292;&#28982;&#21518;&#35299;&#20915;&#36895;&#20889;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#30340;&#38543;&#26426;&#31639;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#22788;&#29702;&#36890;&#29992;&#30340;Frechet&#23376;&#24494;&#20998;&#27491;&#21017;&#21270;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;&#20984;&#25110;&#38750;&#20984;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#30340;&#21407;&#22987;&#38382;&#39064;&#30340;&#20248;&#21270;&#32467;&#26524;&#21644;&#36895;&#20889;&#38382;&#39064;&#20043;&#38388;&#30340;&#36817;&#20284;&#35823;&#24046;&#25552;&#20379;&#20102;&#19968;&#33324;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#23545;&#20110;&#20219;&#24847;&#30340;&#20984;&#27491;&#21017;&#21270;&#22120;&#65292;&#35777;&#26126;&#20102;&#30456;&#23545;&#35823;&#24046;&#30028;&#38480;&#30340;&#36817;&#20284;&#35823;&#24046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#19968;&#33324;&#26497;&#23567;&#26497;&#22823;&#36895;&#20889;&#31232;&#30095;&#20984;&#25110;&#38750;&#20984;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#31232;&#30095;&#20449;&#21495;&#20272;&#35745;&#30340;&#26497;&#23567;&#21270;&#36895;&#29575;&#20063;&#24471;&#21040;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized algorithms are important for solving large-scale optimization problems. In this paper, we propose a fast sketching algorithm for least square problems regularized by convex or nonconvex regularization functions, Sketching for Regularized Optimization (SRO). Our SRO algorithm first generates a sketch of the original data matrix, then solves the sketched problem. Different from existing randomized algorithms, our algorithm handles general Frechet subdifferentiable regularization functions in an unified framework. We present general theoretical result for the approximation error between the optimization results of the original problem and the sketched problem for regularized least square problems which can be convex or nonconvex. For arbitrary convex regularizer, relative-error bound is proved for the approximation error. Importantly, minimax rates for sparse signal estimation by solving the sketched sparse convex or nonconvex learning problems are also obtained using our gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35780;&#20998;&#27861;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#29702;&#35770;&#20272;&#35745;&#65292;&#24182;&#22312;&#20572;&#27490;&#35757;&#32451;&#26102;&#21487;&#20197;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#12290;&#36827;&#19968;&#27493;&#23558;&#23450;&#37327;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#26223;&#12290;</title><link>http://arxiv.org/abs/2311.01797</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Properties of Diffusion Models. (arXiv:2311.01797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35780;&#20998;&#27861;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#29702;&#35770;&#20272;&#35745;&#65292;&#24182;&#22312;&#20572;&#27490;&#35757;&#32451;&#26102;&#21487;&#20197;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#12290;&#36827;&#19968;&#27493;&#23558;&#23450;&#37327;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#24314;&#31435;&#19968;&#20010;&#38543;&#26426;&#20256;&#36755;&#26144;&#23556;&#65292;&#23558;&#32463;&#39564;&#35266;&#27979;&#21040;&#30340;&#20294;&#26410;&#30693;&#30340;&#30446;&#26631;&#20998;&#24067;&#19982;&#24050;&#30693;&#30340;&#20808;&#39564;&#20998;&#24067;&#32852;&#31995;&#36215;&#26469;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#26410;&#20805;&#20998;&#21457;&#23637;&#12290;&#26412;&#25991;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#23646;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;&#35780;&#20998;&#27861;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#29702;&#35770;&#20272;&#35745;&#65292;&#34920;&#26126;&#22312;&#26679;&#26412;&#22823;&#23567;$n$&#21644;&#27169;&#22411;&#23481;&#37327;$m$&#19978;&#37117;&#23384;&#22312;&#22810;&#39033;&#24335;&#23567;&#30340;&#27867;&#21270;&#35823;&#24046;($O(n^{-2/5}+m^{-4/5})$)&#65292;&#22312;&#20572;&#27490;&#35757;&#32451;&#26102;&#21487;&#20197;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#65288;&#21363;&#25968;&#25454;&#32500;&#24230;&#19981;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23450;&#37327;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#19968;&#20010;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#26223;&#65292;&#20854;&#20013;&#30446;&#26631;&#20998;&#24067;&#34987;&#25551;&#32472;&#20026;&#19968;&#31995;&#21015;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$ and the model capacity $m$, evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when early-stopped. Furthermore, we extend our quantitative analysis to a data-dependent scenario, wherein target distributions are portrayed as a succession of densities with progr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#20998;&#24067;&#30340;&#26041;&#24335;&#26469;&#21306;&#20998;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#26410;&#30693;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#26500;&#36896;&#21253;&#21547;&#22312;&#36741;&#21161;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#21608;&#22260;&#30340;Wasserstein&#29699;&#20013;&#25152;&#26377;&#20998;&#24067;&#30340;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#38598;&#65292;&#26469;&#20943;&#23567;&#20998;&#24067;&#24046;&#24322;&#65292;&#20174;&#32780;&#25552;&#21319;&#24320;&#25918;&#19990;&#30028;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01796</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#20998;&#24067;&#29992;&#20110;&#21306;&#20998;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#26410;&#30693;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning to Augment Distributions for Out-of-Distribution Detection. (arXiv:2311.01796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#20998;&#24067;&#30340;&#26041;&#24335;&#26469;&#21306;&#20998;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#26410;&#30693;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#26500;&#36896;&#21253;&#21547;&#22312;&#36741;&#21161;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#21608;&#22260;&#30340;Wasserstein&#29699;&#20013;&#25152;&#26377;&#20998;&#24067;&#30340;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#38598;&#65292;&#26469;&#20943;&#23567;&#20998;&#24067;&#24046;&#24322;&#65292;&#20174;&#32780;&#25552;&#21319;&#24320;&#25918;&#19990;&#30028;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;&#31995;&#32479;&#24212;&#35813;&#21306;&#20998;&#37027;&#20123;&#26631;&#31614;&#19982;&#20869;&#37096;&#20998;&#24067;&#24773;&#20917;&#19981;&#19968;&#33268;&#30340;&#26410;&#30693;&#26679;&#26412;&#65292;&#36825;&#20419;&#20351;&#20102;&#23545;&#26410;&#30693;&#26679;&#26412;&#26816;&#27979;&#30340;&#30740;&#31350;&#12290;&#23613;&#31649;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#36825;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#23545;&#26410;&#30693;&#26679;&#26412;&#32570;&#20047;&#39044;&#20808;&#20102;&#35299;&#65292;&#23427;&#20204;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#20173;&#21487;&#33021;&#22833;&#36133;&#12290;&#34429;&#28982;&#21487;&#20197;&#35775;&#38382;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#36741;&#21161;&#26410;&#30693;&#26679;&#26412;&#65288;&#19982;&#26410;&#30693;&#26679;&#26412;&#19981;&#21516;&#65289;&#65292;&#20294;&#20173;&#38656;&#20998;&#26512;&#36825;&#20123;&#36741;&#21161;&#25968;&#25454;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#25928;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#23398;&#20064;&#29702;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#21457;&#29616;&#36741;&#21161;&#26410;&#30693;&#26679;&#26412;&#21644;&#30495;&#23454;&#26410;&#30693;&#26679;&#26412;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#26159;&#24433;&#21709;&#24320;&#25918;&#19990;&#30028;&#26816;&#27979;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20998;&#24067;&#22686;&#24378;&#30340;&#26410;&#30693;&#26679;&#26412;&#23398;&#20064;(DAL)&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#22312;&#36741;&#21161;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#21608;&#22260;&#30340;Wasserstein&#29699;&#20013;&#30340;&#25152;&#26377;&#20998;&#24067;&#30340;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#38598;&#26469;&#20943;&#23567;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#24046;&#24322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;DAL&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#20855;&#26377;&#36739;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-world classification systems should discern out-of-distribution (OOD) data whose labels deviate from those of in-distribution (ID) cases, motivating recent studies in OOD detection. Advanced works, despite their promising progress, may still fail in the open world, owing to the lack of knowledge about unseen OOD data in advance. Although one can access auxiliary OOD data (distinct from unseen ones) for model training, it remains to analyze how such auxiliary data will work in the open world. To this end, we delve into such a problem from a learning theory perspective, finding that the distribution discrepancy between the auxiliary and the unseen real OOD data is the key to affecting the open-world detection performance. Accordingly, we propose Distributional-Augmented OOD Learning (DAL), alleviating the OOD distribution discrepancy by crafting an OOD distribution set that contains all distributions in a Wasserstein ball centered on the auxiliary OOD distribution. We justify that t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#33016;&#37096;X&#23556;&#32447;&#29255;&#20013;&#20998;&#21106;&#32954;&#37096;&#24322;&#24120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20108;&#20803;&#23450;&#20301;U-net&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30142;&#30149;&#21644;&#26410;&#35265;&#36807;&#30340;&#30142;&#30149;&#20013;&#37117;&#20855;&#26377;&#36739;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01777</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#33016;&#37096;X&#23556;&#32447;&#29255;&#20013;&#20998;&#21106;&#32954;&#37096;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
CheX-Nomaly: Segmenting Lung Abnormalities from Chest Radiographs using Machine Learning. (arXiv:2311.01777v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#33016;&#37096;X&#23556;&#32447;&#29255;&#20013;&#20998;&#21106;&#32954;&#37096;&#24322;&#24120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20108;&#20803;&#23450;&#20301;U-net&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30142;&#30149;&#21644;&#26410;&#35265;&#36807;&#30340;&#30142;&#30149;&#20013;&#37117;&#20855;&#26377;&#36739;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#24322;&#24120;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#32463;&#24120;&#34987;&#38169;&#35823;&#35786;&#26029;&#65292;&#20027;&#35201;&#19982;&#24863;&#30693;&#38169;&#35823;&#30456;&#20851;&#65292;&#21307;&#30103;&#25552;&#20379;&#32773;&#38590;&#20197;&#20934;&#30830;&#35782;&#21035;&#24322;&#24120;&#30340;&#20301;&#32622;&#65292;&#32780;&#19981;&#26159;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#30446;&#21069;&#36890;&#36807;&#30142;&#30149;&#29305;&#23450;&#30340;&#20998;&#21106;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#33021;&#22312;&#29616;&#22330;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#33016;&#33108;&#30142;&#30149;&#20013;&#30340;&#25512;&#24191;&#24615;&#36739;&#24046;&#12290;&#24403;&#36935;&#21040;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#20195;&#34920;&#30340;&#30142;&#30149;&#26102;&#65292;&#20108;&#20803;&#27169;&#22411;&#30340;&#24615;&#33021;&#24448;&#24448;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CheX-nomaly:&#19968;&#31181;&#20108;&#20803;&#23450;&#20301;U-net&#27169;&#22411;&#65292;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#21644;&#21019;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#21253;&#21547;14&#31181;&#19981;&#21516;&#30142;&#30149;&#20197;&#21450;&#8220;&#26080;&#21457;&#29616;&#8221;&#30149;&#20363;&#30340;VinDr-CXR&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#30340;&#27169;&#22411;&#22312;&#36825;14&#31181;&#30142;&#30149;&#20197;&#21450;&#20854;&#20182;&#26410;&#35265;&#36807;&#30340;&#30142;&#30149;&#20013;&#23454;&#29616;&#20102;&#25512;&#24191;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
The global challenge in chest radiograph X-ray (CXR) abnormalities often being misdiagnosed is primarily associated with perceptual errors, where healthcare providers struggle to accurately identify the location of abnormalities, rather than misclassification errors. We currently address this problem through disease-specific segmentation models. Unfortunately, these models cannot be released in the field due to their lack of generalizability across all thoracic diseases. A binary model tends to perform poorly when it encounters a disease that isn't represented in the dataset. We present CheX-nomaly: a binary localization U-net model that leverages transfer learning techniques with the incorporation of an innovative contrastive learning approach. Trained on the VinDr-CXR dataset, which encompasses 14 distinct diseases in addition to 'no finding' cases, my model achieves generalizability across these 14 diseases and others it has not seen before. We show that we can significantly improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;G-LowTESTR&#31639;&#27861;&#26469;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2311.01771</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Generalized Low-Rank Tensor Contextual Bandits. (arXiv:2311.01771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;G-LowTESTR&#31639;&#27861;&#26469;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#19968;&#31181;&#26032;&#39062;&#30340;&#36172;&#21338;&#31639;&#27861;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#22810;&#32500;&#25968;&#25454;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#22266;&#26377;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#25552;&#20379;&#39640;&#21487;&#29992;&#21644;&#36127;&#36131;&#20219;&#30340;&#20915;&#31574;&#26381;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#21160;&#20316;&#30001;&#19977;&#20010;&#29305;&#24449;&#21521;&#37327;&#32452;&#25104;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#24352;&#37327;&#34920;&#31034;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#22870;&#21169;&#26159;&#36890;&#36807;&#23558;&#21160;&#20316;&#30340;&#29305;&#24449;&#24352;&#37327;&#19982;&#19968;&#20010;&#22266;&#23450;&#20294;&#26410;&#30693;&#30340;&#21442;&#25968;&#24352;&#37327;&#30340;&#20869;&#31215;&#24212;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#20989;&#25968;&#26469;&#30830;&#23450;&#30340;&#65292;&#32780;&#36825;&#20010;&#21442;&#25968;&#24352;&#37327;&#20855;&#26377;&#36739;&#20302;&#30340;&#31649;&#29366;&#31209;&#12290;&#20026;&#20102;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#25506;&#32034;&#23376;&#31354;&#38388;&#28982;&#21518;&#32454;&#21270;&#8221;&#30340;&#26032;&#31639;&#27861;&#65288;G-LowTESTR&#65289;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#65292;&#20197;&#25506;&#32034;&#23884;&#20837;&#22312;&#20915;&#31574;&#24773;&#22659;&#20013;&#30340;&#26412;&#36136;&#20302;&#31209;&#24352;&#37327;&#23376;&#31354;&#38388;&#20449;&#24687;&#65292;&#28982;&#21518;&#23558;&#21407;&#22987;&#27010;&#29575;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to build a novel bandits algorithm that is capable of fully harnessing the power of multi-dimensional data and the inherent non-linearity of reward functions to provide high-usable and accountable decision-making services. To this end, we introduce a generalized low-rank tensor contextual bandits model in which an action is formed from three feature vectors, and thus can be represented by a tensor. In this formulation, the reward is determined through a generalized linear function applied to the inner product of the action's feature tensor and a fixed but unknown parameter tensor with a low tubal rank. To effectively achieve the trade-off between exploration and exploitation, we introduce a novel algorithm called "Generalized Low-Rank Tensor Exploration Subspace then Refine" (G-LowTESTR). This algorithm first collects raw data to explore the intrinsic low-rank tensor subspace information embedded in the decision-making scenario, and then converts the original prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#38750;&#24120;&#25968;&#26680;&#30340;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#24102;&#23485;&#65292;&#36991;&#20813;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#23485;&#26356;&#26032;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20351;&#29992;&#24120;&#25968;&#24102;&#23485;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01762</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#38750;&#24120;&#25968;&#26680;&#30340;&#26680;&#23725;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Solving Kernel Ridge Regression with Gradient Descent for a Non-Constant Kernel. (arXiv:2311.01762v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#38750;&#24120;&#25968;&#26680;&#30340;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#24102;&#23485;&#65292;&#36991;&#20813;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#23485;&#26356;&#26032;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20351;&#29992;&#24120;&#25968;&#24102;&#23485;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#26159;&#32447;&#24615;&#23725;&#22238;&#24402;&#30340;&#25512;&#24191;&#65292;&#23427;&#22312;&#25968;&#25454;&#20013;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#20294;&#22312;&#21442;&#25968;&#20013;&#26159;&#32447;&#24615;&#30340;&#12290;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36890;&#36807;&#38381;&#24335;&#35299;&#33719;&#24471;&#65292;&#20854;&#20013;&#21253;&#25324;&#30697;&#38453;&#27714;&#36870;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#33719;&#24471;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25913;&#21464;&#26680;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#36825;&#23545;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24179;&#31227;&#19981;&#21464;&#26680;&#30340;&#24102;&#23485;&#26356;&#26032;&#26041;&#26696;&#65292;&#20854;&#20013;&#24102;&#23485;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#33267;&#38646;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#24102;&#23485;&#30340;&#20248;&#20110;&#20351;&#29992;&#24120;&#25968;&#24102;&#23485;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#21644;&#36793;&#32536;&#20284;&#28982;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#24102;&#23485;&#12290;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#20351;&#29992;&#36880;&#28176;&#20943;&#23567;&#30340;&#24102;&#23485;&#26102;&#65292;&#25105;&#20204;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Kernel ridge regression, KRR, is a generalization of linear ridge regression that is non-linear in the data, but linear in the parameters. The solution can be obtained either as a closed-form solution, which includes a matrix inversion, or iteratively through gradient descent. Using the iterative approach opens up for changing the kernel during training, something that is investigated in this paper. We theoretically address the effects this has on model complexity and generalization. Based on our findings, we propose an update scheme for the bandwidth of translational-invariant kernels, where we let the bandwidth decrease to zero during training, thus circumventing the need for hyper-parameter selection. We demonstrate on real and synthetic data how decreasing the bandwidth during training outperforms using a constant bandwidth, selected by cross-validation and marginal likelihood maximization. We also show theoretically and empirically that using a decreasing bandwidth, we are able to
&lt;/p&gt;</description></item><item><title>TinyFormer&#26159;&#19968;&#20010;&#20855;&#26377;SuperNAS&#12289;SparseNAS&#21644;SparseEngine&#32452;&#25104;&#30340;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;&#22312;MCUs&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;&#36164;&#28304;&#39640;&#25928;&#30340;transformer&#27169;&#22411;&#12290;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25552;&#20986;&#20102;SparseEngine&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;MCUs&#19978;&#25191;&#34892;&#31232;&#30095;&#27169;&#22411;&#30340;transformer&#25512;&#29702;&#30340;&#37096;&#32626;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2311.01759</link><description>&lt;p&gt;
TinyFormer: &#39640;&#25928;&#30340;Transformer&#35774;&#35745;&#21644;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
TinyFormer: Efficient Transformer Design and Deployment on Tiny Devices. (arXiv:2311.01759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01759
&lt;/p&gt;
&lt;p&gt;
TinyFormer&#26159;&#19968;&#20010;&#20855;&#26377;SuperNAS&#12289;SparseNAS&#21644;SparseEngine&#32452;&#25104;&#30340;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;&#22312;MCUs&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;&#36164;&#28304;&#39640;&#25928;&#30340;transformer&#27169;&#22411;&#12290;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25552;&#20986;&#20102;SparseEngine&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;MCUs&#19978;&#25191;&#34892;&#31232;&#30095;&#27169;&#22411;&#30340;transformer&#25512;&#29702;&#30340;&#37096;&#32626;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#23884;&#20837;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#65292;&#20197;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803;&#65288;MCUs&#65289;&#20026;&#20195;&#34920;&#30340;&#23567;&#22411;&#35774;&#22791;&#19978;&#24320;&#21457;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20005;&#37325;&#30340;&#30828;&#20214;&#36164;&#28304;&#38480;&#21046;&#65292;&#22914;&#20309;&#39640;&#25928;&#22320;&#35774;&#35745;&#21644;&#37096;&#32626;&#26368;&#26032;&#30340;&#20808;&#36827;&#27169;&#22411;&#65288;&#22914;transformer&#65289;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TinyFormer&#65292;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#22312;MCUs&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;&#36164;&#28304;&#39640;&#25928;&#30340;transformer&#30340;&#26694;&#26550;&#12290;TinyFormer&#20027;&#35201;&#30001;SuperNAS&#12289;SparseNAS&#21644;SparseEngine&#32452;&#25104;&#12290;&#20854;&#20013;&#65292;SuperNAS&#26088;&#22312;&#20174;&#24191;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#23547;&#25214;&#36866;&#24403;&#30340;&#36229;&#32593;&#32476;&#12290;SparseNAS&#35780;&#20272;&#26368;&#20339;&#30340;&#31232;&#30095;&#21333;&#36335;&#24452;&#27169;&#22411;&#65292;&#21253;&#25324;&#20174;&#24050;&#35782;&#21035;&#30340;&#36229;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;transformer&#26550;&#26500;&#12290;&#26368;&#21518;&#65292;SparseEngine&#23558;&#25628;&#32034;&#21040;&#30340;&#31232;&#30095;&#27169;&#22411;&#39640;&#25928;&#22320;&#37096;&#32626;&#21040;MCUs&#19978;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SparseEngine&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;MCUs&#19978;&#25191;&#34892;&#31232;&#30095;&#27169;&#22411;&#30340;transformer&#25512;&#29702;&#30340;&#37096;&#32626;&#26694;&#26550;&#12290;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;TinyFormer&#22312;&#20445;&#25345;&#25512;&#29702;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;transformer&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#22823;&#32422;78&#65285;&#30340;&#25512;&#29702;&#35745;&#31639;&#37327;&#21644;53&#65285;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing deep learning models on tiny devices (e.g. Microcontroller units, MCUs) has attracted much attention in various embedded IoT applications. However, it is challenging to efficiently design and deploy recent advanced models (e.g. transformers) on tiny devices due to their severe hardware resource constraints. In this work, we propose TinyFormer, a framework specifically designed to develop and deploy resource-efficient transformers on MCUs. TinyFormer mainly consists of SuperNAS, SparseNAS and SparseEngine. Separately, SuperNAS aims to search for an appropriate supernet from a vast search space. SparseNAS evaluates the best sparse single-path model including transformer architecture from the identified supernet. Finally, SparseEngine efficiently deploys the searched sparse models onto MCUs. To the best of our knowledge, SparseEngine is the first deployment framework capable of performing inference of sparse models with transformer on MCUs. Evaluation results on the CIFAR-10 da
&lt;/p&gt;</description></item><item><title>RiskQ&#26159;&#19968;&#31181;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#39118;&#38505;&#25935;&#24863;&#21327;&#35843;&#35201;&#27714;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#25935;&#24863;&#30340;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;RIGM&#65289;&#21407;&#21017;&#21644;&#24314;&#27169;&#32852;&#21512;&#22238;&#25253;&#20998;&#24067;&#23454;&#29616;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#12290;</title><link>http://arxiv.org/abs/2311.01753</link><description>&lt;p&gt;
RiskQ: &#39118;&#38505;&#25935;&#24863;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization. (arXiv:2311.01753v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01753
&lt;/p&gt;
&lt;p&gt;
RiskQ&#26159;&#19968;&#31181;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#39118;&#38505;&#25935;&#24863;&#21327;&#35843;&#35201;&#27714;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#25935;&#24863;&#30340;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;RIGM&#65289;&#21407;&#21017;&#21644;&#24314;&#27169;&#32852;&#21512;&#22238;&#25253;&#20998;&#24067;&#23454;&#29616;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#29305;&#28857;&#26159;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#12289;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#39118;&#38505;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23398;&#20064;&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#21327;&#35843;&#21644;&#20998;&#25955;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#22312;&#39118;&#38505;&#25935;&#24863;&#30340;MARL&#20013;&#21046;&#23450;&#21327;&#35843;&#35201;&#27714;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39118;&#38505;&#25935;&#24863;&#30340;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;RIGM&#65289;&#21407;&#29702;&#65292;&#20316;&#20026;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;IGM&#65289;&#21644;&#20998;&#24067;&#24335;IGM&#65288;DIGM&#65289;&#21407;&#29702;&#30340;&#19968;&#31181;&#25512;&#24191;&#12290;&#35813;&#21407;&#29702;&#35201;&#27714;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#39118;&#38505;&#25935;&#24863;&#21160;&#20316;&#36873;&#25321;&#38598;&#21512;&#24212;&#19982;&#20013;&#22830;&#31574;&#30053;&#30340;&#39118;&#38505;&#25935;&#24863;&#21160;&#20316;&#36873;&#25321;&#31561;&#20215;&#12290;&#24403;&#21069;&#30340;MARL&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#26041;&#27861;&#23545;&#20110;&#24120;&#35265;&#30340;&#39118;&#38505;&#24230;&#37327;&#65288;&#20363;&#22914;&#39118;&#38505;&#20215;&#20540;&#65288;VaR&#65289;&#24230;&#37327;&#25110;&#25197;&#26354;&#30340;&#39118;&#38505;&#24230;&#37327;&#65289;&#19981;&#28385;&#36275;RIGM&#21407;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RiskQ&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36890;&#36807;&#24314;&#27169;&#32852;&#21512;&#22238;&#25253;&#20998;&#24067;&#26469;&#23454;&#29616;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeli
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#27969;&#34892;&#30149;&#20915;&#31574;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#25919;&#24220;&#22312;&#20860;&#39038;&#20844;&#20849;&#23433;&#20840;&#21644;&#32463;&#27982;&#21457;&#23637;&#26041;&#38754;&#20570;&#20986;&#20915;&#31574;&#65292;&#24182;&#22788;&#29702;&#27969;&#34892;&#30149;&#25968;&#25454;&#26679;&#26412;&#26377;&#38480;&#21644;&#38544;&#31169;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01749</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#27969;&#34892;&#30149;&#20915;&#31574;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Epidemic Decision-making System Based Federated Reinforcement Learning. (arXiv:2311.01749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01749
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#27969;&#34892;&#30149;&#20915;&#31574;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#25919;&#24220;&#22312;&#20860;&#39038;&#20844;&#20849;&#23433;&#20840;&#21644;&#32463;&#27982;&#21457;&#23637;&#26041;&#38754;&#20570;&#20986;&#20915;&#31574;&#65292;&#24182;&#22788;&#29702;&#27969;&#34892;&#30149;&#25968;&#25454;&#26679;&#26412;&#26377;&#38480;&#21644;&#38544;&#31169;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30149;&#20915;&#31574;&#21487;&#20197;&#26377;&#25928;&#24110;&#21161;&#25919;&#24220;&#32508;&#21512;&#32771;&#34385;&#20844;&#20849;&#23433;&#20840;&#21644;&#32463;&#27982;&#21457;&#23637;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#21644;&#23433;&#20840;&#32039;&#24613;&#24773;&#20917;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#24110;&#21161;&#25919;&#24220;&#20570;&#20986;&#27969;&#34892;&#30149;&#20915;&#31574;&#65292;&#20174;&#32780;&#23454;&#29616;&#21355;&#29983;&#23433;&#20840;&#21644;&#32463;&#27982;&#21457;&#23637;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#27969;&#34892;&#30149;&#25968;&#25454;&#24448;&#24448;&#20855;&#26377;&#26679;&#26412;&#26377;&#38480;&#21644;&#38544;&#31169;&#24615;&#39640;&#30340;&#29305;&#28857;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#21508;&#30465;&#20221;&#30340;&#30123;&#24773;&#25968;&#25454;&#36827;&#34892;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epidemic decision-making can effectively help the government to comprehensively consider public security and economic development to respond to public health and safety emergencies. Epidemic decision-making can effectively help the government to comprehensively consider public security and economic development to respond to public health and safety emergencies. Some studies have shown that intensive learning can effectively help the government to make epidemic decision, thus achieving the balance between health security and economic development. Some studies have shown that intensive learning can effectively help the government to make epidemic decision, thus achieving the balance between health security and economic development. However, epidemic data often has the characteristics of limited samples and high privacy. However, epidemic data often has the characteristics of limited samples and high privacy. This model can combine the epidemic situation data of various provinces for coop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22320;&#19979;LoRaWAN&#32593;&#32476;&#33021;&#25928;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#20998;&#37197;&#25193;&#39057;&#22240;&#23376;&#65292;&#20197;&#26368;&#23567;&#21270;&#20849;&#20139;&#25193;&#39057;&#22240;&#23376;&#24178;&#25200;&#24182;&#20248;&#21270;&#31995;&#32479;&#30340;&#33021;&#25928;&#12290;</title><link>http://arxiv.org/abs/2311.01743</link><description>&lt;p&gt;
&#22320;&#19979;LoRaWAN&#32593;&#32476;&#33021;&#25928;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65306;&#30452;&#25509;&#21355;&#26143;&#36830;&#25509;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Energy Efficiency Optimization for Subterranean LoRaWAN Using A Reinforcement Learning Approach: A Direct-to-Satellite Scenario. (arXiv:2311.01743v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22320;&#19979;LoRaWAN&#32593;&#32476;&#33021;&#25928;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#20998;&#37197;&#25193;&#39057;&#22240;&#23376;&#65292;&#20197;&#26368;&#23567;&#21270;&#20849;&#20139;&#25193;&#39057;&#22240;&#23376;&#24178;&#25200;&#24182;&#20248;&#21270;&#31995;&#32479;&#30340;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20559;&#36828;&#20892;&#19994;&#21644;&#28798;&#23475;&#25937;&#25588;&#34892;&#21160;&#20013;&#65292;&#22320;&#19979;LoRaWAN&#19982;&#38750;&#22320;&#29699;&#32593;&#32476;&#65288;NTN&#65289;&#30340;&#38598;&#25104;&#20026;&#32463;&#27982;&#21644;&#31038;&#20250;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#12290;LoRa&#35843;&#21046;&#21033;&#29992;&#20934;&#27491;&#20132;&#25193;&#39057;&#22240;&#23376;&#65288;SFs&#65289;&#26469;&#20248;&#21270;&#25968;&#25454;&#36895;&#29575;&#12289;&#31354;&#38386;&#26102;&#38388;&#12289;&#35206;&#30422;&#33539;&#22260;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#22320;&#19979;LoRaWAN NTN&#20013;&#65292;&#26377;&#25928;&#22320;&#20026;&#32456;&#31471;&#35774;&#22791;&#20998;&#37197;SFs&#20197;&#26368;&#23567;&#21270;&#20849;&#20139;&#25193;&#39057;&#22240;&#23376;&#24178;&#25200;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;SFs&#20998;&#37197;&#26041;&#26696;&#26469;&#20248;&#21270;&#31995;&#32479;&#30340;&#33021;&#25928;&#12290;&#20026;&#20102;&#26377;&#25928;&#25429;&#25417;&#23494;&#38598;&#32593;&#32476;&#20013;&#35774;&#22791;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#20998;&#26512;&#22870;&#21169;&#26426;&#21046;&#30340;&#22810;&#26234;&#33021;&#20307;dueling double deep Q-network&#65288;MAD3QN&#65289;&#21644;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;MAA2C&#65289;&#31639;&#27861;&#30340;SFs&#20998;&#37197;&#25216;&#26415;&#12290;&#19982;&#22235;&#20010;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;SFs&#20998;&#37197;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of subterranean LoRaWAN and non-terrestrial networks (NTN) delivers substantial economic and societal benefits in remote agriculture and disaster rescue operations. The LoRa modulation leverages quasi-orthogonal spreading factors (SFs) to optimize data rates, airtime, coverage and energy consumption. However, it is still challenging to effectively assign SFs to end devices for minimizing co-SF interference in massive subterranean LoRaWAN NTN. To address this, we investigate a reinforcement learning (RL)-based SFs allocation scheme to optimize the system's energy efficiency (EE). To efficiently capture the device-to-environment interactions in dense networks, we proposed an SFs allocation technique using the multi-agent dueling double deep Q-network (MAD3QN) and the multi-agent advantage actor-critic (MAA2C) algorithms based on an analytical reward mechanism. Our proposed RL-based SFs allocation approach evinces better performance compared to four benchmarks in the extre
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20840;&#29699;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#36229;&#24179;&#38754;&#30340;&#20915;&#31574;&#26641;&#26469;&#36817;&#20284;&#38750;&#32447;&#24615;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#20854;&#20182;MIO&#21487;&#34920;&#31034;&#30340;ML&#27169;&#22411;&#26469;&#25193;&#23637;&#21407;&#22987;&#38382;&#39064;&#30340;&#36817;&#20284;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#37319;&#26679;&#31243;&#24207;&#21644;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#26469;&#25552;&#39640;&#32422;&#26463;&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01742</link><description>&lt;p&gt;
&#20840;&#29699;&#20248;&#21270;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global Optimization: A Machine Learning Approach. (arXiv:2311.01742v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01742
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20840;&#29699;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#36229;&#24179;&#38754;&#30340;&#20915;&#31574;&#26641;&#26469;&#36817;&#20284;&#38750;&#32447;&#24615;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#20854;&#20182;MIO&#21487;&#34920;&#31034;&#30340;ML&#27169;&#22411;&#26469;&#25193;&#23637;&#21407;&#22987;&#38382;&#39064;&#30340;&#36817;&#20284;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#37319;&#26679;&#31243;&#24207;&#21644;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#26469;&#25552;&#39640;&#32422;&#26463;&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20840;&#29699;&#20248;&#21270;&#38382;&#39064;&#30340;&#35768;&#22810;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#29305;&#23450;&#25968;&#23398;&#21407;&#35821;&#30340;&#38750;&#32447;&#24615;&#32422;&#26463;&#30340;&#25918;&#26494;&#12290;&#36825;&#22312;&#24212;&#29992;&#20013;&#23545;&#20110;&#40657;&#30418;&#12289;&#38544;&#24335;&#25110;&#26356;&#19968;&#33324;&#30340;&#32422;&#26463;&#26159;&#26377;&#38480;&#21046;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#38480;&#21046;&#65292;Bertsimas&#21644;Ozturk&#65288;2023&#65289;&#25552;&#20986;&#20102;OCTHaGOn&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#36229;&#24179;&#38754;&#30340;&#20915;&#31574;&#26641;&#36817;&#20284;&#38750;&#32447;&#24615;&#32422;&#26463;&#26469;&#35299;&#20915;&#40657;&#30418;&#20840;&#29699;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#26641;&#26500;&#24314;&#21407;&#22987;&#38382;&#39064;&#30340;&#32479;&#19968;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#65288;MIO&#65289;&#36817;&#20284;&#12290;&#25105;&#20204;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#36890;&#36807;&#65288;i&#65289;&#20351;&#29992;&#38500;&#20915;&#31574;&#26641;&#20043;&#22806;&#30340;&#20854;&#20182;MIO&#21487;&#34920;&#31034;&#30340;ML&#27169;&#22411;&#65288;&#22914;&#26799;&#24230;&#25552;&#21319;&#26641;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#26469;&#36817;&#20284;&#21407;&#22987;&#38382;&#39064;&#65292;&#65288;ii&#65289;&#25552;&#20986;&#36866;&#24212;&#24615;&#37319;&#26679;&#31243;&#24207;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32422;&#26463;&#36817;&#20284;&#65292;&#65288;iii&#65289;&#21033;&#29992;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Many approaches for addressing Global Optimization problems typically rely on relaxations of nonlinear constraints over specific mathematical primitives. This is restricting in applications with constraints that are black-box, implicit or consist of more general primitives. Trying to address such limitations, Bertsimas and Ozturk (2023) proposed OCTHaGOn as a way of solving black-box global optimization problems by approximating the nonlinear constraints using hyperplane-based Decision-Trees and then using those trees to construct a unified mixed integer optimization (MIO) approximation of the original problem. We provide extensions to this approach, by (i) approximating the original problem using other MIO-representable ML models besides Decision Trees, such as Gradient Boosted Trees, Multi Layer Perceptrons and Suport Vector Machines, (ii) proposing adaptive sampling procedures for more accurate machine learning-based constraint approximations, (iii) utilizing robust optimization to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;CDGraph&#29992;&#20110;&#21512;&#25104;&#31038;&#20132;&#22270;&#65292;&#36890;&#36807;&#25429;&#25417;&#21452;&#26465;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#21644;&#20445;&#25345;&#33410;&#28857;&#36830;&#36890;&#24615;&#26469;&#23454;&#29616;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31038;&#20132;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2311.01729</link><description>&lt;p&gt;
CDGraph&#65306;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21452;&#26465;&#20214;&#31038;&#20132;&#22270;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
CDGraph: Dual Conditional Social Graph Synthesizing via Diffusion Model. (arXiv:2311.01729v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;CDGraph&#29992;&#20110;&#21512;&#25104;&#31038;&#20132;&#22270;&#65292;&#36890;&#36807;&#25429;&#25417;&#21452;&#26465;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#21644;&#20445;&#25345;&#33410;&#28857;&#36830;&#36890;&#24615;&#26469;&#23454;&#29616;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31038;&#20132;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#30340;&#31038;&#20132;&#22270;&#22240;&#25968;&#25454;&#31232;&#32570;&#21644;&#29992;&#25143;&#38544;&#31169;&#30340;&#25285;&#24551;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#38656;&#27714;&#12290;&#29983;&#25104;&#31038;&#20132;&#32593;&#32476;&#30340;&#20851;&#38190;&#24615;&#33021;&#26631;&#20934;&#20043;&#19968;&#26159;&#23545;&#29305;&#23450;&#26465;&#20214;&#30340;&#24544;&#23454;&#24230;&#65292;&#20363;&#22914;&#20855;&#26377;&#29305;&#23450;&#20250;&#21592;&#36164;&#26684;&#21644;&#36130;&#21153;&#29366;&#20917;&#30340;&#29992;&#25143;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#26465;&#20214;&#31038;&#20132;&#22270;&#30340;&#21512;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#32593;&#32476;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;CDGraph&#65292;&#23427;&#22522;&#20110;&#20004;&#20010;&#25351;&#23450;&#26465;&#20214;&#26469;&#35757;&#32451;&#21644;&#21512;&#25104;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CDGraph&#30340;&#21435;&#22122;&#36807;&#31243;&#20013;&#30340;&#20849;&#21516;&#28436;&#21270;&#20381;&#36182;&#24615;&#65292;&#20197;&#25429;&#25417;&#21452;&#26465;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36827;&#19968;&#27493;&#32467;&#21512;&#31038;&#20250;&#21516;&#36136;&#24615;&#21644;&#31038;&#20250;&#20256;&#26579;&#21147;&#20197;&#20445;&#25345;&#33410;&#28857;&#20043;&#38388;&#30340;&#36830;&#25509;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#25351;&#23450;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
The social graphs synthesized by the generative models are increasingly in demand due to data scarcity and concerns over user privacy. One of the key performance criteria for generating social networks is the fidelity to specified conditionals, such as users with certain membership and financial status. While recent diffusion models have shown remarkable performance in generating images, their effectiveness in synthesizing graphs has not yet been explored in the context of conditional social graphs. In this paper, we propose the first kind of conditional diffusion model for social networks, CDGraph, which trains and synthesizes graphs based on two specified conditions. We propose the co-evolution dependency in the denoising process of CDGraph to capture the mutual dependencies between the dual conditions and further incorporate social homophily and social contagion to preserve the connectivity between nodes while satisfying the specified conditions. Moreover, we introduce a novel class
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#24378;&#21270;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#28789;&#27963;&#22320;&#32531;&#35299;&#37327;&#23376;&#36807;&#31243;&#20013;&#30340;&#21508;&#31181;&#22122;&#22768;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#37327;&#23376;&#36807;&#31243;&#20013;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01727</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#24378;&#21270;&#30340;&#31070;&#32463;&#27169;&#22411;&#23545;&#37327;&#23376;&#36807;&#31243;&#36827;&#34892;&#28789;&#27963;&#30340;&#35823;&#24046;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Flexible Error Mitigation of Quantum Processes with Data Augmentation Empowered Neural Model. (arXiv:2311.01727v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01727
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#24378;&#21270;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#28789;&#27963;&#22320;&#32531;&#35299;&#37327;&#23376;&#36807;&#31243;&#20013;&#30340;&#21508;&#31181;&#22122;&#22768;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#37327;&#23376;&#36807;&#31243;&#20013;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#23376;&#35745;&#31639;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#37327;&#23376;&#35823;&#24046;&#32531;&#35299;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#23545;&#26080;&#22122;&#22768;&#32479;&#35745;&#30340;&#20381;&#36182;&#38480;&#21046;&#65292;&#36825;&#26159;&#23454;&#29616;&#23454;&#38469;&#37327;&#23376;&#36827;&#23637;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#24378;&#21270;&#30340;&#31070;&#32463;&#27169;&#22411;&#29992;&#20110;&#35823;&#24046;&#32531;&#35299;&#65288;DAEM&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#29305;&#23450;&#22122;&#22768;&#31867;&#22411;&#21644;&#27979;&#37327;&#35774;&#32622;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#20165;&#26681;&#25454;&#30446;&#26631;&#37327;&#23376;&#36807;&#31243;&#30340;&#22122;&#22768;&#27979;&#37327;&#32467;&#26524;&#20272;&#35745;&#26080;&#22122;&#22768;&#32479;&#35745;&#20540;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#23454;&#38469;&#23454;&#26045;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#32531;&#35299;&#21508;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#65288;&#21253;&#25324;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#65289;&#26041;&#38754;&#19982;&#20808;&#21069;&#30340;&#35823;&#24046;&#32531;&#35299;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#35813;&#27169;&#22411;&#26469;&#32531;&#35299;&#22810;&#31181;&#31867;&#22411;&#30340;&#37327;&#23376;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#26469;&#23637;&#31034;&#20854;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have shown their effectiveness in various tasks in the realm of quantum computing. However, their application in quantum error mitigation, a crucial step towards realizing practical quantum advancements, has been restricted by reliance on noise-free statistics. To tackle this critical challenge, we propose a data augmentation empowered neural model for error mitigation (DAEM). Our model does not require any prior knowledge about the specific noise type and measurement settings and can estimate noise-free statistics solely from the noisy measurement results of the target quantum process, rendering it highly suitable for practical implementation. In numerical experiments, we show the model's superior performance in mitigating various types of noise, including Markovian noise and Non-Markovian noise, compared with previous error mitigation methods. We further demonstrate its versatility by employing the model to mitigate errors in diverse types of quantum processes, includ
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;FAIR&#26041;&#27861;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#31354;&#38388;&#19981;&#21516;&#30340;&#35774;&#22791;&#19978;&#38598;&#20307;&#35757;&#32451;&#23884;&#20837;&#34920;&#65292;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2311.01722</link><description>&lt;p&gt;
&#20351;&#29992;FAIR&#30340;&#24322;&#26500;&#32852;&#37030;&#21327;&#21516;&#36807;&#28388;&#65306;&#22312;&#38543;&#26426;&#23376;&#31354;&#38388;&#20013;&#30340;&#32852;&#37030;&#24179;&#22343; (arXiv:2311.01722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Heterogeneous federated collaborative filtering using FAIR: Federated Averaging in Random Subspaces. (arXiv:2311.01722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01722
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;FAIR&#26041;&#27861;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#31354;&#38388;&#19981;&#21516;&#30340;&#35774;&#22791;&#19978;&#38598;&#20307;&#35757;&#32451;&#23884;&#20837;&#34920;&#65292;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#20114;&#32852;&#32593;&#24179;&#21488;&#19978;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#30340;&#21916;&#22909;&#20010;&#24615;&#21270;&#25512;&#33616;&#20869;&#23481;&#12290;&#20256;&#32479;&#30340;&#25512;&#33616;&#27169;&#22411;&#26159;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#20851;&#27880;&#21644;GDPR&#31561;&#27861;&#35268;&#30340;&#20986;&#21488;&#65292;&#32852;&#37030;&#23398;&#20064;&#25104;&#20026;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#33539;&#24335;&#65292;&#20854;&#20013;&#25968;&#25454;&#27704;&#36828;&#19981;&#31163;&#24320;&#23458;&#25143;&#31471;&#35774;&#22791;&#12290;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20110;&#25512;&#33616;&#27169;&#22411;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23884;&#20837;&#34920;&#24448;&#24448;&#36229;&#20986;&#20102;&#22823;&#22810;&#25968;&#29992;&#25143;&#35774;&#22791;&#30340;&#20869;&#23384;&#38480;&#21046;&#12290;&#20026;&#20102;&#23558;&#25152;&#26377;&#35774;&#22791;&#30340;&#25968;&#25454;&#32435;&#20837;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#24517;&#39035;&#23454;&#29616;&#22312;&#20869;&#23384;&#33021;&#21147;&#19981;&#21516;&#30340;&#35774;&#22791;&#19978;&#23545;&#23884;&#20837;&#34920;&#36827;&#34892;&#38598;&#20307;&#35757;&#32451;&#12290;&#24403;&#21069;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21482;&#33021;&#23481;&#32435;&#19968;&#23567;&#37096;&#20998;&#33021;&#21147;&#33539;&#22260;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#33021;&#21442;&#19982;&#35757;&#32451;&#30340;&#35774;&#22791;&#25968;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#38543;&#26426;&#23376;&#31354;&#38388;&#20013;&#30340;&#32852;&#37030;&#24179;&#22343;&#65288;FAIR&#65289;&#65292;&#23427;&#20801;&#35768;&#23545;&#23884;&#20837;&#34920;&#36827;&#34892;&#20219;&#24847;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems (RS) for items (e.g., movies, books) and ads are widely used to tailor content to users on various internet platforms. Traditionally, recommendation models are trained on a central server. However, due to rising concerns for data privacy and regulations like the GDPR, federated learning is an increasingly popular paradigm in which data never leaves the client device. Applying federated learning to recommendation models is non-trivial due to large embedding tables, which often exceed the memory constraints of most user devices. To include data from all devices in federated learning, we must enable collective training of embedding tables on devices with heterogeneous memory capacities. Current solutions to heterogeneous federated learning can only accommodate a small range of capacities and thus limit the number of devices that can participate in training. We present Federated Averaging in Random subspaces (FAIR), which allows arbitrary compression of embedding tab
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#29983;&#25104;&#22120;-&#32534;&#30721;&#22120;&#23545;&#25239;&#32593;&#32476;&#21644;&#28508;&#22312;&#31354;&#38388;&#21305;&#37197;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#27491;&#21521;&#12289;&#21453;&#21521;&#21644;&#28151;&#21512;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#38388;&#25509;&#21305;&#37197;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#65292;&#26377;&#25928;&#36991;&#20813;&#39640;&#32500;&#36755;&#20837;&#21644;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#26356;&#31934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#20943;&#36731;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01708</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#29983;&#25104;&#22120;-&#32534;&#30721;&#22120;&#23545;&#25239;&#32593;&#32476;&#21644;&#28508;&#22312;&#31354;&#38388;&#21305;&#37197;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Generator-Encoder Adversarial Networks with Latent Space Matching for Stochastic Differential Equations. (arXiv:2311.01708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01708
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#29983;&#25104;&#22120;-&#32534;&#30721;&#22120;&#23545;&#25239;&#32593;&#32476;&#21644;&#28508;&#22312;&#31354;&#38388;&#21305;&#37197;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#27491;&#21521;&#12289;&#21453;&#21521;&#21644;&#28151;&#21512;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#38388;&#25509;&#21305;&#37197;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#65292;&#26377;&#25928;&#36991;&#20813;&#39640;&#32500;&#36755;&#20837;&#21644;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#26356;&#31934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#20943;&#36731;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#29983;&#25104;&#22120;-&#32534;&#30721;&#22120;&#23545;&#25239;&#32593;&#32476;&#65292;&#26469;&#26377;&#25928;&#35299;&#20915;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#27491;&#21521;&#12289;&#21453;&#21521;&#21644;&#28151;&#21512;&#38382;&#39064;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#34429;&#28982;&#32479;&#27835;&#26041;&#31243;&#24050;&#30693;&#65292;&#20294;&#21487;&#29992;&#30340;&#25968;&#25454;&#20165;&#30001;&#31995;&#32479;&#21442;&#25968;&#30340;&#26377;&#38480;&#24555;&#29031;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#29983;&#25104;&#22120;&#21644;&#32534;&#30721;&#22120;&#65292;&#20004;&#32773;&#37117;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#20132;&#26367;&#26356;&#26032;&#12290;&#19982;&#30452;&#25509;&#21305;&#37197;&#36817;&#20284;&#35299;&#19982;&#30495;&#23454;&#24555;&#29031;&#30340;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20302;&#32500;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20869;&#30340;&#38388;&#25509;&#21305;&#37197;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#39640;&#32500;&#36755;&#20837;&#21644;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#30456;&#27604;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;&#20135;&#29983;&#20102;&#26356;&#31934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#20943;&#36731;&#20102;&#20808;&#21069;&#26041;&#27861;&#20013;&#36935;&#21040;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new class of physics-informed neural networks, called Physics-Informed Generator-Encoder Adversarial Networks, to effectively address the challenges posed by forward, inverse, and mixed problems in stochastic differential equations. In these scenarios, while the governing equations are known, the available data consist of only a limited set of snapshots for system parameters. Our model consists of two key components: the generator and the encoder, both updated alternately by gradient descent. In contrast to previous approaches of directly matching the approximated solutions with real snapshots, we employ an indirect matching that operates within the lower-dimensional latent feature space. This method circumvents challenges associated with high-dimensional inputs and complex data distributions, while yielding more accurate solutions compared to existing neural network solvers. In addition, the approach also mitigates the training instability issues encountered in previous a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#36172;&#24466;&#38382;&#39064;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#21516;&#36136;&#21644;&#24322;&#36136;&#35774;&#32622;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#30446;&#26631;&#26159;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21516;&#36136;&#35774;&#32622;&#20013;&#65292;&#36890;&#36807;&#25915;&#20987;&#21482;&#19968;&#20010;&#26234;&#33021;&#20307;&#65292;&#21363;&#21487;&#20351;&#25152;&#26377;&#26234;&#33021;&#20307;&#36873;&#25321;&#29305;&#23450;&#30446;&#26631;&#33218;&#65292;&#32780;&#22312;&#24322;&#36136;&#35774;&#32622;&#20013;&#65292;&#25915;&#20987;&#30446;&#26631;&#33218;&#38656;&#35201;&#32447;&#24615;&#25915;&#20987;&#25104;&#26412;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#25915;&#20987;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#26368;&#22823;&#25968;&#37327;&#30340;&#26234;&#33021;&#20307;&#25215;&#21463;&#32447;&#24615;&#36951;&#25022;&#21516;&#26102;&#25215;&#25285;&#20122;&#32447;&#24615;&#25104;&#26412;&#65292;&#24182;&#21482;&#25805;&#32437;&#20102;&#23569;&#25968;&#30446;&#26631;&#26234;&#33021;&#20307;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01698</link><description>&lt;p&gt;
&#23545;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#36172;&#24466;&#38382;&#39064;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Cooperative Multi-agent Bandits. (arXiv:2311.01698v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01698
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#36172;&#24466;&#38382;&#39064;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#21516;&#36136;&#21644;&#24322;&#36136;&#35774;&#32622;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#30446;&#26631;&#26159;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21516;&#36136;&#35774;&#32622;&#20013;&#65292;&#36890;&#36807;&#25915;&#20987;&#21482;&#19968;&#20010;&#26234;&#33021;&#20307;&#65292;&#21363;&#21487;&#20351;&#25152;&#26377;&#26234;&#33021;&#20307;&#36873;&#25321;&#29305;&#23450;&#30446;&#26631;&#33218;&#65292;&#32780;&#22312;&#24322;&#36136;&#35774;&#32622;&#20013;&#65292;&#25915;&#20987;&#30446;&#26631;&#33218;&#38656;&#35201;&#32447;&#24615;&#25915;&#20987;&#25104;&#26412;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#25915;&#20987;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#26368;&#22823;&#25968;&#37327;&#30340;&#26234;&#33021;&#20307;&#25215;&#21463;&#32447;&#24615;&#36951;&#25022;&#21516;&#26102;&#25215;&#25285;&#20122;&#32447;&#24615;&#25104;&#26412;&#65292;&#24182;&#21482;&#25805;&#32437;&#20102;&#23569;&#25968;&#30446;&#26631;&#26234;&#33021;&#20307;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#24466;&#38382;&#39064;&#65288;CMA2B&#65289;&#32771;&#34385;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#20849;&#20139;&#30340;&#22810;&#33218;&#36172;&#24466;&#28216;&#25103;&#20013;&#30340;&#21327;&#21516;&#21162;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#21327;&#20316;&#25152;&#26292;&#38706;&#20986;&#30340;&#28508;&#22312;&#28431;&#27934;&#65292;&#24182;&#32771;&#34385;&#23545;&#19968;&#37096;&#20998;&#26234;&#33021;&#20307;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#65292;&#30446;&#30340;&#26159;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;CMA2B&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#21253;&#25324;&#21516;&#36136;&#35774;&#32622;&#65288;&#26234;&#33021;&#20307;&#20855;&#26377;&#30456;&#21516;&#30340;&#33218;&#38598;&#65289;&#21644;&#24322;&#36136;&#35774;&#32622;&#65288;&#26234;&#33021;&#20307;&#20855;&#26377;&#19981;&#21516;&#30340;&#33218;&#38598;&#65289;&#12290;&#22312;&#21516;&#36136;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25915;&#20987;&#31574;&#30053;&#65292;&#36890;&#36807;&#21482;&#38024;&#23545;&#19968;&#20010;&#26234;&#33021;&#20307;&#65292;&#20351;&#24471;&#25152;&#26377;&#26234;&#33021;&#20307;&#22312;T&#36718;&#20013;&#36873;&#25321;&#19968;&#20010;&#29305;&#23450;&#30340;&#30446;&#26631;&#33218;T-o(T)&#27425;&#65292;&#21516;&#26102;&#25215;&#25285;o(T)&#30340;&#25915;&#20987;&#25104;&#26412;&#12290;&#22312;&#24322;&#36136;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#30446;&#26631;&#33218;&#30340;&#25915;&#20987;&#38656;&#35201;&#32447;&#24615;&#30340;&#25915;&#20987;&#25104;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#25915;&#20987;&#31574;&#30053;&#65292;&#21487;&#20197;&#36843;&#20351;&#26368;&#22823;&#25968;&#37327;&#30340;&#26234;&#33021;&#20307;&#25215;&#21463;&#32447;&#24615;&#30340;&#36951;&#25022;&#65292;&#21516;&#26102;&#25215;&#25285;&#20122;&#32447;&#24615;&#30340;&#25104;&#26412;&#65292;&#24182;&#19988;&#21482;&#25805;&#32437;&#20102;&#23569;&#25968;&#30446;&#26631;&#26234;&#33021;&#20307;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative multi-agent multi-armed bandits (CMA2B) consider the collaborative efforts of multiple agents in a shared multi-armed bandit game. We study latent vulnerabilities exposed by this collaboration and consider adversarial attacks on a few agents with the goal of influencing the decisions of the rest. More specifically, we study adversarial attacks on CMA2B in both homogeneous settings, where agents operate with the same arm set, and heterogeneous settings, where agents have distinct arm sets. In the homogeneous setting, we propose attack strategies that, by targeting just one agent, convince all agents to select a particular target arm $T-o(T)$ times while incurring $o(T)$ attack costs in $T$ rounds. In the heterogeneous setting, we prove that a target arm attack requires linear attack costs and propose attack strategies that can force a maximum number of agents to suffer linear regrets while incurring sublinear costs and only manipulating the observations of a few target agent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GO-UCB&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#36890;&#29992;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#30340;&#32852;&#37030;&#36172;&#21338;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2311.01695</link><description>&lt;p&gt;
&#39640;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#38750;&#32447;&#24615;&#36172;&#21338;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Non-Linear Bandit Optimization. (arXiv:2311.01695v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GO-UCB&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#36890;&#29992;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#30340;&#32852;&#37030;&#36172;&#21338;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#20248;&#21270;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#19979;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#25110;&#32452;&#32455;&#65289;&#20043;&#38388;&#30340;&#21327;&#21516;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#30001;&#20110;&#25968;&#25454;&#30001;&#27599;&#20010;&#23458;&#25143;&#31471;&#21333;&#29420;&#25910;&#38598;&#24182;&#22987;&#32456;&#20445;&#25345;&#20998;&#25955;&#65292;&#32852;&#37030;&#20248;&#21270;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#24182;&#20801;&#35768;&#22823;&#35268;&#27169;&#35745;&#31639;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#23613;&#31649;&#23427;&#36890;&#24120;&#29992;&#20110;&#22312;&#32447;&#20219;&#21153;&#65292;&#20363;&#22914;&#38190;&#30424;&#24212;&#29992;&#31243;&#24207;&#19978;&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#65292;&#20294;&#22823;&#22810;&#25968;&#24037;&#20316;&#23558;&#20854;&#23450;&#20041;&#20026;&#31163;&#32447;&#38382;&#39064;&#12290;&#24456;&#23569;&#25968;&#20363;&#22806;&#32771;&#34385;&#32852;&#37030;&#36172;&#21338;&#20248;&#21270;&#65292;&#20294;&#23427;&#20204;&#23616;&#38480;&#20110;&#38750;&#24120;&#31616;&#21333;&#30340;&#20989;&#25968;&#31867;&#21035;&#65292;&#20363;&#22914;&#32447;&#24615;&#12289;&#24191;&#20041;&#32447;&#24615;&#25110;&#24102;&#26377;&#26377;&#30028;RKHS&#33539;&#25968;&#30340;&#38750;&#21442;&#25968;&#20989;&#25968;&#31867;&#21035;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#30340;&#23454;&#38469;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21517;&#20026;Fed-GO-UCB&#65292;&#29992;&#20110;&#20855;&#26377;&#36890;&#29992;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#30340;&#32852;&#37030;&#36172;&#21338;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated optimization studies the problem of collaborative function optimization among multiple clients (e.g. mobile devices or organizations) under the coordination of a central server. Since the data is collected separately by each client and always remains decentralized, federated optimization preserves data privacy and allows for large-scale computing, which makes it a promising decentralized machine learning paradigm. Though it is often deployed for tasks that are online in nature, e.g., next-word prediction on keyboard apps, most works formulate it as an offline problem. The few exceptions that consider federated bandit optimization are limited to very simplistic function classes, e.g., linear, generalized linear, or non-parametric function class with bounded RKHS norm, which severely hinders its practical usage. In this paper, we propose a new algorithm, named Fed-GO-UCB, for federated bandit optimization with generic non-linear objective function. Under some mild conditions, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20256;&#36755;&#20449;&#24687;&#29942;&#39048;&#26469;&#23454;&#29616;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21387;&#32553;&#34920;&#31034;&#20449;&#24687;&#21644;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#20043;&#38388;&#32500;&#25345;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#21487;&#35745;&#31639;&#20272;&#35745;&#30340;DisTIB&#12290;</title><link>http://arxiv.org/abs/2311.01686</link><description>&lt;p&gt;
&#20351;&#29992;&#20256;&#36755;&#30340;&#20449;&#24687;&#29942;&#39048;&#23454;&#29616;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning with Transmitted Information Bottleneck. (arXiv:2311.01686v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20256;&#36755;&#20449;&#24687;&#29942;&#39048;&#26469;&#23454;&#29616;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21387;&#32553;&#34920;&#31034;&#20449;&#24687;&#21644;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#20043;&#38388;&#32500;&#25345;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#21487;&#35745;&#31639;&#20272;&#35745;&#30340;DisTIB&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#32534;&#30721;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#21407;&#22987;&#25968;&#25454;&#20449;&#24687;&#65292;&#21363;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#34429;&#28982;&#22312;&#34920;&#31034;&#20013;&#21033;&#29992;&#20449;&#24687;&#29702;&#35770;&#23545;&#20449;&#24687;&#36827;&#34892;&#35268;&#33539;&#21270;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#34920;&#31034;&#21387;&#32553;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65307;2&#65289;&#23545;&#34920;&#31034;&#30340;&#35299;&#32544;&#32422;&#26463;&#23384;&#22312;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20256;&#36755;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#25551;&#36848;&#35299;&#32544;&#36807;&#31243;&#20013;&#36755;&#20837;&#21644;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"DisTIB"&#65288;&#29992;&#20110;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#20256;&#36755;&#20449;&#24687;&#29942;&#39048;&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#20449;&#24687;&#21387;&#32553;&#21644;&#20445;&#30041;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#23548;&#20986;DisTIB&#30340;&#21487;&#35745;&#31639;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoding only the task-related information from the raw data, \ie, disentangled representation learning, can greatly contribute to the robustness and generalizability of models. Although significant advances have been made by regularizing the information in representations with information theory, two major challenges remain: 1) the representation compression inevitably leads to performance drop; 2) the disentanglement constraints on representations are in complicated optimization. To these issues, we introduce Bayesian networks with transmitted information to formulate the interaction among input and representations during disentanglement. Building upon this framework, we propose \textbf{DisTIB} (\textbf{T}ransmitted \textbf{I}nformation \textbf{B}ottleneck for \textbf{Dis}entangled representation learning), a novel objective that navigates the balance between information compression and preservation. We employ variational inference to derive a tractable estimation for DisTIB. This es
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#37096;&#20998;&#21512;&#25104;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#24179;&#21488;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#37232;&#33018;&#36136;&#23376;&#36716;&#31227;&#65288;APT&#65289;&#25928;&#24212;&#12290;&#36890;&#36807;&#23558;&#27169;&#25311;&#25968;&#25454;&#21644;&#23454;&#27979;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#37096;&#20998;&#21512;&#25104;&#30340;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#27169;&#25311;&#30340;&#28789;&#27963;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2311.01683</link><description>&lt;p&gt;
&#20351;&#29992;&#37096;&#20998;&#21512;&#25104;&#25968;&#25454;&#21450;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#32959;&#30244;&#37232;&#33018;&#36136;&#23376;&#36716;&#31227;&#65288;APT&#65289;&#25104;&#20687;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Amide Proton Transfer (APT) imaging in tumor with a machine learning approach using partially synthetic data. (arXiv:2311.01683v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#37096;&#20998;&#21512;&#25104;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#24179;&#21488;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#37232;&#33018;&#36136;&#23376;&#36716;&#31227;&#65288;APT&#65289;&#25928;&#24212;&#12290;&#36890;&#36807;&#23558;&#27169;&#25311;&#25968;&#25454;&#21644;&#23454;&#27979;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#37096;&#20998;&#21512;&#25104;&#30340;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#27169;&#25311;&#30340;&#28789;&#27963;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#37327;&#21270;&#21270;&#23398;&#20132;&#25442;&#39281;&#21644;&#36716;&#31227;&#65288;CEST&#65289;&#25928;&#24212;&#12290;ML&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#23454;&#27979;&#25968;&#25454;&#25110;&#23436;&#20840;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#23454;&#27979;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#24448;&#24448;&#32570;&#20047;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#20351;&#29992;&#23436;&#20840;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#30001;&#20110;&#26377;&#38480;&#30340;&#27169;&#25311;&#36164;&#28304;&#32780;&#24341;&#20837;&#20559;&#24046;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#24179;&#21488;&#65292;&#23558;&#27169;&#25311;&#25968;&#25454;&#21644;&#23454;&#27979;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#37096;&#20998;&#21512;&#25104;&#30340;CEST&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#20854;&#29992;&#20110;&#35757;&#32451;ML&#27169;&#22411;&#39044;&#27979;&#37232;&#33018;&#36136;&#23376;&#36716;&#31227;&#65288;APT&#65289;&#25928;&#24212;&#30340;&#21487;&#34892;&#24615;&#12290;&#37096;&#20998;&#21512;&#25104;&#30340;CEST&#20449;&#21495;&#20351;&#29992;&#20174;&#27169;&#25311;&#20013;&#30340;APT&#25928;&#24212;&#30340;&#36870;&#27714;&#21644;&#21644;&#26469;&#33258;&#23454;&#27979;&#25968;&#25454;&#30340;&#20854;&#20182;&#25104;&#20998;&#21019;&#24314;&#12290;&#36890;&#36807;&#25913;&#21464;APT&#27169;&#25311;&#21442;&#25968;&#24182;&#24212;&#29992;&#32553;&#25918;&#22240;&#23376;&#26469;&#35843;&#25972;&#23454;&#27979;&#25968;&#25454;&#30340;&#25104;&#20998;&#65292;&#29983;&#25104;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#27169;&#25311;&#30340;&#28789;&#27963;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has been increasingly used to quantify chemical exchange saturation transfer (CEST) effect. ML models are typically trained using either measured data or fully simulated data. However, training with measured data often lacks sufficient training data, while training with fully simulated data may introduce bias due to limited simulations pools. This study introduces a new platform that combines simulated and measured components to generate partially synthetic CEST data, and to evaluate its feasibility for training ML models to predict amide proton transfer (APT) effect. Partially synthetic CEST signals were created using an inverse summation of APT effects from simulations and the other components from measurements. Training data were generated by varying APT simulation parameters and applying scaling factors to adjust the measured components, achieving a balance between simulation flexibility and fidelity. First, tissue-mimicking CEST signals along with ground trut
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#37325;&#35201;&#25277;&#26679;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#38656;&#27714;&#65292;&#22914;&#28151;&#21512;&#20998;&#37197;&#21644;&#31665;&#23610;&#23544;&#65292;&#20943;&#36731;&#20102;&#20174;&#19994;&#20154;&#21592;&#30340;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2311.01660</link><description>&lt;p&gt;
&#24341;&#20837;&#37325;&#35201;&#25277;&#26679;&#30340;&#26580;&#24615;&#29983;&#23384;&#23494;&#24230;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Estimation of Flexible Survival Densities with Importance Sampling. (arXiv:2311.01660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#37325;&#35201;&#25277;&#26679;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#38656;&#27714;&#65292;&#22914;&#28151;&#21512;&#20998;&#37197;&#21644;&#31665;&#23610;&#23544;&#65292;&#20943;&#36731;&#20102;&#20174;&#19994;&#20154;&#21592;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#26512;&#20855;&#26377;&#25130;&#23614;&#30340;&#26102;&#38388;&#33267;&#20107;&#20214;&#25968;&#25454;&#30340;&#25216;&#26415;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#33021;&#22815;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#24182;&#25918;&#26494;&#20256;&#32479;&#20551;&#35774;&#65288;&#22914;&#27604;&#20363;&#39118;&#38505;&#65289;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#65288;&#22914;&#31163;&#25955;&#27169;&#22411;&#30340;&#31665;&#25968;&#21644;&#31665;&#23610;&#23544;&#65292;&#20197;&#21450;&#22522;&#20110;&#28151;&#21512;&#27169;&#22411;&#30340;&#31751;&#20998;&#37197;&#25968;&#65289;&#38656;&#35201;&#22823;&#37327;&#35843;&#25972;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#20197;&#19979;&#20107;&#23454;&#65306;&#65288;1&#65289;&#26368;&#20339;&#31665;&#23610;&#23544;&#21487;&#33021;&#20250;&#22240;&#25152;&#20851;&#27880;&#30340;&#25351;&#26631;&#65288;&#22914;&#19968;&#33268;&#24615;&#21644;&#24067;&#37324;&#23572;&#20998;&#25968;&#65289;&#32780;&#22823;&#20026;&#19981;&#21516;&#65292;&#20197;&#21450;&#65288;2&#65289;&#28151;&#21512;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#21463;&#27169;&#24335;&#22349;&#22604;&#21644;&#25968;&#20540;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#28151;&#21512;&#20998;&#37197;&#21644;&#31665;&#23610;&#23544;&#31561;&#36229;&#21442;&#25968;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#20174;&#19994;&#20154;&#21592;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is a widely-used technique for analyzing time-to-event data in the presence of censoring. In recent years, numerous survival analysis methods have emerged which scale to large datasets and relax traditional assumptions such as proportional hazards. These models, while being performant, are very sensitive to model hyperparameters including: (1) number of bins and bin size for discrete models and (2) number of cluster assignments for mixture-based models. Each of these choices requires extensive tuning by practitioners to achieve optimal performance. In addition, we demonstrate in empirical studies that: (1) optimal bin size may drastically differ based on the metric of interest (e.g., concordance vs brier score), and (2) mixture models may suffer from mode collapse and numerical instability. We propose a survival analysis approach which eliminates the need to tune hyperparameters such as mixture assignments and bin sizes, reducing the burden on practitioners. We show t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26816;&#27979;&#28508;&#22312;&#30340;&#20266;&#30456;&#20851;&#20851;&#31995;&#65292;&#24182;&#19988;&#30456;&#27604;&#20043;&#19979;&#38656;&#35201;&#26356;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2311.01655</link><description>&lt;p&gt;
&#36890;&#36807;&#40065;&#26834;&#35270;&#35273;&#27010;&#24565;&#22312;&#30495;&#23454;&#22270;&#20687;&#21644;AI&#29983;&#25104;&#22270;&#20687;&#20998;&#31867;&#20013;&#26816;&#27979;&#20266;&#30456;&#20851;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Detecting Spurious Correlations via Robust Visual Concepts in Real and AI-Generated Image Classification. (arXiv:2311.01655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01655
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26816;&#27979;&#28508;&#22312;&#30340;&#20266;&#30456;&#20851;&#20851;&#31995;&#65292;&#24182;&#19988;&#30456;&#27604;&#20043;&#19979;&#38656;&#35201;&#26356;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20542;&#21521;&#20110;&#33258;&#21160;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20851;&#32852;&#65292;&#32780;&#19981;&#20250;&#36136;&#30097;&#20854;&#26377;&#25928;&#24615;&#25110;&#36866;&#24403;&#24615;&#12290;&#36825;&#31181;&#19981;&#21487;&#21462;&#30340;&#29305;&#24615;&#26159;&#20266;&#30456;&#20851;&#20851;&#31995;&#30340;&#20307;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#19981;&#21487;&#38752;&#19988;&#23481;&#26131;&#22833;&#36133;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#35797;&#22270;&#32416;&#27491;&#20266;&#30456;&#20851;&#20851;&#31995;&#30340;&#26041;&#27861;&#20165;&#23545;&#27169;&#22411;&#24050;&#30693;&#30340;&#20266;&#20851;&#32852;&#26377;&#25928;&#12290;&#24403;&#21069;&#30340;&#20266;&#30456;&#20851;&#20851;&#31995;&#26816;&#27979;&#31639;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#65292;&#35201;&#20040;&#22312;&#20854;&#21046;&#23450;&#20013;&#22826;&#36807;&#38480;&#21046;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#23545;&#35270;&#35273;&#24037;&#20214;&#30340;&#20005;&#26684;&#23450;&#20041;&#65292;&#36825;&#22312;&#30001;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#20197;&#20135;&#29983;&#19981;&#31526;&#21512;&#26631;&#20934;&#35268;&#33539;&#30340;&#20869;&#23481;&#32780;&#38395;&#21517;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26816;&#27979;&#28508;&#22312;&#30340;&#20266;&#30456;&#20851;&#20851;&#31995;&#65292;&#24182;&#19988;&#30456;&#27604;&#20043;&#19979;&#38656;&#35201;&#26356;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
Often machine learning models tend to automatically learn associations present in the training data without questioning their validity or appropriateness. This undesirable property is the root cause of the manifestation of spurious correlations, which render models unreliable and prone to failure in the presence of distribution shifts. Research shows that most methods attempting to remedy spurious correlations are only effective for a model's known spurious associations. Current spurious correlation detection algorithms either rely on extensive human annotations or are too restrictive in their formulation. Moreover, they rely on strict definitions of visual artifacts that may not apply to data produced by generative models, as they are known to hallucinate contents that do not conform to standard specifications. In this work, we introduce a general-purpose method that efficiently detects potential spurious correlations, and requires significantly less human interference in comparison t
&lt;/p&gt;</description></item><item><title>MARRS&#26159;&#19968;&#20010;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#22810;&#27169;&#24577;&#21442;&#32771;&#35299;&#26512;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#23545;&#35805;&#24335;&#12289;&#35270;&#35273;&#21644;&#32972;&#26223;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#19978;&#19979;&#25991;&#26597;&#35810;&#30340;&#22788;&#29702;&#12290;&#36825;&#20010;&#31995;&#32479;&#33021;&#22815;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#29702;&#35299;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2311.01650</link><description>&lt;p&gt;
MARRS: &#22810;&#27169;&#24577;&#21442;&#32771;&#35299;&#26512;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MARRS: Multimodal Reference Resolution System. (arXiv:2311.01650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01650
&lt;/p&gt;
&lt;p&gt;
MARRS&#26159;&#19968;&#20010;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#22810;&#27169;&#24577;&#21442;&#32771;&#35299;&#26512;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#23545;&#35805;&#24335;&#12289;&#35270;&#35273;&#21644;&#32972;&#26223;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#19978;&#19979;&#25991;&#26597;&#35810;&#30340;&#22788;&#29702;&#12290;&#36825;&#20010;&#31995;&#32479;&#33021;&#22815;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#29702;&#35299;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#22788;&#29702;&#19978;&#19979;&#25991;&#23545;&#20110;&#20219;&#20309;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#20010;&#19978;&#19979;&#25991;&#21487;&#33021;&#26159;&#23545;&#35805;&#24335;&#30340;&#65288;&#20381;&#36182;&#20110;&#20043;&#21069;&#30340;&#29992;&#25143;&#26597;&#35810;&#25110;&#31995;&#32479;&#22238;&#31572;&#65289;&#65292;&#20063;&#21487;&#33021;&#26159;&#35270;&#35273;&#30340;&#65288;&#20381;&#36182;&#20110;&#29992;&#25143;&#30475;&#21040;&#30340;&#19996;&#35199;&#65292;&#20363;&#22914;&#20182;&#20204;&#30340;&#23631;&#24149;&#19978;&#65289;&#65292;&#25110;&#32773;&#26159;&#32972;&#26223;&#30340;&#65288;&#22522;&#20110;&#19968;&#20123;&#20449;&#21495;&#65292;&#27604;&#22914;&#21709;&#36215;&#30340;&#38393;&#38047;&#25110;&#32773;&#27491;&#22312;&#25773;&#25918;&#30340;&#38899;&#20048;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MARRS&#65288;&#22810;&#27169;&#24577;&#21442;&#32771;&#35299;&#26512;&#31995;&#32479;&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#22312;&#22788;&#29702;&#23545;&#35805;&#24335;&#12289;&#35270;&#35273;&#21644;&#32972;&#26223;&#19978;&#19979;&#25991;&#26041;&#38754;&#36127;&#36131;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23454;&#29616;&#19978;&#19979;&#25991;&#26597;&#35810;&#30340;&#22788;&#29702;&#65307;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#32771;&#35299;&#26512;&#65292;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#26597;&#35810;&#37325;&#20889;&#22788;&#29702;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#30456;&#20114;&#34917;&#20805;&#65292;&#24418;&#25104;&#19968;&#20010;&#32479;&#19968;&#12289;&#36830;&#36143;&#12289;&#36731;&#37327;&#32423;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#29702;&#35299;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successfully handling context is essential for any dialog understanding task. This context maybe be conversational (relying on previous user queries or system responses), visual (relying on what the user sees, for example, on their screen), or background (based on signals such as a ringing alarm or playing music). In this work, we present an overview of MARRS, or Multimodal Reference Resolution System, an on-device framework within a Natural Language Understanding system, responsible for handling conversational, visual and background context. In particular, we present different machine learning models to enable handing contextual queries; specifically, one to enable reference resolution, and one to handle context via query rewriting. We also describe how these models complement each other to form a unified, coherent, lightweight system that can understand context while preserving user privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#22810;&#20851;&#31995;&#21644;&#26102;&#38388;&#22270;&#19978;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;R$^2$-GNN&#27169;&#22411;&#22312;&#26576;&#20123;&#21463;&#38480;&#20294;&#21512;&#29702;&#30340;&#24773;&#20917;&#19979;&#31561;&#20215;&#20110;$\mathcal{FOC}_2$&#20998;&#31867;&#22120;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20811;&#26381;R$^2$-GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#36716;&#25442;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2311.01647</link><description>&lt;p&gt;
&#26657;&#20934;&#21644;&#22686;&#24378;GNN&#22312;&#22810;&#20851;&#31995;&#21644;&#26102;&#38388;&#22270;&#19978;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Calibrate and Boost Logical Expressiveness of GNN Over Multi-Relational and Temporal Graphs. (arXiv:2311.01647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#22810;&#20851;&#31995;&#21644;&#26102;&#38388;&#22270;&#19978;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;R$^2$-GNN&#27169;&#22411;&#22312;&#26576;&#20123;&#21463;&#38480;&#20294;&#21512;&#29702;&#30340;&#24773;&#20917;&#19979;&#31561;&#20215;&#20110;$\mathcal{FOC}_2$&#20998;&#31867;&#22120;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20811;&#26381;R$^2$-GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#36716;&#25442;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20851;&#20110;GNN&#20316;&#20026;&#22810;&#20851;&#31995;&#22270;&#20013;&#24067;&#23572;&#33410;&#28857;&#20998;&#31867;&#22120;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#23578;&#26410;&#36827;&#34892;&#27491;&#24335;&#20998;&#26512;&#65292;&#20854;&#20013;&#27599;&#26465;&#36793;&#37117;&#20855;&#26377;&#29305;&#23450;&#30340;&#20851;&#31995;&#31867;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;$\mathcal{FOC}_2$&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#21464;&#37327;&#21644;&#35745;&#25968;&#37327;&#21270;&#22120;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#12290;&#22312;&#28040;&#26497;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;&#20840;&#23616;&#35835;&#21462;&#24341;&#20837;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;GNN&#30340;R$^2$-GNN&#26550;&#26500;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26080;&#27861;&#25429;&#25417;$\mathcal{FOC}_2$&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#31215;&#26497;&#26041;&#38754;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;R$^2$-GNN&#27169;&#22411;&#22312;&#26576;&#20123;&#21463;&#38480;&#20294;&#21512;&#29702;&#30340;&#24773;&#20917;&#19979;&#31561;&#20215;&#20110;$\mathcal{FOC}_2$&#20998;&#31867;&#22120;&#12290;&#20026;&#20102;&#35299;&#20915;R$^2$-GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#36716;&#25442;&#25216;&#26415;&#65292;&#31867;&#20284;&#20110;...
&lt;/p&gt;
&lt;p&gt;
As a powerful framework for graph representation learning, Graph Neural Networks (GNNs) have garnered significant attention in recent years. However, to the best of our knowledge, there has been no formal analysis of the logical expressiveness of GNNs as Boolean node classifiers over multi-relational graphs, where each edge carries a specific relation type. In this paper, we investigate $\mathcal{FOC}_2$, a fragment of first-order logic with two variables and counting quantifiers. On the negative side, we demonstrate that the R$^2$-GNN architecture, which extends the local message passing GNN by incorporating global readout, fails to capture $\mathcal{FOC}_2$ classifiers in the general case. Nevertheless, on the positive side, we establish that R$^2$-GNNs models are equivalent to $\mathcal{FOC}_2$ classifiers under certain restricted yet reasonable scenarios. To address the limitations of R$^2$-GNNs regarding expressiveness, we propose a simple graph transformation technique, akin to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;SemiGPC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#21644;&#26631;&#31614;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#24067;&#24863;&#30693;&#26631;&#31614;&#31934;&#28860;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SemiGPC&#22312;&#19981;&#21516;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#31574;&#30053;&#20013;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#25968;&#25454;&#21306;&#38388;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2311.01646</link><description>&lt;p&gt;
SemiGPC&#65306;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#23454;&#29616;&#23545;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#24067;&#24863;&#30693;&#26631;&#31614;&#31934;&#28860;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes. (arXiv:2311.01646v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;SemiGPC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#21644;&#26631;&#31614;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#24067;&#24863;&#30693;&#26631;&#31614;&#31934;&#28860;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SemiGPC&#22312;&#19981;&#21516;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#31574;&#30053;&#20013;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#25968;&#25454;&#21306;&#38388;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SemiGPC&#65292;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20998;&#24067;&#24863;&#30693;&#26631;&#31614;&#31934;&#28860;&#31574;&#30053;&#65292;&#22312;&#35813;&#31574;&#30053;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26469;&#33258;&#20110;&#26631;&#31614;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#19982;&#20854;&#20182;&#22522;&#20110;&#32531;&#20914;&#21306;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65288;&#22914;CoMatch&#21644;SimMatch&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;SemiGPC&#21253;&#21547;&#19968;&#20010;&#24402;&#19968;&#21270;&#39033;&#65292;&#29992;&#20110;&#35299;&#20915;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#23616;&#37096;&#25935;&#24863;&#24615;&#12290;&#36825;&#31181;&#26174;&#24335;&#30340;&#25511;&#21046;&#20351;&#24471;SemiGPC&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30830;&#35748;&#20559;&#24046;&#30340;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SemiGPC&#19982;&#19981;&#21516;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65288;&#22914;FixMatch&#12289;ReMixMatch&#12289;SimMatch&#21644;FreeMatch&#65289;&#20197;&#21450;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65288;&#21253;&#25324;MSN&#21644;Dino&#65289;&#37197;&#23545;&#21518;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;SemiGPC&#22312;&#26631;&#20934;&#30340;CIFAR10-LT/CIFAR100-LT&#30340;&#19981;&#21516;&#31243;&#24230;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#25968;&#25454;&#21306;&#38388;&#12290;&#30456;&#27604;&#20110;&#19968;&#20010;&#26032;&#30340;&#31454;&#20105;&#26041;&#27861;&#65292;&#20351;&#29992;SemiGPC&#36824;&#21487;&#20197;&#25552;&#39640;&#32422;2%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce SemiGPC, a distribution-aware label refinement strategy based on Gaussian Processes where the predictions of the model are derived from the labels posterior distribution. Differently from other buffer-based semi-supervised methods such as CoMatch and SimMatch, our SemiGPC includes a normalization term that addresses imbalances in the global data distribution while maintaining local sensitivity. This explicit control allows SemiGPC to be more robust to confirmation bias especially under class imbalance. We show that SemiGPC improves performance when paired with different Semi-Supervised methods such as FixMatch, ReMixMatch, SimMatch and FreeMatch and different pre-training strategies including MSN and Dino. We also show that SemiGPC achieves state of the art results under different degrees of class imbalance on standard CIFAR10-LT/CIFAR100-LT especially in the low data-regime. Using SemiGPC also results in about 2% avg.accuracy increase compared to a new compe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#23398;&#29983;&#32593;&#32476;&#26159;&#21542;&#24212;&#35813;&#22797;&#21046;&#25945;&#24072;&#31070;&#32463;&#20803;&#25110;&#24179;&#22343;&#19968;&#32452;&#25945;&#24072;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#12290;&#30740;&#31350;&#21457;&#29616;&#23545;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#36755;&#20837;&#20998;&#24067;&#65292;&#24403;&#25945;&#24072;&#32593;&#32476;&#30340;&#36755;&#20837;&#21521;&#37327;&#27491;&#20132;&#19988;&#36755;&#20986;&#26435;&#37325;&#20026;&#37193;&#26102;&#65292;&#22797;&#21046;-&#24179;&#22343;&#37197;&#32622;&#23558;&#36798;&#21040;&#20248;&#21270;&#32467;&#26524;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#23398;&#29983;&#31070;&#32463;&#20803;&#22797;&#21046;&#19968;&#20010;&#25945;&#24072;&#31070;&#32463;&#20803;&#65292;&#26368;&#21518;&#19968;&#20010;&#23398;&#29983;&#31070;&#32463;&#20803;&#23545;&#25152;&#26377;&#25945;&#24072;&#31070;&#32463;&#20803;&#21462;&#24179;&#22343;&#20540;&#12290;</title><link>http://arxiv.org/abs/2311.01644</link><description>&lt;p&gt;
&#23398;&#29983;&#32593;&#32476;&#26159;&#21542;&#24212;&#35813;&#22797;&#21046;&#25110;&#24179;&#22343;&#25945;&#24072;&#26435;&#37325;&#65311;
&lt;/p&gt;
&lt;p&gt;
Should Under-parameterized Student Networks Copy or Average Teacher Weights?. (arXiv:2311.01644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27424;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#23398;&#29983;&#32593;&#32476;&#26159;&#21542;&#24212;&#35813;&#22797;&#21046;&#25945;&#24072;&#31070;&#32463;&#20803;&#25110;&#24179;&#22343;&#19968;&#32452;&#25945;&#24072;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#12290;&#30740;&#31350;&#21457;&#29616;&#23545;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#36755;&#20837;&#20998;&#24067;&#65292;&#24403;&#25945;&#24072;&#32593;&#32476;&#30340;&#36755;&#20837;&#21521;&#37327;&#27491;&#20132;&#19988;&#36755;&#20986;&#26435;&#37325;&#20026;&#37193;&#26102;&#65292;&#22797;&#21046;-&#24179;&#22343;&#37197;&#32622;&#23558;&#36798;&#21040;&#20248;&#21270;&#32467;&#26524;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#23398;&#29983;&#31070;&#32463;&#20803;&#22797;&#21046;&#19968;&#20010;&#25945;&#24072;&#31070;&#32463;&#20803;&#65292;&#26368;&#21518;&#19968;&#20010;&#23398;&#29983;&#31070;&#32463;&#20803;&#23545;&#25152;&#26377;&#25945;&#24072;&#31070;&#32463;&#20803;&#21462;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#36830;&#32493;&#20989;&#25968; $f^*$ &#37117;&#21487;&#20197;&#29992;&#36275;&#22815;&#22810;&#30340;&#31070;&#32463;&#20803; $k$&#26469;&#36817;&#20284;&#12290;&#25105;&#20204;&#32771;&#34385; $f^*$ &#26412;&#36523;&#26159;&#19968;&#20010;&#20855;&#26377;&#19968;&#20010;&#38544;&#34255;&#23618;&#21644; $k$ &#20010;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#12290;&#29992;&#20855;&#26377; $n&lt;k$ &#20010;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817; $f^*$ &#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#19968;&#20010;&#27424;&#21442;&#25968;&#21270;&#30340;&#8220;&#23398;&#29983;&#8221;&#32593;&#32476;&#19982; $k$ &#20010;&#31070;&#32463;&#20803;&#30340;&#8220;&#25945;&#24072;&#8221;&#32593;&#32476;&#36827;&#34892;&#25311;&#21512;&#12290;&#30001;&#20110;&#23398;&#29983;&#20855;&#26377;&#36739;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#25152;&#20197;&#19981;&#28165;&#26970;&#27599;&#20010; $n$ &#20010;&#23398;&#29983;&#31070;&#32463;&#20803;&#24212;&#35813;&#22797;&#21046;&#19968;&#20010;&#25945;&#24072;&#31070;&#32463;&#20803;&#36824;&#26159;&#24179;&#22343;&#19968;&#32452;&#25945;&#24072;&#31070;&#32463;&#20803;&#12290;&#23545;&#20110;&#20855;&#26377; erf &#28608;&#27963;&#20989;&#25968;&#21644;&#26631;&#20934;&#39640;&#26031;&#36755;&#20837;&#20998;&#24067;&#30340;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#25945;&#24072;&#30340;&#36755;&#20837;&#21521;&#37327;&#26159;&#27491;&#20132;&#30340;&#24182;&#19988;&#36755;&#20986;&#26435;&#37325;&#26159;&#37193;&#30340;&#26102;&#20505;&#65292;&#8220;&#22797;&#21046;-&#24179;&#22343;&#8221;&#37197;&#32622;&#26159;&#20020;&#30028;&#28857;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#26679;&#30340;&#37197;&#32622;&#20013;&#65292;&#20248;&#21270;&#32467;&#26524;&#26159;&#24403; $n-1$ &#20010;&#23398;&#29983;&#31070;&#32463;&#20803;&#20998;&#21035;&#22797;&#21046;&#19968;&#20010;&#25945;&#24072;&#31070;&#32463;&#20803;&#65292;&#24182;&#19988;&#31532; $n$ &#20010;&#23398;&#29983;&#31070;&#32463;&#20803;&#26159;&#25152;&#26377;&#25945;&#24072;&#31070;&#32463;&#20803;&#30340;&#24179;&#22343;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any continuous function $f^*$ can be approximated arbitrarily well by a neural network with sufficiently many neurons $k$. We consider the case when $f^*$ itself is a neural network with one hidden layer and $k$ neurons. Approximating $f^*$ with a neural network with $n&lt; k$ neurons can thus be seen as fitting an under-parameterized "student" network with $n$ neurons to a "teacher" network with $k$ neurons. As the student has fewer neurons than the teacher, it is unclear, whether each of the $n$ student neurons should copy one of the teacher neurons or rather average a group of teacher neurons. For shallow neural networks with erf activation function and for the standard Gaussian input distribution, we prove that "copy-average" configurations are critical points if the teacher's incoming vectors are orthonormal and its outgoing weights are unitary. Moreover, the optimum among such configurations is reached when $n-1$ student neurons each copy one teacher neuron and the $n$-th student ne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;&#30340;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#22797;&#26434;&#30340;&#38797;&#28857;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#40065;&#26834;&#24615;&#23545;&#25239;&#25915;&#20987;&#21644;&#20998;&#24067;&#20559;&#31227;&#65292;&#20174;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2311.01642</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#30028;&#29702;&#24615;&#38454;&#26799;&#30340;&#24378;&#21270;&#23398;&#20064;&#40065;&#26834;&#23545;&#25239;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula. (arXiv:2311.01642v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;&#30340;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#22797;&#26434;&#30340;&#38797;&#28857;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#40065;&#26834;&#24615;&#23545;&#25239;&#25915;&#20987;&#21644;&#20998;&#24067;&#20559;&#31227;&#65292;&#20174;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#23545;&#25239;&#25915;&#20987;&#21644;&#20998;&#24067;&#20559;&#31227;&#19968;&#30452;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#38271;&#26399;&#30446;&#26631;&#12290;&#20026;&#27492;&#65292;&#40065;&#26834;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;(RARL)&#22312;&#19968;&#20010;&#31454;&#20105;&#24615;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#35757;&#32451;&#20027;&#35282;&#26469;&#23545;&#25239;&#25932;&#23545;&#21147;&#37327;&#65292;&#36825;&#37324;&#30340;&#26368;&#20248;&#35299;&#21363;&#29702;&#24615;&#31574;&#30053;&#23545;&#24212;&#20110;&#32435;&#20160;&#22343;&#34913;&#12290;&#28982;&#32780;&#65292;&#35201;&#25214;&#21040;&#32435;&#20160;&#22343;&#34913;&#38656;&#35201;&#38754;&#23545;&#22797;&#26434;&#30340;&#38797;&#28857;&#20248;&#21270;&#38382;&#39064;&#65292;&#23545;&#20110;&#39640;&#32500;&#25511;&#21046;&#23588;&#20854;&#38590;&#20197;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;&#30340;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#38797;&#28857;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20010;&#29109;&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#35299;&#23545;&#24212;&#20110;&#37327;&#21709;&#24212;&#22343;&#34913;(QRE)&#65292;&#23427;&#26159;&#32435;&#20160;&#22343;&#34913;&#30340;&#25512;&#24191;&#65292;&#24182;&#32771;&#34385;&#20102;&#26377;&#30028;&#29702;&#24615;&#65292;&#21363;&#20195;&#29702;&#26377;&#26102;&#20250;&#38543;&#26426;&#25191;&#34892;&#21160;&#20316;&#32780;&#38750;&#26368;&#20248;&#21160;&#20316;&#12290;&#20851;&#38190;&#26159;&#65292;&#36830;&#25509;&#29109;&#27491;&#21017;&#21270;&#21644;&#37327;&#21709;&#24212;&#22343;&#34913;&#30340;&#20851;&#31995;&#20351;&#24471;&#35757;&#32451;&#40065;&#26834;&#30340;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness against adversarial attacks and distribution shifts is a long-standing goal of Reinforcement Learning (RL). To this end, Robust Adversarial Reinforcement Learning (RARL) trains a protagonist against destabilizing forces exercised by an adversary in a competitive zero-sum Markov game, whose optimal solution, i.e., rational strategy, corresponds to a Nash equilibrium. However, finding Nash equilibria requires facing complex saddle point optimization problems, which can be prohibitive to solve, especially for high-dimensional control. In this paper, we propose a novel approach for adversarial RL based on entropy regularization to ease the complexity of the saddle point optimization problem. We show that the solution of this entropy-regularized problem corresponds to a Quantal Response Equilibrium (QRE), a generalization of Nash equilibria that accounts for bounded rationality, i.e., agents sometimes play random actions instead of optimal ones. Crucially, the connection between 
&lt;/p&gt;</description></item><item><title>VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2311.01623</link><description>&lt;p&gt;
VQPy&#65306;&#19968;&#31181;&#38754;&#21521;&#29616;&#20195;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01623
&lt;/p&gt;
&lt;p&gt;
VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#20170;&#31995;&#32479;&#21644;&#26381;&#21153;&#20013;&#12290;&#22312;&#35270;&#39057;&#20998;&#26512;&#30340;&#21069;&#27839;&#26159;&#29992;&#25143;&#24320;&#21457;&#30340;&#35270;&#39057;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#65288;&#20363;&#22914;&#20154;&#65292;&#21160;&#29289;&#65292;&#27773;&#36710;&#31561;&#65289;&#19982;&#20256;&#32479;&#38754;&#21521;&#23545;&#35937;&#35821;&#35328;&#24314;&#27169;&#30340;&#23545;&#35937;&#30456;&#20284;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21517;&#20026;VQPy&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#31471;&#65288;&#19968;&#31181;Python&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#21487;&#20197;&#34920;&#36798;&#35270;&#39057;&#23545;&#35937;&#21450;&#20854;&#20132;&#20114;&#30340;&#32467;&#26500;&#65289;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#33258;&#21160;&#29983;&#25104;&#21644;&#20248;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#26045;&#21644;&#24320;&#28304;&#20102;VQPy&#65292;&#23427;&#24050;&#32463;&#20316;&#20026;Cisco DeepVision&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#20135;&#21697;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#26426;&#21046;&#65292;&#21033;&#29992;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26469;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36890;&#36807;&#35266;&#23519;&#20887;&#20313;&#35825;&#23548;&#33021;&#21147;&#65292;&#35782;&#21035;&#24182;&#20445;&#30041;&#23545;&#31070;&#32463;&#32593;&#32476;&#36716;&#31227;&#33021;&#21147;&#26368;&#26377;&#36129;&#29486;&#30340;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#22312;&#20219;&#21153;&#36793;&#30028;&#26102;&#30340;&#36873;&#25321;&#24615;&#21487;&#22609;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01617</link><description>&lt;p&gt;
&#25552;&#21069;&#36873;&#25321;&#24615;&#21487;&#22609;&#24615;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look-Ahead Selective Plasticity for Continual Learning of Visual Tasks. (arXiv:2311.01617v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01617
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#26426;&#21046;&#65292;&#21033;&#29992;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26469;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36890;&#36807;&#35266;&#23519;&#20887;&#20313;&#35825;&#23548;&#33021;&#21147;&#65292;&#35782;&#21035;&#24182;&#20445;&#30041;&#23545;&#31070;&#32463;&#32593;&#32476;&#36716;&#31227;&#33021;&#21147;&#26368;&#26377;&#36129;&#29486;&#30340;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#22312;&#20219;&#21153;&#36793;&#30028;&#26102;&#30340;&#36873;&#25321;&#24615;&#21487;&#22609;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#23398;&#20064;&#21040;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#20855;&#26377;&#40065;&#26834;&#24615;&#24182;&#19988;&#23545;&#26410;&#26469;&#30340;&#20219;&#21153;&#26377;&#24456;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#34920;&#31034;&#12290;&#20197;&#22823;&#33041;&#20013;&#21019;&#24314;&#21644;&#26356;&#26032;&#30340;&#20107;&#20214;&#27169;&#22411;&#20026;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21457;&#29983;&#22312;&#20219;&#21153;&#36793;&#30028;&#65292;&#21363;&#19968;&#20010;&#20219;&#21153;&#32467;&#26463;&#24182;&#21478;&#19968;&#20010;&#20219;&#21153;&#24320;&#22987;&#26102;&#12290;&#36890;&#36807;&#35266;&#23519;&#23545;&#27604;&#25439;&#22833;&#23545;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#20887;&#20313;&#35825;&#23548;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26032;&#20219;&#21153;&#30340;&#21069;&#20960;&#20010;&#26679;&#26412;&#65292;&#35782;&#21035;&#21644;&#20445;&#30041;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#36716;&#31227;&#33021;&#21147;&#26368;&#26377;&#36129;&#29486;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#37322;&#25918;&#32593;&#32476;&#30340;&#20854;&#20313;&#37096;&#20998;&#26469;&#23398;&#20064;&#26032;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#35832;&#22914;CIFAR10&#21644;TinyImagenet&#31561;&#22522;&#20934;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20219;&#21153;&#22686;&#37327;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive representation learning has emerged as a promising technique for continual learning as it can learn representations that are robust to catastrophic forgetting and generalize well to unseen future tasks. Previous work in continual learning has addressed forgetting by using previous task data and trained models. Inspired by event models created and updated in the brain, we propose a new mechanism that takes place during task boundaries, i.e., when one task finishes and another starts. By observing the redundancy-inducing ability of contrastive loss on the output of a neural network, our method leverages the first few samples of the new task to identify and retain parameters contributing most to the transfer ability of the neural network, freeing up the remaining parts of the network to learn new features. We evaluate the proposed methods on benchmark computer vision datasets including CIFAR10 and TinyImagenet and demonstrate state-of-the-art performance in the task-incrementa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRED&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01605</link><description>&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#39044;&#27979;&#30340;&#24544;&#23454;&#21644;&#31283;&#20581;&#30340;&#26412;&#22320;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful and Robust Local Interpretability for Textual Predictions. (arXiv:2311.01605v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRED&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#24471;&#21040;&#20449;&#20219;&#21644;&#37096;&#32626;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#22797;&#26434;&#65292;&#24182;&#19988;&#32570;&#20047;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#19981;&#33021;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;FRED&#65288;Faithful and Robust Explainer for textual Documents&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24403;&#36825;&#20123;&#35789;&#34987;&#31227;&#38500;&#26102;&#23545;&#39044;&#27979;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#27491;&#24335;&#30340;&#23450;&#20041;&#21644;&#23545;&#21487;&#35299;&#37322;&#20998;&#31867;&#22120;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#30830;&#31435;&#20102;FRED&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FRED&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is essential for machine learning models to be trusted and deployed in critical domains. However, existing methods for interpreting text models are often complex, lack solid mathematical foundations, and their performance is not guaranteed. In this paper, we propose FRED (Faithful and Robust Explainer for textual Documents), a novel method for interpreting predictions over text. FRED identifies key words in a document that significantly impact the prediction when removed. We establish the reliability of FRED through formal definitions and theoretical analyses on interpretable classifiers. Additionally, our empirical evaluation against state-of-the-art methods demonstrates the effectiveness of FRED in providing insights into text models.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25299;&#25169;&#23398;&#20013;&#30340;Borsuk-Ulam&#23450;&#29702;&#65292;&#30740;&#31350;&#20102;&#21015;&#34920;&#21487;&#22797;&#21046;&#21644;&#20840;&#23616;&#31283;&#23450;&#23398;&#20064;&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#22312;&#19981;&#21487;&#30693;PAC&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#23398;&#20064;&#31639;&#27861;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#21487;&#23454;&#29616;&#30340;PAC&#35774;&#32622;&#20013;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#30028;&#38480;&#21644;&#19979;&#30028;&#65292;&#19982;Littlestone&#32500;&#24230;&#26377;&#25351;&#25968;&#32423;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2311.01599</link><description>&lt;p&gt;
&#26412;&#22320;Borsuk-Ulam, &#31283;&#23450;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;
&lt;/p&gt;
&lt;p&gt;
Local Borsuk-Ulam, Stability, and Replicability. (arXiv:2311.01599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01599
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25299;&#25169;&#23398;&#20013;&#30340;Borsuk-Ulam&#23450;&#29702;&#65292;&#30740;&#31350;&#20102;&#21015;&#34920;&#21487;&#22797;&#21046;&#21644;&#20840;&#23616;&#31283;&#23450;&#23398;&#20064;&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#22312;&#19981;&#21487;&#30693;PAC&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#23398;&#20064;&#31639;&#27861;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#21487;&#23454;&#29616;&#30340;PAC&#35774;&#32622;&#20013;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#30028;&#38480;&#21644;&#19979;&#30028;&#65292;&#19982;Littlestone&#32500;&#24230;&#26377;&#25351;&#25968;&#32423;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#24182;&#25913;&#32534;&#20102;&#25299;&#25169;&#23398;&#20013;&#30340;Borsuk-Ulam&#23450;&#29702;&#65292;&#25512;&#23548;&#20986;&#23545;&#20110;&#21015;&#34920;&#21487;&#22797;&#21046;&#21644;&#20840;&#23616;&#31283;&#23450;&#23398;&#20064;&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#32452;&#21512;&#23398;&#21644;&#25299;&#25169;&#23398;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#19981;&#21487;&#30693;PAC(setting)&#20013;&#65292;&#38500;&#20102;&#24179;&#20961;&#30340;&#24773;&#20917;&#22806;&#65292;&#21015;&#34920;&#21487;&#22797;&#21046;&#21644;&#20840;&#23616;&#31283;&#23450;&#23398;&#20064;&#37117;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#36825;&#19982;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#24418;&#25104;&#20102;&#23545;&#27604;&#65292;&#24050;&#30693;&#20855;&#26377;&#26377;&#38480;Littlestone&#32500;&#24230;&#30340;&#20219;&#20309;&#31867;&#37117;&#21487;&#20197;&#36890;&#36807;&#36825;&#26679;&#30340;&#31639;&#27861;&#23398;&#20064;&#12290;&#22312;&#21487;&#23454;&#29616;&#30340;PAC(setting)&#20013;&#65292;&#25105;&#20204;&#21152;&#24378;&#20102;&#20197;&#21069;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#24182;&#25193;&#22823;&#20102;&#23427;&#20204;&#30340;&#33539;&#22260;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;&#26377;&#38480;&#31867;&#21035;&#20013;&#30340;&#21015;&#34920;&#21487;&#22797;&#21046;&#21644;&#20840;&#23616;&#31283;&#23450;&#25968;&#37327;&#30830;&#23450;&#20102;&#26368;&#20339;&#30028;&#38480;&#12290;&#36825;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#25351;&#25968;&#32423;&#30340;&#25913;&#36827;&#65292;&#24182;&#24847;&#21619;&#30528;&#19982;Littlestone&#32500;&#24230;&#30340;&#25351;&#25968;&#32423;&#20998;&#31163;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#24369;&#23398;&#20064;&#32773;&#30340;&#19979;&#30028;&#65292;&#21363;&#20165;&#27604;&#38543;&#26426;&#29468;&#27979;&#31245;&#24494;&#22909;&#19968;&#28857;&#30340;&#23398;&#20064;&#32773;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#25552;&#20379;&#20102;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
We use and adapt the Borsuk-Ulam Theorem from topology to derive limitations on list-replicable and globally stable learning algorithms. We further demonstrate the applicability of our methods in combinatorics and topology.  We show that, besides trivial cases, both list-replicable and globally stable learning are impossible in the agnostic PAC setting. This is in contrast with the realizable case where it is known that any class with a finite Littlestone dimension can be learned by such algorithms. In the realizable PAC setting, we sharpen previous impossibility results and broaden their scope. Specifically, we establish optimal bounds for list replicability and global stability numbers in finite classes. This provides an exponential improvement over previous works and implies an exponential separation from the Littlestone dimension. We further introduce lower bounds for weak learners, i.e., learners that are only marginally better than random guessing. Lower bounds from previous work
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20844;&#24179;GNN&#30340;&#23545;&#25239;&#24615;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#20844;&#24179;GNN&#30340;&#20551;&#35774;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01591</link><description>&lt;p&gt;
&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#32988;&#20110;&#36951;&#25022;&#65306;&#38024;&#23545;&#20844;&#24179;GNN&#30340;&#23545;&#25239;&#24615;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;
&lt;/p&gt;
&lt;p&gt;
Better Fair than Sorry: Adversarial Missing Data Imputation for Fair GNNs. (arXiv:2311.01591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20844;&#24179;GNN&#30340;&#23545;&#25239;&#24615;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#20844;&#24179;GNN&#30340;&#20551;&#35774;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32570;&#22833;&#20445;&#25252;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#20915;&#31574;&#21487;&#33021;&#20250;&#23545;&#29305;&#23450;&#31038;&#21306;&#20135;&#29983;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#65292;&#32780;GNNs&#24050;&#32463;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;GNNs&#24037;&#20316;&#35201;&#20040;&#20551;&#35774;&#20445;&#25252;&#23646;&#24615;&#26159;&#23436;&#20840;&#34987;&#35266;&#23519;&#21040;&#30340;&#65292;&#35201;&#20040;&#20551;&#35774;&#32570;&#22833;&#25968;&#25454;&#30340;&#22635;&#20805;&#26159;&#20844;&#24179;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#22635;&#20805;&#20013;&#30340;&#20559;&#24046;&#20250;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#32467;&#26524;&#20013;&#65292;&#23548;&#33268;&#23427;&#20204;&#36807;&#39640;&#22320;&#20272;&#35745;&#20102;&#20854;&#39044;&#27979;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;Better Fair than Sorry&#65288;BFtS&#65289;&#65292;&#20026;&#20844;&#24179;GNNs&#20351;&#29992;&#30340;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;BFtS&#32972;&#21518;&#30340;&#20851;&#38190;&#35774;&#35745;&#21407;&#21017;&#26159;&#22635;&#20805;&#24212;&#35813;&#36817;&#20284;&#20110;&#20844;&#24179;GNN&#30340;&#26368;&#22256;&#38590;&#24773;&#20917;&#65292;&#21363;&#22312;&#26368;&#20248;&#21270;&#20844;&#24179;&#24615;&#26368;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19977;&#26041;&#23545;&#25239;&#26041;&#26696;&#26469;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;&#22312;&#36825;&#20010;&#26041;&#26696;&#20013;&#65292;&#20004;&#20010;&#23545;&#25163;&#20849;&#21516;&#23545;&#25239;&#20844;&#24179;GNN&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;BFtS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of learning fair Graph Neural Networks (GNNs) under missing protected attributes. GNNs have achieved state-of-the-art results in many relevant tasks where decisions might disproportionately impact specific communities. However, existing work on fair GNNs assumes that either protected attributes are fully-observed or that the missing data imputation is fair. In practice, biases in the imputation will be propagated to the model outcomes, leading them to overestimate the fairness of their predictions. We address this challenge by proposing Better Fair than Sorry (BFtS), a fair missing data imputation model for protected attributes used by fair GNNs. The key design principle behind BFtS is that imputations should approximate the worst-case scenario for the fair GNN -- i.e. when optimizing fairness is the hardest. We implement this idea using a 3-player adversarial scheme where two adversaries collaborate against the fair GNN. Experiments using synthetic and
&lt;/p&gt;</description></item><item><title>&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#36275;&#22815;&#22810;&#26679;&#30340;&#28304;&#20219;&#21153;&#35757;&#32451;&#34920;&#31034;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#26032;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.01589</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#20013;&#34920;&#31034;&#36716;&#31227;&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
A Statistical Guarantee for Representation Transfer in Multitask Imitation Learning. (arXiv:2311.01589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01589
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#36275;&#22815;&#22810;&#26679;&#30340;&#28304;&#20219;&#21153;&#35757;&#32451;&#34920;&#31034;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#26032;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#34920;&#31034;&#36716;&#31227;&#26377;&#28508;&#21147;&#27604;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#26356;&#26377;&#25928;&#22320;&#25552;&#20379;&#23398;&#20064;&#26032;&#20219;&#21153;&#25152;&#38656;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#35745;&#20445;&#35777;&#65292;&#34920;&#26126;&#24403;&#20351;&#29992;&#36275;&#22815;&#22810;&#26679;&#30340;&#28304;&#20219;&#21153;&#35757;&#32451;&#34920;&#31034;&#26102;&#65292;&#25105;&#20204;&#30830;&#23454;&#21487;&#20197;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#23454;&#29616;&#25913;&#36827;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#21487;&#20197;&#36731;&#26494;&#25512;&#24191;&#21040;&#32771;&#34385;&#24120;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#29616;&#23454;&#20551;&#35774;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19982;&#29702;&#35770;&#21457;&#29616;&#30456;&#19968;&#33268;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#22312;&#22235;&#20010;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#8212;&#8212;&#29305;&#21035;&#26159;&#21033;&#29992;&#28304;&#20219;&#21153;&#20013;&#26356;&#22810;&#30340;&#25968;&#25454;&#21487;&#20197;&#25913;&#36827;&#23545;&#26032;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferring representation for multitask imitation learning has the potential to provide improved sample efficiency on learning new tasks, when compared to learning from scratch. In this work, we provide a statistical guarantee indicating that we can indeed achieve improved sample efficiency on the target task when a representation is trained using sufficiently diverse source tasks. Our theoretical results can be readily extended to account for commonly used neural network architectures with realistic assumptions. We conduct empirical analyses that align with our theoretical findings on four simulated environments$\unicode{x2014}$in particular leveraging more data from source tasks can improve sample efficiency on learning in the new task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#25429;&#25417;&#23431;&#23449;&#23398;&#20449;&#24687;&#21644;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01588</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#32422;&#26463;&#23431;&#23449;&#23398;&#21442;&#25968;&#30340;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets. (arXiv:2311.01588v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#25429;&#25417;&#23431;&#23449;&#23398;&#20449;&#24687;&#21644;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#25688;&#35201;&#32479;&#35745;&#37327;&#65288;&#22914;&#21151;&#29575;&#35889;&#65289;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20174;&#22797;&#26434;&#23431;&#23449;&#23398;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#27169;&#25311;&#22871;&#20214;&#20013;&#30340;&#23376;&#32593;&#26684;&#29289;&#29702;&#23454;&#29616;&#21644;&#25968;&#20540;&#36924;&#36817;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#22312;&#19968;&#20010;&#23431;&#23449;&#23398;&#27169;&#25311;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#21518;&#65292;&#22312;&#21478;&#19968;&#20010;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20250;&#19979;&#38477;&#12290;&#21516;&#26679;&#65292;&#23545;&#20110;&#20219;&#20309;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#24212;&#29992;&#20110;&#35266;&#27979;&#25968;&#25454;&#26102;&#20063;&#21487;&#33021;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#19981;&#21516;&#22871;&#20214;&#30340;CAMELS&#27700;&#21160;&#21147;&#23431;&#23449;&#23398;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DA-GNNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#25429;&#25417;&#26469;&#33258;&#26143;&#31995;&#20998;&#24067;&#30340;&#32467;&#26500;&#26080;&#26631;&#24230;&#23431;&#23449;&#23398;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21253;&#25324;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#36866;&#37197;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20004;&#20010;&#27169;&#25311;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have been shown to outperform methods that rely on summary statistics, like the power spectrum, in extracting information from complex cosmological data sets. However, due to differences in the subgrid physics implementation and numerical approximations across different simulation suites, models trained on data from one cosmological simulation show a drop in performance when tested on another. Similarly, models trained on any of the simulations would also likely experience a drop in performance when applied to observational data. Training on data from two different suites of the CAMELS hydrodynamic cosmological simulations, we examine the generalization capabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing GNNs, we capitalize on their capacity to capture structured scale-free cosmological information from galaxy distributions. Moreover, by including unsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable our models to ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#20165;&#20351;&#29992;&#32534;&#30721;&#22120;&#30340;&#27973;&#23618;Transformer&#22312;&#26377;&#38480;&#23485;&#24230;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#29702;&#35770;&#65292;&#24182;&#36890;&#36807;&#22788;&#29702;softmax&#30340;&#36755;&#20837;/&#36755;&#20986;&#21644;&#35777;&#26126;&#20108;&#27425;&#36229;&#21442;&#25968;&#21270;&#30340;&#26377;&#25928;&#24615;&#26469;&#35299;&#20915;&#20854;&#25910;&#25947;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2311.01575</link><description>&lt;p&gt;
&#20851;&#20110;&#20165;&#20351;&#29992;&#32534;&#30721;&#22120;&#30340;&#27973;&#23618;Transformer&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Encoder-only Shallow Transformers. (arXiv:2311.01575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#20165;&#20351;&#29992;&#32534;&#30721;&#22120;&#30340;&#27973;&#23618;Transformer&#22312;&#26377;&#38480;&#23485;&#24230;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#29702;&#35770;&#65292;&#24182;&#36890;&#36807;&#22788;&#29702;softmax&#30340;&#36755;&#20837;/&#36755;&#20986;&#21644;&#35777;&#26126;&#20108;&#27425;&#36229;&#21442;&#25968;&#21270;&#30340;&#26377;&#25928;&#24615;&#26469;&#35299;&#20915;&#20854;&#25910;&#25947;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20174;&#26550;&#26500;&#12289;&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#22312;&#26377;&#38480;&#23485;&#24230;&#30340;&#26465;&#20214;&#19979;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#19979;&#26500;&#24314;&#20165;&#20351;&#29992;&#32534;&#30721;&#22120;&#30340;&#27973;&#23618;Transformer&#30340;&#20840;&#23616;&#25910;&#25947;&#29702;&#35770;&#12290;&#38590;&#28857;&#22312;&#20110;&#22914;&#20309;&#22788;&#29702;Transformer&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;softmax&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35786;&#26029;&#20102;&#32553;&#25918;&#26041;&#26696;&#65292;&#20180;&#32454;&#22788;&#29702;&#20102;softmax&#30340;&#36755;&#20837;/&#36755;&#20986;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23454;&#36341;&#20013;&#24120;&#29992;&#30340;He/LeCun&#21021;&#22987;&#21270;&#19979;&#65292;&#25105;&#20204;&#30340;&#27973;&#23618;Transformer&#30340;&#20840;&#23616;&#25910;&#25947;&#20165;&#38656;&#35201;&#20108;&#27425;&#36229;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;&#31070;&#32463;&#20999;&#25442;&#26680;&#65288;NTK&#65289;&#30340;&#20998;&#26512;&#65292;&#36825;&#26377;&#21161;&#20110;&#20840;&#38754;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23637;&#31034;&#20102;&#19981;&#21516;&#32553;&#25918;&#26041;&#26696;&#21644;&#21021;&#22987;&#21270;&#30340;&#37325;&#35201;&#24615;&#20998;&#31163;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;&#29616;&#20195;Transformer&#65292;&#29305;&#21035;&#26159;&#35757;&#32451;&#21160;&#21147;&#23398;&#65292;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to build the global convergence theory of encoder-only shallow Transformers under a realistic setting from the perspective of architectures, initialization, and scaling under a finite width regime. The difficulty lies in how to tackle the softmax in self-attention mechanism, the core ingredient of Transformer. In particular, we diagnose the scaling scheme, carefully tackle the input/output of softmax, and prove that quadratic overparameterization is sufficient for global convergence of our shallow Transformers under commonly-used He/LeCun initialization in practice. Besides, neural tangent kernel (NTK) based analysis is also given, which facilitates a comprehensive comparison. Our theory demonstrates the separation on the importance of different scaling schemes and initialization. We believe our results can pave the way for a better understanding of modern Transformers, particularly on training dynamics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;FDG-18&#20840;&#36523;PET / CT&#25195;&#25551;&#20013;&#30149;&#21464;&#30340;&#20998;&#21106;&#25928;&#26524;&#65292;&#36890;&#36807;&#20998;&#21106;&#22120;&#23448;&#21644;&#30149;&#21464;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;AutoPET II&#25361;&#25112;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2311.01574</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26631;&#31614;&#26041;&#27861;&#25913;&#36827;FDG-18&#20840;&#36523;PET / CT&#25195;&#25551;&#20013;&#30340;&#30149;&#21464;&#20998;&#21106;&#65306;AutoPET II&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Improving Lesion Segmentation in FDG-18 Whole-Body PET/CT scans using Multilabel approach: AutoPET II challenge. (arXiv:2311.01574v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;FDG-18&#20840;&#36523;PET / CT&#25195;&#25551;&#20013;&#30149;&#21464;&#30340;&#20998;&#21106;&#25928;&#26524;&#65292;&#36890;&#36807;&#20998;&#21106;&#22120;&#23448;&#21644;&#30149;&#21464;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;AutoPET II&#25361;&#25112;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#20998;&#21106;FDG-18&#20840;&#36523;PET / CT&#25195;&#25551;&#20013;&#30340;&#30149;&#21464;&#23545;&#20110;&#30830;&#23450;&#27835;&#30103;&#21453;&#24212;&#12289;&#20248;&#21270;&#21058;&#37327;&#21644;&#25512;&#21160;&#32959;&#30244;&#23398;&#20013;&#30340;&#27835;&#30103;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#25918;&#23556;&#24615;&#31034;&#36394;&#21058;&#25668;&#21462;&#21319;&#39640;&#30340;&#22120;&#23448;&#65288;&#22914;&#32925;&#33039;&#12289;&#33086;&#33039;&#12289;&#22823;&#33041;&#21644;&#33152;&#33009;&#65289;&#24120;&#24120;&#23548;&#33268;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#21306;&#22495;&#24448;&#24448;&#34987;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38169;&#35823;&#22320;&#26631;&#35760;&#20026;&#30149;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#21106;&#22120;&#23448;&#21644;&#30149;&#21464;&#65292;&#26088;&#22312;&#25552;&#21319;&#33258;&#21160;&#30149;&#21464;&#20998;&#21106;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;AutoPET II&#25361;&#25112;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;1014&#21517;&#24739;&#32773;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21253;&#21547;&#39069;&#22806;&#26631;&#31614;&#21644;&#25968;&#25454;&#23545;&#27169;&#22411;&#20998;&#21106;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#38500;&#20102;&#19987;&#23478;&#26631;&#27880;&#30340;&#30149;&#21464;&#26631;&#31614;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20843;&#20010;&#21253;&#25324;&#32925;&#33039;&#12289;&#32958;&#33039;&#12289;&#36755;&#23615;&#31649;&#31561;&#22120;&#23448;&#30340;&#38468;&#21152;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic segmentation of lesions in FDG-18 Whole Body (WB) PET/CT scans using deep learning models is instrumental for determining treatment response, optimizing dosimetry, and advancing theranostic applications in oncology. However, the presence of organs with elevated radiotracer uptake, such as the liver, spleen, brain, and bladder, often leads to challenges, as these regions are often misidentified as lesions by deep learning models. To address this issue, we propose a novel approach of segmenting both organs and lesions, aiming to enhance the performance of automatic lesion segmentation methods. In this study, we assessed the effectiveness of our proposed method using the AutoPET II challenge dataset, which comprises 1014 subjects. We evaluated the impact of inclusion of additional labels and data in the segmentation performance of the model. In addition to the expert-annotated lesion labels, we introduced eight additional labels for organs, including the liver, kidneys, urinary 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#39537;&#21160;&#30340;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#23398;&#20064;&#21644;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#36335;&#24452;&#26469;&#32534;&#36753;&#21463;&#20445;&#25252;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#20943;&#36731;&#20102;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#38598;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01573</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#39537;&#21160;&#30340;&#22270;&#20687;&#22686;&#24378;&#25913;&#21892;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness using Vision-Language Driven Image Augmentation. (arXiv:2311.01573v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#39537;&#21160;&#30340;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#23398;&#20064;&#21644;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#36335;&#24452;&#26469;&#32534;&#36753;&#21463;&#20445;&#25252;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#20943;&#36731;&#20102;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#38598;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#37492;&#21035;&#27169;&#22411;&#26102;&#65292;&#20844;&#24179;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#37096;&#39046;&#22495;&#12290;&#27169;&#22411;&#24448;&#24448;&#23558;&#29305;&#23450;&#29305;&#24449;&#65288;&#22914;&#24180;&#40836;&#21644;&#32932;&#33394;&#65289;&#19982;&#26080;&#20851;&#23646;&#24615;&#65288;&#19979;&#28216;&#20219;&#21153;&#65289;&#30456;&#20851;&#32852;&#65292;&#23548;&#33268;&#20559;&#35265;&#19982;&#29616;&#23454;&#19981;&#31526;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#20123;&#30456;&#20851;&#24615;&#23384;&#22312;&#20110;&#25968;&#25454;&#20013;&#65292;&#28982;&#21518;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20256;&#36882;&#32473;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#30456;&#20851;&#24615;&#65292;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#22312;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65288;DiffAE&#65289;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#20301;&#20110;&#21487;&#35299;&#37322;&#21644;&#26377;&#24847;&#20041;&#30340;&#36335;&#24452;&#65292;&#36825;&#20123;&#36335;&#24452;&#30001;&#23545;&#27604;&#24615;&#25991;&#26412;&#20108;&#26497;&#20307;&#36827;&#34892;&#30417;&#30563;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#32534;&#36753;&#21463;&#20445;&#25252;&#29305;&#24449;&#65288;&#24180;&#40836;&#21644;&#32932;&#33394;&#65289;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#36335;&#24452;&#24212;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#65292;&#20197;&#25552;&#39640;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#22312;CelebA-HQ&#21644;UTKFace&#19978;&#23545;&#24180;&#40836;&#21644;&#32932;&#33394;&#20316;&#20026;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20102;&#35813;&#26041;&#27861;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness is crucial when training a deep-learning discriminative model, especially in the facial domain. Models tend to correlate specific characteristics (such as age and skin color) with unrelated attributes (downstream tasks), resulting in biases which do not correspond to reality. It is common knowledge that these correlations are present in the data and are then transferred to the models during training. This paper proposes a method to mitigate these correlations to improve fairness. To do so, we learn interpretable and meaningful paths lying in the semantic space of a pre-trained diffusion model (DiffAE) -- such paths being supervised by contrastive text dipoles. That is, we learn to edit protected characteristics (age and skin color). These paths are then applied to augment images to improve the fairness of a given dataset. We test the proposed method on CelebA-HQ and UTKFace on several downstream tasks with age and skin color as protected characteristics. As a proxy for fairnes
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#23376;&#38598;&#21305;&#37197;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#31934;&#31616;&#26102;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#36890;&#36807;&#20248;&#21270;&#27599;&#20010;&#21512;&#25104;&#23454;&#20363;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01570</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#31616;&#30340;&#24207;&#21015;&#23376;&#38598;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Sequential Subset Matching for Dataset Distillation. (arXiv:2311.01570v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01570
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#23376;&#38598;&#21305;&#37197;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#31934;&#31616;&#26102;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#36890;&#36807;&#20248;&#21270;&#27599;&#20010;&#21512;&#25104;&#23454;&#20363;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#31616;&#26159;&#19968;&#39033;&#26032;&#20852;&#20219;&#21153;&#65292;&#29992;&#20110;&#21512;&#25104;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#20197;&#38477;&#20302;&#25968;&#25454;&#23384;&#20648;&#21644;&#27169;&#22411;&#35757;&#32451;&#25104;&#26412;&#12290;&#26399;&#26395;&#21512;&#25104;&#25968;&#25454;&#38598;&#33021;&#22815;&#25429;&#25417;&#21040;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#25152;&#21253;&#21547;&#30340;&#30693;&#35782;&#31934;&#39635;&#65292;&#20174;&#32780;&#20135;&#29983;&#19982;&#30495;&#23454;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#38598;&#31934;&#31616;&#26041;&#27861;&#30340;&#36827;&#23637;&#22312;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#23558;&#25972;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#35270;&#20026;&#32479;&#19968;&#23454;&#20307;&#65292;&#24182;&#23545;&#27599;&#20010;&#21512;&#25104;&#23454;&#20363;&#36827;&#34892;&#30456;&#21516;&#30340;&#20248;&#21270;&#12290;&#36825;&#31181;&#38745;&#24577;&#20248;&#21270;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#38598;&#31934;&#31616;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35748;&#20026;&#38745;&#24577;&#20248;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#21512;&#25104;&#25968;&#25454;&#20013;&#30340;&#32806;&#21512;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20248;&#21270;&#36739;&#22823;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#26102;&#12290;&#36825;&#31181;&#32806;&#21512;&#38382;&#39064;&#36827;&#32780;&#23548;&#33268;&#31934;&#31616;&#25968;&#25454;&#38598;&#26080;&#27861;&#25552;&#21462;&#39640;&#32423;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation is a newly emerging task that synthesizes a small-size dataset used in training deep neural networks (DNNs) for reducing data storage and model training costs. The synthetic datasets are expected to capture the essence of the knowledge contained in real-world datasets such that the former yields a similar performance as the latter. Recent advancements in distillation methods have produced notable improvements in generating synthetic datasets. However, current state-of-the-art methods treat the entire synthetic dataset as a unified entity and optimize each synthetic instance equally. This static optimization approach may lead to performance degradation in dataset distillation. Specifically, we argue that static optimization can give rise to a coupling issue within the synthetic data, particularly when a larger amount of synthetic data is being optimized. This coupling issue, in turn, leads to the failure of the distilled dataset to extract the high-level features le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#20808;&#39564;&#30340;&#20219;&#24847;&#26102;&#21051;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65288;A-CMDP&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;ACRL&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20445;&#35777;&#20219;&#24847;&#26102;&#21051;&#30340;&#25104;&#26412;&#32422;&#26463;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#22238;&#25253;&#24615;&#33021;&#21644;&#25104;&#26412;&#32422;&#26463;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.01568</link><description>&lt;p&gt;
Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG]) &#65288;&#20855;&#26377;&#31574;&#30053;&#20808;&#39564;&#30340;&#20219;&#24847;&#26102;&#21051;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#65289;
&lt;/p&gt;
&lt;p&gt;
Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#20808;&#39564;&#30340;&#20219;&#24847;&#26102;&#21051;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65288;A-CMDP&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;ACRL&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20445;&#35777;&#20219;&#24847;&#26102;&#21051;&#30340;&#25104;&#26412;&#32422;&#26463;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#22238;&#25253;&#24615;&#33021;&#21644;&#25104;&#26412;&#32422;&#26463;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26102;&#21051;&#31454;&#20105;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;A-CMDP&#65289;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#21463;&#38480;&#21046;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#30340;&#30740;&#31350;&#26088;&#22312;&#22312;&#38543;&#26426;&#21160;&#24577;&#20013;&#20248;&#21270;&#26399;&#26395;&#22238;&#25253;&#21516;&#26102;&#32422;&#26463;&#26399;&#26395;&#25104;&#26412;&#65292;&#20294;&#26159;&#29305;&#23450;&#24773;&#33410;&#20013;&#30340;&#25104;&#26412;&#20173;&#28982;&#21487;&#33021;&#38750;&#24120;&#39640;&#12290;&#30456;&#21453;&#65292;A-CMDP&#30340;&#30446;&#26631;&#26159;&#22312;&#20219;&#24847;&#19968;&#36718;&#30340;&#20219;&#20309;&#24773;&#33410;&#20013;&#65292;&#20248;&#21270;&#26399;&#26395;&#22238;&#25253;&#21516;&#26102;&#20445;&#35777;&#26377;&#30028;&#30340;&#25104;&#26412;&#65292;&#24182;&#38024;&#23545;&#31574;&#30053;&#20808;&#39564;&#36827;&#34892;&#20102;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;Anytime-Competitive Reinforcement Learning&#65288;ACRL&#65289;&#65292;&#23427;&#21487;&#20197;&#35777;&#26126;&#22320;&#20445;&#35777;&#20102;&#20219;&#24847;&#26102;&#21051;&#30340;&#25104;&#26412;&#32422;&#26463;&#12290;&#36951;&#25022;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#31574;&#30053;&#22312;&#20219;&#24847;&#30340;&#31454;&#20105;&#24615;&#32422;&#26463;&#19979;&#28176;&#36817;&#22320;&#19982;&#26368;&#20248;&#22238;&#25253;&#30456;&#21305;&#37197;&#12290;&#22312;&#30899;&#26234;&#33021;&#35745;&#31639;&#24212;&#29992;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;ACRL&#30340;&#22238;&#25253;&#24615;&#33021;&#21644;&#25104;&#26412;&#32422;&#26463;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of Anytime-Competitive Markov Decision Process (A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim to optimize the expected reward while constraining the expected cost over random dynamics, but the cost in a specific episode can still be unsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the expected reward while guaranteeing a bounded cost in each round of any episode against a policy prior. We propose a new algorithm, called Anytime-Competitive Reinforcement Learning (ACRL), which provably guarantees the anytime cost constraints. The regret analysis shows the policy asymptotically matches the optimal reward achievable under the anytime competitive constraints. Experiments on the application of carbon-intelligent computing verify the reward performance and cost constraint guarantee of ACRL.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21387;&#32553;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;DTM&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#19988;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DTM&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#20462;&#21098;&#25481;&#36229;&#36807;90%&#30340;LLM&#32452;&#20214;&#21644;&#37327;&#21270;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2311.01544</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65306;&#36890;&#36807;&#27979;&#37327;&#34928;&#20943;&#26469;&#20462;&#21098;LLM&#32452;&#20214;&#24182;&#20248;&#21270;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21387;&#32553;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;DTM&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#19988;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DTM&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#20462;&#21098;&#25481;&#36229;&#36807;90%&#30340;LLM&#32452;&#20214;&#21644;&#37327;&#21270;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#22823;&#23567;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#30340;&#26377;&#25928;&#37096;&#32626;&#21644;LLM&#21387;&#32553;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#21387;&#32553;LLM&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25351;&#26631;&#22914;&#22256;&#24785;&#24230;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#23616;&#38480;&#24615;&#12290;DTM&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;DTM&#36824;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#35780;&#20272;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#31532;&#19968;&#20010;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;FDTM&#65289;&#22312;&#27169;&#22411;&#31232;&#30095;&#21270;&#20013;&#26174;&#31034;&#65292;&#36229;&#36807;90%&#30340;&#25152;&#26377;&#32452;&#20214;&#21487;&#20197;&#20462;&#21098;&#25481;&#12290;&#23545;&#20110;&#37327;&#21270;&#65292;FDTM&#34920;&#26126;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#21487;&#20197;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. Their ever-increasing size, however, raised concerns about their effective deployment and the need for LLM compressions. This study introduces the Divergent Token metrics (DTMs), a novel approach for assessing compressed LLMs, addressing the limitations of traditional measures like perplexity that fail to accurately reflect text generation quality. DTMs focus on token divergence, providing deeper insights into the subtleties of model compression. Our results indicate that significant levels of precision and sparsity can be achieved without compromising text generation quality. Moreover, DTMs offers a more precise evaluation of each component's impact individually. Utilizing the First Divergent Token metric (FDTM) in model sparsification reveals that nearly 20% of all components can be pruned over 90%. In terms of quantization, the FDTM suggests that over 80% of parameters can be s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#27604;&#36739;&#20013;&#30340;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#30340;&#20004;&#26679;&#26412;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33258;&#21160;&#30456;&#20851;&#24615;&#26816;&#27979;&#26435;&#37325;&#26469;&#22686;&#24378;&#27979;&#35797;&#30340;&#21151;&#25928;&#65292;&#24182;&#24341;&#20837;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#27491;&#21017;&#21270;&#21442;&#25968;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01537</link><description>&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20998;&#24067;&#27604;&#36739;&#20013;&#30340;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#20013;&#30340;&#21464;&#37327;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Variable Selection in Maximum Mean Discrepancy for Interpretable Distribution Comparison. (arXiv:2311.01537v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#27604;&#36739;&#20013;&#30340;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#30340;&#20004;&#26679;&#26412;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33258;&#21160;&#30456;&#20851;&#24615;&#26816;&#27979;&#26435;&#37325;&#26469;&#22686;&#24378;&#27979;&#35797;&#30340;&#21151;&#25928;&#65292;&#24182;&#24341;&#20837;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#27491;&#21017;&#21270;&#21442;&#25968;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#26679;&#26412;&#27979;&#35797;&#26159;&#20026;&#20102;&#21028;&#26029;&#20004;&#20010;&#25968;&#25454;&#38598;&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#26679;&#26412;&#27979;&#35797;&#20013;&#30340;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#65292;&#21363;&#35782;&#21035;&#36896;&#25104;&#20004;&#20010;&#20998;&#24067;&#24046;&#24322;&#30340;&#21464;&#37327;&#65288;&#25110;&#32500;&#24230;&#65289;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#19982;&#27169;&#24335;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#38382;&#39064;&#30456;&#20851;&#65292;&#22914;&#25968;&#25454;&#38598;&#28418;&#31227;&#36866;&#24212;&#12289;&#22240;&#26524;&#25512;&#26029;&#21644;&#27169;&#22411;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22522;&#20110;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#65288;MMD&#65289;&#30340;&#20004;&#26679;&#26412;&#26816;&#39564;&#12290;&#25105;&#20204;&#20248;&#21270;&#38024;&#23545;&#21508;&#20010;&#21464;&#37327;&#23450;&#20041;&#30340;&#33258;&#21160;&#30456;&#20851;&#24615;&#26816;&#27979;&#65288;ARD&#65289;&#26435;&#37325;&#65292;&#20197;&#26368;&#22823;&#21270;&#22522;&#20110;MMD&#30340;&#26816;&#39564;&#30340;&#21151;&#29575;&#12290;&#23545;&#20110;&#36825;&#31181;&#20248;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#27491;&#21017;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36873;&#25321;&#36866;&#24403;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;&#19968;&#31181;&#26041;&#27861;&#26159;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#21512;&#24182;&#19981;&#21516;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30830;&#35748;&#20102;&#36825;&#20010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two-sample testing decides whether two datasets are generated from the same distribution. This paper studies variable selection for two-sample testing, the task being to identify the variables (or dimensions) responsible for the discrepancies between the two distributions. This task is relevant to many problems of pattern analysis and machine learning, such as dataset shift adaptation, causal inference and model validation. Our approach is based on a two-sample test based on the Maximum Mean Discrepancy (MMD). We optimise the Automatic Relevance Detection (ARD) weights defined for individual variables to maximise the power of the MMD-based test. For this optimisation, we introduce sparse regularisation and propose two methods for dealing with the issue of selecting an appropriate regularisation parameter. One method determines the regularisation parameter in a data-driven way, and the other aggregates the results of different regularisation parameters. We confirm the validity of the pr
&lt;/p&gt;</description></item><item><title>ATGNN&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#38899;&#39057;&#26631;&#35760;&#20219;&#21153;&#20013;&#30340;&#39057;&#35889;&#22270;&#35270;&#20026;&#22270;&#32467;&#26500;&#65292;&#24182;&#32467;&#21512;&#20102;CNN&#30340;&#33021;&#21147;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#20449;&#24687;&#20849;&#20139;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#26144;&#23556;&#20102;&#21487;&#23398;&#20064;&#31867;&#21035;&#23884;&#20837;&#21644;&#30456;&#24212;&#30340;&#39057;&#35889;&#22270;&#21306;&#22495;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2311.01526</link><description>&lt;p&gt;
ATGNN:&#38899;&#39057;&#26631;&#35760;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ATGNN: Audio Tagging Graph Neural Network. (arXiv:2311.01526v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01526
&lt;/p&gt;
&lt;p&gt;
ATGNN&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#38899;&#39057;&#26631;&#35760;&#20219;&#21153;&#20013;&#30340;&#39057;&#35889;&#22270;&#35270;&#20026;&#22270;&#32467;&#26500;&#65292;&#24182;&#32467;&#21512;&#20102;CNN&#30340;&#33021;&#21147;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#20449;&#24687;&#20849;&#20139;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#26144;&#23556;&#20102;&#21487;&#23398;&#20064;&#31867;&#21035;&#23884;&#20837;&#21644;&#30456;&#24212;&#30340;&#39057;&#35889;&#22270;&#21306;&#22495;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;CNN&#21644;Transformers&#65292;&#24050;&#32463;&#22312;&#31471;&#21040;&#31471;&#38899;&#39057;&#26631;&#35760;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#22534;&#21472;&#20102;&#22810;&#20010;&#23618;&#65292;CNN&#30340;&#24863;&#21463;&#37326;&#20173;&#28982;&#20005;&#37325;&#21463;&#38480;&#12290;&#32780;Transformers&#21017;&#33021;&#22815;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20840;&#23616;&#35821;&#22659;&#26144;&#23556;&#65292;&#20294;&#23558;&#39057;&#35889;&#22270;&#35270;&#20026;&#24207;&#21015;&#34917;&#19969;&#30340;&#26041;&#24335;&#19981;&#36275;&#20197;&#25429;&#25417;&#19981;&#35268;&#21017;&#38899;&#39057;&#23545;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20197;&#26356;&#21152;&#28789;&#27963;&#30340;&#26041;&#24335;&#23558;&#39057;&#35889;&#22270;&#35270;&#20026;&#22270;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;ATGNN&#36827;&#34892;&#22788;&#29702;&#12290;ATGNN&#19981;&#20165;&#32467;&#21512;&#20102;CNN&#30340;&#33021;&#21147;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#20449;&#24687;&#20849;&#20139;&#33021;&#21147;&#65292;&#36824;&#26144;&#23556;&#20102;&#21487;&#23398;&#20064;&#31867;&#21035;&#23884;&#20837;&#21644;&#30456;&#24212;&#30340;&#39057;&#35889;&#22270;&#21306;&#22495;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#38899;&#39057;&#26631;&#35760;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;ATGNN&#65292;&#22312;FSD50K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;0.585&#30340;mAP&#65292;&#22312;AudioSet-balanced&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;0.335&#30340;mAP&#65292;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models such as CNNs and Transformers have achieved impressive performance for end-to-end audio tagging. Recent works have shown that despite stacking multiple layers, the receptive field of CNNs remains severely limited. Transformers on the other hand are able to map global context through self-attention, but treat the spectrogram as a sequence of patches which is not flexible enough to capture irregular audio objects. In this work, we treat the spectrogram in a more flexible way by considering it as graph structure and process it with a novel graph neural architecture called ATGNN. ATGNN not only combines the capability of CNNs with the global information sharing ability of Graph Neural Networks, but also maps semantic relationships between learnable class embeddings and corresponding spectrogram regions. We evaluate ATGNN on two audio tagging tasks, where it achieves 0.585 mAP on the FSD50K dataset and 0.335 mAP on the AudioSet-balanced dataset, achieving comparable res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27700;&#19979;&#23545;&#25509;&#26816;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#36924;&#30495;&#20223;&#30495;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#27700;&#19979;&#23545;&#25509;&#30340;&#23454;&#26102;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2311.01522</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#36924;&#30495;&#20223;&#30495;&#30340;&#27700;&#19979;&#23545;&#25509;&#26816;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#65306;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Detection and Control System for Underwater Docking using Machine Learning and Realistic Simulation: A Comprehensive Approach. (arXiv:2311.01522v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27700;&#19979;&#23545;&#25509;&#26816;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#36924;&#30495;&#20223;&#30495;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#27700;&#19979;&#23545;&#25509;&#30340;&#23454;&#26102;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#19979;&#23545;&#25509;&#23545;&#20110;&#20351;&#33258;&#20027;&#27700;&#19979;&#36710;&#36742;(AUVs)&#33021;&#25345;&#20037;&#36816;&#34892;&#38750;&#24120;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;AUV&#24517;&#39035;&#33021;&#22815;&#26816;&#27979;&#21644;&#23450;&#20301;&#23545;&#25509;&#31449;&#65292;&#30001;&#20110;&#28023;&#24213;&#29615;&#22659;&#30340;&#39640;&#24230;&#21160;&#24577;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#38750;&#24120;&#22797;&#26434;&#12290;&#22522;&#20110;&#22270;&#20687;&#30340;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#39640;&#37319;&#38598;&#36895;&#29575;&#21644;&#36866;&#24212;&#29615;&#22659;&#30340;&#28789;&#27963;&#24615;; &#28982;&#32780;&#65292;&#27700;&#19979;&#29615;&#22659;&#23384;&#22312;&#20302;&#33021;&#35265;&#24230;&#12289;&#39640;&#27985;&#27978;&#24230;&#21644;&#22833;&#30495;&#31561;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#39564;&#35777;&#27700;&#19979;&#23545;&#25509;&#33021;&#21147;&#65292;&#29616;&#22330;&#23454;&#39564;&#21487;&#33021;&#20250;&#30001;&#20110;&#38656;&#35201;&#19987;&#29992;&#35774;&#22791;&#21644;&#23433;&#20840;&#32771;&#34385;&#32780;&#26114;&#36149;&#21644;&#21361;&#38505;&#12290;&#26412;&#24037;&#20316;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#36827;&#34892;&#27700;&#19979;&#23545;&#25509;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#24615;&#33021;&#26368;&#22909;&#30340;&#26550;&#26500;&#28982;&#21518;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#22312;&#24072;&#29983;&#33539;&#24335;&#19979;&#36827;&#34892;&#21387;&#32553;&#65292;&#20197;&#20943;&#23569;&#32593;&#32476;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#23454;&#29616;&#23454;&#26102;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Underwater docking is critical to enable the persistent operation of Autonomous Underwater Vehicles (AUVs). For this, the AUV must be capable of detecting and localizing the docking station, which is complex due to the highly dynamic undersea environment. Image-based solutions offer a high acquisition rate and versatile alternative to adapt to this environment; however, the underwater environment presents challenges such as low visibility, high turbidity, and distortion. In addition to this, field experiments to validate underwater docking capabilities can be costly and dangerous due to the specialized equipment and safety considerations required to conduct the experiments. This work compares different deep-learning architectures to perform underwater docking detection and classification. The architecture with the best performance is then compressed using knowledge distillation under the teacher-student paradigm to reduce the network's memory footprint, allowing real-time implementatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;2D&#27431;&#20960;&#37324;&#24471;&#32676;E(2)&#31561;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#26143;&#31995;&#24418;&#24577;&#20998;&#31867;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#31561;&#21464;&#32593;&#32476;&#22312;&#40065;&#26834;&#24615;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#38750;&#31561;&#21464;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2311.01500</link><description>&lt;p&gt;
E(2)&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#40065;&#26834;&#30340;&#26143;&#31995;&#24418;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
E(2) Equivariant Neural Networks for Robust Galaxy Morphology Classification. (arXiv:2311.01500v1 [astro-ph.GA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01500
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;2D&#27431;&#20960;&#37324;&#24471;&#32676;E(2)&#31561;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#26143;&#31995;&#24418;&#24577;&#20998;&#31867;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#31561;&#21464;&#32593;&#32476;&#22312;&#40065;&#26834;&#24615;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#38750;&#31561;&#21464;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;2D&#27431;&#20960;&#37324;&#24471;&#32676;E(2)&#31561;&#21464;&#30340;&#32676;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;GCNNs&#65289;&#26469;&#36827;&#34892;&#26143;&#31995;&#24418;&#24577;&#20998;&#31867;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#26143;&#31995;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#23545;&#31216;&#24615;&#20316;&#20026;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20154;&#20026;&#25200;&#21160;&#65292;&#22914;&#27850;&#26494;&#22122;&#22768;&#25554;&#20837;&#21644;&#21333;&#20687;&#32032;&#23545;&#25239;&#25915;&#20987;&#65292;&#36827;&#34892;&#40065;&#26834;&#24615;&#30740;&#31350;&#65292;&#20197;&#27169;&#25311;&#26377;&#38480;&#35266;&#27979;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#23545;E(2)&#30340;&#31163;&#25955;&#23376;&#32676; - &#24490;&#29615;&#32676;&#21644;&#20108;&#38754;&#20307;&#32676; - &#26159;&#31561;&#21464;&#30340;GCNNs&#65292;&#21457;&#29616;GCNNs&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#22987;&#32456;&#27604;&#38750;&#31561;&#21464;&#30340;&#23545;&#24212;&#29289;&#26356;&#22909;&#65292;&#22312;&#31561;&#21464;&#32676;D16&#23454;&#29616;&#20102;95.52&#177;0.18%&#30340;&#27979;&#35797;&#38598;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;50%&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#21482;&#25439;&#22833;&#19981;&#21040;6%&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25152;&#26377;&#30340;GCNNs&#37117;&#23545;&#21333;&#20687;&#32032;&#25200;&#21160;&#26356;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the use of group convolutional neural network architectures (GCNNs) equivariant to the 2D Euclidean group, $E(2)$, for the task of galaxy morphology classification by utilizing symmetries of the data present in galaxy images as an inductive bias in the architecture. We conduct robustness studies by introducing artificial perturbations via Poisson noise insertion and one-pixel adversarial attacks to simulate the effects of limited observational capabilities. We train, validate, and test GCNNs equivariant to discrete subgroups of $E(2)$ - the cyclic and dihedral groups of order $N$ - on the Galaxy10 DECals dataset and find that GCNNs achieve higher classification accuracy and are consistently more robust than their non-equivariant counterparts, with an architecture equivariant to the group $D_{16}$ achieving a $95.52 \pm 0.18\%$ test-set accuracy. We also find that the model loses $&lt;6\%$ accuracy on a $50\%$-noise dataset and all GCNNs are less susceptible to one-pixel perturb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#31532;&#19968;&#21407;&#29702;&#25968;&#25454;&#38598;&#12290;&#27169;&#22411;&#25512;&#26029;&#36807;&#31243;&#21253;&#25324;&#25506;&#32034;&#38454;&#27573;&#21644;&#26494;&#24347;&#38454;&#27573;&#65292;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21183;&#33021;&#34920;&#38754;&#30340;&#19968;&#38454;&#21644;&#39640;&#38454;&#32467;&#26500;&#65292;&#24182;&#19988;&#26494;&#24347;&#38454;&#27573;&#36824;&#21487;&#20197;&#29992;&#20110;&#37319;&#26679;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2311.01491</link><description>&lt;p&gt;
&#30740;&#31350;&#25193;&#25955;&#27169;&#22411;&#20197;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Investigating the Behavior of Diffusion Models for Accelerating Electronic Structure Calculations. (arXiv:2311.01491v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#31532;&#19968;&#21407;&#29702;&#25968;&#25454;&#38598;&#12290;&#27169;&#22411;&#25512;&#26029;&#36807;&#31243;&#21253;&#25324;&#25506;&#32034;&#38454;&#27573;&#21644;&#26494;&#24347;&#38454;&#27573;&#65292;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21183;&#33021;&#34920;&#38754;&#30340;&#19968;&#38454;&#21644;&#39640;&#38454;&#32467;&#26500;&#65292;&#24182;&#19988;&#26494;&#24347;&#38454;&#27573;&#36824;&#21487;&#20197;&#29992;&#20110;&#37319;&#26679;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#20998;&#23376;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#20204;&#30340;&#39044;&#27979;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#35745;&#31639;&#32467;&#26524;&#30340;&#27604;&#36739;&#12290;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#30740;&#31350;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22823;&#24133;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#31532;&#19968;&#21407;&#29702;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21407;&#23376;&#38388;&#21183;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29992;&#20110;&#29983;&#25104;&#26032;&#20998;&#23376;&#30340;&#19968;&#20010;&#27969;&#34892;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#20998;&#20026;&#25506;&#32034;&#38454;&#27573;&#65288;&#27169;&#22411;&#36873;&#25321;&#21407;&#23376;&#31181;&#31867;&#65289;&#21644;&#26494;&#24347;&#38454;&#27573;&#65288;&#27169;&#22411;&#35843;&#25972;&#21407;&#23376;&#22352;&#26631;&#20197;&#23547;&#25214;&#20302;&#33021;&#20960;&#20309;&#32467;&#26500;&#65289;&#12290;&#38543;&#30528;&#35757;&#32451;&#30340;&#36827;&#34892;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#26368;&#21021;&#23398;&#20064;&#20102;&#21183;&#33021;&#34920;&#38754;&#30340;&#19968;&#38454;&#32467;&#26500;&#65292;&#28982;&#21518;&#36880;&#28176;&#23398;&#20064;&#20102;&#39640;&#38454;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#26494;&#24347;&#38454;&#27573;&#21487;&#20197;&#34987;&#37325;&#26032;&#29992;&#26469;&#37319;&#26679;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an investigation into diffusion models for molecular generation, with the aim of better understanding how their predictions compare to the results of physics-based calculations. The investigation into these models is driven by their potential to significantly accelerate electronic structure calculations using machine learning, without requiring expensive first-principles datasets for training interatomic potentials. We find that the inference process of a popular diffusion model for de novo molecular generation is divided into an exploration phase, where the model chooses the atomic species, and a relaxation phase, where it adjusts the atomic coordinates to find a low-energy geometry. As training proceeds, we show that the model initially learns about the first-order structure of the potential energy surface, and then later learns about higher-order structure. We also find that the relaxation phase of the diffusion model can be re-purposed to sample the Boltzmann distributio
&lt;/p&gt;</description></item><item><title>FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01483</link><description>&lt;p&gt;
FedSN&#65306;&#19968;&#20010;&#36866;&#29992;&#20110;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01483
&lt;/p&gt;
&lt;p&gt;
FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#24050;&#32463;&#30001;&#21830;&#19994;&#20844;&#21496;&#25104;&#21151;&#22320;&#21457;&#23556;&#21644;&#37096;&#32626;&#21040;&#22826;&#31354;&#20013;&#65292;&#22914;SpaceX&#12290;&#30001;&#20110;LEO&#21355;&#26143;&#37197;&#22791;&#20102;&#22810;&#27169;&#20256;&#24863;&#22120;&#65292;&#23427;&#20204;&#19981;&#20165;&#29992;&#20110;&#36890;&#20449;&#65292;&#36824;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#31354;&#38388;&#35843;&#21046;&#35782;&#21035;&#12289;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19982;LEO&#21355;&#26143;&#30340;&#26377;&#38480;&#25509;&#35302;&#26102;&#38388;&#65288;&#20363;&#22914;5&#20998;&#38047;&#65289;&#65292;&#22320;&#38754;&#31449;&#65288;GS&#65289;&#21487;&#33021;&#26080;&#27861;&#19979;&#36733;&#22914;&#27492;&#22823;&#37327;&#30340;&#21407;&#22987;&#24863;&#27979;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35201;&#22312;LEO&#21355;&#26143;&#19978;&#20351;&#29992;FL&#65292;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;i&#65289;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#65292;ii&#65289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#65292;&#20197;&#21450;iii&#65289;&#27169;&#22411;&#38472;&#26087;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSN&#30340;&#36890;&#29992;FL&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#19968;
&lt;/p&gt;
&lt;p&gt;
Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#32676;&#20869;&#29305;&#24449;&#32858;&#38598;&#21644;&#32676;&#22806;&#29305;&#24449;&#31163;&#25955;&#30340;&#24615;&#36136;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21644;&#26435;&#37325;&#21521;&#37327;&#25509;&#36817;&#31243;&#24230;&#30340;&#31070;&#32463;&#22349;&#22604;&#65288;NC-OOD&#65289;&#26816;&#27979;&#22120;&#26469;&#25552;&#39640;OAD&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01479</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#22349;&#22604;&#30340;&#35270;&#35282;&#26816;&#27979;&#21040;&#32676;&#22806;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Detecting Out-of-Distribution Through the Lens of Neural Collapse. (arXiv:2311.01479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#32676;&#20869;&#29305;&#24449;&#32858;&#38598;&#21644;&#32676;&#22806;&#29305;&#24449;&#31163;&#25955;&#30340;&#24615;&#36136;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21644;&#26435;&#37325;&#21521;&#37327;&#25509;&#36817;&#31243;&#24230;&#30340;&#31070;&#32463;&#22349;&#22604;&#65288;NC-OOD&#65289;&#26816;&#27979;&#22120;&#26469;&#25552;&#39640;OAD&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#23545;&#20110;&#23433;&#20840;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;OOD&#26816;&#27979;&#22120;&#24212;&#35813;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#27867;&#21270;&#12290;&#20026;&#20102;&#25913;&#36827;&#29616;&#26377;OOD&#26816;&#27979;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#24230;&#28789;&#27963;&#30340;OOD&#26816;&#27979;&#22120;&#65292;&#31216;&#20026;&#31070;&#32463;&#22349;&#22604;&#65288;NC-OOD&#65289;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26222;&#36941;&#35266;&#23519;&#21040;&#30340;&#32676;&#20869;&#65288;ID&#65289;&#29305;&#24449;&#20542;&#21521;&#20110;&#24418;&#25104;&#31751;&#65292;&#32780;&#32676;&#22806;&#29305;&#24449;&#21017;&#36828;&#31163;&#30340;&#35266;&#23519;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;&#26368;&#36817;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#31070;&#32463;&#22349;&#22604;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;ID&#29305;&#24449;&#20542;&#21521;&#20110;&#22312;&#25509;&#36817;&#26435;&#37325;&#21521;&#37327;&#30340;&#20301;&#32622;&#32858;&#38598;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#25193;&#23637;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#19982;&#26435;&#37325;&#21521;&#37327;&#30340;&#25509;&#36817;&#31243;&#24230;&#26469;&#26816;&#27979;OOD&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25490;&#38500;OOD&#26679;&#26412;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;OOD&#29305;&#24449;&#20542;&#21521;&#20110;&#27604;ID&#29305;&#24449;&#26356;&#25509;&#36817;&#21407;&#28857;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;OOD&#26816;&#27979;&#26041;&#38754;&#22987;&#32456;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential for the safe deployment of AI. Particularly, OOD detectors should generalize effectively across diverse scenarios. To improve upon the generalizability of existing OOD detectors, we introduce a highly versatile OOD detector, called Neural Collapse inspired OOD detector (NC-OOD). We extend the prevalent observation that in-distribution (ID) features tend to form clusters, whereas OOD features are far away. Particularly, based on the recent observation, Neural Collapse, we further demonstrate that ID features tend to cluster in proximity to weight vectors. From our extended observation, we propose to detect OOD based on feature proximity to weight vectors. To further rule out OOD samples, we leverage the observation that OOD features tend to reside closer to the origin than ID features. Extensive experiments show that our approach enhances the generalizability of existing work and can consistently achieve state-of-the-art OOD detection per
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#21644;&#35780;&#20272;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22312;&#33014;&#24102;&#12289;&#28034;&#40486;&#21644;&#20809;&#29031;&#31561;&#19977;&#31181;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#38887;&#24615;&#65292;&#20027;&#35201;&#38024;&#23545;&#23545;&#35937;&#20998;&#31867;&#22120;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#27169;&#22411;&#22312;&#27867;&#21270;&#36755;&#20837;&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#36807;&#25311;&#21512;&#21644;&#27424;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01478</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#22522;&#20110;&#20154;&#31867;&#24863;&#30693;&#26426;&#21046;&#30340;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversary ML Resilience in Autonomous Driving Through Human Centered Perception Mechanisms. (arXiv:2311.01478v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#21644;&#35780;&#20272;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22312;&#33014;&#24102;&#12289;&#28034;&#40486;&#21644;&#20809;&#29031;&#31561;&#19977;&#31181;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#38887;&#24615;&#65292;&#20027;&#35201;&#38024;&#23545;&#23545;&#35937;&#20998;&#31867;&#22120;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#27169;&#22411;&#22312;&#27867;&#21270;&#36755;&#20837;&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#36807;&#25311;&#21512;&#21644;&#27424;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#36947;&#36335;&#26631;&#24535;&#30340;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#19981;&#26029;&#21033;&#29992;&#29616;&#20195;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#28431;&#27934;&#65292;&#22952;&#30861;&#20854;&#27491;&#30830;&#20998;&#31867;&#25152;&#36935;&#21040;&#30340;&#36947;&#36335;&#26631;&#24535;&#31867;&#22411;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#36755;&#20837;&#25968;&#25454;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#25110;&#27424;&#25311;&#21512;&#12290;&#22312;&#36807;&#25311;&#21512;&#20013;&#65292;&#27169;&#22411;&#35760;&#24518;&#36755;&#20837;&#25968;&#25454;&#20294;&#19981;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#24773;&#26223;&#12290;&#22312;&#27424;&#25311;&#21512;&#20013;&#65292;&#27169;&#22411;&#23545;&#36755;&#20837;&#25968;&#25454;&#23398;&#20064;&#19981;&#36275;&#65292;&#19981;&#33021;&#20934;&#30830;&#20998;&#31867;&#36825;&#20123;&#36947;&#36335;&#26631;&#24535;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22312;&#19977;&#31181;&#20027;&#35201;&#30340;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#65288;&#33014;&#24102;&#12289;&#28034;&#40486;&#12289;&#20809;&#29031;&#65289;&#19979;&#30340;&#38887;&#24615;&#65292;&#29305;&#21035;&#38024;&#23545;&#23545;&#35937;&#20998;&#31867;&#22120;&#12290;&#24320;&#21457;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65306;&#36947;&#36335;&#26631;&#24535;&#65288;&#20572;&#36710;&#26631;&#24535;&#12289;&#38480;&#36895;&#26631;&#24535;&#12289;&#20132;&#36890;&#28783;&#21644;&#20154;&#34892;&#27178;&#36947;&#26631;&#24535;&#65289;&#21644;&#20960;&#20309;&#24418;&#29366;&#65288;&#20843;&#36793;&#24418;&#12289;&#22278;&#24418;&#12289;&#27491;&#26041;&#24418;&#21644;&#19977;&#35282;&#24418;&#65289;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#26465;&#20214;&#19979;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical adversarial attacks on road signs are continuously exploiting vulnerabilities in modern day autonomous vehicles (AVs) and impeding their ability to correctly classify what type of road sign they encounter. Current models cannot generalize input data well, resulting in overfitting or underfitting. In overfitting, the model memorizes the input data but cannot generalize to new scenarios. In underfitting, the model does not learn enough of the input data to accurately classify these road signs. This paper explores the resilience of autonomous driving systems against three main physical adversarial attacks (tape, graffiti, illumination), specifically targeting object classifiers. Several machine learning models were developed and evaluated on two distinct datasets: road signs (stop signs, speed limit signs, traffic lights, and pedestrian crosswalk signs) and geometric shapes (octagons, circles, squares, and triangles). The study compared algorithm performance under different condi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#32858;&#21512;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65288;AMP&#65289;&#30340;&#29702;&#35770;&#22914;&#20309;&#24212;&#29992;&#20110;&#38543;&#26426;&#23398;&#20064;&#29702;&#35770;&#20013;&#65292;&#20197;&#38477;&#20302;&#32500;&#24230;&#24182;&#23454;&#29616;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2311.01476</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#23398;&#20064;&#29702;&#35770;&#20013;&#24212;&#29992;&#32858;&#21512;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Applications of the Theory of Aggregated Markov Processes in Stochastic Learning Theory. (arXiv:2311.01476v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#32858;&#21512;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65288;AMP&#65289;&#30340;&#29702;&#35770;&#22914;&#20309;&#24212;&#29992;&#20110;&#38543;&#26426;&#23398;&#20064;&#29702;&#35770;&#20013;&#65292;&#20197;&#38477;&#20302;&#32500;&#24230;&#24182;&#23454;&#29616;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20989;&#25968;&#19982;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#32452;&#21512;&#24471;&#21040;&#30340;&#38543;&#26426;&#36807;&#31243;&#34987;&#31216;&#20026;&#32858;&#21512;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65288;AMP&#65289;&#12290;&#23558;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#19982;&#20989;&#25968;&#32452;&#21512;&#30340;&#30446;&#30340;&#21487;&#20197;&#26159;&#38477;&#20302;&#32500;&#24230;&#65292;&#20363;&#22914;&#22312;&#29305;&#23450;&#22352;&#26631;&#19978;&#36827;&#34892;&#25237;&#24433;&#12290;AMP&#30340;&#29702;&#35770;&#24050;&#32463;&#34987;Dynkin&#12289;Cameron&#12289;Rogers&#21644;Pitman&#20197;&#21450;Kelly&#31561;&#20154;&#24191;&#27867;&#30740;&#31350;&#65292;&#20182;&#20204;&#25552;&#20379;&#20102;AMP&#20445;&#25345;&#39532;&#23572;&#21487;&#22827;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#22312;&#21478;&#19968;&#20010;&#26041;&#21521;&#19978;&#65292;Larget&#25552;&#20379;&#20102;AMP&#30340;&#35268;&#33539;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20110;&#39564;&#35777;&#20004;&#20010;AMP&#30340;&#31561;&#20215;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25551;&#36848;&#22914;&#20309;&#23558;AMP&#30340;&#29702;&#35770;&#24212;&#29992;&#20110;&#38543;&#26426;&#23398;&#20064;&#29702;&#35770;&#20013;&#65292;&#20197;&#23454;&#29616;&#20182;&#20204;&#22312;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#26102;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
A stochastic process that arises by composing a function with a Markov process is called an aggregated Markov process (AMP). The purpose of composing a Markov process with a function can be a reduction of dimensions, e.g., a projection onto certain coordinates. The theory around AMP has been extensively studied e.g. by Dynkin, Cameron, Rogers and Pitman, and Kelly, all of whom provided sufficient conditions for an AMP to remain Markov. In another direction, Larget provided a canonical representation for AMP, which can be used to verify the equivalence of two AMPs. The purpose of this paper is to describe how the theory of AMP can be applied to stochastic learning theory as they learn a particular task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#30340;&#28145;&#24230;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#31574;&#30053;&#65292;&#23558;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#21644;&#32463;&#20856;&#30340;&#22270;&#21106;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#35757;&#32451;&#31616;&#21333;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#22270;&#20687;&#34917;&#19969;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#22270;&#21106;&#31639;&#27861;&#36827;&#34892;&#36845;&#20195;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01475</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21106;&#30340;&#22522;&#20110;&#34917;&#19969;&#30340;&#28145;&#24230;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Patch-Based Deep Unsupervised Image Segmentation using Graph Cuts. (arXiv:2311.01475v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#30340;&#28145;&#24230;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#31574;&#30053;&#65292;&#23558;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#21644;&#32463;&#20856;&#30340;&#22270;&#21106;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#35757;&#32451;&#31616;&#21333;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#22270;&#20687;&#34917;&#19969;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#22270;&#21106;&#31639;&#27861;&#36827;&#34892;&#36845;&#20195;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#22312;&#19981;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#23558;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#35821;&#20041;&#27169;&#24335;&#20998;&#32452;&#12290;&#31867;&#20284;&#22320;&#65292;&#22270;&#20687;&#32858;&#31867;&#26681;&#25454;&#23427;&#20204;&#30340;&#35821;&#20041;&#20869;&#23481;&#25628;&#32034;&#22270;&#20687;&#30340;&#20998;&#32452;&#65292;&#20063;&#19981;&#38656;&#35201;&#30417;&#30563;&#12290;&#32463;&#20856;&#22320;&#65292;&#36825;&#20004;&#20010;&#38382;&#39064;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#20174;&#22362;&#23454;&#30340;&#25968;&#23398;&#27010;&#24565;&#20013;&#20135;&#29983;&#20102;&#20855;&#20307;&#30340;&#24212;&#29992;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#31185;&#23398;&#30028;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#20102;&#22522;&#20110;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#24456;&#23569;&#21033;&#29992;&#32463;&#20856;&#26041;&#27861;&#21462;&#24471;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#31574;&#30053;&#65292;&#23558;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#36827;&#23637;&#19982;&#32463;&#20856;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22270;&#21106;&#36827;&#34892;&#36845;&#20195;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised image segmentation aims at grouping different semantic patterns in an image without the use of human annotation. Similarly, image clustering searches for groupings of images based on their semantic content without supervision. Classically, both problems have captivated researchers as they drew from sound mathematical concepts to produce concrete applications. With the emergence of deep learning, the scientific community turned its attention to complex neural network-based solvers that achieved impressive results in those domains but rarely leveraged the advances made by classical methods. In this work, we propose a patch-based unsupervised image segmentation strategy that bridges advances in unsupervised feature extraction from deep clustering methods with the algorithmic help of classical graph-based methods. We show that a simple convolutional neural network, trained to classify image patches and iteratively regularized using graph cuts, naturally leads to a state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#23545;PAEs&#29420;&#29305;&#29305;&#24449;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2311.01473</link><description>&lt;p&gt;
&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples in the Physical World: A Survey. (arXiv:2311.01473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#23545;PAEs&#29420;&#29305;&#29305;&#24449;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23545;&#23545;&#25239;&#26679;&#26412;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#33030;&#24369;&#24615;&#12290;&#38500;&#20102;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#25915;&#20987;&#22806;&#65292;&#23545;&#25239;&#26679;&#26412;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23545;&#29289;&#29702;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#30740;&#31350;&#32570;&#20047;&#23545;&#20854;&#29420;&#29305;&#29305;&#24449;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#23548;&#33268;&#20854;&#37325;&#35201;&#24615;&#21644;&#29702;&#35299;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#35757;&#32451;&#12289;&#21046;&#36896;&#21644;&#37325;&#37319;&#26679;&#36807;&#31243;&#20013;&#20840;&#38754;&#32771;&#23519;PAEs&#30340;&#29305;&#28857;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#36890;&#36807;&#20998;&#26512;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#30830;&#23450;&#21046;&#36896;&#21644;&#37325;&#37319;&#26679;&#26159;PAEs&#20013;&#29420;&#29305;&#23646;&#24615;&#21644;&#29305;&#27530;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#65292;&#25105;&#20204;&#22522;&#20110;&#20854;&#29305;&#23450;&#29305;&#24449;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;PAEs&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#29289;&#29702;&#23545;&#25239;&#19990;&#30028;&#30740;&#31350;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated high vulnerability to adversarial examples. Besides the attacks in the digital world, the practical implications of adversarial examples in the physical world present significant challenges and safety concerns. However, current research on physical adversarial examples (PAEs) lacks a comprehensive understanding of their unique characteristics, leading to limited significance and understanding. In this paper, we address this gap by thoroughly examining the characteristics of PAEs within a practical workflow encompassing training, manufacturing, and re-sampling processes. By analyzing the links between physical adversarial attacks, we identify manufacturing and re-sampling as the primary sources of distinct attributes and particularities in PAEs. Leveraging this knowledge, we develop a comprehensive analysis and classification framework for PAEs based on their specific characteristics, covering over 100 studies on physical-world adversarial e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#21270;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#30340;&#25968;&#23398;&#24418;&#24335;&#65292;&#24314;&#31435;&#20102;&#20248;&#21270;&#30340;ClimateBERT&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#32467;&#26524;&#27604;&#36739;&#20998;&#26512;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#33391;&#22909;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2311.01469</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#29615;&#20445;&#34394;&#20551;&#23459;&#20256;
&lt;/p&gt;
&lt;p&gt;
Leveraging Language Models to Detect Greenwashing. (arXiv:2311.01469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#21270;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#30340;&#25968;&#23398;&#24418;&#24335;&#65292;&#24314;&#31435;&#20102;&#20248;&#21270;&#30340;ClimateBERT&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#32467;&#26524;&#27604;&#36739;&#20998;&#26512;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#33391;&#22909;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#27668;&#20505;&#21464;&#21270;&#30340;&#21518;&#26524;&#36234;&#26469;&#36234;&#24341;&#36215;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#20225;&#19994;&#22312;&#21487;&#25345;&#32493;&#21457;&#23637;&#25253;&#21578;&#20013;&#24378;&#35843;&#20854;&#29615;&#20445;&#21162;&#21147;&#20197;&#22686;&#24378;&#20844;&#20247;&#24418;&#35937;&#12290;&#28982;&#32780;&#65292;&#23545;&#27492;&#31867;&#25253;&#21578;&#30340;&#23457;&#26680;&#32570;&#20047;&#20005;&#26684;&#30340;&#30417;&#31649;&#65292;&#21487;&#33021;&#23548;&#33268;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#36827;&#34892;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#26469;&#37327;&#21270;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#35813;&#38382;&#39064;&#30340;&#20248;&#21270;ClimateBERT&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#32467;&#26524;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#21487;&#25345;&#32493;&#21457;&#23637;&#25253;&#21578;&#30340;&#27979;&#35797;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#23454;&#29616;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;86.34%&#21644;F1&#20540;0.67&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#25506;&#32034;&#30340;&#33391;&#22909;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, climate change repercussions have increasingly captured public interest. Consequently, corporations are emphasizing their environmental efforts in sustainability reports to bolster their public image. Yet, the absence of stringent regulations in review of such reports allows potential greenwashing. In this study, we introduce a novel methodology to train a language model on generated labels for greenwashing risk. Our primary contributions encompass: developing a mathematical formulation to quantify greenwashing risk, a fine-tuned ClimateBERT model for this problem, and a comparative analysis of results. On a test set comprising of sustainability reports, our best model achieved an average accuracy score of 86.34% and F1 score of 0.67, demonstrating that our methods show a promising direction of exploration for this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#27169;&#25311;&#26426;&#22120;&#20154;&#21046;&#23450;&#35745;&#21010;&#65292;&#22312;ScienceWorld&#20013;&#23454;&#29616;30&#31867;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#22312;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;1.4&#20493;&#65292;&#24403;&#22635;&#20805;&#23613;&#21487;&#33021;&#22810;&#30340;&#20808;&#21069;&#27493;&#39588;&#26102;&#25552;&#39640;&#21040;3.5&#20493;&#65292;&#21363;&#20351;&#21482;&#35757;&#32451;&#20102;6.5%&#30340;&#25968;&#25454;&#65292;&#20063;&#27604;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;2.2&#20493;&#12290;&#19981;&#21516;&#31867;&#21035;&#30340;&#21160;&#20316;&#34920;&#29616;&#24046;&#24322;&#24456;&#22823;&#65292;&#35828;&#26126;&#24179;&#22343;&#20219;&#21153;&#21487;&#33021;&#20250;&#38544;&#34255;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01468</link><description>&lt;p&gt;
&#35760;&#20303;&#20320;&#25152;&#20570;&#30340;&#65292;&#36825;&#26679;&#20320;&#23601;&#30693;&#36947;&#25509;&#19979;&#26469;&#35813;&#20570;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remember what you did so you know what to do next. (arXiv:2311.01468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#27169;&#25311;&#26426;&#22120;&#20154;&#21046;&#23450;&#35745;&#21010;&#65292;&#22312;ScienceWorld&#20013;&#23454;&#29616;30&#31867;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#22312;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;1.4&#20493;&#65292;&#24403;&#22635;&#20805;&#23613;&#21487;&#33021;&#22810;&#30340;&#20808;&#21069;&#27493;&#39588;&#26102;&#25552;&#39640;&#21040;3.5&#20493;&#65292;&#21363;&#20351;&#21482;&#35757;&#32451;&#20102;6.5%&#30340;&#25968;&#25454;&#65292;&#20063;&#27604;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;2.2&#20493;&#12290;&#19981;&#21516;&#31867;&#21035;&#30340;&#21160;&#20316;&#34920;&#29616;&#24046;&#24322;&#24456;&#22823;&#65292;&#35828;&#26126;&#24179;&#22343;&#20219;&#21153;&#21487;&#33021;&#20250;&#38544;&#34255;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#19968;&#20010;&#20013;&#31561;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-J 6B&#21442;&#25968;&#65289;&#65292;&#20026;&#27169;&#25311;&#26426;&#22120;&#20154;&#22312;ScienceWorld&#20013;&#23454;&#29616;30&#31867;&#30446;&#26631;&#65288;&#19968;&#20010;&#29992;&#20110;&#23567;&#23398;&#31185;&#23398;&#23454;&#39564;&#30340;&#25991;&#26412;&#28216;&#25103;&#27169;&#25311;&#22120;&#65289;&#21046;&#23450;&#35745;&#21010;&#12290;&#20808;&#21069;&#21457;&#34920;&#30340;&#23454;&#35777;&#30740;&#31350;&#22768;&#31216;&#65292;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36866;&#29992;&#24615;&#36739;&#24046;&#65288;Wang&#31561;&#65292;2022&#65289;&#12290;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#65288;&#21333;&#20010;&#21069;&#19968;&#20010;&#27493;&#39588;&#65289;&#65292;LLM&#30340;&#24615;&#33021;&#36229;&#36807;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;1.4&#20493;&#12290;&#24403;&#25105;&#20204;&#23613;&#21487;&#33021;&#22810;&#22320;&#22635;&#20805;LLM&#30340;&#36755;&#20837;&#32531;&#20914;&#21306;&#26102;&#65292;&#25913;&#36827;&#25928;&#26524;&#25552;&#39640;&#21040;3.5&#20493;&#12290;&#21363;&#20351;&#21482;&#23545;6.5%&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;2.2&#20493;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#23545;&#20110;30&#31867;&#21160;&#20316;&#65292;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#65292;&#34920;&#26126;&#23545;&#20219;&#21153;&#36827;&#34892;&#24179;&#22343;&#21487;&#33021;&#20250;&#38544;&#34255;&#26174;&#33879;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#19982;&#25105;&#20204;&#21516;&#26102;&#36827;&#34892;&#30340;Lin&#31561;&#20154;&#65288;2023&#65289;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#19968;&#31181;&#20004;&#37096;&#20998;&#26041;&#27861;&#65288;SwiftSa&#65289;
&lt;/p&gt;
&lt;p&gt;
We explore using a moderately sized large language model (GPT-J 6B parameters) to create a plan for a simulated robot to achieve 30 classes of goals in ScienceWorld, a text game simulator for elementary science experiments. Previously published empirical work claimed that large language models (LLMs) are a poor fit (Wang et al., 2022) compared to reinforcement learning. Using the Markov assumption (a single previous step), the LLM outperforms the reinforcement learning-based approach by a factor of 1.4. When we fill the LLM's input buffer with as many prior steps as possible, improvement rises to 3.5x. Even when training on only 6.5% of the training data, we observe a 2.2x improvement over the reinforcement-learning-based approach. Our experiments show that performance varies widely across the 30 classes of actions, indicating that averaging over tasks can hide significant performance issues. In work contemporaneous with ours, Lin et al. (2023) demonstrated a two-part approach (SwiftSa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#21019;&#24314;&#21487;&#38752;&#12289;&#21487;&#20449;&#21644;&#26080;&#20559;&#32622;&#30340;LLM&#27169;&#22411;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#30528;&#37325;&#20110;&#37327;&#21270;&#12289;&#39564;&#35777;&#21644;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;LLM&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2311.01463</link><description>&lt;p&gt;
&#25903;&#25345;&#21487;&#20449;&#24230;&#30340;LLM&#21019;&#24314;&#36807;&#31243;&#65306;&#22788;&#29702;&#21307;&#30103;AI&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI. (arXiv:2311.01463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01463
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#21019;&#24314;&#21487;&#38752;&#12289;&#21487;&#20449;&#21644;&#26080;&#20559;&#32622;&#30340;LLM&#27169;&#22411;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#30528;&#37325;&#20110;&#37327;&#21270;&#12289;&#39564;&#35777;&#21644;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;LLM&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30701;&#26102;&#38388;&#20869;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36805;&#36895;&#22686;&#22810;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20934;&#30830;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#24187;&#35273;&#31561;&#38382;&#39064;&#65292;&#21307;&#30103;&#39046;&#22495;&#23545;&#20854;&#37319;&#29992;&#23384;&#22312;&#29369;&#35947;&#12290;&#37492;&#20110;&#21307;&#30103;&#20107;&#20851;&#37325;&#22823;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#29978;&#33267;&#25552;&#20986;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#20043;&#21069;&#19981;&#24212;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21019;&#24314;&#21487;&#38752;&#12289;&#21487;&#20449;&#21644;&#26080;&#20559;&#32622;&#27169;&#22411;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#36825;&#26159;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#24212;&#29992;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#22312;&#21307;&#30103;&#32972;&#26223;&#19979;&#23545;&#24187;&#35273;&#36827;&#34892;&#37327;&#21270;&#12289;&#39564;&#35777;&#21644;&#32531;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLM&#22312;&#21307;&#30103;&#39046;&#22495;&#26410;&#26469;&#30340;&#21487;&#33021;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have proliferated across multiple domains in as short period of time. There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations. Give the high stakes nature of healthcare, many researchers have even cautioned against its usage until these issues are resolved. The key to the implementation and deployment of LLMs in healthcare is to make these models trustworthy, transparent (as much possible) and explainable. In this paper we describe the key elements in creating reliable, trustworthy, and unbiased models as a necessary condition for their adoption in healthcare. Specifically we focus on the quantification, validation, and mitigation of hallucinations in the context in healthcare. Lastly, we discuss how the future of LLMs in healthcare may look like.
&lt;/p&gt;</description></item><item><title>FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2311.01282</link><description>&lt;p&gt;
FlashDecoding++: &#22312;GPU&#19978;&#21152;&#36895;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26356;&#24555;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01282
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#26410;&#35299;&#20915;&#65306;(1) &#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12290;softmax&#25805;&#20316;&#38656;&#35201;&#21516;&#27493;&#26356;&#26032;&#27599;&#20010;&#37096;&#20998;softmax&#32467;&#26524;&#65292;&#23548;&#33268;LLM&#20013;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#24320;&#38144;&#22686;&#21152;&#32422;20%&#12290;(2) &#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#12290;&#22312;LLM&#25512;&#29702;&#20013;&#25191;&#34892;GEMM&#30340;&#30697;&#38453;&#24418;&#29366;&#26159;&#25153;&#24179;&#30340;&#65292;&#23548;&#33268;&#22312;&#20808;&#21069;&#30340;&#35774;&#35745;&#20013;&#22635;&#20805;&#38646;&#21518;&#35745;&#31639;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#24615;&#33021;&#25439;&#22833;&#36229;&#36807;50%&#12290;(3) &#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;LLM&#20013;&#30340;&#20869;&#26680;&#24615;&#33021;&#21462;&#20915;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#25968;&#25454;&#29305;&#24449;&#12289;&#30828;&#20214;&#37197;&#32622;&#31561;&#12290;&#21333;&#19968;&#21644;&#38745;&#24577;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#23548;&#33268;LLM&#25512;&#29702;&#20013;&#19981;&#21516;&#24418;&#29366;&#30340;GEMM&#30340;&#24615;&#33021;&#25439;&#22833;&#36798;&#21040;50.25%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FlashDecoding++&#65292;&#19968;&#31181;&#24555;&#36895;&#25903;&#25345;&#20027;&#27969;LLM&#21644;&#30828;&#20214;&#21518;&#31471;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;FlashDecoding++&#23454;&#29616;&#20102;&#20197;&#19979;&#30446;&#26631;&#65306;
&lt;/p&gt;
&lt;p&gt;
As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and &gt;50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#12289;&#29983;&#25104;&#27807;&#36890;&#20505;&#36873;&#20197;&#21450;&#27169;&#25311;&#21463;&#20247;&#21453;&#24212;&#65292;&#26469;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;&#12290;&#36890;&#36807;&#35780;&#20272;&#20843;&#20010;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#22522;&#26412;&#36807;&#31243;&#30340;&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00687</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#21463;&#20247;&#32676;&#20307;&#65292;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#12289;&#29983;&#25104;&#27807;&#36890;&#20505;&#36873;&#20197;&#21450;&#27169;&#25311;&#21463;&#20247;&#21453;&#24212;&#65292;&#26469;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;&#12290;&#36890;&#36807;&#35780;&#20272;&#20843;&#20010;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#22522;&#26412;&#36807;&#31243;&#30340;&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#19982;&#20182;&#20154;&#36827;&#34892;&#27807;&#36890;&#20197;&#23454;&#29616;&#33258;&#24049;&#30340;&#30446;&#26631;&#65311;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#32463;&#39564;&#25110;&#20182;&#20154;&#30340;&#24314;&#35758;&#65292;&#25110;&#32773;&#36890;&#36807;&#39044;&#27979;&#23545;&#26041;&#30340;&#21453;&#24212;&#26469;&#26500;&#36896;&#20505;&#36873;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#26159;&#26377;&#38480;&#21644;&#26377;&#20559;&#35265;&#30340;&#65292;&#32780;&#19988;&#23545;&#28508;&#22312;&#32467;&#26524;&#36827;&#34892;&#25512;&#29702;&#21487;&#33021;&#26159;&#22256;&#38590;&#19988;&#35748;&#30693;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#26469;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#27807;&#36890;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25506;&#32034;-&#29983;&#25104;-&#27169;&#25311;&#65288;EGS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25509;&#21463;&#20219;&#20309;&#19968;&#20010;&#20010;&#20307;&#19982;&#19968;&#20010;&#30446;&#26631;&#21463;&#20247;&#36827;&#34892;&#27807;&#36890;&#30340;&#22330;&#26223;&#20316;&#20026;&#36755;&#20837;&#12290;EGS&#65288;1&#65289;&#36890;&#36807;&#29983;&#25104;&#19982;&#22330;&#26223;&#30456;&#20851;&#30340;&#22810;&#26679;&#21270;&#24314;&#35758;&#26469;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#65292;&#65288;2&#65289;&#29983;&#25104;&#20197;&#37096;&#20998;&#24314;&#35758;&#20026;&#26465;&#20214;&#30340;&#27807;&#36890;&#20505;&#36873;&#65292;&#65288;3&#65289;&#27169;&#25311;&#19981;&#21516;&#21463;&#20247;&#30340;&#21453;&#24212;&#65292;&#20197;&#30830;&#23450;&#26368;&#20339;&#20505;&#36873;&#21644;&#24314;&#35758;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#21313;&#20010;&#22522;&#26412;&#36807;&#31243;&#30340;&#20843;&#20010;&#22330;&#26223;&#19978;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38598;&#25104;&#25216;&#26415;&#25193;&#23637;&#20102;DeepONet&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#39640;&#36229;&#22768;&#36895;&#27969;&#21160;&#29615;&#22659;&#20013;&#23545;&#26410;&#30693;&#36755;&#20837;&#36827;&#34892;&#21487;&#38752;&#21644;&#32622;&#20449;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2311.00060</link><description>&lt;p&gt;
&#38598;&#25104;&#27169;&#22411;&#32988;&#36807;&#21333;&#19968;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#39640;&#36229;&#22768;&#36895;&#27969;&#21160;&#30340;&#25805;&#20316;&#23398;&#20064;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ensemble models outperform single model uncertainties and predictions for operator-learning of hypersonic flows. (arXiv:2311.00060v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38598;&#25104;&#25216;&#26415;&#25193;&#23637;&#20102;DeepONet&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#39640;&#36229;&#22768;&#36895;&#27969;&#21160;&#29615;&#22659;&#20013;&#23545;&#26410;&#30693;&#36755;&#20837;&#36827;&#34892;&#21487;&#38752;&#21644;&#32622;&#20449;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20445;&#30495;&#30340;&#35745;&#31639;&#27169;&#25311;&#21644;&#29289;&#29702;&#23454;&#39564;&#23545;&#20110;&#39640;&#36229;&#22768;&#36895;&#27969;&#21160;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#12290;&#22312;&#26377;&#38480;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#19978;&#35757;&#32451;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#27169;&#22411;&#26159;&#36805;&#36895;&#39044;&#27979;&#26410;&#35265;&#24773;&#20917;&#19979;&#34892;&#20026;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#39640;&#20445;&#30495;&#25968;&#25454;&#26412;&#36523;&#30340;&#25968;&#37327;&#26377;&#38480;&#65292;&#26080;&#27861;&#39564;&#35777;SciML&#27169;&#22411;&#22312;&#26410;&#25506;&#32034;&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20840;&#37096;&#36755;&#20986;&#12290;&#22240;&#27492;&#65292;&#24076;&#26395;&#33021;&#22815;&#33719;&#24471;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;SciML&#27169;&#22411;&#12290;&#28982;&#21518;&#21487;&#20197;&#20351;&#29992;SciML&#27169;&#22411;&#30340;&#36755;&#20986;&#19981;&#30830;&#23450;&#24615;&#26469;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#32622;&#20449;&#24230;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26426;&#21046;&#65288;&#22343;&#20540;-&#26041;&#24046;&#20272;&#35745;&#65292;&#35777;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#38598;&#25104;&#65289;&#26469;&#25193;&#23637;DeepONet&#12290;&#36825;&#20123;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#30340;DeepONet&#27169;&#22411;&#22312;&#39640;&#36229;&#22768;&#36895;&#27969;&#21160;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20135;&#29983;&#30340;&#25968;&#25454;&#65292;&#23545;&#19968;&#20010;&#38045;&#38181;&#20307;&#23545;&#35937;&#21608;&#22260;&#30340;&#27969;&#21160;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#38598;&#25104;&#25216;&#26415;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
High-fidelity computational simulations and physical experiments of hypersonic flows are resource intensive. Training scientific machine learning (SciML) models on limited high-fidelity data offers one approach to rapidly predict behaviors for situations that have not been seen before. However, high-fidelity data is itself in limited quantity to validate all outputs of the SciML model in unexplored input space. As such, an uncertainty-aware SciML model is desired. The SciML model's output uncertainties could then be used to assess the reliability and confidence of the model's predictions. In this study, we extend a DeepONet using three different uncertainty quantification mechanisms: mean-variance estimation, evidential uncertainty, and ensembling. The uncertainty aware DeepONet models are trained and evaluated on the hypersonic flow around a blunt cone object with data generated via computational fluid dynamics over a wide range of Mach numbers and altitudes. We find that ensembling o
&lt;/p&gt;</description></item><item><title>CapsFusion&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.20550</link><description>&lt;p&gt;
CapsFusion: &#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20550
&lt;/p&gt;
&lt;p&gt;
CapsFusion&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#25191;&#34892;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26174;&#33879;&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#35268;&#27169;&#22522;&#20110;&#32593;&#32476;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#22312;&#36825;&#19968;&#25104;&#21151;&#20013;&#36215;&#30528;&#26681;&#26412;&#24615;&#30340;&#36129;&#29486;&#65292;&#20294;&#23384;&#22312;&#30528;&#36807;&#22810;&#30340;&#22122;&#22768;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#30001;&#29983;&#25104;&#24335;&#23383;&#24149;&#27169;&#22411;&#21512;&#25104;&#30340;&#26367;&#20195;&#23383;&#24149;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22522;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#21512;&#25104;&#23383;&#24149;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#21487;&#25193;&#23637;&#24615;&#19981;&#36275;&#21644;&#19990;&#30028;&#30693;&#35782;&#20007;&#22833;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#20854;&#21021;&#22987;&#22522;&#20934;&#25104;&#21151;&#20013;&#22823;&#37096;&#20998;&#34987;&#25513;&#30422;&#20102;&#12290;&#32463;&#36807;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#26159;&#29616;&#26377;&#21512;&#25104;&#23383;&#24149;&#20013;&#36807;&#20110;&#31616;&#21270;&#30340;&#35821;&#35328;&#32467;&#26500;&#21644;&#32570;&#20047;&#30693;&#35782;&#32454;&#33410;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CapsFusion&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dropout&#25216;&#26415;&#26469;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#20013;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#22686;&#38271;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;PPO&#31639;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;D-PPO&#31639;&#27861;&#30456;&#36739;&#20110;PPO&#31639;&#27861;&#22312;Atari 2600&#28216;&#25103;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.20380</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Dropout&#31574;&#30053;&#65306;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#20013;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#22686;&#38271;
&lt;/p&gt;
&lt;p&gt;
Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods. (arXiv:2310.20380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dropout&#25216;&#26415;&#26469;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#20013;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#22686;&#38271;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;PPO&#31639;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;D-PPO&#31639;&#27861;&#30456;&#36739;&#20110;PPO&#31639;&#27861;&#22312;Atari 2600&#28216;&#25103;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20854;&#20013;&#65292;&#20027;&#27969;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#22914;PPO&#21644;TRPO&#24341;&#20837;&#20102;&#37325;&#35201;&#24615;&#37319;&#26679;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#20801;&#35768;&#37325;&#29992;&#21382;&#21490;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#22686;&#21152;&#65292;&#38388;&#25509;&#24433;&#21709;&#20102;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#19978;&#30028;&#65292;&#23427;&#21487;&#20197;&#38543;&#26367;&#20195;&#30446;&#26631;&#30340;&#22686;&#21152;&#32780;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Dropout&#25216;&#26415;&#65292;&#20197;&#36991;&#20813;&#37325;&#35201;&#24615;&#37319;&#26679;&#24341;&#36215;&#30340;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#36807;&#24230;&#22686;&#21152;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20027;&#27969;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#23558;Dropout&#25216;&#26415;&#24212;&#29992;&#20110;PPO&#31639;&#27861;&#65292;&#24471;&#21040;&#20102;D-PPO&#21464;&#20307;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;Atari 2600&#28216;&#25103;&#19978;&#23545;D-PPO&#21644;PPO&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy-based reinforcement learning algorithms are widely used in various fields. Among them, mainstream policy optimization algorithms such as PPO and TRPO introduce importance sampling into reinforcement learning, which allows the reuse of historical data. However, this also results in high variance of the surrogate objective and indirectly affects the stability and convergence of the algorithm. In this paper, we first derived an upper bound of the variance of the surrogate objective, which can grow quadratically with the increase of the surrogate objective. Next, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling. Then, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#25512;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Lipschitz&#27491;&#21017;&#21270;&#12289;&#25439;&#22833;&#21152;&#26435;&#21644;&#20449;&#20219;&#21306;&#22495;&#37325;&#26032;&#21327;&#35843;&#65292;&#20197;&#20943;&#23567;&#22312;&#28508;&#22312;&#31354;&#38388;&#21644;&#30446;&#26631;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;&#22810;&#20010;&#20248;&#21270;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20258</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#25512;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Advancing Bayesian Optimization via Learning Correlated Latent Space. (arXiv:2310.20258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#25512;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Lipschitz&#27491;&#21017;&#21270;&#12289;&#25439;&#22833;&#21152;&#26435;&#21644;&#20449;&#20219;&#21306;&#22495;&#37325;&#26032;&#21327;&#35843;&#65292;&#20197;&#20943;&#23567;&#22312;&#28508;&#22312;&#31354;&#38388;&#21644;&#30446;&#26631;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;&#22810;&#20010;&#20248;&#21270;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#26377;&#38480;&#30340;&#20989;&#25968;&#35780;&#20272;&#26469;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65289;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#23545;&#32467;&#26500;&#21270;&#25110;&#31163;&#25955;&#25968;&#25454;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20248;&#21270;&#19981;&#26159;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#65292;&#36825;&#23548;&#33268;&#20102;&#28508;&#22312;&#30340;&#24046;&#36317;&#65292;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20851;&#28508;&#22312;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CoBO&#65289;&#65292;&#23427;&#19987;&#27880;&#20110;&#23398;&#20064;&#30456;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20854;&#29305;&#28857;&#26159;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#21644;&#30446;&#26631;&#20989;&#25968;&#20869;&#30340;&#36317;&#31163;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;Lipschitz&#27491;&#21017;&#21270;&#12289;&#25439;&#22833;&#21152;&#26435;&#21644;&#20449;&#20219;&#21306;&#22495;&#37325;&#26032;&#21327;&#35843;&#65292;&#20197;&#26368;&#23567;&#21270;&#26377;&#24076;&#26395;&#21306;&#22495;&#21608;&#22260;&#30340;&#28508;&#22312;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20248;&#21270;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a powerful method for optimizing black-box functions with limited function evaluations. Recent works have shown that optimization in a latent space through deep generative models such as variational autoencoders leads to effective and efficient Bayesian optimization for structured or discrete data. However, as the optimization does not take place in the input space, it leads to an inherent gap that results in potentially suboptimal solutions. To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in disc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#38750;&#32447;&#24615;&#28040;&#24687;&#20256;&#36882;&#30340;&#35268;&#33539;&#31561;&#21464;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#32593;&#26684;&#19978;&#24314;&#27169;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#34920;&#38754;PDE&#65292;&#23454;&#29616;&#20102;&#27604;&#21367;&#31215;&#25110;&#27880;&#24847;&#21147;&#32593;&#32476;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19589</link><description>&lt;p&gt;
&#20351;&#29992;&#27979;&#22320;&#32447;&#31561;&#21464;&#38750;&#32447;&#24615;&#28040;&#24687;&#20256;&#36882;&#23545;&#32593;&#26684;&#36827;&#34892;&#21160;&#21147;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message Passing. (arXiv:2310.19589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19589
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#38750;&#32447;&#24615;&#28040;&#24687;&#20256;&#36882;&#30340;&#35268;&#33539;&#31561;&#21464;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#32593;&#26684;&#19978;&#24314;&#27169;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#34920;&#38754;PDE&#65292;&#23454;&#29616;&#20102;&#27604;&#21367;&#31215;&#25110;&#27880;&#24847;&#21147;&#32593;&#32476;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24503;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#65292;&#36890;&#24120;&#20197;&#34920;&#38754;&#32593;&#26684;&#30340;&#24418;&#24335;&#31163;&#25955;&#21270;&#65292;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#12289;&#29983;&#29289;&#21644;&#29289;&#29702;&#31995;&#32479;&#20013;&#33258;&#28982;&#20135;&#29983;&#12290;&#29305;&#21035;&#26159;&#65292;&#27969;&#24418;&#19978;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20005;&#37325;&#20381;&#36182;&#20110;&#24213;&#23618;&#20960;&#20309;&#32467;&#26500;&#12290;&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;PDE&#65292;&#20294;&#23427;&#20204;&#19981;&#21253;&#21547;&#34920;&#38754;&#20960;&#20309;&#32467;&#26500;&#65292;&#20063;&#19981;&#32771;&#34385;&#27969;&#24418;&#30340;&#23616;&#37096;&#35268;&#33539;&#23545;&#31216;&#24615;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#26368;&#36817;&#22312;&#32593;&#26684;&#19978;&#30340;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#21644;&#27880;&#24847;&#21147;&#26550;&#26500;&#30340;&#30740;&#31350;&#21033;&#29992;&#24213;&#23618;&#20960;&#20309;&#32467;&#26500;&#65292;&#20294;&#22312;&#24314;&#27169;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#34920;&#38754;PDE&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#38750;&#32447;&#24615;&#28040;&#24687;&#20256;&#36882;&#30340;&#35268;&#33539;&#31561;&#21464;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26032;&#26550;&#26500;&#22312;&#20855;&#26377;&#39640;&#24230;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#22495;&#19978;&#27604;&#21367;&#31215;&#25110;&#27880;&#24847;&#21147;&#32593;&#32476;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#38750;&#32593;&#26684;&#24773;&#20917;&#31867;&#20284;&#65292;&#35774;&#35745;&#19978;&#30340;&#26435;&#34913;&#26356;&#20542;&#21521;&#20110;&#21367;&#31215;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data over non-Euclidean manifolds, often discretized as surface meshes, naturally arise in computer graphics and biological and physical systems. In particular, solutions to partial differential equations (PDEs) over manifolds depend critically on the underlying geometry. While graph neural networks have been successfully applied to PDEs, they do not incorporate surface geometry and do not consider local gauge symmetries of the manifold. Alternatively, recent works on gauge equivariant convolutional and attentional architectures on meshes leverage the underlying geometry but underperform in modeling surface PDEs with complex nonlinear dynamics. To address these issues, we introduce a new gauge equivariant architecture using nonlinear message passing. Our novel architecture achieves higher performance than either convolutional or attentional networks on domains with highly complex and nonlinear dynamics. However, similar to the non-mesh case, design trade-offs favor convolutional, atten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20844;&#24335;&#30340;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#26469;&#35843;&#33410;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#25351;&#23450;&#20505;&#36873;&#30340;&#955;&#24182;&#19988;&#22312;&#22823;&#26679;&#26412;&#19979;&#21487;&#20197;&#25214;&#21040;&#21807;&#19968;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.18860</link><description>&lt;p&gt;
Bayes&#25112;&#32988;&#20132;&#21449;&#39564;&#35777;&#65306;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23725;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Bayes beats Cross Validation: Efficient and Accurate Ridge Regression via Expectation Maximization. (arXiv:2310.18860v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20844;&#24335;&#30340;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#26469;&#35843;&#33410;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#25351;&#23450;&#20505;&#36873;&#30340;&#955;&#24182;&#19988;&#22312;&#22823;&#26679;&#26412;&#19979;&#21487;&#20197;&#25214;&#21040;&#21807;&#19968;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35843;&#33410;&#23725;&#22238;&#24402;&#30340;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#955;&#65292;&#35813;&#26041;&#27861;&#30340;&#35745;&#31639;&#36895;&#24230;&#27604;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;(LOOCV)&#24555;&#65292;&#21516;&#26102;&#22312;&#31232;&#30095;&#21327;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#19982;LOOCV&#30456;&#31561;&#25110;&#26356;&#22909;&#30340;&#22238;&#24402;&#21442;&#25968;&#20272;&#35745;&#12290;&#23545;&#20110;&#26377;&#38480;&#30340;n&#65292;LOOCV&#39118;&#38505;&#21487;&#33021;&#21463;&#21040;&#22810;&#20010;&#21644;&#19981;&#22909;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#25351;&#23450;&#19968;&#32452;&#20505;&#36873;&#30340;&#955;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#33391;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36275;&#22815;&#22823;&#30340;n&#19979;&#21487;&#20197;&#25214;&#21040;&#21807;&#19968;&#30340;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#25351;&#23450;&#20219;&#20309;&#38590;&#20197;&#30830;&#23450;&#30340;&#36229;&#21442;&#25968;&#12290;&#36825;&#26159;&#22522;&#20110;&#23725;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#20844;&#24335;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#36275;&#22815;&#22823;&#30340;n&#65292;&#21518;&#39564;&#26159;&#21333;&#23792;&#30340;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#26368;&#20248;&#30340;&#955;&#21644;&#22238;&#24402;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method for tuning the regularization hyper-parameter, $\lambda$, of a ridge regression that is faster to compute than leave-one-out cross-validation (LOOCV) while yielding estimates of the regression parameters of equal, or particularly in the setting of sparse covariates, superior quality to those obtained by minimising the LOOCV risk. The LOOCV risk can suffer from multiple and bad local minima for finite $n$ and thus requires the specification of a set of candidate $\lambda$, which can fail to provide good solutions. In contrast, we show that the proposed method is guaranteed to find a unique optimal solution for large enough $n$, under relatively mild conditions, without requiring the specification of any difficult to determine hyper-parameters. This is based on a Bayesian formulation of ridge regression that we prove to have a unimodal posterior for large enough $n$, allowing for both the optimal $\lambda$ and the regression coefficients to be jointly learned wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.18144</link><description>&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#36827;&#20869;&#22312;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#22870;&#21169;&#36890;&#36807;&#23450;&#20041;&#33258;&#23450;&#20041;&#30340;&#20869;&#22312;&#30446;&#26631;&#26469;&#24341;&#23548;&#38271;&#26399;&#25506;&#32034;&#12290;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#20351;&#29992;&#29366;&#24577;&#35775;&#38382;&#39057;&#29575;&#26469;&#33719;&#24471;&#25506;&#32034;&#22870;&#21169;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20219;&#20309;&#20174;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#37117;&#26159;&#38750;&#22266;&#23450;&#30340;&#65292;&#22240;&#27492;&#20026;&#20195;&#29702;&#20154;&#26500;&#24314;&#20102;&#19968;&#20010;&#38590;&#20197;&#20248;&#21270;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;&#22686;&#24378;&#29366;&#24577;&#34920;&#31034;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#25506;&#32034;&#30340;&#22266;&#23450;&#30446;&#26631;&#65288;SOFE&#65289;&#26694;&#26550;&#12290;SOFE&#38656;&#35201;&#35782;&#21035;&#19981;&#21516;&#25506;&#32034;&#22870;&#21169;&#30340;&#36275;&#22815;&#32479;&#35745;&#37327;&#65292;&#24182;&#25214;&#21040;&#19968;&#31181;&#23558;&#36825;&#20123;&#32479;&#35745;&#37327;&#39640;&#25928;&#32534;&#30721;&#20316;&#20026;&#28145;&#24230;&#32593;&#32476;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;SOFE&#22522;&#20110;&#25552;&#20986;&#25193;&#23637;&#29366;&#24577;&#31354;&#38388;&#30340;&#29366;&#24577;&#22686;&#24378;&#65292;&#20294;&#26377;&#24076;&#26395;&#31616;&#21270;&#20195;&#29702;&#30446;&#26631;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SOFE&#25913;&#21892;&#20102;&#25506;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#23398;&#20064;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#25805;&#20316;&#21407;&#35821;&#65292;&#24182;&#21033;&#29992;&#29615;&#22659;&#25913;&#21464;&#29289;&#20307;&#23039;&#24577;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#22312;&#30446;&#26631;&#29289;&#20307;&#25235;&#21462;&#34987;&#29615;&#22659;&#36974;&#25377;&#26102;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17785</link><description>&lt;p&gt;
&#20351;&#29992;&#21442;&#25968;&#21270;&#25805;&#20316;&#21407;&#35821;&#23398;&#20064;&#22806;&#22312;&#28789;&#24039;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Extrinsic Dexterity with Parameterized Manipulation Primitives. (arXiv:2310.17785v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17785
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#23398;&#20064;&#19968;&#31995;&#21015;&#21442;&#25968;&#21270;&#25805;&#20316;&#21407;&#35821;&#65292;&#24182;&#21033;&#29992;&#29615;&#22659;&#25913;&#21464;&#29289;&#20307;&#23039;&#24577;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#22312;&#30446;&#26631;&#29289;&#20307;&#25235;&#21462;&#34987;&#29615;&#22659;&#36974;&#25377;&#26102;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#30456;&#20851;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#38382;&#39064;&#37117;&#28041;&#21450;&#21040;&#30446;&#26631;&#29289;&#20307;&#65292;&#20854;&#25152;&#26377;&#25235;&#21462;&#37117;&#34987;&#29615;&#22659;&#36974;&#25377;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21333;&#27425;&#25235;&#21462;&#35745;&#21010;&#24635;&#26159;&#22833;&#36133;&#30340;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#39318;&#20808;&#23558;&#29289;&#20307;&#25805;&#20316;&#21040;&#19968;&#20010;&#36866;&#21512;&#36827;&#34892;&#25235;&#21462;&#30340;&#37197;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#31995;&#21015;&#21033;&#29992;&#29615;&#22659;&#25913;&#21464;&#29289;&#20307;&#23039;&#24577;&#30340;&#21160;&#20316;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26469;&#32467;&#21512;&#19968;&#31995;&#21015;&#23398;&#20064;&#21040;&#30340;&#21442;&#25968;&#21270;&#25805;&#20316;&#21407;&#35821;&#12290;&#36890;&#36807;&#23398;&#20064;&#20302;&#32423;&#25805;&#20316;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#29289;&#20307;&#12289;&#22841;&#20855;&#21644;&#29615;&#22659;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#25511;&#21046;&#29289;&#20307;&#30340;&#29366;&#24577;&#12290;&#22312;&#26080;&#25511;&#21046;&#26465;&#20214;&#19979;&#20998;&#26512;&#35774;&#35745;&#36825;&#26679;&#19968;&#20010;&#22797;&#26434;&#34892;&#20026;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#20998;&#26512;&#26041;&#27861;&#38656;&#35201;&#20934;&#30830;&#24314;&#27169;&#20114;&#21160;&#21644;&#25509;&#35302;&#21160;&#21147;&#23398;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#22312;&#28145;&#24230;&#22270;&#20687;&#19978;&#30452;&#25509;&#25805;&#20316;&#30340;&#20998;&#23618;&#31574;&#30053;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many practically relevant robot grasping problems feature a target object for which all grasps are occluded, e.g., by the environment. Single-shot grasp planning invariably fails in such scenarios. Instead, it is necessary to first manipulate the object into a configuration that affords a grasp. We solve this problem by learning a sequence of actions that utilize the environment to change the object's pose. Concretely, we employ hierarchical reinforcement learning to combine a sequence of learned parameterized manipulation primitives. By learning the low-level manipulation policies, our approach can control the object's state through exploiting interactions between the object, the gripper, and the environment. Designing such a complex behavior analytically would be infeasible under uncontrolled conditions, as an analytic approach requires accurate physical modeling of the interaction and contact dynamics. In contrast, we learn a hierarchical policy model that operates directly on depth
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;</title><link>http://arxiv.org/abs/2310.17688</link><description>&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#26102;&#20195;&#31649;&#29702;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17688
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#20849;&#35782;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#22823;&#35268;&#27169;&#30340;&#31038;&#20250;&#21361;&#23475;&#21644;&#24694;&#24847;&#20351;&#29992;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22833;&#21435;&#25511;&#21046;&#30340;&#19981;&#21487;&#36870;&#36716;&#30340;&#25439;&#22833;&#12290;&#37492;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21644;&#25345;&#32493;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#21457;&#21644;&#27835;&#29702;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&amp;D and governance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#31070;&#32463;ODE&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#23558;&#23545;&#25239;&#35757;&#32451;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#25216;&#26415;&#26469;&#23454;&#29616;&#40065;&#26834;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.17584</link><description>&lt;p&gt;
&#19968;&#31181;&#40065;&#26834;&#31070;&#32463;ODE&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A minimax optimal control approach for robust neural ODEs. (arXiv:2310.17584v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#31070;&#32463;ODE&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#23558;&#23545;&#25239;&#35757;&#32451;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#25216;&#26415;&#26469;&#23454;&#29616;&#40065;&#26834;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#40065;&#26834;&#25511;&#21046;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#31070;&#32463;ODE&#30340;&#23545;&#25239;&#35757;&#32451;&#38382;&#39064;&#12290;&#36825;&#26159;&#19968;&#31181;&#26367;&#20195;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#30830;&#20445;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#21487;&#38752;&#32467;&#26524;&#12290;&#31070;&#32463;ODE&#20801;&#35768;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#20026;&#25511;&#21046;&#31995;&#32479;&#30340;&#31163;&#25955;&#21270;&#65292;&#20174;&#25511;&#21046;&#29702;&#35770;&#20013;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#21457;&#21644;&#29702;&#35299;&#12290;&#22312;&#36825;&#31181;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558;&#25200;&#21160;&#25968;&#25454;&#30340;&#23545;&#25239;&#35757;&#32451;&#20844;&#24335;&#21270;&#20026;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;Pontryagin&#26368;&#22823;&#20540;&#21407;&#29702;&#30340;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#40065;&#26834;&#35757;&#32451;&#30340;&#26032;&#35299;&#37322;&#65292;&#23548;&#33268;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#21152;&#26435;&#25216;&#26415;&#65292;&#24182;&#22312;&#20302;&#32500;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the adversarial training of neural ODEs from a robust control perspective. This is an alternative to the classical training via empirical risk minimization, and it is widely used to enforce reliable outcomes for input perturbations. Neural ODEs allow the interpretation of deep neural networks as discretizations of control systems, unlocking powerful tools from control theory for the development and the understanding of machine learning. In this specific case, we formulate the adversarial training with perturbed data as a minimax optimal control problem, for which we derive first order optimality conditions in the form of Pontryagin's Maximum Principle. We provide a novel interpretation of robust training leading to an alternative weighted technique, which we test on a low-dimensional classification task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#38598;&#25104;&#30340;CATE&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#20854;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#23454;&#29616;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#36951;&#25022;&#29575;</title><link>http://arxiv.org/abs/2310.16945</link><description>&lt;p&gt;
Causal Q-Aggregation for CATE Model Selection&#65288;CATE&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#22240;&#26524;Q&#38598;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Causal Q-Aggregation for CATE Model Selection. (arXiv:2310.16945v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#38598;&#25104;&#30340;CATE&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#20854;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#23454;&#29616;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#36951;&#25022;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#26159;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#29992;&#20110;CATE&#20272;&#35745;&#30340;&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#27169;&#22411;&#36873;&#25321;&#26159;&#19968;&#39033;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#23454;&#35777;&#24037;&#20316;&#25552;&#20379;&#20102;&#26377;&#21033;&#20110;&#20855;&#26377;&#21452;&#37325;&#40065;&#26834;&#24615;&#36136;&#30340;&#20195;&#29702;&#25439;&#22833;&#24230;&#37327;&#21644;&#27169;&#22411;&#38598;&#25104;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#29702;&#35770;&#29702;&#35299;&#36824;&#19981;&#22815;&#12290;&#30452;&#25509;&#24212;&#29992;&#20808;&#21069;&#30340;&#29702;&#35770;&#24037;&#20316;&#20250;&#30001;&#20110;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#29575;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#26377;&#20027;&#35201;CATE&#38598;&#25104;&#26041;&#27861;&#30340;&#36951;&#25022;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#30340;Q&#38598;&#25104;&#30340;&#26032;&#30340;CATE&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#22240;&#26524;Q&#38598;&#25104;&#22312;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#30340;&#36951;&#25022;&#29575;&#19978;&#36798;&#21040;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20248;&#20540;&#20026;$\frac{\log(M)}{n}$&#65288;&#20854;&#20013;$M$&#20026;&#27169;&#22411;&#25968;&#65292;$n$&#20026;&#26679;&#26412;&#25968;&#65289;&#65292;&#21152;&#19978;&#39640;&#38454;&#20272;&#35745;&#35823;&#24046;&#39033;
&lt;/p&gt;
&lt;p&gt;
Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error term
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26222;&#36866;&#24494;&#20998;&#26041;&#31243;&#65288;UDEs&#65289;&#25913;&#36827;SIR&#27169;&#22411;&#26469;&#23398;&#20064;COVID-19&#30340;&#21306;&#22495;&#20256;&#25773;&#12290;&#36890;&#36807;&#22312;SIR&#26041;&#31243;&#20013;&#21152;&#20837;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#37051;&#36817;&#22320;&#21306;&#30340;&#24433;&#21709;&#65292;&#24182;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16804</link><description>&lt;p&gt;
&#20351;&#29992;&#26222;&#36866;&#24494;&#20998;&#26041;&#31243;&#22312;SIR&#27169;&#22411;&#20013;&#23398;&#20064;COVID-19&#30340;&#21306;&#22495;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Learning COVID-19 Regional Transmission Using Universal Differential Equations in a SIR model. (arXiv:2310.16804v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26222;&#36866;&#24494;&#20998;&#26041;&#31243;&#65288;UDEs&#65289;&#25913;&#36827;SIR&#27169;&#22411;&#26469;&#23398;&#20064;COVID-19&#30340;&#21306;&#22495;&#20256;&#25773;&#12290;&#36890;&#36807;&#22312;SIR&#26041;&#31243;&#20013;&#21152;&#20837;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#37051;&#36817;&#22320;&#21306;&#30340;&#24433;&#21709;&#65292;&#24182;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24230;&#20114;&#36830;&#30340;&#31038;&#20250;&#24456;&#38590;&#23545;&#20256;&#26579;&#30149;&#22914;COVID-19&#30340;&#20256;&#25773;&#36827;&#34892;&#24314;&#27169;&#12290;&#21333;&#19968;&#21306;&#22495;&#30340;SIR&#27169;&#22411;&#26080;&#27861;&#32771;&#34385;&#21040;&#20837;&#20405;&#24615;&#24863;&#26579;&#21147;&#65292;&#24182;&#19988;&#23558;&#20854;&#25193;&#23637;&#21040;&#22823;&#37327;&#20114;&#30456;&#20316;&#29992;&#30340;&#21306;&#22495;&#28041;&#21450;&#21040;&#35768;&#22810;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#19981;&#33021;&#25104;&#31435;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#26222;&#36866;&#24494;&#20998;&#26041;&#31243;&#65288;UDEs&#65289;&#26469;&#25429;&#25417;&#37051;&#36817;&#22320;&#21306;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#32467;&#21512;SIR + UDE&#27169;&#22411;&#20013;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;UDEs&#26159;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23436;&#20840;&#25110;&#37096;&#20998;&#23450;&#20041;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#22312;SIR&#26041;&#31243;&#20013;&#21152;&#20837;&#19968;&#20010;&#30001;DNN&#32452;&#25104;&#30340;&#38468;&#21152;&#39033;&#65292;&#35813;DNN&#20174;&#20854;&#20182;&#22320;&#21306;&#23398;&#20064;&#20837;&#20405;&#24615;&#24863;&#26579;&#21147;&#12290;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#21644;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#36924;&#36817;&#37051;&#36817;&#22320;&#21306;&#29366;&#24577;&#24341;&#36215;&#30340;&#30446;&#26631;&#31995;&#32479;&#21464;&#21270;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#21333;&#19968;&#21306;&#22495;SIR&#27169;&#22411;&#21644;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Highly-interconnected societies difficult to model the spread of infectious diseases such as COVID-19. Single-region SIR models fail to account for incoming forces of infection and expanding them to a large number of interacting regions involves many assumptions that do not hold in the real world. We propose using Universal Differential Equations (UDEs) to capture the influence of neighboring regions and improve the model's predictions in a combined SIR+UDE model. UDEs are differential equations totally or partially defined by a deep neural network (DNN). We include an additive term to the SIR equations composed by a DNN that learns the incoming force of infection from the other regions. The learning is performed using automatic differentiation and gradient descent to approach the change in the target system caused by the state of the neighboring regions. We compared the proposed model using a simulated COVID-19 outbreak against a single-region SIR and a fully data-driven model compose
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;&#21644;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#21644;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16789</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Detecting Pretraining Data from Large Language Models. (arXiv:2310.16789v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;&#21644;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#21644;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#29992;&#20110;&#35757;&#32451;&#23427;&#20204;&#30340;&#25968;&#25454;&#24456;&#23569;&#34987;&#20844;&#24320;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#25968;&#25454;&#30340;&#35268;&#27169;&#20043;&#22823;&#65292;&#21487;&#33021;&#21253;&#21547;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#26448;&#26009;&#12289;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#20197;&#21450;&#29992;&#20110;&#24191;&#27867;&#25253;&#36947;&#30340;&#21442;&#32771;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#25105;&#20204;&#20960;&#20046;&#21487;&#20197;&#32943;&#23450;&#23427;&#20204;&#21253;&#21547;&#20102;&#28508;&#22312;&#30340;&#38382;&#39064;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30446;&#21069;&#26080;&#27861;&#30693;&#36947;&#36825;&#20123;&#25991;&#26412;&#20013;&#21253;&#21547;&#20102;&#21738;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#20197;&#21450;&#27604;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#26816;&#27979;&#38382;&#39064;&#65306;&#22312;&#19981;&#30693;&#36947;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#27573;&#25991;&#26412;&#21644;&#23545;LLM&#30340;&#40657;&#30418;&#35775;&#38382;&#65292;&#25105;&#20204;&#33021;&#21542;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#26159;&#22312;&#25552;&#20379;&#30340;&#25991;&#26412;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65311;&#20026;&#20102;&#26041;&#20415;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;WIKIMIA&#65292;&#20351;&#29992;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#21644;&#20043;&#21518;&#21019;&#24314;&#30340;&#25968;&#25454;&#26469;&#25903;&#25345;&#37329;&#26631;&#20934;&#26816;&#27979;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;Min-K% Prob&#65292;&#22522;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20551;&#35774;&#65306;&#19968;&#20010;&#26410;&#35265;&#36807;&#30340;&#20363;&#23376;&#21487;&#33021;&#21253;&#21547;&#20960;&#20010;&#20855;&#26377;&#36739;&#20302;&#27010;&#29575;&#30340;&#31163;&#32676;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26368;&#20248;&#25511;&#21046;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36718;&#20132;&#20114;&#30340;&#25552;&#31034;&#24037;&#31243;&#26694;&#26550;&#65292;&#31995;&#32479;&#21270;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25193;&#22823;&#20102;&#36866;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2310.14201</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#25511;&#21046;&#30340;&#35270;&#35282;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering Through the Lens of Optimal Control. (arXiv:2310.14201v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26368;&#20248;&#25511;&#21046;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36718;&#20132;&#20114;&#30340;&#25552;&#31034;&#24037;&#31243;&#26694;&#26550;&#65292;&#31995;&#32479;&#21270;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25193;&#22823;&#20102;&#36866;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#65288;PE&#65289;&#24050;&#32463;&#25104;&#20026;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#23427;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#20854;&#28508;&#21147;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#38543;&#30528;&#20219;&#21153;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#26368;&#36817;&#30340;&#39640;&#32423;PE&#26041;&#27861;&#24050;&#32463;&#36229;&#36234;&#20102;&#21333;&#36718;&#20132;&#20114;&#30340;&#38480;&#21046;&#65292;&#32780;&#26159;&#37319;&#29992;&#20102;&#22810;&#36718;&#20132;&#20114;&#65292;&#36825;&#26679;&#21487;&#20197;&#26356;&#21152;&#28145;&#20837;&#21644;&#32454;&#33268;&#22320;&#19982;LLM&#36827;&#34892;&#20132;&#20114;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;LLM&#30340;&#22810;&#36718;&#20132;&#20114;&#30340;&#26368;&#20248;&#25511;&#21046;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#19981;&#20165;&#31995;&#32479;&#21270;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#36824;&#20026;&#20005;&#35880;&#30340;&#20998;&#26512;&#25913;&#36827;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#35813;&#26694;&#26550;&#25193;&#23637;&#21040;&#21253;&#25324;&#36890;&#36807;&#38598;&#21512;&#26041;&#27861;&#21644;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#20174;&#32780;&#25193;&#22823;&#20102;&#36866;&#29992;&#33539;&#22260;&#12290;&#36890;&#36807;&#37319;&#29992;&#26368;&#20248;&#25511;&#21046;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering (PE) has emerged as a critical technique for guiding Large Language Models (LLMs) in solving intricate tasks. Its importance is highlighted by its potential to significantly enhance the efficiency and effectiveness of human-machine interaction. As tasks grow increasingly complex, recent advanced PE methods have extended beyond the limitations of single-round interactions to embrace multi-round interactions, which allows for a deeper and more nuanced engagement with LLMs. In this paper, we propose an optimal control framework tailored for multi-round interactions with LLMs. This framework provides a unified mathematical structure that not only systematizes the existing PE methods but also sets the stage for rigorous analytical improvements. Furthermore, we extend this framework to include PE via ensemble methods and multi-agent collaboration, thereby enlarging the scope of applicability. By adopting an optimal control perspective, we offer fresh insights into existing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#34920;&#36798;GC2&#26597;&#35810;&#65292;&#19982;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#23384;&#22312;&#20998;&#31163;&#65292;&#36825;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13139</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks with polynomial activations have limited expressivity. (arXiv:2310.13139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#34920;&#36798;GC2&#26597;&#35810;&#65292;&#19982;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#23384;&#22312;&#20998;&#31163;&#65292;&#36825;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#21487;&#20197;&#23436;&#20840;&#30001;&#36866;&#24403;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#26469;&#25551;&#36848;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20219;&#20309;&#22312;&#26631;&#35760;&#22270;&#19978;&#35299;&#37322;&#30340;&#20851;&#20110;&#20108;&#20803;&#36923;&#36753;&#29255;&#27573;&#65288;GC2&#65289;&#30340;&#26597;&#35810;&#37117;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#22823;&#23567;&#20165;&#21462;&#20915;&#20110;&#26597;&#35810;&#28145;&#24230;&#30340;GNN&#26469;&#34920;&#31034;&#12290;&#27491;&#22914;[Barcelo&#65286;Al&#12290;&#65292;2020&#65292;Grohe&#65292;2021]&#25351;&#20986;&#30340;&#37027;&#26679;&#65292;&#36825;&#20010;&#25551;&#36848;&#36866;&#29992;&#20110;&#19968;&#32452;&#28608;&#27963;&#20989;&#25968;&#30340;&#23478;&#26063;&#65292;&#36825;&#34920;&#26126;GNN&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#26469;&#34920;&#36798;&#19981;&#21516;&#30340;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#23618;&#27425;&#32467;&#26500;&#30340;&#23384;&#22312;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#26080;&#27861;&#34920;&#31034;GC2&#26597;&#35810;&#12290;&#36825;&#24847;&#21619;&#30528;&#22810;&#39033;&#24335;&#21644;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#12289;sigmoid&#12289;&#21452;&#26354;&#27491;&#20999;&#31561;&#65289;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#20998;&#31163;&#65292;&#24182;&#22238;&#31572;&#20102;[Grohe&#65292;2021]&#25552;&#20986;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expressivity of Graph Neural Networks (GNNs) can be entirely characterized by appropriate fragments of the first order logic. Namely, any query of the two variable fragment of graded modal logic (GC2) interpreted over labelled graphs can be expressed using a GNN whose size depends only on the depth of the query. As pointed out by [Barcelo &amp; Al., 2020, Grohe, 2021 ], this description holds for a family of activation functions, leaving the possibibility for a hierarchy of logics expressible by GNNs depending on the chosen activation function. In this article, we show that such hierarchy indeed exists by proving that GC2 queries cannot be expressed by GNNs with polynomial activation functions. This implies a separation between polynomial and popular non polynomial activations (such as ReLUs, sigmoid and hyperbolic tan and others) and answers an open question formulated by [Grohe, 2021].
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35889;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#38750;&#20984;&#24615;&#38382;&#39064;&#19979;&#23398;&#29983;&#32593;&#32476;&#19982;&#25945;&#24072;&#32593;&#32476;&#20043;&#38388;&#23384;&#22312;&#30340;&#19981;&#21464;&#23376;&#32593;&#32476;&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12612</link><description>&lt;p&gt;
&#23398;&#29983;&#22914;&#20309;&#25104;&#20026;&#25945;&#24072;&#65306;&#36890;&#36807;&#35889;&#26041;&#27861;&#23398;&#20064;&#21644;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
How a student becomes a teacher: learning and forgetting through Spectral methods. (arXiv:2310.12612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35889;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#38750;&#20984;&#24615;&#38382;&#39064;&#19979;&#23398;&#29983;&#32593;&#32476;&#19982;&#25945;&#24072;&#32593;&#32476;&#20043;&#38388;&#23384;&#22312;&#30340;&#19981;&#21464;&#23376;&#32593;&#32476;&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#24120;&#34987;&#29992;&#20316;&#29616;&#23454;&#29983;&#27963;&#20013;&#25945;&#23398;&#30340;&#26377;&#25928;&#38544;&#21947;&#12290;&#24403;&#23398;&#29983;&#32593;&#32476;&#30456;&#23545;&#20110;&#25945;&#24072;&#32593;&#32476;&#36807;&#24230;&#21442;&#25968;&#21270;&#26102;&#65292;&#19978;&#36848;&#27169;&#22411;&#23588;&#20026;&#30456;&#20851;&#12290;&#22312;&#36825;&#31181;&#25805;&#20316;&#26465;&#20214;&#19979;&#65292;&#24456;&#23481;&#26131;&#25512;&#27979;&#23398;&#29983;&#22788;&#29702;&#32473;&#23450;&#20219;&#21153;&#30340;&#33021;&#21147;&#26368;&#32456;&#21487;&#33021;&#20648;&#23384;&#22312;&#25972;&#20010;&#32593;&#32476;&#30340;&#19968;&#20010;&#23376;&#37096;&#20998;&#20013;&#12290;&#26681;&#25454;&#36866;&#24403;&#30340;&#25351;&#26631;&#65292;&#36825;&#20010;&#23376;&#37096;&#20998;&#24212;&#35813;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#31867;&#20284;&#20110;&#20923;&#32467;&#30340;&#25945;&#24072;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#23398;&#29983;&#20505;&#36873;&#32593;&#32476;&#30340;&#19981;&#21516;&#26550;&#26500;&#19979;&#36817;&#20284;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25152;&#30740;&#31350;&#38382;&#39064;&#30340;&#22266;&#26377;&#38750;&#20984;&#24615;&#31243;&#24230;&#65292;&#26368;&#26032;&#30340;&#20256;&#32479;&#23398;&#20064;&#25216;&#26415;&#26080;&#27861;&#35782;&#21035;&#36825;&#26679;&#19968;&#20010;&#19981;&#21464;&#23376;&#32593;&#32476;&#30340;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#20010;&#26681;&#26412;&#19981;&#21516;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#24314;&#31435;&#22312;&#35889;&#34920;&#31034;&#30340;&#22522;&#30784;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In theoretical ML, the teacher-student paradigm is often employed as an effective metaphor for real-life tuition. The above scheme proves particularly relevant when the student network is overparameterized as compared to the teacher network. Under these operating conditions, it is tempting to speculate that the student ability to handle the given task could be eventually stored in a sub-portion of the whole network. This latter should be to some extent reminiscent of the frozen teacher structure, according to suitable metrics, while being approximately invariant across different architectures of the student candidate network. Unfortunately, state-of-the-art conventional learning techniques could not help in identifying the existence of such an invariant subnetwork, due to the inherent degree of non-convexity that characterizes the examined problem. In this work, we take a leap forward by proposing a radically different optimization scheme which builds on a spectral representation of th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#27169;&#22411;&#21435;&#20559;&#32622;&#30340;&#26694;&#26550;&#65288;FMD&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#28040;&#38500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25104;&#26412;&#21644;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.12560</link><description>&lt;p&gt;
&#24555;&#36895;&#27169;&#22411;&#21435;&#20559;&#32622;&#19982;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Model Debias with Machine Unlearning. (arXiv:2310.12560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12560
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#27169;&#22411;&#21435;&#20559;&#32622;&#30340;&#26694;&#26550;&#65288;FMD&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#28040;&#38500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25104;&#26412;&#21644;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#34920;&#29616;&#20986;&#20559;&#24046;&#30340;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20154;&#33080;&#35782;&#21035;&#25968;&#25454;&#38598;CelebA&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#32593;&#32476;&#20542;&#21521;&#20110;&#39044;&#27979;&#22899;&#24615;&#30340;&#37329;&#33394;&#22836;&#21457;&#21644;&#30007;&#24615;&#30340;&#40657;&#33394;&#22836;&#21457;&#12290;&#36825;&#20123;&#20559;&#24046;&#19981;&#20165;&#21361;&#23475;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#20250;&#25345;&#32493;&#21644;&#25918;&#22823;&#31038;&#20250;&#20559;&#35265;&#65292;&#36825;&#23545;&#20110;&#21307;&#30103;&#12289;&#25307;&#32856;&#31561;&#33258;&#21160;&#20915;&#31574;&#36807;&#31243;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#21152;&#21095;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#19981;&#20844;&#24179;&#32463;&#27982;&#21644;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;&#29616;&#26377;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#22312;&#20559;&#35265;&#26631;&#35760;&#25110;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26041;&#38754;&#25104;&#26412;&#39640;&#26114;&#65292;&#21516;&#26102;&#20063;&#22312;&#38416;&#26126;&#27169;&#22411;&#20869;&#37096;&#20559;&#35265;&#30340;&#36215;&#28304;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#27169;&#22411;&#21435;&#20559;&#32622;&#26694;&#26550;(FMD)&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#28040;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#12290;FMD&#36890;&#36807;&#26174;&#24335;&#30340;&#21453;&#20107;&#23454;&#26426;&#21046;&#26469;&#35782;&#21035;&#20559;&#32622;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing framework (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual 
&lt;/p&gt;</description></item><item><title>Rank-DETR&#26159;&#19968;&#31181;&#39640;&#36136;&#37327;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25490;&#21517;&#23548;&#21521;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;DETR&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.08854</link><description>&lt;p&gt;
&#39640;&#36136;&#37327;&#30446;&#26631;&#26816;&#27979;&#30340;Rank-DETR&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rank-DETR for High Quality Object Detection. (arXiv:2310.08854v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08854
&lt;/p&gt;
&lt;p&gt;
Rank-DETR&#26159;&#19968;&#31181;&#39640;&#36136;&#37327;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25490;&#21517;&#23548;&#21521;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;DETR&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26816;&#27979;&#21464;&#25442;&#22120;&#65288;DETR&#65289;&#20351;&#29992;&#19968;&#32452;&#23545;&#35937;&#26597;&#35810;&#26469;&#39044;&#27979;&#36793;&#30028;&#26694;&#21015;&#34920;&#65292;&#36890;&#36807;&#23558;&#20854;&#20998;&#31867;&#32622;&#20449;&#24230;&#24471;&#20998;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#36873;&#25321;&#25490;&#21517;&#38752;&#21069;&#30340;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#32473;&#23450;&#36755;&#20837;&#22270;&#20687;&#30340;&#26368;&#32456;&#26816;&#27979;&#32467;&#26524;&#12290;&#24615;&#33021;&#21331;&#36234;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#38656;&#35201;&#23545;&#36793;&#30028;&#26694;&#39044;&#27979;&#36827;&#34892;&#20934;&#30830;&#30340;&#25490;&#24207;&#12290;&#23545;&#20110;&#22522;&#20110;DETR&#30340;&#26816;&#27979;&#22120;&#65292;&#25490;&#21517;&#38752;&#21069;&#30340;&#36793;&#30028;&#26694;&#30001;&#20110;&#20998;&#31867;&#24471;&#20998;&#19982;&#23450;&#20301;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#32780;&#23548;&#33268;&#23450;&#20301;&#36136;&#37327;&#36739;&#24046;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#39640;&#36136;&#37327;&#26816;&#27979;&#22120;&#30340;&#26500;&#24314;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31995;&#21015;&#38754;&#21521;&#25490;&#21517;&#30340;&#35774;&#35745;&#65292;&#20849;&#21516;&#31216;&#20026;Rank-DETR&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;DETR&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;i&#65289;&#19968;&#20010;&#38754;&#21521;&#25490;&#21517;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#21487;&#20197;&#20419;&#36827;&#27491;&#38754;&#39044;&#27979;&#24182;&#25233;&#21046;&#36127;&#38754;&#39044;&#27979;&#65292;&#20197;&#30830;&#20445;&#26356;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#19968;&#20010;&#38754;&#21521;&#25490;&#21517;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#21305;&#37197;&#25104;&#26412;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern detection transformers (DETRs) use a set of object queries to predict a list of bounding boxes, sort them by their classification confidence scores, and select the top-ranked predictions as the final detection results for the given input image. A highly performant object detector requires accurate ranking for the bounding box predictions. For DETR-based detectors, the top-ranked bounding boxes suffer from less accurate localization quality due to the misalignment between classification scores and localization accuracy, thus impeding the construction of high-quality detectors. In this work, we introduce a simple and highly performant DETR-based object detector by proposing a series of rank-oriented designs, combinedly called Rank-DETR. Our key contributions include: (i) a rank-oriented architecture design that can prompt positive predictions and suppress the negative ones to ensure lower false positive rates, as well as (ii) a rank-oriented loss function and matching cost design 
&lt;/p&gt;</description></item><item><title>Bucks for Buckets (B4B) is the first active defense against stealing encoders, which prevents model stealing attacks by adaptively adjusting the utility of the returned representations based on the coverage of the embedding space by the user.</title><link>http://arxiv.org/abs/2310.08571</link><description>&lt;p&gt;
Buckets for Buckets&#65288;B4B&#65289;&#65306;&#38024;&#23545;&#30423;&#21462;&#32534;&#30721;&#22120;&#30340;&#20027;&#21160;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders. (arXiv:2310.08571v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08571
&lt;/p&gt;
&lt;p&gt;
Bucks for Buckets (B4B) is the first active defense against stealing encoders, which prevents model stealing attacks by adaptively adjusting the utility of the returned representations based on the coverage of the embedding space by the user.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;API&#25552;&#20379;&#20102;&#29616;&#25104;&#19988;&#39640;&#25928;&#30340;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#32473;&#23450;&#36755;&#20837;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#30001;&#20110;&#36825;&#20123;&#32534;&#30721;&#22120;&#30340;&#35757;&#32451;&#25104;&#26412;&#24456;&#39640;&#65292;&#23427;&#20204;&#25104;&#20026;&#27169;&#22411;&#30423;&#21462;&#25915;&#20987;&#30340;&#26377;&#21033;&#30446;&#26631;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#23545;API&#30340;&#26597;&#35810;&#35775;&#38382;&#65292;&#22312;&#21407;&#22987;&#35757;&#32451;&#25104;&#26412;&#30340;&#19968;&#23567;&#37096;&#20998;&#24773;&#20917;&#19979;&#26412;&#22320;&#22797;&#21046;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Bucks for Buckets (B4B)&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#25915;&#20987;&#21457;&#29983;&#26102;&#33021;&#38450;&#27490;&#30423;&#21462;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#21512;&#27861;&#29992;&#25143;&#30340;&#34920;&#31034;&#36136;&#37327;&#30340;&#20027;&#21160;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;&#20381;&#36182;&#20110;&#20197;&#19979;&#35266;&#23519;&#32467;&#26524;&#65306;&#23581;&#35797;&#30423;&#21462;&#32534;&#30721;&#22120;&#21151;&#33021;&#30340;&#25915;&#20987;&#32773;&#36820;&#22238;&#30340;&#34920;&#31034;&#28085;&#30422;&#20102;&#23884;&#20837;&#31354;&#38388;&#30340;&#19968;&#20010;&#26174;&#30528;&#36739;&#22823;&#30340;&#37096;&#20998;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;&#35299;&#20915;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#21512;&#27861;&#29992;&#25143;&#30340;&#34920;&#31034;&#28085;&#30422;&#33539;&#22260;&#36739;&#23567;&#12290;B4B&#21033;&#29992;&#36825;&#19968;&#28857;&#26681;&#25454;&#29992;&#25143;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#35206;&#30422;&#24773;&#20917;&#33258;&#36866;&#24212;&#35843;&#25972;&#36820;&#22238;&#34920;&#31034;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose Bucks for Buckets (B4B), the first active defense that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task.vB4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding sp
&lt;/p&gt;</description></item><item><title>Observatory&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#26694;&#26550;&#26469;&#20998;&#26512;&#20851;&#31995;&#34920;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07736</link><description>&lt;p&gt;
Observatory: &#21051;&#30011;&#20851;&#31995;&#34920;&#23884;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Observatory: Characterizing Embeddings of Relational Tables. (arXiv:2310.07736v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07736
&lt;/p&gt;
&lt;p&gt;
Observatory&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#26694;&#26550;&#26469;&#20998;&#26512;&#20851;&#31995;&#34920;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#38376;&#30340;&#34920;&#23884;&#20837;&#27169;&#22411;&#22312;&#35768;&#22810;&#34920;&#26684;&#25968;&#25454;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#37117;&#28212;&#26395;&#22312;&#35768;&#22810;&#26032;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65307;&#20294;&#26159;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#20197;&#21450;&#23427;&#20204;&#29983;&#25104;&#30340;&#34920;&#26684;&#34920;&#31034;&#30340;&#29702;&#35299;&#26377;&#38480;&#65292;&#23548;&#33268;&#22312;&#23547;&#25214;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;&#20381;&#36182;&#20110;&#35797;&#38169;&#12290;&#36843;&#20999;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;&#19979;&#28216;&#20351;&#29992;&#20013;&#30340;&#20302;&#25928;&#29575;&#21644;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Observatory&#30340;&#27491;&#24335;&#26694;&#26550;&#65292;&#20197;&#31995;&#32479;&#22320;&#20998;&#26512;&#20851;&#31995;&#34920;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#20851;&#31995;&#25968;&#25454;&#27169;&#22411;&#30340;&#19981;&#21464;&#24615;&#21644;&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#32479;&#35745;&#32771;&#34385;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20843;&#20010;&#21407;&#22987;&#23646;&#24615;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#24230;&#37327;&#26469;&#23450;&#37327;&#22320;&#21051;&#30011;&#36825;&#20123;&#23646;&#24615;&#30340;&#34920;&#26684;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models and specialized table embedding models have recently demonstrated strong performance on many tasks over tabular data. Researchers and practitioners are keen to leverage these models in many new application contexts; but limited understanding of the strengths and weaknesses of these models, and the table representations they generate, makes the process of finding a suitable model for a given task reliant on trial and error. There is an urgent need to gain a comprehensive understanding of these models to minimize inefficiency and failures in downstream usage.  To address this need, we propose Observatory, a formal framework to systematically analyze embedding representations of relational tables. Motivated both by invariants of the relational data model and by statistical considerations regarding data distributions, we define eight primitive properties, and corresponding measures to quantitatively characterize table embeddings for these properties. Based on these properti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;MODIS&#22810;&#20809;&#35889;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;LULC&#31867;&#21035;&#30340;&#30450;&#30446;&#20809;&#35889;&#20998;&#31163;&#12290;&#36890;&#36807;&#28155;&#21152;&#22320;&#29702;&#21152;&#22320;&#24418;&#21644;&#27668;&#20505;&#36741;&#21161;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07223</link><description>&lt;p&gt;
&#29992;MODIS&#22810;&#20809;&#35889;&#26102;&#38388;&#24207;&#21015;&#21644;&#36741;&#21161;&#25968;&#25454;&#36827;&#34892;&#30450;&#30446;&#20809;&#35889;&#20998;&#31163;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data. (arXiv:2310.07223v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;MODIS&#22810;&#20809;&#35889;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;LULC&#31867;&#21035;&#30340;&#30450;&#30446;&#20809;&#35889;&#20998;&#31163;&#12290;&#36890;&#36807;&#28155;&#21152;&#22320;&#29702;&#21152;&#22320;&#24418;&#21644;&#27668;&#20505;&#36741;&#21161;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#25968;&#25454;&#20013;LULC&#31867;&#21035;&#36890;&#24120;&#23384;&#22312;&#28151;&#21512;&#24773;&#20917;&#65292;&#20809;&#35889;&#20998;&#31163;&#26159;&#19968;&#31181;&#20174;&#28151;&#21512;&#20687;&#32032;&#20013;&#25552;&#21462;&#20449;&#24687;&#21040;&#20854;&#32452;&#25104;LULC&#31867;&#22411;&#21644;&#30456;&#24212;&#20016;&#24230;&#20998;&#25968;&#30340;&#25216;&#26415;&#12290;&#20256;&#32479;&#19978;&#65292;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#35201;&#20040;&#20381;&#36182;&#20110;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#32463;&#20856;&#26041;&#27861;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#36991;&#20813;&#26174;&#24335;&#25104;&#20998;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20063;&#23601;&#26159;&#30450;&#30446;&#20809;&#35889;&#20998;&#31163;&#65288;BSU&#65289;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;BSU&#30740;&#31350;&#20391;&#37325;&#20110;&#21333;&#20010;&#26102;&#38388;&#27493;&#30340;&#39640;&#20809;&#35889;&#25968;&#25454;&#65292;&#28982;&#32780;&#19982;&#22810;&#20809;&#35889;&#25968;&#25454;&#30456;&#27604;&#65292;&#20854;&#33719;&#21462;&#25104;&#26412;&#20173;&#28982;&#30456;&#24403;&#39640;&#26114;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20379;&#20102;&#31532;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#22810;&#20809;&#35889;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;DL&#27169;&#22411;&#36827;&#34892;LULC&#31867;&#21035;&#30340;BSU&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#28155;&#21152;&#22320;&#29702;&#21152;&#22320;&#24418;&#65288;geo-topographic&#65289;&#21644;&#27668;&#20505;&#36741;&#21161;&#20449;&#24687;&#26469;&#25552;&#21319;&#22522;&#20110;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20809;&#35889;&#26102;&#38388;&#36755;&#20837;&#25968;&#25454;&#19982;&#22320;&#29702;&#26102;&#24577;&#25968;&#25454;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC) types. Spectral unmixing is a technique to extract information from mixed pixels into their constituent LULC types and corresponding abundance fractions. Traditionally, solving this task has relied on either classical methods that require prior knowledge of endmembers or machine learning methods that avoid explicit endmembers calculation, also known as blind spectral unmixing (BSU). Most BSU studies based on Deep Learning (DL) focus on one time-step hyperspectral data, yet its acquisition remains quite costly compared with multispectral data. To our knowledge, here we provide the first study on BSU of LULC classes using multispectral time series data with DL models. We further boost the performance of a Long-Short Term Memory (LSTM)-based model by incorporating geographic plus topographic (geo-topographic) and climatic ancillary information. Our experiments show that combining spectral-temporal input data togeth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#20984;&#38750;&#20984;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#27491;&#21017;&#21270;&#65307;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24369;&#20984;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#23545;&#25239;&#24615;&#26041;&#27861;&#30340;&#25968;&#20540;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05812</link><description>&lt;p&gt;
&#21487;&#35777;&#25910;&#25947;&#30340;&#25968;&#25454;&#39537;&#21160;&#20984;&#38750;&#20984;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Provably Convergent Data-Driven Convex-Nonconvex Regularization. (arXiv:2310.05812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#20984;&#38750;&#20984;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#27491;&#21017;&#21270;&#65307;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24369;&#20984;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#23545;&#25239;&#24615;&#26041;&#27861;&#30340;&#25968;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;&#22120;&#26159;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#26032;&#20852;&#33539;&#24335;&#12290;&#36825;&#23548;&#33268;&#20102;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#26080;&#27861;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20984;&#38750;&#20984;&#65288;CNC&#65289;&#26694;&#26550;&#20013;&#20986;&#29616;&#20102;&#33391;&#23450;&#20041;&#24615;&#21644;&#25910;&#25947;&#24615;&#27491;&#21017;&#21270;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24369;&#20984;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65288;IWCNN&#65289;&#26500;&#24314;&#65292;&#23558;&#23398;&#20064;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#36866;&#24212;&#21040;CNC&#26694;&#26550;&#20013;&#12290;&#20174;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#20043;&#21069;&#23545;&#25239;&#24615;&#26041;&#27861;&#30340;&#25968;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging new paradigm for solving inverse problems is via the use of deep learning to learn a regularizer from data. This leads to high-quality results, but often at the cost of provable guarantees. In this work, we show how well-posedness and convergent regularization arises within the convex-nonconvex (CNC) framework for inverse problems. We introduce a novel input weakly convex neural network (IWCNN) construction to adapt the method of learned adversarial regularization to the CNC framework. Empirically we show that our method overcomes numerical issues of previous adversarial methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANDE&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22362;&#30828;&#12289;&#36724;&#23545;&#40784;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#24182;&#32467;&#21512;&#20102;&#36724;&#23545;&#40784;&#20998;&#21106;&#21644;&#26799;&#24230;&#20248;&#21270;&#30340;&#28789;&#27963;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36880;&#20010;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#20415;&#20110;&#23398;&#20064;&#31616;&#21333;&#21644;&#22797;&#26434;&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.17130</link><description>&lt;p&gt;
GRANDE: &#22522;&#20110;&#26799;&#24230;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GRANDE: Gradient-Based Decision Tree Ensembles. (arXiv:2309.17130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANDE&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22362;&#30828;&#12289;&#36724;&#23545;&#40784;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#24182;&#32467;&#21512;&#20102;&#36724;&#23545;&#40784;&#20998;&#21106;&#21644;&#26799;&#24230;&#20248;&#21270;&#30340;&#28789;&#27963;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36880;&#20010;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#20415;&#20110;&#23398;&#20064;&#31616;&#21333;&#21644;&#22797;&#26434;&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#27169;&#22411;&#20173;&#28982;&#26159;&#22788;&#29702;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39640;&#28789;&#27963;&#24615;&#65292;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#26469;&#35828;&#65292;&#23384;&#22312;&#23545;&#29305;&#23450;&#20110;&#34920;&#26684;&#30340;&#26799;&#24230;&#26041;&#27861;&#30340;&#26174;&#33879;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANDE&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#26799;&#24230;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22362;&#30828;&#12289;&#36724;&#23545;&#40784;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#12290;GRANDE&#22522;&#20110;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340;&#31264;&#23494;&#34920;&#31034;&#65292;&#21487;&#20197;&#20351;&#29992;&#30452;&#36890;&#25805;&#20316;&#31526;&#21644;&#21453;&#21521;&#20256;&#25773;&#19968;&#36215;&#20248;&#21270;&#25152;&#26377;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#36724;&#23545;&#40784;&#20998;&#21106;&#65288;&#36825;&#26159;&#34920;&#26684;&#25968;&#25454;&#30340;&#19968;&#20010;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#65289;&#21644;&#26799;&#24230;&#20248;&#21270;&#30340;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36880;&#20010;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#20415;&#20110;&#23398;&#20064;&#31616;&#21333;&#21644;&#22797;&#26434;&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;GRANDE&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of deep learning for text and image data, tree-based ensemble models are still state-of-the-art for machine learning with heterogeneous tabular data. However, there is a significant need for tabular-specific gradient-based methods due to their high flexibility. In this paper, we propose $\text{GRANDE}$, $\text{GRA}$die$\text{N}$t-Based $\text{D}$ecision Tree $\text{E}$nsembles, a novel approach for learning hard, axis-aligned decision tree ensembles using end-to-end gradient descent. GRANDE is based on a dense representation of tree ensembles, which affords to use backpropagation with a straight-through operator to jointly optimize all model parameters. Our method combines axis-aligned splits, which is a useful inductive bias for tabular data, with the flexibility of gradient-based optimization. Furthermore, we introduce an advanced instance-wise weighting that facilitates learning representations for both, simple and complex relations, within a single model. We con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#23545;&#35937;&#26816;&#27979;&#20013;&#20266;&#26631;&#31614;&#30340;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#29983;&#25104;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#21644;&#37327;&#21270;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09599</link><description>&lt;p&gt;
MEDL-U&#65306;&#22522;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;3D&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential Deep Learning. (arXiv:2309.09599v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#23545;&#35937;&#26816;&#27979;&#20013;&#20266;&#26631;&#31614;&#30340;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#29983;&#25104;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#21644;&#37327;&#21270;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#20110;3D&#23545;&#35937;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36825;&#38656;&#35201;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#35201;&#27714;&#24341;&#20837;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#25361;&#25112;&#65292;&#36825;&#36890;&#24120;&#26159;&#32321;&#37325;&#19988;&#32791;&#26102;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25991;&#29486;&#20013;&#20986;&#29616;&#20102;&#20960;&#31181;&#24369;&#30417;&#30563;&#30340;3D&#23545;&#35937;&#26816;&#27979;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20266;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#21253;&#21547;&#22122;&#22768;&#65292;&#19981;&#22914;&#20154;&#24037;&#26631;&#27880;&#30340;&#20934;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#35299;&#20915;&#20266;&#26631;&#31614;&#20013;&#22266;&#26377;&#27169;&#31946;&#24615;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MEDL-U&#65292;&#23427;&#26159;&#22522;&#20110;MTrans&#30340;EDL&#26694;&#26550;&#65292;&#19981;&#20165;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#36824;&#37327;&#21270;&#20102;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;EDL&#24212;&#29992;&#20110;3D&#23545;&#35937;&#26816;&#27979;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;(1)&#30456;&#23545;&#36739;&#20302;&#30340;&#20266;&#26631;&#31614;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in deep learning-based 3D object detection necessitate the availability of large-scale datasets. However, this requirement introduces the challenge of manual annotation, which is often both burdensome and time-consuming. To tackle this issue, the literature has seen the emergence of several weakly supervised frameworks for 3D object detection which can automatically generate pseudo labels for unlabeled data. Nevertheless, these generated pseudo labels contain noise and are not as accurate as those labeled by humans. In this paper, we present the first approach that addresses the inherent ambiguities present in pseudo labels by introducing an Evidential Deep Learning (EDL) based uncertainty estimation framework. Specifically, we propose MEDL-U, an EDL framework based on MTrans, which not only generates pseudo labels but also quantifies the associated uncertainties. However, applying EDL to 3D object detection presents three primary challenges: (1) relatively lower pseudolab
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#19981;&#20855;&#22791;&#20256;&#32479;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#23427;&#20204;&#22312;&#39044;&#27979;&#25216;&#33021;&#19978;&#30340;&#20248;&#21183;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#36825;&#20123;&#29305;&#27530;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08473</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the limitations of data-driven weather forecasting models. (arXiv:2309.08473v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08473
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#19981;&#20855;&#22791;&#20256;&#32479;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#23427;&#20204;&#22312;&#39044;&#27979;&#25216;&#33021;&#19978;&#30340;&#20248;&#21183;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#36825;&#20123;&#29305;&#27530;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#21457;&#23637;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#36890;&#24120;&#22768;&#31216;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#21069;&#19968;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;Pangu-Weather&#30340;&#39044;&#27979;&#26041;&#38754;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#19982;&#24863;&#30693;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20027;&#35201;&#32467;&#35770;&#26159;Pangu-Weather&#30340;&#39044;&#27979;&#65292;&#20197;&#21450;&#31867;&#20284;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#19981;&#20855;&#22791;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#32780;&#23427;&#20204;&#22312;&#20256;&#32479;&#30340;&#30830;&#23450;&#24615;&#39044;&#27979;&#25216;&#33021;&#25351;&#26631;&#19978;&#30340;&#20248;&#21183;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#36825;&#20123;&#29305;&#27530;&#24615;&#12290;&#19982;&#20854;&#20182;&#24403;&#21069;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
As in many other areas of engineering and applied science, Machine Learning (ML) is having a profound impact in the domain of Weather and Climate Prediction. A very recent development in this area has been the emergence of fully data-driven ML prediction models which routinely claim superior performance to that of traditional physics-based models. In this work, we examine some aspects of the forecasts produced by an exemplar of the current generation of ML models, Pangu-Weather, with a focus on the fidelity and physical consistency of those forecasts and how these characteristics relate to perceived forecast performance. The main conclusion is that Pangu-Weather forecasts, and by extension those of similar ML models, do not have the fidelity and physical consistency of physics-based models and their advantage in accuracy on traditional deterministic metrics of forecast skill can be attributed, to a large extent, to these peculiarities. Similarly to other current post-processing technol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36974;&#34109;&#27169;&#24577;&#24490;&#29615;&#19982;&#26465;&#20214;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#22810;&#27169;&#24577;MRI&#20013;&#30340;&#24322;&#24120;&#36827;&#34892;&#20998;&#21106;&#12290;&#26041;&#27861;&#22522;&#20110;&#24490;&#29615;&#27169;&#24577;&#36716;&#25442;&#21644;&#26465;&#20214;&#25193;&#25955;&#30340;&#24605;&#24819;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#35757;&#32451;&#20013;&#26410;&#36935;&#21040;&#30340;&#24322;&#24120;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.16150</link><description>&lt;p&gt;
&#20351;&#29992;&#36974;&#34109;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#24577;&#24490;&#29615;&#36827;&#34892;MRI&#26080;&#30417;&#30563;&#24322;&#24120;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI. (arXiv:2308.16150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36974;&#34109;&#27169;&#24577;&#24490;&#29615;&#19982;&#26465;&#20214;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#22810;&#27169;&#24577;MRI&#20013;&#30340;&#24322;&#24120;&#36827;&#34892;&#20998;&#21106;&#12290;&#26041;&#27861;&#22522;&#20110;&#24490;&#29615;&#27169;&#24577;&#36716;&#25442;&#21644;&#26465;&#20214;&#25193;&#25955;&#30340;&#24605;&#24819;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#35757;&#32451;&#20013;&#26410;&#36935;&#21040;&#30340;&#24322;&#24120;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#20998;&#21106;&#26088;&#22312;&#26816;&#27979;&#19982;&#35757;&#32451;&#36807;&#31243;&#20013;&#22788;&#29702;&#30340;&#20219;&#20309;&#27169;&#24335;&#19981;&#21516;&#30340;&#27169;&#24335;&#65292;&#36890;&#24120;&#31216;&#20026;&#24322;&#24120;&#25110;&#36229;&#20986;&#20998;&#24067;&#30340;&#27169;&#24335;&#65292;&#32780;&#19981;&#25552;&#20379;&#20219;&#20309;&#20851;&#32852;&#30340;&#25163;&#21160;&#20998;&#21106;&#12290;&#30001;&#20110;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#24322;&#24120;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22833;&#25928;&#65292;&#26816;&#27979;&#24322;&#24120;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#22312;&#21307;&#23398;&#25104;&#20687;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#36974;&#34109;&#27169;&#24577;&#24490;&#29615;&#19982;&#26465;&#20214;&#25193;&#25955;&#65288;MMCCD&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#27169;&#24577;MRI&#20013;&#20998;&#21106;&#21508;&#31181;&#27169;&#24335;&#30340;&#24322;&#24120;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#22522;&#26412;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24490;&#29615;&#27169;&#24577;&#36716;&#25442;&#20316;&#20026;&#21551;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#26426;&#21046;&#12290;&#22270;&#20687;&#36716;&#25442;&#27169;&#22411;&#23398;&#20064;&#32452;&#32455;&#29305;&#24322;&#30340;&#27169;&#24577;&#26144;&#23556;&#65292;&#36825;&#26159;&#32452;&#32455;&#29983;&#29702;&#23398;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#26144;&#23556;&#26080;&#27861;&#23558;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#26410;&#36935;&#21040;&#30340;&#32452;&#32455;&#25110;&#22270;&#20687;&#27169;&#24335;&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#20135;&#29983;&#38169;&#35823;&#65292;&#20351;&#24471;&#24322;&#24120;&#33021;&#22815;&#34987;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomaly segmentation aims to detect patterns that are distinct from any patterns processed during training, commonly called abnormal or out-of-distribution patterns, without providing any associated manual segmentations. Since anomalies during deployment can lead to model failure, detecting the anomaly can enhance the reliability of models, which is valuable in high-risk domains like medical imaging. This paper introduces Masked Modality Cycles with Conditional Diffusion (MMCCD), a method that enables segmentation of anomalies across diverse patterns in multimodal MRI. The method is based on two fundamental ideas. First, we propose the use of cyclic modality translation as a mechanism for enabling abnormality detection. Image-translation models learn tissue-specific modality mappings, which are characteristic of tissue physiology. Thus, these learned mappings fail to translate tissues or image patterns that have never been encountered during training, and the error enables
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;SMOTE&#21040;Mixup&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;SMOTE&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#32467;&#21512;Mixup&#25216;&#26415;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Mixup&#25216;&#26415;&#36890;&#36807;&#23454;&#29616;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38388;&#38553;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;Mixup&#25216;&#26415;&#65292;&#26356;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#19981;&#24179;&#34913;&#38388;&#38553;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15457</link><description>&lt;p&gt;
&#20174;SMOTE&#21040;Mixup&#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
From SMOTE to Mixup for Deep Imbalanced Classification. (arXiv:2308.15457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;SMOTE&#21040;Mixup&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;SMOTE&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#32467;&#21512;Mixup&#25216;&#26415;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Mixup&#25216;&#26415;&#36890;&#36807;&#23454;&#29616;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38388;&#38553;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;Mixup&#25216;&#26415;&#65292;&#26356;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#19981;&#24179;&#34913;&#38388;&#38553;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#22909;&#30340;&#20998;&#31867;&#22120;&#22240;&#20026;&#23569;&#25968;&#31867;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#32780;&#22256;&#38590;&#37325;&#37325;&#12290;&#20256;&#32479;&#19978;&#65292;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#30693;&#21517;&#23569;&#25968;&#31867;&#21512;&#25104;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#38754;&#21521;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#34987;&#29992;&#26469;&#25913;&#21892;&#36825;&#31181;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;SMOTE&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#26159;&#21542;&#20063;&#26377;&#30410;&#22788;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#21407;&#22987;&#30340;SMOTE&#23545;&#28145;&#24230;&#23398;&#20064;&#26469;&#35828;&#26159;&#19981;&#36275;&#30340;&#65292;&#24182;&#20351;&#29992;&#36719;&#26631;&#31614;&#22686;&#24378;&#20102;SMOTE&#12290;&#23558;&#24471;&#21040;&#30340;&#36719;SMOTE&#19982;Mixup&#65292;&#19968;&#31181;&#29616;&#20195;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#20256;&#32479;&#21644;&#29616;&#20195;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#32435;&#20837;&#21516;&#19968;&#20010;&#33539;&#30068;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#34920;&#26126;&#65292;Mixup&#36890;&#36807;&#38544;&#24335;&#22320;&#23454;&#29616;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38388;&#38553;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;Mixup&#25216;&#26415;&#65292;&#26356;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#19981;&#24179;&#34913;&#38388;&#38553;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#37117;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02618</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;GTFS: &#20174;&#25991;&#23383;&#21040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20132;&#36890;&#34892;&#25968;&#25454;&#21457;&#24067;&#26631;&#20934;General Transit Feed Specification&#65288;GTFS&#65289;&#26159;&#34920;&#26684;&#25968;&#25454;&#65292;&#20449;&#24687;&#20998;&#25955;&#22312;&#19981;&#21516;&#30340;&#25991;&#20214;&#20013;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#24037;&#20855;&#25110;&#21253;&#26469;&#26816;&#32034;&#20449;&#24687;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#36235;&#21183;&#20063;&#22312;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#30340;&#24819;&#27861;&#26159;&#30475;&#30475;&#24403;&#21069;&#24191;&#27867;&#37319;&#29992;&#30340;LLMs&#65288;ChatGPT&#65289;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20174;GTFS&#20013;&#26816;&#32034;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#27979;&#35797;ChatGPT&#65288;GPT-3.5&#65289;&#26159;&#21542;&#29702;&#35299;GTFS&#35268;&#33539;&#12290;GPT-3.5&#22312;&#25105;&#20204;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQ&#65289;&#20013;&#27491;&#30830;&#22238;&#31572;&#20102;77%&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#36807;&#28388;&#30340;GTFS&#25968;&#25454;&#38598;&#23545;LLM&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#12290;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#38646;-shot&#21644;&#31243;&#24207;&#21512;&#25104;&#12290;&#31243;&#24207;&#21512;&#25104;&#30340;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#31616;&#21333;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;90%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;40%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#20110;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#20250;&#22823;&#22823;&#38477;&#20302;&#23545;&#26410;&#32771;&#34385;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01923</link><description>&lt;p&gt;
&#22810;&#37325;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Fairness Improvement with Multiple Protected Attributes. (arXiv:2308.01923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#20110;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#20250;&#22823;&#22823;&#38477;&#20302;&#23545;&#26410;&#32771;&#34385;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36719;&#20214;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#65292;&#20294;&#32771;&#34385;&#21040;&#35768;&#22810;&#29992;&#25143;&#20855;&#26377;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#23545;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#32771;&#34385;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;ML&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25913;&#21892;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#22823;&#22823;&#38477;&#20302;&#20102;&#26410;&#32771;&#34385;&#30340;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;88.3&#65285;&#30340;&#24773;&#20917;&#19979;&#35266;&#23519;&#21040;&#36825;&#31181;&#38477;&#20302;&#65288;&#24179;&#22343;&#20026;57.5&#65285;&#65289;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#32771;&#34385;&#21333;&#20010;&#21644;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#20934;&#30830;&#29575;&#25439;&#22833;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#28023;&#24213;&#28369;&#32724;&#22120;&#22312;&#19981;&#21487;&#39044;&#27979;&#28023;&#27915;&#29615;&#22659;&#20013;&#27491;&#24120;&#25805;&#20316;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#28369;&#32724;&#22120;&#37096;&#32626;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#12290;&#31639;&#27861;&#33021;&#22815;&#23454;&#26102;&#25552;&#20379;&#24322;&#24120;&#35686;&#25253;&#65292;&#20351;&#39550;&#39542;&#21592;&#33021;&#22815;&#25511;&#21046;&#28369;&#32724;&#22120;&#24182;&#36991;&#20813;&#36827;&#19968;&#27493;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2308.00180</link><description>&lt;p&gt;
&#28023;&#24213;&#28369;&#32724;&#22120;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#39564;&#35777;&#26041;&#27861;&#8212;&#8212;&#22823;&#35268;&#27169;&#37096;&#32626;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset. (arXiv:2308.00180v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#28023;&#24213;&#28369;&#32724;&#22120;&#22312;&#19981;&#21487;&#39044;&#27979;&#28023;&#27915;&#29615;&#22659;&#20013;&#27491;&#24120;&#25805;&#20316;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#28369;&#32724;&#22120;&#37096;&#32626;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#12290;&#31639;&#27861;&#33021;&#22815;&#23454;&#26102;&#25552;&#20379;&#24322;&#24120;&#35686;&#25253;&#65292;&#20351;&#39550;&#39542;&#21592;&#33021;&#22815;&#25511;&#21046;&#28369;&#32724;&#22120;&#24182;&#36991;&#20813;&#36827;&#19968;&#27493;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#35780;&#20272;&#22312;&#19981;&#21487;&#39044;&#27979;&#30340;&#28023;&#27915;&#29615;&#22659;&#20013;&#28023;&#24213;&#28369;&#32724;&#22120;&#30340;&#27491;&#24120;&#25805;&#20316;&#12290;&#19968;&#26086;&#26816;&#27979;&#21040;&#20219;&#20309;&#24322;&#24120;&#65292;&#21487;&#20197;&#21521;&#28369;&#32724;&#22120;&#39550;&#39542;&#21592;&#25552;&#20379;&#23454;&#26102;&#35686;&#25253;&#65292;&#20351;&#20854;&#33021;&#22815;&#25509;&#31649;&#28369;&#32724;&#22120;&#24182;&#38450;&#27490;&#36827;&#19968;&#27493;&#30340;&#25439;&#23475;&#12290;&#35813;&#26816;&#27979;&#31639;&#27861;&#24212;&#29992;&#20110;&#30001;Skidaway&#28023;&#27915;&#30740;&#31350;&#25152;&#65288;SkIO&#65289;&#21644;&#21335;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#65288;USF&#65289;&#39046;&#23548;&#30340;&#23454;&#38469;&#28369;&#32724;&#22120;&#37096;&#32626;&#20013;&#25910;&#38598;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#12290;&#23601;&#27867;&#21270;&#24615;&#32780;&#35328;&#65292;&#23454;&#39564;&#35780;&#20272;&#21253;&#25324;&#31163;&#32447;&#21644;&#22312;&#32447;&#26816;&#27979;&#27169;&#24335;&#12290;&#31163;&#32447;&#26816;&#27979;&#21033;&#29992;&#23436;&#25972;&#30340;&#22238;&#25910;&#21518;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#30340;&#20449;&#24687;&#65292;&#23545;&#24322;&#24120;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#24182;&#19982;&#39550;&#39542;&#21592;&#26085;&#24535;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#32447;&#26816;&#27979;&#19987;&#27880;&#20110;&#20174;&#28369;&#32724;&#22120;&#20256;&#36755;&#30340;&#23454;&#26102;&#25968;&#25454;&#23376;&#38598;&#12290;&#34429;&#28982;&#23454;&#26102;&#25968;&#25454;&#21487;&#33021;&#19981;&#21253;&#21547;&#19982;&#22238;&#25910;&#21518;&#25968;&#25454;&#19968;&#26679;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20294;&#22312;&#32447;&#26816;&#27979;&#26159;&#23454;&#26102;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper employs an anomaly detection algorithm to assess the normal operation of underwater gliders in unpredictable ocean environments. Real-time alerts can be provided to glider pilots upon detecting any anomalies, enabling them to assume control of the glider and prevent further harm. The detection algorithm is applied to abundant data sets collected in real glider deployments led by the Skidaway Institute of Oceanography (SkIO) and the University of South Florida (USF). Regarding generality, the experimental evaluation is composed of both offline and online detection modes. The offline detection utilizes full post-recovery data sets, which carries high-resolution information, to present detailed analysis of the anomaly and compare it with pilot logs. The online detection focuses on the real-time subsets of data transmitted from the glider at the surfacing events. While the real-time data may not contain as much rich information as the post-recovery data, the online detection is 
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#30340;&#24615;&#36136;&#12289;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#24320;&#25918;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#20801;&#35768;&#26681;&#25454;&#20854;&#24615;&#36136;&#21644;&#38480;&#21046;&#36880;&#27493;&#21306;&#20998;GPAIS&#30340;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14283</link><description>&lt;p&gt;
&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#65306;&#24615;&#36136;&#12289;&#23450;&#20041;&#12289;&#20998;&#31867;&#12289;&#24320;&#25918;&#25361;&#25112;&#21644;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications. (arXiv:2307.14283v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#30340;&#24615;&#36136;&#12289;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#24320;&#25918;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#20801;&#35768;&#26681;&#25454;&#20854;&#24615;&#36136;&#21644;&#38480;&#21046;&#36880;&#27493;&#21306;&#20998;GPAIS&#30340;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#37117;&#35774;&#35745;&#29992;&#20110;&#29305;&#23450;&#21644;&#26377;&#38480;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#35768;&#22810;&#22330;&#26223;&#38656;&#35201;&#26356;&#36890;&#29992;&#30340;AI&#65292;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#32780;&#19981;&#38656;&#35201;&#19987;&#38376;&#20026;&#23427;&#20204;&#35774;&#35745;&#12290;&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#36825;&#20010;&#26415;&#35821;&#34987;&#23450;&#20041;&#20026;&#25351;&#20195;&#36825;&#20123;AI&#31995;&#32479;&#12290;&#23613;&#31649;&#36804;&#20170;&#20026;&#27490;&#65292;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#36275;&#22815;&#24378;&#22823;&#20197;&#27169;&#25311;&#20154;&#31867;&#24182;&#25913;&#36827;&#21508;&#31181;&#26234;&#21147;&#20219;&#21153;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#24895;&#26395;&#12289;&#34394;&#26500;&#30340;&#27010;&#24565;&#65292;&#24182;&#34987;&#35748;&#20026;&#23545;&#25105;&#20204;&#31038;&#20250;&#26500;&#25104;&#39118;&#38505;&#12290;&#34429;&#28982;&#25105;&#20204;&#31163;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#21487;&#33021;&#36824;&#24456;&#36965;&#36828;&#65292;&#20294;GPAIS&#26159;&#29616;&#23454;&#23384;&#22312;&#24182;&#20301;&#23621;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#21069;&#27839;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#29616;&#26377;GPAIS&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#20801;&#35768;&#26681;&#25454;&#20854;&#24615;&#36136;&#21644;&#38480;&#21046;&#36880;&#27493;&#21306;&#20998;GPAIS&#30340;&#31867;&#22411;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#23553;&#38381;&#19990;&#30028;&#21644;&#24320;&#25918;&#19990;&#30028;&#30340;GPAIS&#65292;&#25551;&#36848;&#20854;&#33258;&#20027;&#31243;&#24230;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General-Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research.  This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21407;&#20301;&#36827;&#34892;&#36731;&#37327;&#32423;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#21644;&#20302;&#38656;&#27714;&#30340;&#35745;&#31639;&#36164;&#28304;&#20419;&#36827;&#20102;&#20809;&#23398;&#35745;&#31639;&#25216;&#26415;&#20174;&#23454;&#39564;&#23460;&#30740;&#31350;&#21521;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2307.11957</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#20301;&#26080;&#27169;&#22411;&#20248;&#21270;&#23454;&#29616;&#39640;&#24615;&#33021;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
High-performance real-world optical computing trained by in situ model-free optimization. (arXiv:2307.11957v1 [physics.optics] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21407;&#20301;&#36827;&#34892;&#36731;&#37327;&#32423;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#21644;&#20302;&#38656;&#27714;&#30340;&#35745;&#31639;&#36164;&#28304;&#20419;&#36827;&#20102;&#20809;&#23398;&#35745;&#31639;&#25216;&#26415;&#20174;&#23454;&#39564;&#23460;&#30740;&#31350;&#21521;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#39640;&#36895;&#21644;&#20302;&#33021;&#32791;&#30340;&#25968;&#25454;&#22788;&#29702;&#65292;&#20294;&#22312;&#35745;&#31639;&#23494;&#38598;&#30340;&#35757;&#32451;&#21644;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#36716;&#25442;&#20013;&#23384;&#22312;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#26799;&#24230;&#20272;&#35745;&#31639;&#27861;&#30340;&#36731;&#37327;&#32423;&#21407;&#20301;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#31995;&#32479;&#35270;&#20026;&#40657;&#30418;&#23376;&#65292;&#30452;&#25509;&#23558;&#25439;&#22833;&#21453;&#21521;&#20256;&#25773;&#21040;&#20809;&#23398;&#26435;&#37325;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#35745;&#31639;&#23494;&#38598;&#21644;&#26377;&#20559;&#35265;&#30340;&#31995;&#32479;&#27169;&#25311;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#22312;&#21333;&#23618;&#34893;&#23556;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#22312;MNIST&#21644;FMNIST&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#26080;&#22270;&#29255;&#21644;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#65292;&#32467;&#21512;&#20854;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#20302;&#38656;&#27714;&#65292;&#21152;&#36895;&#20102;&#20809;&#23398;&#35745;&#31639;&#20174;&#23454;&#39564;&#23460;&#28436;&#31034;&#21040;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25968;&#38477;&#22122;&#31639;&#27861;&#65292;&#29992;&#20110;3D&#20998;&#23376;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#22122;&#22768;&#31574;&#30053;&#35299;&#20915;&#20102;&#26679;&#26412;&#35206;&#30422;&#29575;&#20302;&#21644;&#21508;&#21521;&#21516;&#24615;&#21147;&#22330;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35299;&#32806;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#20811;&#26381;&#20102;&#20256;&#32479;&#38477;&#22122;&#26041;&#27861;&#26080;&#27861;&#23398;&#20064;&#21147;&#22330;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10683</link><description>&lt;p&gt;
&#20998;&#25968;&#38477;&#22122;&#29992;&#20110;3D&#20998;&#23376;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Fractional Denoising for 3D Molecular Pre-training. (arXiv:2307.10683v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25968;&#38477;&#22122;&#31639;&#27861;&#65292;&#29992;&#20110;3D&#20998;&#23376;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#22122;&#22768;&#31574;&#30053;&#35299;&#20915;&#20102;&#26679;&#26412;&#35206;&#30422;&#29575;&#20302;&#21644;&#21508;&#21521;&#21516;&#24615;&#21147;&#22330;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35299;&#32806;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#20811;&#26381;&#20102;&#20256;&#32479;&#38477;&#22122;&#26041;&#27861;&#26080;&#27861;&#23398;&#20064;&#21147;&#22330;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22352;&#26631;&#38477;&#22122;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;3D&#20998;&#23376;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#33647;&#29289;&#21457;&#29616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#20854;&#30446;&#26631;&#31561;&#21516;&#20110;&#23398;&#20064;&#21147;&#22330;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#22352;&#26631;&#38477;&#22122;&#23398;&#20064;&#26377;&#25928;&#21147;&#22330;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65292;&#21363;&#26679;&#26412;&#35206;&#30422;&#29575;&#20302;&#21644;&#21508;&#21521;&#21516;&#24615;&#21147;&#22330;&#12290;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#29616;&#26377;&#38477;&#22122;&#26041;&#27861;&#25152;&#20551;&#35774;&#30340;&#20998;&#23376;&#20998;&#24067;&#19981;&#33021;&#25429;&#25417;&#20998;&#23376;&#30340;&#21508;&#21521;&#24322;&#24615;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#22122;&#22768;&#31574;&#30053;&#65292;&#21253;&#25324;&#20108;&#38754;&#35282;&#21644;&#22352;&#26631;&#30340;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#20197;&#20256;&#32479;&#26041;&#24335;&#38477;&#22122;&#36825;&#31181;&#28151;&#21512;&#22122;&#22768;&#19981;&#20877;&#31561;&#21516;&#20110;&#23398;&#20064;&#21147;&#22330;&#12290;&#36890;&#36807;&#29702;&#35770;&#25512;&#23548;&#65292;&#25105;&#20204;&#21457;&#29616;&#38382;&#39064;&#26159;&#30001;&#20110;&#36755;&#20837;&#26500;&#35937;&#23545;&#21327;&#26041;&#24046;&#30340;&#20381;&#36182;&#24615;&#25152;&#23548;&#33268;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Coordinate denoising is a promising 3D molecular pre-training method, which has achieved remarkable performance in various downstream drug discovery tasks. Theoretically, the objective is equivalent to learning the force field, which is revealed helpful for downstream tasks. Nevertheless, there are two challenges for coordinate denoising to learn an effective force field, i.e. low coverage samples and isotropic force field. The underlying reason is that molecular distributions assumed by existing denoising methods fail to capture the anisotropic characteristic of molecules. To tackle these challenges, we propose a novel hybrid noise strategy, including noises on both dihedral angel and coordinate. However, denoising such hybrid noise in a traditional way is no more equivalent to learning the force field. Through theoretical deductions, we find that the problem is caused by the dependency of the input conformation for covariance. To this end, we propose to decouple the two types of nois
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26223;&#35266;&#26367;&#20195;&#21697;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#19979;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#22120;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08964</link><description>&lt;p&gt;
&#26223;&#35266;&#26367;&#20195;&#21697;&#65306;&#22312;&#37096;&#20998;&#20449;&#24687;&#19979;&#23398;&#20064;&#25968;&#23398;&#20248;&#21270;&#30340;&#20915;&#31574;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26223;&#35266;&#26367;&#20195;&#21697;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#19979;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#22120;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23398;&#20064;&#38598;&#25104;&#20248;&#21270;&#24037;&#20316;&#22312;&#20248;&#21270;&#38382;&#39064;&#21482;&#26377;&#37096;&#20998;&#21487;&#35266;&#27979;&#25110;&#36890;&#29992;&#20248;&#21270;&#22120;&#22312;&#26080;&#19987;&#23478;&#35843;&#20248;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20248;&#21270;&#22120;$ \mathbf{g} $&#26469;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#12290;&#20248;&#21270;&#22120;&#21487;&#20197;&#36890;&#36807;&#24050;&#30693;&#26368;&#20248;&#35299;&#30340;&#30417;&#30563;&#25110;&#36890;&#36807;&#20248;&#21270;&#22797;&#21512;&#20989;&#25968;$ f\circ \mathbf{g} $&#30340;&#38544;&#24335;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#38544;&#24335;&#26041;&#27861;&#21487;&#33021;&#19981;&#38656;&#35201;&#26368;&#20248;&#35299;&#20316;&#20026;&#26631;&#31614;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#39057;&#32321;&#35843;&#29992;&#20248;&#21270;&#22120;$ \mathbf{g} $&#65292;&#22240;&#27492;&#35757;&#32451;&#21644;&#37096;&#32626;&#32531;&#24930;&#12290;&#23545;&#20110;&#32452;&#21512;&#27714;&#35299;&#22120;&#65292;&#30001;&#20110;$ \mathbf{g} $&#30340;&#31232;&#30095;&#26799;&#24230;&#65292;&#35757;&#32451;&#36827;&#19968;&#27493;&#21463;&#21040;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24179;&#28369;&#21487;&#23398;&#20064;&#30340;&#26223;&#35266;&#26367;&#20195;&#21697;$ M $&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replace
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#23612;&#31163;&#24046;&#26469;&#26367;&#20195;&#26041;&#24046;&#65292;&#32531;&#35299;&#20102;&#26041;&#24046;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08873</link><description>&lt;p&gt;
&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#24046;&#26367;&#20195;&#65306;&#22522;&#23612;&#31163;&#24046;
&lt;/p&gt;
&lt;p&gt;
An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#23612;&#31163;&#24046;&#26469;&#26367;&#20195;&#26041;&#24046;&#65292;&#32531;&#35299;&#20102;&#26041;&#24046;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39118;&#38505;&#21388;&#24694;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#38480;&#21046;&#31574;&#30053;&#22238;&#25253;&#30340;&#26041;&#24046;&#26159;&#19968;&#31181;&#24120;&#35265;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26126;&#30830;&#30340;&#25968;&#23398;&#23450;&#20041;&#21644;&#26131;&#20110;&#35299;&#37322;&#12290;&#20256;&#32479;&#26041;&#27861;&#30452;&#25509;&#38480;&#21046;&#24635;&#22238;&#25253;&#26041;&#24046;&#65292;&#32780;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#27599;&#27493;&#22870;&#21169;&#26041;&#24046;&#20316;&#20026;&#20195;&#29702;&#12290;&#26412;&#25991;&#24443;&#24213;&#30740;&#31350;&#20102;&#36825;&#20123;&#22522;&#20110;&#26041;&#24046;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#25968;&#23383;&#23610;&#24230;&#30340;&#25935;&#24863;&#24615;&#21644;&#38459;&#30861;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#26367;&#20195;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#8212;&#8212;&#22522;&#23612;&#31163;&#24046;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26032;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#30340;&#21508;&#31181;&#23646;&#24615;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#23567;&#21270;&#22522;&#23612;&#31163;&#24046;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#22312;&#39118;&#38505;&#21388;&#24694;&#21487;&#20197;&#26126;&#30830;&#23450;&#20041;&#30340;&#39046;&#22495;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#32531;&#35299;&#22522;&#20110;&#26041;&#24046;&#30340;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#20854;&#20182;&#31574;&#30053;&#26080;&#27861;&#23398;&#21040;&#21512;&#29702;&#31574;&#30053;&#26102;&#23454;&#29616;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#65292;&#20197;&#26041;&#24046;&#21644;&#22522;&#23612;&#31163;&#24046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#25552;&#39640;&#30456;&#24178;&#31995;&#32479;&#20013;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#19968;&#20010;"&#21333;&#19968;"&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#21457;&#23556;&#21151;&#29575;&#12289;&#31526;&#21495;&#36895;&#29575;&#25110;&#20256;&#36755;&#36317;&#31163;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23558;Q&#22240;&#23376;&#25552;&#39640;&#26368;&#39640;&#36798;4 dB&#12290;</title><link>http://arxiv.org/abs/2307.05374</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#25552;&#39640;&#30456;&#24178;&#20809;&#31995;&#32479;&#20013;&#31070;&#32463;&#32593;&#32476;&#22343;&#34913;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems. (arXiv:2307.05374v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05374
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#25552;&#39640;&#30456;&#24178;&#31995;&#32479;&#20013;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#19968;&#20010;"&#21333;&#19968;"&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#21457;&#23556;&#21151;&#29575;&#12289;&#31526;&#21495;&#36895;&#29575;&#25110;&#20256;&#36755;&#36317;&#31163;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23558;Q&#22240;&#23376;&#25552;&#39640;&#26368;&#39640;&#36798;4 dB&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#25552;&#39640;&#30456;&#24178;&#31995;&#32479;&#20013;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#30340;&#28789;&#27963;&#24615;&#12290;&#19982;&#24120;&#35268;&#25968;&#23383;&#26102;&#38047;&#24674;&#22797;&#26041;&#27861;&#30456;&#27604;&#65292;&#19968;&#20010;"&#21333;&#19968;"&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#22120;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#21457;&#23556;&#21151;&#29575;&#12289;&#31526;&#21495;&#36895;&#29575;&#25110;&#20256;&#36755;&#36317;&#31163;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23558;Q&#22240;&#23376;&#25552;&#39640;&#26368;&#39640;&#36798;4 dB&#12290;
&lt;/p&gt;
&lt;p&gt;
For the first time, multi-task learning is proposed to improve the flexibility of NN-based equalizers in coherent systems. A "single" NN-based equalizer improves Q-factor by up to 4 dB compared to CDC, without re-training, even with variations in launch power, symbol rate, or transmission distance.
&lt;/p&gt;</description></item><item><title>Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#26159;&#22686;&#24378;&#35760;&#24518;&#33021;&#21147;&#32780;&#19981;&#26159;&#25913;&#36827;&#20449;&#29992;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.03864</link><description>&lt;p&gt;
&#20309;&#26102;&#20351;&#29992;Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21457;&#20809;&#65311;&#20174;&#35760;&#24518;&#21644;&#20449;&#29992;&#20998;&#37197;&#20013;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment. (arXiv:2307.03864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03864
&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#26159;&#22686;&#24378;&#35760;&#24518;&#33021;&#21147;&#32780;&#19981;&#26159;&#25913;&#36827;&#20449;&#29992;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#20004;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;&#23398;&#20064;&#26377;&#25928;&#30340;&#36807;&#21435;&#21644;&#24403;&#21069;&#35266;&#27979;&#30340;&#34920;&#31034;&#65292;&#24182;&#30830;&#23450;&#34892;&#21160;&#22914;&#20309;&#24433;&#21709;&#26410;&#26469;&#30340;&#25910;&#30410;&#12290;&#36825;&#20004;&#20010;&#25361;&#25112;&#37117;&#28041;&#21450;&#21040;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#26550;&#26500;&#22312;&#35299;&#20915;&#28041;&#21450;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;Transformer&#22522;&#20110;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#24378;&#21170;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#65306;&#26159;&#22240;&#20026;&#23427;&#20204;&#23398;&#20064;&#20102;&#26377;&#25928;&#30340;&#35760;&#24518;&#65292;&#36824;&#26159;&#22240;&#20026;&#23427;&#20204;&#25191;&#34892;&#20102;&#26377;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65311;&#22312;&#24341;&#20837;&#35760;&#24518;&#38271;&#24230;&#21644;&#20449;&#29992;&#20998;&#37197;&#38271;&#24230;&#30340;&#24418;&#24335;&#23450;&#20041;&#20043;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31616;&#21333;&#30340;&#21487;&#37197;&#32622;&#20219;&#21153;&#26469;&#27979;&#37327;&#36825;&#20123;&#19981;&#21516;&#30340;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#21487;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;&#38656;&#35201;&#35760;&#20303;1500&#27493;&#21069;&#35266;&#23519;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;Transformer&#26080;&#27861;&#25913;&#36827;&#38271;&#26399;&#30340;&#20449;&#29992;&#20998;&#37197;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35760;&#24518;&#21644;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) algorithms face two distinct challenges: learning effective representations of past and present observations, and determining how actions influence future returns. Both challenges involve modeling long-term dependencies. The transformer architecture has been very successful to solve problems that involve long-term dependencies, including in the RL domain. However, the underlying reason for the strong performance of Transformer-based RL methods remains unclear: is it because they learn effective memory, or because they perform effective credit assignment? After introducing formal definitions of memory length and credit assignment length, we design simple configurable tasks to measure these distinct quantities. Our empirical results reveal that Transformers can enhance the memory capacity of RL algorithms, scaling up to tasks that require memorizing observations $1500$ steps ago. However, Transformers do not improve long-term credit assignment. In summary, our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#21644;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#21457;&#29616;&#65292;Bellman &#35823;&#24046;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02345</link><description>&lt;p&gt;
LLQL: &#36923;&#36753;&#20284;&#28982; Q-Learning &#29992;&#20110;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning. (arXiv:2307.02345v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#21644;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#21457;&#29616;&#65292;Bellman &#35823;&#24046;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#20998;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447;&#20004;&#31181;&#21464;&#20307;&#12290;&#20316;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447; RL &#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#24403;&#21069;&#23545; Bellman &#26041;&#31243;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#25216;&#26415;&#21644;&#24615;&#33021;&#22686;&#24378;&#19978;&#65292;&#32780;&#19981;&#26159;&#25506;&#32034; Bellman &#35823;&#24046;&#30340;&#22266;&#26377;&#32467;&#26500;&#29305;&#24615;&#65292;&#22914;&#20854;&#20998;&#24067;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545; Bellman &#26041;&#31243;&#36827;&#34892;&#36845;&#20195;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#22312;&#32447; RL &#21644;&#31163;&#32447; RL &#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26080;&#35770;&#26159;&#22312;&#32447; RL &#36824;&#26159;&#31163;&#32447; RL&#65292;Bellman &#35823;&#24046;&#37117;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#65288;LLoss&#65289;&#20316;&#20026;&#24120;&#29992;&#30340; MSE Loss &#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20551;&#35774; Bellman &#35823;&#24046;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#22312;&#19981;&#21516;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern reinforcement learning (RL) can be categorized into online and offline variants. As a pivotal aspect of both online and offline RL, current research on the Bellman equation revolves primarily around optimization techniques and performance enhancement rather than exploring the inherent structural properties of the Bellman error, such as its distribution characteristics. This study investigates the distribution of the Bellman approximation error in both online and offline settings through iterative exploration of the Bellman equation. We observed that both in online RL and offline RL, the Bellman error conforms to a Logistic distribution. Building upon this discovery, this study employed the Logistics maximum likelihood function (LLoss) as an alternative to the commonly used MSE Loss, assuming that Bellman errors adhere to a normal distribution. We validated our hypotheses through extensive numerical experiments across diverse online and offline environments. In particular, we app
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24773;&#22659;&#36172;&#21338;&#35774;&#32622;&#30340;&#26032;&#22411;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#36172;&#21338;&#31639;&#27861;&#65292;&#20855;&#26377;&#31616;&#21333;&#21644;&#32047;&#31215;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20248;&#21183;&#65292;&#24182;&#21487;&#33258;&#36866;&#24212;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#21644;&#36830;&#32493;&#33218;&#35774;&#32622;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;"&#19968;&#33268;&#33218;&#38598;"&#65288;CAS&#65289;&#26469;&#25552;&#20379;&#22312;&#27599;&#20010;&#24773;&#22659;&#19979;&#22218;&#25324;&#24773;&#22659;&#29305;&#23450;&#30340;&#26368;&#20339;&#33218;&#30340;&#19968;&#32452;&#33218;&#65292;&#36328;&#36234;&#24773;&#22659;&#20998;&#24067;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#31616;&#21333;&#21644;&#32047;&#31215;&#36951;&#25022;&#20445;&#35777;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#27491;&#38754;&#32467;&#26524;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#26080;&#27861;&#23454;&#29616;&#23454;&#20363;&#20381;&#36182;&#24615;&#30340;&#31616;&#21333;&#36951;&#25022;&#20445;&#35777;&#30340;&#28040;&#26497;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.02108</link><description>&lt;p&gt;
&#27604;&#20363;&#21709;&#24212;&#65306;&#29992;&#20110;&#31616;&#21333;&#21644;&#32047;&#31215;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization. (arXiv:2307.02108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02108
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24773;&#22659;&#36172;&#21338;&#35774;&#32622;&#30340;&#26032;&#22411;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#36172;&#21338;&#31639;&#27861;&#65292;&#20855;&#26377;&#31616;&#21333;&#21644;&#32047;&#31215;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20248;&#21183;&#65292;&#24182;&#21487;&#33258;&#36866;&#24212;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#21644;&#36830;&#32493;&#33218;&#35774;&#32622;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;"&#19968;&#33268;&#33218;&#38598;"&#65288;CAS&#65289;&#26469;&#25552;&#20379;&#22312;&#27599;&#20010;&#24773;&#22659;&#19979;&#22218;&#25324;&#24773;&#22659;&#29305;&#23450;&#30340;&#26368;&#20339;&#33218;&#30340;&#19968;&#32452;&#33218;&#65292;&#36328;&#36234;&#24773;&#22659;&#20998;&#24067;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#31616;&#21333;&#21644;&#32047;&#31215;&#36951;&#25022;&#20445;&#35777;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#27491;&#38754;&#32467;&#26524;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#26080;&#27861;&#23454;&#29616;&#23454;&#20363;&#20381;&#36182;&#24615;&#30340;&#31616;&#21333;&#36951;&#25022;&#20445;&#35777;&#30340;&#28040;&#26497;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#30005;&#23376;&#21830;&#21153;&#31561;&#39046;&#22495;&#65292;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#26159;&#23398;&#20064;&#26368;&#20339;&#27835;&#30103;&#20998;&#37197;&#31574;&#30053;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24773;&#22659;&#36172;&#21338;&#35774;&#32622;&#20013;&#30340;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#20173;&#26410;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#36172;&#21338;&#31639;&#27861;&#26063;&#65292;&#38024;&#23545;&#38543;&#26426;&#24773;&#22659;&#36172;&#21338;&#35774;&#32622;&#65292;&#22312;&#32047;&#31215;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#26497;&#23567;&#26497;&#22823;&#20445;&#35777;&#65289;&#21644;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;&#20855;&#26377;SOTA&#20445;&#35777;&#65289;&#26041;&#38754;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#24182;&#25193;&#23637;&#21040;&#36830;&#32493;&#33218;&#35774;&#32622;&#12290;&#36825;&#20123;&#20248;&#21183;&#26469;&#33258;&#20110;&#26500;&#24314;&#21644;&#20381;&#36182;&#20110;&#8220;&#19968;&#33268;&#33218;&#38598;&#8221;&#65288;CAS&#65289;&#65292;CAS&#22312;&#27599;&#20010;&#24773;&#22659;&#19979;&#25552;&#20379;&#19968;&#32452;&#33218;&#65292;&#36825;&#20123;&#33218;&#20197;&#19968;&#23450;&#30340;&#27010;&#29575;&#22218;&#25324;&#20102;&#24773;&#22659;&#29305;&#23450;&#30340;&#26368;&#20339;&#33218;&#65292;&#36328;&#36234;&#20102;&#24773;&#22659;&#20998;&#24067;&#12290;&#25105;&#20204;&#20851;&#20110;&#31616;&#21333;&#21644;&#32047;&#31215;&#36951;&#25022;&#20445;&#35777;&#30340;&#31215;&#26497;&#32467;&#26524;&#19982;&#19968;&#20010;&#28040;&#26497;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#34920;&#26126;&#19968;&#20010;&#31639;&#27861;&#26080;&#27861;&#23454;&#29616;&#23454;&#20363;&#20381;&#36182;&#24615;&#30340;&#31616;&#21333;&#36951;&#25022;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simple regret minimization is a critical problem in learning optimal treatment assignment policies across various domains, including healthcare and e-commerce. However, it remains understudied in the contextual bandit setting. We propose a new family of computationally efficient bandit algorithms for the stochastic contextual bandit settings, with the flexibility to be adapted for cumulative regret minimization (with near-optimal minimax guarantees) and simple regret minimization (with SOTA guarantees). Furthermore, our algorithms adapt to model misspecification and extend to the continuous arm settings. These advantages come from constructing and relying on "conformal arm sets" (CASs), which provide a set of arms at every context that encompass the context-specific optimal arm with some probability across the context distribution. Our positive results on simple and cumulative regret guarantees are contrasted by a negative result, which shows that an algorithm can't achieve instance-de
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#26041;&#21521;&#19978;&#25193;&#23637;&#20102;&#24050;&#26377;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#21487;&#20197;&#22686;&#21152;&#37325;&#24314;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#31070;&#32463;&#20803;&#25968;&#37327;&#23545;&#32593;&#32476;&#26131;&#21463;&#37325;&#24314;&#26041;&#26696;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.01827</link><description>&lt;p&gt;
&#35299;&#26500;&#25968;&#25454;&#37325;&#24314;&#65306;&#22810;&#31867;&#21035;&#12289;&#26435;&#37325;&#34928;&#20943;&#21644;&#36890;&#29992;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses. (arXiv:2307.01827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01827
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#26041;&#21521;&#19978;&#25193;&#23637;&#20102;&#24050;&#26377;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#21487;&#20197;&#22686;&#21152;&#37325;&#24314;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#31070;&#32463;&#20803;&#25968;&#37327;&#23545;&#32593;&#32476;&#26131;&#21463;&#37325;&#24314;&#26041;&#26696;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#36816;&#20316;&#30340;&#29702;&#35299;&#36824;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#26368;&#36817;&#65292;Haim&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#26696;&#65292;&#21487;&#20197;&#20174;&#22810;&#23618;&#24863;&#30693;&#22120;&#20108;&#20803;&#20998;&#31867;&#22120;&#20013;&#37325;&#24314;&#35757;&#32451;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#32593;&#32476;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#22823;&#37096;&#20998;&#35757;&#32451;&#26679;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#26041;&#21521;&#19978;&#25193;&#23637;&#20102;&#20182;&#20204;&#30340;&#21457;&#29616;&#65292;&#21253;&#25324;&#20174;&#22810;&#31867;&#21035;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#37325;&#24314;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#37325;&#24314;&#26041;&#26696;&#65292;&#21487;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#22238;&#24402;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24433;&#21709;&#32593;&#32476;&#26131;&#21463;&#27492;&#31867;&#37325;&#24314;&#26041;&#26696;&#24433;&#21709;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#20250;&#22686;&#21152;&#37325;&#24314;&#33021;&#21147;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#37327;&#36824;&#26159;&#36136;&#37327;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26816;&#39564;&#20102;&#31070;&#32463;&#20803;&#25968;&#37327;&#30456;&#23545;&#20110;&#35757;&#32451;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memorization of training data is an active research area, yet our understanding of the inner workings of neural networks is still in its infancy. Recently, Haim et al. (2022) proposed a scheme to reconstruct training samples from multilayer perceptron binary classifiers, effectively demonstrating that a large portion of training samples are encoded in the parameters of such networks. In this work, we extend their findings in several directions, including reconstruction from multiclass and convolutional neural networks. We derive a more general reconstruction scheme which is applicable to a wider range of loss functions such as regression losses. Moreover, we study the various factors that contribute to networks' susceptibility to such reconstruction schemes. Intriguingly, we observe that using weight decay during training increases reconstructability both in terms of quantity and quality. Additionally, we examine the influence of the number of neurons relative to the number of training
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#36816;&#36755;&#21644;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31354;&#38388;&#25955;&#24230;&#30340;&#37319;&#26679;&#21644;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#39062;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#22238;&#28779;&#27969;&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#30446;&#26631;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.01050</link><description>&lt;p&gt;
&#36816;&#36755;&#12289;&#21464;&#20998;&#25512;&#26029;&#21644;&#25193;&#25955;&#65306;&#24212;&#29992;&#20110;&#22238;&#28779;&#27969;&#21644;&#34203;&#23450;&#35860;&#26725;&#30340;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges. (arXiv:2307.01050v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#36816;&#36755;&#21644;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31354;&#38388;&#25955;&#24230;&#30340;&#37319;&#26679;&#21644;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#39062;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#22238;&#28779;&#27969;&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#30446;&#26631;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#20248;&#36816;&#36755;&#19982;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#27491;&#21521;&#21644;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#21450;Girsanov&#21464;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36335;&#24452;&#31354;&#38388;&#25955;&#24230;&#30340;&#37319;&#26679;&#21644;&#29983;&#25104;&#24314;&#27169;&#30340;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26368;&#32456;&#21457;&#23637;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#22238;&#28779;&#27969;&#25216;&#26415;&#65288;&#19982;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;Jarzynski&#21644;Crooks&#24658;&#31561;&#24335;&#26377;&#20851;&#65289;&#21644;&#19968;&#20010;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#65288;IPF&#65289;&#22411;&#30446;&#26631;&#65292;&#19981;&#21516;&#20110;&#26631;&#20934;IPF&#30340;&#39034;&#24207;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#29983;&#25104;&#24314;&#27169;&#31034;&#20363;&#21644;&#22522;&#20110;&#21452;&#20117;&#30340;&#31232;&#26377;&#20107;&#20214;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the connections between optimal transport and variational inference, with a focus on forward and reverse time stochastic differential equations and Girsanov transformations.We present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of a novel score-based annealed flow technique (with connections to Jarzynski and Crooks identities from statistical physics) and a regularised iterative proportional fitting (IPF)-type objective, departing from the sequential nature of standard IPF. Through a series of generative modelling examples and a double-well-based rare event task, we showcase the potential of the proposed methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#23485;&#26494;Pareto&#38598;&#30340;&#35782;&#21035;&#65292;&#36890;&#36807;&#25918;&#26494;&#31574;&#30053;&#26469;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.00424</link><description>&lt;p&gt;
&#23485;&#26494;Pareto&#38598;&#35782;&#21035;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Algorithms for Relaxed Pareto Set Identification. (arXiv:2307.00424v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#23485;&#26494;Pareto&#38598;&#30340;&#35782;&#21035;&#65292;&#36890;&#36807;&#25918;&#26494;&#31574;&#30053;&#26469;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#22810;&#30446;&#26631;&#22810;&#33218;&#36172;&#21338;&#26426;&#27169;&#22411;&#20013;&#22266;&#23450;&#32622;&#20449;&#24230;&#19979;&#30340;Pareto&#26368;&#20248;&#38598;&#21512;&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;&#30001;&#20110;&#20934;&#30830;&#35782;&#21035;Pareto&#38598;&#21512;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#33021;&#38750;&#24120;&#22823;&#65292;&#22240;&#27492;&#30740;&#31350;&#20102;&#20801;&#35768;&#36755;&#20986;&#19968;&#20123;&#39069;&#22806;&#36817;&#20284;&#26368;&#20248;&#33218;&#30340;&#25918;&#26494;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20854;&#20182;&#20801;&#35768;&#35782;&#21035;Pareto&#38598;&#21512;&#30340;&#30456;&#20851;&#23376;&#38598;&#30340;&#25918;&#26494;&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#36866;&#24212;Pareto&#25506;&#32034;&#30340;&#21333;&#19968;&#25277;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#19982;&#19981;&#21516;&#30340;&#20572;&#27490;&#35268;&#21017;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#32771;&#34385;Pareto&#38598;&#21512;&#35782;&#21035;&#38382;&#39064;&#30340;&#19981;&#21516;&#25918;&#26494;&#31574;&#30053;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#19981;&#21516;&#32452;&#21512;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#29305;&#21035;&#37327;&#21270;&#20102;&#22312;&#23547;&#25214;&#35782;&#21035;&#26368;&#22810;$k$&#20010;Pareto&#26368;&#20248;&#33218;&#26102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#20943;&#23569;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#36866;&#24212;Pareto&#25506;&#32034;&#22312;&#19968;&#20010;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#33391;&#22909;&#23454;&#38469;&#24615;&#33021;&#65292;&#20854;&#20013;&#25105;&#20204;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#20102;&#20960;&#31181;&#30123;&#33495;&#25509;&#31181;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we revisit the fixed-confidence identification of the Pareto optimal set in a multi-objective multi-armed bandit model. As the sample complexity to identify the exact Pareto set can be very large, a relaxation allowing to output some additional near-optimal arms has been studied. In this work we also tackle alternative relaxations that allow instead to identify a relevant subset of the Pareto set. Notably, we propose a single sampling strategy, called Adaptive Pareto Exploration, that can be used in conjunction with different stopping rules to take into account different relaxations of the Pareto Set Identification problem. We analyze the sample complexity of these different combinations, quantifying in particular the reduction in sample complexity that occurs when one seeks to identify at most $k$ Pareto optimal arms. We showcase the good practical performance of Adaptive Pareto Exploration on a real-world scenario, in which we adaptively explore several vaccination stra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27599;&#20010;&#21608;&#26399;&#23558;&#19968;&#21333;&#20301;&#21487;&#20998;&#36164;&#28304;&#20998;&#37197;&#21040;&#22810;&#20010;&#33218;&#19978;&#30340;&#38382;&#39064;&#65292;&#33218;&#19978;&#30340;&#22870;&#21169;&#26159;&#26410;&#30693;&#21644;&#38543;&#26426;&#30340;&#65292;&#32780;&#19988;&#19982;&#20998;&#37197;&#30340;&#36164;&#28304;&#25104;&#27604;&#20363;&#65292;&#32780;&#26041;&#24046;&#19982;&#20998;&#37197;&#36164;&#28304;&#30340;&#38454;&#25968;&#25104;&#27604;&#20363;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#38454;&#25968;&#19979;&#30340;&#26368;&#20248;&#26377;&#30028;&#21644;&#26080;&#30028;&#36951;&#25022;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#38454;&#25968;&#20026;1/2&#26102;&#23384;&#22312;&#30456;&#21464;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2306.16578</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#26410;&#30693;&#21644;&#38543;&#26426;&#22870;&#21169;&#30340;&#33218;&#19978;&#20998;&#37197;&#21487;&#20998;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Allocating Divisible Resources on Arms with Unknown and Random Rewards. (arXiv:2306.16578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27599;&#20010;&#21608;&#26399;&#23558;&#19968;&#21333;&#20301;&#21487;&#20998;&#36164;&#28304;&#20998;&#37197;&#21040;&#22810;&#20010;&#33218;&#19978;&#30340;&#38382;&#39064;&#65292;&#33218;&#19978;&#30340;&#22870;&#21169;&#26159;&#26410;&#30693;&#21644;&#38543;&#26426;&#30340;&#65292;&#32780;&#19988;&#19982;&#20998;&#37197;&#30340;&#36164;&#28304;&#25104;&#27604;&#20363;&#65292;&#32780;&#26041;&#24046;&#19982;&#20998;&#37197;&#36164;&#28304;&#30340;&#38454;&#25968;&#25104;&#27604;&#20363;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#38454;&#25968;&#19979;&#30340;&#26368;&#20248;&#26377;&#30028;&#21644;&#26080;&#30028;&#36951;&#25022;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#38454;&#25968;&#20026;1/2&#26102;&#23384;&#22312;&#30456;&#21464;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#20915;&#31574;&#32773;&#22312;&#27599;&#20010;&#21608;&#26399;&#23558;&#19968;&#20010;&#21487;&#20877;&#29983;&#21644;&#21487;&#20998;&#36164;&#28304;&#20998;&#37197;&#21040;&#22810;&#20010;&#33218;&#19978;&#12290;&#36825;&#20123;&#33218;&#20855;&#26377;&#26410;&#30693;&#21644;&#38543;&#26426;&#30340;&#22870;&#21169;&#65292;&#20854;&#22343;&#20540;&#19982;&#20998;&#37197;&#30340;&#36164;&#28304;&#25104;&#27604;&#20363;&#65292;&#26041;&#24046;&#19982;&#20998;&#37197;&#36164;&#28304;&#30340;&#38454;&#25968;$b$&#25104;&#27604;&#20363;&#12290;&#29305;&#21035;&#22320;&#65292;&#22914;&#26524;&#20915;&#31574;&#32773;&#22312;&#19968;&#20010;&#21608;&#26399;&#23558;&#36164;&#28304;$A_i$&#20998;&#37197;&#32473;&#33218;$i$&#65292;&#37027;&#20040;&#22870;&#21169;$Y_i$&#26159;$Y_i(A_i)=A_i\mu_i+A_i^b\xi_i$&#65292;&#20854;&#20013;$\mu_i$&#26159;&#26410;&#30693;&#30340;&#22343;&#20540;&#65292;&#22122;&#22768;$\xi_i$&#26159;&#29420;&#31435;&#19988;&#23376;&#39640;&#26031;&#30340;&#12290;&#24403;&#38454;&#25968;$b$&#20174;0&#21040;1&#21464;&#21270;&#26102;&#65292;&#35813;&#26694;&#26550;&#24179;&#28369;&#22320;&#36830;&#25509;&#20102;&#26631;&#20934;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#24102;&#26377;&#23436;&#20840;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#23427;&#20204;&#23454;&#29616;&#20102;$b\in[0,1]$&#26102;&#30340;&#26368;&#20248;&#26377;&#30028;&#24046;&#21644;&#26080;&#30028;&#24046;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;$b=1/2$&#22788;&#30340;&#30456;&#21464;&#12290;&#29702;&#35770;&#32467;&#26524;&#20381;&#36182;&#20110;&#25105;&#20204;&#24320;&#21457;&#30340;&#19968;&#31181;&#26032;&#22411;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#23427;&#38480;&#21046;&#20102;&#23376;&#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a decision maker allocating one unit of renewable and divisible resource in each period on a number of arms. The arms have unknown and random rewards whose means are proportional to the allocated resource and whose variances are proportional to an order $b$ of the allocated resource. In particular, if the decision maker allocates resource $A_i$ to arm $i$ in a period, then the reward $Y_i$ is$Y_i(A_i)=A_i \mu_i+A_i^b \xi_{i}$, where $\mu_i$ is the unknown mean and the noise $\xi_{i}$ is independent and sub-Gaussian. When the order $b$ ranges from 0 to 1, the framework smoothly bridges the standard stochastic multi-armed bandit and online learning with full feedback. We design two algorithms that attain the optimal gap-dependent and gap-independent regret bounds for $b\in [0,1]$, and demonstrate a phase transition at $b=1/2$. The theoretical results hinge on a novel concentration inequality we have developed that bounds a linear combination of sub-Gaussian random variables w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#35270;&#22120;&#24341;&#23548;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#31867;&#22411;&#12289;&#21151;&#33021;&#25110;API&#31561;&#20840;&#23616;&#19978;&#19979;&#25991;&#26102;&#65292;&#33021;&#22815;&#25552;&#39640;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10763</link><description>&lt;p&gt;
&#20351;&#29992;&#30417;&#35270;&#22120;&#24341;&#23548;&#20840;&#23616;&#19978;&#19979;&#25991;&#25351;&#23548;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Guiding Language Models of Code with Global Context using Monitors. (arXiv:2306.10763v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#35270;&#22120;&#24341;&#23548;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#31867;&#22411;&#12289;&#21151;&#33021;&#25110;API&#31561;&#20840;&#23616;&#19978;&#19979;&#25991;&#26102;&#65292;&#33021;&#22815;&#25552;&#39640;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21608;&#22260;&#20195;&#30721;&#25552;&#20379;&#36275;&#22815;&#19978;&#19979;&#25991;&#26102;&#25928;&#26524;&#24456;&#22909;&#12290;&#20294;&#24403;&#38656;&#35201;&#22312;&#23384;&#20648;&#24211;&#25110;&#38142;&#25509;&#24211;&#20013;&#20351;&#29992;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#22411;&#12289;&#21151;&#33021;&#25110;API&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#23601;&#19981;&#20877;&#25104;&#31435;&#12290;LMs&#22312;&#23545;&#36825;&#31181;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#24847;&#35782;&#26377;&#38480;&#26102;&#20250;&#20986;&#29616;&#38169;&#35823;&#39044;&#27979;&#30340;&#24773;&#20917;&#12290;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDEs&#65289;&#36890;&#36807;&#38745;&#24577;&#20998;&#26512;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#20102;&#35299;&#23384;&#20648;&#24211;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#23558;&#24320;&#21457;&#20154;&#21592;&#20139;&#21463;&#21040;&#30340;&#36825;&#31181;&#24110;&#21161;&#25193;&#23637;&#21040;&#20102;LMs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#35270;&#22120;&#24341;&#23548;&#35299;&#30721;&#65288;MGD&#65289;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#30417;&#35270;&#22120;&#20351;&#29992;&#38745;&#24577;&#20998;&#26512;&#26469;&#24341;&#23548;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;Java&#26041;&#27861;&#34917;&#20840;&#30340;&#23384;&#20648;&#24211;&#32423;&#25968;&#25454;&#38598;PragmaticCode&#65292;&#24182;&#22312;&#20854;&#19978;&#35780;&#20272;&#20102;MGD&#12290;&#22312;&#19981;&#21516;&#21442;&#25968;&#35268;&#27169;&#30340;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#30417;&#35270;&#31867;&#22411;&#19968;&#33268;&#30340;&#23545;&#35937;&#35299;&#24341;&#29992;&#65292;MGD&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#32534;&#35793;&#29575;&#24182;&#19982;&#30495;&#23454;&#32467;&#26524;&#36798;&#25104;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#26356;&#23569;&#21442;&#25968;&#30340;LMs&#65292;&#22312;&#19982;MGD&#30456;&#32467;&#21512;&#26102;&#33021;&#22815;&#36229;&#36234;&#26356;&#22823;&#30340;LMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.  Integrated development environments (IDEs) assist developers in understanding repository context using static analysis. We extend this assistance, enjoyed by developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LM
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#30340;&#39318;&#20010;&#26377;&#38480;&#26102;&#38388;&#23545;&#25968;&#36951;&#25022;&#36793;&#30028;&#65292;&#24182;&#29992;&#20110;&#39640;&#26031;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#20808;&#39564;&#20215;&#20540;&#20197;&#21450;&#23545;$\tilde{O}(\sqrt{n})$&#30028;&#38480;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2306.09136</link><description>&lt;p&gt;
&#23545;&#25968;&#36125;&#21494;&#26031;&#36951;&#25022;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Logarithmic Bayes Regret Bounds. (arXiv:2306.09136v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#30340;&#39318;&#20010;&#26377;&#38480;&#26102;&#38388;&#23545;&#25968;&#36951;&#25022;&#36793;&#30028;&#65292;&#24182;&#29992;&#20110;&#39640;&#26031;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#20808;&#39564;&#20215;&#20540;&#20197;&#21450;&#23545;$\tilde{O}(\sqrt{n})$&#30028;&#38480;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#23548;&#20986;&#20102;&#39318;&#20010;&#26377;&#38480;&#26102;&#38388;&#23545;&#25968;&#36951;&#25022;&#36793;&#30028;&#12290;&#23545;&#20110;&#39640;&#26031;&#36172;&#21338;&#26426;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;$O(c_h \log^2 n)$&#30340;&#36793;&#30028;&#65292;&#20854;&#20013;$c_h$&#26159;&#19982;&#20808;&#39564;&#30456;&#20851;&#30340;&#24120;&#37327;&#12290;&#36825;&#19982;Lai&#65288;1987&#65289;&#30340;&#28176;&#36817;&#19979;&#38480;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#26377;&#25152;&#19981;&#21516;&#65292;&#19988;&#31616;&#21333;&#19988;&#26222;&#36941;&#12290;&#20026;&#20102;&#26174;&#31034;&#19968;&#33324;&#24615;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#24212;&#29992;&#20110;&#32447;&#24615;&#36172;&#21338;&#26426;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#38416;&#26126;&#20102;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#20808;&#39564;&#30340;&#20215;&#20540;&#65292;&#26082;&#21487;&#20197;&#20316;&#20026;&#30446;&#26631;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#20256;&#36882;&#32473;&#23398;&#20064;&#32773;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#23427;&#20204;&#26174;&#30528;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;$\tilde{O}(\sqrt{n})$&#30028;&#38480;&#65292;&#23613;&#31649;&#23384;&#22312;&#19979;&#38480;&#65292;&#20294;&#24050;&#25104;&#20026;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive the first finite-time logarithmic regret bounds for Bayesian bandits. For Gaussian bandits, we obtain a $O(c_h \log^2 n)$ bound, where $c_h$ is a prior-dependent constant. This matches the asymptotic lower bound of Lai (1987). Our proofs mark a technical departure from prior works, and are simple and general. To show generality, we apply our technique to linear bandits. Our bounds shed light on the value of the prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve the $\tilde{O}(\sqrt{n})$ bounds, that despite the existing lower bounds, have become standard in the literature.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;Hopfield-like&#31070;&#32463;&#32593;&#32476;&#24207;&#21015;&#35760;&#24518;&#27169;&#22411;&#30340;&#24207;&#21015;&#23481;&#37327;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#39033;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;Hopfield&#32593;&#32476;&#65292;&#21516;&#26102;&#20063;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22238;&#24518;&#35268;&#21017;&#20197;&#22238;&#24518;&#36830;&#32493;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2306.04532</link><description>&lt;p&gt;
&#38271;&#24207;&#21015; Hopfield&#20869;&#23384;
&lt;/p&gt;
&lt;p&gt;
Long Sequence Hopfield Memory. (arXiv:2306.04532v2 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04532
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;Hopfield-like&#31070;&#32463;&#32593;&#32476;&#24207;&#21015;&#35760;&#24518;&#27169;&#22411;&#30340;&#24207;&#21015;&#23481;&#37327;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#39033;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;Hopfield&#32593;&#32476;&#65292;&#21516;&#26102;&#20063;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22238;&#24518;&#35268;&#21017;&#20197;&#22238;&#24518;&#36830;&#32493;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#35760;&#24518;&#26159;&#33258;&#28982;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#23646;&#24615;&#65292;&#23427;&#20351;&#20195;&#29702;&#33021;&#22815;&#32534;&#30721;&#12289;&#23384;&#20648;&#21644;&#26816;&#32034;&#22797;&#26434;&#30340;&#21050;&#28608;&#21644;&#34892;&#20026;&#24207;&#21015;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35745;&#31639;&#27169;&#22411;&#65292;&#20854;&#20013;&#29992;&#26102;&#38388;&#38750;&#23545;&#31216;&#30340;Hebbian&#35268;&#21017;&#35757;&#32451;&#36882;&#24402;&#30340;Hopfield&#26679;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#30001;&#20110;&#35760;&#24518;&#20043;&#38388;&#30340;&#24178;&#25200;&#32780;&#20855;&#26377;&#26377;&#38480;&#30340;&#24207;&#21015;&#23481;&#37327;&#65288;&#23384;&#20648;&#24207;&#21015;&#30340;&#26368;&#22823;&#38271;&#24230;&#65289;&#12290;&#21463;&#23494;&#38598;&#20851;&#32852;&#35760;&#24518;&#30340;&#26368;&#26032;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#39033;&#26469;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#24207;&#21015;&#23481;&#37327;&#65292;&#22686;&#24378;&#27169;&#24335;&#20043;&#38388;&#30340;&#20998;&#31163;&#24615;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#24207;&#21015;&#23481;&#37327;&#19982;&#32593;&#32476;&#22823;&#23567;&#30340;&#26032;&#30340;&#26631;&#24230;&#23450;&#24459;&#65292;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#20256;&#32479;Hopfield&#32593;&#32476;&#30340;&#29616;&#26377;&#26631;&#24230;&#23450;&#24459;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24191;&#20041;&#20266;&#36870;&#35268;&#21017;&#26469;&#22238;&#24518;&#39640;&#24230;&#36830;&#32493;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence memory is an essential attribute of natural and artificial intelligence that enables agents to encode, store, and retrieve complex sequences of stimuli and actions. Computational models of sequence memory have been proposed where recurrent Hopfield-like neural networks are trained with temporally asymmetric Hebbian rules. However, these networks suffer from limited sequence capacity (maximal length of the stored sequence) due to interference between the memories. Inspired by recent work on Dense Associative Memories, we expand the sequence capacity of these models by introducing a nonlinear interaction term, enhancing separation between the patterns. We derive novel scaling laws for sequence capacity with respect to network size, significantly outperforming existing scaling laws for models based on traditional Hopfield networks, and verify these theoretical results with numerical simulation. Moreover, we introduce a generalized pseudoinverse rule to recall sequences of highly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20855;&#26377;&#26410;&#30693;&#24178;&#39044;&#30340;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#26465;&#20214;&#30830;&#23450;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#22270;&#24182;&#20174;&#20013;&#37325;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#21442;&#25968;&#20551;&#35774;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#27979;&#37327;&#27169;&#22411;&#20013;&#28508;&#22312;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.02899</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#26410;&#30693;&#24178;&#39044;&#30340;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning nonparametric latent causal graphs with unknown interventions. (arXiv:2306.02899v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20855;&#26377;&#26410;&#30693;&#24178;&#39044;&#30340;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#26465;&#20214;&#30830;&#23450;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#22270;&#24182;&#20174;&#20013;&#37325;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#21442;&#25968;&#20551;&#35774;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#27979;&#37327;&#27169;&#22411;&#20013;&#28508;&#22312;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26410;&#30693;&#24178;&#39044;&#30340;&#28508;&#22312;&#31354;&#38388;&#24314;&#31435;&#26465;&#20214;&#65292;&#20197;&#30830;&#23450;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#22270;&#24182;&#20174;&#20013;&#37325;&#26500;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#27979;&#37327;&#27169;&#22411;&#20013;&#28508;&#22312;&#32467;&#26500;&#30340;&#35782;&#21035;&#65292;&#21363;&#22240;&#26524;&#22270;&#27169;&#22411;&#65292;&#22312;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#19982;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#30456;&#27604;&#65292;&#24182;&#19981;&#20570;&#20986;&#21442;&#25968;&#20551;&#35774;&#65292;&#22914;&#32447;&#24615;&#25110;&#39640;&#26031;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#38544;&#34255;&#21464;&#37327;&#30340;&#25968;&#37327;&#24050;&#30693;&#65292;&#24182;&#19988;&#25105;&#20204;&#34920;&#26126;&#27599;&#20010;&#38544;&#34255;&#21464;&#37327;&#26368;&#22810;&#21482;&#38656;&#35201;&#19968;&#20010;&#26410;&#30693;&#30340;&#24178;&#39044;&#12290;&#36825;&#25193;&#23637;&#20102;&#26368;&#36817;&#20851;&#20110;&#20174;&#35266;&#27979;&#21644;&#24178;&#39044;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#30340;&#24037;&#20316;&#12290;&#35777;&#26126;&#26159;&#24314;&#35774;&#24615;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#22270;&#24418;&#27010;&#24565;&#8212;&#8212;&#24819;&#35937;&#23376;&#38598;&#21644;&#23396;&#31435;&#36793;&#8212;&#8212;&#23427;&#20204;&#26412;&#36523;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;&#20316;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#36824;&#28041;&#21450;&#23545;&#36793;&#32536;&#23450;&#21521;&#38480;&#21046;&#30340;&#26032;&#30340;&#29305;&#24449;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish conditions under which latent causal graphs are nonparametrically identifiable and can be reconstructed from unknown interventions in the latent space. Our primary focus is the identification of the latent structure in a measurement model, i.e. causal graphical models where dependence between observed variables is insignificant compared to dependence between latent representations, without making parametric assumptions such as linearity or Gaussianity. Moreover, we do not assume the number of hidden variables is known, and we show that at most one unknown intervention per hidden variable is needed. This extends a recent line of work on learning causal representations from observations and interventions. The proofs are constructive and introduce two new graphical concepts -- imaginary subsets and isolated edges -- that may be useful in their own right. As a matter of independent interest, the proofs also involve a novel characterization of the limits of edge orientations wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;APA&#65292;&#20854;&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65288;PPO&#65289;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#30340;&#23849;&#28291;&#19982;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02231</link><description>&lt;p&gt;
&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#30340;Fine-Tuning&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;APA&#65292;&#20854;&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65288;PPO&#65289;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#30340;&#23849;&#28291;&#19982;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#21487;&#38752;&#26041;&#27861;&#12290;&#22312;&#20247;&#22810;RLHF&#25216;&#26415;&#20013;&#65292;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;PPO&#24456;&#27969;&#34892;&#65292;&#20294;&#23427;&#21487;&#33021;&#20250;&#36973;&#21463;&#27169;&#24335;&#23849;&#28291;&#12289;&#19981;&#31283;&#23450;&#21644;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;--&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#65288;APA&#65289;&#65292;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#26102;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#22987;&#32456;&#27604;PPO&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;PPO&#30456;&#27604;&#65292;APA&#21487;&#20197;&#26356;&#31283;&#23450;&#22320;&#25511;&#21046;&#27169;&#22411;&#19982;&#21021;&#22987;&#31574;&#30053;&#30340;&#20559;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#20250;&#23849;&#28291;&#20026;&#30830;&#23450;&#24615;&#36755;&#20986;&#12290;&#38500;&#20102;&#32463;&#39564;&#32467;&#26524;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;APA&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#25216;&#26415;&#65292;&#24182;&#25512;&#24191;&#20102;&#24191;&#20041;&#24179;&#28369;&#24230;&#26465;&#20214;&#65292;&#20351;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#33719;&#24471;&#26356;&#24378;&#30340;&#32467;&#26524;&#12290;&#22312;&#35813;&#26465;&#20214;&#19979;&#65292;&#33719;&#24471;&#20102;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#21644;Nesterov&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#32463;&#20856;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01264</link><description>&lt;p&gt;
&#24191;&#20041;&#24179;&#28369;&#24230;&#19979;&#30340;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Convex and Non-Convex Optimization under Generalized Smoothness. (arXiv:2306.01264v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#25216;&#26415;&#65292;&#24182;&#25512;&#24191;&#20102;&#24191;&#20041;&#24179;&#28369;&#24230;&#26465;&#20214;&#65292;&#20351;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#33719;&#24471;&#26356;&#24378;&#30340;&#32467;&#26524;&#12290;&#22312;&#35813;&#26465;&#20214;&#19979;&#65292;&#33719;&#24471;&#20102;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#21644;Nesterov&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#32463;&#20856;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#30340;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#30340;&#20998;&#26512;&#36890;&#24120;&#38656;&#35201;&#26799;&#24230;&#30340;Lipshitz&#24615;&#36136;&#65292;&#36825;&#38480;&#21046;&#20102;&#20998;&#26512;&#33539;&#22260;&#20165;&#38480;&#20110;&#20108;&#27425;&#20989;&#25968;&#30340;&#30028;&#38480;&#20869;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25918;&#26494;&#20102;&#36825;&#20010;&#35201;&#27714;&#65292;&#36716;&#32780;&#20351;&#29992;&#19968;&#31181;&#38750;&#22343;&#21248;&#24179;&#28369;&#26465;&#20214;&#65292;&#20854;&#20013;Hessian&#33539;&#25968;&#21463;&#26799;&#24230;&#33539;&#25968;&#30340;&#20223;&#23556;&#20989;&#25968;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#35009;&#21098;&#35777;&#26126;&#20102;&#38750;&#20984;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#20551;&#35774;&#23384;&#22312;&#26377;&#30028;&#22122;&#22768;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#36825;&#31181;&#38750;&#22343;&#21248;&#24179;&#28369;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;&#20998;&#26512;&#25216;&#26415;&#65292;&#21487;&#20197;&#27839;&#36712;&#36857;&#26041;&#21521;&#38480;&#21046;&#26799;&#24230;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#24378;&#30340;&#20984;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#36825;&#20010;&#24191;&#20041;&#24179;&#28369;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#21644;Nesterov&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#32463;&#20856;&#25910;&#25947;&#29575;&#65292;&#36866;&#29992;&#20110;&#20984;&#21644;&#65288;&#25110;&#65289;&#38750;&#20984;&#35774;&#23450;&#12290;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#19981;&#38656;&#35201;&#26799;&#24230;&#35009;&#21098;&#65292;&#24182;&#20801;&#35768;&#26377;&#37325;&#23614;&#22122;&#22768;&#65292;&#36825;&#26159;&#19968;&#31181;&#38750;&#24120;&#23454;&#29992;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical analysis of convex and non-convex optimization methods often requires the Lipshitzness of the gradient, which limits the analysis to functions bounded by quadratics. Recent work relaxed this requirement to a non-uniform smoothness condition with the Hessian norm bounded by an affine function of the gradient norm, and proved convergence in the non-convex setting via gradient clipping, assuming bounded noise. In this paper, we further generalize this non-uniform smoothness condition and develop a simple, yet powerful analysis technique that bounds the gradients along the trajectory, thereby leading to stronger results for both convex and non-convex optimization problems. In particular, we obtain the classical convergence rates for (stochastic) gradient descent and Nesterov's accelerated gradient method in the convex and/or non-convex setting under this general smoothness condition. The new analysis approach does not require gradient clipping and allows heavy-tailed noise with b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#21644;&#23436;&#20840;&#20934;&#30830;&#26102;&#20998;&#21035;&#37319;&#21462;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;ImageNet&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#24635;&#32467;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.00265</link><description>&lt;p&gt;
&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#21644;&#23436;&#20840;&#20934;&#30830;&#26102;&#20998;&#21035;&#37319;&#21462;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;ImageNet&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#24635;&#32467;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35757;&#32451;&#26159;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#24182;&#23558;&#20854;&#19982;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#32467;&#21512;&#20351;&#29992;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#33258;&#25105;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#36825;&#20123;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#24179;&#34913;&#12290;&#24403;&#20266;&#26631;&#31614;&#23436;&#20840;&#19981;&#27491;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#34987;&#20943;&#23569;&#21040;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#24403;&#20266;&#26631;&#31614;&#23436;&#20840;&#20934;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21464;&#25104;&#21033;&#29992;&#25152;&#26377;&#20266;&#26631;&#31614;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#21152;&#26377;&#25928;&#30340;&#26679;&#26412;&#37327;&#12290;&#36890;&#36807;&#22312;ImageNet&#22270;&#20687;&#20998;&#31867;&#21644;nuScenes&#33258;&#20027;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#37325;&#31283;&#20581;&#25439;&#22833;&#20248;&#20110;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#22522;&#32447;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18381</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#30719;&#30707;&#20013;&#25552;&#28860;&#40644;&#37329;: &#22522;&#20110;&#20851;&#38190;&#26679;&#26412;&#36873;&#25321;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18381
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25928;&#29575;&#23398;&#20064;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25317;&#26377;&#22823;&#37327;&#22810;&#27169;&#22411;&#30340;&#29616;&#22312;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#25104;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#36807;&#31243;&#26412;&#36523;&#20173;&#28982;&#38750;&#24120;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#29992;&#20449;&#24687;&#29702;&#35770;&#26469;&#24314;&#27169;&#33976;&#39311;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#25454;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20197;&#20415;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26679;&#26412;&#36873;&#25321;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#20248;&#21270;&#36807;&#31243;&#12290;&#36825;&#31181;&#26032;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#25193;&#22823;&#29616;&#26377;&#31639;&#27861;&#33539;&#22260;&#20197;&#23545;&#26356;&#24222;&#22823;&#21644;&#22810;&#20803;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#20363;&#22914;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;0.04&#65285;&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#20445;&#25345;&#21487;&#27604;&#30340;&#33976;&#39311;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20854;&#36129;&#29486;&#21487;&#33021;&#20026;&#33976;&#39311;&#36807;&#31243;&#30340;&#21160;&#21147;&#23398;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36724;&#21521;&#20998;&#35299;&#26680;&#31215;&#20998;&#30340;Factorized Transformer&#27169;&#22411;&#65292;&#29992;&#20110;PDE&#27169;&#22411;&#20195;&#29992;&#12290;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#22823;&#37327;&#32593;&#26684;&#28857;&#30340;&#38382;&#39064;&#19978;&#33021;&#22815;&#20445;&#25345;&#36739;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.17560</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;PDE&#27169;&#22411;&#20195;&#29992;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Scalable Transformer for PDE Surrogate Modeling. (arXiv:2305.17560v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17560
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36724;&#21521;&#20998;&#35299;&#26680;&#31215;&#20998;&#30340;Factorized Transformer&#27169;&#22411;&#65292;&#29992;&#20110;PDE&#27169;&#22411;&#20195;&#29992;&#12290;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#22823;&#37327;&#32593;&#26684;&#28857;&#30340;&#38382;&#39064;&#19978;&#33021;&#22815;&#20445;&#25345;&#36739;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26368;&#36817;&#25104;&#20026;&#20102;PDE&#27169;&#22411;&#20195;&#29702;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;Attention&#65292;&#20294;&#26159;&#23558;Transformer&#24212;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#32593;&#26684;&#28857;&#30340;&#38382;&#39064;&#21487;&#33021;&#20250;&#22312;&#25968;&#20540;&#19978;&#19981;&#31283;&#23450;&#19988;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Factorized Transformer&#65288;FactFormer&#65289;&#65292;&#23427;&#22522;&#20110;&#19968;&#20010;&#36724;&#21521;&#20998;&#35299;&#26680;&#31215;&#20998;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#25237;&#24433;&#31639;&#23376;&#65292;&#23558;&#36755;&#20837;&#20989;&#25968;&#20998;&#35299;&#20026;&#20855;&#26377;&#19968;&#32500;&#22495;&#30340;&#22810;&#20010;&#23376;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#23376;&#20989;&#25968;&#34987;&#35780;&#20272;&#24182;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#36724;&#21521;&#20998;&#35299;&#26041;&#26696;&#30340;&#22522;&#20110;&#23454;&#20363;&#30340;&#26680;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;$256\times 256$&#30340;&#32593;&#26684;&#19978;&#27169;&#25311;2D Kolmogorov&#27969;&#21644;$64\times64\times64$&#32593;&#26684;&#19978;&#30340;3D&#28895;&#38654;&#28014;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#35299;&#26041;&#26696;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#30340;&#20248;&#21270;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer has shown state-of-the-art performance on various applications and has recently emerged as a promising tool for surrogate modeling of partial differential equations (PDEs). Despite the introduction of linear-complexity attention, applying Transformer to problems with a large number of grid points can be numerically unstable and computationally expensive. In this work, we propose Factorized Transformer (FactFormer), which is based on an axial factorized kernel integral. Concretely, we introduce a learnable projection operator that decomposes the input function into multiple sub-functions with one-dimensional domain. These sub-functions are then evaluated and used to compute the instance-based kernel with an axial factorized scheme. We showcase that the proposed model is able to simulate 2D Kolmogorov flow on a $256\times 256$ grid and 3D smoke buoyancy on a $64\times64\times64$ grid with good accuracy and efficiency. The proposed factorized scheme can serve as a computationa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;&#65292;&#20026; 16 &#31181;&#24773;&#20917;&#20013;&#30340; 9 &#31181;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#21462;&#24471;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38543;&#26426; DR-submodular &#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.16671</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach for Maximizing Continuous DR-submodular Functions. (arXiv:2305.16671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;&#65292;&#20026; 16 &#31181;&#24773;&#20917;&#20013;&#30340; 9 &#31181;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#21462;&#24471;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38543;&#26426; DR-submodular &#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493;&#30340; DR-submodular &#20989;&#25968;&#65292;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#38024;&#23545;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#20989;&#25968;&#30340; Frank-Wolfe &#31867;&#22411;&#31163;&#32447;&#31639;&#27861;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#19968;&#33324;&#20984;&#38598;&#38480;&#21046;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102; Oracle &#25552;&#20379;&#20989;&#25968;&#26799;&#24230;&#25110;&#20165;&#20989;&#25968;&#20540;&#30340;&#35775;&#38382;&#20197;&#21450;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#24615;&#35775;&#38382;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#30830;&#23450;&#20102;&#25152;&#38656;&#30340; Oracle &#35775;&#38382;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026; 16 &#20010;&#32771;&#34385;&#30340;&#24773;&#20917;&#20013;&#30340; 9 &#20010;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#20004;&#20010;&#24773;&#20917;&#19979;&#36991;&#20813;&#20102;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#25237;&#24433;&#65292;&#32780;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20854;&#20313;&#20116;&#20010;&#24773;&#20917;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#30340;&#26041;&#27861;&#65292;&#20026;&#38543;&#26426; DR-submodular &#20989;&#25968;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#25506;&#38505;&#21453;&#39304;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a unified approach for maximizing continuous DR-submodular functions that encompasses a range of settings and oracle access types. Our approach includes a Frank-Wolfe type offline algorithm for both monotone and non-monotone functions, with different restrictions on the general convex set. We consider settings where the oracle provides access to either the gradient of the function or only the function value, and where the oracle access is either deterministic or stochastic. We determine the number of required oracle accesses in all cases. Our approach gives new/improved results for nine out of the sixteen considered cases, avoids computationally expensive projections in two cases, with the proposed framework matching performance of state-of-the-art approaches in the remaining five cases. Notably, our approach for the stochastic function value-based oracle enables the first regret bounds with bandit feedback for stochastic DR-submodular functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35270;&#35273;-&#20195;&#30721;Transformer&#26041;&#27861;&#65292;&#36890;&#36807;actor-critic&#24494;&#35843;&#26469;&#25913;&#21892;&#22522;&#32447;&#65292;&#27604;&#36739;&#20102;Vision Transformer&#21644;Document Image Transformer&#36825;&#20004;&#31181;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#23631;&#24149;&#25130;&#22270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#21019;&#24314;&#20102;30,000&#20010;&#29420;&#29305;&#30340;&#20195;&#30721;&#21644;&#23545;&#24212;&#25130;&#22270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14637</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#35270;&#35273;-&#20195;&#30721;Transformer&#29992;&#20110;UI-to-Code&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning finetuned Vision-Code Transformer for UI-to-Code Generation. (arXiv:2305.14637v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35270;&#35273;-&#20195;&#30721;Transformer&#26041;&#27861;&#65292;&#36890;&#36807;actor-critic&#24494;&#35843;&#26469;&#25913;&#21892;&#22522;&#32447;&#65292;&#27604;&#36739;&#20102;Vision Transformer&#21644;Document Image Transformer&#36825;&#20004;&#31181;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#23631;&#24149;&#25130;&#22270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#21019;&#24314;&#20102;30,000&#20010;&#29420;&#29305;&#30340;&#20195;&#30721;&#21644;&#23545;&#24212;&#25130;&#22270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23631;&#24149;&#25130;&#22270;&#33258;&#21160;&#29983;&#25104;HTML/CSS&#20195;&#30721;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;-&#20195;&#30721;Transformer&#26041;&#27861;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#21516;&#26102;&#25506;&#32034;actor-critic&#24494;&#35843;&#20316;&#20026;&#25913;&#36827;&#22522;&#32447;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#27604;&#36739;&#20102;&#20004;&#20010;&#22270;&#20687;&#32534;&#30721;&#22120;&#65306;Vision Transformer (ViT) &#21644; Document Image Transformer (DiT)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#23631;&#24149;&#25130;&#22270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#31616;&#21270;&#20102;&#24320;&#21457;&#20154;&#21592;&#30340;&#32593;&#31449;&#21019;&#24314;&#36807;&#31243;&#12290;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;30,000&#20010;&#29420;&#29305;&#30340;&#20195;&#30721;&#21644;&#23545;&#24212;&#25130;&#22270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;MSE&#12289;BLEU&#12289;IoU&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;htmlBLEU&#24471;&#20998;&#31561;&#33258;&#21160;&#21270;&#25351;&#26631;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#29992;DiT-GPT2&#27169;&#22411;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated HTML/CSS code generation from screenshots is an important yet challenging problem with broad applications in website development and design. In this paper, we present a novel vision-code transformer approach that leverages an Encoder-Decoder architecture as well as explore actor-critic fine-tuning as a method for improving upon the baseline. For this purpose, two image encoders are compared: Vision Transformer (ViT) and Document Image Transformer (DiT).  We propose an end-to-end pipeline that can generate high-quality code snippets directly from screenshots, streamlining the website creation process for developers. To train and evaluate our models, we created a synthetic dataset of 30,000 unique pairs of code and corresponding screenshots.  We evaluate the performance of our approach using a combination of automated metrics such as MSE, BLEU, IoU, and a novel htmlBLEU score, where our models demonstrated strong performance. We establish a strong baseline with the DiT-GPT2 mod
&lt;/p&gt;</description></item><item><title>Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13484</link><description>&lt;p&gt;
Flover&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13484
&lt;/p&gt;
&lt;p&gt;
Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#27169;&#22411;&#25512;&#26029;&#24615;&#33021;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23588;&#20854;&#26159;&#22312;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#24182;&#34987;&#37096;&#32626;&#22312;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24773;&#20917;&#19979;&#12290;&#33258;&#22238;&#24402;&#27169;&#22411;&#30001;&#20110;&#22312;&#20247;&#22810;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22240;&#27492;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#19978;&#37319;&#29992;&#20102;&#19968;&#31181;&#26102;&#38388;&#20381;&#36182;&#32467;&#26500;&#65292;&#20854;&#20013;&#24403;&#21069;token&#30340;&#27010;&#29575;&#20998;&#24067;&#21463;&#21040;&#21069;&#38754;token&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26412;&#36136;&#19978;&#30340;&#24207;&#21015;&#29305;&#24615;&#36981;&#24490;&#39532;&#23572;&#21487;&#22827;&#38142;&#20551;&#35774;&#65292;&#32570;&#20047;&#26102;&#38388;&#24182;&#34892;&#24615;&#65292;&#22240;&#27492;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#32972;&#26223;&#19979;&#65292;&#25512;&#26029;&#35831;&#27714;&#36981;&#24490;&#27850;&#26494;&#26102;&#38388;&#20998;&#24067;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#21709;&#24212;&#38271;&#24230;&#65292;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#32570;&#22833;&#26356;&#21152;&#26126;&#26174;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22914;&#21160;&#24577;&#25209;&#22788;&#29702;&#21644;&#24182;&#21457;&#27169;&#22411;&#23454;&#20363;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#31895;&#31890;&#24230;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#30340;&#24320;&#38144;&#21644;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#22495;&#36866;&#24212;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;UniDA&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#22522;&#20934;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#30446;&#26631;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11092</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Universal Domain Adaptation from Foundation Models. (arXiv:2305.11092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#22495;&#36866;&#24212;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;UniDA&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#22522;&#20934;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#30446;&#26631;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#25110;DINOv2&#65289;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#24191;&#27867;&#35270;&#35273;&#20219;&#21153;&#20013;&#21331;&#36234;&#30340;&#23398;&#20064;&#21644;&#36716;&#31227;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#25968;&#25454;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#24182;&#36866;&#24212;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#22522;&#30784;&#27169;&#22411;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#36890;&#29992;&#22495;&#36866;&#24212;&#65288;UniDA&#65289;&#65292;&#21363;&#20351;&#29992;&#28304;&#22495;&#26631;&#35760;&#25968;&#25454;&#21644;&#30446;&#26631;&#22495;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#36866;&#24212;&#30446;&#26631;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#23545;&#29616;&#26377;&#29366;&#24577;&#19979;&#30340;UniDA&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#65292;&#23613;&#31649;&#22522;&#30784;&#27169;&#22411;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20165;&#22312;&#28304;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#30340;UniDA&#26041;&#27861;&#36890;&#24120;&#19981;&#33021;&#36229;&#36234;&#22522;&#20934;&#12290;&#36825;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;UniDA&#38656;&#35201;&#26032;&#30340;&#30740;&#31350;&#21162;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#30340;&#30446;&#26631;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transferring capabilities on a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first demonstrate that, while foundation models greatly improve the performance of the baseline methods that train the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. To this end, we propose a very simple method of target 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24182;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#30340;&#31169;&#26377;&#25345;&#20037;&#22270;&#65292;&#25552;&#20986;&#20351;&#29992; $L^1$-&#36317;&#31163;&#35745;&#31639;&#25345;&#20037;&#22270;&#24182;&#37319;&#29992;&#25351;&#25968;&#26426;&#21046;&#20445;&#25252;&#38544;&#31169;&#65292;&#25104;&#21151;&#23454;&#29616;&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#20998;&#26512;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.03609</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#22312;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Topological Data Analysis. (arXiv:2305.03609v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24182;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#30340;&#31169;&#26377;&#25345;&#20037;&#22270;&#65292;&#25552;&#20986;&#20351;&#29992; $L^1$-&#36317;&#31163;&#35745;&#31639;&#25345;&#20037;&#22270;&#24182;&#37319;&#29992;&#25351;&#25968;&#26426;&#21046;&#20445;&#25252;&#38544;&#31169;&#65292;&#25104;&#21151;&#23454;&#29616;&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#20998;&#26512;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#39318;&#31687;&#23581;&#35797;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24182;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#30340;&#31169;&#26377;&#25345;&#20037;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#29942;&#39048;&#36317;&#31163;&#20998;&#26512;&#25345;&#20037;&#22270;&#30340;&#28789;&#25935;&#24230;&#65292;&#21457;&#29616;&#24120;&#29992;&#30340; \v{C}ech &#22797;&#24418;&#30340;&#28789;&#25935;&#24230;&#24182;&#19981;&#20250;&#38543;&#30528;&#26679;&#26412;&#37327; $n$ &#30340;&#22686;&#21152;&#32780;&#38477;&#20302;&#65292;&#36825;&#20351;&#24471; \v{C}ech &#22797;&#24418;&#25345;&#20037;&#22270;&#38590;&#20197;&#38544;&#31169;&#21270;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992; $L^1$-&#36317;&#31163;&#26469;&#35745;&#31639;&#25345;&#20037;&#22270;&#65292;&#21457;&#29616;&#20854;&#28789;&#25935;&#24230;&#20026; $O(1/n)$&#12290;&#22522;&#20110;&#28789;&#25935;&#24230;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#25351;&#25968;&#26426;&#21046;&#65292;&#20854;&#25928;&#29992;&#20989;&#25968;&#23450;&#20041;&#20026; $L^1$-DTM &#25345;&#20037;&#22270;&#30340;&#29942;&#39048;&#36317;&#31163;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#25105;&#20204;&#38544;&#31169;&#26426;&#21046;&#30340;&#31934;&#24230;&#19978;&#19979;&#30028;&#65307;&#24471;&#21040;&#30340;&#30028;&#38480;&#34920;&#26126;&#25105;&#20204;&#30340;&#26426;&#21046;&#38544;&#31169;&#35823;&#24046;&#25509;&#36817;&#26368;&#20248;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31169;&#26377;&#25345;&#20037;&#22270;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is the first to attempt differentially private (DP) topological data analysis (TDA), producing near-optimal private persistence diagrams. We analyze the sensitivity of persistence diagrams in terms of the bottleneck distance, and we show that the commonly used \v{C}ech complex has sensitivity that does not decrease as the sample size $n$ increases. This makes it challenging for the persistence diagrams of \v{C}ech complexes to be privatized. As an alternative, we show that the persistence diagram obtained by the $L^1$-distance to measure (DTM) has sensitivity $O(1/n)$. Based on the sensitivity analysis, we propose using the exponential mechanism whose utility function is defined in terms of the bottleneck distance of the $L^1$-DTM persistence diagrams. We also derive upper and lower bounds of the accuracy of our privacy mechanism; the obtained bounds indicate that the privacy error of our mechanism is near-optimal. We demonstrate the performance of our privatized persistence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03515</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#24230;&#30340;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#26641;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#29983;&#38271;&#31639;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#19978;&#23616;&#37096;&#26368;&#23567;&#21270;&#19981;&#32431;&#24230;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#36138;&#24515;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#36724;&#23545;&#40784;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#30452;&#36890;&#31639;&#23376;&#22312;&#23494;&#38598;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#19978;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.03843</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35201;&#36880;&#27493;&#24605;&#32771;&#65311;&#25512;&#29702;&#28304;&#20110;&#32463;&#39564;&#30340;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#30528;&#24378;&#22823;&#32780;&#31070;&#31192;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#32431;&#31929;&#30340;&#24605;&#32500;&#27493;&#39588;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#29702;&#20986;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#24471;&#20986;&#30340;&#25512;&#35770; - &#23613;&#31649;&#25105;&#20204;&#20174;&#19990;&#30028;&#19978;&#27809;&#26377;&#24471;&#21040;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#12290;&#21516;&#26679;&#22320;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#29983;&#25104;&#20013;&#38388;&#27493;&#39588;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36825;&#20123;&#35757;&#32451;&#26465;&#20214;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#23450;&#20041;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#21697;&#23545;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#27599;&#20010;&#26679;&#21697;&#21482;&#21253;&#25324;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#21464;&#37327;&#12290;&#25105;&#20204;&#27604;&#36739;&#20351;&#29992;&#25512;&#29702;&#29983;&#25104;&#30340;&#21464;&#37327;&#23376;&#38598;&#19982;&#20351;&#29992;&#23436;&#25972;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#36827;&#21046;&#30456;&#20284;&#24615;&#27169;&#22411;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#38887;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#26377;&#38024;&#23545;&#24615;&#21644;&#26080;&#38024;&#23545;&#24615;&#25915;&#20987;&#65292;&#24182;&#21521;&#40657;&#30418;&#21644;&#30333;&#30418;&#25915;&#20987;&#32773;&#23637;&#31034;&#20102;&#36825;&#31181;&#26131;&#21463;&#25915;&#20987;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2303.11143</link><description>&lt;p&gt;
&#23545;&#25239;&#20108;&#36827;&#21046;&#30456;&#20284;&#24615;&#31995;&#32479;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks against Binary Similarity Systems. (arXiv:2303.11143v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#36827;&#21046;&#30456;&#20284;&#24615;&#27169;&#22411;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#38887;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#26377;&#38024;&#23545;&#24615;&#21644;&#26080;&#38024;&#23545;&#24615;&#25915;&#20987;&#65292;&#24182;&#21521;&#40657;&#30418;&#21644;&#30333;&#30418;&#25915;&#20987;&#32773;&#23637;&#31034;&#20102;&#36825;&#31181;&#26131;&#21463;&#25915;&#20987;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#20108;&#36827;&#21046;&#20998;&#26512;&#20316;&#20026;&#19968;&#31181;&#26816;&#26597;&#36719;&#20214;&#24182;&#30830;&#20445;&#20854;&#23433;&#20840;&#24615;&#30340;&#22522;&#26412;&#26041;&#27861;&#24471;&#21040;&#20102;&#20851;&#27880;&#12290;&#30001;&#20110;&#36816;&#34892;&#36719;&#20214;&#30340;&#35774;&#22791;&#25968;&#37327;&#25351;&#25968;&#22686;&#38271;&#65292;&#35768;&#22810;&#30740;&#31350;&#29616;&#22312;&#27491;&#36716;&#21521;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#22411;&#33258;&#20027;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35299;&#20915;&#20108;&#36827;&#21046;&#20998;&#26512;&#38382;&#39064;&#26102;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#20108;&#36827;&#21046;&#30456;&#20284;&#24615;&#26159;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#23427;&#28041;&#21450;&#30830;&#23450;&#27719;&#32534;&#20195;&#30721;&#20013;&#30340;&#20004;&#20010;&#20989;&#25968;&#26159;&#21542;&#26469;&#33258;&#30456;&#21516;&#30340;&#28304;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#23545;&#20110;&#20108;&#36827;&#21046;&#30456;&#20284;&#24615;&#30340;&#34892;&#20026;&#22914;&#20309;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#36827;&#21046;&#30456;&#20284;&#24615;&#27169;&#22411;&#23545;&#25239;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#38887;&#24615;&#65292;&#26174;&#31034;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#40657;&#30418;&#21644;&#30333;&#30418;&#25915;&#20987;&#32773;&#23545;&#20110;&#30456;&#20284;&#24615;&#30446;&#26631;&#30340;&#26377;&#38024;&#23545;&#24615;&#21644;&#26080;&#38024;&#23545;&#24615;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35814;&#32454;&#27979;&#35797;&#20102;&#19977;&#31181;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20108;&#36827;&#21046;&#30456;&#20284;&#24615;&#35299;&#20915;&#26041;&#26696;&#23545;&#20004;&#31181;&#40657;&#30418;
&lt;/p&gt;
&lt;p&gt;
In recent years, binary analysis gained traction as a fundamental approach to inspect software and guarantee its security. Due to the exponential increase of devices running software, much research is now moving towards new autonomous solutions based on deep learning models, as they have been showing state-of-the-art performances in solving binary analysis problems. One of the hot topics in this context is binary similarity, which consists in determining if two functions in assembly code are compiled from the same source code. However, it is unclear how deep learning models for binary similarity behave in an adversarial context. In this paper, we study the resilience of binary similarity models against adversarial examples, showing that they are susceptible to both targeted and untargeted attacks (w.r.t. similarity goals) performed by black-box and white-box attackers. In more detail, we extensively test three current state-of-the-art solutions for binary similarity against two black-b
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#30340;&#27133;&#25193;&#25955;&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Latent Slot Diffusion (LSD)&#27169;&#22411;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#27133;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#20063;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26080;&#30417;&#30563;&#36827;&#34892;&#32452;&#21512;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.10834</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#30340;&#27133;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Slot Diffusion. (arXiv:2303.10834v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10834
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#30340;&#27133;&#25193;&#25955;&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Latent Slot Diffusion (LSD)&#27169;&#22411;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#27133;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#20063;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26080;&#30417;&#30563;&#36827;&#34892;&#32452;&#21512;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;Transformer&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#30340;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36825;&#31361;&#26174;&#20102;&#24378;&#22823;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#20013;&#30340;&#25972;&#21512;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20173;&#28982;&#36739;&#23569;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Latent Slot Diffusion (LSD)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#20004;&#20010;&#30446;&#26631;&#65306;&#39318;&#20808;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23558;&#20256;&#32479;&#30340;&#27133;&#35299;&#30721;&#22120;&#26367;&#25442;&#20026;&#20197;&#23545;&#35937;&#27133;&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#27169;&#22411;&#65307;&#20854;&#27425;&#65292;&#23427;&#20063;&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#20687;&#25991;&#26412;&#36825;&#26679;&#30340;&#30417;&#30563;&#27880;&#37322;&#32780;&#33021;&#22815;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#32452;&#21512;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#23545;&#35937;&#20013;&#24515;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#39318;&#27425;&#22312;FFHQ&#25968;&#25454;&#38598;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#20351;&#29992;&#21333;&#20010;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#24494;&#35843;&#30340;&#27169;&#22411;&#38598;&#21512;&#65292;&#21457;&#29616;&#36890;&#36807;&#26356;&#22909;&#22320;&#25506;&#32034;&#39044;&#35757;&#32451;&#22522;&#22495;&#21487;&#20197;&#25913;&#36827;&#38598;&#25104;&#27169;&#22411;&#65292;&#20294;&#31163;&#24320;&#22522;&#22495;&#20250;&#23548;&#33268;&#22833;&#21435;&#36801;&#31227;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#24182;&#19988;&#38477;&#20302;&#38598;&#25104;&#36136;&#37327;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#20462;&#25913;&#26041;&#27861;StarSSE&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#24378;&#30340;&#38598;&#25104;&#27169;&#22411;&#21644;&#22343;&#21248;&#30340;&#27169;&#22411;&#28151;&#21512;&#12290;</title><link>http://arxiv.org/abs/2303.03374</link><description>&lt;p&gt;
&#20572;&#30041;&#36824;&#26159;&#31163;&#24320;&#39044;&#35757;&#32451;&#22522;&#22495;&#65306;&#20851;&#20110;&#38598;&#25104;&#23398;&#20064;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning. (arXiv:2303.03374v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#20351;&#29992;&#21333;&#20010;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#24494;&#35843;&#30340;&#27169;&#22411;&#38598;&#21512;&#65292;&#21457;&#29616;&#36890;&#36807;&#26356;&#22909;&#22320;&#25506;&#32034;&#39044;&#35757;&#32451;&#22522;&#22495;&#21487;&#20197;&#25913;&#36827;&#38598;&#25104;&#27169;&#22411;&#65292;&#20294;&#31163;&#24320;&#22522;&#22495;&#20250;&#23548;&#33268;&#22833;&#21435;&#36801;&#31227;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#24182;&#19988;&#38477;&#20302;&#38598;&#25104;&#36136;&#37327;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#20462;&#25913;&#26041;&#27861;StarSSE&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#24378;&#30340;&#38598;&#25104;&#27169;&#22411;&#21644;&#22343;&#21248;&#30340;&#27169;&#22411;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#21644;&#38598;&#25104;&#23398;&#20064;&#26159;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#30340;&#20004;&#31181;&#28909;&#38376;&#25216;&#26415;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65292;&#36890;&#24120;&#23454;&#36341;&#20013;&#20351;&#29992;&#20174;&#21333;&#20010;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#24494;&#35843;&#30340;&#27169;&#22411;&#38598;&#21512;&#12290;&#36825;&#20123;&#27169;&#22411;&#26368;&#32456;&#20250;&#36827;&#20837;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#19979;&#38477;&#31354;&#38388;&#30340;&#30456;&#21516;&#21306;&#22495;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39044;&#35757;&#32451;&#22522;&#22495;&#65292;&#22240;&#27492;&#20855;&#26377;&#26377;&#38480;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#21333;&#20010;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#35757;&#32451;&#30340;&#38598;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26356;&#22909;&#22320;&#25506;&#32034;&#39044;&#35757;&#32451;&#22522;&#22495;&#26469;&#25913;&#36827;&#65292;&#28982;&#32780;&#65292;&#31163;&#24320;&#22522;&#22495;&#20250;&#23548;&#33268;&#22833;&#21435;&#36801;&#31227;&#23398;&#20064;&#30340;&#22909;&#22788;&#24182;&#23548;&#33268;&#38598;&#25104;&#36136;&#37327;&#30340;&#19979;&#38477;&#12290;&#22522;&#20110;&#23545;&#29616;&#26377;&#25506;&#32034;&#26041;&#27861;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#20462;&#25913;Transfer Learning Setup&#20013;&#30340;Snapshot Ensembles&#65288;SSE&#65289;&#26041;&#27861;&#65292;&#21517;&#20026;StarSSE&#65292;&#23427;&#33021;&#20135;&#29983;&#26356;&#24378;&#30340;&#38598;&#25104;&#27169;&#22411;&#21644;&#22343;&#21248;&#30340;&#27169;&#22411;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning and ensembling are two popular techniques for improving the performance and robustness of neural networks. Due to the high cost of pre-training, ensembles of models fine-tuned from a single pre-trained checkpoint are often used in practice. Such models end up in the same basin of the loss landscape, which we call the pre-train basin, and thus have limited diversity. In this work, we show that ensembles trained from a single pre-trained checkpoint may be improved by better exploring the pre-train basin, however, leaving the basin results in losing the benefits of transfer learning and in degradation of the ensemble quality. Based on the analysis of existing exploration methods, we propose a more effective modification of the Snapshot Ensembles (SSE) for transfer learning setup, StarSSE, which results in stronger ensembles and uniform model soups.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24402;&#19968;&#21270;&#27969;&#22312;&#26684;&#28857;&#22330;&#35770;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#24402;&#19968;&#21270;&#27969;&#32463;&#24120;&#36973;&#21463;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#23558;&#30456;&#20851;&#27169;&#24335;&#30340;&#27010;&#29575;&#36136;&#37327;&#36171;&#20104;&#26497;&#20302;&#30340;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#27169;&#24335;&#23849;&#28291;&#31243;&#24230;&#30340;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#20559;&#24046;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2302.14082</link><description>&lt;p&gt;
&#26816;&#27979;&#21644;&#20943;&#36731;&#26684;&#28857;&#22330;&#35770;&#30340;&#27969;&#37319;&#26679;&#20013;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Detecting and Mitigating Mode-Collapse for Flow-based Sampling of Lattice Field Theories. (arXiv:2302.14082v2 [hep-lat] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24402;&#19968;&#21270;&#27969;&#22312;&#26684;&#28857;&#22330;&#35770;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#24402;&#19968;&#21270;&#27969;&#32463;&#24120;&#36973;&#21463;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#23558;&#30456;&#20851;&#27169;&#24335;&#30340;&#27010;&#29575;&#36136;&#37327;&#36171;&#20104;&#26497;&#20302;&#30340;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#27169;&#24335;&#23849;&#28291;&#31243;&#24230;&#30340;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#36731;&#20559;&#24046;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26684;&#28857;&#22330;&#35770;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24402;&#19968;&#21270;&#27969;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#30340;&#21518;&#26524;&#12290;&#24402;&#19968;&#21270;&#27969;&#21487;&#20197;&#36827;&#34892;&#29420;&#31435;&#37319;&#26679;&#65292;&#22240;&#27492;&#24076;&#26395;&#23427;&#20204;&#21487;&#20197;&#36991;&#20813;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#23616;&#37096;&#26356;&#26032;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#30340;&#38567;&#36947;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#65292;&#38567;&#36947;&#38382;&#39064;&#22312;&#24402;&#19968;&#21270;&#27969;&#20013;&#20063;&#23384;&#22312;&#65292;&#20294;&#24050;&#32463;&#20174;&#37319;&#26679;&#38454;&#27573;&#36716;&#31227;&#21040;&#20102;&#31639;&#27861;&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24402;&#19968;&#21270;&#27969;&#32463;&#24120;&#36973;&#21463;&#27169;&#24335;&#23849;&#28291;&#65292;&#35757;&#32451;&#36807;&#31243;&#23558;&#29289;&#29702;&#20998;&#24067;&#30340;&#30456;&#20851;&#27169;&#24335;&#36171;&#20104;&#26497;&#20302;&#30340;&#27010;&#29575;&#36136;&#37327;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#27969;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#38142;&#37319;&#26679;&#22120;&#25110;&#37325;&#35201;&#37319;&#26679;&#26102;&#20135;&#29983;&#26174;&#33879;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#27169;&#24335;&#23849;&#28291;&#31243;&#24230;&#30340;&#25351;&#26631;&#65292;&#24182;&#25512;&#23548;&#20102;&#30001;&#27492;&#23548;&#33268;&#30340;&#20559;&#24046;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20272;&#35745;&#28909;&#21147;&#23398;&#35266;&#27979;&#37327;&#65288;&#22914;&#28909;&#21147;&#23398;&#21183;&#65289;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#22810;&#31181;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the consequences of mode-collapse of normalizing flows in the context of lattice field theory. Normalizing flows allow for independent sampling. For this reason, it is hoped that they can avoid the tunneling problem of local-update MCMC algorithms for multi-modal distributions. In this work, we first point out that the tunneling problem is also present for normalizing flows but is shifted from the sampling to the training phase of the algorithm. Specifically, normalizing flows often suffer from mode-collapse for which the training process assigns vanishingly low probability mass to relevant modes of the physical distribution. This may result in a significant bias when the flow is used as a sampler in a Markov-Chain or with Importance Sampling. We propose a metric to quantify the degree of mode-collapse and derive a bound on the resulting bias. Furthermore, we propose various mitigation strategies in particular in the context of estimating thermodynamic observables, such as the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;TAILOR&#65292;&#23427;&#22312;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#24212;&#29992;&#30340;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20505;&#36873;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.07317</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithm Selection for Deep Active Learning with Imbalanced Datasets. (arXiv:2302.07317v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;TAILOR&#65292;&#23427;&#22312;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#24212;&#29992;&#30340;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20505;&#36873;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#25928;&#29575;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#25152;&#38656;&#30340;&#26631;&#35760;&#31034;&#20363;&#25968;&#37327;&#65292;&#20294;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#20013;&#65292;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#35777;&#24615;&#33021;&#21487;&#33021;&#20250;&#22823;&#24133;&#24230;&#21464;&#21270;&#12290;&#20107;&#20808;&#24456;&#38590;&#30693;&#36947;&#21738;&#31181;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#34920;&#29616;&#33391;&#22909;&#25110;&#26368;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;&#12290;&#23545;&#20110;&#20219;&#20309;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;(meta)&#31639;&#27861;TAILOR (Thompson ActIve Learning algORithm selection)&#36845;&#20195;&#22320;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#19968;&#32452;&#20505;&#36873;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#12290;TAILOR&#20351;&#29992;&#26088;&#22312;&#25910;&#38598;&#31867;&#24179;&#34913;&#31034;&#20363;&#30340;&#26032;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#24212;&#29992;&#30340;&#22823;&#37327;&#23454;&#39564;&#20013;&#65292;TAILOR&#22312;&#23454;&#29616;&#19982;&#20505;&#36873;&#31639;&#27861;&#20013;&#26368;&#20339;&#31639;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label efficiency has become an increasingly important objective in deep learning applications. Active learning aims to reduce the number of labeled examples needed to train deep networks, but the empirical performance of active learning algorithms can vary dramatically across datasets and applications. It is difficult to know in advance which active learning strategy will perform well or best in a given application. To address this, we propose the first adaptive algorithm selection strategy for deep active learning. For any unlabeled dataset, our (meta) algorithm TAILOR (Thompson ActIve Learning algORithm selection) iteratively and adaptively chooses among a set of candidate active learning algorithms. TAILOR uses novel reward functions aimed at gathering class-balanced examples. Extensive experiments in multi-class and multi-label applications demonstrate TAILOR's effectiveness in achieving accuracy comparable or better than that of the best of the candidate algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;SEGA&#30340;&#35821;&#20041;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#19982;&#25193;&#25955;&#36807;&#31243;&#30340;&#20114;&#21160;&#65292;SEGA&#21487;&#20197;&#28789;&#27963;&#22320;&#22312;&#35821;&#20041;&#26041;&#21521;&#19978;&#24341;&#23548;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#23454;&#29616;&#32454;&#24494;&#21644;&#24191;&#27867;&#30340;&#32534;&#36753;&#20197;&#21450;&#20248;&#21270;&#25972;&#20307;&#33402;&#26415;&#26500;&#24605;&#12290;&#23454;&#39564;&#35777;&#26126;SEGA&#22312;&#22810;&#31181;&#20219;&#21153;&#21644;&#29983;&#25104;&#26550;&#26500;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#22810;&#21151;&#33021;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2301.12247</link><description>&lt;p&gt;
SEGA&#65306;&#20351;&#29992;&#35821;&#20041;&#24341;&#23548;&#25351;&#23548;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
SEGA: Instructing Text-to-Image Models using Semantic Guidance. (arXiv:2301.12247v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;SEGA&#30340;&#35821;&#20041;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#19982;&#25193;&#25955;&#36807;&#31243;&#30340;&#20114;&#21160;&#65292;SEGA&#21487;&#20197;&#28789;&#27963;&#22320;&#22312;&#35821;&#20041;&#26041;&#21521;&#19978;&#24341;&#23548;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#23454;&#29616;&#32454;&#24494;&#21644;&#24191;&#27867;&#30340;&#32534;&#36753;&#20197;&#21450;&#20248;&#21270;&#25972;&#20307;&#33402;&#26415;&#26500;&#24605;&#12290;&#23454;&#39564;&#35777;&#26126;SEGA&#22312;&#22810;&#31181;&#20219;&#21153;&#21644;&#29983;&#25104;&#26550;&#26500;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#22810;&#21151;&#33021;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#20196;&#20154;&#24778;&#35766;&#30340;&#33021;&#21147;&#21487;&#20197;&#20165;&#36890;&#36807;&#25991;&#26412;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#30340;&#21333;&#27425;&#29983;&#25104;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#32780;&#36755;&#20837;&#25552;&#31034;&#30340;&#24494;&#23567;&#25913;&#21464;&#24448;&#24448;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#22270;&#20687;&#12290;&#36825;&#20351;&#24471;&#29992;&#25143;&#22312;&#35821;&#20041;&#25511;&#21046;&#26041;&#38754;&#26377;&#38480;&#12290;&#20026;&#20102;&#20351;&#29992;&#25143;&#23454;&#29616;&#25511;&#21046;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#19982;&#25193;&#25955;&#36807;&#31243;&#20114;&#21160;&#26469;&#28789;&#27963;&#22320;&#24341;&#23548;&#35821;&#20041;&#26041;&#21521;&#12290;&#36825;&#31181;&#35821;&#20041;&#24341;&#23548;(SEGA)&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#20309;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#29983;&#25104;&#26550;&#26500;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20801;&#35768;&#36827;&#34892;&#32454;&#24494;&#21644;&#24191;&#27867;&#30340;&#32534;&#36753;&#65292;&#32452;&#25104;&#21644;&#39118;&#26684;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#20248;&#21270;&#25972;&#20307;&#33402;&#26415;&#26500;&#24605;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#20219;&#21153;&#23637;&#31034;&#20102;SEGA&#22312;&#28508;&#22312;&#21644;&#20687;&#32032;&#32423;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;Stable Diffusion&#65292;Paella&#21644;DeepFloyd-IF&#65289;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#20026;&#20854;&#22810;&#21151;&#33021;&#24615;&#65292;&#28789;&#27963;&#24615;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) generalizes to any generative architecture using classifier-free guidance. More importantly, it allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on both latent and pixel-based diffusion models such as Stable Diffusion, Paella, and DeepFloyd-IF using a variety of tasks, thus providing strong evidence for its versatility, flexibility, and improvements ov
&lt;/p&gt;</description></item><item><title>Tracr&#26159;&#19968;&#20010;&#32534;&#35793;&#22120;&#65292;&#23558;&#21487;&#35835;&#24615;&#24378;&#30340;&#31243;&#24207;&#32534;&#35793;&#25104;&#26631;&#20934;&#30340;&#20165;&#35299;&#30721;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#32534;&#35793;&#27169;&#22411;&#30340;&#24050;&#30693;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#21644;&#35780;&#20272;&#21487;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.05062</link><description>&lt;p&gt;
Tracr: &#32534;&#35793;&#21464;&#21387;&#22120;&#27169;&#22411;&#20316;&#20026;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#23460;
&lt;/p&gt;
&lt;p&gt;
Tracr: Compiled Transformers as a Laboratory for Interpretability. (arXiv:2301.05062v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05062
&lt;/p&gt;
&lt;p&gt;
Tracr&#26159;&#19968;&#20010;&#32534;&#35793;&#22120;&#65292;&#23558;&#21487;&#35835;&#24615;&#24378;&#30340;&#31243;&#24207;&#32534;&#35793;&#25104;&#26631;&#20934;&#30340;&#20165;&#35299;&#30721;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#32534;&#35793;&#27169;&#22411;&#30340;&#24050;&#30693;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#21644;&#35780;&#20272;&#21487;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#21487;&#35835;&#24615;&#24378;&#30340;&#31243;&#24207;&#32534;&#35793;&#25104;&#26631;&#20934;&#30340;&#20165;&#35299;&#30721;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32534;&#35793;&#22120;Tracr&#29983;&#25104;&#20855;&#26377;&#24050;&#30693;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#30740;&#31350;&#25191;&#34892;&#22810;&#27493;&#31639;&#27861;&#30340;&#21464;&#21387;&#22120;&#20013;&#30340;&#8220;&#21472;&#21152;&#8221;&#12290;&#27492;&#22806;&#65292;Tracr&#32534;&#35793;&#27169;&#22411;&#30340;&#24050;&#30693;&#32467;&#26500;&#21487;&#20197;&#20316;&#20026;&#35780;&#20272;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#30495;&#23454;&#22522;&#20934;&#12290;&#36890;&#24120;&#65292;&#30001;&#20110;&#21464;&#21387;&#22120;&#23398;&#20064;&#30340;&#8220;&#31243;&#24207;&#8221;&#26159;&#26410;&#30693;&#30340;&#65292;&#22240;&#27492;&#19981;&#28165;&#26970;&#35299;&#37322;&#26159;&#21542;&#25104;&#21151;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#21644;&#26816;&#26597;&#21253;&#25324;&#35745;&#31639;&#20196;&#29260;&#39057;&#29575;&#12289;&#25490;&#24207;&#21644;&#25324;&#21495;&#26816;&#26597;&#22312;&#20869;&#30340;&#31243;&#24207;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;https://github.com/deepmind/tracr&#25552;&#20379;&#20102;Tracr&#30340;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.
&lt;/p&gt;</description></item><item><title>&#33521;&#22269;COVID-19 Vocal Audio Dataset&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;SARS-CoV-2 PCR&#21442;&#32771;&#38899;&#39057;&#35760;&#24405;&#38598;&#21512;&#65292;&#26088;&#22312;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;SARS-CoV-2&#24863;&#26579;&#29366;&#24577;&#25110;&#30456;&#20851;&#21628;&#21560;&#30151;&#29366;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2212.07738</link><description>&lt;p&gt;
&#19968;&#20221;&#22823;&#35268;&#27169;&#30340;&#12289;&#22522;&#20110;PCR&#30340;COVID-19&#22768;&#38899;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A large-scale and PCR-referenced vocal audio dataset for COVID-19. (arXiv:2212.07738v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07738
&lt;/p&gt;
&lt;p&gt;
&#33521;&#22269;COVID-19 Vocal Audio Dataset&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;SARS-CoV-2 PCR&#21442;&#32771;&#38899;&#39057;&#35760;&#24405;&#38598;&#21512;&#65292;&#26088;&#22312;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;SARS-CoV-2&#24863;&#26579;&#29366;&#24577;&#25110;&#30456;&#20851;&#21628;&#21560;&#30151;&#29366;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The UK COVID-19 Vocal Audio Dataset is the largest collection of SARS-CoV-2 PCR-referenced audio recordings to date, designed for the training and evaluation of machine learning models that classify SARS-CoV-2 infection status or associated respiratory symptoms using vocal audio.
&lt;/p&gt;
&lt;p&gt;
&#33521;&#22269;COVID-19 Vocal Audio Dataset&#26088;&#22312;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;SARS-CoV-2&#24863;&#26579;&#29366;&#24577;&#25110;&#30456;&#20851;&#21628;&#21560;&#30151;&#29366;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#12290;&#33521;&#22269;&#21355;&#29983;&#23433;&#20840;&#23616;&#36890;&#36807;&#22269;&#23478;&#27979;&#35797;&#21644;&#36861;&#36394;&#35745;&#21010;&#21644;REACT-1&#35843;&#26597;&#22312;2021&#24180;3&#26376;&#33267;2022&#24180;3&#26376;&#26399;&#38388;&#25307;&#21215;&#20102;&#33258;&#24895;&#21442;&#19982;&#32773;&#65292;&#25910;&#38598;&#20102;&#33258;&#24895;&#21683;&#22013;&#12289;&#21628;&#27668;&#21644;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#65292;&#24182;&#23558;&#20854;&#19982;SARS-CoV-2&#26816;&#27979;&#32467;&#26524;&#30456;&#20851;&#32852;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;SARS-CoV-2 PCR&#21442;&#32771;&#38899;&#39057;&#35760;&#24405;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The UK COVID-19 Vocal Audio Dataset is designed for the training and evaluation of machine learning models that classify SARS-CoV-2 infection status or associated respiratory symptoms using vocal audio. The UK Health Security Agency recruited voluntary participants through the national Test and Trace programme and the REACT-1 survey in England from March 2021 to March 2022, during dominant transmission of the Alpha and Delta SARS-CoV-2 variants and some Omicron variant sublineages. Audio recordings of volitional coughs, exhalations, and speech were collected in the 'Speak up to help beat coronavirus' digital survey alongside demographic, self-reported symptom and respiratory condition data, and linked to SARS-CoV-2 test results. The UK COVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2 PCR-referenced audio recordings to date. PCR results were linked to 70,794 of 72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms were reported by 45.6
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#20998;&#31163;PINN&#65288;SPINN&#65289;&#30340;&#32593;&#32476;&#26550;&#26500;&#26469;&#20943;&#36731;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2211.08761</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;PINN&#65306;&#20943;&#36731;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Separable PINN: Mitigating the Curse of Dimensionality in Physics-Informed Neural Networks. (arXiv:2211.08761v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08761
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#20998;&#31163;PINN&#65288;SPINN&#65289;&#30340;&#32593;&#32476;&#26550;&#26500;&#26469;&#20943;&#36731;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#25104;&#20026;&#21069;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#30340;&#26032;&#22411;&#25968;&#25454;&#39537;&#21160;PDE&#27714;&#35299;&#22120;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#33719;&#24471;&#35299;&#30340;&#26114;&#36149;&#35745;&#31639;&#25104;&#26412;&#36890;&#24120;&#38480;&#21046;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#35757;&#32451;PINN&#26102;&#65292;&#36890;&#36807;&#21033;&#29992;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;&#28982;&#32780;&#65292;&#23545;&#20256;&#32479;PINN&#36827;&#34892;&#31616;&#21333;&#30340;&#21069;&#21521;&#27169;&#24335;AD&#24212;&#29992;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#35745;&#31639;&#37327;&#65292;&#22833;&#21435;&#20854;&#23454;&#38469;&#25928;&#30410;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#20998;&#31163;PINN&#65288;SPINN&#65289;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#21487;&#20197;&#20419;&#36827;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#30340;&#21069;&#21521;&#27169;&#24335;AD&#12290;SPINN&#22312;&#27599;&#20010;&#36724;&#19978;&#25805;&#20316;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;PINN&#20013;&#30340;&#36880;&#28857;&#22788;&#29702;&#65292;&#20943;&#23569;&#20102;&#32593;&#32476;&#27491;&#21521;&#20256;&#36882;&#30340;&#27425;&#25968;&#12290;&#27492;&#22806;&#65292;&#26631;&#20934;PINN&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#38543;&#30528;&#32593;&#26684;&#20998;&#36776;&#29575;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32780;&#25105;&#20204;&#27169;&#22411;&#30340;&#25104;&#26412;&#30456;&#23545;&#36739;&#20302;&#65292;&#20943;&#36731;&#20102;&#32500;&#24230;&#35781;&#21650;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have emerged as new data-driven PDE solvers for both forward and inverse problems. While promising, the expensive computational costs to obtain solutions often restrict their broader applicability. We demonstrate that the computations in automatic differentiation (AD) can be significantly reduced by leveraging forward-mode AD when training PINN. However, a naive application of forward-mode AD to conventional PINNs results in higher computation, losing its practical benefit. Therefore, we propose a network architecture, called separable PINN (SPINN), which can facilitate forward-mode AD for more efficient computation. SPINN operates on a per-axis basis instead of point-wise processing in conventional PINNs, decreasing the number of network forward passes. Besides, while the computation and memory costs of standard PINNs grow exponentially along with the grid resolution, that of our model is remarkably less susceptible, mitigating the curse of dim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25277;&#35937;&#25688;&#35201;&#30340;&#22522;&#30784;&#19978;&#20248;&#21270;&#26102;&#38388;&#32447;&#25688;&#35201;&#30340;&#36136;&#37327;&#21644;&#21487;&#35835;&#24615;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.07596</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#25277;&#35937;&#26102;&#38388;&#32447;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Towards Abstractive Timeline Summarisation using Preference-based Reinforcement Learning. (arXiv:2211.07596v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25277;&#35937;&#25688;&#35201;&#30340;&#22522;&#30784;&#19978;&#20248;&#21270;&#26102;&#38388;&#32447;&#25688;&#35201;&#30340;&#36136;&#37327;&#21644;&#21487;&#35835;&#24615;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#24635;&#32467;&#22810;&#20010;&#26032;&#38395;&#26469;&#28304;&#25253;&#36947;&#30340;&#26102;&#38388;&#32447;&#12290;&#22522;&#20110;Transformer&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#19988;&#31616;&#27905;&#30340;&#38271;&#25991;&#26723;&#25688;&#35201;&#65292;&#20294;&#22312;&#26102;&#38388;&#32447;&#25688;&#35201;&#65288;TLS&#65289;&#31561;&#29305;&#23450;&#20219;&#21153;&#19978;&#21487;&#33021;&#26080;&#27861;&#36229;&#36234;&#24050;&#24314;&#31435;&#30340;&#25277;&#21462;&#26041;&#27861;&#12290;&#34429;&#28982;&#25277;&#21462;&#25688;&#35201;&#26356;&#24544;&#23454;&#20110;&#26469;&#28304;&#65292;&#20294;&#21487;&#33021;&#19981;&#22826;&#26131;&#35835;&#19988;&#21253;&#21547;&#20887;&#20313;&#25110;&#19981;&#24517;&#35201;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PBRL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#25688;&#35201;&#22120;&#36866;&#24212;&#20110;TLS&#65292;&#21487;&#20197;&#20811;&#26381;&#25277;&#21462;&#26102;&#38388;&#32447;&#25688;&#35201;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#22797;&#21512;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#20851;&#38190;&#35789;&#21644;&#20559;&#22909;&#26631;&#31614;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36890;&#36807;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#23545;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#25688;&#35201;&#22120;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#21487;&#27604;&#36739;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel pipeline for summarising timelines of events reported by multiple news sources. Transformer-based models for abstractive summarisation generate coherent and concise summaries of long documents but can fail to outperform established extractive methods on specialised tasks such as timeline summarisation (TLS). While extractive summaries are more faithful to their sources, they may be less readable and contain redundant or unnecessary information. This paper proposes a preference-based reinforcement learning (PBRL) method for adapting pretrained abstractive summarisers to TLS, which can overcome the drawbacks of extractive timeline summaries. We define a compound reward function that learns from keywords of interest and pairwise preference labels, which we use to fine-tune a pretrained abstractive summariser via offline reinforcement learning. We carry out both automated and human evaluation on three datasets, finding that our method outperforms a comparable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25104;&#26412;&#24863;&#30693;&#30340;&#36890;&#29992;&#945;&#25237;&#36164;&#27861;&#36827;&#34892;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#65292;&#25299;&#23637;&#20102;&#945;&#25237;&#36164;&#35268;&#21017;&#20197;&#32771;&#34385;&#26679;&#26412;&#22823;&#23567;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#33258;&#28982;&#23545;&#25239;&#30340;&#21338;&#24328;&#26469;&#20248;&#21270;&#945;&#36130;&#23500;&#30340;&#26399;&#26395;&#22238;&#25253;(ERO)&#24182;&#25552;&#20379;&#26368;&#20339;&#26679;&#26412;&#22823;&#23567;&#12290;&#32463;&#23454;&#35777;&#34920;&#26126;&#65292;&#35813;&#35268;&#21017;&#33021;&#26356;&#20934;&#30830;&#22320;&#25298;&#32477;&#34394;&#20551;&#38646;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2210.17514</link><description>&lt;p&gt;
&#25104;&#26412;&#24863;&#30693;&#30340;&#36890;&#29992;&#945;&#25237;&#36164;&#27861;&#29992;&#20110;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Cost-aware Generalized $\alpha$-investing for Multiple Hypothesis Testing. (arXiv:2210.17514v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25104;&#26412;&#24863;&#30693;&#30340;&#36890;&#29992;&#945;&#25237;&#36164;&#27861;&#36827;&#34892;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#65292;&#25299;&#23637;&#20102;&#945;&#25237;&#36164;&#35268;&#21017;&#20197;&#32771;&#34385;&#26679;&#26412;&#22823;&#23567;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#33258;&#28982;&#23545;&#25239;&#30340;&#21338;&#24328;&#26469;&#20248;&#21270;&#945;&#36130;&#23500;&#30340;&#26399;&#26395;&#22238;&#25253;(ERO)&#24182;&#25552;&#20379;&#26368;&#20339;&#26679;&#26412;&#22823;&#23567;&#12290;&#32463;&#23454;&#35777;&#34920;&#26126;&#65292;&#35813;&#35268;&#21017;&#33021;&#26356;&#20934;&#30830;&#22320;&#25298;&#32477;&#34394;&#20551;&#38646;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22312;&#25968;&#25454;&#25910;&#38598;&#20855;&#26377;&#38750;&#24179;&#20961;&#25104;&#26412;&#26102;&#36827;&#34892;&#39034;&#24207;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#22312;&#35782;&#21035;&#30142;&#30149;&#36807;&#31243;&#20013;&#30340;&#24046;&#24322;&#34920;&#36798;&#22522;&#22240;&#31561;&#29983;&#29289;&#23454;&#39564;&#20013;&#20986;&#29616;&#12290;&#26412;&#25991;&#26500;&#24314;&#22312;&#36890;&#29992;&#945;&#25237;&#36164;&#26694;&#26550;&#19978;&#65292;&#33021;&#22312;&#39034;&#24207;&#26816;&#39564;&#29615;&#22659;&#19979;&#36827;&#34892;&#35823;&#21457;&#29616;&#29575;&#25511;&#21046;&#12290;&#25105;&#20204;&#23545;&#945;-&#36130;&#23500;&#30340;&#38271;&#26399;&#28176;&#36827;&#34892;&#20026;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24341;&#20986;&#20102;&#22312;&#945;&#25237;&#36164;&#20915;&#31574;&#35268;&#21017;&#20013;&#32771;&#34385;&#26679;&#26412;&#22823;&#23567;&#30340;&#24605;&#32771;&#12290;&#23558;&#26816;&#39564;&#36807;&#31243;&#35270;&#20026;&#19982;&#33258;&#28982;&#23545;&#25239;&#30340;&#21338;&#24328;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#31181;&#20915;&#31574;&#35268;&#21017;&#65292;&#20248;&#21270;&#945;&#36130;&#23500;&#30340;&#26399;&#26395;&#22238;&#25253;(ERO)&#65292;&#24182;&#20026;&#27979;&#35797;&#25552;&#20379;&#20102;&#26368;&#20339;&#26679;&#26412;&#22823;&#23567;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#25104;&#26412;&#24863;&#30693;&#30340;ERO&#20915;&#31574;&#35268;&#21017;&#27604;&#20854;&#20182;&#26041;&#27861;&#27491;&#30830;&#22320;&#25298;&#32477;&#20102;&#26356;&#22810;&#30340;&#34394;&#20551;&#38646;&#20551;&#35774;&#12290;&#26412;&#25991;&#25193;&#23637;&#20102;&#25104;&#26412;&#24863;&#30693;&#30340;ERO&#25237;&#36164;&#33267;&#26377;&#38480;&#26102;&#38388;&#26816;&#39564;&#65292;&#20351;&#20915;&#31574;&#35268;&#21017;&#33021;&#22815;&#36328;&#36234;&#22810;&#20010;&#26816;&#39564;&#20998;&#37197;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sequential multiple hypothesis testing with nontrivial data collection cost. This problem appears, for example, when conducting biological experiments to identify differentially expressed genes in a disease process. This work builds on the generalized $\alpha$-investing framework that enables control of the false discovery rate in a sequential testing setting. We make a theoretical analysis of the long term asymptotic behavior of $\alpha$-wealth which motivates a consideration of sample size in the $\alpha$-investing decision rule. Posing the testing process as a game with nature, we construct a decision rule that optimizes the expected return (ERO) of $\alpha$-wealth and provides an optimal sample size for the test. Empirical results show that a cost-aware ERO decision rule correctly rejects more false null hypotheses than other methods. We extend cost-aware ERO investing to finite-horizon testing which enables the decision rule to allocate samples across ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21270;&#65292;&#21457;&#29616;&#30446;&#21069;&#24120;&#29992;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#35843;&#25972;&#21516;&#36136;&#24615;&#65292;&#35813;&#25351;&#26631;&#28385;&#36275;&#26356;&#22810;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#20855;&#26377;&#36739;&#23569;&#22312;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2209.06177</link><description>&lt;p&gt;
&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21270;&#65306;&#21516;&#36136;&#24615;-&#24322;&#36136;&#24615;&#20108;&#20998;&#27861;&#21450;&#20854;&#24310;&#20280;
&lt;/p&gt;
&lt;p&gt;
Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond. (arXiv:2209.06177v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21270;&#65292;&#21457;&#29616;&#30446;&#21069;&#24120;&#29992;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#35843;&#25972;&#21516;&#36136;&#24615;&#65292;&#35813;&#25351;&#26631;&#28385;&#36275;&#26356;&#22810;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#20855;&#26377;&#36739;&#23569;&#22312;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#36136;&#24615;&#26159;&#25551;&#36848;&#36793;&#36830;&#25509;&#30456;&#20284;&#33410;&#28857;&#20542;&#21521;&#30340;&#22270;&#24418;&#23646;&#24615;&#65307;&#30456;&#21453;&#30340;&#27010;&#24565;&#20026;&#24322;&#36136;&#24615;&#12290;&#20154;&#20204;&#36890;&#24120;&#35748;&#20026;&#24322;&#36136;&#24615;&#22270;&#23545;&#20110;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#19988;&#24050;&#32463;&#20184;&#20986;&#20102;&#35768;&#22810;&#21162;&#21147;&#26469;&#24320;&#21457;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#25991;&#29486;&#20013;&#27809;&#26377;&#26222;&#36941;&#34987;&#25509;&#21463;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20026;&#27491;&#30830;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#25351;&#26631;&#24418;&#24335;&#21270;&#20102;&#26399;&#26395;&#30340;&#29305;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#21738;&#20123;&#25351;&#26631;&#28385;&#36275;&#21738;&#20123;&#29305;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#35843;&#25972;&#21516;&#36136;&#24615;&#30340;&#25351;&#26631;&#28385;&#36275;&#27604;&#20854;&#20182;&#27969;&#34892;&#21516;&#36136;&#24615;&#27979;&#37327;&#26041;&#27861;&#26356;&#22810;&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#32780;&#22312;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24456;&#23569;&#34987;&#20351;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#21516;&#36136;&#24615;-&#24322;&#36136;&#24615;&#20108;&#20998;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#65292;&#20351;&#24471;...
&lt;/p&gt;
&lt;p&gt;
Homophily is a graph property describing the tendency of edges to connect similar nodes; the opposite is called heterophily. It is often believed that heterophilous graphs are challenging for standard message-passing graph neural networks (GNNs), and much effort has been put into developing efficient methods for this setting. However, there is no universally agreed-upon measure of homophily in the literature. In this work, we show that commonly used homophily measures have critical drawbacks preventing the comparison of homophily levels across different datasets. For this, we formalize desirable properties for a proper homophily measure and verify which measures satisfy which properties. In particular, we show that a measure that we call adjusted homophily satisfies more desirable properties than other popular homophily measures while being rarely used in graph machine learning literature. Then, we go beyond the homophily-heterophily dichotomy and propose a new characteristic that allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#26680;&#26426;&#22120;&#21644;&#26497;&#38480;&#23398;&#20064;&#26426;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20219;&#21153;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#29305;&#24449;&#31354;&#38388;&#30340;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#20248;&#21270;RBF&#26680;&#21442;&#25968;&#12289;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#22810;&#36755;&#20986;&#31232;&#30095;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.03028</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#23398;&#20064;&#22810;&#20219;&#21153;&#38382;&#39064;&#30340;&#29305;&#24449;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Bayesian learning of feature spaces for multitasks problems. (arXiv:2209.03028v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#26680;&#26426;&#22120;&#21644;&#26497;&#38480;&#23398;&#20064;&#26426;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20219;&#21153;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#29305;&#24449;&#31354;&#38388;&#30340;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#20248;&#21270;RBF&#26680;&#21442;&#25968;&#12289;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#22810;&#36755;&#20986;&#31232;&#30095;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;(RFFs)&#36817;&#20284;&#24452;&#21521;&#22522;&#20989;&#25968;(RBF)&#26680;&#65292;&#23558;&#26680;&#26426;&#22120;(KM)&#21644;&#26497;&#38480;&#23398;&#20064;&#26426;(ELM)&#30456;&#36830;&#25509;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#36129;&#29486;&#26159;&#34920;&#26126;&#23545;&#20110;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;KM&#21644;ELM&#30340;&#24418;&#24335;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#21516;&#19968;&#26522;&#30828;&#24065;&#30340;&#20004;&#38754;&#12290;&#36825;&#20123;&#25552;&#20986;&#30340;&#27169;&#22411;&#31216;&#20026;RFF-BLR&#65292;&#24314;&#31435;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19978;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#35774;&#35745;&#30446;&#26631;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#22522;&#20110;&#24102;&#26377;RBF&#26680;&#30340;KM&#25311;&#21512;&#22810;&#20219;&#21153;&#22238;&#24402;&#22120;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#31181;&#36328;&#20219;&#21153;&#30340;&#20849;&#21516;&#20808;&#39564;&#65292;&#20419;&#36827;&#20102;ELM&#35270;&#22270;&#20013;&#30340;&#22810;&#36755;&#20986;&#31232;&#30095;&#24615;&#12290;&#36825;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#20351;&#24471;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;KM&#21644;ELM&#30340;&#35266;&#28857;&#65292;&#23454;&#29616;&#20102;(i)&#22312;&#27010;&#29575;&#26694;&#26550;&#20869;&#20248;&#21270;RBF&#26680;&#21442;&#25968;$\gamma$&#65292;(ii)&#20248;&#21270;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#21644;(iii)
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach for multi-task regression that connects Kernel Machines (KMs) and Extreme Learning Machines (ELMs) through the exploitation of the Random Fourier Features (RFFs) approximation of the RBF kernel. In this sense, one of the contributions of this paper shows that for the proposed models, the KM and the ELM formulations can be regarded as two sides of the same coin. These proposed models, termed RFF-BLR, stand on a Bayesian framework that simultaneously addresses two main design goals. On the one hand, it fits multitask regressors based on KMs endowed with RBF kernels. On the other hand, it enables the introduction of a common-across-tasks prior that promotes multioutput sparsity in the ELM view. This Bayesian approach facilitates the simultaneous consideration of both the KM and ELM perspectives enabling (i) the optimisation of the RBF kernel parameter $\gamma$ within a probabilistic framework, (ii) the optimisation of the model complexity, and (iii) 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23398;&#20064;&#39046;&#22495;&#25351;&#21335;&#20013;&#30340;&#25554;&#22270;&#36827;&#34892;&#26410;&#30693;&#40479;&#31867;&#29289;&#31181;&#30340;&#38646;&#26679;&#26412;&#35782;&#21035;&#12290;&#36890;&#36807;&#23545;&#25554;&#22270;&#36827;&#34892;&#23545;&#27604;&#32534;&#30721;&#21644;&#32467;&#26500;&#19978;&#26356;&#31867;&#20284;&#20110;&#29031;&#29255;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;12%&#30340;top-1&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;38%&#30340;top-10&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#35777;&#26126;&#20102;&#39046;&#22495;&#25351;&#21335;&#25554;&#22270;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.01466</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#39046;&#22495;&#25351;&#21335;&#35782;&#21035;&#26410;&#30693;&#40479;&#31867;&#29289;&#31181;
&lt;/p&gt;
&lt;p&gt;
Recognition of Unseen Bird Species by Learning from Field Guides. (arXiv:2206.01466v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01466
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23398;&#20064;&#39046;&#22495;&#25351;&#21335;&#20013;&#30340;&#25554;&#22270;&#36827;&#34892;&#26410;&#30693;&#40479;&#31867;&#29289;&#31181;&#30340;&#38646;&#26679;&#26412;&#35782;&#21035;&#12290;&#36890;&#36807;&#23545;&#25554;&#22270;&#36827;&#34892;&#23545;&#27604;&#32534;&#30721;&#21644;&#32467;&#26500;&#19978;&#26356;&#31867;&#20284;&#20110;&#29031;&#29255;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;12%&#30340;top-1&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;38%&#30340;top-10&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#35777;&#26126;&#20102;&#39046;&#22495;&#25351;&#21335;&#25554;&#22270;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#39046;&#22495;&#25351;&#21335;&#26469;&#23398;&#20064;&#40479;&#31867;&#29289;&#31181;&#35782;&#21035;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26410;&#30693;&#29289;&#31181;&#30340;&#38646;&#26679;&#26412;&#35782;&#21035;&#12290;&#39046;&#22495;&#25351;&#21335;&#20013;&#30340;&#25554;&#22270;&#26377;&#24847;&#19987;&#27880;&#20110;&#27599;&#20010;&#29289;&#31181;&#30340;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#23558;&#24050;&#30693;&#29289;&#31181;&#30340;&#30693;&#35782;&#36801;&#31227;&#21040;&#26410;&#30693;&#40479;&#31867;&#29289;&#31181;&#19978;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;1&#65289;&#23545;&#25554;&#22270;&#36827;&#34892;&#23545;&#27604;&#32534;&#30721;&#65292;&#21487;&#20197;&#36755;&#20837;&#26631;&#20934;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#26696;&#65307;&#65288;2&#65289;&#19968;&#31181;&#21033;&#29992;&#25554;&#22270;&#20063;&#26159;&#22270;&#20687;&#30340;&#20107;&#23454;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#19978;&#26356;&#31867;&#20284;&#20110;&#29031;&#29255;&#32780;&#19981;&#26159;&#20854;&#20182;&#31867;&#22411;&#36741;&#21161;&#20449;&#24687;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26469;&#33258;&#39046;&#22495;&#25351;&#21335;&#30340;&#25554;&#22270;&#30830;&#23454;&#26159;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#19968;&#20010;&#26377;&#31454;&#20105;&#21147;&#30340;&#36741;&#21161;&#20449;&#24687;&#26469;&#28304;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26041;&#20415;&#22320;&#33719;&#21462;&#22810;&#31181;&#29289;&#31181;&#30340;&#25554;&#22270;&#12290;&#22312;iNaturalist2021&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#20013;&#65292;&#21547;&#26377;749&#20010;&#24050;&#30693;&#29289;&#31181;&#21644;739&#20010;&#26410;&#30693;&#29289;&#31181;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;12%&#30340;&#26410;&#30693;&#40479;&#31867;&#29289;&#31181;&#30340;top-1&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;38%&#30340;top-10&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#36825;&#34920;&#26126;&#20102;&#20854;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We exploit field guides to learn bird species recognition, in particular zero-shot recognition of unseen species. Illustrations contained in field guides deliberately focus on discriminative properties of each species, and can serve as side information to transfer knowledge from seen to unseen bird species. We study two approaches: (1) a contrastive encoding of illustrations, which can be fed into standard zero-shot learning schemes; and (2) a novel method that leverages the fact that illustrations are also images and as such structurally more similar to photographs than other kinds of side information. Our results show that illustrations from field guides, which are readily available for a wide range of species, are indeed a competitive source of side information for zero-shot learning. On a subset of the iNaturalist2021 dataset with 749 seen and 739 unseen species, we obtain a classification accuracy of unseen bird species of $12\%$ @top-1 and $38\%$ @top-10, which shows the potentia
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GND-Nets &#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#27973;&#23618;&#32593;&#32476;&#21644;&#23616;&#37096;&#12289;&#20840;&#23616;&#37051;&#22495;&#20449;&#24687;&#26469;&#35299;&#20915;&#22270;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#27424;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.09698</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#25193;&#25955;&#32593;&#32476;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Diffusion Networks for Semi-supervised Learning. (arXiv:2201.09698v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09698
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GND-Nets &#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#27973;&#23618;&#32593;&#32476;&#21644;&#23616;&#37096;&#12289;&#20840;&#23616;&#37051;&#22495;&#20449;&#24687;&#26469;&#35299;&#20915;&#22270;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#27424;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476; (GCN) &#26159;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20808;&#39537;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;GCN &#22312;&#26631;&#35760;&#31232;&#30095;&#30340;&#22270;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20854;&#20004;&#23618;&#29256;&#26412;&#19981;&#33021;&#26377;&#25928;&#22320;&#23558;&#26631;&#31614;&#20449;&#24687;&#20256;&#25773;&#21040;&#25972;&#20010;&#22270;&#32467;&#26500;&#65288;&#21363;&#27424;&#24179;&#28369;&#38382;&#39064;&#65289;&#65292;&#32780;&#20854;&#28145;&#23618;&#29256;&#26412;&#21017;&#36807;&#24230;&#24179;&#28369;&#19988;&#38590;&#20197;&#35757;&#32451;&#65288;&#21363;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026; GND-Nets&#65288;&#22270;&#31070;&#32463;&#25193;&#25955;&#32593;&#32476;&#65289;&#65292;&#23427;&#22312;&#21333;&#23618;&#20013;&#21033;&#29992;&#20102;&#39030;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#37051;&#22495;&#20449;&#24687;&#12290;&#21033;&#29992;&#27973;&#23618;&#32593;&#32476;&#21487;&#20197;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#32780;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#37051;&#22495;&#20449;&#24687;&#21487;&#20197;&#32531;&#35299;&#27424;&#24179;&#28369;&#38382;&#39064;&#12290;&#39030;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#37051;&#22495;&#20449;&#24687;&#30340;&#21033;&#29992;&#26159;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#25193;&#25955;&#30340;&#26032;&#22270;&#25193;&#25955;&#26041;&#27861;&#23454;&#29616;&#30340;&#65292;&#35813;&#26041;&#27861;&#23558;&#31070;&#32463;&#32593;&#32476;&#34701;&#20837;&#20256;&#32479;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#22270;&#25193;&#25955;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCN) is a pioneering model for graph-based semi-supervised learning. However, GCN does not perform well on sparsely-labeled graphs. Its two-layer version cannot effectively propagate the label information to the whole graph structure (i.e., the under-smoothing problem) while its deep version over-smoothens and is hard to train (i.e., the over-smoothing problem). To solve these two issues, we propose a new graph neural network called GND-Nets (for Graph Neural Diffusion Networks) that exploits the local and global neighborhood information of a vertex in a single layer. Exploiting the shallow network mitigates the over-smoothing problem while exploiting the local and global neighborhood information mitigates the under-smoothing problem. The utilization of the local and global neighborhood information of a vertex is achieved by a new graph diffusion method called neural diffusions, which integrate neural networks into the conventional linear and nonlinear gra
&lt;/p&gt;</description></item><item><title>"Feature-Attending Recurrent Modules" (FARM)&#26159;&#19968;&#31181;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#29305;&#24449;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#26102;&#38388;&#35268;&#24459;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2112.08369</link><description>&lt;p&gt;
&#29305;&#24449;&#27880;&#24847;&#30340;&#36882;&#24402;&#27169;&#22359;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning. (arXiv:2112.08369v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08369
&lt;/p&gt;
&lt;p&gt;
"Feature-Attending Recurrent Modules" (FARM)&#26159;&#19968;&#31181;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#29305;&#24449;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#26102;&#38388;&#35268;&#24459;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#37325;&#35201;&#30340;&#20219;&#21153;&#37117;&#26159;&#20197;&#29289;&#20307;&#20026;&#22522;&#30784;&#23450;&#20041;&#30340;&#12290;&#20026;&#20102;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#23454;&#29616;&#27867;&#21270;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#21033;&#29992;&#29289;&#20307;&#25152;&#24341;&#21457;&#30340;&#32467;&#26500;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#30828;&#32534;&#30721;&#23545;&#35937;&#20013;&#24515;&#30340;&#29305;&#24449;&#65292;&#35201;&#20040;&#20351;&#29992;&#22797;&#26434;&#30340;&#23545;&#35937;&#20013;&#24515;&#29983;&#25104;&#27169;&#22411;&#65292;&#35201;&#20040;&#20351;&#29992;&#23616;&#37096;&#31354;&#38388;&#29305;&#24449;&#26356;&#26032;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#29616;&#27867;&#21270;RL&#20195;&#29702;&#26041;&#38754;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;"&#29305;&#24449;&#27880;&#24847;&#30340;&#36882;&#24402;&#27169;&#22359;"&#65288;FARM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20381;&#36182;&#20110;&#31616;&#21333;&#12289;&#24191;&#27867;&#36866;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#26102;&#38388;&#35268;&#24459;&#24615;&#12290;FARM&#23398;&#20064;&#20102;&#19968;&#31181;&#20998;&#24067;&#20110;&#22810;&#20010;&#27169;&#22359;&#20043;&#38388;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#20351;&#29992;&#20855;&#26377;&#34920;&#29616;&#21147;&#30340;&#29305;&#24449;&#27880;&#24847;&#26426;&#21046;&#20851;&#27880;&#26102;&#31354;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#25913;&#21892;&#20102;RL&#20195;&#29702;&#22312;&#23545;&#35937;&#20013;&#24515;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;2D&#21644;3D&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#20219;&#21153;&#22871;&#20214;&#65292;&#24182;&#21457;&#29616;FARM&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many important tasks are defined in terms of object. To generalize across these tasks, a reinforcement learning (RL) agent needs to exploit the structure that the objects induce. Prior work has either hard-coded object-centric features, used complex object-centric generative models, or updated state using local spatial features. However, these approaches have had limited success in enabling general RL agents. Motivated by this, we introduce "Feature-Attending Recurrent Modules" (FARM), an architecture for learning state representations that relies on simple, broadly applicable inductive biases for capturing spatial and temporal regularities. FARM learns a state representation that is distributed across multiple modules that each attend to spatiotemporal features with an expressive feature attention mechanism. We show that this improves an RL agent's ability to generalize across object-centric tasks. We study task suites in both 2D and 3D environments and find that FARM better generaliz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#30456;&#21464;&#38382;&#39064;&#65292;&#26681;&#25454;&#26368;&#23567;&#26497;&#22823;MISE&#36895;&#29575;&#30340;&#19981;&#21516;&#24773;&#20917;&#65292;&#30830;&#23450;&#20102;&#19981;&#21516;&#33539;&#22260;&#20869;&#26368;&#20339;&#30340;&#24179;&#28369;&#24230;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#29992;&#20110;&#24179;&#28369;&#20989;&#25968;&#31867;&#21035;&#30340;&#24230;&#37327;&#29109;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2112.03626</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Phase transitions in nonparametric regressions. (arXiv:2112.03626v6 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#30456;&#21464;&#38382;&#39064;&#65292;&#26681;&#25454;&#26368;&#23567;&#26497;&#22823;MISE&#36895;&#29575;&#30340;&#19981;&#21516;&#24773;&#20917;&#65292;&#30830;&#23450;&#20102;&#19981;&#21516;&#33539;&#22260;&#20869;&#26368;&#20339;&#30340;&#24179;&#28369;&#24230;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#29992;&#20110;&#24179;&#28369;&#20989;&#25968;&#31867;&#21035;&#30340;&#24230;&#37327;&#29109;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24050;&#30693;&#21333;&#21464;&#37327;&#30340;&#26410;&#30693;&#22238;&#24402;&#20989;&#25968;&#30340;&#23548;&#25968;&#22312;&#32477;&#23545;&#20540;&#19978;&#30452;&#21040;$(\gamma+1)$&#38454;&#37117;&#21463;&#21040;&#19968;&#20010;&#20844;&#20849;&#24120;&#25968;&#30340;&#30028;&#38480;&#65288;&#21363;$(\gamma+1)$&#38454;&#24179;&#28369;&#24230;&#65289;&#65292;&#25991;&#29486;&#20013;&#32473;&#20986;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MISE&#65289;&#30340;&#26368;&#23567;&#26497;&#22823;&#36895;&#29575;&#20026;$\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$&#12290;&#26412;&#25991;&#34920;&#26126;&#65306;&#65288;i&#65289;&#22914;&#26524;$n\leq\left(\gamma+1\right)^{2\gamma+3}$&#65292;&#21017;&#26368;&#23567;&#26497;&#22823;MISE&#36895;&#29575;&#20026;$\frac{\log n}{n\log(\log n)}$&#65292;&#24182;&#19988;&#26368;&#20339;&#21033;&#29992;&#30340;&#24179;&#28369;&#24230;&#20026;&#22823;&#32422;$\max\left\{ \left\lfloor \frac{\log n}{2\log\left(\log n\right)}\right\rfloor ,\,1\right\}$&#65307;&#65288;ii&#65289;&#22914;&#26524;$n&gt;\left(\gamma+1\right)^{2\gamma+3}$&#65292;&#21017;&#26368;&#23567;&#26497;&#22823;MISE&#36895;&#29575;&#20026;$\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$&#65292;&#24182;&#19988;&#26368;&#20339;&#21033;&#29992;&#30340;&#24179;&#28369;&#24230;&#20026;$\gamma+1$&#12290;&#26412;&#25991;&#30340;&#22522;&#26412;&#36129;&#29486;&#26159;&#25105;&#20204;&#20026;&#24179;&#28369;&#20989;&#25968;&#31867;&#21035;&#21046;&#23450;&#30340;&#19968;&#32452;&#24230;&#37327;&#29109;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the unknown regression function of a single variable is known to have derivatives up to the $(\gamma+1)$th order bounded in absolute values by a common constant everywhere or a.e. (i.e., $(\gamma+1)$th degree of smoothness), the minimax optimal rate of the mean integrated squared error (MISE) is stated as $\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$ in the literature. This paper shows that: (i) if $n\leq\left(\gamma+1\right)^{2\gamma+3}$, the minimax optimal MISE rate is $\frac{\log n}{n\log(\log n)}$ and the optimal degree of smoothness to exploit is roughly $\max\left\{ \left\lfloor \frac{\log n}{2\log\left(\log n\right)}\right\rfloor ,\,1\right\} $; (ii) if $n&gt;\left(\gamma+1\right)^{2\gamma+3}$, the minimax optimal MISE rate is $\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$ and the optimal degree of smoothness to exploit is $\gamma+1$. The fundamental contribution of this paper is a set of metric entropy bounds we develop for smooth function classes. Some 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#21704;&#24076;&#20989;&#25968;&#23646;&#24615;&#36827;&#34892;&#25968;&#23398;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20998;&#31867;&#23383;&#27597;&#34920;&#19978;&#30340;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#26368;&#22823;&#27744;&#21270;&#31561;&#20215;&#20110;&#36873;&#25321;&#19968;&#31181;&#26368;&#23567;&#21270;&#22120;&#25490;&#24207;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#19982;&#20854;&#20182;&#26368;&#23567;&#21270;&#22120;&#36317;&#31163;&#36739;&#36817;&#20294;&#19982;&#24207;&#21015;&#20013;&#30340;k-mer&#30456;&#36317;&#36739;&#36828;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2111.08452</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#23567;&#21270;&#22120;&#21644;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#29702;&#35770;&#36830;&#25509;&#21450;&#20854;&#22312;&#22522;&#22240;&#32452;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On minimizers and convolutional filters: theoretical connections and applications to genome analysis. (arXiv:2111.08452v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#21704;&#24076;&#20989;&#25968;&#23646;&#24615;&#36827;&#34892;&#25968;&#23398;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20998;&#31867;&#23383;&#27597;&#34920;&#19978;&#30340;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#26368;&#22823;&#27744;&#21270;&#31561;&#20215;&#20110;&#36873;&#25321;&#19968;&#31181;&#26368;&#23567;&#21270;&#22120;&#25490;&#24207;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#19982;&#20854;&#20182;&#26368;&#23567;&#21270;&#22120;&#36317;&#31163;&#36739;&#36817;&#20294;&#19982;&#24207;&#21015;&#20013;&#30340;k-mer&#30456;&#36317;&#36739;&#36828;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#21270;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26159;&#20004;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#27969;&#34892;&#25216;&#26415;&#65292;&#22343;&#34987;&#29992;&#20110;&#20998;&#26512;&#29983;&#29289;&#24207;&#21015;&#12290;&#20174;&#34920;&#38754;&#19978;&#30475;&#65292;&#36825;&#20123;&#26041;&#27861;&#20284;&#20046;&#23436;&#20840;&#19981;&#21516;&#12290;&#26368;&#23567;&#21270;&#22120;&#20351;&#29992;&#28378;&#21160;&#31383;&#21475;&#30340;&#26368;&#23567;&#21704;&#24076;&#26041;&#27861;&#25552;&#21462;&#27599;&#20010;&#31383;&#21475;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;k-mer&#29305;&#24449;&#12290;CNN&#21017;&#20197;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#27744;&#21270;&#25805;&#20316;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#22810;&#20010;&#31070;&#32463;&#23618;&#26469;&#23398;&#20064;&#28388;&#27874;&#22120;&#26412;&#36523;&#21450;&#20854;&#29992;&#20110;&#20998;&#31867;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#21704;&#24076;&#20989;&#25968;&#23646;&#24615;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#26174;&#31034;&#23545;&#20110;&#20998;&#31867;&#23383;&#27597;&#34920;&#19978;&#30340;&#24207;&#21015;&#65292;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#26368;&#22823;&#27744;&#21270;&#31561;&#20215;&#20110;&#36873;&#25321;&#19968;&#20010;&#26368;&#23567;&#21270;&#22120;&#25490;&#24207;&#65292;&#20351;&#24471;&#36873;&#25321;&#30340;k-mer&#19982;&#24207;&#21015;&#20013;&#30340;k-mer&#65288;&#25353;&#27721;&#26126;&#36317;&#31163;&#65289;&#30456;&#36317;&#36739;&#36828;&#65292;&#20294;&#19982;&#20854;&#20182;&#26368;&#23567;&#21270;&#22120;&#30456;&#36317;&#36739;&#36817;&#12290;&#22312;&#23454;&#35777;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimizers and convolutional neural networks (CNNs) are two quite distinct popular techniques that have both been employed to analyze categorical biological sequences. At face value, the methods seem entirely dissimilar. Minimizers use min-wise hashing on a rolling window to extract a single important k-mer feature per window. CNNs start with a wide array of randomly initialized convolutional filters, paired with a pooling operation, and then multiple additional neural layers to learn both the filters themselves and how they can be used to classify the sequence.  Here, our main result is a careful mathematical analysis of hash function properties showing that for sequences over a categorical alphabet, random Gaussian initialization of convolutional filters with max-pooling is equivalent to choosing a minimizer ordering such that selected k-mers are (in Hamming distance) far from the k-mers within the sequence but close to other minimizers. In empirical experiments, we find that this pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;ReLU'(0)&#20540;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#21453;&#21521;&#20256;&#25773;&#30340;&#25968;&#20540;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;32&#20301;&#31934;&#24230;&#19979;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#32780;&#22312;16&#20301;&#31934;&#24230;&#19979;&#26159;&#31995;&#32479;&#24615;&#30340;&#12290;&#22312;&#26222;&#36890;&#30340;SGD&#35757;&#32451;&#20013;&#65292;&#36873;&#25321;ReLU'(0) = 0&#20284;&#20046;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861; tend to buffer ReLU'(0)&#20540;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2106.12915</link><description>&lt;p&gt;
ReLU'(0)&#23545;&#21453;&#21521;&#20256;&#25773;&#30340;&#25968;&#20540;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Numerical influence of ReLU'(0) on backpropagation. (arXiv:2106.12915v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.12915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;ReLU'(0)&#20540;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#21453;&#21521;&#20256;&#25773;&#30340;&#25968;&#20540;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;32&#20301;&#31934;&#24230;&#19979;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#32780;&#22312;16&#20301;&#31934;&#24230;&#19979;&#26159;&#31995;&#32479;&#24615;&#30340;&#12290;&#22312;&#26222;&#36890;&#30340;SGD&#35757;&#32451;&#20013;&#65292;&#36873;&#25321;ReLU'(0) = 0&#20284;&#20046;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861; tend to buffer ReLU'(0)&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35770;&#19978;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013;ReLU'(0)&#22312;[0, 1]&#33539;&#22260;&#20869;&#30340;&#36873;&#25321;&#23545;&#21453;&#21521;&#20256;&#25773;&#21644;&#35757;&#32451;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;32&#20301;&#40664;&#35748;&#31934;&#24230;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#30340;&#35268;&#27169;&#65292;&#20351;&#20854;&#25104;&#20026;&#35757;&#32451;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ReLU'(0)&#20540;&#23545;&#20960;&#31181;&#31934;&#24230;&#27700;&#24179;&#65288;16&#20301;&#65292;32&#20301;&#65292;64&#20301;&#65289;&#12289;&#21508;&#31181;&#32593;&#32476;&#65288;&#20840;&#36830;&#25509;&#12289;VGG&#12289;ResNet&#65289;&#21644;&#25968;&#25454;&#38598;&#65288;MNIST&#12289;CIFAR10&#12289;SVHN&#65289;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;32&#20301;&#31934;&#24230;&#19979;&#65292;&#21453;&#21521;&#20256;&#25773;&#36755;&#20986;&#20986;&#29616;&#20102;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#36825;&#31181;&#24773;&#20917;&#22823;&#32422;&#20986;&#29616;&#20102;&#19968;&#21322;&#30340;&#26102;&#38388;&#12290;&#36825;&#31181;&#24433;&#21709;&#22312;&#21452;&#31934;&#24230;&#19979;&#28040;&#22833;&#65292;&#32780;&#22312;16&#20301;&#31934;&#24230;&#19979;&#26159;&#31995;&#32479;&#24615;&#30340;&#12290;&#23545;&#20110;&#26222;&#36890;&#30340;SGD&#35757;&#32451;&#32780;&#35328;&#65292;&#36873;&#25321;ReLU'(0) = 0&#20284;&#20046;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25209;&#24402;&#19968;&#21270;&#25110;ADAM&#31561;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861; tend to buffer ReLU'(0)&#20540;&#30340;&#24433;&#21709;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24819;&#35201;&#20256;&#36798;&#30340;&#20449;&#24687;&#26159;&#65292;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#31639;&#27861;&#24494;&#20998;&#21487;&#33021;&#38544;&#34255;&#20102;&#19968;&#20123;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In theory, the choice of ReLU'(0) in [0, 1] for a neural network has a negligible influence both on backpropagation and training. Yet, in the real world, 32 bits default precision combined with the size of deep learning problems makes it a hyperparameter of training methods. We investigate the importance of the value of ReLU'(0) for several precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN). We observe considerable variations of backpropagation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU'(0) = 0 seems to be the most efficient. We also evidence that reconditioning approaches as batch-norm or ADAM tend to buffer the influence of ReLU'(0)'s value. Overall, the message we want to convey is that algorithmic differentiation of nonsmooth problems potentially hides parameters that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37325;&#26032;&#32553;&#25918;&#30340;&#29790;&#21033;&#21830;&#20989;&#25968;&#20316;&#20026;&#20934;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#24182;&#37319;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#35745;&#31639;&#31232;&#30095;&#35268;&#33539;&#21521;&#37327;&#30340;&#20272;&#35745;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36830;&#32493;&#21644;&#25130;&#26029;&#25968;&#25454;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2010.08627</link><description>&lt;p&gt;
&#26368;&#23567;&#26368;&#22823;&#25311;&#36125;&#21494;&#26031;&#20272;&#35745;&#22312;&#31232;&#30095;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110;&#29790;&#21033;&#21830;&#20989;&#25968;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Minimax Quasi-Bayesian estimation in sparse canonical correlation analysis via a Rayleigh quotient function. (arXiv:2010.08627v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.08627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37325;&#26032;&#32553;&#25918;&#30340;&#29790;&#21033;&#21830;&#20989;&#25968;&#20316;&#20026;&#20934;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#24182;&#37319;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#35745;&#31639;&#31232;&#30095;&#35268;&#33539;&#21521;&#37327;&#30340;&#20272;&#35745;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36830;&#32493;&#21644;&#25130;&#26029;&#25968;&#25454;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#25968;&#25454;&#38598;&#20043;&#38388;&#20851;&#31995;&#30340;&#27969;&#34892;&#32479;&#35745;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;&#31232;&#30095;&#35268;&#33539;&#21521;&#37327;&#30340;&#20272;&#35745;&#24050;&#25104;&#20026;CCA&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21464;&#20307;&#65292;&#24212;&#29992;&#24191;&#27867;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#31232;&#30095;&#35268;&#33539;&#21521;&#37327;&#30340;&#26368;&#20248;&#20272;&#35745;&#22120;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#36125;&#21494;&#26031;&#20272;&#35745;&#36807;&#31243;&#65292;&#26082;&#36798;&#21040;&#20102;&#26368;&#23567;&#26368;&#22823;&#20272;&#35745;&#36895;&#29575;&#65292;&#21448;&#21487;&#20197;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#36731;&#26494;&#35745;&#31639;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Tan&#31561;&#20154;&#65288;2018&#65289;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#37325;&#26032;&#32553;&#25918;&#30340;&#29790;&#21033;&#21830;&#20989;&#25968;&#20316;&#20026;&#20934;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#19982;Tan&#31561;&#20154;&#65288;2018&#65289;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#23558;&#36825;&#20010;&#20934;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#19982;&#23574;&#23792;-&#24179;&#26495;&#20808;&#39564;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#35268;&#33539;&#21270;&#25512;&#29702;&#21644;&#20419;&#36827;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#36830;&#32493;&#21644;&#25130;&#26029;&#25968;&#25454;&#19978;&#30340;&#32463;&#39564;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#23427;&#20248;&#20110;&#20960;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Canonical correlation analysis (CCA) is a popular statistical technique for exploring relationships between datasets. In recent years, the estimation of sparse canonical vectors has emerged as an important but challenging variant of the CCA problem, with widespread applications. Unfortunately, existing rate-optimal estimators for sparse canonical vectors have high computational cost. We propose a quasi-Bayesian estimation procedure that not only achieves the minimax estimation rate, but also is easy to compute by Markov Chain Monte Carlo (MCMC). The method builds on Tan et al. (2018) and uses a re-scaled Rayleigh quotient function as the quasi-log-likelihood. However, unlike Tan et al. (2018), we adopt a Bayesian framework that combines this quasi-log-likelihood with a spike-and-slab prior to regularize the inference and promote sparsity. We investigate the empirical behavior of the proposed method on both continuous and truncated data, and we demonstrate that it outperforms several st
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#31070;&#32463;&#32447;&#24615;&#21518;&#39564;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#24179;&#31283;&#24773;&#22659;&#19979;&#30340;&#24773;&#22659;&#36172;&#21338;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#30456;&#20851;&#24773;&#22659;&#24182;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2007.04750</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#32972;&#26223;&#19979;&#30340;&#36882;&#24402;&#31070;&#32463;&#32447;&#24615;&#21518;&#39564;&#25277;&#26679;&#24212;&#29992;&#20110;&#24773;&#22659;&#36172;&#21338;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural-Linear Posterior Sampling for Nonstationary Contextual Bandits. (arXiv:2007.04750v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.04750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#31070;&#32463;&#32447;&#24615;&#21518;&#39564;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#24179;&#31283;&#24773;&#22659;&#19979;&#30340;&#24773;&#22659;&#36172;&#21338;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#30456;&#20851;&#24773;&#22659;&#24182;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#24179;&#31283;&#24773;&#22659;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#38656;&#35201;&#22312;&#25506;&#32034;&#21644;&#21033;&#29992;&#20854;&#20808;&#21069;&#32463;&#39564;&#20013;&#23384;&#22312;&#30340;(&#21608;&#26399;&#24615;&#25110;&#32467;&#26500;&#21270;)&#27169;&#24335;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#25163;&#24037;&#35774;&#35745;&#19968;&#20010;&#21512;&#36866;&#30340;&#21382;&#21490;&#24773;&#22659;&#26159;&#23558;&#38750;&#24179;&#31283;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#30340;&#24179;&#31283;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#31934;&#24515;&#35774;&#35745;&#30340;&#21382;&#21490;&#24773;&#22659;&#20063;&#21487;&#33021;&#24341;&#20837;&#34394;&#20551;&#20851;&#31995;&#25110;&#32570;&#20047;&#20851;&#38190;&#20449;&#24687;&#30340;&#26041;&#20415;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20165;&#22522;&#20110;&#26234;&#33021;&#20307;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#21407;&#22987;&#20132;&#20114;&#21382;&#21490;&#26469;&#34920;&#31034;&#30456;&#20851;&#24773;&#22659;&#24182;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#30001;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#30340;&#29305;&#24449;&#19982;&#22522;&#20110;&#21518;&#39564;&#25277;&#26679;&#30340;&#24773;&#22659;&#32447;&#24615;&#36172;&#21338;&#31639;&#27861;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#30340;&#24773;&#22659;&#21644;&#38750;&#24773;&#22659;&#38750;&#24179;&#31283;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#36882;&#24402;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An agent in a nonstationary contextual bandit problem should balance between exploration and the exploitation of (periodic or structured) patterns present in its previous experiences. Handcrafting an appropriate historical context is an attractive alternative to transform a nonstationary problem into a stationary problem that can be solved efficiently. However, even a carefully designed historical context may introduce spurious relationships or lack a convenient representation of crucial information. In order to address these issues, we propose an approach that learns to represent the relevant context for a decision based solely on the raw history of interactions between the agent and the environment. This approach relies on a combination of features extracted by recurrent neural networks with a contextual linear bandit algorithm based on posterior sampling. Our experiments on a diverse selection of contextual and noncontextual nonstationary problems show that our recurrent approach co
&lt;/p&gt;</description></item></channel></rss>