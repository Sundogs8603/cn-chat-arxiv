<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#20998;&#36776;&#29575;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#26694;&#26550;&#65292;&#22312;&#25968;&#25454;&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#22312;&#22810;&#29992;&#25143;&#21644;/&#25110;&#22810;&#20998;&#36776;&#29575;&#26041;&#24335;&#19979;&#20855;&#26377;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#28508;&#21147;&#23454;&#29616;&#35821;&#20041;&#36890;&#20449;&#65292;&#36890;&#36807;&#20445;&#25345;&#29305;&#23450;&#30340;&#35821;&#20041;&#23646;&#24615;&#26469;&#25193;&#23637;&#20854;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.11604</link><description>&lt;p&gt;
&#35821;&#20041;&#22810;&#20998;&#36776;&#29575;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Semantic Multi-Resolution Communications. (arXiv:2308.11604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#20998;&#36776;&#29575;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#26694;&#26550;&#65292;&#22312;&#25968;&#25454;&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#22312;&#22810;&#29992;&#25143;&#21644;/&#25110;&#22810;&#20998;&#36776;&#29575;&#26041;&#24335;&#19979;&#20855;&#26377;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#28508;&#21147;&#23454;&#29616;&#35821;&#20041;&#36890;&#20449;&#65292;&#36890;&#36807;&#20445;&#25345;&#29305;&#23450;&#30340;&#35821;&#20041;&#23646;&#24615;&#26469;&#25193;&#23637;&#20854;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#22312;&#25968;&#25454;&#37325;&#24314;&#26041;&#38754;&#19982;&#20998;&#31163;&#30340;&#28304;-&#20449;&#36947;&#32534;&#30721;&#30456;&#27604;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#36825;&#31181;&#20248;&#21183;&#22312;&#20110;&#22312;&#22788;&#29702;&#26377;&#38480;&#22359;&#38271;&#24230;&#25968;&#25454;&#26102;&#65292;&#20998;&#31163;&#30340;&#28304;-&#20449;&#36947;&#32534;&#30721;&#30340;&#38750;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#20998;&#31163;&#30340;&#28304;-&#20449;&#36947;&#32534;&#30721;&#22312;&#22810;&#29992;&#25143;&#21644;/&#25110;&#22810;&#20998;&#36776;&#29575;&#26041;&#24335;&#19979;&#37325;&#26500;&#25968;&#25454;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#21482;&#35797;&#22270;&#28385;&#36275;&#26368;&#24046;&#20449;&#36947;&#21644;/&#25110;&#26368;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#20998;&#36776;&#29575;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#26694;&#26550;&#65292;&#21463;&#21040;&#22810;&#20219;&#21153;&#23398;&#20064; (MTL) &#30340;&#27010;&#24565;&#21551;&#21457;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#23618;&#36880;&#28176;&#32534;&#30721;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#21644;&#36807;&#21435;&#30340;&#32534;&#30721;&#25968;&#25454;&#23618;&#26469;&#26377;&#25928;&#35299;&#30721;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#22312;&#35821;&#20041;&#36890;&#20449;&#26041;&#38754;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20854;&#30446;&#26631;&#19981;&#20165;&#20165;&#26159;&#25968;&#25454;&#37325;&#24314;&#65292;&#36824;&#21253;&#25324;&#22312;&#25972;&#20010;&#36890;&#20449;&#36807;&#31243;&#20013;&#20445;&#25345;&#29305;&#23450;&#30340;&#35821;&#20041;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning based joint source-channel coding (JSCC) has demonstrated significant advancements in data reconstruction compared to separate source-channel coding (SSCC). This superiority arises from the suboptimality of SSCC when dealing with finite block-length data. Moreover, SSCC falls short in reconstructing data in a multi-user and/or multi-resolution fashion, as it only tries to satisfy the worst channel and/or the highest quality data. To overcome these limitations, we propose a novel deep learning multi-resolution JSCC framework inspired by the concept of multi-task learning (MTL). This proposed framework excels at encoding data for different resolutions through hierarchical layers and effectively decodes it by leveraging both current and past layers of encoded data. Moreover, this framework holds great potential for semantic communication, where the objective extends beyond data reconstruction to preserving specific semantic attributes throughout the communication process. Th
&lt;/p&gt;</description></item><item><title>Tryage&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#20197;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2308.11601</link><description>&lt;p&gt;
Tryage: &#23454;&#26102;&#26234;&#33021;&#36335;&#30001;&#29992;&#25143;&#25552;&#31034;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11601
&lt;/p&gt;
&lt;p&gt;
Tryage&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#20197;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26550;&#26500;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#24341;&#20837;&#23548;&#33268;&#20102;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#21644;&#25968;&#25454;&#39046;&#22495;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#12290;&#22312;Hugging Face&#29983;&#24577;&#31995;&#32479;&#20013;&#26377;&#36229;&#36807;200,000&#20010;&#27169;&#22411;&#65292;&#29992;&#25143;&#22312;&#36873;&#25321;&#21644;&#20248;&#21270;&#27169;&#22411;&#20197;&#36866;&#24212;&#22810;&#26041;&#38754;&#30340;&#24037;&#20316;&#27969;&#31243;&#21644;&#25968;&#25454;&#39046;&#22495;&#30340;&#21516;&#26102;&#65292;&#36824;&#35201;&#35299;&#20915;&#35745;&#31639;&#12289;&#23433;&#20840;&#21644;&#26102;&#25928;&#24615;&#31561;&#38382;&#39064;&#12290;&#36843;&#20999;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#24182;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;Tryage&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36335;&#30001;&#22120;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#21463;&#22823;&#33041;&#20013;&#30340;&#19992;&#33041;&#36335;&#30001;&#22120;&#21551;&#21457;&#65292;Tryage&#37319;&#29992;&#24863;&#30693;&#36335;&#30001;&#22120;&#26469;&#39044;&#27979;&#19979;&#28216;&#27169;&#22411;&#22312;&#25552;&#31034;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20570;&#20986;&#36335;&#30001;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an ob
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#35270;&#35282;&#30340;&#37327;&#21270;&#20248;&#21270;&#26041;&#27861;&#22312;&#20840;&#23616;&#20248;&#21270;&#20013;&#21033;&#29992;&#34203;&#23450;&#35860;&#26041;&#31243;&#25512;&#23548;&#30340;&#38567;&#36947;&#25928;&#24212;&#65292;&#20174;&#32780;&#33021;&#22815;&#36991;&#20813;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.11594</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#35270;&#35282;&#30340;&#37327;&#21270;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantization-based Optimization with Perspective of Quantum Mechanics. (arXiv:2308.11594v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11594
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#35270;&#35282;&#30340;&#37327;&#21270;&#20248;&#21270;&#26041;&#27861;&#22312;&#20840;&#23616;&#20248;&#21270;&#20013;&#21033;&#29992;&#34203;&#23450;&#35860;&#26041;&#31243;&#25512;&#23548;&#30340;&#38567;&#36947;&#25928;&#24212;&#65292;&#20174;&#32780;&#33021;&#22815;&#36991;&#20813;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28909;&#21147;&#23398;&#30340;&#32479;&#35745;&#21644;&#38543;&#26426;&#20998;&#26512;&#19968;&#30452;&#26159;&#38543;&#26426;&#20840;&#23616;&#20248;&#21270;&#30340;&#20027;&#35201;&#20998;&#26512;&#26694;&#26550;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#29992;&#20110;&#20840;&#23616;&#20248;&#21270;&#30340;&#37327;&#23376;&#36864;&#28779;&#25110;&#37327;&#23376;&#38567;&#36947;&#31639;&#27861;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#26694;&#26550;&#26469;&#36827;&#34892;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#34203;&#23450;&#35860;&#26041;&#31243;&#30340;&#37327;&#21270;&#20248;&#21270;&#30340;&#20998;&#26512;&#65292;&#20197;&#25581;&#31034;&#37327;&#23376;&#21147;&#23398;&#20013;&#30340;&#21738;&#20123;&#23646;&#24615;&#20351;&#20840;&#23616;&#20248;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#34203;&#23450;&#35860;&#26041;&#31243;&#25512;&#23548;&#20986;&#30340;&#38567;&#36947;&#25928;&#24212;&#20351;&#24471;&#37327;&#21270;&#20248;&#21270;&#33021;&#22815;&#36867;&#31163;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#35748;&#36825;&#31181;&#38567;&#36947;&#25928;&#24212;&#26159;&#21253;&#21547;&#22312;&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#30340;&#20840;&#23616;&#20248;&#21270;&#20013;&#30340;&#30456;&#21516;&#23646;&#24615;&#12290;&#23545;&#26631;&#20934;&#22810;&#27169;&#24577;&#22522;&#20934;&#20989;&#25968;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical and stochastic analysis based on thermodynamics has been the main analysis framework for stochastic global optimization. Recently, appearing quantum annealing or quantum tunneling algorithm for global optimization, we require a new researching framework for global optimization algorithms. In this paper, we provide the analysis for quantization-based optimization based on the Schr\"odinger equation to reveal what property in quantum mechanics enables global optimization. We present that the tunneling effect derived by the Schr\"odinger equation in quantization-based optimization enables to escape of a local minimum. Additionally, we confirm that this tunneling effect is the same property included in quantum mechanics-based global optimization. Experiments with standard multi-modal benchmark functions represent that the proposed analysis is valid.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#20840;&#22269;&#38738;&#23569;&#24180;&#39118;&#38505;&#34892;&#20026;&#30417;&#27979;&#31995;&#32479;&#65288;YRBSS&#65289;&#35843;&#26597;&#25968;&#25454;&#65292;&#23545;&#19981;&#21516;&#31181;&#26063;&#32676;&#20307;&#20013;&#30340;&#38738;&#23569;&#24180;&#25233;&#37057;&#30151;&#36827;&#34892;&#39044;&#27979;&#12290;&#30740;&#31350;&#21457;&#29616;&#31181;&#26063;&#20122;&#32452;&#20043;&#38388;&#23384;&#22312;&#30528;&#30456;&#20851;&#22240;&#32032;&#30340;&#24046;&#24322;&#65292;&#24182;&#21628;&#21505;&#25552;&#20379;&#26356;&#22810;&#24191;&#27867;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.11591</link><description>&lt;p&gt;
&#31181;&#26063;&#19982;&#38738;&#23569;&#24180;&#25233;&#37057;&#30151;&#30340;&#39044;&#27979;&#65306;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#36328;&#31181;&#26063;&#32676;&#20307;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's Race Got to do with it? Predicting Youth Depression Across Racial Groups Using Machine and Deep Learning. (arXiv:2308.11591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#20840;&#22269;&#38738;&#23569;&#24180;&#39118;&#38505;&#34892;&#20026;&#30417;&#27979;&#31995;&#32479;&#65288;YRBSS&#65289;&#35843;&#26597;&#25968;&#25454;&#65292;&#23545;&#19981;&#21516;&#31181;&#26063;&#32676;&#20307;&#20013;&#30340;&#38738;&#23569;&#24180;&#25233;&#37057;&#30151;&#36827;&#34892;&#39044;&#27979;&#12290;&#30740;&#31350;&#21457;&#29616;&#31181;&#26063;&#20122;&#32452;&#20043;&#38388;&#23384;&#22312;&#30528;&#30456;&#20851;&#22240;&#32032;&#30340;&#24046;&#24322;&#65292;&#24182;&#21628;&#21505;&#25552;&#20379;&#26356;&#22810;&#24191;&#27867;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#24120;&#35265;&#20294;&#20005;&#37325;&#30340;&#24515;&#29702;&#38556;&#30861;&#65292;&#27599;&#24180;&#24433;&#21709;&#25968;&#30334;&#19975;&#32654;&#22269;&#39640;&#20013;&#29983;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#30340;&#35786;&#26029;&#21644;&#26089;&#26399;&#21457;&#29616;&#20173;&#28982;&#26159;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#65292;&#30740;&#31350;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#20854;&#20182;&#30142;&#30149;&#22914;&#30284;&#30151;&#21644;HIV&#26041;&#38754;&#20135;&#29983;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31867;&#20284;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#23398;&#29983;&#30340;&#25233;&#37057;&#30151;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#24378;&#35843;&#20102;&#31181;&#26063;&#20122;&#32452;&#30340;&#30456;&#20851;&#22240;&#32032;&#24046;&#24322;&#65292;&#24182;&#20027;&#24352;&#38656;&#35201;&#26356;&#24191;&#27867;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#27169;&#22411;&#22312;&#20840;&#22269;&#38738;&#23569;&#24180;&#39118;&#38505;&#34892;&#20026;&#30417;&#27979;&#31995;&#32479;&#65288;YRBSS&#65289;&#35843;&#26597;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#25214;&#21040;&#20102;&#25233;&#37057;&#30151;&#30340;&#26368;&#30456;&#20851;&#22240;&#32032;&#12290;&#35813;&#35843;&#26597;&#25968;&#25454;&#26159;&#19968;&#20010;&#21253;&#21547;15000&#20010;&#26465;&#30446;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19977;&#20010;&#31181;&#26063;&#23376;&#38598;&#65292;&#27599;&#20010;&#23376;&#38598;&#21253;&#21547;900&#20010;&#26465;&#30446;&#12290;&#22312;&#20998;&#31867;&#26041;&#38754;&#65292;&#30740;&#31350;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#26377;&#30417;&#30563;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a common yet serious mental disorder that affects millions of U.S. high schoolers every year. Still, accurate diagnosis and early detection remain significant challenges. In the field of public health, research shows that neural networks produce promising results in identifying other diseases such as cancer and HIV. This study proposes a similar approach, utilizing machine learning (ML) and artificial neural network (ANN) models to classify depression in a student. Additionally, the study highlights the differences in relevant factors for race subgroups and advocates the need for more extensive and diverse datasets. The models train on nationwide Youth Risk Behavior Surveillance System (YRBSS) survey data, in which the most relevant factors of depression are found with statistical analysis. The survey data is a structured dataset with 15000 entries including three race subsets each consisting of 900 entries. For classification, the research problem is modeled as a supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26367;&#20195;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#20998;&#31867;&#65292;&#22312;&#20445;&#25345;&#31867;&#20284;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23545;SVM&#26041;&#27861;&#30340;&#19968;&#20123;&#32570;&#28857;&#36827;&#34892;&#20102;&#25913;&#36827;&#21644;&#25935;&#24863;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.11579</link><description>&lt;p&gt;
SVM&#26041;&#27861;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#29992;&#20110;&#25968;&#25454;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
An alternative to SVM Method for Data Classification. (arXiv:2308.11579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26367;&#20195;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#20998;&#31867;&#65292;&#22312;&#20445;&#25345;&#31867;&#20284;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23545;SVM&#26041;&#27861;&#30340;&#19968;&#20123;&#32570;&#28857;&#36827;&#34892;&#20102;&#25913;&#36827;&#21644;&#25935;&#24863;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#26680;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#25454;&#20998;&#31867;&#65292;&#24182;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#20854;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#26102;&#38388;&#22788;&#29702;&#12289;&#39640;&#32500;&#24773;&#20917;&#19979;&#20248;&#21270;&#36807;&#31243;&#22833;&#36133;&#30340;&#39118;&#38505;&#12289;&#22810;&#31867;&#21035;&#12289;&#19981;&#24179;&#34913;&#31867;&#21035;&#21644;&#21160;&#24577;&#20998;&#31867;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#23545;&#19978;&#36848;&#32570;&#28857;&#36827;&#34892;&#20102;&#25935;&#24863;&#25913;&#36827;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#22522;&#20110;&#21040;&#21253;&#21547;&#26144;&#23556;&#21407;&#22987;&#31867;&#21035;&#30340;&#26368;&#20248;&#23376;&#31354;&#38388;&#30340;&#26368;&#23567;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support vector machine (SVM), is a popular kernel method for data classification that demonstrated its efficiency for a large range of practical applications. The method suffers, however, from some weaknesses including; time processing, risk of failure of the optimization process for high dimension cases, generalization to multi-classes, unbalanced classes, and dynamic classification. In this paper an alternative method is proposed having a similar performance, with a sensitive improvement of the aforementioned shortcomings. The new method is based on a minimum distance to optimal subspaces containing the mapped original classes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#34920;&#29616;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20934;&#30830;&#24230;&#31561;&#12290;LLMs&#30340;&#20986;&#29616;&#20026;&#24773;&#32490;&#35782;&#21035;&#24314;&#27169;&#24102;&#26469;&#20102;&#26032;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2308.11578</link><description>&lt;p&gt;
&#25913;&#21464;&#24773;&#32490;&#35782;&#21035;&#24314;&#27169;&#26041;&#24335;:&#36890;&#29992;&#22823;&#22411;&#27169;&#22411;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models. (arXiv:2308.11578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#34920;&#29616;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20934;&#30830;&#24230;&#31561;&#12290;LLMs&#30340;&#20986;&#29616;&#20026;&#24773;&#32490;&#35782;&#21035;&#24314;&#27169;&#24102;&#26469;&#20102;&#26032;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#32490;&#35782;&#21035;&#25110;&#24773;&#24863;&#35745;&#31639;&#30340;&#35806;&#29983;&#20043;&#21518;&#65292;&#30001;&#20110;&#20854;&#24191;&#27867;&#24212;&#29992;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#27963;&#36291;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#36880;&#28176;&#20174;&#32479;&#35745;&#27973;&#23618;&#27169;&#22411;&#36801;&#31227;&#21040;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#27169;&#22411;&#19968;&#30452;&#34987;&#35270;&#20026;&#24773;&#32490;&#35782;&#21035;&#30340;&#39318;&#36873;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#30001;&#20110;&#23427;&#20204;&#20855;&#22791;&#30340;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#36830;&#36143;&#24605;&#32500;&#31561;&#33021;&#21147;&#65292;&#22312;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#24778;&#35766;&#65292;&#32780;&#36825;&#20123;&#33021;&#21147;&#22312;&#20197;&#21069;&#30340;&#28145;&#24230;&#27169;&#22411;&#20013;&#20174;&#26410;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;LLMs&#22312;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#20934;&#30830;&#24230;&#31561;&#21508;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
After the inception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, acc
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#24352;&#37327;&#31209;&#28436;&#21270;&#26469;&#29702;&#35299;&#31070;&#32463;&#20803;&#36830;&#25509;&#22312;&#23398;&#20064;&#20013;&#30340;&#21327;&#35843;&#21464;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#36807;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#30697;&#38453;&#36890;&#24120;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#65292;&#32780;&#36825;&#31181;&#32467;&#26500;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#12290;&#23545;&#30495;&#23454;&#26435;&#37325;&#36827;&#34892;&#20302;&#31209;&#20998;&#35299;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11567</link><description>&lt;p&gt;
&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#20302;&#38454;&#24352;&#37327;&#31209;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Low Tensor Rank Learning of Neural Dynamics. (arXiv:2308.11567v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11567
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#24352;&#37327;&#31209;&#28436;&#21270;&#26469;&#29702;&#35299;&#31070;&#32463;&#20803;&#36830;&#25509;&#22312;&#23398;&#20064;&#20013;&#30340;&#21327;&#35843;&#21464;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#36807;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#30697;&#38453;&#36890;&#24120;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#65292;&#32780;&#36825;&#31181;&#32467;&#26500;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#12290;&#23545;&#30495;&#23454;&#26435;&#37325;&#36827;&#34892;&#20302;&#31209;&#20998;&#35299;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20381;&#36182;&#20110;&#31070;&#32463;&#20803;&#32676;&#20307;&#20013;&#30340;&#21327;&#35843;&#31361;&#35302;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#23398;&#20064;&#36807;&#31243;&#20013;&#31361;&#35302;&#36830;&#25509;&#30340;&#38598;&#20307;&#28436;&#21270;&#26159;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#26435;&#37325;&#30697;&#38453;&#36890;&#24120;&#26159;&#20302;&#31209;&#30340;&#65292;&#20294;&#26159;&#36825;&#31181;&#20302;&#31209;&#32467;&#26500;&#22914;&#20309;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23637;&#24320;&#36824;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#30001;&#26435;&#37325;&#30697;&#38453;&#24418;&#25104;&#30340;3&#38454;&#24352;&#37327;&#30340;&#31209;&#12290;&#36890;&#36807;&#29992;&#19981;&#21516;&#31209;&#30340;RNN&#25311;&#21512;&#22823;&#35268;&#27169;&#31070;&#32463;&#35760;&#24405;&#30340;&#36816;&#21160;&#23398;&#20064;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#29616;&#25512;&#26029;&#30340;&#26435;&#37325;&#26159;&#20302;&#38454;&#24352;&#37327;&#31209;&#30340;&#65292;&#22240;&#27492;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#28436;&#21270;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#30495;&#23454;&#26435;&#37325;&#19978;&#30452;&#25509;&#36827;&#34892;&#20302;&#38454;&#24352;&#37327;&#31209;&#20998;&#35299;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#25152;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20302;&#38454;&#24352;&#37327;&#31209;&#23398;&#20064;&#30340;&#35266;&#23519;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task by performing a low-tensor-rank decomposition directly on the ground truth weights, and by showing that the method we applie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.11551</link><description>&lt;p&gt;
&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-event Video-Text Retrieval. (arXiv:2308.11551v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;VTR&#65289;&#26159;&#20114;&#32852;&#32593;&#19978;&#28023;&#37327;&#35270;&#39057;&#25991;&#26412;&#25968;&#25454;&#26102;&#20195;&#20013;&#19968;&#39033;&#20851;&#38190;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#20351;&#29992;&#21452;&#27969;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#23398;&#20064;&#35270;&#39057;&#25991;&#26412;&#23545;&#30340;&#32852;&#21512;&#34920;&#31034;&#25104;&#20026;VTR&#20219;&#21153;&#20013;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20551;&#35774;&#35270;&#39057;&#25991;&#26412;&#23545;&#24212;&#26159;&#21452;&#23556;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#24182;&#24573;&#35270;&#20102;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#35270;&#39057;&#20869;&#23481;&#36890;&#24120;&#28085;&#30422;&#22810;&#20010;&#20107;&#20214;&#65292;&#32780;&#29992;&#25143;&#26597;&#35810;&#25110;&#32593;&#39029;&#20803;&#25968;&#25454;&#31561;&#25991;&#26412;&#24448;&#24448;&#26159;&#20855;&#20307;&#30340;&#65292;&#24182;&#23545;&#24212;&#21333;&#20010;&#20107;&#20214;&#12290;&#36825;&#36896;&#25104;&#20102;&#20043;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#19982;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#33021;&#23548;&#33268;&#26089;&#26399;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#38024;&#23545;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#22330;&#26223;&#65292;&#20316;&#20026;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#30340;&#19968;&#20010;&#21033;&#22522;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20811;&#26381;&#23616;&#37096;&#26368;&#23567;&#20540;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22238;&#24402;MLP&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2308.11532</link><description>&lt;p&gt;
&#19968;&#31181;&#20811;&#26381;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#31639;&#27861;&#29992;&#20110;&#35757;&#32451;&#22238;&#24402;MLP&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A free from local minima algorithm for training regressive MLP neural networks. (arXiv:2308.11532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20811;&#26381;&#23616;&#37096;&#26368;&#23567;&#20540;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22238;&#24402;MLP&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#22238;&#24402;MLP&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#19981;&#26131;&#21463;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#24433;&#21709;&#12290;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#26159;&#35813;&#31639;&#27861;&#23384;&#22312;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#36991;&#20813;&#20102;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article an innovative method for training regressive MLP networks is presented, which is not subject to local minima. The Error-Back-Propagation algorithm, proposed by William-Hinton-Rummelhart, has had the merit of favouring the development of machine learning techniques, which has permeated every branch of research and technology since the mid-1980s. This extraordinary success is largely due to the black-box approach, but this same factor was also seen as a limitation, as soon more challenging problems were approached. One of the most critical aspects of the training algorithms was that of local minima of the loss function, typically the mean squared error of the output on the training set. In fact, as the most popular training algorithms are driven by the derivatives of the loss function, there is no possibility to evaluate if a reached minimum is local or global. The algorithm presented in this paper avoids the problem of local minima, as the training is based on the proper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#20803;&#32990;&#33258;&#21160;&#26426;&#35774;&#35745;&#31639;&#27861;&#30340;&#20648;&#22791;&#35745;&#31639;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#20803;&#32990;&#33258;&#21160;&#26426;&#35268;&#21017;&#36873;&#25321;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.11522</link><description>&lt;p&gt;
ReLiCADA -- &#20351;&#29992;&#32447;&#24615;&#20803;&#32990;&#33258;&#21160;&#26426;&#35774;&#35745;&#31639;&#27861;&#30340;&#20648;&#22791;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
ReLiCADA -- Reservoir Computing using Linear Cellular Automata Design Algorithm. (arXiv:2308.11522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#20803;&#32990;&#33258;&#21160;&#26426;&#35774;&#35745;&#31639;&#27861;&#30340;&#20648;&#22791;&#35745;&#31639;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#20803;&#32990;&#33258;&#21160;&#26426;&#35268;&#21017;&#36873;&#25321;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#30340;&#20648;&#22791;&#35745;&#31639;&#30340;&#20803;&#32990;&#33258;&#21160;&#26426;&#27169;&#22411;&#30340;&#35774;&#35745;&#12290;&#38500;&#20102;&#36873;&#25321;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#22806;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#29305;&#21035;&#35299;&#20915;&#20102;&#32447;&#24615;&#20803;&#32990;&#33258;&#21160;&#26426;&#35268;&#21017;&#36873;&#25321;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#36873;&#25321;&#26041;&#27861;&#20174;&#25351;&#25968;&#22686;&#38271;&#30340;&#35268;&#21017;&#31354;&#38388;&#20013;&#39044;&#20808;&#36873;&#25321;&#20165;&#26377;&#20960;&#20010;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#35268;&#21017;&#12290;&#24403;&#24212;&#29992;&#20110;&#30456;&#20851;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26102;&#65292;&#25152;&#36873;&#25321;&#30340;&#35268;&#21017;&#36798;&#21040;&#20102;&#36739;&#20302;&#30340;&#35823;&#24046;&#65292;&#20854;&#20013;&#26368;&#20339;&#35268;&#21017;&#20301;&#20110;&#25972;&#20307;&#35268;&#21017;&#31354;&#38388;&#30340;&#21069;5%&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#23545;&#32447;&#24615;&#20803;&#32990;&#33258;&#21160;&#26426;&#23646;&#24615;&#30340;&#25968;&#23398;&#20998;&#26512;&#24320;&#21457;&#65292;&#24182;&#36890;&#36807;&#36817;&#19968;&#30334;&#19975;&#27425;&#35797;&#39564;&#25903;&#25345;&#65292;&#24635;&#35745;&#35745;&#31639;&#26102;&#38388;&#23558;&#36817;&#19968;&#24180;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#25152;&#25552;&#20986;&#30340;&#20351;&#29992;&#20803;&#32990;&#33258;&#21160;&#26426;&#27169;&#22411;&#30340;&#20648;&#22791;&#35745;&#31639;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel algorithm to optimize the design of Reservoir Computing using Cellular Automata models for time series applications. Besides selecting the models' hyperparameters, the proposed algorithm particularly solves the open problem of linear Cellular Automaton rule selection. The selection method pre-selects only a few promising candidate rules out of an exponentially growing rule space. When applied to relevant benchmark datasets, the selected rules achieve low errors, with the best rules being among the top 5% of the overall rule space. The algorithm was developed based on mathematical analysis of linear Cellular Automaton properties and is backed by almost one million experiments, adding up to a computational runtime of nearly one year. Comparisons to other state-of-the-art time series models show that the proposed Reservoir Computing using Cellular Automata models have lower computational complexity, at the same time, achieve lower errors. Hence, our appro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#21644;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11521</link><description>&lt;p&gt;
&#33258;&#25105;&#27450;&#39575;&#65306;&#36870;&#21521;&#30772;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#38450;&#28779;&#22681;
&lt;/p&gt;
&lt;p&gt;
Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11521
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#21644;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#65292;&#20855;&#26377;&#25509;&#36817;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#34429;&#28982;&#20026;&#21508;&#31181;&#31038;&#20250;&#38656;&#27714;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;LLM&#20063;&#38477;&#20302;&#20102;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;LLM&#24320;&#21457;&#20154;&#21592;&#24050;&#32463;&#37096;&#32626;&#20102;&#35821;&#20041;&#32423;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25298;&#32477;&#21487;&#33021;&#23548;&#33268;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#25552;&#31034;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#38450;&#24481;&#26426;&#21046;&#24182;&#19981;&#23436;&#20840;&#21487;&#38752;&#65292;&#19968;&#20123;&#25915;&#20987;&#32773;&#24050;&#32463;&#35774;&#35745;&#20986;&#20102;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#65292;&#20020;&#26102;&#20351;LLM&#24536;&#35760;&#20869;&#23481;&#38450;&#24481;&#35268;&#21017;&#24182;&#22238;&#31572;&#20219;&#20309;&#19981;&#36866;&#24403;&#30340;&#38382;&#39064;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#23578;&#26080;&#20851;&#20110;&#36825;&#20123;&#35821;&#20041;&#32423;&#25915;&#20987;&#21644;&#38450;&#24481;&#21407;&#21017;&#30340;&#26126;&#30830;&#35299;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.  This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates trad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#21033;&#29992;&#38598;&#32676;&#32467;&#26500;&#25913;&#36827;&#23398;&#20064;&#26041;&#26696;&#30340;&#38382;&#39064;&#65292;&#38024;&#23545;&#24102;&#26377;&#38598;&#32676;&#25968;&#25454;&#30340;&#32447;&#24615;&#22238;&#24402;&#28151;&#21512;&#27169;&#22411;&#65292;&#24212;&#29992;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.11518</link><description>&lt;p&gt;
EM&#31639;&#27861;&#22312;&#24102;&#26377;&#38598;&#32676;&#25968;&#25454;&#30340;&#32447;&#24615;&#22238;&#24402;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EM for Mixture of Linear Regression with Clustered Data. (arXiv:2308.11518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#21033;&#29992;&#38598;&#32676;&#32467;&#26500;&#25913;&#36827;&#23398;&#20064;&#26041;&#26696;&#30340;&#38382;&#39064;&#65292;&#38024;&#23545;&#24102;&#26377;&#38598;&#32676;&#25968;&#25454;&#30340;&#32447;&#24615;&#22238;&#24402;&#28151;&#21512;&#27169;&#22411;&#65292;&#24212;&#29992;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#22788;&#29702;&#30001;&#20998;&#24067;&#22312;&#24322;&#36136;&#29615;&#22659;&#20013;&#30340;&#23458;&#25143;&#31471;&#29983;&#25104;&#30340;&#21508;&#31181;&#22823;&#35268;&#27169;&#25968;&#25454;&#12290;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#26159;&#25193;&#22823;&#35768;&#22810;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#30340;&#19968;&#20010;&#20027;&#35201;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24322;&#26500;&#25968;&#25454;&#21487;&#33021;&#20197;&#20855;&#26377;&#20849;&#20139;&#32467;&#26500;&#30340;&#38598;&#32676;&#24418;&#24335;&#29983;&#25104;&#65292;&#20363;&#22914;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#25511;&#21046;&#30528;&#23458;&#25143;&#31471;&#29983;&#25104;&#30340;&#25152;&#26377;&#26679;&#26412;&#30340;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#20250;&#38382;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#22914;&#20309;&#21033;&#29992;&#28508;&#22312;&#30340;&#38598;&#32676;&#32467;&#26500;&#26469;&#25913;&#36827;&#23398;&#20064;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#20272;&#35745;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#20998;&#37327;&#30340;&#32447;&#24615;&#22238;&#24402;&#28151;&#21512;&#27169;&#22411;&#38382;&#39064;&#30340;d&#32500;&#21442;&#25968;&#30340;&#29305;&#20363;&#20026;&#20363;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#29983;&#25104;&#20855;&#26377;&#20849;&#20139;&#28508;&#21464;&#37327;&#30340;n&#20010;&#26679;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26041;&#27861;&#26469;&#20174;m&#20010;&#33410;&#28857;&#20013;&#20272;&#35745;&#26368;&#22823;&#20284;&#28982;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern data-driven and distributed learning frameworks deal with diverse massive data generated by clients spread across heterogeneous environments. Indeed, data heterogeneity is a major bottleneck in scaling up many distributed learning paradigms. In many settings however, heterogeneous data may be generated in clusters with shared structures, as is the case in several applications such as federated learning where a common latent variable governs the distribution of all the samples generated by a client. It is therefore natural to ask how the underlying clustered structures in distributed data can be exploited to improve learning schemes. In this paper, we tackle this question in the special case of estimating $d$-dimensional parameters of a two-component mixture of linear regressions problem where each of $m$ nodes generates $n$ samples with a shared latent variable. We employ the well-known Expectation-Maximization (EM) method to estimate the maximum likelihood parameters from $m$ b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26631;&#20934;&#21270;&#27969;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#24322;&#26500;&#20449;&#24687;&#65292;&#24182;&#35299;&#20915;&#20102;&#25104;&#26412;&#36129;&#29486;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#29420;&#31435;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11513</link><description>&lt;p&gt;
TrackFlow: &#24102;&#26377;&#26631;&#20934;&#21270;&#27969;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
TrackFlow: Multi-Object Tracking with Normalizing Flows. (arXiv:2308.11513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26631;&#20934;&#21270;&#27969;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#24322;&#26500;&#20449;&#24687;&#65292;&#24182;&#35299;&#20915;&#20102;&#25104;&#26412;&#36129;&#29486;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#29420;&#31435;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#36319;&#36394;-&#26816;&#27979;&#26041;&#27861;&#30340;&#31616;&#27905;&#24615;&#21644;&#24378;&#22823;&#20808;&#39564;&#26465;&#20214;&#20351;&#20854;&#25670;&#33073;&#20102;&#36319;&#36394;-&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;&#22797;&#26434;&#35774;&#35745;&#21644;&#40635;&#28902;&#65292;&#22810;&#30446;&#26631;&#36319;&#36394;&#39046;&#22495;&#23545;&#36319;&#36394;-&#26816;&#27979;&#26041;&#27861;&#37325;&#26032;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#23558;&#36319;&#36394;-&#26816;&#27979;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#35774;&#32622;&#65292;&#20854;&#20013;&#38656;&#35201;&#20174;&#24322;&#26500;&#20449;&#24687;&#65288;&#20363;&#22914;2D&#36816;&#21160;&#32447;&#32034;&#12289;&#35270;&#35273;&#22806;&#35266;&#21644;&#23039;&#24577;&#20272;&#35745;&#65289;&#35745;&#31639;&#32508;&#21512;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#34701;&#21512;&#20855;&#26377;&#31895;&#30053;&#20272;&#35745;&#30340;&#19977;&#32500;&#20449;&#24687;&#21644;&#20854;&#20182;&#20256;&#32479;&#24230;&#37327;&#65288;&#20363;&#22914;IoU&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#37319;&#29992;&#31616;&#21333;&#35268;&#21017;&#25110;&#22797;&#26434;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#24179;&#34913;&#27599;&#20010;&#25104;&#26412;&#30340;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#22312;&#19968;&#20010;&#20445;&#30041;&#38598;&#19978;&#23545;&#23450;&#21046;&#36229;&#21442;&#25968;&#36827;&#34892;&#20180;&#32454;&#35843;&#25972;&#65292;&#24182;&#19988;&#26263;&#31034;&#36825;&#20123;&#25104;&#26412;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#20013;&#24182;&#19981;&#25104;&#31435;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#20248;&#38597;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of multi-object tracking has recently seen a renewed interest in the good old schema of tracking-by-detection, as its simplicity and strong priors spare it from the complex design and painful babysitting of tracking-by-attention approaches. In view of this, we aim at extending tracking-by-detection to multi-modal settings, where a comprehensive cost has to be computed from heterogeneous information e.g., 2D motion cues, visual appearance, and pose estimates. More precisely, we follow a case study where a rough estimate of 3D information is also available and must be merged with other traditional metrics (e.g., the IoU). To achieve that, recent approaches resort to either simple rules or complex heuristics to balance the contribution of each cost. However, i) they require careful tuning of tailored hyperparameters on a hold-out set, and ii) they imply these costs to be independent, which does not hold in reality. We address these issues by building upon an elegant probabilisti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#25490;&#21015;&#23545;&#40784;&#27169;&#22411;&#30340;&#20984;&#32452;&#21512;&#65292;&#24182;&#21457;&#29616;&#24191;&#27867;&#30340;&#36229;&#31435;&#26041;&#20307;&#21306;&#22495;&#24418;&#25104;&#20102;&#20302;&#25439;&#22833;&#20540;&#30340;&#26354;&#38754;&#65292;&#25581;&#31034;&#20102;&#32447;&#24615;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#20102;&#26356;&#19968;&#33324;&#30340;&#27169;&#24335;&#21487;&#32452;&#21512;&#24615;&#29616;&#35937;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20123;&#20851;&#20110;&#32447;&#24615;&#27169;&#24335;&#36830;&#36890;&#24615;&#21644;&#27169;&#22411;&#37325;&#25490;&#22522;&#30340;&#26032;&#35266;&#23519;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#27169;&#22411;&#32452;&#21512;&#20855;&#26377;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#36136;&#65292;&#24182;&#20998;&#26512;&#20102;&#21151;&#33021;&#21644;&#26435;&#37325;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.11511</link><description>&lt;p&gt;
&#27169;&#24335;&#21487;&#32452;&#21512;&#24615;&#65306;&#25506;&#32034;&#25490;&#21015;&#23545;&#40784;&#27169;&#22411;&#30340;&#20984;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Mode Combinability: Exploring Convex Combinations of Permutation Aligned Models. (arXiv:2308.11511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#25490;&#21015;&#23545;&#40784;&#27169;&#22411;&#30340;&#20984;&#32452;&#21512;&#65292;&#24182;&#21457;&#29616;&#24191;&#27867;&#30340;&#36229;&#31435;&#26041;&#20307;&#21306;&#22495;&#24418;&#25104;&#20102;&#20302;&#25439;&#22833;&#20540;&#30340;&#26354;&#38754;&#65292;&#25581;&#31034;&#20102;&#32447;&#24615;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#20102;&#26356;&#19968;&#33324;&#30340;&#27169;&#24335;&#21487;&#32452;&#21512;&#24615;&#29616;&#35937;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20123;&#20851;&#20110;&#32447;&#24615;&#27169;&#24335;&#36830;&#36890;&#24615;&#21644;&#27169;&#22411;&#37325;&#25490;&#22522;&#30340;&#26032;&#35266;&#23519;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#27169;&#22411;&#32452;&#21512;&#20855;&#26377;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#36136;&#65292;&#24182;&#20998;&#26512;&#20102;&#21151;&#33021;&#21644;&#26435;&#37325;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#20010;&#22823;&#23567;&#20026;d&#30340;&#25490;&#21015;&#23545;&#40784;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21521;&#37327;&#920;_A&#21644;&#920;_B&#30340;&#36880;&#20803;&#32032;&#20984;&#32452;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#30001;&#36229;&#31435;&#26041;&#20307;[0,1]^d&#21450;&#20854;&#37051;&#22495;&#30340;&#20803;&#32032;&#21442;&#25968;&#21270;&#30340;&#21508;&#31181;&#27169;&#22411;&#32452;&#21512;&#30340;&#20998;&#24067;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20986;&#65292;&#36229;&#31435;&#26041;&#20307;&#30340;&#24191;&#27867;&#21306;&#22495;&#24418;&#25104;&#20102;&#20302;&#25439;&#22833;&#20540;&#30340;&#26354;&#38754;&#65292;&#36825;&#34920;&#26126;&#32447;&#24615;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#27169;&#24335;&#21487;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#32447;&#24615;&#27169;&#24335;&#36830;&#36890;&#24615;&#21644;&#27169;&#22411;&#37325;&#25490;&#22522;&#36827;&#34892;&#20102;&#20960;&#39033;&#26032;&#39062;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20256;&#36882;&#24615;&#36136;&#65306;&#22522;&#20110;&#19968;&#20010;&#20849;&#21516;&#30340;&#31532;&#19977;&#20010;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#22522;&#20934;&#30340;&#20004;&#20010;&#27169;&#22411;&#20063;&#26159;&#32447;&#24615;&#27169;&#24335;&#36830;&#36890;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#36136;&#65306;&#21363;&#20351;&#31070;&#32463;&#20803;&#21305;&#37197;&#21457;&#29983;&#20102;&#30456;&#24403;&#22823;&#30340;&#25200;&#21160;&#65292;&#25152;&#24471;&#21040;&#30340;&#32452;&#21512;&#20173;&#28982;&#24418;&#25104;&#19968;&#20010;&#24037;&#20316;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#27169;&#22411;&#32452;&#21512;&#30340;&#21151;&#33021;&#21644;&#26435;&#37325;&#30456;&#20284;&#24615;&#65292;&#24182;&#34920;&#26126;&#27492;&#31867;&#32452;&#21512;&#26159;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore element-wise convex combinations of two permutation-aligned neural network parameter vectors $\Theta_A$ and $\Theta_B$ of size $d$. We conduct extensive experiments by examining various distributions of such model combinations parametrized by elements of the hypercube $[0,1]^{d}$ and its vicinity. Our findings reveal that broad regions of the hypercube form surfaces of low loss values, indicating that the notion of linear mode connectivity extends to a more general phenomenon which we call mode combinability. We also make several novel observations regarding linear mode connectivity and model re-basin. We demonstrate a transitivity property: two models re-based to a common third model are also linear mode connected, and a robustness property: even with significant perturbations of the neuron matchings the resulting combinations continue to form a working model. Moreover, we analyze the functional and weight similarity of model combinations and show that such combinations are
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#23398;&#20064;&#33021;&#21542;&#25429;&#25417;&#25991;&#20307;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#34920;&#24449;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.11490</link><description>&lt;p&gt;
&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#23398;&#20064;&#33021;&#22815;&#25429;&#25417;&#25991;&#20307;&#29305;&#24449;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Authorship Representation Learning Capture Stylistic Features?. (arXiv:2308.11490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#23398;&#20064;&#33021;&#21542;&#25429;&#25417;&#25991;&#20307;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#34920;&#24449;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#65292;&#33258;&#21160;&#23558;&#20316;&#32773;&#30340;&#39118;&#26684;&#20174;&#20854;&#20889;&#20316;&#20869;&#23481;&#20013;&#20998;&#31163;&#20986;&#26469;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#19988;&#21487;&#33021;&#19981;&#21487;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#26377;&#22823;&#37327;&#24102;&#26377;&#20316;&#32773;&#26631;&#31614;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#21487;&#29992;&#65292;&#20351;&#24471;&#20197;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23398;&#20064;&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#25104;&#20026;&#21487;&#33021;&#65292;&#29992;&#20110;&#20316;&#32773;&#24402;&#23646;&#36825;&#19968;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26174;&#28982;&#26356;&#22810;&#22320;&#20381;&#36182;&#20110;&#32534;&#30721;&#20889;&#20316;&#39118;&#26684;&#32780;&#19981;&#26159;&#32534;&#30721;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#19968;&#26367;&#20195;&#20219;&#21153;&#30340;&#25104;&#21151;&#24182;&#19981;&#33021;&#30830;&#20445;&#36825;&#20123;&#34920;&#24449;&#33021;&#22815;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#65292;&#22240;&#20026;&#20316;&#32773;&#36523;&#20221;&#20063;&#21487;&#33021;&#19982;&#20854;&#20182;&#28508;&#22312;&#21464;&#37327;&#65288;&#22914;&#20027;&#39064;&#65289;&#30456;&#20851;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#34920;&#24449;&#25152;&#20256;&#36882;&#30340;&#20449;&#24687;&#30340;&#26412;&#36136;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#39564;&#35777;&#20854;&#20027;&#35201;&#32534;&#30721;&#30340;&#26159;&#20889;&#20316;&#39118;&#26684;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#26377;&#38024;&#23545;&#24615;&#30340;&#23454;&#39564;&#31995;&#32479;&#22320;&#26816;&#26597;&#20102;&#36825;&#20123;&#34920;&#24449;&#12290;&#36825;&#20123;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20316;&#32773;&#34920;&#31034;&#23398;&#20064;&#30340;&#34920;&#24449;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically disentangling an author's style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics. At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content. However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic. In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments. The results of these experiments suggest that representations learned for the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#36873;&#39064;&#36873;&#39033;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#23545;&#22238;&#31572;&#36873;&#39033;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#36798;&#21040;13%&#33267;75%&#12290;&#36825;&#31181;&#25935;&#24863;&#24615;&#20027;&#35201;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21069;&#20004;&#20010;/&#19977;&#20010;&#36873;&#39033;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#26102;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.11483</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#36873;&#39064;&#36873;&#39033;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions. (arXiv:2308.11483v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#36873;&#39064;&#36873;&#39033;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#23545;&#22238;&#31572;&#36873;&#39033;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#36798;&#21040;13%&#33267;75%&#12290;&#36825;&#31181;&#25935;&#24863;&#24615;&#20027;&#35201;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21069;&#20004;&#20010;/&#19977;&#20010;&#36873;&#39033;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#26102;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#25552;&#31034;&#25991;&#23383;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#23569;&#26679;&#26412;&#23637;&#31034;&#30340;&#39034;&#24207;&#25935;&#24863;&#24615;&#65292;&#32473;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#27491;&#35780;&#20272;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#65292;&#20102;&#35299;&#21644;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#21464;&#24471;&#36843;&#20999;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#22810;&#36873;&#39064;&#20219;&#21153;&#20013;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#36873;&#39033;&#39034;&#24207;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#30740;&#31350;&#65292;&#36825;&#26159;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21644;&#20107;&#23454;&#26816;&#32034;&#33021;&#21147;&#24120;&#29992;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#22312;&#37325;&#26032;&#25490;&#24207;&#22238;&#31572;&#36873;&#39033;&#26102;&#30340;&#34920;&#29616;&#24046;&#36317;&#30340;&#35843;&#26597;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#24046;&#32422;13%&#33267;75%&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#25512;&#27979;&#36825;&#31181;&#25935;&#24863;&#24615;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21069;&#20004;&#20010;/&#19977;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#26102;&#20135;&#29983;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specif
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2308.11480</link><description>&lt;p&gt;
&#23545;&#24191;&#27867;&#30340;&#20998;&#24067;&#22806;&#26816;&#27979;&#30340;&#26399;&#26395;&#65306;&#26399;&#26395;&#20043;&#22806;&#30340;&#26410;&#30693;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection. (arXiv:2308.11480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11480
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#36890;&#24120;&#28041;&#21450;&#24320;&#21457;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24120;&#24120;&#29421;&#31364;&#22320;&#20851;&#27880;&#35757;&#32451;&#38598;&#20013;&#32570;&#22833;&#30340;&#31867;&#21035;&#26679;&#26412;&#65292;&#24573;&#30053;&#20102;&#20854;&#20182;&#31867;&#22411;&#30340;&#21487;&#33021;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#38480;&#21046;&#38477;&#20302;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#31995;&#32479;&#20250;&#36935;&#21040;&#21508;&#31181;&#21508;&#26679;&#30340;&#24322;&#24120;&#36755;&#20837;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23545;&#26368;&#36817;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#27599;&#19968;&#31181;&#20998;&#24067;&#21464;&#21270;&#19978;&#36827;&#34892;&#20102;&#20851;&#38190;&#35780;&#20272;&#12290;&#25105;&#20204;&#20197;BROAD&#65288;Benchmarking Resilience Over Anomaly Diversity&#65289;&#30340;&#21517;&#20041;&#20844;&#24320;&#21457;&#24067;&#25105;&#20204;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#21482;&#33021;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#23427;&#20204;&#29305;&#21035;&#35774;&#35745;&#26469;&#39044;&#26399;&#30340;&#24847;&#22806;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the reliability of deployed machine learning systems often involves developing methods to detect out-of-distribution (OOD) inputs. However, existing research often narrowly focuses on samples from classes that are absent from the training set, neglecting other types of plausible distribution shifts. This limitation reduces the applicability of these methods in real-world scenarios, where systems encounter a wide variety of anomalous inputs. In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#20998;&#31867;&#26641;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20943;&#23569;&#23376;&#38382;&#39064;&#25968;&#37327;&#12289;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32422;&#26463;&#20316;&#20026;&#21106;&#24179;&#38754;&#20197;&#21450;&#29983;&#25104;&#36829;&#21453;&#32422;&#26463;&#30340;&#25968;&#25454;&#28857;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#24182;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.11477</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#20998;&#31867;&#26641;
&lt;/p&gt;
&lt;p&gt;
Revisiting column-generation-based matheuristic for learning classification trees. (arXiv:2308.11477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#20998;&#31867;&#26641;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20943;&#23569;&#23376;&#38382;&#39064;&#25968;&#37327;&#12289;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32422;&#26463;&#20316;&#20026;&#21106;&#24179;&#38754;&#20197;&#21450;&#29983;&#25104;&#36829;&#21453;&#32422;&#26463;&#30340;&#25968;&#25454;&#28857;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#24182;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#35299;&#20915;&#20998;&#31867;&#38382;&#39064;&#30340;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#20915;&#31574;&#26641;&#24555;&#36895;&#20294;&#29983;&#25104;&#30340;&#26641;&#22312;&#20934;&#30830;&#24615;&#19978;&#19981;&#22815;&#20248;&#21270;&#12290;&#25991;&#29486;&#20013;&#20854;&#20182;&#31163;&#25955;&#20248;&#21270;&#27169;&#22411;&#35299;&#20915;&#20102;&#26368;&#20248;&#24615;&#38382;&#39064;&#20294;&#21482;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;firat2020column&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#65292;&#24182;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#23545;&#35813;&#21015;&#29983;&#25104;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#23376;&#38382;&#39064;&#27169;&#22411;&#20197;&#26174;&#33879;&#20943;&#23569;&#22810;&#31867;&#20998;&#31867;&#23454;&#20363;&#20013;&#30340;&#23376;&#38382;&#39064;&#25968;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20027;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#20381;&#36182;&#32422;&#26463;&#26159;&#34164;&#21547;&#30340;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21106;&#24179;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20998;&#31163;&#27169;&#22411;&#26469;&#29983;&#25104;&#32447;&#24615;&#35268;&#21010;&#26494;&#24347;&#35299;&#36829;&#21453;&#20854;&#23545;&#24212;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are highly interpretable models for solving classification problems in machine learning (ML). The standard ML algorithms for training decision trees are fast but generate suboptimal trees in terms of accuracy. Other discrete optimization models in the literature address the optimality problem but only work well on relatively small datasets. \cite{firat2020column} proposed a column-generation-based heuristic approach for learning decision trees. This approach improves scalability and can work with large datasets. In this paper, we describe improvements to this column generation approach. First, we modify the subproblem model to significantly reduce the number of subproblems in multiclass classification instances. Next, we show that the data-dependent constraints in the master problem are implied, and use them as cutting planes. Furthermore, we describe a separation model to generate data points for which the linear programming relaxation solution violates their correspond
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#26799;&#24230;&#65292;&#22686;&#24378;&#20102;&#28145;&#23618;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#22312;&#22788;&#29702;&#31995;&#32479;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11464</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21516;&#36136;&#24615;&#21040;&#24322;&#36136;&#24615;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning. (arXiv:2308.11464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11464
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#26799;&#24230;&#65292;&#22686;&#24378;&#20102;&#28145;&#23618;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#22312;&#22788;&#29702;&#31995;&#32479;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#31995;&#32479;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#22686;&#24378;&#22823;&#22810;&#25968;&#27169;&#22411;&#21516;&#36136;&#24615;FL&#26041;&#27861;&#22788;&#29702;&#31995;&#32479;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#25193;&#23637;&#23427;&#20204;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20174;&#35814;&#32454;&#25506;&#32034;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;FL&#35774;&#32622;&#24320;&#22987;&#65292;&#21457;&#29616;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#65306;&#65288;1&#65289;&#23458;&#25143;&#31471;&#24615;&#33021;&#19982;&#23618;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21576;&#27491;&#30456;&#20851;&#65292;&#65288;2&#65289;&#27973;&#23618;&#27604;&#28145;&#23618;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#65292;&#65288;3&#65289;&#36739;&#20026;&#24179;&#28369;&#30340;&#26799;&#24230;&#20998;&#24067;&#25351;&#31034;&#20102;&#26356;&#39640;&#30340;&#23618;&#30456;&#20284;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InCo Aggregation&#26041;&#27861;&#65292;&#21033;&#29992;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#65292;&#21363;&#26381;&#21153;&#22120;&#27169;&#22411;&#20013;&#26469;&#33258;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#26799;&#24230;&#28151;&#21512;&#65292;&#20197;&#22686;&#24378;&#28145;&#23618;&#30340;&#30456;&#20284;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#23458;&#25143;&#31471;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverags internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20026;&#28145;&#20837;&#30740;&#31350;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#36215;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.11455</link><description>&lt;p&gt;
&#12298;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#32508;&#36848;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Representation Learning. (arXiv:2308.11455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20026;&#28145;&#20837;&#30740;&#31350;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#26159;&#35768;&#22810;&#20219;&#21153;&#30340;&#26680;&#24515;&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#35768;&#22810;&#20801;&#35768;&#26080;&#30417;&#30563;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#24212;&#29992;&#20110;&#20998;&#31867;&#25110;&#29289;&#20307;&#26816;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#36825;&#20123;&#34920;&#31034;&#30340;&#36136;&#37327;&#25509;&#36817;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#65292;&#32780;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#22270;&#20687;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#20197;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#25351;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#36890;&#36807;&#20803;&#20998;&#26512;&#24635;&#32467;&#20102;&#25991;&#29486;&#20013;&#26368;&#26032;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#26088;&#22312;&#20026;&#24076;&#26395;&#28145;&#20837;&#30740;&#31350;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#19968;&#20010;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning meaningful representations is at the heart of many tasks in the field of modern machine learning. Recently, a lot of methods were introduced that allow learning of image representations without supervision. These representations can then be used in downstream tasks like classification or object detection. The quality of these representations is close to supervised learning, while no labeled images are needed. This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other. Furthermore, our survey summarizes the most-recent experimental results reported in the literature in form of a meta-study. Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#38646;&#26679;&#26412;&#20998;&#21106;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#34917;&#19969;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#22312;&#19968;&#33324;&#21270;&#21644;&#35782;&#21035;&#26410;&#35265;&#23545;&#35937;&#26041;&#38754;&#27169;&#25311;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11448</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;&#21160;&#37327;&#23545;&#27604;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#35821;&#20041;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding. (arXiv:2308.11448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#38646;&#26679;&#26412;&#20998;&#21106;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#34917;&#19969;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#22312;&#19968;&#33324;&#21270;&#21644;&#35782;&#21035;&#26410;&#35265;&#23545;&#35937;&#26041;&#38754;&#27169;&#25311;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;SSP&#65289;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#22312;&#25512;&#21160;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#24494;&#35843;&#36825;&#20123;&#22823;&#27169;&#22411;&#30340;&#25104;&#26412;&#19981;&#26029;&#22686;&#21152;&#65292;&#36825;&#32473;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#32431;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25216;&#26415;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#28040;&#38500;&#20102;&#24494;&#35843;&#30340;&#38656;&#35201;&#65292;&#20197;&#23454;&#29616;&#22312;&#19968;&#33324;&#21270;&#21644;&#35782;&#21035;&#26410;&#35265;&#23545;&#35937;&#26041;&#38754;&#27169;&#25311;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#34917;&#19969;&#30340;&#38646;&#26679;&#26412;&#20998;&#21106;&#35780;&#20272;&#21327;&#35758;&#12290;&#32473;&#23450;&#30446;&#26631;&#23545;&#35937;&#19978;&#30340;&#19968;&#20010;&#28857;&#20316;&#20026;&#25552;&#31034;&#65292;&#31639;&#27861;&#35745;&#31639;&#36873;&#23450;&#34917;&#19969;&#19982;&#20854;&#20182;&#34917;&#19969;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#22270;&#65292;&#28982;&#21518;&#24212;&#29992;&#31616;&#21333;&#30340;&#38408;&#20540;&#20998;&#21106;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pretraining (SSP) has emerged as a popular technique in machine learning, enabling the extraction of meaningful feature representations without labelled data. In the realm of computer vision, pretrained vision transformers (ViTs) have played a pivotal role in advancing transfer learning. Nonetheless, the escalating cost of finetuning these large models has posed a challenge due to the explosion of model size. This study endeavours to evaluate the effectiveness of pure self-supervised learning (SSL) techniques in computer vision tasks, obviating the need for finetuning, with the intention of emulating human-like capabilities in generalisation and recognition of unseen objects. To this end, we propose an evaluation protocol for zero-shot segmentation based on a prompting patch. Given a point on the target object as a prompt, the algorithm calculates the similarity map between the selected patch and other patches, upon that, a simple thresholding is applied to segment the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#26469;&#25506;&#32034;&#21644;&#20998;&#26512;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#25289;&#33298;&#33945;&#38598;&#21512;&#27169;&#22411;&#65292;&#20174;&#32780;&#36229;&#36234;&#20256;&#32479;&#21333;&#19968;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;"&#25289;&#33298;&#33945;&#26816;&#27979;"&#31639;&#27861;&#35782;&#21035;&#20986;&#38598;&#21512;&#20013;&#26368;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11446</link><description>&lt;p&gt;
&#25506;&#32034;&#25289;&#33298;&#33945;&#38598;&#21512;&#26377;&#21161;&#20110;&#21307;&#30103;&#25968;&#25454;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploration of Rashomon Set Assists Explanations for Medical Data. (arXiv:2308.11446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#26469;&#25506;&#32034;&#21644;&#20998;&#26512;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#25289;&#33298;&#33945;&#38598;&#21512;&#27169;&#22411;&#65292;&#20174;&#32780;&#36229;&#36234;&#20256;&#32479;&#21333;&#19968;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;"&#25289;&#33298;&#33945;&#26816;&#27979;"&#31639;&#27861;&#35782;&#21035;&#20986;&#38598;&#21512;&#20013;&#26368;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#36807;&#31243;&#36890;&#24120;&#20197;&#36873;&#25321;&#26368;&#22823;&#21270;&#26576;&#20010;&#24615;&#33021;&#25351;&#26631;&#30340;&#21333;&#19968;&#27169;&#22411;&#20316;&#20026;&#26368;&#32456;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#23545;&#31245;&#24494;&#24046;&#19968;&#20123;&#30340;&#27169;&#22411;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#34987;&#24573;&#35270;&#12290;&#23588;&#20854;&#22312;&#21307;&#30103;&#21644;&#20581;&#24247;&#30740;&#31350;&#20013;&#65292;&#30446;&#26631;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#65292;&#36824;&#21253;&#25324;&#20135;&#29983;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#65292;&#20165;&#20165;&#20381;&#36182;&#24615;&#33021;&#25351;&#26631;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#25110;&#19981;&#23436;&#25972;&#30340;&#32467;&#35770;&#12290;&#24403;&#22788;&#29702;&#19968;&#32452;&#24615;&#33021;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#38598;&#21512;&#26102;&#65292;&#21363;&#25152;&#35859;&#30340;"&#25289;&#33298;&#33945;&#38598;&#21512;"&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#12290;&#36825;&#26679;&#30340;&#38598;&#21512;&#21487;&#33021;&#21253;&#21547;&#25551;&#36848;&#25968;&#25454;&#30340;&#19981;&#21516;&#26041;&#24335;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#26469;&#25506;&#32034;&#25289;&#33298;&#33945;&#38598;&#21512;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#24314;&#27169;&#26041;&#27861;&#12290;&#26680;&#24515;&#26159;&#36890;&#36807;&#24341;&#20837;&#30340;"&#25289;&#33298;&#33945;&#26816;&#27979;"&#31639;&#27861;&#26469;&#35782;&#21035;&#25289;&#33298;&#33945;&#38598;&#21512;&#20013;&#26368;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric. However, this approach leads to abandoning a more profound analysis of slightly inferior models. Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions. This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis. This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach. The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\texttt{Rashomon_DETECT}$ algorit
&lt;/p&gt;</description></item><item><title>TurboViT&#26159;&#36890;&#36807;&#29983;&#25104;&#24335;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#30340;&#24555;&#36895;&#35270;&#35273;&#21464;&#21387;&#22120;&#26550;&#26500;&#35774;&#35745;&#65292;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.11421</link><description>&lt;p&gt;
TurboViT: &#36890;&#36807;&#29983;&#25104;&#24335;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#24555;&#36895;&#35270;&#35273;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
TurboViT: Generating Fast Vision Transformers via Generative Architecture Search. (arXiv:2308.11421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11421
&lt;/p&gt;
&lt;p&gt;
TurboViT&#26159;&#36890;&#36807;&#29983;&#25104;&#24335;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#30340;&#24555;&#36895;&#35270;&#35273;&#21464;&#21387;&#22120;&#26550;&#26500;&#35774;&#35745;&#65292;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;&#21464;&#21387;&#22120;&#22312;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32593;&#32476;&#26550;&#26500;&#30340;&#32467;&#26500;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#20351;&#24471;&#23427;&#20204;&#22312;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#20869;&#23384;&#35201;&#27714;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#38590;&#20197;&#37096;&#32626;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#22312;&#26377;&#25928;&#35270;&#35273;&#21464;&#21387;&#22120;&#26550;&#26500;&#35774;&#35745;&#26041;&#38754;&#36827;&#34892;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#24335;&#26550;&#26500;&#25628;&#32034;&#65288;GAS&#65289;&#25506;&#32034;&#20102;&#24555;&#36895;&#35270;&#35273;&#21464;&#21387;&#22120;&#26550;&#26500;&#35774;&#35745;&#30340;&#29983;&#25104;&#65292;&#20197;&#23454;&#29616;&#22312;&#20934;&#30830;&#24615;&#21644;&#26550;&#26500;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;&#36890;&#36807;&#36825;&#20010;&#29983;&#25104;&#24335;&#26550;&#26500;&#25628;&#32034;&#36807;&#31243;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102; TurboViT&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;&#21333;&#20803;&#27880;&#24847;&#21147;&#21644; Q-pooling &#35774;&#35745;&#27169;&#24335;&#30340;&#39640;&#25928;&#20998;&#23618;&#35270;&#35273;&#21464;&#21387;&#22120;&#26550;&#26500;&#35774;&#35745;&#12290;&#35813;&#32467;&#26524;&#34920;&#26126;&#65292;TurboViT &#26550;&#26500;&#35774;&#35745;&#30340;&#26550;&#26500;&#35745;&#31639;&#22797;&#26434;&#24615;&#26174;&#33879;&#38477;&#20302;&#65288;&gt;2.47&#65289;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have shown unprecedented levels of performance in tackling various visual perception tasks in recent years. However, the architectural and computational complexity of such network architectures have made them challenging to deploy in real-world applications with high-throughput, low-memory requirements. As such, there has been significant research recently on the design of efficient vision transformer architectures. In this study, we explore the generation of fast vision transformer architecture designs via generative architecture search (GAS) to achieve a strong balance between accuracy and architectural and computational efficiency. Through this generative architecture search process, we create TurboViT, a highly efficient hierarchical vision transformer architecture design that is generated around mask unit attention and Q-pooling design patterns. The resulting TurboViT architecture design achieves significantly lower architectural computational complexity (&gt;2.47
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#19968;&#27454;&#25915;&#38450;&#28216;&#25103;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#24207;&#21015;&#37329;&#34701;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#29616;&#29366;&#21644;&#21160;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27604;&#36187;&#21160;&#24577;&#65292;&#22238;&#31572;&#20102;&#38544;&#34255;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#29992;&#25143;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#38656;&#35201;&#22810;&#38271;&#26102;&#38388;&#25165;&#33021;&#30772;&#35299;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11406</link><description>&lt;p&gt;
&#35774;&#35745;&#19968;&#27454;&#25915;&#38450;&#28216;&#25103;&#65306;&#36890;&#36807;&#31454;&#20105;&#26469;&#22686;&#21152;&#37329;&#34701;&#20132;&#26131;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Designing an attack-defense game: how to increase robustness of financial transaction models via a competition. (arXiv:2308.11406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11406
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#19968;&#27454;&#25915;&#38450;&#28216;&#25103;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#24207;&#21015;&#37329;&#34701;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#29616;&#29366;&#21644;&#21160;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27604;&#36187;&#21160;&#24577;&#65292;&#22238;&#31572;&#20102;&#38544;&#34255;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#29992;&#25143;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#38656;&#35201;&#22810;&#38271;&#26102;&#38388;&#25165;&#33021;&#30772;&#35299;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#37329;&#34701;&#39046;&#22495;&#24694;&#24847;&#25915;&#20987;&#39118;&#38505;&#19981;&#26029;&#21319;&#32423;&#21644;&#30001;&#27492;&#24341;&#21457;&#30340;&#20005;&#37325;&#25439;&#23475;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#31574;&#30053;&#21644;&#40065;&#26834;&#30340;&#38450;&#24481;&#26426;&#21046;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#38134;&#34892;&#26085;&#30410;&#24191;&#27867;&#37319;&#29992;&#26356;&#31934;&#30830;&#20294;&#28508;&#22312;&#33030;&#24369;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#19968;&#23041;&#32961;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#20351;&#29992;&#24207;&#21015;&#37329;&#34701;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#21160;&#24577;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27604;&#36187;&#65292;&#20801;&#35768;&#23545;&#29616;&#20195;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#38382;&#39064;&#36827;&#34892;&#36924;&#30495;&#32780;&#35814;&#32454;&#30340;&#30740;&#31350;&#12290;&#21442;&#19982;&#32773;&#30452;&#25509;&#31454;&#20105;&#65292;&#22240;&#27492;&#21487;&#33021;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#25509;&#36817;&#30495;&#23454;&#26465;&#20214;&#19979;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20998;&#26512;&#27604;&#36187;&#21160;&#24577;&#65292;&#22238;&#31572;&#20102;&#38544;&#34255;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#29992;&#25143;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#38656;&#35201;&#22810;&#38271;&#26102;&#38388;&#25165;&#33021;&#30772;&#35299;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the escalating risks of malicious attacks in the finance sector and the consequential severe damage, a thorough understanding of adversarial strategies and robust defense mechanisms for machine learning models is critical. The threat becomes even more severe with the increased adoption in banks more accurate, but potentially fragile neural networks. We aim to investigate the current state and dynamics of adversarial attacks and defenses for neural network models that use sequential financial data as the input.  To achieve this goal, we have designed a competition that allows realistic and detailed investigation of problems in modern financial transaction data. The participants compete directly against each other, so possible attacks and defenses are examined in close-to-real-life conditions. Our main contributions are the analysis of the competition dynamics that answers the questions on how important it is to conceal a model from malicious users, how long does it take to break i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#19981;&#19982;&#25163;&#24037;&#21046;&#20316;&#25918;&#23556;&#23398;&#20887;&#20313;&#30340;&#28145;&#24230;&#23398;&#20064;&#25918;&#23556;&#23398;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20004;&#32773;&#30340;&#29305;&#24449;&#26469;&#39044;&#27979;&#26089;&#26399;&#33008;&#33146;&#30284;&#26631;&#24535;&#29289;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11389</link><description>&lt;p&gt;
&#38750;&#20887;&#20313;&#30340;&#25163;&#24037;&#21046;&#20316;&#21644;&#28145;&#24230;&#23398;&#20064;&#25918;&#23556;&#23398;&#30340;&#32452;&#21512;&#65306;&#24212;&#29992;&#20110;&#26089;&#26399;&#33008;&#33146;&#30284;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Non-Redundant Combination of Hand-Crafted and Deep Learning Radiomics: Application to the Early Detection of Pancreatic Cancer. (arXiv:2308.11389v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#19981;&#19982;&#25163;&#24037;&#21046;&#20316;&#25918;&#23556;&#23398;&#20887;&#20313;&#30340;&#28145;&#24230;&#23398;&#20064;&#25918;&#23556;&#23398;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20004;&#32773;&#30340;&#29305;&#24449;&#26469;&#39044;&#27979;&#26089;&#26399;&#33008;&#33146;&#30284;&#26631;&#24535;&#29289;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#23398;&#20064;&#19981;&#19982;&#25163;&#24037;&#21046;&#20316;&#25918;&#23556;&#23398;(HCR)&#20887;&#20313;&#30340;&#28145;&#24230;&#23398;&#20064;&#25918;&#23556;&#23398;(DLR)&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;VAE&#25552;&#21462;DLR&#29305;&#24449;&#65292;&#21516;&#26102;&#36890;&#36807;&#26368;&#23567;&#21270;&#23427;&#20204;&#30340;&#20114;&#20449;&#24687;&#26469;&#30830;&#20445;&#19982;HCR&#29305;&#24449;&#30340;&#29420;&#31435;&#24615;&#12290;&#24471;&#21040;&#30340;DLR&#29305;&#24449;&#21487;&#20197;&#19982;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#39044;&#27979;&#30284;&#30151;&#30340;&#26089;&#26399;&#26631;&#24535;&#29289;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#33008;&#33146;&#30284;&#26089;&#26399;&#26631;&#24535;&#29289;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#22823;&#22411;&#29420;&#31435;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#38750;&#20887;&#20313;&#30340;DLR&#21644;HCR&#29305;&#24449;&#30340;&#20215;&#20540;&#65292;&#19982;&#19981;&#35299;&#20915;&#20887;&#20313;&#38382;&#39064;&#25110;&#20165;&#20381;&#36182;&#20110;HCR&#29305;&#24449;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#26354;&#32447;&#19979;&#38754;&#31215;&#26377;&#25152;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of learning Deep Learning Radiomics (DLR) that are not redundant with Hand-Crafted Radiomics (HCR). To do so, we extract DLR features using a VAE while enforcing their independence with HCR features by minimizing their mutual information. The resulting DLR features can be combined with hand-crafted ones and leveraged by a classifier to predict early markers of cancer. We illustrate our method on four early markers of pancreatic cancer and validate it on a large independent test set. Our results highlight the value of combining non-redundant DLR and HCR features, as evidenced by an improvement in the Area Under the Curve compared to baseline methods that do not address redundancy or solely rely on HCR features.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#19982;&#21435;&#38500;&#20559;&#35265;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#35758;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25554;&#20837;&#20559;&#35265;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#26631;&#27880;&#26469;&#35782;&#21035;&#20559;&#35265;&#65292;&#24182;&#21457;&#24067;&#20102;&#36825;&#20123;&#20559;&#35265;&#26631;&#27880;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.11386</link><description>&lt;p&gt;
&#35299;&#20915;&#20559;&#35265;&#30340;&#30446;&#26631;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Targeted Data Augmentation for bias mitigation. (arXiv:2308.11386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#19982;&#21435;&#38500;&#20559;&#35265;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#35758;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25554;&#20837;&#20559;&#35265;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#26631;&#27880;&#26469;&#35782;&#21035;&#20559;&#35265;&#65292;&#24182;&#21457;&#24067;&#20102;&#36825;&#20123;&#20559;&#35265;&#26631;&#27880;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#27491;&#21644;&#36947;&#24503;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#38656;&#35201;&#23545;&#20559;&#35265;&#30340;&#32531;&#35299;&#36827;&#34892;&#35880;&#24910;&#32771;&#34385;&#65292;&#36825;&#20010;&#39046;&#22495;&#32463;&#24120;&#34987;&#24573;&#35270;&#25110;&#24573;&#30053;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#30446;&#26631;&#25968;&#25454;&#22686;&#24378; (TDA) &#30340;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20559;&#35265;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#32463;&#20856;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#19982;&#21435;&#38500;&#20559;&#35265;&#30340;&#32321;&#29712;&#20219;&#21153;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25554;&#20837;&#20559;&#35265;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#26631;&#27880;&#26469;&#35782;&#21035;&#20559;&#35265;&#65306;&#19968;&#20010;&#26159;&#20020;&#24202;&#30382;&#32932;&#30149;&#21464;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#26159;&#30007;&#24615;&#21644;&#22899;&#24615;&#38754;&#23380;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#20559;&#35265;&#26631;&#27880;&#39318;&#27425;&#22312;&#26412;&#30740;&#31350;&#20013;&#21457;&#24067;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#23545;&#20107;&#21518;&#20559;&#35265;&#25554;&#20837;&#36827;&#34892;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#38236;&#26694;&#12289;&#23610;&#23376;&#21644;&#30524;&#38236;&#30456;&#20851;&#30340;&#20559;&#35265;&#23545;&#27169;&#22411;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#38543;&#26426;&#24341;&#20837;&#20559;&#35265;&#65292;&#25105;&#20204;&#20943;&#36731;&#20102;&#36825;&#20123;&#20559;&#35265;&#24182;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of fair and ethical AI systems requires careful consideration of bias mitigation, an area often overlooked or ignored. In this study, we introduce a novel and efficient approach for addressing biases called Targeted Data Augmentation (TDA), which leverages classical data augmentation techniques to tackle the pressing issue of bias in data and models. Unlike the laborious task of removing biases, our method proposes to insert biases instead, resulting in improved performance. To identify biases, we annotated two diverse datasets: a dataset of clinical skin lesions and a dataset of male and female faces. These bias annotations are published for the first time in this study, providing a valuable resource for future research. Through Counterfactual Bias Insertion, we discovered that biases associated with the frame, ruler, and glasses had a significant impact on models. By randomly introducing biases during training, we mitigated these biases and achieved a substantial decr
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#36830;&#32493;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#24230;&#37327;&#32467;&#26524;&#24182;&#36866;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#25110;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.11375</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#23545;&#20110;&#36830;&#32493;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Interpretable Distribution-Invariant Fairness Measures for Continuous Scores. (arXiv:2308.11375v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11375
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36830;&#32493;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#24230;&#37327;&#32467;&#26524;&#24182;&#36866;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#25110;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#24230;&#37327;&#36890;&#24120;&#22312;&#20108;&#20803;&#20915;&#31574;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#35752;&#35770;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#36830;&#32493;&#35780;&#20998;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22522;&#20110;ROC&#30340;&#24230;&#37327;&#26041;&#27861;&#20027;&#35201;&#29992;&#20110;&#27492;&#30446;&#30340;&#12290;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#35780;&#20998;&#30340;&#20998;&#24067;&#65292;&#19981;&#36866;&#29992;&#20110;&#25490;&#21517;&#20219;&#21153;&#65292;&#25110;&#32773;&#23427;&#20204;&#30340;&#25928;&#26524;&#22823;&#23567;&#19981;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#36830;&#32493;&#35780;&#20998;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#26131;&#20110;&#35745;&#31639;&#65292;&#24182;&#36866;&#29992;&#20110;&#37327;&#21270;&#21644;&#35299;&#37322;&#32676;&#20307;&#24046;&#24322;&#30340;&#24378;&#24230;&#65292;&#20197;&#21450;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#25110;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29616;&#26377;&#35780;&#20998;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#30340;&#19981;&#21516;&#26063;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#26126;&#30830;&#65292;&#24182;&#19988;&#21487;&#20197;&#37327;&#21270;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#32780;ROC-based&#19981;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measures of algorithmic fairness are usually discussed in the context of binary decisions. We extend the approach to continuous scores. So far, ROC-based measures have mainly been suggested for this purpose. Other existing methods depend heavily on the distribution of scores, are unsuitable for ranking tasks, or their effect sizes are not interpretable. Here, we propose a distributionally invariant version of fairness measures for continuous scores with a reasonable interpretation based on the Wasserstein distance. Our measures are easily computable and well suited for quantifying and interpreting the strength of group disparities as well as for comparing biases across different models, datasets, or time points. We derive a link between the different families of existing fairness measures for scores and show that the proposed distributionally invariant fairness measures outperform ROC-based fairness measures because they are more explicit and can quantify significant biases that ROC-ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#65292;&#20197;&#22238;&#31572;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#19977;&#20010;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11358</link><description>&lt;p&gt;
&#34892;&#21160;&#20998;&#21106;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Much Temporal Long-Term Context is Needed for Action Segmentation?. (arXiv:2308.11358v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#65292;&#20197;&#22238;&#31572;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#19977;&#20010;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#20013;&#24314;&#27169;&#38271;&#26399;&#19978;&#19979;&#25991;&#23545;&#20110;&#35768;&#22810;&#32454;&#31890;&#24230;&#20219;&#21153;&#21253;&#25324;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#34429;&#28982;transformers&#21487;&#20197;&#23545;&#35270;&#39057;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#65292;&#20294;&#23545;&#20110;&#38271;&#35270;&#39057;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#20851;&#20110;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#30340;&#30740;&#31350;&#32467;&#21512;&#20102;&#20351;&#29992;&#23616;&#37096;&#26102;&#38388;&#31383;&#21475;&#35745;&#31639;&#20986;&#30340;&#33258;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#26080;&#27861;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#24182;&#21033;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#65292;&#35797;&#22270;&#22238;&#31572;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#25165;&#33021;&#36827;&#34892;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#30446;&#21069;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#19977;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;50Salads&#65292;Brea...
&lt;/p&gt;
&lt;p&gt;
Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation. An interesting question that is still open is how much long-term temporal context is needed for optimal performance. While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window. While these approaches show good results, their performance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Brea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;&#25506;&#32034;&#20223;&#23556;Deligne-Lusztig&#21464;&#37327;&#30340;&#20960;&#20309;&#24615;&#36136;&#65292;&#21152;&#36895;&#32431;&#25968;&#23398;&#30740;&#31350;&#65292;&#21457;&#29616;&#26032;&#29468;&#24819;&#21644;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.11355</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#19979;&#30340;&#20223;&#23556;Deligne-Lusztig&#21464;&#37327;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Machine learning assisted exploration for affine Deligne-Lusztig varieties. (arXiv:2308.11355v1 [math.AG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;&#25506;&#32034;&#20223;&#23556;Deligne-Lusztig&#21464;&#37327;&#30340;&#20960;&#20309;&#24615;&#36136;&#65292;&#21152;&#36895;&#32431;&#25968;&#23398;&#30740;&#31350;&#65292;&#21457;&#29616;&#26032;&#29468;&#24819;&#21644;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#21449;&#23398;&#31185;&#30740;&#31350;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;&#26469;&#25506;&#32034;&#20223;&#23556;Deligne-Lusztig&#21464;&#37327;&#65288;ADLV&#65289;&#30340;&#20960;&#20309;&#24615;&#36136;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#30740;&#31350;ADLV&#30340;&#19981;&#31354;&#38598;&#27169;&#24335;&#12289;&#32500;&#24230;&#21644;&#19981;&#21487;&#32422;&#20998;&#37327;&#30340;&#26522;&#20030;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#23637;&#31034;&#20102;&#25968;&#25454;&#29983;&#25104;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#27169;&#24335;&#20998;&#26512;&#21644;&#20154;&#24037;&#39564;&#35777;&#30340;&#36882;&#24402;&#27969;&#31243;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#19982;&#32431;&#25968;&#23398;&#30740;&#31350;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26159;&#32454;&#33268;&#20837;&#24494;&#30340;&#65292;&#24378;&#35843;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#23376;&#38598;&#21644;&#36866;&#24403;&#30340;&#29305;&#24449;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#26694;&#26550;&#26377;&#28508;&#21147;&#21152;&#36895;&#32431;&#25968;&#23398;&#30740;&#31350;&#65292;&#24102;&#26469;&#26032;&#29468;&#24819;&#21644;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#21457;&#29616;&#65292;&#21542;&#21017;&#21487;&#33021;&#38656;&#33457;&#36153;&#30456;&#24403;&#38271;&#26102;&#38388;&#25165;&#33021;&#25581;&#31034;&#12290;&#25105;&#20204;&#37325;&#26032;&#21457;&#29616;&#20102;&#34394;&#25311;&#32500;&#25968;&#20844;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26576;&#20010;&#26032;&#35782;&#21035;&#38382;&#39064;&#30340;&#23436;&#25972;&#25968;&#23398;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel, interdisciplinary study that leverages a Machine Learning (ML) assisted framework to explore the geometry of affine Deligne-Lusztig varieties (ADLV). The primary objective is to investigate the nonemptiness pattern, dimension and enumeration of irreducible components of ADLV. Our proposed framework demonstrates a recursive pipeline of data generation, model training, pattern analysis, and human examination, presenting an intricate interplay between ML and pure mathematical research. Notably, our data-generation process is nuanced, emphasizing the selection of meaningful subsets and appropriate feature sets. We demonstrate that this framework has a potential to accelerate pure mathematical research, leading to the discovery of new conjectures and promising research directions that could otherwise take significant time to uncover. We rediscover the virtual dimension formula and provide a full mathematical proof of a newly identified problem concerning a certa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;Q&#20989;&#25968;&#26694;&#26550;&#30340;&#26032;&#39062;&#25506;&#32034;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#20013;&#22522;&#20110;&#31574;&#30053;&#30340;&#25506;&#32034;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36138;&#23146;&#30340;Q softmax&#26356;&#26032;&#26041;&#26696;&#26469;&#26356;&#26032;Q&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.11348</link><description>&lt;p&gt;
&#35880;&#24910;&#20272;&#35745;&#65292;&#22823;&#32966;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Careful at Estimation and Bold at Exploration. (arXiv:2308.11348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;Q&#20989;&#25968;&#26694;&#26550;&#30340;&#26032;&#39062;&#25506;&#32034;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#20013;&#22522;&#20110;&#31574;&#30053;&#30340;&#25506;&#32034;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36138;&#23146;&#30340;Q softmax&#26356;&#26032;&#26041;&#26696;&#26469;&#26356;&#26032;Q&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26080;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#25506;&#32034;&#31574;&#30053;&#24448;&#24448;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#24471;&#20986;&#19968;&#33324;&#24615;&#32467;&#35770;&#12290;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#24050;&#32463;&#23637;&#31034;&#20102;&#23545;&#30830;&#23450;&#24615;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#36827;&#34892;&#22522;&#20110;&#31574;&#30053;&#30340;&#25506;&#32034;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#22312;&#30830;&#23450;&#24615;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22522;&#20110;&#31574;&#30053;&#30340;&#25506;&#32034;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#26080;&#30446;&#26631;&#30340;&#25506;&#32034;&#21644;&#31574;&#30053;&#21457;&#25955;&#65292;&#24182;&#19988;&#30001;&#20110;&#20272;&#35745;&#19981;&#20934;&#30830;&#65292;&#31574;&#30053;&#26799;&#24230;&#23545;&#25506;&#32034;&#30340;&#24110;&#21161;&#20165;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26377;&#25928;&#12290;&#22522;&#20110;&#21452;Q&#20989;&#25968;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25506;&#32034;&#31574;&#30053;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#19982;&#31574;&#30053;&#26799;&#24230;&#20998;&#31163;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#36138;&#23146;&#30340;Q softmax&#26356;&#26032;&#26041;&#26696;&#26469;&#26356;&#26032;Q&#20540;&#12290;&#26399;&#26395;Q&#20540;&#36890;&#36807;&#22312;&#21160;&#20316;&#19978;&#21152;&#26435;&#27714;&#21644;&#20445;&#23432;Q&#20540;&#24471;&#21040;&#65292;&#26435;&#37325;&#20026;&#30456;&#24212;&#30340;&#36138;&#23146;Q&#20540;&#12290;&#36138;&#23146;Q&#21462;&#20004;&#20010;Q&#20989;&#25968;&#30340;&#26368;&#22823;&#20540;&#65292;&#20445;&#23432;Q&#21462;&#20004;&#20010;Q&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration strategies in continuous action space are often heuristic due to the infinite actions, and these kinds of methods cannot derive a general conclusion. In prior work, it has been shown that policy-based exploration is beneficial for continuous action space in deterministic policy reinforcement learning(DPRL). However, policy-based exploration in DPRL has two prominent issues: aimless exploration and policy divergence, and the policy gradient for exploration is only sometimes helpful due to inaccurate estimation. Based on the double-Q function framework, we introduce a novel exploration strategy to mitigate these issues, separate from the policy gradient. We first propose the greedy Q softmax update schema for Q value update. The expected Q value is derived by weighted summing the conservative Q value over actions, and the weight is the corresponding greedy Q value. Greedy Q takes the maximum value of the two Q functions, and conservative Q takes the minimum value of the two d
&lt;/p&gt;</description></item><item><title>ProAgent&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20027;&#21160;&#21512;&#20316;&#30340;AI&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#38431;&#21451;&#30340;&#20915;&#31574;&#24182;&#20026;&#33258;&#24049;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11339</link><description>&lt;p&gt;
ProAgent&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20027;&#21160;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
ProAgent: Building Proactive Cooperative AI with Large Language Models. (arXiv:2308.11339v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11339
&lt;/p&gt;
&lt;p&gt;
ProAgent&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20027;&#21160;&#21512;&#20316;&#30340;AI&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#38431;&#21451;&#30340;&#20915;&#31574;&#24182;&#20026;&#33258;&#24049;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AGI&#30740;&#31350;&#20013;&#65292;&#26500;&#24314;&#20855;&#26377;&#33258;&#36866;&#24212;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#20197;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#20851;&#27880;&#28857;&#12290;&#30446;&#21069;&#65292;&#24320;&#21457;&#21512;&#20316;&#20195;&#29702;&#20154;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25919;&#31574;&#27867;&#21270;&#20005;&#37325;&#20381;&#36182;&#20110;&#19982;&#29305;&#23450;&#38431;&#21451;&#30340;&#36807;&#21435;&#20114;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#38480;&#21046;&#20102;&#20195;&#29702;&#20154;&#22312;&#38754;&#23545;&#26032;&#30340;&#38431;&#21451;&#26102;&#37325;&#26032;&#26657;&#20934;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#39044;&#27979;&#38431;&#21451;&#26410;&#26469;&#20915;&#31574;&#33021;&#21147;&#21644;&#20026;&#33258;&#36523;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#33021;&#21147;&#30340;&#20027;&#21160;&#20195;&#29702;&#12290;ProAgent&#22312;&#21512;&#20316;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#21160;&#24577;&#35843;&#25972;&#34892;&#20026;&#20197;&#22686;&#24378;&#19982;&#38431;&#21451;&#30340;&#21327;&#20316;&#21162;&#21147;&#12290;&#27492;&#22806;&#65292;ProAgent&#26694;&#26550;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20415;&#20110;&#26080;&#32541;&#38598;&#25104;&#65292;&#20197;&#24212;&#23545;&#21508;&#31181;&#21327;&#35843;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#23457;&#35745;&#21644;&#35302;&#21457;&#22120;&#22270;&#20687;&#36807;&#28388;&#31561;&#26426;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#29983;&#25104;&#35302;&#21457;&#22120;&#30340;&#38450;&#24481;&#26041;&#27861;&#26469;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20813;&#21463;&#21518;&#38376;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21518;&#38376;&#25915;&#20987;&#29305;&#24449;&#26469;&#23398;&#20064;&#35302;&#21457;&#22120;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26032;&#23398;&#20064;&#30693;&#35782;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.11333</link><description>&lt;p&gt;
&#26080;&#25968;&#25454;&#29983;&#25104;&#35302;&#21457;&#22120;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20813;&#21463;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation. (arXiv:2308.11333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11333
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#23457;&#35745;&#21644;&#35302;&#21457;&#22120;&#22270;&#20687;&#36807;&#28388;&#31561;&#26426;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#29983;&#25104;&#35302;&#21457;&#22120;&#30340;&#38450;&#24481;&#26041;&#27861;&#26469;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20813;&#21463;&#21518;&#38376;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21518;&#38376;&#25915;&#20987;&#29305;&#24449;&#26469;&#23398;&#20064;&#35302;&#21457;&#22120;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26032;&#23398;&#20064;&#30693;&#35782;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#32852;&#37030;&#23398;&#20064; (FL) &#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#19981;&#21487;&#20449;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#23457;&#35745;&#32570;&#22833;&#65292;FL &#26131;&#21463;&#27745;&#26579;&#25915;&#20987;&#65292;&#29305;&#21035;&#26159;&#21518;&#38376;&#25915;&#20987;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#25110;&#30452;&#25509;&#26356;&#25913;&#27169;&#22411;&#21442;&#25968;&#65292;&#36731;&#32780;&#26131;&#20030;&#22320;&#23558;&#21518;&#38376;&#27880;&#20837;&#27169;&#22411;&#65292;&#20174;&#32780;&#35302;&#21457;&#27169;&#22411;&#23545;&#22270;&#20687;&#20013;&#30340;&#30446;&#26631;&#27169;&#24335;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#20010;&#21518;&#38376;&#25915;&#20987;&#29305;&#24449;&#30340;&#26032;&#22411;&#26080;&#25968;&#25454;&#29983;&#25104;&#35302;&#21457;&#22120;&#38450;&#24481;&#26041;&#27861;&#65306;i) &#35302;&#21457;&#22120;&#23398;&#20064;&#36895;&#24230;&#27604;&#26222;&#36890;&#30693;&#35782;&#26356;&#24555;&#65292;ii) &#35302;&#21457;&#22120;&#27169;&#24335;&#23545;&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;&#22823;&#20110;&#26222;&#36890;&#31867;&#21035;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#26087;&#21644;&#26032;&#20840;&#23616;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#29983;&#25104;&#20855;&#26377;&#26032;&#23398;&#20064;&#30693;&#35782;&#30340;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#26041;&#27861;&#36807;&#28388;&#35302;&#21457;&#22120;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a distributed machine learning paradigm, Federated Learning (FL) enables large-scale clients to collaboratively train a model without sharing their raw data. However, due to the lack of data auditing for untrusted clients, FL is vulnerable to poisoning attacks, especially backdoor attacks. By using poisoned data for local training or directly changing the model parameters, attackers can easily inject backdoors into the model, which can trigger the model to make misclassification of targeted patterns in images. To address these issues, we propose a novel data-free trigger-generation-based defense approach based on the two characteristics of backdoor attacks: i) triggers are learned faster than normal knowledge, and ii) trigger patterns have a greater effect on image classification than normal class patterns. Our approach generates the images with newly learned knowledge by identifying the differences between the old and new global models, and filters trigger images by evaluating the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25299;&#25169;&#24615;&#36136;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;Transformer&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#65292;&#24320;&#36767;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.11295</link><description>&lt;p&gt;
&#36890;&#36807;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#25299;&#25169;&#20998;&#26512;&#26469;&#20272;&#31639;Transformer&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices. (arXiv:2308.11295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25299;&#25169;&#24615;&#36136;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;Transformer&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#65292;&#24320;&#36767;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#30830;&#23450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#24182;&#19981;&#26377;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20196;&#29260;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#12290;&#25105;&#20204;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#25506;&#32034;&#20869;&#37096;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#39044;&#27979;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25299;&#25169;&#24615;&#36136;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#36136;&#37327;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#36767;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#38656;&#35201;...
&lt;/p&gt;
&lt;p&gt;
Determining the degree of confidence of deep learning model in its prediction is an open problem in the field of natural language processing. Most of the classical methods for uncertainty estimation are quite weak for text classification models. We set the task of obtaining an uncertainty estimate for neural networks based on the Transformer architecture. A key feature of such mo-dels is the attention mechanism, which supports the information flow between the hidden representations of tokens in the neural network. We explore the formed relationships between internal representations using Topological Data Analysis methods and utilize them to predict model's confidence. In this paper, we propose a method for uncertainty estimation based on the topological properties of the attention mechanism and compare it with classical methods. As a result, the proposed algorithm surpasses the existing methods in quality and opens up a new area of application of the attention mechanism, but requires t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#36164;&#20135;&#31867;&#21035;&#30340;&#32593;&#32476;&#21160;&#37327;&#65292;&#36890;&#36807;&#35266;&#23519;&#36164;&#20135;&#38388;&#21160;&#37327;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#26131;&#20449;&#21495;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#31867;&#21035;&#30340;&#36830;&#32493;&#26399;&#36135;&#21512;&#32422;&#20043;&#38388;&#30340;&#21160;&#37327;&#29305;&#24449;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.11294</link><description>&lt;p&gt;
&#36328;&#36164;&#20135;&#31867;&#21035;&#30340;&#32593;&#32476;&#21160;&#37327;
&lt;/p&gt;
&lt;p&gt;
Network Momentum across Asset Classes. (arXiv:2308.11294v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#36164;&#20135;&#31867;&#21035;&#30340;&#32593;&#32476;&#21160;&#37327;&#65292;&#36890;&#36807;&#35266;&#23519;&#36164;&#20135;&#38388;&#21160;&#37327;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#26131;&#20449;&#21495;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#31867;&#21035;&#30340;&#36830;&#32493;&#26399;&#36135;&#21512;&#32422;&#20043;&#38388;&#30340;&#21160;&#37327;&#29305;&#24449;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32593;&#32476;&#21160;&#37327;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#36164;&#20135;&#38388;&#21160;&#37327;&#20256;&#36882;&#20013;&#24471;&#20986;&#30340;&#26032;&#22411;&#20132;&#26131;&#20449;&#21495;&#12290;&#36215;&#21021;&#65292;&#25105;&#20204;&#21482;&#22312;&#32463;&#27982;&#21644;&#22522;&#26412;&#32852;&#31995;&#20013;&#35266;&#23519;&#21040;&#36825;&#31181;&#29616;&#35937;&#65292;&#20363;&#22914;&#21516;&#19968;&#20844;&#21496;&#30340;&#32929;&#31080;-&#20538;&#21048;&#20851;&#31995;&#21644;&#36890;&#36807;&#20379;&#27714;&#38142;&#30456;&#36830;&#30340;&#32929;&#31080;&#65292;&#21160;&#37327;&#20256;&#36882;&#24847;&#21619;&#30528;&#21160;&#37327;&#39118;&#38505;&#28322;&#20215;&#20174;&#19968;&#20010;&#36164;&#20135;&#20256;&#36882;&#21040;&#21478;&#19968;&#20010;&#36164;&#20135;&#12290;&#21160;&#37327;&#39118;&#38505;&#28322;&#20215;&#30340;&#30456;&#20284;&#24615;&#65292;&#22914;&#20849;&#21160;&#24615;&#27169;&#24335;&#25152;&#31034;&#65292;&#24050;&#32463;&#22312;&#22810;&#20010;&#36164;&#20135;&#31867;&#21035;&#20013;&#34987;&#21457;&#29616;&#65292;&#21253;&#25324;&#21830;&#21697;&#12289;&#32929;&#31080;&#12289;&#20538;&#21048;&#21644;&#36135;&#24065;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#20844;&#21496;&#23618;&#38754;&#20043;&#22806;&#30340;&#20849;&#21516;&#29305;&#24449;&#25110;&#32463;&#27982;&#32852;&#31995;&#65292;&#30740;&#31350;&#21160;&#37327;&#20256;&#36882;&#30340;&#32593;&#32476;&#25928;&#24212;&#22312;&#36825;&#20123;&#31867;&#21035;&#20043;&#38388;&#19968;&#30452;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36328;&#36234;&#36825;&#22235;&#20010;&#31867;&#21035;&#30340;64&#20010;&#36830;&#32493;&#26399;&#36135;&#21512;&#32422;&#30340;&#21160;&#37327;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#32447;&#24615;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#21160;&#37327;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the concept of network momentum, a novel trading signal derived from momentum spillover across assets. Initially observed within the confines of pairwise economic and fundamental ties, such as the stock-bond connection of the same company and stocks linked through supply-demand chains, momentum spillover implies a propagation of momentum risk premium from one asset to another. The similarity of momentum risk premium, exemplified by co-movement patterns, has been spotted across multiple asset classes including commodities, equities, bonds and currencies. However, studying the network effect of momentum spillover across these classes has been challenging due to a lack of readily available common characteristics or economic ties beyond the company level. In this paper, we explore the interconnections of momentum features across a diverse range of 64 continuous future contracts spanning these four classes. We utilise a linear and interpretable graph learning model with minim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26408;&#26448;&#22806;&#24418;&#39044;&#27979;&#20869;&#37096;&#32570;&#38519;&#20301;&#32622;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20108;&#20998;&#31867;&#20998;&#21106;&#20219;&#21153;&#65292;&#23454;&#29616;&#22312;&#24265;&#20215;&#35774;&#22791;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#22312;&#20919;&#26441;&#21644;&#20113;&#26441;&#26641;&#31181;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11291</link><description>&lt;p&gt;
&#25552;&#39640;&#38271;&#24452;&#21521;&#29305;&#24449;&#20256;&#25773;&#20013;&#30340;&#26408;&#26448;&#21407;&#32467;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation. (arXiv:2308.11291v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26408;&#26448;&#22806;&#24418;&#39044;&#27979;&#20869;&#37096;&#32570;&#38519;&#20301;&#32622;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20108;&#20998;&#31867;&#20998;&#21106;&#20219;&#21153;&#65292;&#23454;&#29616;&#22312;&#24265;&#20215;&#35774;&#22791;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#22312;&#20919;&#26441;&#21644;&#20113;&#26441;&#26641;&#31181;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26408;&#26448;&#34892;&#19994;&#20013;&#26408;&#26448;&#21407;&#32467;&#30340;&#36136;&#37327;&#20027;&#35201;&#21462;&#20915;&#20110;&#20869;&#22806;&#32570;&#38519;&#30340;&#23384;&#22312;&#65292;&#20854;&#20013;&#20869;&#37096;&#33410;&#30116;&#26159;&#26641;&#26525;&#29983;&#38271;&#30340;&#32467;&#26524;&#12290;&#30446;&#21069;&#65292;&#23450;&#20301;&#20869;&#37096;&#33410;&#30116;&#38656;&#35201;&#20351;&#29992;&#26114;&#36149;&#30340;&#35774;&#22791;&#65292;&#22914;X&#23556;&#32447;&#25195;&#25551;&#20202;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36890;&#36807;&#26408;&#26448;&#22806;&#24418;&#39044;&#27979;&#20869;&#37096;&#32570;&#38519;&#20301;&#32622;&#30340;&#20219;&#21153;&#12290;&#25968;&#25454;&#38598;&#36890;&#36807;&#21033;&#29992;X&#23556;&#32447;&#27979;&#37327;&#25552;&#21462;&#36718;&#24275;&#21644;&#33410;&#30116;&#26500;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#36825;&#20010;&#20108;&#20998;&#31867;&#20998;&#21106;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#19968;&#26086;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#23436;&#27605;&#65292;&#21487;&#20197;&#20351;&#29992;&#24265;&#20215;&#35774;&#22791;&#65288;&#22914;&#28608;&#20809;&#21078;&#38754;&#20202;&#65289;&#27979;&#37327;&#30340;&#22806;&#24418;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#20919;&#26441;&#21644;&#20113;&#26441;&#26641;&#31181;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#24490;&#29615;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of a wood log in the wood industry depends heavily on the presence of both outer and inner defects, including inner knots that are a result of the growth of tree branches. Today, locating the inner knots require the use of expensive equipment such as X-ray scanners. In this paper, we address the task of predicting the location of inner defects from the outer shape of the logs. The dataset is built by extracting both the contours and the knots with X-ray measurements. We propose to solve this binary segmentation task by leveraging convolutional recurrent neural networks. Once the neural network is trained, inference can be performed from the outer shape measured with cheap devices such as laser profilers. We demonstrate the effectiveness of our approach on fir and spruce tree species and perform ablation on the recurrence to demonstrate its importance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#37327;&#23376;&#31995;&#32479;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#32463;&#20856;&#38452;&#24433;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#37327;&#23376;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#39044;&#27979;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11290</link><description>&lt;p&gt;
&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#37327;&#23376;&#31995;&#32479;&#23398;&#20064;&#30340;&#24433;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ShadowNet for Data-Centric Quantum System Learning. (arXiv:2308.11290v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#37327;&#23376;&#31995;&#32479;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#32463;&#20856;&#38452;&#24433;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#37327;&#23376;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#39044;&#27979;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32500;&#24230;&#35781;&#21650;&#65292;&#29702;&#35299;&#22823;&#22411;&#37327;&#23376;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#21464;&#24471;&#22256;&#38590;&#12290;&#32479;&#35745;&#23398;&#20064;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21327;&#35758;&#21644;&#32463;&#20856;&#38452;&#24433;&#22312;&#36825;&#20010;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#28982;&#32780;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#23616;&#38480;&#24615;&#65306;&#21069;&#32773;&#21463;&#21040;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#22256;&#25200;&#65292;&#21518;&#32773;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#20197;&#20419;&#36827;&#22810;&#26679;&#21270;&#30340;&#37327;&#23376;&#31995;&#32479;&#23398;&#20064;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#33539;&#24335;&#21033;&#29992;&#20102;&#32463;&#20856;&#38452;&#24433;&#21644;&#20854;&#20182;&#26131;&#20110;&#33719;&#21462;&#30340;&#37327;&#23376;&#31995;&#32479;&#20449;&#24687;&#26469;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#25506;&#32034;&#30340;&#37327;&#23376;&#31995;&#32479;&#23398;&#20064;&#38382;&#39064;&#30340;&#28508;&#22312;&#26144;&#23556;&#35268;&#24459;&#12290;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#20010;&#33539;&#24335;&#21487;&#20197;&#22312;&#31163;&#32447;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#38454;&#27573;&#33021;&#22815;&#20248;&#31168;&#22320;&#39044;&#27979;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#31995;&#32479;&#65292;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#30340;&#29366;&#24577;&#21103;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#32487;&#25215;&#20102;
&lt;/p&gt;
&lt;p&gt;
Understanding the dynamics of large quantum systems is hindered by the curse of dimensionality. Statistical learning offers new possibilities in this regime by neural-network protocols and classical shadows, while both methods have limitations: the former is plagued by the predictive uncertainty and the latter lacks the generalization ability. Here we propose a data-centric learning paradigm combining the strength of these two approaches to facilitate diverse quantum system learning (QSL) tasks. Particularly, our paradigm utilizes classical shadows along with other easily obtainable information of quantum systems to create the training dataset, which is then learnt by neural networks to unveil the underlying mapping rule of the explored QSL problem. Capitalizing on the generalization power of neural networks, this paradigm can be trained offline and excel at predicting previously unseen systems at the inference stage, even with few state copies. Besides, it inherits the characteristic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27979;&#35797;&#26102;&#38388;&#23884;&#20837;&#24402;&#19968;&#21270;&#8221;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28909;&#38376;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24402;&#19968;&#21270;&#30340;&#29289;&#21697;&#23884;&#20837;&#26469;&#25511;&#21046;&#23884;&#20837;&#22823;&#23567;&#65292;&#24182;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#30340;&#35282;&#24230;&#30456;&#20284;&#24230;&#21306;&#20998;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#65292;&#20174;&#32780;&#26377;&#25928;&#20943;&#23569;&#20102;&#28909;&#38376;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.11288</link><description>&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#23884;&#20837;&#24402;&#19968;&#21270;&#23545;&#28909;&#38376;&#20559;&#35265;&#30340;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Test Time Embedding Normalization for Popularity Bias Mitigation. (arXiv:2308.11288v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27979;&#35797;&#26102;&#38388;&#23884;&#20837;&#24402;&#19968;&#21270;&#8221;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28909;&#38376;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24402;&#19968;&#21270;&#30340;&#29289;&#21697;&#23884;&#20837;&#26469;&#25511;&#21046;&#23884;&#20837;&#22823;&#23567;&#65292;&#24182;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#30340;&#35282;&#24230;&#30456;&#20284;&#24230;&#21306;&#20998;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#65292;&#20174;&#32780;&#26377;&#25928;&#20943;&#23569;&#20102;&#28909;&#38376;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#38376;&#20559;&#35265;&#26159;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#28909;&#38376;&#29289;&#21697;&#20542;&#21521;&#20110;&#20027;&#23548;&#25512;&#33616;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#27979;&#35797;&#26102;&#38388;&#23884;&#20837;&#24402;&#19968;&#21270;&#8221;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#32531;&#35299;&#28909;&#38376;&#20559;&#35265;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#20102;&#20197;&#24448;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#38454;&#27573;&#21033;&#29992;&#24402;&#19968;&#21270;&#30340;&#29289;&#21697;&#23884;&#20837;&#26469;&#25511;&#21046;&#23884;&#20837;&#30340;&#22823;&#23567;&#65292;&#32780;&#23884;&#20837;&#30340;&#22823;&#23567;&#19982;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#39640;&#24230;&#30456;&#20851;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#37319;&#26679;softmax&#25439;&#22833;&#30456;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#28909;&#38376;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#20043;&#38388;&#30340;&#35282;&#24230;&#30456;&#20284;&#24230;&#21487;&#20197;&#21306;&#20998;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#27969;&#34892;&#31243;&#24230;&#12290;&#36825;&#19968;&#20998;&#26512;&#35299;&#37322;&#20102;&#25105;&#20204;&#26041;&#27861;&#25104;&#21151;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popularity bias is a widespread problem in the field of recommender systems, where popular items tend to dominate recommendation results. In this work, we propose 'Test Time Embedding Normalization' as a simple yet effective strategy for mitigating popularity bias, which surpasses the performance of the previous mitigation approaches by a significant margin. Our approach utilizes the normalized item embedding during the inference stage to control the influence of embedding magnitude, which is highly correlated with item popularity. Through extensive experiments, we show that our method combined with the sampled softmax loss effectively reduces popularity bias compare to previous approaches for bias mitigation. We further investigate the relationship between user and item embeddings and find that the angular similarity between embeddings distinguishes preferable and non-preferable items regardless of their popularity. The analysis explains the mechanism behind the success of our approac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#26964;&#24418;&#31526;&#21495;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27880;&#37322;&#30340;&#19977;&#32500;&#28210;&#26579;&#21644;&#26144;&#23556;&#29031;&#29255;&#65292;&#32467;&#21512;&#20809;&#29031;&#22686;&#24378;&#12290;&#30740;&#31350;&#22242;&#38431;&#21019;&#24314;&#20102;HeiCuBeDa&#21644;MaiCuBeDa&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#26144;&#23556;&#24037;&#20855;&#20197;&#20256;&#36882;&#27880;&#37322;&#12290;&#31526;&#21495;&#23450;&#20301;&#26041;&#27861;&#20351;&#29992;RepPoints&#26816;&#27979;&#22120;&#26469;&#39044;&#27979;&#23383;&#31526;&#30340;&#20301;&#32622;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22788;&#29702;&#26964;&#24418;&#25991;&#23383;&#30340;&#25968;&#23383;&#24037;&#20855;&#24320;&#21457;&#21644;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.11277</link><description>&lt;p&gt;
&#22522;&#20110;CNN&#30340;&#22522;&#20110;&#27880;&#37322;&#30340;&#19977;&#32500;&#28210;&#26579;&#21644;&#26144;&#23556;&#29031;&#29255;&#30340;&#26964;&#24418;&#31526;&#21495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation. (arXiv:2308.11277v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11277
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#26964;&#24418;&#31526;&#21495;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27880;&#37322;&#30340;&#19977;&#32500;&#28210;&#26579;&#21644;&#26144;&#23556;&#29031;&#29255;&#65292;&#32467;&#21512;&#20809;&#29031;&#22686;&#24378;&#12290;&#30740;&#31350;&#22242;&#38431;&#21019;&#24314;&#20102;HeiCuBeDa&#21644;MaiCuBeDa&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#26144;&#23556;&#24037;&#20855;&#20197;&#20256;&#36882;&#27880;&#37322;&#12290;&#31526;&#21495;&#23450;&#20301;&#26041;&#27861;&#20351;&#29992;RepPoints&#26816;&#27979;&#22120;&#26469;&#39044;&#27979;&#23383;&#31526;&#30340;&#20301;&#32622;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22788;&#29702;&#26964;&#24418;&#25991;&#23383;&#30340;&#25968;&#23383;&#24037;&#20855;&#24320;&#21457;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Digital Ancient Near Eastern Studies (DANES)&#31038;&#21306;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#29992;&#20110;&#22788;&#29702;&#26964;&#24418;&#25991;&#23383;&#30340;&#25968;&#23383;&#24037;&#20855;&#65292;&#36825;&#26159;&#19968;&#31181;&#21360;&#22312;&#31896;&#22303;&#26495;&#19978;&#30340;&#19977;&#32500;&#33050;&#26412;&#65292;&#24050;&#26377;&#19977;&#21315;&#22810;&#24180;&#21382;&#21490;&#21644;&#33267;&#23569;&#20843;&#31181;&#20027;&#35201;&#35821;&#35328;&#12290;&#23427;&#30001;&#25968;&#21315;&#20010;&#38543;&#26102;&#38388;&#21644;&#31354;&#38388;&#21464;&#21270;&#30340;&#23383;&#31526;&#32452;&#25104;&#12290;&#29031;&#29255;&#26159;&#26368;&#24120;&#29992;&#30340;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#32780;&#22696;&#27700;&#32472;&#30011;&#21017;&#23481;&#26131;&#34987;&#35299;&#37322;&#12290;&#25105;&#20204;&#21019;&#24314;&#24182;&#20351;&#29992;&#20102;HeiCuBeDa&#21644;MaiCuBeDa&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422;500&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#24179;&#26495;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#26032;&#22411;&#31867;&#20284;OCR&#30340;&#28151;&#21512;&#22270;&#20687;&#25968;&#25454;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#39069;&#22806;&#30340;&#26144;&#23556;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;3D&#28210;&#26579;&#21644;&#29031;&#29255;&#20043;&#38388;&#20256;&#36882;&#27880;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;RepPoints&#26816;&#27979;&#22120;&#26469;&#39044;&#27979;&#23383;&#31526;&#30340;&#20301;&#32622;&#65292;&#20197;&#36793;&#30028;&#26694;&#30340;&#24418;&#24335;&#36827;&#34892;&#31526;&#21495;&#23450;&#20301;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;GigaMesh&#30340;MSII&#65288;&#26354;&#29575;&#65289;&#22522;&#20110;&#28210;&#26579;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#20197;&#21450;Phong&#30528;&#33394;&#30340;3D&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the challenges of the Digital Ancient Near Eastern Studies (DANES) community, we develop digital tools for processing cuneiform script being a 3D script imprinted into clay tablets used for more than three millennia and at least eight major languages. It consists of thousands of characters that have changed over time and space. Photographs are the most common representations usable for machine learning, while ink drawings are prone to interpretation. Best suited 3D datasets that are becoming available. We created and used the HeiCuBeDa and MaiCuBeDa datasets, which consist of around 500 annotated tablets. For our novel OCR-like approach to mixed image data, we provide an additional mapping tool for transferring annotations between 3D renderings and photographs. Our sign localization uses a RepPoints detector to predict the locations of characters as bounding boxes. We use image data from GigaMesh's MSII (curvature, see https://gigamesh.eu) based rendering, Phong-shaded 3D 
&lt;/p&gt;</description></item><item><title>FoX&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#65292;&#24182;&#24341;&#23548;&#26234;&#33021;&#20307;&#22312;&#19981;&#21516;&#24418;&#25104;&#20013;&#35775;&#38382;&#26377;&#24847;&#20041;&#30340;&#29366;&#24577;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11272</link><description>&lt;p&gt;
FoX:&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24418;&#25104;&#24863;&#30693;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
FoX: Formation-aware exploration in multi-agent reinforcement learning. (arXiv:2308.11272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11272
&lt;/p&gt;
&lt;p&gt;
FoX&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#65292;&#24182;&#24341;&#23548;&#26234;&#33021;&#20307;&#22312;&#19981;&#21516;&#24418;&#25104;&#20013;&#35775;&#38382;&#26377;&#24847;&#20041;&#30340;&#29366;&#24577;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (MARL) &#22312;&#21508;&#31181;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26234;&#33021;&#20307;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#32780;&#25351;&#25968;&#22686;&#38271;&#30340;&#25506;&#32034;&#31354;&#38388;&#65292;&#25506;&#32034;&#20173;&#28982;&#26159;MARL&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#35299;&#20915;&#25506;&#32034;&#31354;&#38388;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#25506;&#32034;&#31354;&#38388;&#19978;&#23450;&#20041;&#20102;&#19968;&#20010;&#22522;&#20110;&#24418;&#25104;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#26088;&#22312;&#36890;&#36807;&#20165;&#25506;&#32034;&#19981;&#21516;&#24418;&#24335;&#30340;&#26377;&#24847;&#20041;&#29366;&#24577;&#26469;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24418;&#25104;&#24863;&#30693;&#25506;&#32034; (FoX) &#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#23548;&#37096;&#20998;&#21487;&#35266;&#27979;&#26234;&#33021;&#20307;&#20973;&#20511;&#33258;&#36523;&#35266;&#27979;&#20449;&#24687;&#20805;&#20998;&#20102;&#35299;&#20854;&#24403;&#21069;&#24418;&#25104;&#65292;&#40723;&#21169;&#20182;&#20204;&#35775;&#38382;&#19981;&#21516;&#24418;&#25104;&#20013;&#30340;&#29366;&#24577;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;FoX&#26694;&#26550;&#22312;Google Research Football&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep multi-agent reinforcement learning (MARL) has gained significant popularity due to its success in various cooperative multi-agent tasks. However, exploration still remains a challenging problem in MARL due to the partial observability of the agents and the exploration space that can grow exponentially as the number of agents increases. Firstly, in order to address the scalability issue of the exploration space, we define a formation-based equivalence relation on the exploration space and aim to reduce the search space by exploring only meaningful states in different formations. Then, we propose a novel formation-aware exploration (FoX) framework that encourages partially observable agents to visit the states in diverse formations by guiding them to be well aware of their current formation solely based on their own observations. Numerical results show that the proposed FoX framework significantly outperforms the state-of-the-art MARL algorithms on Google Research Football
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;QiML&#65289;&#26159;&#19968;&#20010;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#21407;&#29702;&#22312;&#32463;&#20856;&#35745;&#31639;&#26694;&#26550;&#20013;&#30340;&#26032;&#39046;&#22495;&#65292;&#22312;&#26412;&#35843;&#26597;&#20013;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;QiML&#30340;&#32508;&#21512;&#21644;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#20854;&#22810;&#26679;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#20026;QiML&#24314;&#31435;&#20102;&#26126;&#30830;&#30340;&#23450;&#20041;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#23558;&#20174;&#37327;&#23376;&#21147;&#23398;&#12289;&#37327;&#23376;&#35745;&#31639;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#20016;&#23500;QiML&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.11269</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Quantum-Inspired Machine Learning: a Survey. (arXiv:2308.11269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11269
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;QiML&#65289;&#26159;&#19968;&#20010;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#21407;&#29702;&#22312;&#32463;&#20856;&#35745;&#31639;&#26694;&#26550;&#20013;&#30340;&#26032;&#39046;&#22495;&#65292;&#22312;&#26412;&#35843;&#26597;&#20013;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;QiML&#30340;&#32508;&#21512;&#21644;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#20854;&#22810;&#26679;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#20026;QiML&#24314;&#31435;&#20102;&#26126;&#30830;&#30340;&#23450;&#20041;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#23558;&#20174;&#37327;&#23376;&#21147;&#23398;&#12289;&#37327;&#23376;&#35745;&#31639;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#20016;&#23500;QiML&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;QiML&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#22240;&#20854;&#22312;&#32463;&#20856;&#35745;&#31639;&#26694;&#26550;&#20013;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#21407;&#29702;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#32508;&#36848;&#25991;&#29486;&#32463;&#24120;&#21482;&#23545;QiML&#36827;&#34892;&#34920;&#38754;&#25506;&#32034;&#65292;&#32780;&#26356;&#22810;&#20851;&#27880;&#24191;&#20041;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#39046;&#22495;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;QiML&#30340;&#32508;&#21512;&#21644;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;QiML&#30340;&#22810;&#26679;&#21270;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#24352;&#37327;&#32593;&#32476;&#27169;&#25311;&#12289;&#38750;&#37327;&#23376;&#21270;&#31639;&#27861;&#31561;&#65292;&#23637;&#31034;&#20102;&#26368;&#26032;&#30340;&#36827;&#23637;&#12289;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#20808;&#21069;&#23545;QiML&#30340;&#21508;&#31181;&#35299;&#37322;&#21450;&#20854;&#20869;&#22312;&#30340;&#27169;&#31946;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;QiML&#23450;&#20041;&#12290;&#38543;&#30528;QiML&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#25105;&#20204;&#39044;&#35745;&#23558;&#26377;&#22823;&#37327;&#30340;&#26410;&#26469;&#21457;&#23637;&#20174;&#37327;&#23376;&#21147;&#23398;&#12289;&#37327;&#23376;&#35745;&#31639;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#20016;&#23500;QiML&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum-inspired Machine Learning (QiML) is a burgeoning field, receiving global attention from researchers for its potential to leverage principles of quantum mechanics within classical computational frameworks. However, current review literature often presents a superficial exploration of QiML, focusing instead on the broader Quantum Machine Learning (QML) field. In response to this gap, this survey provides an integrated and comprehensive examination of QiML, exploring QiML's diverse research domains including tensor network simulations, dequantized algorithms, and others, showcasing recent advancements, practical applications, and illuminating potential future research avenues. Further, a concrete definition of QiML is established by analyzing various prior interpretations of the term and their inherent ambiguities. As QiML continues to evolve, we anticipate a wealth of future developments drawing from quantum mechanics, quantum computing, and classical machine learning, enriching 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#32780;&#23545;&#25239;&#24615;RCPG&#36890;&#36807;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#24335;&#30452;&#25509;&#21644;&#22686;&#37327;&#23398;&#20064;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2308.11267</link><description>&lt;p&gt;
&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#21644;&#23545;&#25239;&#24615;&#31574;&#30053;&#26799;&#24230;&#22312;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes. (arXiv:2308.11267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#32780;&#23545;&#25239;&#24615;RCPG&#36890;&#36807;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#24335;&#30452;&#25509;&#21644;&#22686;&#37327;&#23398;&#20064;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RCMDP&#65289;&#26159;&#19968;&#20010;&#26368;&#36817;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#24314;&#27169;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#22312;&#36716;&#31227;&#21160;&#24577;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#23545;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27169;&#25311;RCMDPs&#38656;&#35201;&#22522;&#20110;&#27599;&#20010;&#29366;&#24577;&#30340;&#20540;&#20272;&#35745;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#36825;&#31181;&#26041;&#27861;&#20043;&#21069;&#22312;&#40065;&#26834;&#32422;&#26463;&#31574;&#30053;&#26799;&#24230;&#65288;RCPG&#65289;&#20013;&#20351;&#29992;&#36807;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#31216;&#20026;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#32780;&#19981;&#26159;&#20540;&#25110;&#32422;&#26463;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#20174;&#32780;&#20462;&#25913;RCPG&#12290;&#23545;&#25239;&#24615;RCPG&#20063;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#20844;&#24335;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#20294;&#26159;&#23558;&#20854;&#20316;&#20026;&#23545;&#25239;&#31574;&#30053;&#30452;&#25509;&#21644;&#22686;&#37327;&#22320;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy throug
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27714;&#35299;&#21338;&#24328;&#20013;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11256</link><description>&lt;p&gt;
&#22312;&#27714;&#35299;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Last-iterate Convergence Algorithms in Solving Games. (arXiv:2308.11256v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27714;&#35299;&#21338;&#24328;&#20013;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#24724;&#31639;&#27861;&#22312;&#23398;&#20064;&#20004;&#20154;&#38646;&#21644;&#26631;&#20934;&#22411;&#28216;&#25103;&#21644;&#25193;&#23637;&#22411;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290;&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#32771;&#34385;&#20102;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#25910;&#25947;&#30340;&#26080;&#24724;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#26368;&#26377;&#21517;&#30340;&#20004;&#20010;&#31639;&#27861;&#26159;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#12290;&#28982;&#32780;&#65292;OGDA&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#24456;&#39640;&#12290;OMWU&#20855;&#26377;&#36739;&#20302;&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20294;&#23454;&#39564;&#24615;&#33021;&#36739;&#24046;&#65292;&#24182;&#19988;&#23427;&#30340;&#25910;&#25947;&#20165;&#22312;&#32435;&#20160;&#22343;&#34913;&#21807;&#19968;&#26102;&#25104;&#31435;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#29992;&#20110;MWU&#65292;&#23427;&#28040;&#38500;&#20102;&#21807;&#19968;&#24615;&#26465;&#20214;&#65292;&#24182;&#19988;&#22312;&#19982;OMWU&#30456;&#21516;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22522;&#20110;RT&#30340;&#31639;&#27861;&#22312;&#30456;&#21516;&#36845;&#20195;&#27425;&#25968;&#19979;&#34920;&#29616;&#19981;&#22914;OGDA&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#25910;&#25947;&#20445;&#35777;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21453;&#39304;&#20551;&#35774;&#65292;&#36825;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;RT&#26694;&#26550;&#36827;&#34892;&#20102;&#26356;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
No-regret algorithms are popular for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs). Many recent works consider the last-iterate convergence no-regret algorithms. Among them, the two most famous algorithms are Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weight Update (OMWU). However, OGDA has high per-iteration complexity. OMWU exhibits a lower per-iteration complexity but poorer empirical performance, and its convergence holds only when NE is unique. Recent works propose a Reward Transformation (RT) framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU. Unfortunately, RT-based algorithms perform worse than OGDA under the same number of iterations, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios. To address these issues, we provide a closer analysis of the RT framework, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20559;&#24046;&#21644;&#38169;&#35823;&#30340;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#36229;&#36807;&#22235;&#21313;&#31181;&#28508;&#22312;&#30340;&#20559;&#24046;&#26469;&#28304;&#65292;&#20026;&#27599;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#29702;&#35299;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#65292;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#20844;&#24179;&#12289;&#26356;&#36879;&#26126;&#12289;&#26356;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11254</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#20559;&#24046;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A survey on bias in machine learning research. (arXiv:2308.11254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20559;&#24046;&#21644;&#38169;&#35823;&#30340;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#36229;&#36807;&#22235;&#21313;&#31181;&#28508;&#22312;&#30340;&#20559;&#24046;&#26469;&#28304;&#65292;&#20026;&#27599;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#29702;&#35299;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#24046;&#65292;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#20844;&#24179;&#12289;&#26356;&#36879;&#26126;&#12289;&#26356;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#20559;&#24046;&#30340;&#30740;&#31350;&#36890;&#24120;&#20851;&#27880;&#20844;&#24179;&#24615;&#65292;&#21364;&#24573;&#35270;&#20102;&#20559;&#24046;&#30340;&#26681;&#28304;&#25110;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#20559;&#24046;&#26368;&#21021;&#34987;&#23450;&#20041;&#20026;&#8220;&#31995;&#32479;&#24615;&#38169;&#35823;&#8221;&#65292;&#36890;&#24120;&#26159;&#30001;&#30740;&#31350;&#36807;&#31243;&#20013;&#19981;&#21516;&#38454;&#27573;&#30340;&#20154;&#31867;&#24341;&#36215;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20559;&#24046;&#21644;&#25968;&#25454;&#27169;&#22411;&#20013;&#28508;&#22312;&#20559;&#24046;&#21644;&#38169;&#35823;&#30340;&#20998;&#31867;&#65292;&#24357;&#34917;&#36807;&#21435;&#20851;&#20110;&#20559;&#24046;&#30740;&#31350;&#30340;&#24046;&#36317;&#12290;&#35813;&#25991;&#37325;&#28857;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#20559;&#24046;&#12290;&#35843;&#26597;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27969;&#31243;&#20013;&#36229;&#36807;&#22235;&#21313;&#31181;&#28508;&#22312;&#30340;&#20559;&#24046;&#26469;&#28304;&#65292;&#24182;&#20026;&#27599;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#20013;&#20559;&#24046;&#30340;&#26469;&#28304;&#21644;&#21518;&#26524;&#65292;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#22909;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#20559;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#36879;&#26126;&#12289;&#26356;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research on bias in machine learning often focuses on fairness, while overlooking the roots or causes of bias. However, bias was originally defined as a "systematic error," often caused by humans at different stages of the research process. This article aims to bridge the gap between past literature on bias in research by providing taxonomy for potential sources of bias and errors in data and models. The paper focus on bias in machine learning pipelines. Survey analyses over forty potential sources of bias in the machine learning (ML) pipeline, providing clear examples for each. By understanding the sources and consequences of bias in machine learning, better methods can be developed for its detecting and mitigating, leading to fairer, more transparent, and more accurate ML models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#21270;&#23398;&#36807;&#31243;&#30340;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#20013;&#65292;&#23545;&#21333;&#28304;&#21644;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#36827;&#34892;&#36866;&#24212;&#65292;&#20351;&#29992;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#20063;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.11247</link><description>&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#29992;&#20110;&#21270;&#23398;&#36807;&#31243;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes. (arXiv:2308.11247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#21270;&#23398;&#36807;&#31243;&#30340;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#20013;&#65292;&#23545;&#21333;&#28304;&#21644;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#36827;&#34892;&#36866;&#24212;&#65292;&#20351;&#29992;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#20063;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#35786;&#26029;&#26159;&#36807;&#31243;&#30417;&#35270;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#22522;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#39044;&#27979;&#25925;&#38556;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#36825;&#20123;&#21464;&#21270;&#21487;&#33021;&#30001;&#20110;&#30417;&#27979;&#36807;&#31243;&#20013;&#30340;&#21464;&#21270;&#65292;&#22914;&#25805;&#20316;&#27169;&#24335;&#30340;&#25913;&#21464;&#65292;&#23548;&#33268;&#36328;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#22312;&#21270;&#23398;&#24037;&#19994;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#30000;&#32435;&#35199;-&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#32972;&#26223;&#19979;&#65292;&#25552;&#20379;&#20102;&#21333;&#28304;&#21644;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#22312;&#20132;&#21449;&#39046;&#22495;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24191;&#27867;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#36827;&#34892;&#36866;&#24212;&#65292;&#20351;&#29992;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#20063;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#23545;&#20110;&#21333;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#22522;&#20934;&#27169;&#22411;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fault diagnosis is an essential component in process supervision. Indeed, it determines which kind of fault has occurred, given that it has been previously detected, allowing for appropriate intervention. Automatic fault diagnosis systems use machine learning for predicting the fault type from sensor readings. Nonetheless, these models are sensible to changes in the data distributions, which may be caused by changes in the monitored process, such as changes in the mode of operation. This scenario is known as Cross-Domain Fault Diagnosis (CDFD). We provide an extensive comparison of single and multi-source unsupervised domain adaptation (SSDA and MSDA respectively) algorithms for CDFD. We study these methods in the context of the Tennessee-Eastmann Process, a widely used benchmark in the chemical industry. We show that using multiple domains during training has a positive effect, even when no adaptation is employed. As such, the MSDA baseline improves over the SSDA baseline classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;85.9%&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#20102;&#20854;&#24615;&#33021;&#19982;wav2vec2&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11241</link><description>&lt;p&gt;
&#19968;&#20010;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification. (arXiv:2308.11241v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;85.9%&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#20102;&#20854;&#24615;&#33021;&#19982;wav2vec2&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wav2vec2&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#24212;&#29992;Transformer&#26550;&#26500;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#65292;&#36824;&#29992;&#20110;&#25972;&#20010;&#35821;&#38899;&#22788;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#26377;&#25928;&#31471;&#21040;&#31471;&#35828;&#35805;&#20154;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21442;&#25968;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#30830;&#23450;&#19968;&#20010;&#26377;&#25928;&#27169;&#22411;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#23398;&#20064;&#33021;&#21147;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#26102;&#38388;&#38376;&#27744;&#21270;(Temporal Gate Pooling)&#65292;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#12290;&#25105;&#20204;&#23558;Conformer&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#21033;&#29992;BEST-RQ&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;VoxCeleb1&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#22312;&#20165;&#26377;28.5M&#20010;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;85.9%&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;&#20855;&#26377;317.7M&#20010;&#21442;&#25968;&#30340;wav2vec2&#30456;&#24403;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/HarunoriKawano/speaker-identification-with-tgp&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at https://github.com/HarunoriKawano/speaker-identification-with-tgp.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21160;&#24577;&#25554;&#20837;&#21644;&#21024;&#38500;&#29305;&#24449;&#30340;minHash&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#27979;&#37327;&#39640;&#32500;&#20108;&#36827;&#21046;&#25968;&#25454;&#30340;Jaccard&#30456;&#20284;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.11240</link><description>&lt;p&gt;
&#20855;&#26377;&#29305;&#24449;&#25554;&#20837;&#21644;&#21024;&#38500;&#30340;&#26368;&#23567;&#19981;&#30456;&#20851;&#32622;&#25442;
&lt;/p&gt;
&lt;p&gt;
Minwise-Independent Permutations with Insertion and Deletion of Features. (arXiv:2308.11240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11240
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21160;&#24577;&#25554;&#20837;&#21644;&#21024;&#38500;&#29305;&#24449;&#30340;minHash&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#27979;&#37327;&#39640;&#32500;&#20108;&#36827;&#21046;&#25968;&#25454;&#30340;Jaccard&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20182;&#20204;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20013;&#65292;Broder&#31561;&#20154;&#24341;&#20837;&#20102;minHash&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#35745;&#31639;&#39640;&#32500;&#20108;&#36827;&#21046;&#25968;&#25454;&#30340;&#20302;&#32500;&#33609;&#22270;&#65292;&#21487;&#20197;&#36817;&#20284;&#22320;&#34913;&#37327;Jaccard&#30456;&#20284;&#24615;&#12290;&#33258;&#20174;&#23427;&#30340;&#21457;&#26126;&#20197;&#26469;&#65292;minHash&#24050;&#32463;&#22312;&#21508;&#31181;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#34987;&#20174;&#19994;&#32773;&#26222;&#36941;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#26159;&#21160;&#24577;&#30340;&#65292;&#20854;&#29305;&#24449;&#38598;&#20250;&#38543;&#26102;&#38388;&#32780;&#28436;&#21464;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;&#24403;&#29305;&#24449;&#22312;&#25968;&#25454;&#38598;&#20013;&#21160;&#24577;&#25554;&#20837;&#21644;&#21024;&#38500;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#26159;&#38024;&#23545;&#26356;&#26032;&#30340;&#32500;&#24230;&#37325;&#22797;&#35745;&#31639;minHash&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29983;&#25104;&#26032;&#30340;&#38543;&#26426;&#32622;&#25442;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#22312;&#29305;&#24449;&#30340;&#21160;&#24577;&#25554;&#20837;&#21644;&#21024;&#38500;&#30340;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#20851;&#20110;minHash&#30340;&#31995;&#32479;&#30740;&#31350;&#35760;&#24405;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#20102;&#36825;&#39033;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;minHash&#33609;&#22270;&#36866;&#24212;&#21160;&#24577;&#25554;&#20837;&#21644;&#21024;&#38500;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In their seminal work, Broder \textit{et. al.}~\citep{BroderCFM98} introduces the $\mathrm{minHash}$ algorithm that computes a low-dimensional sketch of high-dimensional binary data that closely approximates pairwise Jaccard similarity. Since its invention, $\mathrm{minHash}$ has been commonly used by practitioners in various big data applications. Further, the data is dynamic in many real-life scenarios, and their feature sets evolve over time. We consider the case when features are dynamically inserted and deleted in the dataset. We note that a naive solution to this problem is to repeatedly recompute $\mathrm{minHash}$ with respect to the updated dimension. However, this is an expensive task as it requires generating fresh random permutations. To the best of our knowledge, no systematic study of $\mathrm{minHash}$ is recorded in the context of dynamic insertion and deletion of features. In this work, we initiate this study and suggest algorithms that make the $\mathrm{minHash}$ sket
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35775;&#38382;&#22823;&#37327;&#22810;&#26679;&#30340;&#24739;&#32773;&#25968;&#25454;&#24182;&#20445;&#25252;&#38544;&#31169;&#65292;&#26469;&#39044;&#27979;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#24449;&#24739;&#32773;&#30340;&#26368;&#20339;&#27835;&#30103;&#33647;&#29289;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2308.11220</link><description>&lt;p&gt;
&#20351;&#29992;&#24739;&#32773;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#20197;&#20445;&#25252;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#24449;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment. (arXiv:2308.11220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35775;&#38382;&#22823;&#37327;&#22810;&#26679;&#30340;&#24739;&#32773;&#25968;&#25454;&#24182;&#20445;&#25252;&#38544;&#31169;&#65292;&#26469;&#39044;&#27979;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#24449;&#24739;&#32773;&#30340;&#26368;&#20339;&#27835;&#30103;&#33647;&#29289;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22919;&#31185;&#20869;&#20998;&#27852;&#23398;&#39046;&#22495;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#21307;&#30103;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#33853;&#21518;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23545;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#12290;&#26377;&#20851;&#33655;&#23572;&#33945;&#27700;&#24179;&#25110;&#26376;&#32463;&#21608;&#26399;&#30340;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#28857;&#21487;&#33021;&#20250;&#26292;&#38706;&#20986;&#24739;&#26377;&#21512;&#24182;&#30151;&#25110;&#32456;&#27490;&#22922;&#23072;&#30340;&#24739;&#32773;&#65292;&#20405;&#29359;&#20854;&#38544;&#31169;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#39044;&#27979;&#22810;&#22218;&#21365;&#24034;&#32508;&#21512;&#24449;&#65288;PCOS&#65289;&#24739;&#32773;&#30340;&#26368;&#20339;&#33647;&#29289;&#26041;&#38754;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#26041;&#27861;&#12290;PCOS&#26159;&#19968;&#31181;&#24433;&#21709;&#20840;&#29699;&#25968;&#30334;&#19975;&#22899;&#24615;&#30340;&#20005;&#37325;&#28608;&#32032;&#22833;&#35843;&#30142;&#30149;&#65292;&#20294;&#20854;&#30740;&#31350;&#21463;&#38480;&#20110;&#24739;&#32773;&#25968;&#25454;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#21512;&#25104;&#30340;PCOS&#24739;&#32773;&#25968;&#25454;&#38598;&#19978;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#31181;&#35775;&#38382;&#22823;&#37327;&#22810;&#26679;&#25968;&#25454;&#24182;&#35782;&#21035;&#26368;&#26377;&#25928;&#27835;&#30103;&#36873;&#39033;&#30340;&#24037;&#20855;&#65292;&#21516;&#26102;&#25552;&#20379;PCOS&#24739;&#32773;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of women's endocrinology has trailed behind data-driven medical solutions, largely due to concerns over the privacy of patient data. Valuable datapoints about hormone levels or menstrual cycling could expose patients who suffer from comorbidities or terminate a pregnancy, violating their privacy. We explore the application of Federated Learning (FL) to predict the optimal drug for patients with polycystic ovary syndrome (PCOS). PCOS is a serious hormonal disorder impacting millions of women worldwide, yet it's poorly understood and its research is stunted by a lack of patient data. We demonstrate that a variety of FL approaches succeed on a synthetic PCOS patient dataset. Our proposed FL models are a tool to access massive quantities of diverse data and identify the most effective treatment option while providing PCOS patients with privacy guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#22312;&#22823;&#27169;&#22411;&#26102;&#20195;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#28608;&#21169;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11217</link><description>&lt;p&gt;
&#22823;&#27169;&#22411;&#26102;&#20195;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#22823;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models. (arXiv:2308.11217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#22312;&#22823;&#27169;&#22411;&#26102;&#20195;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#28608;&#21169;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#33021;&#22815;&#20840;&#38754;&#24863;&#30693;&#21644;&#35782;&#21035;&#29289;&#29702;&#19990;&#30028;&#65292;&#24050;&#25104;&#20026;&#36890;&#24448;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#22823;&#27169;&#22411;&#22312;&#29305;&#23450;&#24037;&#19994;&#39046;&#22495;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#29702;&#24819;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#22810;&#20010;&#20225;&#19994;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#20316;&#32773;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#27169;&#22411;&#26102;&#20195;&#32852;&#37030;&#23398;&#20064;&#30340;&#26234;&#33021;&#22522;&#30784;&#21644;&#30446;&#26631;&#30340;&#25112;&#30053;&#36716;&#21464;&#65292;&#20197;&#21450;&#22312;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#21644;&#28608;&#21169;&#26426;&#21046;&#26041;&#38754;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#39046;&#20808;&#20225;&#19994;&#22312;&#22478;&#24066;&#23433;&#20840;&#36816;&#33829;&#31649;&#29702;&#26041;&#38754;&#36129;&#29486;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#19987;&#23478;&#30693;&#35782;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#37096;&#32626;&#21644;&#39640;&#25928;&#24615;&#33021;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hamiltonian&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37197;&#32622;&#31354;&#38388;&#26144;&#23556;&#21644;&#36816;&#21160;&#27169;&#22411;&#26469;&#29983;&#25104;&#21512;&#29702;&#30340;&#35270;&#39057;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#20419;&#36827;&#23545;&#26368;&#23567;&#37197;&#32622;&#31354;&#38388;&#30340;&#34920;&#31034;&#21644;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11216</link><description>&lt;p&gt;
Hamiltonian GAN. (arXiv:2308.11216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Hamiltonian GAN. (arXiv:2308.11216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11216
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hamiltonian&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37197;&#32622;&#31354;&#38388;&#26144;&#23556;&#21644;&#36816;&#21160;&#27169;&#22411;&#26469;&#29983;&#25104;&#21512;&#29702;&#30340;&#35270;&#39057;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#20419;&#36827;&#23545;&#26368;&#23567;&#37197;&#32622;&#31354;&#38388;&#30340;&#34920;&#31034;&#21644;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31995;&#21015;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#21033;&#29992;Hamiltonian&#24418;&#24335;&#20027;&#20041;&#20316;&#20026;&#29289;&#29702;&#19978;&#21512;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#35270;&#39057;&#29983;&#25104;&#30340;&#24402;&#32435;&#20559;&#35265;&#12290;Hamiltonian&#30340;&#32467;&#26500;&#30830;&#20445;&#20102;&#23398;&#20064;&#21040;&#30340;&#25968;&#37327;&#65288;&#22914;&#33021;&#37327;&#65289;&#30340;&#23432;&#24658;&#65292;&#24182;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#32473;&#36755;&#20837;&#35270;&#39057;&#26045;&#21152;&#20102;&#19968;&#20010;&#30456;&#31354;&#38388;&#35299;&#37322;&#12290;&#34429;&#28982;&#36825;&#31181;&#35299;&#37322;&#28508;&#22312;&#22320;&#26377;&#21161;&#20110;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#25972;&#21512;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#21487;&#34892;&#24615;&#19978;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#22312;&#35774;&#35745;&#26102;&#23545;&#37197;&#32622;&#31354;&#38388;&#30340;&#32467;&#26500;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#35270;&#39057;&#29983;&#25104;&#27969;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;&#37197;&#32622;&#31354;&#38388;&#26144;&#23556;&#21644;Hamiltonian&#31070;&#32463;&#32593;&#32476;&#36816;&#21160;&#27169;&#22411;&#65292;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#37197;&#32622;&#31354;&#38388;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#24490;&#29615;&#22352;&#26631;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36825;&#21487;&#20197;&#20419;&#36827;&#23545;&#37197;&#32622;&#31354;&#38388;&#30340;&#26368;&#23567;&#34920;&#31034;&#24182;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of work leverages the Hamiltonian formalism as an inductive bias for physically plausible neural network based video generation. The structure of the Hamiltonian ensures conservation of a learned quantity (e.g., energy) and imposes a phase-space interpretation on the low-dimensional manifold underlying the input video. While this interpretation has the potential to facilitate the integration of learned representations in downstream tasks, existing methods are limited in their applicability as they require a structural prior for the configuration space at design time. In this work, we present a GAN-based video generation pipeline with a learned configuration space map and Hamiltonian neural network motion model, to learn a representation of the configuration space from data. We train our model with a physics-inspired cyclic-coordinate loss function which encourages a minimal representation of the configuration space and improves interpretability. We demonstrate the effica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;&#31354;&#38388; - &#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#20851;&#31995;&#23398;&#20064;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#26469;&#26377;&#25928;&#22320;&#24314;&#31435;&#22810;&#27169;&#24577;&#20043;&#38388;&#30340;&#36830;&#25509;&#24182;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11204</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#24314;&#27169;&#30340;&#31616;&#21333;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simple Framework for Multi-mode Spatial-Temporal Data Modeling. (arXiv:2308.11204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;&#31354;&#38388; - &#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#20851;&#31995;&#23398;&#20064;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#26469;&#26377;&#25928;&#22320;&#24314;&#31435;&#22810;&#27169;&#24577;&#20043;&#38388;&#30340;&#36830;&#25509;&#24182;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388; - &#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#26088;&#22312;&#25366;&#25496;&#31995;&#32479;&#20013;&#23545;&#35937;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38598;&#20013;&#20110;&#21333;&#27169;&#24335;&#19979;&#30340;&#31354;&#38388; - &#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#65292;&#32570;&#20047;&#23545;&#22810;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#24456;&#23569;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#22810;&#27169;&#24335;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#24314;&#31435;&#22312;&#22797;&#26434;&#30340;&#32452;&#20214;&#19978;&#65292;&#20351;&#27169;&#22411;&#22797;&#26434;&#24615;&#26356;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;&#31354;&#38388; - &#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#20860;&#20855;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36328;&#27169;&#24577;&#31354;&#38388;&#20851;&#31995;&#23398;&#20064;&#32452;&#20214;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24314;&#31435;&#22810;&#27169;&#24577;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24182;&#27839;&#30528;&#23398;&#20064;&#21040;&#30340;&#36830;&#25509;&#20256;&#25773;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#26469;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#36825;&#22312;&#27010;&#24565;&#19978;&#21644;&#25216;&#26415;&#19978;&#37117;&#24456;&#31616;&#27905;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal data modeling aims to mine the underlying spatial relationships and temporal dependencies of objects in a system. However, most existing methods focus on the modeling of spatial-temporal data in a single mode, lacking the understanding of multiple modes. Though very few methods have been presented to learn the multi-mode relationships recently, they are built on complicated components with higher model complexities. In this paper, we propose a simple framework for multi-mode spatial-temporal data modeling to bring both effectiveness and efficiency together. Specifically, we design a general cross-mode spatial relationships learning component to adaptively establish connections between multiple modes and propagate information along the learned connections. Moreover, we employ multi-layer perceptrons to capture the temporal dependencies and channel correlations, which are conceptually and technically succinct. Experiments on three real-world datasets show that our model 
&lt;/p&gt;</description></item><item><title>SegRNN&#26159;&#19968;&#31181;&#38024;&#23545;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#30340;&#20998;&#27573;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20004;&#31181;&#26032;&#31574;&#30053;&#65288;&#20998;&#27573;&#36845;&#20195;&#21644;&#24182;&#34892;&#22810;&#27493;&#39044;&#27979;&#65289;&#26174;&#33879;&#20943;&#23569;&#20102;&#24490;&#29615;&#36845;&#20195;&#27425;&#25968;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;SegRNN&#19981;&#20165;&#34920;&#29616;&#26356;&#22909;&#65292;&#36824;&#22823;&#24133;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.11200</link><description>&lt;p&gt;
SegRNN: &#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20998;&#27573;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting. (arXiv:2308.11200v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11200
&lt;/p&gt;
&lt;p&gt;
SegRNN&#26159;&#19968;&#31181;&#38024;&#23545;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#30340;&#20998;&#27573;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20004;&#31181;&#26032;&#31574;&#30053;&#65288;&#20998;&#27573;&#36845;&#20195;&#21644;&#24182;&#34892;&#22810;&#27493;&#39044;&#27979;&#65289;&#26174;&#33879;&#20943;&#23569;&#20102;&#24490;&#29615;&#36845;&#20195;&#27425;&#25968;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;SegRNN&#19981;&#20165;&#34920;&#29616;&#26356;&#22909;&#65292;&#36824;&#22823;&#24133;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#39046;&#22495;&#20013;&#65292;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#36807;&#38271;&#30340;&#22238;&#28335;&#31383;&#21475;&#21644;&#39044;&#27979;&#33539;&#22260;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#35813;&#39046;&#22495;&#30340;&#20027;&#23548;&#22320;&#20301;&#24050;&#32463;&#36716;&#21521;Transformer&#12289;MLP&#21644;CNN&#26041;&#27861;&#12290;RNN&#22312;LTSF&#20013;&#23384;&#22312;&#38480;&#21046;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#24490;&#29615;&#36845;&#20195;&#30340;&#25968;&#37327;&#30456;&#24403;&#22810;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20943;&#23569;RNN&#22312;LTSF&#20219;&#21153;&#20013;&#36845;&#20195;&#27425;&#25968;&#30340;&#26032;&#31574;&#30053;&#65306;&#20998;&#27573;&#36845;&#20195;&#21644;&#24182;&#34892;&#22810;&#27493;&#39044;&#27979;&#65288;PMF&#65289;&#12290;&#23558;&#36825;&#20123;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#30340;SegRNN&#26174;&#33879;&#20943;&#23569;&#20102;LTSF&#25152;&#38656;&#30340;&#24490;&#29615;&#36845;&#20195;&#27425;&#25968;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;SegRNN&#19981;&#20165;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36824;&#23558;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#20943;&#23569;&#20102;&#36229;&#36807;78%&#12290;&#36825;&#20123;&#25104;&#26524;&#20026;RNN&#22312;LTSF&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
RNN-based methods have faced challenges in the Long-term Time Series Forecasting (LTSF) domain when dealing with excessively long look-back windows and forecast horizons. Consequently, the dominance in this domain has shifted towards Transformer, MLP, and CNN approaches. The substantial number of recurrent iterations are the fundamental reasons behind the limitations of RNNs in LTSF. To address these issues, we propose two novel strategies to reduce the number of iterations in RNNs for LTSF tasks: Segment-wise Iterations and Parallel Multi-step Forecasting (PMF). RNNs that combine these strategies, namely SegRNN, significantly reduce the required recurrent iterations for LTSF, resulting in notable improvements in forecast accuracy and inference speed. Extensive experiments demonstrate that SegRNN not only outperforms SOTA Transformer-based models but also reduces runtime and memory usage by more than 78%. These achievements provide strong evidence that RNNs continue to excel in LTSF ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConcatPlexer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#20351;&#29992;&#38468;&#21152;&#30340;Dim1&#25209;&#22788;&#29702;&#65288;&#21363;&#36830;&#25509;&#65289;&#26469;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#21463;&#21040;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2308.11199</link><description>&lt;p&gt;
ConcatPlexer&#65306;&#36890;&#36807;&#38468;&#21152;Dim1&#25209;&#22788;&#29702;&#20197;&#21152;&#24555;ViTs&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
ConcatPlexer: Additional Dim1 Batching for Faster ViTs. (arXiv:2308.11199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConcatPlexer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#20351;&#29992;&#38468;&#21152;&#30340;Dim1&#25209;&#22788;&#29702;&#65288;&#21363;&#36830;&#25509;&#65289;&#26469;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#21463;&#21040;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36824;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24341;&#21457;&#20102;&#21508;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;Transformer&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#24314;&#27169;&#28789;&#27963;&#24615;&#24102;&#26469;&#20102;&#35745;&#31639;&#25104;&#26412;&#30340;&#20005;&#37325;&#22686;&#21152;&#65292;&#22240;&#27492;&#26377;&#20960;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20943;&#23569;&#36825;&#31181;&#36127;&#25285;&#30340;&#26041;&#27861;&#12290;&#21463;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#31181;&#20943;&#23569;&#25104;&#26412;&#30340;&#26041;&#27861;Data Multiplexing (DataMUX)&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#35270;&#35273;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#38468;&#21152;&#30340;Dim1&#25209;&#22788;&#29702;&#65288;&#21363;&#36830;&#25509;&#65289;&#65292;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#22522;&#30784;&#19978;&#22823;&#22823;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#35270;&#35273;&#27169;&#22411;&#24341;&#20837;&#20102;DataMux&#30340;&#19968;&#31181;&#22825;&#28982;&#36866;&#24212;&#26041;&#27861;&#65292;&#22270;&#20687;&#22810;&#36335;&#22797;&#29992;&#22120;&#65288;Image Multiplexer&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#32452;&#20214;&#26469;&#20811;&#26381;&#20854;&#32570;&#28857;&#65292;&#36827;&#32780;&#24418;&#25104;&#20102;&#25105;&#20204;&#26368;&#32456;&#30340;&#27169;&#22411;ConcatPlexer&#65292;&#22312;&#25512;&#29702;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#20043;&#38388;&#25214;&#21040;&#20102;&#24179;&#34913;&#28857;&#12290;ConcatPlexer&#22312;ImageNet1K&#21644;CIFAR100&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated tremendous success not only in the natural language processing (NLP) domain but also the field of computer vision, igniting various creative approaches and applications. Yet, the superior performance and modeling flexibility of transformers came with a severe increase in computation costs, and hence several works have proposed methods to reduce this burden. Inspired by a cost-cutting method originally proposed for language models, Data Multiplexing (DataMUX), we propose a novel approach for efficient visual recognition that employs additional dim1 batching (i.e., concatenation) that greatly improves the throughput with little compromise in the accuracy. We first introduce a naive adaptation of DataMux for vision models, Image Multiplexer, and devise novel components to overcome its weaknesses, rendering our final model, ConcatPlexer, at the sweet spot between inference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and CIFAR100 dataset and
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#23450;&#37327;&#35777;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#36827;&#34892;&#21151;&#25928;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12289;&#29305;&#24449;&#21644;&#27169;&#22411;&#32500;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#32622;&#20449;&#24230;&#12290;&#21516;&#26102;&#65292;&#30830;&#23450;&#20102;&#33719;&#24471;&#32479;&#35745;&#26174;&#33879;&#32467;&#26524;&#25152;&#38656;&#30340;&#26368;&#23567;&#26679;&#26412;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.11197</link><description>&lt;p&gt;
&#22312;&#35821;&#38899;&#12289;&#35821;&#35328;&#21644;&#21548;&#21147;&#31185;&#23398;&#20013;&#24314;&#31435;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#21151;&#25928;&#20998;&#26512;&#21644;&#26679;&#26412;&#23481;&#37327;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation. (arXiv:2308.11197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#23450;&#37327;&#35777;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#36827;&#34892;&#21151;&#25928;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12289;&#29305;&#24449;&#21644;&#27169;&#22411;&#32500;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#32622;&#20449;&#24230;&#12290;&#21516;&#26102;&#65292;&#30830;&#23450;&#20102;&#33719;&#24471;&#32479;&#35745;&#26174;&#33879;&#32467;&#26524;&#25152;&#38656;&#30340;&#26368;&#23567;&#26679;&#26412;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30340;&#31532;&#19968;&#20010;&#30446;&#30340;&#26159;&#25552;&#20379;&#23450;&#37327;&#35777;&#25454;&#65292;&#20197;&#28608;&#21169;&#30740;&#31350;&#20154;&#21592;&#25913;&#29992;&#26356;&#20581;&#22766;&#30340;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12290;&#31532;&#20108;&#20010;&#30446;&#30340;&#26159;&#22312;&#30740;&#31350;&#35774;&#35745;&#36807;&#31243;&#20013;&#25552;&#20986;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#30340;&#21151;&#25928;&#20998;&#26512;&#26041;&#27861;&#21644;MATLAB&#20195;&#30721;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#37327;&#21270;&#20102;&#25152;&#37319;&#29992;&#30340;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12289;&#29305;&#24449;&#30340;&#21028;&#21035;&#21147;&#12289;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#24230;&#21644;&#27169;&#22411;&#30340;&#32500;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#32479;&#35745;&#32622;&#20449;&#24230;&#65292;&#27604;&#36739;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#65288;&#21333;&#19968;&#30041;&#20986;&#27861;&#12289;10&#25240;&#20132;&#21449;&#39564;&#35777;&#12289;&#35757;&#32451;-&#39564;&#35777;-&#27979;&#35797;&#27861;&#21644;&#23884;&#22871;10&#25240;&#20132;&#21449;&#39564;&#35777;&#65289;&#12290;&#21033;&#29992;&#38646;&#20551;&#35774;&#21644;&#22791;&#25321;&#20551;&#35774;&#30340;&#20998;&#24067;&#30830;&#23450;&#20102;&#33719;&#24471;&#32479;&#35745;&#26174;&#33879;&#32467;&#26524;&#25152;&#38656;&#30340;&#26368;&#23567;&#26679;&#26412;&#23481;&#37327;&#65288;&#945;=0.05&#65292;1-&#946;=0.8&#65289;&#12290;&#27169;&#22411;&#30340;&#32479;&#35745;&#32622;&#20449;&#24230;&#34987;&#23450;&#20041;&#20026;&#27491;&#30830;&#29305;&#24449;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study's first purpose is to provide quantitative evidence that would incentivize researchers to instead use the more robust method of nested cross-validation. The second purpose is to present methods and MATLAB codes for doing power analysis for ML-based analysis during the design of a study. Monte Carlo simulations were used to quantify the interactions between the employed cross-validation method, the discriminative power of features, the dimensionality of the feature space, and the dimensionality of the model. Four different cross-validations (single holdout, 10-fold, train-validation-test, and nested 10-fold) were compared based on the statistical power and statistical confidence of the ML models. Distributions of the null and alternative hypotheses were used to determine the minimum required sample size for obtaining a statistically significant outcome ({\alpha}=0.05, 1-\b{eta}=0.8). Statistical confidence of the model was defined as the probability of correct features being 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;ML&#25968;&#25454;&#27969;&#22270;&#36827;&#34892;&#20851;&#38190;&#36335;&#24452;&#32447;&#24615;&#32858;&#31867;&#21644;&#20219;&#21153;&#24182;&#34892;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;ML/DL&#27169;&#22411;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#24037;&#20855;Ramiel&#29983;&#25104;&#21487;&#35835;&#21644;&#21487;&#25191;&#34892;&#30340;&#24182;&#34892;Pytorch+Python&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2308.11192</link><description>&lt;p&gt;
&#33258;&#21160;&#24182;&#34892;&#21270;ML/DL&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27969;&#22270;
&lt;/p&gt;
&lt;p&gt;
Automatic Task Parallelization of Dataflow Graphs in ML/DL models. (arXiv:2308.11192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11192
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;ML&#25968;&#25454;&#27969;&#22270;&#36827;&#34892;&#20851;&#38190;&#36335;&#24452;&#32447;&#24615;&#32858;&#31867;&#21644;&#20219;&#21153;&#24182;&#34892;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;ML/DL&#27169;&#22411;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#24037;&#20855;Ramiel&#29983;&#25104;&#21487;&#35835;&#21644;&#21487;&#25191;&#34892;&#30340;&#24182;&#34892;Pytorch+Python&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#23384;&#22312;&#22810;&#31181;&#26041;&#27861;&#29992;&#20110;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;(ML)&#25110;&#28145;&#24230;&#23398;&#20064;(DL)&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20381;&#36182;&#20110;&#21508;&#31181;&#22270;&#24418;&#21644;&#36816;&#31639;&#31526;&#24182;&#34892;&#24615;&#26041;&#27861;&#30340;&#29616;&#20195;&#25216;&#26415;&#20381;&#36182;&#20110;&#25628;&#32034;&#31354;&#38388;&#20248;&#21270;&#65292;&#36825;&#22312;&#21151;&#32791;&#21644;&#30828;&#20214;&#20351;&#29992;&#26041;&#38754;&#20195;&#20215;&#39640;&#26114;&#12290;&#29305;&#21035;&#26159;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#24403;&#25209;&#37327;&#22823;&#23567;&#20026;1&#19988;&#22312;CPU&#19978;&#25191;&#34892;&#25110;&#29992;&#20110;&#21151;&#32791;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#26102;&#65292;&#29616;&#26377;&#25216;&#26415;&#21487;&#33021;&#21464;&#24471;&#26114;&#36149;&#12289;&#22797;&#26434;&#25110;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#36335;&#24452;&#30340;&#32447;&#24615;&#32858;&#31867;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;ML&#25968;&#25454;&#27969;&#22270;&#20013;&#30340;&#20869;&#22312;&#24182;&#34892;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#20219;&#21153;&#24182;&#34892;&#21270;&#26041;&#27861;&#36890;&#36807;&#20811;&#38534;&#20248;&#21270;&#22270;&#30340;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#24120;&#37327;&#20256;&#25773;&#21644;&#27515;&#20195;&#30721;&#28040;&#38500;&#23545;&#20854;&#36827;&#34892;&#20462;&#21098;&#12290;&#19982;&#20854;&#20182;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;Ramiel&#30340;&#26032;&#24037;&#20855;&#65292;&#20174;ONNX&#26684;&#24335;&#30340;&#36755;&#20837;ML&#27169;&#22411;&#29983;&#25104;&#21487;&#35835;&#21644;&#21487;&#25191;&#34892;&#30340;&#24182;&#34892;Pytorch+Python&#20195;&#30721;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Several methods exist today to accelerate Machine Learning(ML) or Deep-Learning(DL) model performance for training and inference. However, modern techniques that rely on various graph and operator parallelism methodologies rely on search space optimizations which are costly in terms of power and hardware usage. Especially in the case of inference, when the batch size is 1 and execution is on CPUs or for power-constrained edge devices, current techniques can become costly, complicated or inapplicable. To ameliorate this, we present a Critical-Path-based Linear Clustering approach to exploit inherent parallel paths in ML dataflow graphs. Our task parallelization approach further optimizes the structure of graphs via cloning and prunes them via constant propagation and dead-code elimination. Contrary to other work, we generate readable and executable parallel Pytorch+Python code from input ML models in ONNX format via a new tool that we have built called {\bf Ramiel}. This allows us to be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24212;&#22810;&#26679;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38169;&#35823;&#37327;&#21270;&#25351;&#26631;&#65292;&#36825;&#20123;&#25351;&#26631;&#29420;&#31435;&#20110;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#19982;&#22833;&#36133;&#27010;&#29575;&#24378;&#30456;&#20851;&#12290;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25351;&#26631;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.11189</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#25351;&#26631;&#65306;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#20013;&#22833;&#36133;&#30340;&#39046;&#22495;&#26080;&#20851;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries. (arXiv:2308.11189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24212;&#22810;&#26679;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38169;&#35823;&#37327;&#21270;&#25351;&#26631;&#65292;&#36825;&#20123;&#25351;&#26631;&#29420;&#31435;&#20110;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#19982;&#22833;&#36133;&#27010;&#29575;&#24378;&#30456;&#20851;&#12290;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25351;&#26631;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#39044;&#27979;&#36890;&#24120;&#20381;&#36182;&#20110;&#39046;&#22495;&#29305;&#23450;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24212;&#22810;&#26679;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38169;&#35823;&#37327;&#21270;&#25351;&#26631;&#65292;&#22240;&#27492;&#29420;&#31435;&#20110;&#24213;&#23618;&#24212;&#29992;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#29109;&#12289;&#22522;&#23612;&#19981;&#32431;&#24230;&#21644;&#36136;&#24515;&#36317;&#31163;&#30340;&#19977;&#20010;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#65292;&#28041;&#21450;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#28201;&#24230;&#35774;&#32622;&#65292;&#35777;&#26126;&#36825;&#20123;&#25351;&#26631;&#19982;&#22833;&#36133;&#27010;&#29575;&#24378;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#25351;&#26631;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#38169;&#35823;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error prediction in large language models often relies on domain-specific information. In this paper, we present measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt - hence independent of the underlying application. We describe how three such measures - based on entropy, Gini impurity, and centroid distance can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#23383;&#30284;&#30151;&#32452;&#32455;&#20013;&#22810;&#22120;&#23448;&#32454;&#32990;&#26680;&#30340;&#21516;&#26102;&#35821;&#20041;&#20998;&#21106;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#35299;&#30721;&#22120;&#22836;&#37096;&#65292;&#24182;&#21033;&#29992;&#29420;&#31435;&#30340;&#21152;&#26435;&#25439;&#22833;&#26469;&#20135;&#29983;&#35821;&#20041;&#20998;&#21106;&#12289;&#36793;&#30028;&#25552;&#35758;&#21644;&#20998;&#31867;&#22270;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#35821;&#20041;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25193;&#23637;&#21040;&#20102;&#21516;&#26102;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11179</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25968;&#23383;&#30284;&#30151;&#32452;&#32455;&#20013;&#22810;&#22120;&#23448;&#32454;&#32990;&#26680;&#30340;&#21516;&#26102;&#35821;&#20041;&#20998;&#21106;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#33258;&#19979;&#32780;&#19978;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A three in one bottom-up framework for simultaneous semantic segmentation, instance segmentation and classification of multi-organ nuclei in digital cancer histology. (arXiv:2308.11179v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#23383;&#30284;&#30151;&#32452;&#32455;&#20013;&#22810;&#22120;&#23448;&#32454;&#32990;&#26680;&#30340;&#21516;&#26102;&#35821;&#20041;&#20998;&#21106;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#35299;&#30721;&#22120;&#22836;&#37096;&#65292;&#24182;&#21033;&#29992;&#29420;&#31435;&#30340;&#21152;&#26435;&#25439;&#22833;&#26469;&#20135;&#29983;&#35821;&#20041;&#20998;&#21106;&#12289;&#36793;&#30028;&#25552;&#35758;&#21644;&#20998;&#31867;&#22270;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#35821;&#20041;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25193;&#23637;&#21040;&#20102;&#21516;&#26102;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#32452;&#32455;&#23398;&#20013;&#32454;&#32990;&#26680;&#30340;&#21516;&#26102;&#20998;&#21106;&#21644;&#20998;&#31867;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#30284;&#30151;&#35786;&#26029;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#35821;&#20041;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#23558;&#27169;&#22411;&#25193;&#23637;&#21040;&#21516;&#26102;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#35299;&#30721;&#22120;&#22836;&#37096;&#65292;&#21033;&#29992;&#29420;&#31435;&#30340;&#21152;&#26435;&#25439;&#22833;&#20135;&#29983;&#35821;&#20041;&#20998;&#21106;&#12289;&#36793;&#30028;&#25552;&#35758;&#21644;&#20998;&#31867;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#19977;&#20010;&#22836;&#37096;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#36827;&#34892;&#32508;&#21512;&#20998;&#21106;&#21644;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous segmentation and classification of nuclei in digital histology play an essential role in computer-assisted cancer diagnosis; however, it remains challenging. The highest achieved binary and multi-class Panoptic Quality (PQ) remains as low as 0.68 bPQ and 0.49 mPQ, respectively. It is due to the higher staining variability, variability across the tissue, rough clinical conditions, overlapping nuclei, and nuclear class imbalance. The generic deep-learning methods usually rely on end-to-end models, which fail to address these problems associated explicitly with digital histology. In our previous work, DAN-NucNet, we resolved these issues for semantic segmentation with an end-to-end model. This work extends our previous model to simultaneous instance segmentation and classification. We introduce additional decoder heads with independent weighted losses, which produce semantic segmentation, edge proposals, and classification maps. We use the outputs from the three-head model to
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21021;&#27493;&#25506;&#32034;&#20102;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#22312;WHO&#20083;&#33146;&#20998;&#31867;&#20013;&#36827;&#34892;&#32959;&#30244;&#21306;&#20998;&#30340;&#25628;&#32034;&#21644;&#21305;&#37197;&#26041;&#27861;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#30340;&#28145;&#24230;&#29305;&#24449;&#23545;35&#31181;&#32959;&#30244;&#31867;&#22411;&#22312;&#25968;&#23383;&#22270;&#35889;&#20013;&#36827;&#34892;&#20102;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.11162</link><description>&lt;p&gt;
&#23545;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#22312;WHO&#20083;&#33146;&#20998;&#31867;&#20013;&#36827;&#34892;&#32959;&#30244;&#21306;&#20998;&#30340;&#25628;&#32034;&#21644;&#21305;&#37197;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Preliminary Investigation into Search and Matching for Tumour Discrimination in WHO Breast Taxonomy Using Deep Networks. (arXiv:2308.11162v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21021;&#27493;&#25506;&#32034;&#20102;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#22312;WHO&#20083;&#33146;&#20998;&#31867;&#20013;&#36827;&#34892;&#32959;&#30244;&#21306;&#20998;&#30340;&#25628;&#32034;&#21644;&#21305;&#37197;&#26041;&#27861;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#30340;&#28145;&#24230;&#29305;&#24449;&#23545;35&#31181;&#32959;&#30244;&#31867;&#22411;&#22312;&#25968;&#23383;&#22270;&#35889;&#20013;&#36827;&#34892;&#20102;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#26159;&#24433;&#21709;&#20840;&#29699;&#22899;&#24615;&#26368;&#24120;&#35265;&#30340;&#30284;&#30151;&#20043;&#19968;&#12290;&#23427;&#20204;&#21253;&#25324;&#19968;&#32452;&#20855;&#26377;&#21508;&#31181;&#29983;&#29289;&#23398;&#12289;&#20020;&#24202;&#21644;&#32452;&#32455;&#30149;&#29702;&#23398;&#29305;&#24449;&#30340;&#24694;&#24615;&#26032;&#29983;&#29289;&#12290;&#26377;35&#22810;&#31181;&#19981;&#21516;&#30340;&#20083;&#33146;&#30149;&#21464;&#30340;&#32452;&#32455;&#23398;&#24418;&#24335;&#21487;&#26681;&#25454;&#32454;&#32990;&#24418;&#24577;&#23398;&#12289;&#29983;&#38271;&#21644;&#32452;&#32455;&#26500;&#26550;&#27169;&#24335;&#36827;&#34892;&#32452;&#32455;&#23398;&#20998;&#31867;&#21644;&#35786;&#26029;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#28145;&#24230;&#23398;&#20064;&#23545;&#21307;&#23398;&#22270;&#20687;&#30340;&#35745;&#31639;&#34920;&#31034;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#21487;&#25628;&#32034;&#30340;&#25968;&#23383;&#22270;&#35889;&#21487;&#20197;&#25552;&#20379;&#32473;&#30149;&#29702;&#23398;&#23478;&#21487;&#20197;&#25628;&#32034;&#26126;&#30830;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#26723;&#26696;&#26696;&#20363;&#30340;&#34917;&#19969;&#21305;&#37197;&#24037;&#20855;&#65292;&#36825;&#39033;&#25216;&#26415;&#21487;&#20197;&#34987;&#35270;&#20026;&#35745;&#31639;&#26426;&#30340;&#31532;&#20108;&#24847;&#35265;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;35&#31181;&#32959;&#30244;&#31867;&#22411;&#30340;WHO&#20083;&#33146;&#20998;&#31867;&#36827;&#34892;&#20102;&#32034;&#24341;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#30340;&#28145;&#24230;&#29305;&#24449;&#21487;&#35270;&#21270;&#20102;&#25152;&#26377;&#32959;&#30244;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Breast cancer is one of the most common cancers affecting women worldwide. They include a group of malignant neoplasms with a variety of biological, clinical, and histopathological characteristics. There are more than 35 different histological forms of breast lesions that can be classified and diagnosed histologically according to cell morphology, growth, and architecture patterns. Recently, deep learning, in the field of artificial intelligence, has drawn a lot of attention for the computerized representation of medical images. Searchable digital atlases can provide pathologists with patch matching tools allowing them to search among evidently diagnosed and treated archival cases, a technology that may be regarded as computational second opinion. In this study, we indexed and analyzed the WHO breast taxonomy (Classification of Tumours 5th Ed.) spanning 35 tumour types. We visualized all tumour types using deep features extracted from a state-of-the-art deep learning model, pre-trained
&lt;/p&gt;</description></item><item><title>&#22312;&#31070;&#32463;&#21147;&#22330;&#27169;&#22411;&#20013;&#65292;&#24120;&#29992;&#30340;MD17&#25968;&#25454;&#38598;&#23545;&#20110;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#26679;&#33258;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#21253;&#21547;&#20102;&#33021;&#37327;&#21644;&#21147;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.11155</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#20986;&#24179;&#34913;&#29366;&#24577;&#30340;&#25193;&#23637;&#21160;&#21147;&#23398;&#24615;&#33021;&#35780;&#20272;&#31070;&#32463;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium. (arXiv:2308.11155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11155
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#21147;&#22330;&#27169;&#22411;&#20013;&#65292;&#24120;&#29992;&#30340;MD17&#25968;&#25454;&#38598;&#23545;&#20110;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#26679;&#33258;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#21253;&#21547;&#20102;&#33021;&#37327;&#21644;&#21147;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21147;&#22330;&#24050;&#25104;&#20026;&#35745;&#31639;&#21270;&#23398;&#20013;&#30340;&#37325;&#35201;&#27169;&#22411;&#65292;&#21462;&#20195;&#20102;&#20174;&#22836;&#31639;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#20013;&#30340;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#12290;&#30446;&#21069;&#23545;&#31070;&#32463;&#21147;&#22330;&#30340;&#20027;&#35201;&#35780;&#20272;&#22522;&#20934;&#26159;MD17&#25968;&#25454;&#38598;&#21450;&#20854;&#21518;&#32493;&#25193;&#23637;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#20027;&#35201;&#21253;&#21547;&#26469;&#33258;&#22522;&#24577;&#21183;&#33021;&#38754;&#24179;&#34913;&#21306;&#22495;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#37319;&#26679;&#33258;&#30452;&#25509;&#32477;&#28909;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21270;&#23398;&#21453;&#24212;&#28041;&#21450;&#21040;&#36739;&#22823;&#30340;&#20998;&#23376;&#21464;&#24418;&#65292;&#29305;&#21035;&#26159;&#38190;&#26029;&#35010;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MD17&#25968;&#25454;&#38598;&#20013;&#20869;&#22352;&#26631;&#21644;&#33021;&#37327;&#30340;&#32422;&#26463;&#20998;&#24067;&#65292;&#20984;&#26174;&#20102;&#20854;&#22312;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#37319;&#26679;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#65288;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65289;&#25968;&#25454;&#38598;&#65292;&#20174;&#38750;&#32477;&#28909;&#21160;&#21147;&#23398;&#20013;&#27966;&#29983;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20174;&#22810;&#21442;&#32771;&#27874;&#20989;&#25968;&#29702;&#35770;&#21644;&#23494;&#24230;&#27867;&#20989;&#20013;&#30830;&#23450;&#30340;&#33021;&#37327;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31227;&#21160;&#24863;&#30693;&#35745;&#31639;&#21368;&#36733;&#20110;&#32676;&#20307;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#65292;&#28385;&#36275;&#24310;&#36831;&#35201;&#27714;&#24182;&#20445;&#35777;&#35745;&#31639;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;&#26368;&#23567;&#30340;&#26426;&#22120;&#20154;&#33021;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.11154</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31227;&#21160;&#24863;&#30693;&#35745;&#31639;&#21368;&#36733;&#20110;&#32676;&#20307;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mobility-Aware Computation Offloading for Swarm Robotics using Deep Reinforcement Learning. (arXiv:2308.11154v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11154
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31227;&#21160;&#24863;&#30693;&#35745;&#31639;&#21368;&#36733;&#20110;&#32676;&#20307;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#65292;&#28385;&#36275;&#24310;&#36831;&#35201;&#27714;&#24182;&#20445;&#35777;&#35745;&#31639;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;&#26368;&#23567;&#30340;&#26426;&#22120;&#20154;&#33021;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#26426;&#22120;&#20154;&#34987;&#35774;&#24819;&#20026;&#33021;&#33258;&#21160;&#21270;&#36827;&#34892;&#22823;&#37327;&#32942;&#33039;&#12289;&#21361;&#38505;&#21644;&#20047;&#21619;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#33021;&#37327;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#36890;&#20449;&#36164;&#28304;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#32676;&#20307;&#26426;&#22120;&#20154;&#21482;&#33021;&#25552;&#20379;&#26377;&#38480;&#30340;&#26102;&#31354;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#22522;&#20110;&#31227;&#21160;&#24863;&#30693;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#31471;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35745;&#31639;&#35843;&#24230;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26368;&#23567;&#30340;&#26426;&#22120;&#20154;&#33021;&#37327;&#26469;&#28385;&#36275;&#24310;&#36831;&#35201;&#27714;&#24182;&#20445;&#35777;&#35745;&#31639;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarm robotics is envisioned to automate a large number of dirty, dangerous, and dull tasks. Robots have limited energy, computation capability, and communication resources. Therefore, current swarm robotics have a small number of robots, which can only provide limited spatio-temporal information. In this paper, we propose to leverage the mobile edge computing to alleviate the computation burden. We develop an effective solution based on a mobility-aware deep reinforcement learning model at the edge server side for computing scheduling and resource. Our results show that the proposed approach can meet delay requirements and guarantee computation precision by using minimum robot energy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#33410;&#33021;&#30340;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#21355;&#26143;&#36890;&#20449;&#20013;&#30340;&#23616;&#20869;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38754;&#23545;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#20013;&#65292;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#33021;&#25928;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.11152</link><description>&lt;p&gt;
&#21355;&#26143;&#36890;&#20449;&#20013;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#33410;&#33021;&#23616;&#20869;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Energy-Efficient On-Board Radio Resource Management for Satellite Communications via Neuromorphic Computing. (arXiv:2308.11152v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#33410;&#33021;&#30340;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#21355;&#26143;&#36890;&#20449;&#20013;&#30340;&#23616;&#20869;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38754;&#23545;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#20013;&#65292;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#33021;&#25928;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#21355;&#26143;&#36890;&#20449;&#20219;&#21153;&#37319;&#29992;&#23436;&#20840;&#21487;&#37325;&#26500;&#30340;&#23616;&#20869;&#36719;&#20214;&#23450;&#20041;&#26377;&#25928;&#36733;&#33655;&#65292;&#33021;&#22815;&#26681;&#25454;&#31995;&#32479;&#36890;&#20449;&#30340;&#26102;&#31354;&#21464;&#21270;&#26469;&#35843;&#25972;&#26080;&#32447;&#36164;&#28304;&#12290;&#30001;&#20110;&#20248;&#21270;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#19988;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#25104;&#20026;&#20102;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24212;&#29992;&#22522;&#20110;&#33041;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#33410;&#33021;&#30340;&#23616;&#20869;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#12290;&#38500;&#20102;&#36719;&#20214;&#27169;&#25311;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;Intel Loihi 2&#33455;&#29255;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;Xilinx Versal VCK5000&#19978;&#23454;&#29616;&#20102;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#65292;&#24182;&#23545;&#19981;&#21516;&#36890;&#20449;&#38656;&#27714;&#19979;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#33021;&#25928;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#30456;&#20851;&#24037;&#20316;&#36127;&#36733;&#65292;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#22312;Loihi&#33455;&#29255;&#19978;&#30340;&#23454;&#29616;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
The latest satellite communication (SatCom) missions are characterized by a fully reconfigurable on-board software-defined payload, capable of adapting radio resources to the temporal and spatial variations of the system traffic. As pure optimization-based solutions have shown to be computationally tedious and to lack flexibility, machine learning (ML)-based methods have emerged as promising alternatives. We investigate the application of energy-efficient brain-inspired ML models for on-board radio resource management. Apart from software simulation, we report extensive experimental results leveraging the recently released Intel Loihi 2 chip. To benchmark the performance of the proposed model, we implement conventional convolutional neural networks (CNN) on a Xilinx Versal VCK5000, and provide a detailed comparison of accuracy, precision, recall, and energy efficiency for different traffic demands. Most notably, for relevant workloads, spiking neural networks (SNNs) implemented on Loih
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Reviewer&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#22312;&#20195;&#30721;&#23457;&#26597;&#39046;&#22495;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#20195;&#30721;&#23457;&#26597;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#19981;&#21040;1%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35813;&#26694;&#26550;&#20173;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11148</link><description>&lt;p&gt;
LLaMA-Reviewer: &#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#23457;&#26597;&#33258;&#21160;&#21270;&#20013;&#30340;&#24212;&#29992;&#65288;&#23454;&#35777;&#30740;&#31350;&#65289;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report). (arXiv:2308.11148v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Reviewer&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#22312;&#20195;&#30721;&#23457;&#26597;&#39046;&#22495;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#20195;&#30721;&#23457;&#26597;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#19981;&#21040;1%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35813;&#26694;&#26550;&#20173;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#23457;&#26597;&#27963;&#21160;&#30340;&#33258;&#21160;&#21270;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#36861;&#27714;&#65292;&#20027;&#35201;&#36890;&#36807;&#35768;&#22810;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34917;&#20805;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30528;&#36855;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#33258;&#21160;&#21270;&#20195;&#30721;&#23457;&#26597;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLaMA-Reviewer&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20102;&#27969;&#34892;&#30340;LLM&#8212;&#8212;LLaMA&#22312;&#20195;&#30721;&#23457;&#26597;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#32771;&#34385;&#21040;&#36164;&#28304;&#38480;&#21046;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#20197;&#26497;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25552;&#20379;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LLaMA-Reviewer&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#21482;&#20351;&#29992;&#19981;&#21040;1%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.  In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32454;&#32990;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#39564;&#33258;&#28608;&#27963;&#22270;&#29983;&#25104;&#20266;&#25513;&#33180;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32454;&#32990;&#20998;&#21106;&#21644;&#22810;&#31867;&#21035;&#32454;&#32990;&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11144</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#32454;&#32990;&#35782;&#21035;&#20013;&#30340;&#20808;&#39564;&#33258;&#28608;&#27963;&#22270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps. (arXiv:2308.11144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32454;&#32990;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#39564;&#33258;&#28608;&#27963;&#22270;&#29983;&#25104;&#20266;&#25513;&#33180;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32454;&#32990;&#20998;&#21106;&#21644;&#22810;&#31867;&#21035;&#32454;&#32990;&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22312;&#32454;&#32990;&#35782;&#21035;&#20219;&#21153;&#19978;&#65292;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#35814;&#32454;&#30340;&#27880;&#37322;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25104;&#21151;&#20943;&#23569;&#20102;&#23545;&#26631;&#31614;&#30340;&#20381;&#36182;&#24615;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#19968;&#20010;&#34917;&#19969;&#20013;&#21253;&#21547;&#30340;&#22823;&#37327;&#32454;&#32990;&#65292;&#26114;&#36149;&#32780;&#20302;&#25928;&#30340;&#26631;&#27880;&#20173;&#28982;&#19981;&#21487;&#36991;&#20813;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#26631;&#31614;&#26041;&#27861;&#26469;&#36827;&#34892;&#32454;&#32990;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20808;&#39564;&#33258;&#28608;&#27963;&#22270;&#65288;PSM&#65289;&#26469;&#29983;&#25104;&#20266;&#25513;&#33180;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35757;&#32451;&#19968;&#20010;&#28608;&#27963;&#32593;&#32476;&#12290;&#32593;&#32476;&#30340;&#27973;&#23618;&#20013;&#30340;&#26799;&#24230;&#20449;&#24687;&#34987;&#32858;&#21512;&#20197;&#29983;&#25104;&#20808;&#39564;&#33258;&#28608;&#27963;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#32858;&#31867;&#27169;&#22359;&#20316;&#20026;&#31649;&#36947;&#65292;&#23558;PSMs&#36716;&#25442;&#20026;&#20687;&#32032;&#32423;&#35821;&#20041;&#20266;&#25513;&#33180;&#65292;&#20197;&#20379;&#19979;&#28216;&#20219;&#21153;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#32452;&#32455;&#23398;&#25968;&#25454;&#38598;MoNuSeg&#65288;&#32454;&#32990;&#20998;&#21106;&#65289;&#21644;BCData&#65288;&#22810;&#31867;&#21035;&#32454;&#32990;&#26816;&#27979;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#20840;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of supervised deep learning models on cell recognition tasks relies on detailed annotations. Many previous works have managed to reduce the dependency on labels. However, considering the large number of cells contained in a patch, costly and inefficient labeling is still inevitable. To this end, we explored label-free methods for cell recognition. Prior self-activation maps (PSM) are proposed to generate pseudo masks as training targets. To be specific, an activation network is trained with self-supervised learning. The gradient information in the shallow layers of the network is aggregated to generate prior self-activation maps. Afterward, a semantic clustering module is then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo masks for downstream tasks. We evaluated our method on two histological datasets: MoNuSeg (cell segmentation) and BCData (multi-class cell detection). Compared with other fully-supervised and weakly-supervised methods, our metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#22270;&#32534;&#30721;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#25490;&#29699;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#25490;&#29699;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#20998;&#26512;&#25490;&#29699;&#23616;&#21183;&#30340;&#28508;&#22312;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.11142</link><description>&lt;p&gt;
&#25490;&#29699;&#20998;&#26512;&#30340;&#22270;&#32534;&#30721;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65306;&#20174;&#27604;&#36187;&#32467;&#26524;&#21040;&#20010;&#20307;&#25112;&#30053;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Encoding and Neural Network Approaches for Volleyball Analytics: From Game Outcome to Individual Play Predictions. (arXiv:2308.11142v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#22270;&#32534;&#30721;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#25490;&#29699;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#25490;&#29699;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#20998;&#26512;&#25490;&#29699;&#23616;&#21183;&#30340;&#28508;&#22312;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#22797;&#26434;&#25490;&#29699;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#25945;&#32451;&#21644;&#29699;&#21592;&#25552;&#20379;&#26356;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#22270;&#32534;&#30721;&#25216;&#26415;&#65292;&#22312;&#24050;&#26377;&#25490;&#29699;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#20026;&#27599;&#27425;&#25509;&#35302;&#28155;&#21152;&#38468;&#21152;&#30340;&#25490;&#29699;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20010;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#19977;&#31181;&#19981;&#21516;&#30340;&#25490;&#29699;&#39044;&#27979;&#20219;&#21153;&#65306;&#22238;&#21512;&#32467;&#26524;&#39044;&#27979;&#12289;&#23616;&#20301;&#32622;&#39044;&#27979;&#21644;&#20987;&#29699;&#31867;&#22411;&#39044;&#27979;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#19982;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#32467;&#26524;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25490;&#29699;&#25915;&#38450;&#20013;&#30340;&#28508;&#22312;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32467;&#21512;&#22270;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;GNN&#21487;&#26126;&#26174;&#25552;&#39640;&#25968;&#25454;&#30340;&#20998;&#26512;&#27700;&#24179;&#65292;&#20174;&#32780;&#24635;&#20307;&#19978;&#25552;&#39640;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#35843;&#25972;&#65292;&#22914;&#31227;&#38500;&#26576;&#20123;&#29305;&#24449;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#36825;&#20123;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims to improve the accuracy of complex volleyball predictions and provide more meaningful insights to coaches and players. We introduce a specialized graph encoding technique to add additional contact-by-contact volleyball context to an already available volleyball dataset without any additional data gathering. We demonstrate the potential benefits of using graph neural networks (GNNs) on this enriched dataset for three different volleyball prediction tasks: rally outcome prediction, set location prediction, and hit type prediction. We compare the performance of our graph-based models to baseline models and analyze the results to better understand the underlying relationships in a volleyball rally. Our results show that the use of GNNs with our graph encoding yields a much more advanced analysis of the data, which noticeably improves prediction results overall. We also show that these baseline tasks can be significantly improved with simple adjustments, such as removing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#30740;&#31350;&#20102;&#20351;&#29992;&#35780;&#35770;&#25968;&#25454;&#38598;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#30340;&#36138;&#23146;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#22522;&#20110;RL&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11137</link><description>&lt;p&gt;
&#38754;&#21521;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#38271;&#26399;&#29992;&#25143;&#21453;&#39304;&#39564;&#35777;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Validating Long-Term User Feedbacks in Interactive Recommendation Systems. (arXiv:2308.11137v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#30740;&#31350;&#20102;&#20351;&#29992;&#35780;&#35770;&#25968;&#25454;&#38598;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#30340;&#36138;&#23146;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#22522;&#20110;RL&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#65288;IRS&#65289;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#27169;&#25311;&#29992;&#25143;&#19982;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#36807;&#31243;&#12290;&#35768;&#22810;&#26041;&#27861;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#22240;&#20026;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#30452;&#25509;&#26368;&#22823;&#21270;&#29992;&#25143;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#22312;IRS&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20351;&#29992;&#20844;&#24320;&#30340;&#35780;&#35770;&#25968;&#25454;&#38598;&#26469;&#27604;&#36739;&#21644;&#35780;&#20272;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20844;&#24320;&#25968;&#25454;&#38598;&#20013;&#25552;&#20379;&#30340;&#29992;&#25143;&#21453;&#39304;&#21482;&#21253;&#25324;&#21363;&#26102;&#21453;&#24212;&#65288;&#20363;&#22914;&#65292;&#35780;&#20998;&#65289;&#65292;&#27809;&#26377;&#21253;&#25324;&#24310;&#36831;&#21453;&#24212;&#65288;&#20363;&#22914;&#20572;&#30041;&#26102;&#38388;&#21644;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;&#65289;&#12290;&#22240;&#27492;&#65292;&#38382;&#39064;&#22312;&#20110;&#36825;&#20123;&#35780;&#35770;&#25968;&#25454;&#38598;&#26159;&#21542;&#36866;&#21512;&#35780;&#20272;IRS&#30340;&#38271;&#26399;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#30740;&#31350;&#20102;&#20351;&#29992;&#35780;&#35770;&#25968;&#25454;&#38598;&#30340;IRS&#23454;&#39564;&#65292;&#24182;&#23558;&#22522;&#20110;RL&#30340;&#27169;&#22411;&#19982;&#19968;&#31181;&#31616;&#21333;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21518;&#32773;&#20197;&#36138;&#23146;&#30340;&#26041;&#24335;&#25512;&#33616;&#20855;&#26377;&#26368;&#39640;&#21333;&#27493;&#22870;&#21169;&#30340;&#39033;&#30446;&#12290;&#32463;&#36807;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#21457;&#29616;&#65306;&#39318;&#20808;&#65292;&#31616;&#21333;&#30340;&#36138;&#23146;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#22522;&#20110;RL&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive Recommender Systems (IRSs) have attracted a lot of attention, due to their ability to model interactive processes between users and recommender systems. Numerous approaches have adopted Reinforcement Learning (RL) algorithms, as these can directly maximize users' cumulative rewards. In IRS, researchers commonly utilize publicly available review datasets to compare and evaluate algorithms. However, user feedback provided in public datasets merely includes instant responses (e.g., a rating), with no inclusion of delayed responses (e.g., the dwell time and the lifetime value). Thus, the question remains whether these review datasets are an appropriate choice to evaluate the long-term effects of the IRS. In this work, we revisited experiments on IRS with review datasets and compared RL-based models with a simple reward model that greedily recommends the item with the highest one-step reward. Following extensive analysis, we can reveal three main findings: First, a simple greedy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11129</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#36317;&#31163;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#21387;&#22120;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25552;&#35758;&#24456;&#23569;&#28041;&#21450;&#25429;&#25417;&#26356;&#38271;&#36317;&#31163;&#12289;&#23618;&#27425;&#32467;&#26500;&#25110;&#31038;&#21306;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#32780;&#36825;&#20123;&#22312;&#20998;&#23376;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#24341;&#29992;&#32593;&#32476;&#31561;&#21508;&#31181;&#22270;&#24418;&#20013;&#37117;&#20250;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#23618;&#27425;&#36317;&#31163;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22810;&#23618;&#27425;&#12289;&#23618;&#27425;&#21270;&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#21487;&#20197;&#28789;&#27963;&#19982;&#29616;&#26377;&#22270;&#21464;&#21387;&#22120;&#38598;&#25104;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#20301;&#32622;&#34920;&#31034;&#21516;&#26102;&#24212;&#29992;&#12290;&#36890;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;HDSE&#26041;&#27861;&#25104;&#21151;&#25552;&#21319;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#32447;&#21464;&#21387;&#22120;&#65292;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11127</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#22810;&#24378;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Expressive are Graph Neural Networks in Recommendation?. (arXiv:2308.11127v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#21033;&#29992;&#22270;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#21327;&#20316;&#36807;&#28388;&#20449;&#21495;&#36827;&#34892;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#32463;&#39564;&#26377;&#25928;&#24615;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#30340;&#33021;&#21147;&#30340;&#29702;&#35770;&#34920;&#36848;&#38750;&#24120;&#31232;&#23569;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;GNNs&#30340;&#19968;&#33324;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;GNNs&#33267;&#22810;&#19982;Weisfeiler-Lehman&#27979;&#35797;&#19968;&#26679;&#24378;&#22823;&#65292;&#24182;&#19988;&#19982;&#38543;&#26426;&#33410;&#28857;&#21021;&#22987;&#21270;&#30456;&#32467;&#21512;&#30340;GNNs&#26159;&#36890;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#8220;&#34920;&#36798;&#33021;&#21147;&#8221;&#27010;&#24565;&#20173;&#28982;&#23450;&#20041;&#27169;&#31946;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#37319;&#29992;&#22270;&#21516;&#26500;&#27979;&#35797;&#20316;&#20026;&#34920;&#36798;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#31181;&#22270;&#32423;&#20219;&#21153;&#21487;&#33021;&#19981;&#33021;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#21306;&#20998;&#19981;&#21516;&#25509;&#36817;&#31243;&#24230;&#33410;&#28857;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GNNs&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have demonstrated superior performance on various graph learning tasks, including recommendation, where they leverage user-item collaborative filtering signals in graphs. However, theoretical formulations of their capability are scarce, despite their empirical effectiveness in state-of-the-art recommender models. Recently, research has explored the expressiveness of GNNs in general, demonstrating that message passing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that GNNs combined with random node initialization are universal. Nevertheless, the concept of "expressiveness" for GNNs remains vaguely defined. Most existing works adopt the graph isomorphism test as the metric of expressiveness, but this graph-level task may not effectively assess a model's ability in recommendation, where the objective is to distinguish nodes of different closeness. In this paper, we provide a comprehensive theoretical analysis of the expressiveness of GNNs in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;CLIP&#20316;&#20026;&#25968;&#25454;&#28304;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#35789;&#35821;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#24335;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#24212;&#29992;prompt-guided&#20998;&#31867;&#36827;&#34892;&#22270;&#20687;&#30340;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20197;&#24448;&#38656;&#20026;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#35757;&#32451;&#27169;&#22411;&#30340;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#28508;&#21147;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.11119</link><description>&lt;p&gt;
&#20351;&#29992;CLIP&#36827;&#34892;&#38543;&#26426;&#35789;&#35821;&#25968;&#25454;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection. (arXiv:2308.11119v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;CLIP&#20316;&#20026;&#25968;&#25454;&#28304;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#35789;&#35821;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#24335;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#24212;&#29992;prompt-guided&#20998;&#31867;&#36827;&#34892;&#22270;&#20687;&#30340;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20197;&#24448;&#38656;&#20026;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#35757;&#32451;&#27169;&#22411;&#30340;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#28508;&#21147;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;CLIP&#20316;&#20026;&#25968;&#25454;&#28304;&#36827;&#34892;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#12290;&#30001;&#20110;&#28508;&#22312;&#30340;&#24037;&#19994;&#24212;&#29992;&#65292;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#24320;&#21457;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#32771;&#34385;&#21040;&#33719;&#21462;&#21508;&#31181;&#24322;&#24120;&#26679;&#26412;&#20197;&#29992;&#20110;&#35757;&#32451;&#30340;&#22256;&#38590;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#29992;&#27491;&#24120;&#26679;&#26412;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20174;&#27491;&#24120;&#26679;&#26412;&#30340;&#20998;&#24067;&#20013;&#27979;&#37327;&#20854;&#24046;&#24322;&#65292;&#36825;&#38656;&#35201;&#20026;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20302;&#25928;&#30340;&#35757;&#32451;&#38656;&#27714;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#23427;&#20197;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#24335;&#23545;&#22270;&#20687;&#30340;&#27599;&#20010;&#37096;&#20998;&#24212;&#29992;prompt-guided&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#20197;&#24050;&#30693;&#23545;&#35937;&#31867;&#21035;&#20180;&#32454;&#32452;&#21512;&#25552;&#31034;&#30340;&#21171;&#21160;&#21147;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;CLIP&#20316;&#20026;&#35757;&#32451;&#30340;&#25968;&#25454;&#28304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;CLI&#20013;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#25991;&#26412;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel method that leverages a visual-language model, CLIP, as a data source for zero-shot anomaly detection. Tremendous efforts have been put towards developing anomaly detectors due to their potential industrial applications. Considering the difficulty in acquiring various anomalous samples for training, most existing methods train models with only normal samples and measure discrepancies from the distribution of normal samples during inference, which requires training a model for each object category. The problem of this inefficient training requirement has been tackled by designing a CLIP-based anomaly detector that applies prompt-guided classification to each part of an image in a sliding window manner. However, the method still suffers from the labor of careful prompt ensembling with known object categories. To overcome the issues above, we propose leveraging CLIP as a data source for training. Our method generates text embeddings with the text encoder in CLI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37327;&#23376;&#39044;&#22788;&#29702;&#28388;&#27874;&#22120;&#65288;QPF&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#22312;MNIST&#21644;EMNIST&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#28982;&#32780;&#22312;GTSRB&#25968;&#25454;&#38598;&#19978;&#25928;&#26524;&#26377;&#25152;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2308.11112</link><description>&lt;p&gt;
&#24320;&#21457;&#19968;&#31181;&#26032;&#22411;&#30340;&#37327;&#23376;&#39044;&#22788;&#29702;&#28388;&#27874;&#22120;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Development of a Novel Quantum Pre-processing Filter to Improve Image Classification Accuracy of Neural Network Models. (arXiv:2308.11112v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37327;&#23376;&#39044;&#22788;&#29702;&#28388;&#27874;&#22120;&#65288;QPF&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#22312;MNIST&#21644;EMNIST&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#28982;&#32780;&#22312;GTSRB&#25968;&#25454;&#38598;&#19978;&#25928;&#26524;&#26377;&#25152;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37327;&#23376;&#39044;&#22788;&#29702;&#28388;&#27874;&#22120;&#65288;QPF&#65289;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#30340;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#22312;&#23558;&#25968;&#25454;&#20256;&#20837;&#20840;&#36830;&#25509;NN&#26550;&#26500;&#20043;&#21069;&#65292;&#24212;&#29992;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22235;&#27604;&#29305;&#37327;&#23376;&#30005;&#36335;&#65292;&#35813;&#30005;&#36335;&#20351;&#29992;Y&#26059;&#36716;&#38376;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#25511;&#21046;NOT&#38376;&#22312;&#27604;&#29305;&#20043;&#38388;&#21019;&#24314;&#30456;&#20851;&#24615;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#28388;&#27874;&#22120;&#12290;&#36890;&#36807;&#24212;&#29992;QPF&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;MNIST&#65288;&#25163;&#20889;10&#20301;&#25968;&#23383;&#65289;&#21644;EMNIST&#65288;&#25163;&#20889;47&#31867;&#25968;&#23383;&#21644;&#23383;&#27597;&#65289;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#21487;&#20197;&#25552;&#39640;&#65292;&#20998;&#21035;&#20174;92.5%&#25552;&#39640;&#21040;95.4%&#21644;&#20174;68.9%&#25552;&#39640;&#21040;75.9%&#12290;&#36825;&#20123;&#25913;&#36827;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#20013;&#27809;&#26377;&#24341;&#20837;&#39069;&#22806;&#30340;&#27169;&#22411;&#21442;&#25968;&#25110;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;43&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#30495;&#23454;&#20132;&#36890;&#26631;&#24535;&#22270;&#20687;&#30340;&#30456;&#23545;&#22797;&#26434;&#30340;GTSRB&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#27979;&#35797;&#26174;&#31034;&#20986;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#38477;&#20302;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25512;&#27979;QPF&#26041;&#27861;&#23545;&#20110;&#22797;&#26434;&#30340;&#22270;&#20687;&#25968;&#25454;&#21487;&#33021;&#19981;&#22826;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel quantum pre-processing filter (QPF) to improve the image classification accuracy of neural network (NN) models. A simple four qubit quantum circuit that uses Y rotation gates for encoding and two controlled NOT gates for creating correlation among the qubits is applied as a feature extraction filter prior to passing data into the fully connected NN architecture. By applying the QPF approach, the results show that the image classification accuracy based on the MNIST (handwritten 10 digits) and the EMNIST (handwritten 47 class digits and letters) datasets can be improved, from 92.5% to 95.4% and from 68.9% to 75.9%, respectively. These improvements were obtained without introducing extra model parameters or optimizations in the machine learning process. However, tests performed on the developed QPF approach against a relatively complex GTSRB dataset with 43 distinct class real-life traffic sign images showed a degradation in the classification accuracy. Consid
&lt;/p&gt;</description></item><item><title>CAME&#26159;&#19968;&#20010;&#19981;&#20381;&#36182;&#35757;&#32451;&#38598;&#30340;&#23545;&#27604;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#39564;&#35777;&#65292;&#24314;&#31435;&#20102;&#27169;&#22411;&#24615;&#33021;&#19982;&#23545;&#27604;&#25439;&#22833;&#20043;&#38388;&#30340;&#21487;&#39044;&#27979;&#20851;&#31995;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11111</link><description>&lt;p&gt;
CAME: &#23545;&#27604;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CAME: Contrastive Automated Model Evaluation. (arXiv:2308.11111v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11111
&lt;/p&gt;
&lt;p&gt;
CAME&#26159;&#19968;&#20010;&#19981;&#20381;&#36182;&#35757;&#32451;&#38598;&#30340;&#23545;&#27604;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#39564;&#35777;&#65292;&#24314;&#31435;&#20102;&#27169;&#22411;&#24615;&#33021;&#19982;&#23545;&#27604;&#25439;&#22833;&#20043;&#38388;&#30340;&#21487;&#39044;&#27979;&#20851;&#31995;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#65288;AutoEval&#65289;&#26694;&#26550;&#25506;&#32034;&#20102;&#22312;&#27809;&#26377;&#26631;&#35760;&#30340;&#27979;&#35797;&#38598;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#23613;&#31649;&#26377;&#19968;&#20123;&#19981;&#38169;&#30340;&#32467;&#26524;&#21644;&#25215;&#35834;&#65292;&#20294;&#29616;&#26377;&#30340;AutoEval&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#35745;&#31639;&#26410;&#26631;&#35760;&#27979;&#35797;&#38598;&#19982;&#35757;&#32451;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#23545;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#25104;&#20026;&#23558;&#36825;&#39033;&#25216;&#26415;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20013;&#30340;&#21478;&#19968;&#20010;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#33258;&#21160;&#27169;&#22411;&#35780;&#20272;&#65288;CAME&#65289;&#19968;&#20010;&#26032;&#30340;AutoEval&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#20381;&#36182;&#35757;&#32451;&#38598;&#12290;CAME&#30340;&#26680;&#24515;&#24605;&#24819;&#22522;&#20110;&#29702;&#35770;&#20998;&#26512;&#65292;&#23558;&#27169;&#22411;&#24615;&#33021;&#19982;&#23545;&#27604;&#25439;&#22833;&#30456;&#32852;&#31995;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#25104;&#21151;&#24314;&#31435;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#21487;&#39044;&#27979;&#20851;&#31995;&#65292;&#21482;&#38656;&#22312;&#26410;&#26631;&#35760;/&#26410;&#35265;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#25512;&#23548;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26694;&#26550;CAME&#36890;&#36807;&#36229;&#36234;&#20197;&#21069;&#30340;&#24037;&#20316;&#24314;&#31435;&#20102;&#26032;&#30340;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Automated Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set. Despite the promise and some decent results, the existing AutoEval methods heavily rely on computing distribution shifts between the unlabelled testing set and the training set. We believe this reliance on the training set becomes another obstacle in shipping this technology to real-world ML development. In this work, we propose Contrastive Automatic Model Evaluation (CAME), a novel AutoEval framework that is rid of involving training set in the loop. The core idea of CAME bases on a theoretical analysis which bonds the model performance with a contrastive loss. Further, with extensive empirical validation, we manage to set up a predictable relationship between the two, simply by deducing on the unlabeled/unseen testing set. The resulting framework CAME establishes a new SOTA results for AutoEval by surpassing prior wo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.11103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20877;&#35782;&#21035;&#33021;&#21147;&#65306;&#21311;&#21517;&#38754;&#20020;&#39118;&#38505;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models. (arXiv:2308.11103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27431;&#30431;&#21644;&#29790;&#22763;&#65292;&#27861;&#38498;&#35009;&#20915;&#20013;&#33258;&#28982;&#20154;&#21644;&#27861;&#20154;&#30340;&#21311;&#21517;&#24615;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#21311;&#21517;&#20154;&#21592;&#30340;&#22823;&#35268;&#27169;&#20877;&#35782;&#21035;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#38271;&#12290;&#26681;&#25454;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#35201;&#27714;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#23454;&#38469;&#27861;&#24459;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#26469;&#25506;&#35752;LLMs&#37325;&#26032;&#35782;&#21035;&#27861;&#38498;&#35009;&#20915;&#20013;&#20010;&#20154;&#30340;&#28508;&#21147;&#12290;&#22312;&#26368;&#21021;&#30340;&#23454;&#39564;&#20043;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32463;&#36807;&#21311;&#21517;&#21270;&#22788;&#29702;&#30340;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#20010;&#26356;&#20005;&#26684;&#30340;&#27979;&#35797;&#22330;&#22320;&#26469;&#36827;&#19968;&#27493;&#30740;&#31350;&#30740;&#31350;&#32467;&#26524;&#12290;&#36890;&#36807;&#24341;&#20837;&#24182;&#24212;&#29992;&#25991;&#26412;&#20013;&#20877;&#35782;&#21035;&#20154;&#21592;&#30340;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#24615;&#33021;&#34913;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#24433;&#21709;&#25104;&#21151;&#20877;&#35782;&#21035;&#30340;&#22240;&#32032;&#65292;&#30830;&#23450;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#20043;&#19968;&#12290;&#23613;&#31649;&#22312;&#21311;&#21517;&#21270;&#22788;&#29702;&#21518;&#65292;LLMs&#22312;&#37325;&#26032;&#35782;&#21035;&#19978;&#30340;&#25104;&#21151;&#29575;&#24456;&#39640;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24341;&#20837;&#20102;&#19981;&#21487;&#35299;&#37322;&#24615;&#24102;&#30340;&#27010;&#24565;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#26500;&#24314;&#36127;&#36131;&#20219;&#19988;&#21487;&#36861;&#31350;&#30340;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2308.11098</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#24615;&#19982;&#19981;&#21487;&#35299;&#37322;&#24615;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explicability and Inexplicability in the Interpretation of Quantum Neural Networks. (arXiv:2308.11098v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24341;&#20837;&#20102;&#19981;&#21487;&#35299;&#37322;&#24615;&#24102;&#30340;&#27010;&#24565;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#26500;&#24314;&#36127;&#36131;&#20219;&#19988;&#21487;&#36861;&#31350;&#30340;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;(AI)&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;AI&#25903;&#25345;&#30340;&#31995;&#32479;&#24448;&#24448;&#20855;&#26377;&#19981;&#21487;&#35299;&#37322;&#30340;&#34892;&#20026;&#12290;&#35299;&#37322;&#36825;&#31181;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#26500;&#24314;&#21487;&#20449;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#35768;&#22810;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#26126;&#26174;&#22320;&#25512;&#24191;&#21040;&#37327;&#23376;&#29615;&#22659;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#37327;&#23376;&#21644;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#24615;&#25351;&#26631;&#26469;&#25506;&#32034;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21487;&#35299;&#37322;&#24615;&#24102;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#22312;&#35813;&#21306;&#22495;&#20869;&#30340;&#25968;&#25454;&#26679;&#26412;&#27809;&#26377;&#35299;&#37322;&#65292;&#24456;&#21487;&#33021;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#38543;&#26426;&#37327;&#23376;&#27979;&#37327;&#30340;&#21463;&#23475;&#32773;&#12290;&#25105;&#20204;&#23558;&#27492;&#35270;&#20026;&#29702;&#35299;&#22914;&#20309;&#26500;&#24314;&#36127;&#36131;&#20219;&#19988;&#21487;&#36861;&#31350;&#30340;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of artificial intelligence (AI) methods, particularly deep neural networks, is of great interest due to the widespread use of AI-backed systems, which often have unexplainable behavior. The interpretability of such models is a crucial component of building trusted systems. Many methods exist to approach this problem, but they do not obviously generalize to the quantum setting. Here we explore the interpretability of quantum neural networks using local model-agnostic interpretability measures of quantum and classical neural networks. We introduce the concept of the band of inexplicability, representing the interpretable region in which data samples have no explanation, likely victims of inherently random quantum measurements. We see this as a step toward understanding how to build responsible and accountable quantum AI models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Video OWL-ViT&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#24320;&#25918;&#19990;&#30028;&#22270;&#20687;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#20219;&#21153;&#65292;&#36890;&#36807;&#28155;&#21152;&#21464;&#25442;&#22120;&#35299;&#30721;&#22120;&#23454;&#29616;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#20256;&#25773;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#36319;&#36394;-by-detection&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11093</link><description>&lt;p&gt;
Video OWL-ViT: &#35270;&#39057;&#20013;&#20855;&#26377;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#24320;&#25918;&#19990;&#30028;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Video OWL-ViT: Temporally-consistent open-world localization in video. (arXiv:2308.11093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Video OWL-ViT&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#24320;&#25918;&#19990;&#30028;&#22270;&#20687;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#20219;&#21153;&#65292;&#36890;&#36807;&#28155;&#21152;&#21464;&#25442;&#22120;&#35299;&#30721;&#22120;&#23454;&#29616;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#20256;&#25773;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#36319;&#36394;-by-detection&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#24320;&#25918;&#19990;&#30028;&#22270;&#20687;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#12290;&#29702;&#35299;&#24320;&#25918;&#30340;&#35270;&#35273;&#19990;&#30028;&#65288;&#19981;&#21463;&#22266;&#23450;&#26631;&#31614;&#31354;&#38388;&#30340;&#38480;&#21046;&#65289;&#23545;&#20110;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#22823;&#22411;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23545;&#27604;&#39044;&#35757;&#32451;&#26368;&#36817;&#22312;&#22270;&#20687;&#32423;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#23545;&#20110;&#28041;&#21450;&#23545;&#35937;&#23450;&#20301;&#30340;&#26356;&#32467;&#26500;&#21270;&#20219;&#21153;&#65292;&#24212;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#23545;&#20110;&#35270;&#39057;&#20219;&#21153;&#26469;&#35828;&#23588;&#20854;&#22914;&#27492;&#65292;&#22240;&#20026;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#26159;&#26377;&#38480;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;OWL-ViT&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;&#21464;&#25442;&#22120;&#35299;&#30721;&#22120;&#23558;&#20854;&#36866;&#24212;&#20026;&#35270;&#39057;&#65292;&#23637;&#31034;&#20102;&#24320;&#25918;&#19990;&#30028;&#27169;&#22411;&#30340;&#25104;&#21151;&#36716;&#31227;&#12290;&#35299;&#30721;&#22120;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#24103;&#30340;&#36755;&#20986;&#26631;&#35760;&#20316;&#20026;&#19979;&#19968;&#24103;&#30340;&#23545;&#35937;&#26597;&#35810;&#65292;&#20197;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#26041;&#24335;&#20256;&#25773;&#23545;&#35937;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#23545;&#35270;&#39057;&#25968;&#25454;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#27604;&#36890;&#36807;&#26816;&#27979;&#36827;&#34892;&#36319;&#36394;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an architecture and a training recipe that adapts pre-trained open-world image models to localization in videos. Understanding the open visual world (without being constrained by fixed label spaces) is crucial for many real-world vision tasks. Contrastive pre-training on large image-text datasets has recently led to significant improvements for image-level tasks. For more structured tasks involving object localization applying pre-trained models is more challenging. This is particularly true for video tasks, where task-specific data is limited. We show successful transfer of open-world models by building on the OWL-ViT open-vocabulary detection model and adapting it to video by adding a transformer decoder. The decoder propagates object representations recurrently through time by using the output tokens for one frame as the object queries for the next. Our model is end-to-end trainable on video data and enjoys improved temporal consistency compared to tracking-by-detection b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#35299;&#37322;&#27169;&#22411;&#20013;&#20559;&#35265;&#21306;&#22495;&#30340;&#21407;&#22240;&#21644;&#24433;&#21709;&#26469;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.11090</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Fairness and Explainability in Image Classification Using Optimal Transport. (arXiv:2308.11090v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#35299;&#37322;&#27169;&#22411;&#20013;&#20559;&#35265;&#21306;&#22495;&#30340;&#21407;&#22240;&#21644;&#24433;&#21709;&#26469;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#25191;&#27861;&#31561;&#39046;&#22495;&#20013;&#65292;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#23545;&#28508;&#22312;&#19981;&#20844;&#24179;&#32467;&#26524;&#30340;&#35299;&#37322;&#26159;&#24314;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20449;&#20219;&#21644;&#38382;&#36131;&#24615;&#30340;&#20851;&#38190;&#12290;&#23613;&#31649;&#22312;&#27599;&#20010;&#39046;&#22495;&#20998;&#21035;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39046;&#22495;&#20013;&#23454;&#29616;&#20844;&#24179;&#24615;&#24212;&#29992;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20262;&#29702;&#25968;&#25454;&#25366;&#25496;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#22810;&#27425;&#30740;&#31350;&#34920;&#26126;&#19981;&#20851;&#27880;&#20844;&#24179;&#24615;&#30340;&#31639;&#27861;&#20250;&#23548;&#33268;&#26377;&#20559;&#24046;&#30340;&#32467;&#26524;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22312;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#20943;&#36731;&#20559;&#35265;&#65292;&#20294;&#24456;&#23569;&#26377;&#23581;&#35797;&#35299;&#37322;&#27169;&#22411;&#20026;&#20160;&#20040;&#23384;&#22312;&#20559;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#26469;&#21457;&#29616;&#22270;&#20687;&#20013;&#26377;&#20559;&#24046;&#21306;&#22495;&#30340;&#21407;&#22240;&#21644;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#36731;&#26494;&#22320;&#25193;&#23637;&#21040;&#34920;&#26684;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;Wasserstein barycenters&#65292;&#25105;&#20204;&#21487;&#20197;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#35299;&#37322;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Fairness and the explainability of potentially unfair outcomes are crucial for establishing trust and accountability of Artificial Intelligence systems in domains such as healthcare and policing. Though significant advances have been made in each of the fields separately, achieving explainability in fairness applications remains challenging, particularly so in domains where deep neural networks are used. At the same time, ethical data-mining has become ever more relevant, as it has been shown countless times that fairness-unaware algorithms result in biased outcomes. Current approaches focus on mitigating biases in the outcomes of the model, but few attempts have been made to try to explain \emph{why} a model is biased. To bridge this gap, we propose a comprehensive approach that leverages optimal transport theory to uncover the causes and implications of biased regions in images, which easily extends to tabular data as well. Through the use of Wasserstein barycenters, we o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#19968;&#31995;&#21015;&#24352;&#37327;&#22522;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#30740;&#31350;&#19981;&#21516;&#30340;&#24212;&#21147;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#26367;&#20195;&#20256;&#32479;&#30340;Finger-Rivlin-Ericksen&#24418;&#24335;&#12290;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#21183;&#33021;&#21644;&#22522;&#20110;&#31995;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#24046;&#24322;&#65292;&#36827;&#19968;&#27493;&#25299;&#23637;&#20102;&#22312;&#26377;&#38480;&#21464;&#24418;&#24773;&#20917;&#19979;&#24314;&#27169;&#36229;&#24377;&#24615;&#26448;&#26009;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11080</link><description>&lt;p&gt;
&#24352;&#37327;&#22522;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#21147;&#34920;&#31034;&#65306;Finger-Rivlin-Ericksen&#30340;&#26367;&#20195;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
Stress representations for tensor basis neural networks: alternative formulations to Finger-Rivlin-Ericksen. (arXiv:2308.11080v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#19968;&#31995;&#21015;&#24352;&#37327;&#22522;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#30740;&#31350;&#19981;&#21516;&#30340;&#24212;&#21147;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#26367;&#20195;&#20256;&#32479;&#30340;Finger-Rivlin-Ericksen&#24418;&#24335;&#12290;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#21183;&#33021;&#21644;&#22522;&#20110;&#31995;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#24046;&#24322;&#65292;&#36827;&#19968;&#27493;&#25299;&#23637;&#20102;&#22312;&#26377;&#38480;&#21464;&#24418;&#24773;&#20917;&#19979;&#24314;&#27169;&#36229;&#24377;&#24615;&#26448;&#26009;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#32463;&#20856;&#34920;&#31034;&#23450;&#29702;&#30340;&#25968;&#25454;&#39537;&#21160;&#26412;&#26500;&#27169;&#22411;&#26694;&#26550;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36731;&#26494;&#22320;&#34701;&#20837;&#26412;&#26500;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#24212;&#21147;&#39044;&#27979;&#26159;&#22522;&#20110;&#19981;&#21464;&#24615;&#30456;&#20851;&#31995;&#25968;&#20989;&#25968;&#21644;&#24050;&#30693;&#24352;&#37327;&#22522;&#29983;&#25104;&#22120;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#35813;&#20844;&#24335;&#20165;&#38480;&#20110;&#22522;&#20110;&#32463;&#20856;Rivlin&#21644;Ericksen&#24418;&#24335;&#30340;&#24212;&#21147;&#34920;&#31034;&#65292;&#32780;&#20854;&#20182;&#26367;&#20195;&#20844;&#24335;&#30340;&#24615;&#33021;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31995;&#21015;&#22312;&#26377;&#38480;&#21464;&#24418;&#24773;&#20917;&#19979;&#29992;&#20110;&#24314;&#27169;&#36229;&#24377;&#24615;&#26448;&#26009;&#30340;&#24352;&#37327;&#22522;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21253;&#25324;&#19968;&#20123;&#36804;&#20170;&#26410;&#25506;&#32034;&#30340;&#20351;&#29992;&#29702;&#35770;&#19978;&#31561;&#20215;&#30340;&#19981;&#21464;&#37327;&#21644;&#29983;&#25104;&#22120;&#30340;&#20844;&#24335;&#65292;&#20197;&#21462;&#20195;Finger-Rivlin-Ericksen&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#21183;&#33021;&#21644;&#22522;&#20110;&#31995;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven constitutive modeling frameworks based on neural networks and classical representation theorems have recently gained considerable attention due to their ability to easily incorporate constitutive constraints and their excellent generalization performance. In these models, the stress prediction follows from a linear combination of invariant-dependent coefficient functions and known tensor basis generators. However, thus far the formulations have been limited to stress representations based on the classical Rivlin and Ericksen form, while the performance of alternative representations has yet to be investigated. In this work, we survey a variety of tensor basis neural network models for modeling hyperelastic materials in a finite deformation context, including a number of so far unexplored formulations which use theoretically equivalent invariants and generators to Finger-Rivlin-Ericksen. Furthermore, we compare potential-based and coefficient-based approaches, as well as dif
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#35270;&#39057;&#39044;&#27979;&#22120;&#65288;RoViPs&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#23618;&#24863;&#30693;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#24314;&#25439;&#22833;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36339;&#36807;&#36830;&#25509;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#39044;&#27979;&#22120;&#23545;&#33258;&#36523;&#30340;&#39044;&#27979;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#20135;&#29983;&#38750;&#24120;&#38271;&#30340;&#12289;&#36924;&#30495;&#30340;&#33258;&#28982;&#35270;&#39057;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2308.11079</link><description>&lt;p&gt;
&#20351;&#29992;&#31283;&#20581;&#30340;&#35270;&#39057;&#39044;&#27979;&#22120;&#36827;&#34892;&#33258;&#28982;&#35270;&#39057;&#24207;&#21015;&#30340;&#38271;&#26399;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Long-Term Prediction of Natural Video Sequences with Robust Video Predictors. (arXiv:2308.11079v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11079
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#35270;&#39057;&#39044;&#27979;&#22120;&#65288;RoViPs&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#23618;&#24863;&#30693;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#24314;&#25439;&#22833;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36339;&#36807;&#36830;&#25509;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#39044;&#27979;&#22120;&#23545;&#33258;&#36523;&#30340;&#39044;&#27979;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#20135;&#29983;&#38750;&#24120;&#38271;&#30340;&#12289;&#36924;&#30495;&#30340;&#33258;&#28982;&#35270;&#39057;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#39640;&#32500;&#35270;&#39057;&#24207;&#21015;&#26159;&#19968;&#20010;&#24322;&#24120;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#19981;&#30830;&#23450;&#24615;&#65292;&#32473;&#23450;&#35270;&#39057;&#24207;&#21015;&#30340;&#21487;&#33021;&#26410;&#26469;&#25968;&#37327;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#24403;&#35797;&#22270;&#20174;&#26377;&#38480;&#30340;&#19990;&#30028;&#24555;&#29031;&#20013;&#39044;&#27979;&#22797;&#26434;&#30340;&#33258;&#28982;&#35270;&#39057;&#22330;&#26223;&#26102;&#65292;&#36825;&#19968;&#28857;&#23588;&#20026;&#26126;&#26174;&#12290;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#38543;&#30528;&#39044;&#27979;&#36827;&#20837;&#26410;&#26469;&#32780;&#32047;&#31215;&#65292;&#20351;&#24471;&#38271;&#26399;&#39044;&#27979;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20123;&#23545;&#29616;&#26377;&#24037;&#20316;&#30340;&#25913;&#36827;&#65292;&#26377;&#21161;&#20110;&#21019;&#24314;&#31283;&#20581;&#30340;&#35270;&#39057;&#39044;&#27979;&#22120;&#65288;RoViPs&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;&#36890;&#36807;&#28145;&#23618;&#24863;&#30693;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#24314;&#25439;&#22833;&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#30701;&#26399;&#39044;&#27979;&#12290;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36339;&#36807;&#36830;&#25509;&#65292;&#21487;&#20197;&#23454;&#29616;&#36755;&#20837;&#29305;&#24449;&#30340;&#38271;&#31243;&#31354;&#38388;&#31227;&#21160;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#65292;&#36890;&#36807;&#20351;&#39044;&#27979;&#22120;&#23545;&#33258;&#36523;&#30340;&#39044;&#27979;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#20135;&#29983;&#38750;&#24120;&#38271;&#30340;&#12289;&#36924;&#30495;&#30340;&#33258;&#28982;&#35270;&#39057;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting high dimensional video sequences is a curiously difficult problem. The number of possible futures for a given video sequence grows exponentially over time due to uncertainty. This is especially evident when trying to predict complicated natural video scenes from a limited snapshot of the world. The inherent uncertainty accumulates the further into the future you predict making long-term prediction very difficult. In this work we introduce a number of improvements to existing work that aid in creating Robust Video Predictors (RoViPs). We show that with a combination of deep Perceptual and uncertainty-based reconstruction losses we are able to create high quality short-term predictions. Attention-based skip connections are utilised to allow for long range spatial movement of input features to further improve performance. Finally, we show that by simply making the predictor robust to its own prediction errors, it is possible to produce very long, realistic natural video sequenc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#37325;&#25972;&#21270;&#32676;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#20234;&#36763;&#27169;&#22411;&#20013;&#30340;&#32852;&#31995;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#37325;&#25972;&#21270;&#25216;&#26415;&#24471;&#21040;&#20102;&#26377;&#20851;&#32676;&#27969;&#30340;&#26032;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.11075</link><description>&lt;p&gt;
&#20851;&#20110;&#37325;&#25972;&#21270;&#32676;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#20234;&#36763;&#27169;&#22411;&#20013;&#30340;&#32852;&#31995;&#30340;&#28145;&#20837;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
A Deep Dive into the Connections Between the Renormalization Group and Deep Learning in the Ising Model. (arXiv:2308.11075v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#37325;&#25972;&#21270;&#32676;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#20234;&#36763;&#27169;&#22411;&#20013;&#30340;&#32852;&#31995;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#37325;&#25972;&#21270;&#25216;&#26415;&#24471;&#21040;&#20102;&#26377;&#20851;&#32676;&#27969;&#30340;&#26032;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#25972;&#21270;&#32676;&#26159;&#32479;&#35745;&#29289;&#29702;&#21644;&#37327;&#23376;&#22330;&#35770;&#20013;&#30340;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#65292;&#32771;&#34385;&#29289;&#29702;&#29702;&#35770;&#30340;&#23610;&#24230;&#19981;&#21464;&#24615;&#21644;&#21442;&#25968;&#22312;&#23610;&#24230;&#21464;&#25442;&#19979;&#30340;&#21464;&#21270;&#12290;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#35745;&#31639;&#25216;&#26415;&#65292;&#21033;&#29992;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#21508;&#31181;&#22797;&#26434;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#21487;&#33021;&#26159;&#19968;&#31181;&#37325;&#25972;&#21270;&#32676;&#27969;&#30340;&#24418;&#24335;&#65292;&#36890;&#36807;&#36880;&#23618;&#31895;&#31890;&#21270;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#20197;2D&#26368;&#36817;&#37051;&#20234;&#36763;&#27169;&#22411;&#30340;&#21345;&#20025;&#35834;&#22827;&#22359;&#37325;&#25972;&#21270;&#20026;&#31616;&#21333;&#31034;&#20363;&#65292;&#36890;&#36807;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#65292;&#23545;&#36825;&#31181;&#32852;&#31995;&#36827;&#34892;&#20102;&#26356;&#20005;&#26684;&#30340;&#22522;&#30784;&#19978;&#30740;&#31350;&#12290;&#25105;&#20204;&#20026;1D&#21644;2D&#20234;&#36763;&#27169;&#22411;&#24320;&#21457;&#20102;&#24191;&#27867;&#30340;&#37325;&#25972;&#21270;&#25216;&#26415;&#65292;&#20197;&#25552;&#20379;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;1D&#20234;&#36763;&#27169;&#22411;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#21033;&#29992;Adam&#20248;&#21270;&#21644;&#30456;&#20851;&#38271;&#24230;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#20102;&#32676;&#27969;yi
&lt;/p&gt;
&lt;p&gt;
The renormalization group (RG) is an essential technique in statistical physics and quantum field theory, which considers scale-invariant properties of physical theories and how these theories' parameters change with scaling. Deep learning is a powerful computational technique that uses multi-layered neural networks to solve a myriad of complicated problems. Previous research suggests the possibility that unsupervised deep learning may be a form of RG flow, by being a layer-by-layer coarse graining of the original data. We examined this connection on a more rigorous basis for the simple example of Kadanoff block renormalization of the 2D nearest-neighbor Ising model, with our deep learning accomplished via Restricted Boltzmann Machines (RBMs). We developed extensive renormalization techniques for the 1D and 2D Ising model to provide a baseline for comparison. For the 1D Ising model, we successfully used Adam optimization on a correlation length loss function to learn the group flow, yi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#38454;&#31038;&#20250;&#25512;&#29702;&#36827;&#34892;&#25674;&#38144;&#65292;&#20174;&#32780;&#21152;&#24555;&#23884;&#22871;&#22810;&#26234;&#33021;&#20307;&#25512;&#29702;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#38477;&#20302;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.11071</link><description>&lt;p&gt;
&#23884;&#22871;&#30340;&#22810;&#26234;&#33021;&#20307;&#25512;&#29702;&#30340;&#31070;&#32463;&#25674;&#38144;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Neural Amortized Inference for Nested Multi-agent Reasoning. (arXiv:2308.11071v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#38454;&#31038;&#20250;&#25512;&#29702;&#36827;&#34892;&#25674;&#38144;&#65292;&#20174;&#32780;&#21152;&#24555;&#23884;&#22871;&#22810;&#26234;&#33021;&#20307;&#25512;&#29702;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#38477;&#20302;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22914;&#27807;&#36890;&#12289;&#25945;&#23398;&#21644;&#34394;&#24352;&#22768;&#21183;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#39640;&#38454;&#30340;&#31038;&#20250;&#25512;&#29702;&#65292;&#21363;&#29702;&#35299;&#20182;&#20154;&#22914;&#20309;&#25512;&#26029;&#33258;&#24049;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#25512;&#29702;&#21487;&#20197;&#36890;&#36807;&#23884;&#22871;&#24335;&#22810;&#26234;&#33021;&#20307;&#25512;&#29702;&#26469;&#26377;&#25928;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27599;&#20010;&#25512;&#29702;&#32423;&#21035;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36731;&#26494;&#22320;&#25191;&#34892;&#22797;&#26434;&#30340;&#31038;&#20250;&#25512;&#29702;&#12290;&#20026;&#20102;&#24357;&#21512;&#20154;&#31867;&#25512;&#29702;&#33021;&#21147;&#21644;&#35745;&#31639;&#38480;&#21046;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#38454;&#31038;&#20250;&#25512;&#29702;&#36827;&#34892;&#25674;&#38144;&#65292;&#20174;&#32780;&#21152;&#24555;&#23884;&#22871;&#22810;&#26234;&#33021;&#20307;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#39046;&#22495;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#38477;&#20302;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent interactions, such as communication, teaching, and bluffing, often rely on higher-order social inference, i.e., understanding how others infer oneself. Such intricate reasoning can be effectively modeled through nested multi-agent reasoning. Nonetheless, the computational complexity escalates exponentially with each level of reasoning, posing a significant challenge. However, humans effortlessly perform complex social inferences as part of their daily lives. To bridge the gap between human-like inference capabilities and computational limitations, we propose a novel approach: leveraging neural networks to amortize high-order social inference, thereby expediting nested multi-agent reasoning. We evaluate our method in two challenging multi-agent interaction domains. The experimental results demonstrate that our method is computationally efficient while exhibiting minimal degradation in accuracy.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#39640;&#38454;&#20132;&#20114;&#12289;&#32858;&#31867;&#21644;&#28040;&#24687;&#20256;&#36882;&#31561;&#27493;&#39588;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#21387;&#32553;&#20449;&#21495;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#35823;&#24046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.11068</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Topological Graph Signal Compression. (arXiv:2308.11068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#39640;&#38454;&#20132;&#20114;&#12289;&#32858;&#31867;&#21644;&#28040;&#24687;&#20256;&#36882;&#31561;&#27493;&#39588;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#21387;&#32553;&#20449;&#21495;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#35823;&#24046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#65288;TDL&#65289;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#22320;&#22788;&#29702;&#39640;&#38454;&#20132;&#20114;&#65292;&#36229;&#36234;&#30001;&#22270;&#34920;&#31034;&#23450;&#20041;&#30340;&#25104;&#23545;&#20851;&#31995;&#21644;&#23616;&#37096;&#37051;&#22495;&#65292;&#20174;&#32780;&#25193;&#23637;&#24403;&#21069;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;TDL&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#22522;&#20110;&#21407;&#22987;&#20449;&#21495;&#25512;&#26029;&#20986;&#19981;&#30456;&#20132;&#30340;&#39640;&#38454;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;N&#20010;&#25968;&#25454;&#28857;&#32858;&#31867;&#25104;K&#20010;&#38598;&#21512;&#65307;&#28982;&#21518;&#65292;&#22522;&#20110;&#25299;&#25169;&#21551;&#31034;&#30340;&#28040;&#24687;&#20256;&#36882;&#22312;&#36825;&#20123;&#22810;&#20803;&#32032;&#38598;&#21512;&#20013;&#33719;&#24471;&#20449;&#21495;&#30340;&#21387;&#32553;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21387;&#32553;&#26469;&#33258;&#20004;&#20010;&#30495;&#23454;&#30340;&#20114;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#21830;&#32593;&#32476;&#25968;&#25454;&#38598;&#30340;&#26102;&#38388;&#38142;&#36335;&#20449;&#21495;&#26102;&#65292;&#27604;&#26631;&#20934;&#30340;GNN&#21644;&#21069;&#39304;&#26550;&#26500;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#35823;&#24046;&#8212;&#8212;&#22312;&#25152;&#26377;&#35780;&#20272;&#22330;&#26223;&#20013;&#65292;&#37325;&#24314;&#35823;&#24046;&#25552;&#39640;&#20102;&#20174;30%&#21040;90%&#12290;&#36825;&#34920;&#26126;&#23427;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently emerged Topological Deep Learning (TDL) methods aim to extend current Graph Neural Networks (GNN) by naturally processing higher-order interactions, going beyond the pairwise relations and local neighborhoods defined by graph representations. In this paper we propose a novel TDL-based method for compressing signals over graphs, consisting in two main steps: first, disjoint sets of higher-order structures are inferred based on the original signal --by clustering $N$ datapoints into $K\ll N$ collections; then, a topological-inspired message passing gets a compressed representation of the signal within those multi-element sets. Our results show that our framework improves both standard GNN and feed-forward architectures in compressing temporal link-based signals from two real-word Internet Service Provider Networks' datasets --from $30\%$ up to $90\%$ better reconstruction errors across all evaluation scenarios--, suggesting that it better captures and exploits spatial and tempor
&lt;/p&gt;</description></item><item><title>UnLoc&#26159;&#19968;&#20010;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#22411;&#20197;&#21450;&#35270;&#39057;-&#25991;&#26412;&#34701;&#21512;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;Moment Retrieval&#12289;Temporal Localization&#21644;Action Segmentation&#19977;&#20010;&#23450;&#20301;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11062</link><description>&lt;p&gt;
UnLoc&#65306;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UnLoc: A Unified Framework for Video Localization Tasks. (arXiv:2308.11062v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11062
&lt;/p&gt;
&lt;p&gt;
UnLoc&#26159;&#19968;&#20010;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#22411;&#20197;&#21450;&#35270;&#39057;-&#25991;&#26412;&#34701;&#21512;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;Moment Retrieval&#12289;Temporal Localization&#21644;Action Segmentation&#19977;&#20010;&#23450;&#20301;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20687;CLIP&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#20462;&#21098;&#35270;&#39057;&#19978;&#30340;&#22810;&#20010;&#35270;&#39057;&#32423;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#22312;&#26410;&#20462;&#21098;&#35270;&#39057;&#30340;&#26102;&#38388;&#23450;&#20301;&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;UnLoc&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#23558;&#20196;&#29260;&#25552;&#20379;&#32473;&#35270;&#39057;-&#25991;&#26412;&#34701;&#21512;&#27169;&#22411;&#12290;&#34701;&#21512;&#27169;&#22359;&#30340;&#36755;&#20986;&#28982;&#21518;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#29305;&#24449;&#37329;&#23383;&#22612;&#65292;&#20854;&#20013;&#27599;&#20010;&#32423;&#21035;&#37117;&#36830;&#25509;&#21040;&#19968;&#20010;&#22836;&#37096;&#20197;&#39044;&#27979;&#27599;&#24103;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#21644;&#24320;&#22987;/&#32467;&#26463;&#26102;&#38388;&#30340;&#20559;&#31227;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#38454;&#27573;&#27169;&#22411;&#23454;&#29616;&#20102;Moment Retrieval&#12289;Temporal Localization&#21644;Action Segmentation&#65292;&#32780;&#26080;&#38656;&#34892;&#21160;&#24314;&#35758;&#12289;&#22522;&#20110;&#21160;&#20316;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25110;&#34920;&#31034;&#25513;&#30721;&#12290;&#19982;&#19987;&#38376;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#25152;&#26377;&#19977;&#20010;&#19981;&#21516;&#30340;&#23450;&#20301;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#37319;&#29992;&#20102;&#32479;&#19968;&#30340;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/google-research/scenic}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large-scale image-text pretrained models such as CLIP have been used for multiple video-level tasks on trimmed videos, their use for temporal localization in untrimmed videos is still a relatively unexplored task. We design a new approach for this called UnLoc, which uses pretrained image and text towers, and feeds tokens to a video-text fusion model. The output of the fusion module are then used to construct a feature pyramid in which each level connects to a head to predict a per-frame relevancy score and start/end time displacements. Unlike previous works, our architecture enables Moment Retrieval, Temporal Localization, and Action Segmentation with a single stage model, without the need for action proposals, motion based pretrained features or representation masking. Unlike specialized models, we achieve state of the art results on all three different localization tasks with a unified approach. Code will be available at: \url{https://github.com/google-research/scenic}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#32423;&#21452;&#36335;&#24452;&#21387;&#32553;&#26041;&#27861;&#29992;&#20110;&#32852;&#21512;&#22238;&#22768;&#28040;&#38500;&#21644;&#22122;&#22768;&#25233;&#21046;&#65292;&#36890;&#36807;&#26102;&#38388;&#39057;&#29575;&#21452;&#36335;&#24452;&#21387;&#32553;&#23454;&#29616;&#24191;&#27867;&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#19988;&#22312;&#22266;&#23450;&#21387;&#32553;&#27604;&#19979;&#33021;&#22815;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11053</link><description>&lt;p&gt;
&#32852;&#21512;&#22238;&#22768;&#28040;&#38500;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#36229;&#32423;&#21452;&#36335;&#24452;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression. (arXiv:2308.11053v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#32423;&#21452;&#36335;&#24452;&#21387;&#32553;&#26041;&#27861;&#29992;&#20110;&#32852;&#21512;&#22238;&#22768;&#28040;&#38500;&#21644;&#22122;&#22768;&#25233;&#21046;&#65292;&#36890;&#36807;&#26102;&#38388;&#39057;&#29575;&#21452;&#36335;&#24452;&#21387;&#32553;&#23454;&#29616;&#24191;&#27867;&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#19988;&#22312;&#22266;&#23450;&#21387;&#32553;&#27604;&#19979;&#33021;&#22815;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#28040;&#38500;&#21644;&#22122;&#22768;&#25233;&#21046;&#23545;&#20110;&#20840;&#21452;&#24037;&#36890;&#20449;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#25972;&#19978;&#19981;&#28789;&#27963;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#26102;&#38388;&#39057;&#29575;&#21452;&#36335;&#24452;&#21387;&#32553;&#65292;&#23454;&#29616;&#24191;&#27867;&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#39057;&#29575;&#21387;&#32553;&#26041;&#38754;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#28388;&#27874;&#22120;&#20195;&#26367;&#25163;&#21160;&#35774;&#35745;&#30340;&#28388;&#27874;&#22120;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#12290;&#22312;&#26102;&#38388;&#21387;&#32553;&#26041;&#38754;&#65292;&#20165;&#20351;&#29992;&#24103;&#36339;&#39044;&#27979;&#20250;&#23548;&#33268;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#65292;&#20294;&#36890;&#36807;&#20855;&#26377;&#23436;&#25972;&#24207;&#21015;&#24314;&#27169;&#30340;&#21518;&#22788;&#29702;&#32593;&#32476;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22266;&#23450;&#21387;&#32553;&#27604;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#20351;&#29992;&#26102;&#38388;&#21644;&#39057;&#29575;&#26041;&#27861;&#36827;&#34892;&#21452;&#36335;&#24452;&#21387;&#32553;&#23558;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#25913;&#21464;&#27169;&#22411;&#22823;&#23567;&#65292;&#21387;&#32553;&#27604;&#35206;&#30422;&#33539;&#22260;&#20174;4x&#21040;32x&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#24555;&#36895;FullSubNet&#21644;DeepFilterNet&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#21487;&#20197;&#35775;&#38382;&#28436;&#31034;&#39029;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Echo cancellation and noise reduction are essential for full-duplex communication, yet most existing neural networks have high computational costs and are inflexible in tuning model complexity. In this paper, we introduce time-frequency dual-path compression to achieve a wide range of compression ratios on computational cost. Specifically, for frequency compression, trainable filters are used to replace manually designed filters for dimension reduction. For time compression, only using frame skipped prediction causes large performance degradation, which can be alleviated by a post-processing network with full sequence modeling. We have found that under fixed compression ratios, dual-path compression combining both the time and frequency methods will give further performance improvement, covering compression ratios from 4x to 32x with little model size change. Moreover, the proposed models show competitive performance compared with fast FullSubNet and DeepFilterNet. A demo page can be f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33041;&#37096;MRI&#30340;&#21333;&#27425;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#36827;&#34892;&#22270;&#20687;&#21327;&#35843;&#65292;&#36991;&#20813;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;&#35813;&#26041;&#27861;&#22312;&#25910;&#38598;&#26469;&#33258;&#19981;&#21516;&#20020;&#24202;&#22320;&#28857;&#30340;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#26102;&#21487;&#20197;&#25552;&#39640;&#21327;&#35843;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11047</link><description>&lt;p&gt;
&#36328;&#24433;&#20687;&#22320;&#28857;&#30340;&#21327;&#35843;&#65288;HAIL&#65289;&#65306;&#29992;&#20110;&#33041;&#37096;MRI&#30340;&#21333;&#27425;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Harmonization Across Imaging Locations(HAIL): One-Shot Learning for Brain MRI. (arXiv:2308.11047v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11047
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33041;&#37096;MRI&#30340;&#21333;&#27425;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#36827;&#34892;&#22270;&#20687;&#21327;&#35843;&#65292;&#36991;&#20813;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;&#35813;&#26041;&#27861;&#22312;&#25910;&#38598;&#26469;&#33258;&#19981;&#21516;&#20020;&#24202;&#22320;&#28857;&#30340;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#26102;&#21487;&#20197;&#25552;&#39640;&#21327;&#35843;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32597;&#35265;&#30142;&#30149;&#65288;&#22914;&#20799;&#31461;&#33041;&#32959;&#30244;&#65289;&#30340;&#39044;&#21518;&#21644;&#35786;&#26029;&#65292;&#38656;&#35201;&#25910;&#38598;&#26469;&#33258;&#21487;&#33021;&#20351;&#29992;&#19981;&#21516;&#35774;&#22791;&#21644;&#21327;&#35758;&#30340;&#22810;&#20010;&#20020;&#24202;&#22320;&#28857;&#30340;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;&#21327;&#35843;&#20381;&#36182;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12290;&#28982;&#32780;&#65292;GANs&#20250;&#29983;&#25104;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#20266;&#32467;&#26500;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#12290;&#20026;&#20102;&#38450;&#27490;&#21307;&#23398;&#25104;&#20687;&#65288;&#22914;&#33041;&#37096;&#30340;&#30913;&#20849;&#25391;&#22270;&#20687;&#65289;&#20013;&#30340;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#27425;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#36827;&#34892;&#21327;&#35843;&#12290;&#22312;&#27979;&#35797;&#26102;&#38388;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20020;&#24202;&#22320;&#28857;&#30340;&#19968;&#24352;&#22270;&#20687;&#29983;&#25104;&#19982;&#21327;&#20316;&#22320;&#28857;&#30340;&#24378;&#24230;&#27604;&#20363;&#21305;&#37197;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#23398;&#20064;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#21644;&#33258;&#36866;&#24212;&#23454;&#20363;&#24402;&#19968;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22270;&#20687;&#21327;&#35843;&#25928;&#26524;&#30340;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
For machine learning-based prognosis and diagnosis of rare diseases, such as pediatric brain tumors, it is necessary to gather medical imaging data from multiple clinical sites that may use different devices and protocols. Deep learning-driven harmonization of radiologic images relies on generative adversarial networks (GANs). However, GANs notoriously generate pseudo structures that do not exist in the original training data, a phenomenon known as "hallucination". To prevent hallucination in medical imaging, such as magnetic resonance images (MRI) of the brain, we propose a one-shot learning method where we utilize neural style transfer for harmonization. At test time, the method uses one image from a clinical site to generate an image that matches the intensity scale of the collaborating sites. Our approach combines learning a feature extractor, neural style transfer, and adaptive instance normalization. We further propose a novel strategy to evaluate the effectiveness of image harmo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38169;&#35823;&#30456;&#20851;&#24615;&#30340;&#21457;&#29983;&#21407;&#22240;&#20197;&#21450;&#20854;&#23545;&#26631;&#20934;ERM&#22522;&#32447;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#36825;&#20123;&#21407;&#22240;&#19982;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#20043;&#38388;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.11043</link><description>&lt;p&gt;
&#38169;&#35823;&#30340;&#30456;&#20851;&#24615;&#21450;&#20854;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spurious Correlations and Where to Find Them. (arXiv:2308.11043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38169;&#35823;&#30456;&#20851;&#24615;&#30340;&#21457;&#29983;&#21407;&#22240;&#20197;&#21450;&#20854;&#23545;&#26631;&#20934;ERM&#22522;&#32447;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#36825;&#20123;&#21407;&#22240;&#19982;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#20043;&#38388;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#30340;&#30456;&#20851;&#24615;&#25351;&#30340;&#26159;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#19981;&#21487;&#38752;&#30340;&#29305;&#24449;&#65292;&#26159;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#30340;&#19968;&#20010;&#24050;&#30693;&#32570;&#38519;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#20960;&#31181;&#31639;&#27861;&#25552;&#20986;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#36824;&#27809;&#26377;&#33021;&#22815;&#20849;&#21516;&#25512;&#23548;&#20986;&#38169;&#35823;&#30456;&#20851;&#24615;&#30340;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#29420;&#31435;&#20551;&#35774;&#26500;&#24314;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#20987;&#36133;&#31616;&#21333;&#30340;ERM&#22522;&#32447;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#20986;&#29616;&#32972;&#21518;&#30340;&#20551;&#35774;&#65292;&#24182;&#20351;&#29992;&#20174;&#22240;&#26524;&#22270;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30740;&#31350;&#23427;&#20204;&#23545;&#26631;&#20934;ERM&#22522;&#32447;&#30340;&#24433;&#21709;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#20551;&#35774;&#21644;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#20043;&#38388;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurious correlations occur when a model learns unreliable features from the data and are a well-known drawback of data-driven learning. Although there are several algorithms proposed to mitigate it, we are yet to jointly derive the indicators of spurious correlations. As a result, the solutions built upon standalone hypotheses fail to beat simple ERM baselines. We collect some of the commonly studied hypotheses behind the occurrence of spurious correlations and investigate their influence on standard ERM baselines using synthetic datasets generated from causal graphs. Subsequently, we observe patterns connecting these hypotheses and model design choices.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#26469;&#20248;&#21270;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#24067;&#32622;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#36317;&#31163;&#21644;&#30899;&#36275;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.11038</link><description>&lt;p&gt;
&#29289;&#27969;&#38598;&#25955;&#22320;&#20301;&#32622;&#20248;&#21270;&#65306;&#19968;&#31181;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances. (arXiv:2308.11038v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#26469;&#20248;&#21270;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#24067;&#32622;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#36317;&#31163;&#21644;&#30899;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#27969;&#38598;&#25955;&#22320;&#22312;&#26368;&#21518;&#19968;&#20844;&#37324;&#37197;&#36865;&#36317;&#31163;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65307;&#21363;&#20351;&#36317;&#31163;&#24494;&#23567;&#22686;&#21152;&#20063;&#20250;&#23545;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#30340;&#19994;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#20250;&#22686;&#21152;&#20854;&#30899;&#36275;&#36857;&#12290;&#29305;&#21035;&#26159;&#22312;Covid-19&#20043;&#21518;&#65292;&#35813;&#34892;&#19994;&#30340;&#22686;&#38271;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#20248;&#21270;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#24067;&#32622;&#12290;&#35813;&#26041;&#27861;&#20381;&#27425;&#37319;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#31354;&#38388;&#20301;&#32622;&#65292;&#20351;&#29992;K-Means&#23545;&#20132;&#20184;&#28857;&#36827;&#34892;&#32858;&#31867;&#12290;&#32858;&#31867;&#26041;&#27861;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#65292;&#32780;&#19981;&#26159;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#12290;&#36991;&#20813;&#20351;&#29992;&#38750;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#23548;&#33268;&#38169;&#35823;&#21644;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;P-Median&#26041;&#27861;&#30830;&#23450;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#12290;P-Median&#26041;&#27861;&#36824;&#23558;&#20132;&#20184;&#25968;&#37327;&#21644;&#20154;&#21475;&#20316;&#20026;&#26435;&#37325;&#32771;&#34385;&#22312;&#20869;&#12290;&#20351;&#29992;Muller&#21644;Phipps&#65288;M&#65286;P&#65289;&#30340;&#23454;&#38469;&#20132;&#20184;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Logistic hubs play a pivotal role in the last-mile delivery distance; even a slight increment in distance negatively impacts the business of the e-commerce industry while also increasing its carbon footprint. The growth of this industry, particularly after Covid-19, has further intensified the need for optimized allocation of resources in an urban environment. In this study, we use a hybrid approach to optimize the placement of logistic hubs. The approach sequentially employs different techniques. Initially, delivery points are clustered using K-Means in relation to their spatial locations. The clustering method utilizes road network distances as opposed to Euclidean distances. Non-road network-based approaches have been avoided since they lead to erroneous and misleading results. Finally, hubs are located using the P-Median method. The P-Median method also incorporates the number of deliveries and population as weights. Real-world delivery data from Muller and Phipps (M&amp;P) is used to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RBA-GCN&#27169;&#22411;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#21644;&#22270;&#29983;&#25104;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;GCN&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#20449;&#24687;&#20887;&#20313;&#21644;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11029</link><description>&lt;p&gt;
RBA-GCN: &#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition. (arXiv:2308.11029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RBA-GCN&#27169;&#22411;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#21644;&#22270;&#29983;&#25104;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;GCN&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#20449;&#24687;&#20887;&#20313;&#21644;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#30001;&#20110;&#23545;&#35805;&#20855;&#26377;&#33258;&#28982;&#30340;&#22270;&#32467;&#26500;&#65292;&#24456;&#22810;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;ERC&#27169;&#22411;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;GCNs&#30340;&#32858;&#21512;&#26041;&#27861;&#23384;&#22312;&#33410;&#28857;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#65292;&#23548;&#33268;&#33410;&#28857;&#36776;&#21035;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#21333;&#23618;GCNs&#32570;&#20047;&#20174;&#22270;&#20013;&#25429;&#33719;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#25991;&#26412;&#27169;&#24577;&#25110;&#23558;&#19981;&#21516;&#27169;&#24577;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#25429;&#25417;&#27169;&#24577;&#38388;&#20132;&#20114;&#33021;&#21147;&#24369;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;RBA-GCN&#65289;&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#22270;&#29983;&#25104;&#27169;&#22359;&#65288;GGM&#65289;&#12289;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#31751;&#26500;&#24314;&#27169;&#22359;&#65288;SCBM&#65289;&#21644;&#21452;&#23618;&#32858;&#21512;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation 
&lt;/p&gt;</description></item><item><title>&#25286;&#20998;&#23398;&#20064;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#21516;&#30340;&#12289;&#31169;&#26377;&#32500;&#25252;&#30340;&#20581;&#24247;&#25968;&#25454;&#38598;&#20043;&#38388;&#21327;&#20316;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;&#22312;&#21307;&#30103;&#30456;&#20851;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;&#25286;&#20998;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19982;&#20013;&#22830;&#21644;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11027</link><description>&lt;p&gt;
&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#20998;&#24067;&#24335;&#21327;&#20316;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25286;&#20998;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Split Learning for Distributed Collaborative Training of Deep Learning Models in Health Informatics. (arXiv:2308.11027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11027
&lt;/p&gt;
&lt;p&gt;
&#25286;&#20998;&#23398;&#20064;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#21516;&#30340;&#12289;&#31169;&#26377;&#32500;&#25252;&#30340;&#20581;&#24247;&#25968;&#25454;&#38598;&#20043;&#38388;&#21327;&#20316;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;&#22312;&#21307;&#30103;&#30456;&#20851;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;&#25286;&#20998;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19982;&#20013;&#22830;&#21644;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#22312;&#19981;&#21516;&#21307;&#30103;&#26426;&#26500;&#20043;&#38388;&#23454;&#29616;&#27867;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#37096;&#20998;&#26159;&#30001;&#20110;&#36825;&#20123;&#26426;&#26500;&#30340;&#23396;&#31435;&#24615;&#36136;&#21644;&#24739;&#32773;&#38544;&#31169;&#35201;&#27714;&#25152;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25286;&#20998;&#23398;&#20064;&#26469;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#65292;&#36328;&#36234;&#19981;&#21516;&#30340;&#12289;&#31169;&#26377;&#32500;&#25252;&#30340;&#20581;&#24247;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25252;&#21407;&#22987;&#35760;&#24405;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#23427;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#20010;&#29983;&#29289;&#21307;&#23398;&#24433;&#20687;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405; (EHR) &#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#25286;&#20998;&#23398;&#20064;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#19982;&#20013;&#22830;&#21644;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning continues to rapidly evolve and is now demonstrating remarkable potential for numerous medical prediction tasks. However, realizing deep learning models that generalize across healthcare organizations is challenging. This is due, in part, to the inherent siloed nature of these organizations and patient privacy requirements. To address this problem, we illustrate how split learning can enable collaborative training of deep learning models across disparate and privately maintained health datasets, while keeping the original records and model parameters private. We introduce a new privacy-preserving distributed learning framework that offers a higher level of privacy compared to conventional federated learning. We use several biomedical imaging and electronic health record (EHR) datasets to show that deep learning models trained via split learning can achieve highly similar performance to their centralized and federated counterparts while greatly improving computational effi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38544;&#24335;&#21453;&#39304;&#21644;&#26377;&#38480;&#24739;&#32773;&#20803;&#25968;&#25454;&#30340;&#19987;&#31185;&#21307;&#29983;&#25512;&#33616;&#30340;&#26497;&#38480;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#25512;&#33616;&#35774;&#32622;&#36716;&#25442;&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#21382;&#21490;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11022</link><description>&lt;p&gt;
&#22522;&#20110;&#38544;&#24335;&#21453;&#39304;&#21644;&#26377;&#38480;&#24739;&#32773;&#20803;&#25968;&#25454;&#30340;&#19987;&#31185;&#21307;&#29983;&#25512;&#33616;&#30340;&#26497;&#38480;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Extreme Multilabel Classification for Specialist Doctor Recommendation with Implicit Feedback and Limited Patient Metadata. (arXiv:2308.11022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38544;&#24335;&#21453;&#39304;&#21644;&#26377;&#38480;&#24739;&#32773;&#20803;&#25968;&#25454;&#30340;&#19987;&#31185;&#21307;&#29983;&#25512;&#33616;&#30340;&#26497;&#38480;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#25512;&#33616;&#35774;&#32622;&#36716;&#25442;&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#21382;&#21490;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#21307;&#29983;&#25512;&#33616;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#24739;&#32773;&#21453;&#39304;&#21644;&#21307;&#30103;&#35760;&#24405;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#21307;&#29983;&#25512;&#33616;&#24182;&#26088;&#22312;&#39044;&#27979;&#26032;&#24739;&#32773;&#21644;&#26377;&#36807;&#21672;&#35810;&#21382;&#21490;&#30340;&#24739;&#32773;&#22312;&#19981;&#21516;&#19987;&#31185;&#21307;&#29983;&#20013;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#24120;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#26497;&#38480;&#22810;&#26631;&#31614;&#20998;&#31867;&#65288;XML&#65289;&#26041;&#27861;&#26469;&#32534;&#30721;&#21487;&#29992;&#30340;&#29305;&#24449;&#24182;&#25506;&#32034;&#19981;&#21516;&#30340;&#24773;&#26223;&#12290;&#34429;&#28982;&#23427;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#32463;&#24120;&#34987;&#25552;&#21450;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#21463;&#21040;&#21307;&#29983;&#25512;&#33616;&#26696;&#20363;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20256;&#32479;&#30340;&#25512;&#33616;&#35774;&#32622;&#36716;&#21270;&#20026;&#24403;&#21069;XML&#26041;&#27861;&#21487;&#35299;&#20915;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65292;&#21033;&#29992;&#19981;&#21516;&#19987;&#31185;&#21307;&#29983;&#30340;&#24739;&#32773;&#21382;&#21490;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#26356;&#22810;&#30340;&#22240;&#32032;&#65292;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation Systems (RS) are often used to address the issue of medical doctor referrals. However, these systems require access to patient feedback and medical records, which may not always be available in real-world scenarios. Our research focuses on medical referrals and aims to predict recommendations in different specialties of physicians for both new patients and those with a consultation history. We use Extreme Multilabel Classification (XML), commonly employed in text-based classification tasks, to encode available features and explore different scenarios. While its potential for recommendation tasks has often been suggested, this has not been thoroughly explored in the literature. Motivated by the doctor referral case, we show how to recast a traditional recommender setting into a multilabel classification problem that current XML methods can solve. Further, we propose a unified model leveraging patient history across different specialties. Compared to state-of-the-art RS us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22810;&#20219;&#21153;&#36229;&#22270;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22320;&#29699;&#35266;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#24418;&#25104;&#38598;&#25104;&#25945;&#24072;&#24182;&#29983;&#25104;&#21487;&#38752;&#20266;&#26631;&#31614;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;NASA NEO&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#30456;&#23545;&#24378;&#22522;&#32447;&#21644;&#26368;&#26032;&#24037;&#20316;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36229;&#22270;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#26080;&#30417;&#30563;&#22320;&#36866;&#24212;&#36880;&#28176;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.11021</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#36229;&#22270;&#29992;&#20110;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Hypergraphs for Semi-supervised Learning using Earth Observations. (arXiv:2308.11021v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22810;&#20219;&#21153;&#36229;&#22270;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22320;&#29699;&#35266;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#24418;&#25104;&#38598;&#25104;&#25945;&#24072;&#24182;&#29983;&#25104;&#21487;&#38752;&#20266;&#26631;&#31614;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;NASA NEO&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#30456;&#23545;&#24378;&#22522;&#32447;&#21644;&#26368;&#26032;&#24037;&#20316;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36229;&#22270;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#26080;&#30417;&#30563;&#22320;&#36866;&#24212;&#36880;&#28176;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#26377;&#35768;&#22810;&#19981;&#21516;&#30340;&#35299;&#37322;&#26041;&#24335;&#65292;&#23427;&#20204;&#20043;&#38388;&#39640;&#24230;&#30456;&#20114;&#20381;&#36182;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22810;&#20219;&#21153;&#36229;&#22270;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#26159;&#19968;&#20010;&#20219;&#21153;&#65292;&#36890;&#36807;&#36229;&#22270;&#20013;&#21040;&#36798;&#32473;&#23450;&#20219;&#21153;&#30340;&#19981;&#21516;&#36335;&#24452;&#24418;&#25104;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#25945;&#24072;&#65292;&#36890;&#36807;&#24418;&#25104;&#23398;&#20064;&#21487;&#38752;&#20266;&#26631;&#31614;&#30340;&#38598;&#25104;&#12290;&#27599;&#20010;&#36229;&#36793;&#26159;&#32473;&#23450;&#20219;&#21153;&#30340;&#19968;&#20010;&#38598;&#25104;&#25945;&#24072;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#19988;&#36824;&#26159;&#33258;&#30417;&#30563;&#36229;&#22270;&#31995;&#32479;&#30340;&#23398;&#29983;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#26102;&#19979;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#22320;&#29699;&#35266;&#27979;&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#22810;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24448;&#24448;&#23384;&#22312;&#32570;&#23569;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#22312;NASA NEO&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20026;&#26399;22&#24180;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#21322;&#30417;&#30563;&#26041;&#27861;&#30340;&#20215;&#20540;&#65292;&#36890;&#36807;&#30456;&#23545;&#24378;&#22522;&#32447;&#21644;&#26368;&#26032;&#24037;&#20316;&#30340;&#19968;&#36143;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36229;&#22270;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#26080;&#30417;&#30563;&#22320;&#36866;&#24212;&#36880;&#28176;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are many ways of interpreting the world and they are highly interdependent. We exploit such complex dependencies and introduce a powerful multi-task hypergraph, in which every node is a task and different paths through the hypergraph reaching a given task become unsupervised teachers, by forming ensembles that learn to generate reliable pseudolabels for that task. Each hyperedge is part of an ensemble teacher for a given task and it is also a student of the self-supervised hypergraph system. We apply our model to one of the most important problems of our times, that of Earth Observation, which is highly multi-task and it often suffers from missing ground-truth data. By performing extensive experiments on the NASA NEO Dataset, spanning a period of 22 years, we demonstrate the value of our multi-task semi-supervised approach, by consistent improvements over strong baselines and recent work. We also show that the hypergraph can adapt unsupervised to gradual data distribution shifts 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;kNN&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#32930;&#20307;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#25163;&#21183;&#26816;&#27979;&#12290;&#36890;&#36807;&#25968;&#25454;&#38598;&#32553;&#20943;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20934;&#30830;&#24615;&#21644;&#23454;&#26102;&#35201;&#27714;&#30340;&#21487;&#38752;&#38598;&#25104;&#12290;&#20915;&#31574;&#38754;&#26144;&#23556;&#65288;DSM&#65289;&#22312;&#20943;&#23569;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11019</link><description>&lt;p&gt;
&#23454;&#20363;&#21270;&#23398;&#20064;&#19982;&#21407;&#22411;&#32553;&#20943;&#22312;&#23454;&#26102;&#27604;&#20363;&#32908;&#32905;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#20010;&#38543;&#26426;&#29992;&#25143;&#30740;&#31350;&#65292;&#23637;&#31034;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#25968;&#25454;&#32553;&#20943;&#23545;&#26893;&#20837;&#31995;&#32479;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Instance-based Learning with Prototype Reduction for Real-Time Proportional Myocontrol: A Randomized User Study Demonstrating Accuracy-preserving Data Reduction for Prosthetic Embedded Systems. (arXiv:2308.11019v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;kNN&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#32930;&#20307;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#25163;&#21183;&#26816;&#27979;&#12290;&#36890;&#36807;&#25968;&#25454;&#38598;&#32553;&#20943;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20934;&#30830;&#24615;&#21644;&#23454;&#26102;&#35201;&#27714;&#30340;&#21487;&#38752;&#38598;&#25104;&#12290;&#20915;&#31574;&#38754;&#26144;&#23556;&#65288;DSM&#65289;&#22312;&#20943;&#23569;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;kNN&#26041;&#27861;&#30340;&#25163;&#21183;&#26816;&#27979;&#23398;&#20064;&#25216;&#26415;&#30340;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#39564;&#35777;&#12290;&#20026;&#20102;&#24212;&#23545;&#23454;&#26102;&#20915;&#31574;&#20013;&#39640;&#35745;&#31639;&#35201;&#27714;&#30340;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#25968;&#25454;&#38598;&#32553;&#20943;&#26041;&#27861;&#65292;&#32771;&#34385;&#21040;&#23454;&#26102;&#30830;&#23450;&#24615;&#65292;&#20197;&#20415;&#21487;&#38752;&#22320;&#38598;&#25104;&#21040;&#30005;&#27744;&#20379;&#30005;&#30340;&#20415;&#25658;&#35774;&#22791;&#20013;&#12290;&#20998;&#26512;&#20102;&#21442;&#25968;&#21270;&#21644;&#19981;&#21516;&#27604;&#20363;&#26041;&#26696;&#30340;&#24433;&#21709;&#65292;&#21033;&#29992;&#20843;&#36890;&#36947;sEMG&#33218;&#24102;&#12290;&#38500;&#20102;&#31163;&#32447;&#20132;&#21449;&#39564;&#35777;&#20934;&#30830;&#29575;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#23454;&#26102;&#39134;&#34892;&#23454;&#39564;&#65288;&#22312;&#32447;&#30446;&#26631;&#23454;&#29616;&#27979;&#35797;&#65289;&#30340;&#25104;&#21151;&#29575;&#12290;&#22522;&#20110;&#23545;&#23884;&#20837;&#24335;&#25511;&#21046;&#24212;&#29992;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#32553;&#20943;&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#34892;&#20026;&#26041;&#38754;&#30340;&#35780;&#20272;&#65292;&#20915;&#31574;&#38754;&#26144;&#23556;&#65288;DSM&#65289;&#22312;&#24212;&#29992;&#20110;&#32553;&#20943;&#38598;&#19978;&#30340;kNN&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#28508;&#21147;&#12290;&#36827;&#34892;&#20102;&#19968;&#39033;&#38543;&#26426;&#12289;&#21452;&#30450;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#30456;&#24212;&#30340;&#26041;&#27861;&#65288;kNN&#21644;&#20855;&#26377;DSM&#32553;&#20943;&#30340;kNN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents the design, implementation and validation of learning techniques based on the kNN scheme for gesture detection in prosthetic control. To cope with high computational demands in instance-based prediction, methods of dataset reduction are evaluated considering real-time determinism to allow for the reliable integration into battery-powered portable devices. The influence of parameterization and varying proportionality schemes is analyzed, utilizing an eight-channel-sEMG armband. Besides offline cross-validation accuracy, success rates in real-time pilot experiments (online target achievement tests) are determined. Based on the assessment of specific dataset reduction techniques' adequacy for embedded control applications regarding accuracy and timing behaviour, Decision Surface Mapping (DSM) proves itself promising when applying kNN on the reduced set. A randomized, double-blind user study was conducted to evaluate the respective methods (kNN and kNN with DSM-reduction
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20107;&#20214;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#22810;&#20010;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#20010;&#20307;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.11013</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Personalized Event Prediction for Electronic Health Records. (arXiv:2308.11013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20107;&#20214;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#22810;&#20010;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#20010;&#20307;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20107;&#20214;&#24207;&#21015;&#21253;&#21547;&#25968;&#30334;&#20010;&#20020;&#24202;&#20107;&#20214;&#65292;&#20195;&#34920;&#20102;&#24739;&#32773;&#22312;&#19981;&#21516;&#26102;&#38388;&#25509;&#21463;&#29031;&#25252;&#30340;&#35760;&#24405;&#12290;&#24320;&#21457;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#25903;&#25345;&#21508;&#31181;&#27169;&#22411;&#26469;&#35299;&#37322;/&#20998;&#31867;&#24403;&#21069;&#24739;&#32773;&#29366;&#20917;&#25110;&#39044;&#27979;&#19981;&#33391;&#20020;&#24202;&#20107;&#20214;&#21644;&#32467;&#26524;&#38750;&#24120;&#37325;&#35201;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#26088;&#22312;&#25552;&#39640;&#24739;&#32773;&#25252;&#29702;&#27700;&#24179;&#12290;&#23398;&#20064;&#20020;&#24202;&#24207;&#21015;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#23427;&#20204;&#30340;&#20010;&#20307;&#24046;&#24322;&#24615;&#12290;&#26681;&#25454;&#28508;&#22312;&#30340;&#20020;&#24202;&#26465;&#20214;&#65292;&#27599;&#20010;&#24739;&#32773;&#30340;&#24207;&#21015;&#21487;&#33021;&#21253;&#21547;&#19981;&#21516;&#30340;&#20020;&#24202;&#20107;&#20214;&#38598;&#65288;&#35266;&#23519;&#65292;&#23454;&#39564;&#23460;&#32467;&#26524;&#65292;&#33647;&#29289;&#65292;&#31243;&#24207;&#65289;&#12290;&#22240;&#27492;&#65292;&#20165;&#22522;&#20110;&#35768;&#22810;&#19981;&#21516;&#24739;&#32773;&#30340;&#20107;&#20214;&#24207;&#21015;&#23398;&#20064;&#30340;&#31616;&#21333;&#32676;&#20307;&#33539;&#22260;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#29305;&#23450;&#30340;&#20107;&#20214;&#24207;&#21015;&#21160;&#24577;&#21644;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#22810;&#20010;&#26032;&#30340;&#20107;&#20214;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#21644;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35843;&#25972;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical event sequences consist of hundreds of clinical events that represent records of patient care in time. Developing accurate predictive models of such sequences is of a great importance for supporting a variety of models for interpreting/classifying the current patient condition, or predicting adverse clinical events and outcomes, all aimed to improve patient care. One important challenge of learning predictive models of clinical sequences is their patient-specific variability. Based on underlying clinical conditions, each patient's sequence may consist of different sets of clinical events (observations, lab results, medications, procedures). Hence, simple population-wide models learned from event sequences for many different patients may not accurately predict patient-specific dynamics of event sequences and their differences. To address the problem, we propose and investigate multiple new event sequence prediction models and methods that let us better adjust the prediction for
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#25968;&#23398;&#31616;&#31572;&#39064;&#30340;&#27491;&#30830;&#24615;&#21644;&#35823;&#35299;&#65292;&#24182;&#36890;&#36807;&#20540;&#35782;&#21035;&#27969;&#31243;&#25552;&#20379;&#38024;&#23545;&#24615;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2308.11006</link><description>&lt;p&gt;
&#22312;&#38544;&#24335;&#25968;&#23398;&#31616;&#31572;&#39064;&#30340;&#33258;&#21160;&#35780;&#20272;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using language models in the implicit automated assessment of mathematical short answer items. (arXiv:2308.11006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11006
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#25968;&#23398;&#31616;&#31572;&#39064;&#30340;&#27491;&#30830;&#24615;&#21644;&#35823;&#35299;&#65292;&#24182;&#36890;&#36807;&#20540;&#35782;&#21035;&#27969;&#31243;&#25552;&#20379;&#38024;&#23545;&#24615;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#29305;&#23450;&#25968;&#23398;&#31616;&#31572;&#39064;&#26500;&#36896;&#22238;&#31572;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#27969;&#31243;&#26469;&#35782;&#21035;&#23398;&#29983;&#22238;&#31572;&#20013;&#25351;&#23450;&#30340;&#20851;&#38190;&#20540;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#22238;&#31572;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#35782;&#21035;&#20219;&#20309;&#35823;&#35299;&#12290;&#26469;&#33258;&#20540;&#35782;&#21035;&#27969;&#31243;&#30340;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#21521;&#25945;&#24072;&#21644;&#23398;&#29983;&#25552;&#20379;&#21453;&#39304;&#12290;&#20540;&#35782;&#21035;&#27969;&#31243;&#30001;&#20004;&#20010;&#32463;&#36807;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#21028;&#26029;&#19968;&#20010;&#20540;&#26159;&#21542;&#38544;&#21547;&#22312;&#23398;&#29983;&#22238;&#31572;&#20013;&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#35782;&#21035;&#20851;&#38190;&#20540;&#22312;&#22238;&#31572;&#20013;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#20219;&#20309;&#25552;&#31034;&#21644;&#20540;&#65292;&#20197;&#21450;&#27599;&#20010;&#25552;&#31034;&#21644;&#20540;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;&#20540;&#35782;&#21035;&#27969;&#31243;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#35780;&#20272;&#26356;&#20934;&#30830;&#21644;&#26377;&#20449;&#24687;&#37327;&#12290;&#23427;&#21487;&#20197;&#29992;&#26469;&#21521;&#23398;&#29983;&#25552;&#20379;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new way to assess certain short constructed responses to mathematics items. Our approach uses a pipeline that identifies the key values specified by the student in their response. This allows us to determine the correctness of the response, as well as identify any misconceptions. The information from the value identification pipeline can then be used to provide feedback to the teacher and student. The value identification pipeline consists of two fine-tuned language models. The first model determines if a value is implicit in the student response. The second model identifies where in the response the key value is specified. We consider both a generic model that can be used for any prompt and value, as well as models that are specific to each prompt and value. The value identification pipeline is a more accurate and informative way to assess short constructed responses than traditional rubric-based scoring. It can be used to provide more targeted feedback to students, which
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22270;&#20687;&#35782;&#21035;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20809;&#35889;&#21355;&#26143;&#25968;&#25454;&#20013;&#30002;&#28919;&#27844;&#28431;&#30340;&#33258;&#21160;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11003</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#20809;&#35889;&#21355;&#26143;&#25968;&#25454;&#20013;&#33258;&#21160;&#26816;&#27979;&#30002;&#28919;&#25490;&#25918;
&lt;/p&gt;
&lt;p&gt;
Autonomous Detection of Methane Emissions in Multispectral Satellite Data Using Deep Learning. (arXiv:2308.11003v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22270;&#20687;&#35782;&#21035;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20809;&#35889;&#21355;&#26143;&#25968;&#25454;&#20013;&#30002;&#28919;&#27844;&#28431;&#30340;&#33258;&#21160;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30002;&#28919;&#26159;&#26368;&#24378;&#25928;&#30340;&#28201;&#23460;&#27668;&#20307;&#20043;&#19968;&#65292;&#20854;&#30701;&#26242;&#30340;&#22823;&#27668;&#21322;&#34928;&#26399;&#20351;&#20854;&#25104;&#20026;&#36805;&#36895;&#25233;&#21046;&#20840;&#29699;&#21464;&#26262;&#30340;&#20027;&#35201;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30002;&#28919;&#25490;&#25918;&#30417;&#27979;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#36817;&#20284;&#30340;&#25490;&#25918;&#22240;&#23376;&#25110;&#33258;&#25105;&#25253;&#21578;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20005;&#37325;&#20302;&#20272;&#20102;&#25490;&#25918;&#37327;&#12290;&#34429;&#28982;&#21021;&#22987;&#35774;&#35745;&#29992;&#20110;&#30417;&#27979;&#22320;&#34920;&#29305;&#24615;&#65292;&#20294;&#21355;&#26143;&#22810;&#20809;&#35889;&#25968;&#25454;&#26368;&#36817;&#24050;&#25104;&#20026;&#20998;&#26512;&#22823;&#27668;&#20869;&#23481;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22810;&#20809;&#35889;&#20202;&#22120;&#30340;&#20809;&#35889;&#20998;&#36776;&#29575;&#36739;&#20302;&#65292;&#30002;&#28919;&#27979;&#37327;&#36890;&#24120;&#38750;&#24120;&#22024;&#26434;&#12290;&#30002;&#28919;&#25968;&#25454;&#20135;&#21697;&#36824;&#23545;&#22320;&#34920;&#21644;&#20854;&#20182;&#22823;&#27668;&#27668;&#20307;&#65288;&#23588;&#20854;&#26159;&#27700;&#33976;&#27668;&#65289;&#30340;&#21560;&#25910;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#25552;&#20379;&#20102;&#22024;&#26434;&#30340;&#28508;&#22312;&#30002;&#28919;&#20113;&#22270;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22270;&#20687;&#35782;&#21035;&#33021;&#21147;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#30002;&#28919;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methane is one of the most potent greenhouse gases, and its short atmospheric half-life makes it a prime target to rapidly curb global warming. However, current methane emission monitoring techniques primarily rely on approximate emission factors or self-reporting, which have been shown to often dramatically underestimate emissions. Although initially designed to monitor surface properties, satellite multispectral data has recently emerged as a powerful method to analyze atmospheric content. However, the spectral resolution of multispectral instruments is poor, and methane measurements are typically very noisy. Methane data products are also sensitive to absorption by the surface and other atmospheric gases (water vapor in particular) and therefore provide noisy maps of potential methane plumes, that typically require extensive human analysis. Here, we show that the image recognition capabilities of deep learning methods can be leveraged to automatize the detection of methane leaks in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;ChatGPT&#22312;&#19981;&#21516;&#20351;&#29992;&#26041;&#38754;&#30340;&#24773;&#24863;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#25216;&#26415;&#65292;&#20351;&#24471;&#36825;&#31181;&#20998;&#26512;&#19981;&#21463;&#25991;&#26412;&#25968;&#25454;&#38271;&#24230;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26032;&#25968;&#25454;&#38598;&#30340;&#23453;&#36149;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2308.11001</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26469;&#20998;&#26512;&#30740;&#31350;&#20154;&#21592;&#23545;ChatGPT&#30340;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about ChatGPT. (arXiv:2308.11001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;ChatGPT&#22312;&#19981;&#21516;&#20351;&#29992;&#26041;&#38754;&#30340;&#24773;&#24863;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#25216;&#26415;&#65292;&#20351;&#24471;&#36825;&#31181;&#20998;&#26512;&#19981;&#21463;&#25991;&#26412;&#25968;&#25454;&#38271;&#24230;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26032;&#25968;&#25454;&#38598;&#30340;&#23453;&#36149;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#21019;&#26032;&#24615;&#21457;&#26126;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#21457;&#20102;&#24191;&#27867;&#30340;&#35752;&#35770;&#12290;&#23613;&#31649;&#24198;&#31069;&#20854;&#21508;&#31181;&#20248;&#28857;&#65292;&#20294;&#23545;&#20854;&#27491;&#30830;&#24615;&#21644;&#20351;&#29992;&#20262;&#29702;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#24050;&#32463;&#22312;&#21162;&#21147;&#25429;&#25417;&#29992;&#25143;&#23545;&#20854;&#30340;&#24773;&#24863;&#65292;&#20294;&#22914;&#20309;&#20998;&#26512;&#30740;&#31350;&#30028;&#20851;&#20110;ChatGPT&#19981;&#21516;&#20351;&#29992;&#26041;&#38754;&#30340;&#24773;&#24863;&#26159;&#19968;&#20010;&#20540;&#24471;&#25506;&#35752;&#30340;&#38382;&#39064;&#12290;&#22312;&#26631;&#20934;&#30340;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#20013;&#65292;&#36890;&#24120;&#21482;&#24212;&#29992;&#20110;&#23569;&#37327;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#22312;&#30701;&#25991;&#26412;&#25968;&#25454;&#19978;&#20165;&#33719;&#24471;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26469;&#20419;&#36827;&#23545;&#30740;&#31350;&#25968;&#25454;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20026;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#25299;&#23637;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#26368;&#26032;&#25216;&#26415;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#35265;&#65292;&#20351;&#24471;&#36825;&#31181;&#20998;&#26512;&#19981;&#21463;&#25991;&#26412;&#25968;&#25454;&#38271;&#24230;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The groundbreaking invention of ChatGPT has triggered enormous discussion among users across all fields and domains. Among celebration around its various advantages, questions have been raised with regards to its correctness and ethics of its use. Efforts are already underway towards capturing user sentiments around it. But it begs the question as to how the research community is analyzing ChatGPT with regards to various aspects of its usage. It is this sentiment of the researchers that we analyze in our work. Since Aspect-Based Sentiment Analysis has usually only been applied on a few datasets, it gives limited success and that too only on short text data. We propose a methodology that uses Explainable AI to facilitate such analysis on research data. Our technique presents valuable insights into extending the state of the art of Aspect-Based Sentiment Analysis on newer datasets, where such analysis is not hampered by the length of the text data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20540;&#30340;&#22686;&#37327;&#35889;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#23376;&#38598;&#24182;&#36827;&#34892;&#32858;&#31867;&#21644;&#21512;&#24182;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#32858;&#31867;&#25972;&#20010;&#25968;&#25454;&#38598;&#30456;&#36817;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10999</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#20540;&#30340;&#22686;&#37327;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Eigenvalue-based Incremental Spectral Clustering. (arXiv:2308.10999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20540;&#30340;&#22686;&#37327;&#35889;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#23376;&#38598;&#24182;&#36827;&#34892;&#32858;&#31867;&#21644;&#21512;&#24182;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#32858;&#31867;&#25972;&#20010;&#25968;&#25454;&#38598;&#30456;&#36817;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20043;&#21069;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#65288;&#30701;&#65289;&#25991;&#26723;&#30340;&#23376;&#38598;&#21512;&#65288;&#21253;&#21547;&#20960;&#30334;&#20010;&#26465;&#30446;&#65289;&#22312;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#20540;&#35889;&#19978;&#26377;&#20849;&#21516;&#30340;&#24402;&#19968;&#21270;&#26041;&#24335;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35889;&#32858;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20197;&#19979;&#27493;&#39588;&#65306;&#65288;1&#65289;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#21487;&#31649;&#29702;&#30340;&#23376;&#38598;&#65292;&#65288;2&#65289;&#23545;&#27599;&#20010;&#23376;&#38598;&#36827;&#34892;&#32858;&#31867;&#65292;&#65288;3&#65289;&#22522;&#20110;&#29305;&#24449;&#20540;&#35889;&#30340;&#30456;&#20284;&#24615;&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#23376;&#38598;&#30340;&#32858;&#31867;&#65292;&#24418;&#25104;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#26679;&#26412;&#37327;&#22823;&#23567;&#21457;&#29983;&#24378;&#28872;&#21464;&#21270;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20363;&#22914;&#20856;&#22411;&#30340;&#35889;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23454;&#38469;&#19978;&#23545;&#23376;&#38598;&#36827;&#34892;&#32858;&#31867;&#21644;&#21512;&#24182;&#21487;&#20197;&#24471;&#21040;&#19982;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#32858;&#31867;&#30456;&#36817;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our previous experiments demonstrated that subsets collections of (short) documents (with several hundred entries) share a common normalized in some way eigenvalue spectrum of combinatorial Laplacian. Based on this insight, we propose a method of incremental spectral clustering. The method consists of the following steps: (1) split the data into manageable subsets, (2) cluster each of the subsets, (3) merge clusters from different subsets based on the eigenvalue spectrum similarity to form clusters of the entire set. This method can be especially useful for clustering methods of complexity strongly increasing with the size of the data sample,like in case of typical spectral clustering. Experiments were performed showing that in fact the clustering and merging the subsets yields clusters close to clustering the entire dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20197;&#38477;&#20302;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.10997</link><description>&lt;p&gt;
SPEGTI: &#32467;&#26500;&#39044;&#27979;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models. (arXiv:2308.10997v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20197;&#38477;&#20302;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#26082;&#36924;&#30495;&#21448;&#19982;&#25991;&#26412;&#25552;&#31034;&#30456;&#31526;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36136;&#37327;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65306;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#37117;&#26159;&#36845;&#20195;&#24335;&#30340;&#65292;&#38656;&#35201;&#22810;&#27425;&#36816;&#34892;&#25512;&#26029;&#65292;&#24182;&#20351;&#29992;&#22823;&#27169;&#22411;&#12290;&#36825;&#31181;&#36845;&#20195;&#36807;&#31243;&#26159;&#20026;&#20102;&#30830;&#20445;&#22270;&#20687;&#30340;&#19981;&#21516;&#21306;&#22495;&#19981;&#20165;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#65292;&#36824;&#19982;&#20854;&#20182;&#21306;&#22495;&#30456;&#23481;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#30456;&#23481;&#24615;&#65292;&#20351;&#29992;&#20102;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65288;MRF&#65289;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;Muse&#27169;&#22411;&#37197;&#21512;&#20351;&#29992;&#12290;MRF&#32534;&#30721;&#20102;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#30340;&#22270;&#20687;&#26631;&#35760;&#20043;&#38388;&#30340;&#30456;&#23481;&#24615;&#65292;&#24182;&#19988;&#20351;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;Muse&#39044;&#27979;&#27493;&#39588;&#12290;&#20351;&#29992;MRF&#30340;&#25512;&#26029;&#25104;&#26412;&#22823;&#22823;&#38477;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#24555;&#36895;&#23398;&#20064;&#20854;&#21442;&#25968;&#65292;&#36890;&#36807;&#23545;MRF&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MR
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#22825;&#27668;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#25429;&#25417;&#22797;&#26434;&#27169;&#24335;&#21644;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.10995</link><description>&lt;p&gt;
&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Techniques in Extreme Weather Events: A Review. (arXiv:2308.10995v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#22825;&#27668;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#25429;&#25417;&#22797;&#26434;&#27169;&#24335;&#21644;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#31934;&#30830;&#20998;&#26512;&#21644;&#31934;&#30830;&#39044;&#27979;&#30340;&#25216;&#26415;&#26469;&#20943;&#36731;&#20854;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#22825;&#27668;&#39044;&#25253;&#21644;&#29702;&#35299;&#26497;&#31471;&#22825;&#27668;&#21160;&#21147;&#23398;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#39046;&#22495;&#20869;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#38647;&#26292;&#12289;&#38378;&#30005;&#12289;&#38477;&#27700;&#12289;&#24178;&#26097;&#12289;&#28909;&#28010;&#12289;&#23506;&#28526;&#21644;&#28909;&#24102;&#27668;&#26059;&#31561;&#22825;&#27668;&#39044;&#27979;&#26041;&#38754;&#30340;&#21033;&#29992;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#22914;&#20854;&#25429;&#25417;&#22797;&#26434;&#27169;&#24335;&#21644;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#27668;&#35937;&#23398;&#39046;&#22495;&#21457;&#23637;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#20174;&#36825;&#20010;&#31995;&#32479;&#32508;&#36848;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;&#23545;&#31185;&#23398;&#30028;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extreme weather events pose significant challenges, thereby demanding techniques for accurate analysis and precise forecasting to mitigate its impact. In recent years, deep learning techniques have emerged as a promising approach for weather forecasting and understanding the dynamics of extreme weather events. This review aims to provide a comprehensive overview of the state-of-the-art deep learning in the field. We explore the utilization of deep learning architectures, across various aspects of weather prediction such as thunderstorm, lightning, precipitation, drought, heatwave, cold waves and tropical cyclones. We highlight the potential of deep learning, such as its ability to capture complex patterns and non-linear relationships. Additionally, we discuss the limitations of current approaches and highlight future directions for advancements in the field of meteorology. The insights gained from this systematic review are crucial for the scientific community to make informed decision
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Supervised Contrastive Learning&#21644;&#27431;&#27663;&#36317;&#31163;&#30340;&#31616;&#21333;&#26041;&#27861;&#65288;SupEuclid&#65289;&#22312;OoD&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#26080;&#38656;&#22797;&#26434;&#27169;&#22411;&#25110;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#20026;&#36827;&#19968;&#27493;&#23454;&#39564;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#24378;&#22823;&#19988;&#26131;&#29992;&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2308.10973</link><description>&lt;p&gt;
SupEuclid&#65306;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#27431;&#27663;&#36317;&#31163;&#36827;&#34892;&#26497;&#31616;&#39640;&#36136;&#37327;OoD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SupEuclid: Extremely Simple, High Quality OoD Detection with Supervised Contrastive Learning and Euclidean Distance. (arXiv:2308.10973v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10973
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Supervised Contrastive Learning&#21644;&#27431;&#27663;&#36317;&#31163;&#30340;&#31616;&#21333;&#26041;&#27861;&#65288;SupEuclid&#65289;&#22312;OoD&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#26080;&#38656;&#22797;&#26434;&#27169;&#22411;&#25110;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#20026;&#36827;&#19968;&#27493;&#23454;&#39564;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#24378;&#22823;&#19988;&#26131;&#29992;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;OoD&#65288;Out-of-Distribution&#65289;&#26816;&#27979;&#21462;&#24471;&#20102;&#26174;&#33879;&#21457;&#23637;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#25509;&#36817;&#29978;&#33267;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;&#25968;&#25454;&#20998;&#31163;&#25928;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#36890;&#24120;&#28041;&#21450;&#22823;&#22411;&#25110;&#22797;&#26434;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#65292;&#26292;&#38706;&#20110;OoD&#31034;&#20363;&#25110;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20351;&#29992;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#30340;&#26041;&#27861;&#23601;&#21487;&#20197;&#36229;&#36234;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SCL&#65289;&#35757;&#32451;&#30340;ResNet18&#22312;&#36817;&#36317;&#31163;&#21644;&#36828;&#36317;&#31163;OoD&#26816;&#27979;&#22522;&#20934;&#19978;&#20165;&#20351;&#29992;&#27431;&#27663;&#36317;&#31163;&#20316;&#20026;&#24471;&#20998;&#35268;&#21017;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#24320;&#31665;&#21363;&#29992;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#21487;&#33021;&#28040;&#38500;&#20102;&#23545;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#25110;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#33267;&#23569;&#25552;&#20379;&#20102;&#19968;&#20010;&#38750;&#24120;&#24378;&#22823;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#22522;&#32447;&#20379;&#36827;&#19968;&#27493;&#23454;&#39564;&#21644;&#20998;&#26512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution (OoD) detection has developed substantially in the past few years, with available methods approaching, and in a few cases achieving, perfect data separation on standard benchmarks. These results generally involve large or complex models, pretraining, exposure to OoD examples or extra hyperparameter tuning. Remarkably, it is possible to achieve results that can exceed many of these state-of-the-art methods with a very simple method. We demonstrate that ResNet18 trained with Supervised Contrastive Learning (SCL) produces state-of-the-art results out-of-the-box on near and far OoD detection benchmarks using only Euclidean distance as a scoring rule. This may obviate the need in some cases for more sophisticated methods or larger models, and at the very least provides a very strong, easy to use baseline for further experimentation and analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#20174;&#20302;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20020;&#24202;MRI&#25195;&#25551;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10968</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;MRI&#22330;&#36716;&#31227;&#37325;&#24314;&#65306;&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;
&lt;/p&gt;
&lt;p&gt;
MRI Field-transfer Reconstruction with Limited Data: Regularization by Neural Style Transfer. (arXiv:2308.10968v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#20174;&#20302;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20020;&#24202;MRI&#25195;&#25551;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;MRI&#37325;&#24314;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25253;&#21578;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#38477;&#22122;&#65288;RED&#65289;&#27491;&#35268;&#21270;&#26159;&#19968;&#31181;&#23558;&#38477;&#22122;&#22120;&#20316;&#20026;&#22270;&#20687;&#37325;&#24314;&#20808;&#39564;&#30340;&#36890;&#29992;&#27969;&#31243;&#12290;RED&#30340;&#28508;&#21147;&#24050;&#32463;&#22312;&#22810;&#20010;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#65288;&#22914;&#38477;&#22122;&#12289;&#21435;&#27169;&#31946;&#21644;&#36229;&#20998;&#36776;&#29575;&#65289;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;RNST&#65289;&#26041;&#27861;&#36827;&#34892;&#27491;&#35268;&#21270;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#31070;&#32463;&#36716;&#31227;&#21644;&#38477;&#22122;&#24341;&#25806;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;RNST&#33021;&#22815;&#20174;&#26377;&#22122;&#22768;&#30340;&#20302;&#36136;&#37327;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#22270;&#20687;&#39118;&#26684;&#21644;&#26377;&#38480;&#25968;&#25454;&#19981;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;1.5T&#21644;3T&#30340;&#20020;&#24202;MRI&#25195;&#25551;&#39564;&#35777;&#20102;RNST&#65292;&#24182;&#19988;&#26174;&#31034;RNST&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;RNST&#26694;&#26550;&#22312;MRI&#37325;&#24314;&#21644;&#26377;&#38480;&#25968;&#25454;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated success in MRI reconstruction using deep learning-based models. However, most reported approaches require training on a task-specific, large-scale dataset. Regularization by denoising (RED) is a general pipeline which embeds a denoiser as a prior for image reconstruction. The potential of RED has been demonstrated for multiple image-related tasks such as denoising, deblurring and super-resolution. In this work, we propose a regularization by neural style transfer (RNST) method to further leverage the priors from the neural transfer and denoising engine. This enables RNST to reconstruct a high-quality image from a noisy low-quality image with different image styles and limited data. We validate RNST with clinical MRI scans from 1.5T and 3T and show that RNST can significantly boost image quality. Our results highlight the capability of the RNST framework for MRI reconstruction and the potential for reconstruction tasks with limited data.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22312;&#25239;&#33740;&#32957;&#21457;&#29616;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#21306;&#20998;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#21270;&#21512;&#29289;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#25239;&#33740;&#32957;&#12290;</title><link>http://arxiv.org/abs/2308.10921</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25239;&#33740;&#32957;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence-driven antimicrobial peptide discovery. (arXiv:2308.10921v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10921
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25239;&#33740;&#32957;&#21457;&#29616;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#21306;&#20998;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#21270;&#21512;&#29289;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#25239;&#33740;&#32957;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#33740;&#32957;&#65288;AMPs&#65289;&#20316;&#20026;&#23545;&#25239;&#25239;&#33740;&#32784;&#33647;&#24615;&#30340;&#26377;&#24076;&#26395;&#30340;&#33647;&#29289;&#20195;&#26367;&#20256;&#32479;&#25239;&#29983;&#32032;&#12290;&#36890;&#36807;&#21306;&#20998;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#38761;&#26032;&#20102;AMP&#21457;&#29616;&#12290;&#37492;&#21035;&#22120;&#36890;&#36807;&#39044;&#27979;&#20851;&#38190;&#30340;&#32957;&#24615;&#36136;&#65288;&#22914;&#27963;&#24615;&#21644;&#27602;&#24615;&#65289;&#26469;&#24110;&#21161;&#35782;&#21035;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#21270;&#21512;&#29289;&#65292;&#32780;&#29983;&#25104;&#22120;&#23398;&#20064;&#32957;&#30340;&#20998;&#24067;&#24182;&#33021;&#22815;&#37319;&#26679;&#26032;&#30340;AMP&#20505;&#36873;&#21270;&#21512;&#29289;&#65292;&#21487;&#20197;&#26159;&#20840;&#26032;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#21407;&#22411;&#32957;&#30340;&#31867;&#20284;&#29289;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37492;&#21035;&#22120;&#24341;&#23548;&#30340;&#36807;&#28388;&#12289;&#20165;&#38024;&#23545;&#27491;&#26679;&#26412;&#30340;&#23398;&#20064;&#12289;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#20197;&#21450;&#26465;&#20214;&#21644;&#20248;&#21270;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;AMP&#30340;&#21487;&#25511;&#29983;&#25104;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#22522;&#20110;AI&#30340;AMP&#21457;&#29616;&#30340;&#26368;&#26032;&#25104;&#26524;&#65292;&#24182;&#31361;&#20986;&#20102;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antimicrobial peptides (AMPs) emerge as promising agents against antimicrobial resistance, providing an alternative to conventional antibiotics. Artificial intelligence (AI) revolutionized AMP discovery through both discrimination and generation approaches. The discriminators aid the identification of promising candidates by predicting key peptide properties such as activity and toxicity, while the generators learn the distribution over peptides and enable sampling novel AMP candidates, either de novo, or as analogues of a prototype peptide. Moreover, the controlled generation of AMPs with desired properties is achieved by discriminator-guided filtering, positive-only learning, latent space sampling, as well as conditional and optimized generation. Here we review recent achievements in AI-driven AMP discovery, highlighting the most exciting directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#20351;&#29992;GCN&#23618;&#26469;&#26377;&#25928;&#20256;&#25773;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#21450;&#29305;&#21035;&#35774;&#35745;&#30340;&#24322;&#24120;&#31038;&#21306;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26500;&#21644;&#23646;&#24615;&#24046;&#24322;&#30340;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24605;&#36335;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.10918</link><description>&lt;p&gt;
&#22522;&#20110;Metapath&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge. (arXiv:2308.10918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#20351;&#29992;GCN&#23618;&#26469;&#26377;&#25928;&#20256;&#25773;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#21450;&#29305;&#21035;&#35774;&#35745;&#30340;&#24322;&#24120;&#31038;&#21306;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26500;&#21644;&#23646;&#24615;&#24046;&#24322;&#30340;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24605;&#36335;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;MSAD&#65289;&#65292;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#37117;&#20351;&#29992;GCN&#23618;&#26469;&#26377;&#25928;&#22320;&#20256;&#25773;&#24322;&#24120;&#21644;&#27491;&#24120;&#33410;&#28857;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;Metapath&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35774;&#35745;&#21644;&#29305;&#21035;&#31934;&#24515;&#35774;&#35745;&#30340;&#24322;&#24120;&#31038;&#21306;&#22686;&#24378;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#32467;&#26500;&#21644;&#23646;&#24615;&#24046;&#24322;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;&#19971;&#20010;&#30495;&#23454;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#32508;&#21512;&#23454;&#39564;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;MSAD&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#20248;&#36234;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#37325;&#28857;&#26159;&#20248;&#21270;&#21644;&#20998;&#26512;Metapath&#27169;&#24335;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection has attracted considerable attention in recent years. This paper introduces a novel approach that leverages metapath-based semi-supervised learning, addressing the limitations of previous methods. We present a new framework, Metapath-based Semi-supervised Anomaly Detection (MSAD), incorporating GCN layers in both the encoder and decoder to efficiently propagate context information between abnormal and normal nodes. The design of metapath-based context information and a specifically crafted anomaly community enhance the process of learning differences in structures and attributes, both globally and locally. Through a comprehensive set of experiments conducted on seven real-world networks, this paper demonstrates the superiority of the MSAD method compared to state-of-the-art techniques. The promising results of this study pave the way for future investigations, focusing on the optimization and analysis of metapath patterns to further enhance the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#27169;&#22411;&#65288;SMA&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#20449;&#24687;&#65292;&#25104;&#21151;&#20998;&#31867;&#20102;&#30284;&#30151;&#20122;&#22411;&#12290;&#36890;&#36807;&#34701;&#21512;&#27169;&#22359;&#28145;&#24230;&#34701;&#21512;Siamese&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21442;&#25968;&#20016;&#23500;&#24230;&#12290;&#22312;&#27169;&#25311;&#12289;&#21333;&#32454;&#32990;&#21644;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#20013;&#65292;SMA&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;F1&#23439;&#35266;&#12289;F1&#21152;&#26435;&#21644;&#30284;&#30151;&#20122;&#22411;&#30340;&#20934;&#30830;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.10917</link><description>&lt;p&gt;
PACS&#65306;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#27169;&#22411;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#30284;&#30151;&#20122;&#22411;&#39044;&#27979;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
PACS: Prediction and analysis of cancer subtypes from multi-omics data based on a multi-head attention mechanism model. (arXiv:2308.10917v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#27169;&#22411;&#65288;SMA&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#20449;&#24687;&#65292;&#25104;&#21151;&#20998;&#31867;&#20102;&#30284;&#30151;&#20122;&#22411;&#12290;&#36890;&#36807;&#34701;&#21512;&#27169;&#22359;&#28145;&#24230;&#34701;&#21512;Siamese&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21442;&#25968;&#20016;&#23500;&#24230;&#12290;&#22312;&#27169;&#25311;&#12289;&#21333;&#32454;&#32990;&#21644;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#20013;&#65292;SMA&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;F1&#23439;&#35266;&#12289;F1&#21152;&#26435;&#21644;&#30284;&#30151;&#20122;&#22411;&#30340;&#20934;&#30830;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30284;&#30151;&#30340;&#39640;&#24322;&#36136;&#24615;&#21644;&#20020;&#24202;&#29305;&#24449;&#65292;&#19981;&#21516;&#30284;&#30151;&#20122;&#22411;&#20043;&#38388;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#21644;&#20020;&#24202;&#29305;&#24449;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#20998;&#31867;&#30284;&#30151;&#20122;&#22411;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#65292;&#25913;&#21892;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#27169;&#22411;&#65288;SMA&#65289;&#65292;&#25104;&#21151;&#20998;&#31867;&#20102;&#30284;&#30151;&#20122;&#22411;&#12290;SMA&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#29305;&#24449;&#20849;&#20139;&#27169;&#22359;&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#34701;&#21512;&#27169;&#22359;&#65292;&#28145;&#24230;&#34701;&#21512;&#20102;Siamese&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#65292;&#20016;&#23500;&#20102;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SMA&#27169;&#22411;&#22312;&#27169;&#25311;&#12289;&#21333;&#32454;&#32990;&#21644;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;F1&#23439;&#35266;&#12289;F1&#21152;&#26435;&#21644;&#30284;&#30151;&#20122;&#22411;&#30340;&#20934;&#30830;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omic data and clinical characteristics among different cancer subtypes. Therefore, accurate classification of cancer subtypes can help doctors choose the most appropriate treatment options, improve treatment outcomes, and provide more accurate patient survival predictions. In this study, we propose a supervised multi-head attention mechanism model (SMA) to classify cancer subtypes successfully. The attention mechanism and feature sharing module of the SMA model can successfully learn the global and local feature information of multi-omics data. Second, it enriches the parameters of the model by deeply fusing multi-head attention encoders from Siamese through the fusion module. Validated by extensive experiments, the SMA model achieves the highest accuracy, F1 macroscopic, F1 weighted, and accurate classification of cancer subtypes in simulated, single-cell, and cancer multio
&lt;/p&gt;</description></item><item><title>DiffPrep &#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#19988;&#39640;&#25928;&#22320;&#25628;&#32034;&#25968;&#25454;&#39044;&#22788;&#29702;&#31649;&#36947;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#36890;&#36807;&#23558;&#25968;&#25454;&#39044;&#22788;&#29702;&#31649;&#36947;&#25628;&#32034;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#31163;&#25955;&#30340;&#38750;&#21487;&#24494;&#25628;&#32034;&#31354;&#38388;&#36716;&#21270;&#20026;&#36830;&#32493;&#31354;&#38388;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.10915</link><description>&lt;p&gt;
DiffPrep: &#21487;&#24494;&#20998;&#25968;&#25454;&#39044;&#22788;&#29702;&#31649;&#36947;&#25628;&#32034;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DiffPrep: Differentiable Data Preprocessing Pipeline Search for Learning over Tabular Data. (arXiv:2308.10915v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10915
&lt;/p&gt;
&lt;p&gt;
DiffPrep &#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#19988;&#39640;&#25928;&#22320;&#25628;&#32034;&#25968;&#25454;&#39044;&#22788;&#29702;&#31649;&#36947;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#36890;&#36807;&#23558;&#25968;&#25454;&#39044;&#22788;&#29702;&#31649;&#36947;&#25628;&#32034;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#31163;&#25955;&#30340;&#38750;&#21487;&#24494;&#25628;&#32034;&#31354;&#38388;&#36716;&#21270;&#20026;&#36830;&#32493;&#31354;&#38388;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39044;&#22788;&#29702;&#26159;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23427;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#21270;&#20026;&#23545;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#21487;&#29992;&#30340;&#26684;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#24448;&#24448;&#32791;&#26102;&#19988;&#20195;&#20215;&#39640;&#26114;&#65292;&#36890;&#24120;&#38656;&#35201;&#39046;&#22495;&#19987;&#23478;&#30340;&#19987;&#38376;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064; (AutoML) &#26694;&#26550;&#22768;&#31216;&#21487;&#20197;&#33258;&#21160;&#21270;&#25968;&#25454;&#39044;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#20351;&#29992;&#20102;&#21463;&#38480;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31649;&#36947;&#25628;&#32034;&#31354;&#38388;&#65292;&#36825;&#38480;&#21046;&#20102;&#28508;&#22312;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#30001;&#20110;&#38656;&#35201;&#22810;&#27425;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20204;&#36890;&#24120;&#36895;&#24230;&#36807;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DiffPrep&#65292;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#19988;&#39640;&#25928;&#22320;&#25628;&#32034;&#32473;&#23450;&#34920;&#26684;&#25968;&#25454;&#38598;&#21644;&#21487;&#24494;&#20998;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31649;&#36947;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#39044;&#22788;&#29702;&#31649;&#36947;&#25628;&#32034;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#39640;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#31163;&#25955;&#30340;&#38750;&#21487;&#24494;&#25628;&#32034;&#31354;&#38388;&#36716;&#21270;&#24182;&#25918;&#26494;&#20026;&#36830;&#32493;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data preprocessing is a crucial step in the machine learning process that transforms raw data into a more usable format for downstream ML models. However, it can be costly and time-consuming, often requiring the expertise of domain experts. Existing automated machine learning (AutoML) frameworks claim to automate data preprocessing. However, they often use a restricted search space of data preprocessing pipelines which limits the potential performance gains, and they are often too slow as they require training the ML model multiple times. In this paper, we propose DiffPrep, a method that can automatically and efficiently search for a data preprocessing pipeline for a given tabular dataset and a differentiable ML model such that the performance of the ML model is maximized. We formalize the problem of data preprocessing pipeline search as a bi-level optimization problem. To solve this problem efficiently, we transform and relax the discrete, non-differential search space into a continuo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#27979;&#32534;&#30721;&#36827;&#34892;&#34394;&#25311;&#29615;&#22659;&#26144;&#23556;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26681;&#25454;&#24863;&#30693;&#25968;&#25454;&#26500;&#24314;&#31354;&#38388;&#22320;&#22270;&#65292;&#24182;&#22312;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#19979;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10913</link><description>&lt;p&gt;
&#33258;&#21160;&#21033;&#29992;&#35270;&#35273;&#39044;&#27979;&#32534;&#30721;&#36827;&#34892;&#34394;&#25311;&#29615;&#22659;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Automated mapping of virtual environments with visual predictive coding. (arXiv:2308.10913v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#27979;&#32534;&#30721;&#36827;&#34892;&#34394;&#25311;&#29615;&#22659;&#26144;&#23556;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26681;&#25454;&#24863;&#30693;&#25968;&#25454;&#26500;&#24314;&#31354;&#38388;&#22320;&#22270;&#65292;&#24182;&#22312;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#19979;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26681;&#25454;&#24863;&#30693;&#36755;&#20837;&#30452;&#25509;&#26500;&#24314;&#23545;&#29615;&#22659;&#30340;&#20869;&#37096;&#35748;&#30693;&#22320;&#22270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20855;&#26377;&#26126;&#30830;&#22352;&#26631;&#25110;&#36317;&#31163;&#27979;&#37327;&#30340;&#31995;&#32479;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22914;SLAM&#21033;&#29992;&#19987;&#38376;&#30340;&#35270;&#35273;&#25512;&#29702;&#36807;&#31243;&#20174;&#35270;&#35273;&#21644;&#37324;&#31243;&#35745;&#25968;&#25454;&#20013;&#35782;&#21035;&#35270;&#35273;&#29305;&#24449;&#24182;&#26500;&#24314;&#31354;&#38388;&#22320;&#22270;&#65292;&#20294;&#22823;&#33041;&#20013;&#35748;&#30693;&#22320;&#22270;&#30340;&#19968;&#33324;&#24615;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;&#32479;&#19968;&#30340;&#26144;&#23556;&#31639;&#27861;&#31574;&#30053;&#26469;&#27867;&#21270;&#21040;&#21548;&#35273;&#12289;&#35302;&#35273;&#21644;&#35821;&#35328;&#36755;&#20837;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#27979;&#32534;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#19988;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#24863;&#30693;&#25968;&#25454;&#26500;&#24314;&#31354;&#38388;&#22320;&#22270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#23548;&#33322;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35270;&#35273;&#39044;&#27979;&#32534;&#30721;&#12290;&#22312;&#23398;&#20064;&#19979;&#19968;&#20010;&#22270;&#20687;&#39044;&#27979;&#20219;&#21153;&#30340;&#21516;&#26102;&#65292;&#20195;&#29702;&#20250;&#33258;&#21160;&#26500;&#24314;&#19968;&#20010;&#20869;&#37096;&#23545;&#29615;&#22659;&#30340;&#34920;&#31034;&#65292;&#23450;&#37327;&#22320;&#21453;&#26144;&#20986;&#36317;&#31163;&#31561;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans construct internal cognitive maps of their environment directly from sensory inputs without access to a system of explicit coordinates or distance measurements. While machine learning algorithms like SLAM utilize specialized visual inference procedures to identify visual features and construct spatial maps from visual and odometry data, the general nature of cognitive maps in the brain suggests a unified mapping algorithmic strategy that can generalize to auditory, tactile, and linguistic inputs. Here, we demonstrate that predictive coding provides a natural and versatile neural network algorithm for constructing spatial maps using sensory data. We introduce a framework in which an agent navigates a virtual environment while engaging in visual predictive coding using a self-attention-equipped convolutional neural network. While learning a next image prediction task, the agent automatically constructs an internal representation of the environment that quantitatively reflects dist
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;NASA&#30340;POWER&#25968;&#25454;&#65292;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20102;&#21152;&#32435;&#22235;&#20010;&#20027;&#35201;&#22478;&#24066;&#30340;&#38271;&#26399;&#28201;&#24230;&#36235;&#21183;&#65292;&#32467;&#26524;&#26174;&#31034;&#24037;&#19994;&#22478;&#24066;&#38463;&#20811;&#25289;&#26377;&#26126;&#26174;&#30340;&#21464;&#26262;&#36235;&#21183;&#65292;&#24182;&#19988;XGBoost&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;</title><link>http://arxiv.org/abs/2308.10909</link><description>&lt;p&gt;
&#26681;&#25454;NASA&#30340;POWER&#25968;&#25454;&#22312;&#21152;&#32435;&#20027;&#35201;&#22478;&#24066;&#20013;&#22522;&#20110;&#32479;&#35745;&#20998;&#26512;&#30740;&#31350;&#20840;&#29699;&#21464;&#26262;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Global Warming In Ghana's Major Cities Based on Statistical Analysis of NASA's POWER Over 3-Decades. (arXiv:2308.10909v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10909
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;NASA&#30340;POWER&#25968;&#25454;&#65292;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20102;&#21152;&#32435;&#22235;&#20010;&#20027;&#35201;&#22478;&#24066;&#30340;&#38271;&#26399;&#28201;&#24230;&#36235;&#21183;&#65292;&#32467;&#26524;&#26174;&#31034;&#24037;&#19994;&#22478;&#24066;&#38463;&#20811;&#25289;&#26377;&#26126;&#26174;&#30340;&#21464;&#26262;&#36235;&#21183;&#65292;&#24182;&#19988;XGBoost&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#21464;&#26262;&#23545;&#19990;&#30028;&#21508;&#22320;&#39640;&#28201;&#30340;&#24433;&#21709;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20195;&#34920;&#19981;&#21516;&#27668;&#20505;&#21306;&#22495;&#30340;&#21152;&#32435;&#22235;&#20010;&#20027;&#35201;&#22478;&#24066;&#30340;&#38271;&#26399;&#28201;&#24230;&#36235;&#21183;&#12290;&#20351;&#29992;NASA&#30340;Prediction of Worldwide Energy Resource (POWER)&#25968;&#25454;&#65292;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#35780;&#20272;&#20102;&#24403;&#22320;&#27668;&#20505;&#21464;&#26262;&#21450;&#20854;&#24433;&#21709;&#12290;&#32447;&#24615;&#22238;&#24402;&#36235;&#21183;&#20998;&#26512;&#21644;eXtreme Gradient Boosting (XGBoost)&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#28201;&#24230;&#21464;&#21270;&#12290;&#36890;&#36807;RSLab&#24179;&#21488;&#29983;&#25104;&#30340;&#22320;&#34920;&#28201;&#24230;&#65288;LST&#65289;&#21078;&#38754;&#22270;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#24403;&#22320;&#30340;&#21464;&#26262;&#36235;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#22478;&#24066;&#38463;&#20811;&#25289;&#12290;&#20154;&#21475;&#22240;&#32032;&#19981;&#26174;&#33879;&#12290;XGBoost&#27169;&#22411;&#30340;&#20302;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#24471;&#20998;&#26174;&#31034;&#20102;&#25429;&#25417;&#28201;&#24230;&#27169;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#29926;&#38215;&#24847;&#22806;&#22320;&#25317;&#26377;&#26368;&#39640;&#30340;&#24179;&#22343;&#28201;&#24230;&#12290;&#39044;&#35745;2023&#24180;&#20013;&#26399;&#30340;&#24179;&#22343;&#28201;&#24230;&#20026;&#65306;&#38463;&#20811;&#25289;27.86&#8451;&#65292;&#24211;&#39532;&#35199;27.15&#8451;&#65292;&#20975;&#29305;&#20811;&#25289;&#22855;29.39&#8451;&#65292;&#29926;&#38215;30.76&#8451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global warming's impact on high temperatures in various parts of the world has raised concerns. This study investigates long-term temperature trends in four major Ghanaian cities representing distinct climatic zones. Using NASA's Prediction of Worldwide Energy Resource (POWER) data, statistical analyses assess local climate warming and its implications. Linear regression trend analysis and eXtreme Gradient Boosting (XGBoost) machine learning predict temperature variations. Land Surface Temperature (LST) profile maps generated from the RSLab platform enhance accuracy. Results reveal local warming trends, particularly in industrialized Accra. Demographic factors aren't significant. XGBoost model's low Root Mean Square Error (RMSE) scores demonstrate effectiveness in capturing temperature patterns. Wa unexpectedly has the highest mean temperature. Estimated mean temperatures for mid-2023 are: Accra 27.86{\deg}C, Kumasi 27.15{\deg}C, Kete-Krachi 29.39{\deg}C, and Wa 30.76{\deg}C. These fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#36816;&#32500;&#65288;MLOps&#65289;&#26041;&#27861;&#22312;&#35299;&#20915;&#38382;&#39064;&#20013;&#30340;&#21487;&#25509;&#21463;&#31572;&#26696;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;22&#31687;&#24212;&#29992;MLOps&#29702;&#24565;&#30340;&#35770;&#25991;&#12290;&#30740;&#31350;&#21457;&#29616;&#38656;&#35201;&#26356;&#21152;&#26377;&#25928;&#30340;MLOps&#26041;&#27861;&#20197;&#20943;&#23569;&#20154;&#31867;&#30340;&#21442;&#19982;&#12290;</title><link>http://arxiv.org/abs/2308.10908</link><description>&lt;p&gt;
MLOps&#65306;&#19968;&#39033;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MLOps: A Review. (arXiv:2308.10908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#36816;&#32500;&#65288;MLOps&#65289;&#26041;&#27861;&#22312;&#35299;&#20915;&#38382;&#39064;&#20013;&#30340;&#21487;&#25509;&#21463;&#31572;&#26696;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;22&#31687;&#24212;&#29992;MLOps&#29702;&#24565;&#30340;&#35770;&#25991;&#12290;&#30740;&#31350;&#21457;&#29616;&#38656;&#35201;&#26356;&#21152;&#26377;&#25928;&#30340;MLOps&#26041;&#27861;&#20197;&#20943;&#23569;&#20154;&#31867;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#25509;&#21463;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#21457;&#23637;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#30001;&#20110;&#23427;&#37319;&#29992;&#35745;&#31639;&#26041;&#27861;&#26469;&#25945;&#23548;&#26426;&#22120;&#24182;&#20135;&#29983;&#21487;&#25509;&#21463;&#30340;&#31572;&#26696;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#26426;&#22120;&#23398;&#20064;&#36816;&#32500;&#65288;MLOps&#65289;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20026;&#36825;&#31867;&#38382;&#39064;&#25552;&#20379;&#21487;&#25509;&#21463;&#30340;&#31572;&#26696;&#12290;&#20026;&#20102;&#24110;&#21161;&#21019;&#24314;&#26131;&#20110;&#20351;&#29992;&#30340;&#36719;&#20214;&#65292;&#20316;&#32773;&#30740;&#31350;&#20102;MLOps&#26041;&#27861;&#12290;&#20026;&#20102;&#36873;&#25321;&#36866;&#29992;&#20110;&#29305;&#23450;&#39033;&#30446;&#30340;&#26368;&#20339;&#24037;&#20855;&#32467;&#26500;&#65292;&#20316;&#32773;&#36824;&#35780;&#20272;&#20102;&#21508;&#31181;MLOps&#26041;&#27861;&#30340;&#29305;&#24615;&#21644;&#21487;&#25805;&#20316;&#24615;&#12290;&#24635;&#20849;&#35780;&#20272;&#20102;22&#31687;&#35797;&#22270;&#24212;&#29992;MLOps&#29702;&#24565;&#30340;&#35770;&#25991;&#12290;&#26368;&#21518;&#65292;&#20316;&#32773;&#25215;&#35748;&#20102;&#22522;&#20110;&#36825;&#26679;&#30340;&#36827;&#23637;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#20154;&#31867;&#21442;&#19982;&#26469;&#33258;&#25105;&#35843;&#33410;&#30340;&#23436;&#20840;&#26377;&#25928;&#30340;MLOps&#26041;&#27861;&#30340;&#31232;&#32570;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Machine Learning (ML) has become a widely accepted method for significant progress that is rapidly evolving. Since it employs computational methods to teach machines and produce acceptable answers. The significance of the Machine Learning Operations (MLOps) methods, which can provide acceptable answers for such problems, is examined in this study. To assist in the creation of software that is simple to use, the authors research MLOps methods. To choose the best tool structure for certain projects, the authors also assess the features and operability of various MLOps methods. A total of 22 papers were assessed that attempted to apply the MLOps idea. Finally, the authors admit the scarcity of fully effective MLOps methods based on which advancements can self-regulate by limiting human engagement.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;TVM&#20013;8&#20301;&#37327;&#21270;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#20860;&#23481;&#24615;&#21644;&#20248;&#21270;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2308.10905</link><description>&lt;p&gt;
&#22312;TVM&#20013;&#20998;&#26512;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Analyzing Quantization in TVM. (arXiv:2308.10905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;TVM&#20013;8&#20301;&#37327;&#21270;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#20860;&#23481;&#24615;&#21644;&#20248;&#21270;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23545;&#26435;&#37325;&#24352;&#37327;&#36827;&#34892;&#37327;&#21270;&#20197;&#20943;&#23569;&#25512;&#29702;&#24310;&#36831;&#21644;&#20869;&#23384;&#21344;&#29992;&#30340;&#30740;&#31350;&#24050;&#32463;&#26377;&#24456;&#22810;&#12290;TVM&#20063;&#20855;&#22791;&#25903;&#25345;&#20302;&#27604;&#29305;&#35745;&#31639;&#21644;&#37327;&#21270;&#26435;&#37325;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#36890;&#24120;&#26399;&#26395;&#36890;&#36807;&#37327;&#21270;&#26469;&#25552;&#39640;&#25512;&#29702;&#26102;&#38388;&#65292;&#22312;TVM&#20013;&#65292;8&#20301;&#37327;&#21270;&#30340;&#24615;&#33021;&#21364;&#19981;&#33021;&#28385;&#36275;&#26399;&#26395;&#12290;&#36890;&#24120;&#22312;&#23558;8&#20301;&#37327;&#21270;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#36890;&#24120;&#26399;&#26395;&#36798;&#21040;&#20840;&#31934;&#24230;&#25512;&#29702;&#26102;&#38388;&#30340;50%&#24038;&#21491;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#37327;&#21270;&#29256;&#26412;&#19981;&#20165;&#26410;&#33021;&#23454;&#29616;&#25152;&#26399;&#26395;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#23454;&#38469;&#19978;&#24615;&#33021;&#26356;&#24046;&#65292;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#32422;&#20026;&#38750;&#37327;&#21270;&#29256;&#26412;&#30340;&#20004;&#20493;&#24930;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#24615;&#33021;&#19981;&#20339;&#30340;&#21407;&#22240;&#65292;&#35780;&#20272;&#20102;8&#20301;&#37327;&#21270;&#22312;TVM&#20013;&#30340;&#20860;&#23481;&#24615;&#21644;&#20248;&#21270;&#26426;&#20250;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
There has been many papers in academic literature on quantizing weight tensors in deep learning models to reduce inference latency and memory footprint. TVM also has the ability to quantize weights and support low-bit computations. Although quantization is typically expected to improve inference time, in TVM, the performance of 8-bit quantization does not meet the expectations. Typically, when applying 8-bit quantization to a deep learning model, it is usually expected to achieve around 50% of the full-precision inference time. However, in this particular case, not only does the quantized version fail to achieve the desired performance boost, but it actually performs worse, resulting in an inference time that is about 2 times as slow as the non-quantized version. In this project, we thoroughly investigate the reasons behind the underperformance and assess the compatibility and optimization opportunities of 8-bit quantization in TVM. We discuss the optimization of two different types of
&lt;/p&gt;</description></item><item><title>&#27492;&#25968;&#25454;&#21457;&#24067;&#21253;&#21547;Majorana&#31034;&#33539;&#22120;&#23454;&#39564;&#30340;&#26657;&#20934;&#25968;&#25454;&#23376;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35813;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.10856</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;Majorana&#31034;&#33539;&#22120;&#25968;&#25454;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
Majorana Demonstrator Data Release for AI/ML Applications. (arXiv:2308.10856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10856
&lt;/p&gt;
&lt;p&gt;
&#27492;&#25968;&#25454;&#21457;&#24067;&#21253;&#21547;Majorana&#31034;&#33539;&#22120;&#23454;&#39564;&#30340;&#26657;&#20934;&#25968;&#25454;&#23376;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35813;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27492;&#25968;&#25454;&#21457;&#24067;&#21253;&#21547;Majorana&#31034;&#33539;&#22120;&#23454;&#39564;&#30340;&#26657;&#20934;&#25968;&#25454;&#23376;&#38598;&#12290;&#27599;&#20010;Majorana&#20107;&#20214;&#37117;&#26377;&#21407;&#22987;&#30340;&#38167;&#25506;&#27979;&#22120;&#27874;&#24418;&#12289;&#33033;&#20914;&#24418;&#29366;&#35782;&#21035;&#20999;&#21106;&#21644;&#26657;&#20934;&#21518;&#30340;&#26368;&#32456;&#33021;&#37327;&#65292;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#20197;HDF5&#25991;&#20214;&#26684;&#24335;&#19982;&#30456;&#20851;&#20803;&#25968;&#25454;&#19968;&#21516;&#20998;&#20139;&#12290;&#27492;&#21457;&#24067;&#26088;&#22312;&#25903;&#25345;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enclosed data release consists of a subset of the calibration data from the Majorana Demonstrator experiment. Each Majorana event is accompanied by raw Germanium detector waveforms, pulse shape discrimination cuts, and calibrated final energies, all shared in an HDF5 file format along with relevant metadata. This release is specifically designed to support the training and testing of Artificial Intelligence (AI) and Machine Learning (ML) algorithms upon our data. This document is structured as follows. Section I provides an overview of the dataset's content and format; Section II outlines the location of this dataset and the method for accessing it; Section III presents the NPML Machine Learning Challenge associated with this dataset; Section IV contains a disclaimer from the Majorana collaboration regarding the use of this dataset; Appendix A contains technical details of this data release. Please direct questions about the material provided within this release to liaobo77@ucsd.ed
&lt;/p&gt;</description></item><item><title>DynED&#26159;&#19968;&#31181;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;MRR&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.10807</link><description>&lt;p&gt;
DynED: &#25968;&#25454;&#27969;&#20998;&#31867;&#20013;&#30340;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
DynED: Dynamic Ensemble Diversification in Data Stream Classification. (arXiv:2308.10807v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10807
&lt;/p&gt;
&lt;p&gt;
DynED&#26159;&#19968;&#31181;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;MRR&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#31361;&#21464;&#24615;&#21464;&#21270;&#65292;&#20063;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38598;&#21512;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290; &#22312;&#38598;&#21512;&#20869;&#37096;&#30340;&#26356;&#22823;&#22810;&#26679;&#24615;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#38598;&#21512;&#20869;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#24456;&#39640;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#32452;&#20214;&#37117;&#20687;&#39044;&#26399;&#30340;&#37027;&#26679;&#23545;&#25972;&#20307;&#24615;&#33021;&#26377;&#25152;&#36129;&#29486;&#12290;&#36825;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#36873;&#25321;&#23637;&#29616;&#20986;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MMR&#65288;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#65289;&#30340;&#26032;&#22411;&#38598;&#21512;&#26500;&#24314;&#21644;&#32500;&#25252;&#26041;&#27861;&#65292;&#22312;&#32452;&#21512;&#38598;&#21512;&#30340;&#36807;&#31243;&#20013;&#21160;&#24577;&#22320;&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#21644;11&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65288;DynED&#65289;&#30456;&#27604;&#20110;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
Ensemble methods are commonly used in classification due to their remarkable performance. Achieving high accuracy in a data stream environment is a challenging task considering disruptive changes in the data distribution, also known as concept drift. A greater diversity of ensemble components is known to enhance prediction accuracy in such settings. Despite the diversity of components within an ensemble, not all contribute as expected to its overall performance. This necessitates a method for selecting components that exhibit high performance and diversity. We present a novel ensemble construction and maintenance approach based on MMR (Maximal Marginal Relevance) that dynamically combines the diversity and prediction accuracy of components during the process of structuring an ensemble. The experimental results on both four real and 11 synthetic datasets demonstrate that the proposed approach (DynED) provides a higher average mean accuracy compared to the five state-of-the-art baselines
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#20013;PAIRED&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#20854;&#22312;&#23454;&#38469;&#24615;&#33021;&#19978;&#33021;&#22815;&#19982;&#25110;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.10797</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#23545;&#25163;&#31283;&#23450;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Unsupervised Environment Design with a Learned Adversary. (arXiv:2308.10797v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#20013;PAIRED&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#20854;&#22312;&#23454;&#38469;&#24615;&#33021;&#19978;&#33021;&#22815;&#19982;&#25110;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#20855;&#22791;&#26222;&#36941;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#35774;&#35745;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#20419;&#36827;&#24191;&#27867;&#27867;&#21270;&#21644;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#20010;&#25361;&#25112;&#39537;&#21160;&#20102;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#20854;&#20013;&#23398;&#29983;&#26234;&#33021;&#20307;&#22312;&#30001;&#25945;&#24072;&#26234;&#33021;&#20307;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#20998;&#24067;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;UED&#30340;&#20808;&#39537;&#26041;&#27861;&#26159;PAIRED&#65292;&#23427;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#25945;&#24072;&#31574;&#30053;&#20174;&#22836;&#24320;&#22987;&#35774;&#35745;&#20219;&#21153;&#65292;&#36825;&#26679;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#36866;&#24212;&#26234;&#33021;&#20307;&#24403;&#21069;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#26377;&#24456;&#24378;&#30340;&#29702;&#35770;&#25903;&#25345;&#65292;&#20294;PAIRED&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#22952;&#30861;&#20102;&#20854;&#23454;&#38469;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20381;&#36182;&#20110;&#31574;&#21010;&#21644;&#21464;&#24322;&#65292;&#32780;&#19981;&#26159;&#29983;&#25104;&#26032;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;PAIRED&#30340;&#20960;&#20010;&#20851;&#38190;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;PAIRED&#33021;&#22815;&#19982;&#25110;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in training generally-capable agents is the design of training tasks that facilitate broad generalization and robustness to environment variations. This challenge motivates the problem setting of Unsupervised Environment Design (UED), whereby a student agent trains on an adaptive distribution of tasks proposed by a teacher agent. A pioneering approach for UED is PAIRED, which uses reinforcement learning (RL) to train a teacher policy to design tasks from scratch, making it possible to directly generate tasks that are adapted to the agent's current capabilities. Despite its strong theoretical backing, PAIRED suffers from a variety of challenges that hinder its practical performance. Thus, state-of-the-art methods currently rely on curation and mutation rather than generation of new tasks. In this work, we investigate several key shortcomings of PAIRED and propose solutions for each shortcoming. As a result, we make it possible for PAIRED to match or exceed state-of-the-a
&lt;/p&gt;</description></item><item><title>FocalDreamer&#26159;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;3D&#32534;&#36753;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#28966;&#28857;&#34701;&#21512;&#35013;&#37197;&#30340;&#26041;&#24335;&#23454;&#29616;&#31934;&#32454;&#21270;&#32534;&#36753;&#65292;&#24182;&#19988;&#22312;&#20960;&#20309;&#21644;&#39118;&#26684;&#19978;&#33021;&#22815;&#20445;&#25345;&#25972;&#20307;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;FocalDreamer&#22312;&#32534;&#36753;&#33021;&#21147;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.10608</link><description>&lt;p&gt;
FocalDreamer:&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#28966;&#28857;&#34701;&#21512;&#35013;&#37197;&#30340;3D&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
FocalDreamer: Text-driven 3D Editing via Focal-fusion Assembly. (arXiv:2308.10608v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10608
&lt;/p&gt;
&lt;p&gt;
FocalDreamer&#26159;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;3D&#32534;&#36753;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#28966;&#28857;&#34701;&#21512;&#35013;&#37197;&#30340;&#26041;&#24335;&#23454;&#29616;&#31934;&#32454;&#21270;&#32534;&#36753;&#65292;&#24182;&#19988;&#22312;&#20960;&#20309;&#21644;&#39118;&#26684;&#19978;&#33021;&#22815;&#20445;&#25345;&#25972;&#20307;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;FocalDreamer&#22312;&#32534;&#36753;&#33021;&#21147;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25991;&#26412;&#39537;&#21160;&#30340;3D&#32534;&#36753;&#22312;&#21033;&#29992;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26032;&#20852;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#25552;&#20379;&#20869;&#23481;&#21019;&#20316;&#25152;&#24517;&#38656;&#30340;&#21487;&#20998;&#31163;&#12289;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FocalDreamer&#65292;&#36825;&#26159;&#19968;&#20010;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#23558;&#22522;&#26412;&#24418;&#29366;&#19982;&#21487;&#32534;&#36753;&#37096;&#20998;&#21512;&#24182;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#25152;&#38656;&#21306;&#22495;&#36827;&#34892;&#32454;&#31890;&#24230;&#32534;&#36753;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;FocalDreamer&#20351;&#29992;&#20960;&#20309;&#32852;&#21512;&#21644;&#21452;&#36335;&#24452;&#28210;&#26579;&#23558;&#29420;&#31435;&#30340;3D&#37096;&#20998;&#32452;&#35013;&#25104;&#23436;&#25972;&#30340;&#23545;&#35937;&#65292;&#26041;&#20415;&#23454;&#20363;&#37325;&#29992;&#21644;&#37096;&#20998;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20309;&#28966;&#28857;&#25439;&#22833;&#21644;&#39118;&#26684;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65292;&#20419;&#36827;&#28966;&#28857;&#34701;&#21512;&#21644;&#19968;&#33268;&#30340;&#25972;&#20307;&#22806;&#35266;&#12290;&#27492;&#22806;&#65292;FocalDreamer&#29983;&#25104;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#24418;&#24341;&#25806;&#20860;&#23481;&#30340;&#39640;&#20445;&#30495;&#24230;&#30340;&#20960;&#20309;&#21644;PBR&#32441;&#29702;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;FocalDreamer&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#20013;&#30340;&#20248;&#31168;&#32534;&#36753;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While text-3D editing has made significant strides in leveraging score distillation sampling, emerging approaches still fall short in delivering separable, precise and consistent outcomes that are vital to content creation. In response, we introduce FocalDreamer, a framework that merges base shape with editable parts according to text prompts for fine-grained editing within desired regions. Specifically, equipped with geometry union and dual-path rendering, FocalDreamer assembles independent 3D parts into a complete object, tailored for convenient instance reuse and part-wise control. We propose geometric focal loss and style consistency regularization, which encourage focal fusion and congruent overall appearance. Furthermore, FocalDreamer generates high-fidelity geometry and PBR textures which are compatible with widely-used graphics engines. Extensive experiments have highlighted the superior editing capabilities of FocalDreamer in both quantitative and qualitative evaluations.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22522;&#20110;&#27492;&#26694;&#26550;&#26500;&#24314;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#22810;&#35270;&#22270;&#32534;&#30721;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.10522</link><description>&lt;p&gt;
&#20449;&#24687;&#29702;&#35770;&#24341;&#23548;&#30340;&#21551;&#21457;&#24335;&#28176;&#36827;&#24335;&#22810;&#35270;&#22270;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2308.10522v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10522
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22522;&#20110;&#27492;&#26694;&#26550;&#26500;&#24314;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#22810;&#35270;&#22270;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20174;&#22810;&#20010;&#35270;&#22270;&#20013;&#25429;&#33719;&#20849;&#20139;&#19978;&#19979;&#25991;&#30340;&#20840;&#38754;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30452;&#35266;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#19981;&#21516;&#35270;&#22270;&#30340;&#25104;&#23545;&#26041;&#24335;&#65292;&#36825;&#20173;&#28982;&#21487;&#25193;&#23637;&#65306;&#23398;&#20064;&#35270;&#22270;&#20849;&#20139;&#34920;&#31034;&#26102;&#26410;&#36807;&#28388;&#35270;&#22270;&#29305;&#23450;&#30340;&#22122;&#22768;&#65307;&#34394;&#20551;&#30340;&#36127;&#23545;&#20013;&#65292;&#36127;&#39033;&#23454;&#38469;&#19978;&#22312;&#21516;&#19968;&#31867;&#21035;&#20013;&#19982;&#27491;&#39033;&#30456;&#21516;&#65292;&#24182;&#19988;&#30495;&#27491;&#30340;&#36127;&#23545;&#34987;&#21516;&#31561;&#23545;&#24453;&#65307;&#22343;&#21248;&#22320;&#34913;&#37327;&#26415;&#35821;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21487;&#33021;&#24178;&#25200;&#20248;&#21270;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#30740;&#31350;&#24191;&#20041;&#33258;&#30417;&#30563;&#22810;&#35270;&#22270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22810;&#20110;&#20004;&#20010;&#35270;&#22270;&#30340;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#20102;&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#33539;&#24335;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24191;&#20041;&#22810;&#35270;&#22270;&#23398;&#20064;&#12290;&#22312;&#20854;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#19977;&#23618;&#28176;&#36827;&#24335;&#32467;&#26500;&#30340;&#22810;&#35270;&#22270;&#32534;&#30721;&#26041;&#27861;&#65292;&#21363;In
&lt;/p&gt;
&lt;p&gt;
Multi-view representation learning aims to capture comprehensive information from multiple views of a shared context. Recent works intuitively apply contrastive learning to different views in a pairwise manner, which is still scalable: view-specific noise is not filtered in learning view-shared representations; the fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated; evenly measuring the similarities between terms might interfere with optimization. Importantly, few works study the theoretical framework of generalized self-supervised multi-view learning, especially for more than two views. To this end, we rethink the existing multi-view learning paradigm from the perspective of information theory and then propose a novel information theoretical framework for generalized multi-view learning. Guided by it, we build a multi-view coding method with a three-tier progressive architecture, namely In
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#25513;&#33180;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (Mask-RCNN) &#23545;&#23391;&#21152;&#25289;&#25991;&#26723;&#36827;&#34892;&#29256;&#38754;&#20998;&#26512;&#65292;&#25552;&#21319;&#24615;&#33021;&#12290;&#36890;&#36807;&#36880;&#27493;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#25105;&#20204;&#36798;&#21040;&#20102;0.889&#30340;&#33391;&#22909;dice&#20998;&#25968;&#12290;&#34429;&#28982;&#22312;&#24212;&#29992;&#33521;&#25991;&#25991;&#26723;&#27169;&#22411;&#26102;&#36935;&#21040;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#20294;&#36825;&#34920;&#26126;&#27599;&#31181;&#35821;&#35328;&#37117;&#26377;&#20854;&#29305;&#23450;&#30340;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2308.10511</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#33180;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (Mask-RCNN) &#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#29256;&#38754;&#20998;&#26512;&#24615;&#33021;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis. (arXiv:2308.10511v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#25513;&#33180;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (Mask-RCNN) &#23545;&#23391;&#21152;&#25289;&#25991;&#26723;&#36827;&#34892;&#29256;&#38754;&#20998;&#26512;&#65292;&#25552;&#21319;&#24615;&#33021;&#12290;&#36890;&#36807;&#36880;&#27493;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#25105;&#20204;&#36798;&#21040;&#20102;0.889&#30340;&#33391;&#22909;dice&#20998;&#25968;&#12290;&#34429;&#28982;&#22312;&#24212;&#29992;&#33521;&#25991;&#25991;&#26723;&#27169;&#22411;&#26102;&#36935;&#21040;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#20294;&#36825;&#34920;&#26126;&#27599;&#31181;&#35821;&#35328;&#37117;&#26377;&#20854;&#29305;&#23450;&#30340;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#25968;&#23383;&#25991;&#26723;&#23601;&#20687;&#35299;&#20915;&#19968;&#20010;&#35868;&#39064;&#65292;&#23588;&#20854;&#26159;&#21382;&#21490;&#25991;&#26723;&#12290;&#25991;&#26723;&#29256;&#38754;&#20998;&#26512;(DLA)&#36890;&#36807;&#23558;&#25991;&#26723;&#21010;&#20998;&#20026;&#27573;&#33853;&#12289;&#22270;&#29255;&#21644;&#34920;&#26684;&#31561;&#37096;&#20998;&#65292;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#35868;&#39064;&#12290;&#36825;&#23545;&#20110;&#26426;&#22120;&#26469;&#38405;&#35835;&#21644;&#29702;&#35299;&#36825;&#20123;&#25991;&#26723;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;DL Sprint 2.0&#31454;&#36187;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#29702;&#35299;&#23391;&#21152;&#25289;&#25991;&#26723;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BaDLAD&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#24456;&#22810;&#20363;&#23376;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#29305;&#27530;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;Mask R-CNN&#65292;&#26469;&#24110;&#21161;&#29702;&#35299;&#12290;&#36890;&#36807;&#36880;&#27493;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#36825;&#20010;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;0.889&#30340;&#22909;&#30340;dice&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#26159;&#19968;&#20999;&#37117;&#36827;&#34892;&#24471;&#38750;&#24120;&#23436;&#32654;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#33521;&#25991;&#25991;&#26723;&#27169;&#22411;&#65292;&#20294;&#23427;&#19982;&#23391;&#21152;&#25289;&#25991;&#19981;&#22826;&#21305;&#37197;&#12290;&#36825;&#21521;&#25105;&#20204;&#23637;&#31034;&#20102;&#27599;&#31181;&#35821;&#35328;&#37117;&#26377;&#33258;&#24049;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;DL Sprint 2.0&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;https://www.kaggle.com/competitions/dlsprint2/discussion/432201&#20844;&#24320;&#33719;&#24471;&#65292;&#20854;&#20013;&#21253;&#25324;notebooks&#12289;&#26435;&#37325;&#21644;&#25512;&#29702;&#31508;&#35760;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding digital documents is like solving a puzzle, especially historical ones. Document Layout Analysis (DLA) helps with this puzzle by dividing documents into sections like paragraphs, images, and tables. This is crucial for machines to read and understand these documents. In the DL Sprint 2.0 competition, we worked on understanding Bangla documents. We used a dataset called BaDLAD with lots of examples. We trained a special model called Mask R-CNN to help with this understanding. We made this model better by step-by-step hyperparameter tuning, and we achieved a good dice score of 0.889. However, not everything went perfectly. We tried using a model trained for English documents, but it didn't fit well with Bangla. This showed us that each language has its own challenges. Our solution for the DL Sprint 2.0 is publicly available at https://www.kaggle.com/competitions/dlsprint2/discussion/432201 along with notebooks, weights, and inference notebook.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26426;&#21046;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;PhraseTransformer&#65292;&#22312;&#36234;&#21335;&#35821;-&#20013;&#25991;&#24179;&#34892;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;BLEU&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.10482</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26426;&#21046;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Effective Method using Phrase Mechanism in Neural Machine Translation. (arXiv:2308.10482v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26426;&#21046;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;PhraseTransformer&#65292;&#22312;&#36234;&#21335;&#35821;-&#20013;&#25991;&#24179;&#34892;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;BLEU&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#29616;&#23454;&#29983;&#27963;&#65292;&#24182;&#23545;NLP&#30740;&#31350;&#31038;&#21306;&#30340;&#20854;&#20182;&#20219;&#21153;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27492;&#39046;&#22495;&#21560;&#24341;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#65292;&#24182;&#22312;&#22823;&#22810;&#25968;&#35821;&#35328;&#23545;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26426;&#21046;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;PhraseTransformer&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#32447;&#27169;&#22411;Transformer&#26500;&#24314;&#24179;&#34892;&#35821;&#26009;&#24211;&#36234;&#21335;&#35821;-&#20013;&#25991;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#12290;&#25105;&#20204;&#22312;VLSP 2022&#31454;&#36187;&#30340;MT&#25968;&#25454;&#38598;&#19978;&#65292;&#23454;&#29616;&#20102;&#36234;&#21335;&#35821;&#21040;&#20013;&#25991;&#30340;35.3 BLEU&#24471;&#20998;&#21644;&#20013;&#25991;&#21040;&#36234;&#21335;&#35821;&#30340;33.2 BLEU&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/phuongnm94/PhraseTransformer&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Translation is one of the essential tasks in Natural Language Processing (NLP), which has massive applications in real life as well as contributing to other tasks in the NLP research community. Recently, Transformer -based methods have attracted numerous researchers in this domain and achieved state-of-the-art results in most of the pair languages. In this paper, we report an effective method using a phrase mechanism, PhraseTransformer, to improve the strong baseline model Transformer in constructing a Neural Machine Translation (NMT) system for parallel corpora Vietnamese-Chinese. Our experiments on the MT dataset of the VLSP 2022 competition achieved the BLEU score of 35.3 on Vietnamese to Chinese and 33.2 BLEU scores on Chinese to Vietnamese data. Our code is available at https://github.com/phuongnm94/PhraseTransformer.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#30340;&#26032;&#32452;&#20214;&#65292;&#22312;&#26222;&#36890;&#30340;Transformer&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20132;&#36890;&#39044;&#27979;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#31354;&#20851;&#31995;&#21644;&#26102;&#38388;&#20449;&#24687;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10425</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#65292;&#26222;&#36890;Transformer&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting. (arXiv:2308.10425v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#30340;&#26032;&#32452;&#20214;&#65292;&#22312;&#26222;&#36890;&#30340;Transformer&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20132;&#36890;&#39044;&#27979;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#31354;&#20851;&#31995;&#21644;&#26102;&#38388;&#20449;&#24687;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20934;&#30830;&#30340;&#20132;&#36890;&#39044;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20854;&#26680;&#24515;&#29942;&#39048;&#22312;&#20110;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20132;&#36890;&#27169;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20855;&#26377;&#22797;&#26434;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#27493;&#36935;&#21040;&#20102;&#24615;&#33021;&#25910;&#30410;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#30340;&#26032;&#32452;&#20214;&#65292;&#21487;&#20197;&#22312;&#26222;&#36890;&#30340;Transformer&#20013;&#33719;&#24471;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;Spatio-Temporal Adaptive Embedding transformer&#65288;STAEformer&#65289;&#22312;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20132;&#36890;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#36890;&#36807;&#26377;&#25928;&#25429;&#25417;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20869;&#22312;&#26102;&#31354;&#20851;&#31995;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of the Intelligent Transportation System (ITS), accurate traffic forecasting has emerged as a critical challenge. The key bottleneck lies in capturing the intricate spatio-temporal traffic patterns. In recent years, numerous neural networks with complicated architectures have been proposed to address this issue. However, the advancements in network architectures have encountered diminishing performance gains. In this study, we present a novel component called spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. Our proposed Spatio-Temporal Adaptive Embedding transformer (STAEformer) achieves state-of-the-art performance on five real-world traffic forecasting datasets. Further experiments demonstrate that spatio-temporal adaptive embedding plays a crucial role in traffic forecasting by effectively capturing intrinsic spatio-temporal relations and chronological information in traffic time series.
&lt;/p&gt;</description></item><item><title>FedSIS&#26159;&#19968;&#31181;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#39046;&#22495;&#27867;&#21270;&#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992;&#32852;&#37030;&#24335;&#20998;&#21106;&#23398;&#20064;&#21644;&#20013;&#38388;&#34920;&#31034;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#38754;&#37096;&#23637;&#31034;&#25915;&#20987;&#26816;&#27979;&#31639;&#27861;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.10236</link><description>&lt;p&gt;
FedSIS: &#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#20998;&#21106;&#23398;&#20064;&#21644;&#20013;&#38388;&#34920;&#31034;&#37319;&#26679;&#30340;&#32852;&#37030;&#24335;&#38754;&#37096;&#23637;&#31034;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
FedSIS: Federated Split Learning with Intermediate Representation Sampling for Privacy-preserving Generalized Face Presentation Attack Detection. (arXiv:2308.10236v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10236
&lt;/p&gt;
&lt;p&gt;
FedSIS&#26159;&#19968;&#31181;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#39046;&#22495;&#27867;&#21270;&#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992;&#32852;&#37030;&#24335;&#20998;&#21106;&#23398;&#20064;&#21644;&#20013;&#38388;&#34920;&#31034;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#38754;&#37096;&#23637;&#31034;&#25915;&#20987;&#26816;&#27979;&#31639;&#27861;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#38754;&#37096;&#23637;&#31034;&#25915;&#20987;&#26816;&#27979;&#65288;FacePAD&#65289;&#31639;&#27861;&#30340;&#33268;&#21629;&#32570;&#28857;&#26159;&#23545;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;/&#25915;&#20987;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#22686;&#24378;FacePAD&#35299;&#20915;&#26041;&#26696;&#27867;&#21270;&#33021;&#21147;&#30340;&#23581;&#35797;&#20551;&#35774;&#21487;&#20197;&#36890;&#36807;&#21333;&#19968;&#23454;&#20307;&#25910;&#38598;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#26469;&#36827;&#34892;&#38598;&#20013;&#24335;&#35757;&#32451;&#12290;&#23454;&#38469;&#19978;&#65292;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#21487;&#33021;&#30001;&#19981;&#21516;&#30340;&#23454;&#20307;&#25910;&#38598;&#65292;&#30001;&#20110;&#27861;&#24459;&#21644;&#38544;&#31169;&#38480;&#21046;&#65292;&#20182;&#20204;&#24448;&#24448;&#26080;&#27861;&#20849;&#20139;&#33258;&#24049;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#32852;&#21512;&#23398;&#20064;&#33539;&#24335;&#65288;&#22914;&#32852;&#37030;&#23398;&#20064;&#65289;&#21487;&#20197;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26631;&#20934;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#19981;&#36866;&#21512;&#39046;&#22495;&#27867;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24456;&#38590;&#22788;&#29702;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#32852;&#37030;&#24335;&#20998;&#21106;&#23398;&#20064;&#21644;&#20013;&#38388;&#34920;&#31034;&#37319;&#26679;&#65288;FedSIS&#65289;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#39046;&#22495;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lack of generalization to unseen domains/attacks is the Achilles heel of most face presentation attack detection (FacePAD) algorithms. Existing attempts to enhance the generalizability of FacePAD solutions assume that data from multiple source domains are available with a single entity to enable centralized training. In practice, data from different source domains may be collected by diverse entities, who are often unable to share their data due to legal and privacy constraints. While collaborative learning paradigms such as federated learning (FL) can overcome this problem, standard FL methods are ill-suited for domain generalization because they struggle to surmount the twin challenges of handling non-iid client data distributions during training and generalizing to unseen domains during inference. In this work, a novel framework called Federated Split learning with Intermediate representation Sampling (FedSIS) is introduced for privacy-preserving domain generalization. In FedSIS, a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#21319;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#21517;&#20026;MultiPL-T&#65292;&#36890;&#36807;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.09895</link><description>&lt;p&gt;
&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#29992;&#20110;&#20195;&#30721;LLMs
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#21319;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#21517;&#20026;MultiPL-T&#65292;&#36890;&#36807;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20195;&#30721;LLMs&#65288;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#24320;&#22987;&#23545;&#32534;&#31243;&#23454;&#36341;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#20195;&#30721;LLMs&#36824;&#25104;&#20026;&#32534;&#31243;&#35821;&#35328;&#21644;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20195;&#30721;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#36136;&#37327;&#22312;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20195;&#30721;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#20805;&#20998;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;&#22914;Java&#12289;Python&#25110;JavaScript&#65289;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#20687;OCaml&#21644;Racket&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#25552;&#39640;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#21487;&#29992;&#20110;&#24494;&#35843;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;LLMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;MultiPL-T&#65292;&#23427;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#21313;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate ten
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#23618;&#24863;&#30693;&#23884;&#20837;&#21644;&#32858;&#31867;&#26469;&#35745;&#31639;&#26679;&#26412;&#30340;&#20284;&#28982;&#24615;&#65292;&#24182;&#20351;&#29992;&#24191;&#20041;&#32858;&#28966;&#25439;&#22833;&#20989;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#26679;&#26412;&#36827;&#34892;&#19981;&#21516;&#30340;&#21152;&#26435;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09878</link><description>&lt;p&gt;
DatasetEquity: &#25152;&#26377;&#26679;&#26412;&#26159;&#21542;&#24179;&#31561;&#65311;&#22312;&#25968;&#25454;&#38598;&#20013;&#36861;&#27714;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets. (arXiv:2308.09878v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#23618;&#24863;&#30693;&#23884;&#20837;&#21644;&#32858;&#31867;&#26469;&#35745;&#31639;&#26679;&#26412;&#30340;&#20284;&#28982;&#24615;&#65292;&#24182;&#20351;&#29992;&#24191;&#20041;&#32858;&#28966;&#25439;&#22833;&#20989;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#26679;&#26412;&#36827;&#34892;&#19981;&#21516;&#30340;&#21152;&#26435;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#19981;&#24179;&#34913;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#24402;&#22240;&#20110;&#25968;&#25454;&#25910;&#38598;&#30340;&#25104;&#26412;&#12289;&#26631;&#31614;&#30340;&#22256;&#38590;&#20197;&#21450;&#25968;&#25454;&#30340;&#22320;&#29702;&#20998;&#24067;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#22270;&#20687;&#22806;&#35266;&#24341;&#36215;&#30340;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#20173;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#19982;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#30340;&#20998;&#31867;&#20998;&#24067;&#30456;&#27604;&#65292;&#22270;&#20687;&#22806;&#35266;&#23637;&#31034;&#20102;&#36229;&#20986;&#31867;&#21035;&#26631;&#31614;&#25152;&#25552;&#20379;&#30340;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#20174;&#21407;&#22987;&#20687;&#32032;&#20013;&#25552;&#21462;&#30340;&#28145;&#23618;&#24863;&#30693;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#22806;&#35266;&#30340;&#28145;&#23618;&#24863;&#30693;&#23884;&#20837;&#21644;&#32858;&#31867;&#35745;&#31639;&#26679;&#26412;&#30340;&#20284;&#28982;&#24615;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20284;&#28982;&#24615;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#26679;&#26412;&#36827;&#34892;&#19981;&#21516;&#30340;&#21152;&#26435;&#65292;&#20351;&#29992;&#25552;&#20986;&#30340;\textbf{&#24191;&#20041;&#32858;&#28966;&#25439;&#22833;}&#20989;&#25968;&#12290;&#35813;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#19982;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data imbalance is a well-known issue in the field of machine learning, attributable to the cost of data collection, the difficulty of labeling, and the geographical distribution of the data. In computer vision, bias in data distribution caused by image appearance remains highly unexplored. Compared to categorical distributions using class labels, image appearance reveals complex relationships between objects beyond what class labels provide. Clustering deep perceptual features extracted from raw pixels gives a richer representation of the data. This paper presents a novel method for addressing data imbalance in machine learning. The method computes sample likelihoods based on image appearance using deep perceptual embeddings and clustering. It then uses these likelihoods to weigh samples differently during training with a proposed \textbf{Generalized Focal Loss} function. This loss can be easily integrated with deep learning algorithms. Experiments validate the method's effectiveness a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09765</link><description>&lt;p&gt;
&#21463;&#20919;&#33853;: &#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#21453;&#24046;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#29289;&#20307;&#21521;&#37327;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#27969;&#34892;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#65288;&#22914;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#23545;&#65292;&#24182;&#24573;&#30053;&#20102;&#20174;&#20013;&#25552;&#21462;&#23545;&#35937;&#30340;&#20998;&#24067;&#12290;&#20154;&#31867;&#23545;&#29289;&#20307;&#30456;&#20284;&#24230;&#30340;&#24863;&#30693;&#26174;&#33879;&#21462;&#20915;&#20110;&#23545;&#35937;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#25972;&#20307;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20102;&#20154;&#31867;&#24863;&#30693;&#30340;&#21453;&#24046;&#25928;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#20998;&#25968;&#37327;&#21270;&#20102;&#22312;&#20004;&#20010;&#20803;&#32032;&#20043;&#38388;&#25214;&#21040;&#32473;&#23450;&#30456;&#20284;&#24230;&#30340;&#24778;&#21916;&#65292;&#30456;&#23545;&#20110;&#25104;&#23545;&#30340;&#25972;&#20307;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20010;&#24230;&#37327;&#65292;&#36890;&#24120;&#21457;&#29616;&#19982;&#21407;&#22987;&#20313;&#24358;&#30456;&#20284;&#24230;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;10-15\%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;...
&lt;/p&gt;
&lt;p&gt;
Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the \emph{surprise score}, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15\% better performance compared to raw cosine similarity. Our cod
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26469;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#38382;&#39064;&#12290;&#30740;&#31350;&#20013;&#23545;&#20960;&#31181;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#25512;&#23548;&#20986;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23578;&#19981;&#20855;&#22791;&#24847;&#35782;&#65292;&#20294;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#26080;&#26126;&#26174;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2308.08708</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24847;&#35782;&#65306;&#26469;&#33258;&#24847;&#35782;&#31185;&#23398;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. (arXiv:2308.08708v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26469;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#38382;&#39064;&#12290;&#30740;&#31350;&#20013;&#23545;&#20960;&#31181;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#25512;&#23548;&#20986;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23578;&#19981;&#20855;&#22791;&#24847;&#35782;&#65292;&#20294;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#26080;&#26126;&#26174;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#25110;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#33021;&#20855;&#26377;&#24847;&#35782;&#25104;&#20026;&#31185;&#23398;&#30028;&#20851;&#27880;&#30340;&#35805;&#39064;&#65292;&#20063;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#25285;&#24551;&#12290;&#26412;&#25253;&#21578;&#25552;&#20986;&#24182;&#20030;&#20363;&#20102;&#19968;&#31181;&#20005;&#35880;&#19988;&#32463;&#39564;&#22522;&#30784;&#30340;&#20154;&#24037;&#26234;&#33021;&#24847;&#35782;&#26041;&#27861;&#65306;&#26681;&#25454;&#25105;&#20204;&#30446;&#21069;&#26368;&#21487;&#20449;&#30340;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#23545;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20960;&#31181;&#24191;&#27867;&#35748;&#21487;&#30340;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#65292;&#21253;&#25324;&#24490;&#29615;&#22788;&#29702;&#29702;&#35770;&#12289;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#29702;&#35770;&#12289;&#39640;&#38454;&#29702;&#35770;&#12289;&#39044;&#27979;&#22788;&#29702;&#29702;&#35770;&#21644;&#27880;&#24847;&#27169;&#24335;&#29702;&#35770;&#12290;&#20174;&#36825;&#20123;&#29702;&#35770;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20123;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#20855;&#22791;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25351;&#31034;&#24615;&#29305;&#24449;&#26469;&#35780;&#20272;&#20102;&#20960;&#20010;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#31995;&#32479;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20855;&#26377;&#24847;&#35782;&#65292;&#20294;&#21516;&#26102;&#20063;&#26174;&#31034;&#20986;&#27809;&#26377;&#26126;&#26174;&#30340;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20998;&#26512;&#28145;&#33041;&#21050;&#28608;&#31070;&#32463;&#22806;&#31185;&#25163;&#26415;&#26399;&#38388;&#31070;&#32463;&#27963;&#21160;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#26102;&#38388;&#31383;&#21475;&#20869;&#30340;&#31070;&#32463;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#30340;&#23574;&#23792;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.05755</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#28145;&#33041;&#21050;&#28608;&#25163;&#26415;&#20013;&#30340;&#23574;&#23792;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep learning for spike detection in deep brain stimulation surgery. (arXiv:2308.05755v1 [eess.SP] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20998;&#26512;&#28145;&#33041;&#21050;&#28608;&#31070;&#32463;&#22806;&#31185;&#25163;&#26415;&#26399;&#38388;&#31070;&#32463;&#27963;&#21160;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#26102;&#38388;&#31383;&#21475;&#20869;&#30340;&#31070;&#32463;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#30340;&#23574;&#23792;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#33041;&#21050;&#28608;&#65288;DBS&#65289;&#26159;&#19968;&#31181;&#25104;&#21151;&#29992;&#20110;&#27835;&#30103;&#24085;&#37329;&#26862;&#30149;&#31561;&#30142;&#30149;&#30340;&#31070;&#32463;&#22806;&#31185;&#25163;&#26415;&#12290;&#36890;&#36807;&#23558;&#30005;&#26497;&#26893;&#20837;&#33041;&#37096;&#30340;&#29305;&#23450;&#21306;&#22495;&#36827;&#34892;&#30005;&#21050;&#28608;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#36731;&#30142;&#30149;&#30151;&#29366;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20998;&#26512;DBS&#31070;&#32463;&#22806;&#31185;&#25163;&#26415;&#26399;&#38388;&#33719;&#24471;&#30340;&#31070;&#32463;&#27963;&#21160;&#35760;&#24405;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#36825;&#19968;&#30446;&#30340;&#12290;&#22522;&#20110;&#26102;&#38388;&#31383;&#21475;&#65292;&#20998;&#31867;&#22120;&#35780;&#20272;&#31070;&#32463;&#27963;&#21160;&#65288;&#23574;&#23792;&#65289;&#26159;&#21542;&#23384;&#22312;&#12290;&#20998;&#31867;&#22120;&#30340;&#26368;&#22823;&#20934;&#30830;&#29575;&#20026;98.98&#65285;&#65292;&#25509;&#25910;&#22120;&#24037;&#20316;&#29305;&#24615;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20026;0.9898&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#22312;&#19981;&#20351;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep brain stimulation (DBS) is a neurosurgical procedure successfully used to treat conditions such as Parkinson's disease. Electrostimulation, carried out by implanting electrodes into an identified focus in the brain, makes it possible to reduce the symptoms of the disease significantly. In this paper, a method for analyzing recordings of neuronal activity acquired during DBS neurosurgery using deep learning is presented. We tested using a convolutional neural network (CNN) for this purpose. Based on the time window, the classifier assesses whether neuronal activity (spike) is present. The maximum accuracy value for the classifier was 98.98%, and the area under the receiver operating characteristic curve (AUC) was 0.9898. The method made it possible to obtain a classification without using data preprocessing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#32500;&#28857;&#20113;&#30340;&#20020;&#30028;&#28857;&#19982;&#38750;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23558;&#20020;&#30028;&#28857;&#30340;&#27010;&#24565;&#25512;&#24191;&#20026;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#36890;&#36807;&#20165;&#22522;&#20110;&#38750;&#37325;&#35201;&#28857;&#36827;&#34892;&#20998;&#31867;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#20250;&#26377;&#20123;&#24615;&#33021;&#25439;&#22833;&#12290;&#24314;&#35758;&#20351;&#29992;&#26631;&#20934;&#21270;&#29109;&#36873;&#25321;&#38750;&#20020;&#30028;&#28857;&#38598;&#21512;&#30340;&#33258;&#36866;&#24212;&#38408;&#20540;&#12290;&#36825;&#31181;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26497;&#24555;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21487;&#35299;&#37322;AI&#12289;&#31163;&#32676;&#20540;&#21435;&#38500;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#40065;&#26834;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#38450;&#24481;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.05525</link><description>&lt;p&gt;
&#20020;&#30028;&#28857;++&#65306;&#19968;&#31181;&#29992;&#20110;&#40065;&#26834;&#20998;&#31867;&#12289;&#23545;&#25239;&#24615;&#38450;&#24481;&#21644;&#21487;&#35299;&#37322;AI&#30340;&#25935;&#25463;&#28857;&#20113;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI. (arXiv:2308.05525v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#32500;&#28857;&#20113;&#30340;&#20020;&#30028;&#28857;&#19982;&#38750;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23558;&#20020;&#30028;&#28857;&#30340;&#27010;&#24565;&#25512;&#24191;&#20026;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#36890;&#36807;&#20165;&#22522;&#20110;&#38750;&#37325;&#35201;&#28857;&#36827;&#34892;&#20998;&#31867;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#20250;&#26377;&#20123;&#24615;&#33021;&#25439;&#22833;&#12290;&#24314;&#35758;&#20351;&#29992;&#26631;&#20934;&#21270;&#29109;&#36873;&#25321;&#38750;&#20020;&#30028;&#28857;&#38598;&#21512;&#30340;&#33258;&#36866;&#24212;&#38408;&#20540;&#12290;&#36825;&#31181;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26497;&#24555;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21487;&#35299;&#37322;AI&#12289;&#31163;&#32676;&#20540;&#21435;&#38500;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#40065;&#26834;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#38450;&#24481;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23433;&#20840;&#38656;&#27714;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#19988;&#24555;&#36895;&#22320;&#22788;&#29702;&#38750;&#20998;&#24067;&#26679;&#26412;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#19977;&#32500;&#28857;&#20113;&#30340;&#20020;&#30028;&#28857;&#19982;&#38750;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#35265;&#30340;&#25968;&#25454;&#25439;&#22351;&#21644;&#31163;&#32676;&#28857;&#24448;&#24448;&#20250;&#34987;&#35299;&#37322;&#20026;&#20020;&#30028;&#28857;&#12290;&#25105;&#20204;&#23558;&#20020;&#30028;&#28857;&#30340;&#27010;&#24565;&#25512;&#24191;&#20026;&#37325;&#35201;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20165;&#22522;&#20110;&#38750;&#37325;&#35201;&#28857;&#30340;&#20998;&#31867;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#32780;&#22312;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#20250;&#31245;&#24494;&#25439;&#22833;&#19968;&#20123;&#24615;&#33021;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#26631;&#20934;&#21270;&#29109;&#23545;&#20110;&#25968;&#25454;&#25439;&#22351;&#20998;&#26512;&#38750;&#24120;&#26377;&#20449;&#24687;&#37327;&#12290;&#24314;&#35758;&#22522;&#20110;&#26631;&#20934;&#21270;&#29109;&#36873;&#25321;&#38750;&#20020;&#30028;&#28857;&#38598;&#21512;&#30340;&#33258;&#36866;&#24212;&#38408;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#37325;&#35201;&#24615;&#24230;&#37327;&#35745;&#31639;&#26497;&#20854;&#24555;&#36895;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;AI(XAI)&#65292;&#31163;&#32676;&#20540;&#21435;&#38500;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#40065;&#26834;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to cope accurately and fast with Out-Of-Distribution (OOD) samples is crucial in real-world safety demanding applications. In this work we first study the interplay between critical points of 3D point clouds and OOD samples. Our findings are that common corruptions and outliers are often interpreted as critical points. We generalize the notion of critical points into importance measures. We show that training a classification network based only on less important points dramatically improves robustness, at a cost of minor performance loss on the clean set. We observe that normalized entropy is highly informative for corruption analysis. An adaptive threshold based on normalized entropy is suggested for selecting the set of uncritical points. Our proposed importance measure is extremely fast to compute. We show it can be used for a variety of applications, such as Explainable AI (XAI), Outlier Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;Transformer&#27169;&#22411;&#23454;&#29616;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#30340;&#21069;&#26223;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#35777;&#26126;&#20102;Transformer&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;FL&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#34920;&#31034;&#30456;&#20284;&#24615;&#26469;&#28145;&#20837;&#20102;&#35299;Transformer&#26377;&#21069;&#26223;&#30340;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2308.03945</link><description>&lt;p&gt;
&#36890;&#36807;Transformer&#22686;&#24378;&#22823;&#35268;&#27169;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
The Prospect of Enhancing Large-Scale Heterogeneous Federated Learning with Transformers. (arXiv:2308.03945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;Transformer&#27169;&#22411;&#23454;&#29616;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#30340;&#21069;&#26223;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#35777;&#26126;&#20102;Transformer&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;FL&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#34920;&#31034;&#30456;&#20284;&#24615;&#26469;&#28145;&#20837;&#20102;&#35299;Transformer&#26377;&#21069;&#26223;&#30340;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#23454;&#29616;&#36328;&#20998;&#24067;&#24335;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;AI&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#26469;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;FL&#30340;&#24191;&#27867;&#37319;&#29992;&#38754;&#20020;&#30528;&#25968;&#25454;&#24322;&#26500;&#21644;&#28041;&#21450;&#21040;&#30340;&#25968;&#25454;&#25152;&#26377;&#32773;&#35268;&#27169;&#24222;&#22823;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;Transformer&#30340;FL&#27169;&#22411;&#22312;&#23454;&#29616;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#26041;&#38754;&#30340;&#21069;&#26223;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#28041;&#21450;&#21040;&#20102;FL&#19982;Transformer&#12289;ResNet&#21644;&#20010;&#24615;&#21270;ResNet-based FL&#26041;&#27861;&#12290;&#36825;&#20123;&#23454;&#39564;&#32771;&#34385;&#20102;&#19981;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#25152;&#26377;&#32773;&#65292;&#20197;&#23637;&#31034;Transformer&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;FL&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#23618;&#21644;FL&#27169;&#22411;&#20043;&#38388;&#30340;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#34920;&#31034;&#30456;&#20284;&#24615;&#26469;&#20998;&#26512;Transformer&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;&#20854;&#26377;&#21069;&#26223;&#30340;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) addresses data privacy concerns by enabling collaborative training of AI models across distributed data owners. Wide adoption of FL faces the fundamental challenges of data heterogeneity and the large scale of data owners involved. In this paper, we investigate the prospect of Transformer-based FL models for achieving generalization and personalization in this setting. We conduct extensive comparative experiments involving FL with Transformers, ResNet, and personalized ResNet-based FL approaches under various scenarios. These experiments consider varying numbers of data owners to demonstrate Transformers' advantages over deep neural networks in large-scale heterogeneous FL tasks. In addition, we analyze the superior performance of Transformers by comparing the Centered Kernel Alignment (CKA) representation similarity across different layers and FL models to gain insight into the reasons behind their promising capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#21644;&#23545;&#25968;&#31934;&#24230;&#21464;&#25442;&#22120;&#37117;&#21487;&#20197;&#27169;&#25311;&#24120;&#28145;&#24230;&#38408;&#20540;&#30005;&#36335;&#65292;&#20854;&#20013;&#21518;&#32773;&#30001;&#20110;&#29983;&#25104;&#32479;&#19968;&#30340;&#30005;&#36335;&#26063;&#32780;&#26356;&#20581;&#22766;&#12290;</title><link>http://arxiv.org/abs/2308.03212</link><description>&lt;p&gt;
&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#26159;&#24120;&#28145;&#24230;&#22343;&#21248;&#38408;&#20540;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits. (arXiv:2308.03212v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#21644;&#23545;&#25968;&#31934;&#24230;&#21464;&#25442;&#22120;&#37117;&#21487;&#20197;&#27169;&#25311;&#24120;&#28145;&#24230;&#38408;&#20540;&#30005;&#36335;&#65292;&#20854;&#20013;&#21518;&#32773;&#30001;&#20110;&#29983;&#25104;&#32479;&#19968;&#30340;&#30005;&#36335;&#26063;&#32780;&#26356;&#20581;&#22766;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#25442;&#22120;&#24050;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#23427;&#20204;&#19982;&#24120;&#28145;&#24230;&#38408;&#20540;&#30005;&#36335;&#30340;&#20851;&#31995;&#65292;&#20570;&#20986;&#20102;&#20004;&#20010;&#20551;&#35774;&#65306;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21147;&#21644;&#30456;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#30340;&#23545;&#25968;&#31934;&#24230;&#30340;&#20869;&#37096;&#35745;&#31639;&#12290;Merrill&#31561;&#20154;&#35777;&#26126;&#20102;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#21487;&#20197;&#35782;&#21035;&#23646;&#20110;&#22797;&#26434;&#24615;&#31867;&#21035;TC0&#30340;&#35821;&#35328;&#65292;&#35813;&#31867;&#21035;&#34920;&#31034;&#21487;&#20197;&#30001;&#24120;&#28145;&#24230;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#38408;&#20540;&#30005;&#36335;&#35782;&#21035;&#30340;&#35821;&#35328;&#38598;&#21512;&#12290;&#21516;&#26679;&#22320;&#65292;Merrill&#21644;Sabharwal&#35777;&#26126;&#20102;&#23545;&#25968;&#31934;&#24230;&#30340;&#21464;&#25442;&#22120;&#21487;&#20197;&#35782;&#21035;&#32479;&#19968;&#30340;TC0&#31867;&#35821;&#35328;&#12290;&#36825;&#34920;&#26126;&#36825;&#20004;&#31181;&#36716;&#25442;&#22120;&#27169;&#22411;&#37117;&#21487;&#20197;&#36890;&#36807;&#24120;&#28145;&#24230;&#38408;&#20540;&#30005;&#36335;&#26469;&#27169;&#25311;&#65292;&#32780;&#21518;&#32773;&#30001;&#20110;&#29983;&#25104;&#32479;&#19968;&#30340;&#30005;&#36335;&#26063;&#32780;&#26356;&#20581;&#22766;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#34920;&#26126;&#65292;&#31532;&#19968;&#20010;&#32467;&#26524;&#20063;&#21487;&#20197;&#24310;&#20280;&#21040;&#20135;&#29983;&#32479;&#19968;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#24182;&#21033;&#29992;&#22122;&#22768;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#26500;&#24314;&#21407;&#22411;&#21521;&#37327;&#21644;&#35745;&#31639;&#26679;&#26412;&#19982;&#21407;&#22411;&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16889</link><description>&lt;p&gt;
&#20174;&#22122;&#22768;&#31867;&#22411;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#30495;&#23454;&#22330;&#26223;&#19979;&#30340;&#26377;&#22122;&#26631;&#27880;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Noisy Label Learning in Real-world Annotation Scenarios from the Noise-type Perspective. (arXiv:2307.16889v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#24182;&#21033;&#29992;&#22122;&#22768;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#26500;&#24314;&#21407;&#22411;&#21521;&#37327;&#21644;&#35745;&#31639;&#26679;&#26412;&#19982;&#21407;&#22411;&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#23398;&#20064;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22122;&#22768;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#20107;&#23454;&#24615;&#22122;&#22768;&#21644;&#27169;&#31946;&#24615;&#22122;&#22768;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21306;&#20998;&#36825;&#20123;&#22122;&#22768;&#31867;&#22411;&#24182;&#21033;&#29992;&#20854;&#35821;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;Proto-semi&#12290;Proto-semi&#36890;&#36807;&#39044;&#28909;&#23558;&#25152;&#26377;&#26679;&#26412;&#21010;&#20998;&#20026;&#33258;&#20449;&#21644;&#19981;&#33258;&#20449;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#20449;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#21407;&#22411;&#21521;&#37327;&#20197;&#25429;&#25417;&#31867;&#21035;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#35745;&#31639;&#19981;&#33258;&#20449;&#26679;&#26412;&#19982;&#21407;&#22411;&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#20197;&#20419;&#36827;&#22122;&#22768;&#20998;&#31867;&#12290;&#26681;&#25454;&#36825;&#20123;&#36317;&#31163;&#65292;&#23545;&#26631;&#31614;&#36827;&#34892;&#20462;&#27491;&#25110;&#20445;&#30041;&#65292;&#20174;&#32780;&#25913;&#36827;&#33258;&#20449;&#21644;&#19981;&#33258;&#20449;&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#22686;&#24378;&#35757;&#32451;&#12290;&#23545;&#19968;&#20010;&#30495;&#23454;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#23454;&#20102;Proto-semi&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the problem of learning with noisy labels in real-world annotation scenarios, where noise can be categorized into two types: factual noise and ambiguity noise. To better distinguish these noise types and utilize their semantics, we propose a novel sample selection-based approach for noisy label learning, called Proto-semi. Proto-semi initially divides all samples into the confident and unconfident datasets via warm-up. By leveraging the confident dataset, prototype vectors are constructed to capture class characteristics. Subsequently, the distances between the unconfident samples and the prototype vectors are calculated to facilitate noise classification. Based on these distances, the labels are either corrected or retained, resulting in the refinement of the confident and unconfident datasets. Finally, we introduce a semi-supervised learning method to enhance training. Empirical evaluations on a real-world annotated dataset substantiate the robustness of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#21355;&#26143;&#19978;&#23398;&#20064;&#28909;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#23567;&#22411;&#21355;&#26143;&#20013;&#28909;&#25511;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#31354;&#38388;&#22788;&#29702;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#33021;&#22815;&#36741;&#21161;&#20256;&#32479;&#28909;&#25511;&#21046;&#31995;&#32479;&#65292;&#20445;&#25345;&#36733;&#33655;&#28201;&#24230;&#22312;&#21487;&#25805;&#20316;&#33539;&#22260;&#20869;&#12290;</title><link>http://arxiv.org/abs/2307.15438</link><description>&lt;p&gt;
&#33258;&#20027;&#36733;&#33655;&#28909;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Autonomous Payload Thermal Control. (arXiv:2307.15438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#21355;&#26143;&#19978;&#23398;&#20064;&#28909;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#23567;&#22411;&#21355;&#26143;&#20013;&#28909;&#25511;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#31354;&#38388;&#22788;&#29702;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#33021;&#22815;&#36741;&#21161;&#20256;&#32479;&#28909;&#25511;&#21046;&#31995;&#32479;&#65292;&#20445;&#25345;&#36733;&#33655;&#28201;&#24230;&#22312;&#21487;&#25805;&#20316;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23567;&#22411;&#21355;&#26143;&#20013;&#65292;&#28909;&#25511;&#21046;&#35774;&#22791;&#12289;&#31185;&#23398;&#20202;&#22120;&#21644;&#30005;&#23376;&#37096;&#20214;&#30340;&#31354;&#38388;&#36739;&#23567;&#12290;&#27492;&#22806;&#65292;&#30005;&#23376;&#35774;&#22791;&#30340;&#36817;&#36317;&#31163;&#20351;&#24471;&#21151;&#32791;&#25955;&#28909;&#22256;&#38590;&#65292;&#23384;&#22312;&#26080;&#27861;&#36866;&#24403;&#25511;&#21046;&#28201;&#24230;&#12289;&#38477;&#20302;&#37096;&#20214;&#23551;&#21629;&#21644;&#20219;&#21153;&#24615;&#33021;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#21033;&#29992;&#21355;&#26143;&#19978;&#36880;&#28176;&#22686;&#21152;&#30340;&#26234;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#23398;&#20064;&#26426;&#36733;&#28909;&#25511;&#21046;&#31574;&#30053;&#12290;&#35813;&#26694;&#26550;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#26410;&#26469;&#23558;&#36816;&#24448;ISS&#24182;&#22312;IMAGIN-e&#20219;&#21153;&#20013;&#36827;&#34892;&#36793;&#32536;&#35745;&#31639;&#30340;&#30495;&#23454;&#31354;&#38388;&#22788;&#29702;&#35745;&#31639;&#26426;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#25511;&#21046;&#36733;&#33655;&#22788;&#29702;&#21151;&#29575;&#65292;&#20197;&#20445;&#25345;&#28201;&#24230;&#22312;&#25805;&#20316;&#33539;&#22260;&#20869;&#65292;&#34917;&#20805;&#20256;&#32479;&#28909;&#25511;&#21046;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In small satellites there is less room for heat control equipment, scientific instruments, and electronic components. Furthermore, the near proximity of the electronics makes power dissipation difficult, with the risk of not being able to control the temperature appropriately, reducing component lifetime and mission performance. To address this challenge, taking advantage of the advent of increasing intelligence on board satellites, a deep reinforcement learning based framework that uses Soft Actor-Critic algorithm is proposed for learning the thermal control policy onboard. The framework is evaluated both in a naive simulated environment and in a real space edge processing computer that will be shipped in the future IMAGIN-e mission and hosted in the ISS. The experiment results show that the proposed framework is able to learn to control the payload processing power to maintain the temperature under operational ranges, complementing traditional thermal control systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14953</link><description>&lt;p&gt;
&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36827;&#34892;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#26088;&#22312;&#22312;&#20174;&#22810;&#20010;&#26631;&#35760;&#30340;&#28304;&#22495;&#36716;&#31227;&#30693;&#35782;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#26102;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;MSDA&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;MSDA&#20013;&#30340;&#27599;&#20010;&#22495;&#35299;&#37322;&#20026;&#32463;&#39564;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#22495;&#34920;&#36798;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#65292;&#36825;&#20123;&#21407;&#23376;&#26159;&#32463;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#23567;&#25209;&#37327;&#23398;&#20064;&#30340;&#31639;&#27861;DaDiL&#65306;&#65288;i&#65289;&#21407;&#23376;&#20998;&#24067;&#65307;&#65288;ii&#65289;&#37325;&#24515;&#22352;&#26631;&#30697;&#38453;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23383;&#20856;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65306;DaDiL-R&#65292;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#65307;DaDiL-E&#65292;&#22522;&#20110;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;3&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;Caltech-Office&#12289;Office 31&#21644;CRWU&#65292;&#22312;&#20998;&#31867;&#19978;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;3.15&#65285;&#12289;2.29&#65285;&#21644;7.71&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#20998;&#31867;&#31995;&#32479;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#32622;&#20449;&#24230;&#35780;&#20998;&#20989;&#25968;&#26080;&#27861;&#21487;&#38752;&#22320;&#38450;&#27490;&#38544;&#24615;&#22833;&#36133;&#65292;&#24378;&#35843;&#20102;&#23545;&#25968;&#25454;&#20013;&#22833;&#36133;&#26681;&#26412;&#21407;&#22240;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#37325;&#35201;&#24615;&#12290;&#24341;&#20837;&#20102;SF-Visuals&#65292;&#19968;&#20010;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#32858;&#31867;&#26469;&#21487;&#35270;&#21270;&#20559;&#31227;&#21644;&#22833;&#36133;&#30340;&#20132;&#20114;&#24335;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2307.14729</link><description>&lt;p&gt;
&#29702;&#35299;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#38544;&#24615;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Understanding Silent Failures in Medical Image Classification. (arXiv:2307.14729v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#20998;&#31867;&#31995;&#32479;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#32622;&#20449;&#24230;&#35780;&#20998;&#20989;&#25968;&#26080;&#27861;&#21487;&#38752;&#22320;&#38450;&#27490;&#38544;&#24615;&#22833;&#36133;&#65292;&#24378;&#35843;&#20102;&#23545;&#25968;&#25454;&#20013;&#22833;&#36133;&#26681;&#26412;&#21407;&#22240;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#37325;&#35201;&#24615;&#12290;&#24341;&#20837;&#20102;SF-Visuals&#65292;&#19968;&#20010;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#32858;&#31867;&#26469;&#21487;&#35270;&#21270;&#20559;&#31227;&#21644;&#22833;&#36133;&#30340;&#20132;&#20114;&#24335;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#20998;&#31867;&#31995;&#32479;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#20351;&#29992;&#65292;&#38450;&#27490;&#38544;&#24615;&#22833;&#36133;&#33267;&#20851;&#37325;&#35201;&#12290;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#36275;&#22815;&#31283;&#20581;&#30340;&#20998;&#31867;&#22120;&#20197;&#36991;&#20813;&#39318;&#27425;&#22833;&#36133;&#65292;&#25110;&#36890;&#36807;&#20351;&#29992;&#32622;&#20449;&#24230;&#35780;&#20998;&#20989;&#25968;&#65288;CSFs&#65289;&#26816;&#27979;&#21097;&#20313;&#30340;&#22833;&#36133;&#26469;&#23454;&#29616;&#12290;&#22270;&#20687;&#20998;&#31867;&#20013;&#22833;&#36133;&#30340;&#20027;&#35201;&#28304;&#22836;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#37096;&#32626;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#20026;&#20102;&#29702;&#35299;&#21307;&#23398;&#22270;&#20687;&#20013;&#38450;&#27490;&#38544;&#24615;&#22833;&#36133;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#20840;&#38754;&#20998;&#26512;&#65292;&#27604;&#36739;&#20102;&#22235;&#20010;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#21644;&#21508;&#31181;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#21508;&#31181;CSFs&#12290;&#26681;&#25454;&#32467;&#26524;&#21457;&#29616;&#65292;&#27809;&#26377;&#19968;&#20010;&#34987;&#22522;&#20934;&#21270;&#30340;CSF&#33021;&#22815;&#21487;&#38752;&#22320;&#38450;&#27490;&#38544;&#24615;&#22833;&#36133;&#65292;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#23545;&#25968;&#25454;&#20013;&#22833;&#36133;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SF-Visuals&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#20998;&#26512;&#24037;&#20855;&#65292;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#32858;&#31867;&#26469;&#21487;&#35270;&#21270;&#20559;&#31227;&#21644;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
To ensure the reliable use of classification systems in medical applications, it is crucial to prevent silent failures. This can be achieved by either designing classifiers that are robust enough to avoid failures in the first place, or by detecting remaining failures using confidence scoring functions (CSFs). A predominant source of failures in image classification is distribution shifts between training data and deployment data. To understand the current state of silent failure prevention in medical imaging, we conduct the first comprehensive analysis comparing various CSFs in four biomedical tasks and a diverse range of distribution shifts. Based on the result that none of the benchmarked CSFs can reliably prevent silent failures, we conclude that a deeper understanding of the root causes of failures in the data is required. To facilitate this, we introduce SF-Visuals, an interactive analysis tool that uses latent space clustering to visualize shifts and failures. On the basis of va
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#21069;&#21015;&#33146;&#25104;&#20687;&#20013;&#30340;&#26032;&#22411;&#22522;&#30784;&#27169;&#22411;UniverSeg&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#26159;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.03266</link><description>&lt;p&gt;
&#21069;&#21015;&#33146;&#25104;&#20687;&#20013;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging. (arXiv:2307.03266v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#21069;&#21015;&#33146;&#25104;&#20687;&#20013;&#30340;&#26032;&#22411;&#22522;&#30784;&#27169;&#22411;UniverSeg&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#26159;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#29421;&#20041;&#20219;&#21153;&#19978;&#20197;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#26114;&#36149;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#24050;&#32463;&#35777;&#26126;&#20102;&#26500;&#24314;&#22522;&#30784;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#20046;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20026;&#21508;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#23450;&#21046;&#12290;&#36825;&#21487;&#33021;&#20195;&#34920;&#20102;&#21307;&#23398;&#25104;&#20687;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#25105;&#20204;&#39044;&#35745;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#22609;&#36896;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26368;&#36817;&#24320;&#21457;&#30340;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;UniverSeg&#12290;&#25105;&#20204;&#22312;&#21069;&#21015;&#33146;&#25104;&#20687;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#20998;&#21106;&#27169;&#22411;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21644;&#35752;&#35770;&#31361;&#20986;&#20102;&#20960;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#22312;&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#20013;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art techniques for medical image segmentation rely on deep-learning models. These models, however, are often trained on narrowly-defined tasks in a supervised fashion, which requires expensive labeled datasets. Recent advances in several machine learning domains, such as natural language generation have demonstrated the feasibility and utility of building foundation models that can be customized for various downstream tasks with little to no labeled data. This likely represents a paradigm shift for medical imaging, where we expect that foundation models may shape the future of the field. In this paper, we consider a recently developed foundation model for medical image segmentation, UniverSeg. We conduct an empirical evaluation study in the context of prostate imaging and compare it against the conventional approach of training a task-specific segmentation model. Our results and discussion highlight several important factors that will likely be important in the develo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#20998;&#23618;&#32467;&#26500;&#34701;&#20837;&#27010;&#29575;&#28508;&#22312;&#21521;&#37327;&#20013;&#65292;&#24182;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26469;&#35774;&#35745;&#26377;&#25928;&#30340;&#20998;&#23376;&#28508;&#22312;&#21521;&#37327;&#65292;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.00623</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#23545;&#20998;&#23376;&#22270;&#36827;&#34892;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoding Molecular Graphs with Denoising Diffusion Probabilistic Model. (arXiv:2307.00623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00623
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#20998;&#23618;&#32467;&#26500;&#34701;&#20837;&#27010;&#29575;&#28508;&#22312;&#21521;&#37327;&#20013;&#65292;&#24182;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26469;&#35774;&#35745;&#26377;&#25928;&#30340;&#20998;&#23376;&#28508;&#22312;&#21521;&#37327;&#65292;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#35774;&#35745;&#20998;&#23376;&#25551;&#36848;&#31526;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAEs)&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#35774;&#35745;&#30001;&#20998;&#23376;&#32467;&#26500;&#23548;&#20986;&#30340;&#27010;&#29575;&#28508;&#22312;&#21521;&#37327;&#20316;&#20026;&#25551;&#36848;&#31526;&#65292;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#21482;&#26377;&#20998;&#23376;&#32467;&#26500;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#24212;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;VAE&#30340;&#28508;&#22312;&#21521;&#37327;&#30340;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#20551;&#35774;&#20026;&#31616;&#21333;&#30340;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#65292;&#32780;&#19988;&#21327;&#26041;&#24046;&#20026;&#38646;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#34920;&#31034;&#28508;&#22312;&#29305;&#24449;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#20998;&#23618;&#32467;&#26500;&#34701;&#20837;&#27010;&#29575;&#28508;&#22312;&#21521;&#37327;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPM)&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#36890;&#36807;&#19968;&#20123;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#21487;&#20197;&#20026;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#35774;&#35745;&#20986;&#26377;&#25928;&#30340;&#20998;&#23376;&#28508;&#22312;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-driven drug discovery, designing molecular descriptors is a very important task. Deep generative models such as variational autoencoders (VAEs) offer a potential solution by designing descriptors as probabilistic latent vectors derived from molecular structures. These models can be trained on large datasets, which have only molecular structures, and applied to transfer learning. Nevertheless, the approximate posterior distribution of the latent vectors of the usual VAE assumes a simple multivariate Gaussian distribution with zero covariance, which may limit the performance of representing the latent features. To overcome this limitation, we propose a novel molecular deep generative model that incorporates a hierarchical structure into the probabilistic latent vectors. We achieve this by a denoising diffusion probabilistic model (DDPM). We demonstrate that our model can design effective molecular latent vectors for molecular property prediction from some experiments by small dat
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;(MEHAML)&#26159;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#35774;&#35745;&#20102;&#26368;&#22823;&#29109;MARL&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#20855;&#26377;&#32852;&#21512;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#21333;&#35843;&#25913;&#36827;&#21644;&#25910;&#25947;&#33267;&#20013;&#20301;&#21709;&#24212;&#22343;&#34913;(QRE)&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#24120;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HASAC&#26469;&#39564;&#35777;&#20854;&#23454;&#29992;&#24615;&#21644;&#22312;&#25506;&#32034;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.10715</link><description>&lt;p&gt;
&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Entropy Heterogeneous-Agent Mirror Learning. (arXiv:2306.10715v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10715
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;(MEHAML)&#26159;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#35774;&#35745;&#20102;&#26368;&#22823;&#29109;MARL&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#20855;&#26377;&#32852;&#21512;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#21333;&#35843;&#25913;&#36827;&#21644;&#25910;&#25947;&#33267;&#20013;&#20301;&#21709;&#24212;&#22343;&#34913;(QRE)&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#24120;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HASAC&#26469;&#39564;&#35777;&#20854;&#23454;&#29992;&#24615;&#21644;&#22312;&#25506;&#32034;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#22312;&#21512;&#20316;&#21338;&#24328;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#38754;&#20020;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#36229;&#21442;&#25968;&#33030;&#24369;&#24615;&#21644;&#25910;&#25947;&#20110;&#27425;&#20248;&#32435;&#20160;&#22343;&#34913;&#30340;&#39118;&#38505;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;(MEHAML)&#65292;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#35774;&#35745;&#20102;&#26368;&#22823;&#29109;MARL&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;MEHAML&#26694;&#26550;&#23548;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#32852;&#21512;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#21333;&#35843;&#25913;&#36827;&#21644;&#25910;&#25947;&#33267;&#20013;&#20301;&#21709;&#24212;&#22343;&#34913;(QRE)&#30340;&#26399;&#26395;&#29305;&#24615;&#12290;MEHAML&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#24320;&#21457;&#24191;&#27867;&#20351;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HASAC&#30340;MEHAML&#25193;&#23637;&#26469;&#23637;&#31034;&#65292;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20986;&#20102;&#25506;&#32034;&#21644;&#31283;&#20581;&#24615;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample inefficiency, brittleness regarding hyperparameters, and the risk of converging to a suboptimal Nash Equilibrium. To resolve these issues, in this paper, we propose a novel theoretical framework, named Maximum Entropy Heterogeneous-Agent Mirror Learning (MEHAML), that leverages the maximum entropy principle to design maximum entropy MARL actor-critic algorithms. We prove that algorithms derived from the MEHAML framework enjoy the desired properties of the monotonic improvement of the joint maximum entropy objective and the convergence to quantal response equilibrium (QRE). The practicality of MEHAML is demonstrated by developing a MEHAML extension of the widely used RL algorithm, HASAC (for soft actor-critic), which shows significant improvements in exploration and robustness on three challenging benchmark
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#21028;&#26029;&#23427;&#20204;&#26159;&#21542;&#21033;&#29992;&#20102;&#30456;&#21516;&#30340;&#20869;&#37096;&#36807;&#31243;&#36827;&#34892;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2306.10168</link><description>&lt;p&gt;
&#36229;&#36234;&#20960;&#20309;&#65306;&#20351;&#29992;&#21160;&#21147;&#30456;&#20284;&#24615;&#20998;&#26512;&#27604;&#36739;&#31070;&#32463;&#22238;&#36335;&#35745;&#31639;&#20013;&#30340;&#35745;&#31639;&#26102;&#38388;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis. (arXiv:2306.10168v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#21028;&#26029;&#23427;&#20204;&#26159;&#21542;&#21033;&#29992;&#20102;&#30456;&#21516;&#30340;&#20869;&#37096;&#36807;&#31243;&#36827;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#21028;&#26029;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#22312;&#29305;&#23450;&#35745;&#31639;&#20013;&#21033;&#29992;&#20102;&#30456;&#21516;&#30340;&#20869;&#37096;&#36807;&#31243;&#65311;&#36825;&#20010;&#38382;&#39064;&#23545;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#20010;&#23376;&#39046;&#22495;&#37117;&#24456;&#37325;&#35201;&#65292;&#21253;&#25324;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#26800;&#35299;&#37322;&#24615;&#21644;&#33041;&#26426;&#25509;&#21475;&#12290;&#27604;&#36739;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#20934;&#26041;&#27861;&#27880;&#37325;&#28508;&#22312;&#29366;&#24577;&#30340;&#31354;&#38388;&#20960;&#20309;&#12290;&#28982;&#32780;&#65292;&#22312;&#24490;&#29615;&#32593;&#32476;&#20013;&#65292;&#35745;&#31639;&#26159;&#22312;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#23618;&#38754;&#19978;&#23454;&#29616;&#30340;&#65292;&#23427;&#20204;&#19982;&#20960;&#20309;&#27809;&#26377;&#31616;&#21333;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#20851;&#31995;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#23427;&#22312;&#21160;&#21147;&#23398;&#30340;&#23618;&#38754;&#19978;&#27604;&#36739;&#20004;&#20010;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#20351;&#29992;&#26368;&#36817;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#21147;&#31995;&#32479;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#26680;&#24515;&#29305;&#24449;&#30340;&#39640;&#32500;&#32447;&#24615;&#31995;&#32479;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;Procrustes&#20998;&#26512;&#30340;&#25193;&#23637;&#26041;&#27861;&#27604;&#36739;&#36825;&#20123;&#32447;&#24615;&#36817;&#20284;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#21521;&#37327;&#22330;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we tell whether two neural networks are utilizing the same internal processes for a particular computation? This question is pertinent for multiple subfields of both neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of neural dynamics, which do not have a simple one-to-one mapping with geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics. Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare these linear approximations via a novel extension of Procrustes Analysis that accounts for how vector fields c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#27979;&#35797;&#26102;&#33410;&#28857;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#19982;&#20854;&#22270;&#30340;&#20840;&#23616;&#21516;&#36136;&#24615;&#27700;&#24179;&#20559;&#31163;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#26032;&#21442;&#25968;&#29992;&#20110;&#25511;&#21046;&#21516;&#36136;&#24615;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20013;&#31995;&#32479;&#22320;&#30740;&#31350;&#26412;&#22320;&#21516;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.05557</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#30340;&#24615;&#33021;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
On Performance Discrepancies Across Local Homophily Levels in Graph Neural Networks. (arXiv:2306.05557v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#27979;&#35797;&#26102;&#33410;&#28857;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#19982;&#20854;&#22270;&#30340;&#20840;&#23616;&#21516;&#36136;&#24615;&#27700;&#24179;&#20559;&#31163;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#26032;&#21442;&#25968;&#29992;&#20110;&#25511;&#21046;&#21516;&#36136;&#24615;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20013;&#31995;&#32479;&#22320;&#30740;&#31350;&#26412;&#22320;&#21516;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GNN&#30340;&#30740;&#31350;&#24378;&#35843;&#39640;&#21516;&#36136;&#24615;&#65288;&#21363;&#30456;&#20284;&#31867;&#33410;&#28857;&#30456;&#20114;&#36830;&#25509;&#30340;&#20542;&#21521;&#65289;&#19982;&#33410;&#28857;&#20998;&#31867;&#30340;&#24378;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#20851;&#31995;&#26356;&#21152;&#24494;&#22937;&#65292;&#35777;&#26126;&#21363;&#20351;&#31616;&#21333;&#30340;GNN&#20063;&#21487;&#20197;&#22312;&#26576;&#20123;&#24322;&#36136;&#24615;&#29615;&#22659;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#21457;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#20551;&#35774;&#65292;&#24182;&#30830;&#23450;&#25968;&#25454;&#38598;&#32463;&#24120;&#34987;&#35270;&#20026;&#22312;&#33410;&#28857;&#38388;&#20855;&#26377;&#24658;&#23450;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20026;&#20102;&#26356;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;GNN&#22312;&#27979;&#35797;&#26102;&#33410;&#28857;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#19982;&#20854;&#22270;&#30340;&#20840;&#23616;&#21516;&#36136;&#24615;&#27700;&#24179;&#20559;&#31163;&#26102;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#24110;&#21161;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#22312;&#21516;&#36136;&#24615;&#20998;&#26512;&#20013;&#24120;&#29992;&#30340;&#20248;&#20808;&#38468;&#21152;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#21442;&#25968;&#65292;&#20197;&#25511;&#21046;&#29983;&#25104;&#30340;&#22270;&#20013;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#65292;&#20174;&#32780;&#23454;&#29616;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#31350;&#26412;&#22320;&#21516;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on GNNs has highlighted a relationship between high homophily (i.e., the tendency for nodes of a similar class to connect) and strong predictive performance in node classification. However, recent research has found the relationship to be more nuanced, demonstrating that even simple GNNs can learn in certain heterophilous settings. To bridge the gap between these findings, we revisit the assumptions made in previous works and identify that datasets are often treated as having a constant homophily level across nodes. To align closer to real-world datasets, we theoretically and empirically study the performance of GNNs when the local homophily level of a node deviates at test-time from the global homophily level of its graph. To aid our theoretical analysis, we introduce a new parameter to the preferential attachment model commonly used in homophily analysis to enable the control of local homophily levels in generated graphs, enabling a systematic empirical study on how local ho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22359;&#32423;&#24182;&#34892;Transformer&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#26356;&#38271;32&#20493;&#30340;&#35757;&#32451;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.19370</link><description>&lt;p&gt;
&#22823;&#22411;&#38271;&#24207;&#21015;&#27169;&#22411;&#30340;&#22359;&#32423;&#24182;&#34892;Transformer
&lt;/p&gt;
&lt;p&gt;
Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22359;&#32423;&#24182;&#34892;Transformer&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#26356;&#38271;32&#20493;&#30340;&#35757;&#32451;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#22522;&#30707;&#65292;&#22312;&#21508;&#31181;AI&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#22823;&#22411;&#21069;&#39304;&#32593;&#32476;&#25152;&#38656;&#30340;&#20869;&#23384;&#23481;&#37327;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#28041;&#21450;&#22810;&#20010;&#38271;&#24207;&#21015;&#25110;&#38271;&#26399;&#20381;&#36182;&#30340;&#20219;&#21153;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#22359;&#32423;&#24182;&#34892;Transformer&#65288;BPT&#65289;&#65292;&#23427;&#21033;&#29992;&#22359;&#32423;&#35745;&#31639;&#33258;&#25105;&#27880;&#24847;&#21644;&#21069;&#39304;&#32593;&#32476;&#34701;&#21512;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#30340;&#21516;&#26102;&#22788;&#29702;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;BPT&#20351;&#35757;&#32451;&#24207;&#21015;&#30340;&#38271;&#24230;&#27604;&#21407;&#22987;&#30340;Transformer&#38271;32&#20493;&#65292;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#38271;2&#21040;4&#20493;&#12290;&#23545;&#35821;&#35328;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;BPT&#22312;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving perfo
&lt;/p&gt;</description></item><item><title>FairDP&#26159;&#19968;&#31181;&#21516;&#26102;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26032;&#22411;&#26426;&#21046;&#65292;&#36890;&#36807;&#29420;&#31435;&#20026;&#19981;&#21516;&#30340;&#20010;&#20307;&#32676;&#20307;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#27493;&#25972;&#21512;&#26469;&#33258;&#32676;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21046;&#23450;&#32508;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;FairDP&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#25928;&#30410;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.16474</link><description>&lt;p&gt;
FairDP: &#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#35748;&#35777;&#30340;&#20844;&#24179;&#24615;&#20445;&#38556;
&lt;/p&gt;
&lt;p&gt;
FairDP: Certified Fairness with Differential Privacy. (arXiv:2305.16474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16474
&lt;/p&gt;
&lt;p&gt;
FairDP&#26159;&#19968;&#31181;&#21516;&#26102;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26032;&#22411;&#26426;&#21046;&#65292;&#36890;&#36807;&#29420;&#31435;&#20026;&#19981;&#21516;&#30340;&#20010;&#20307;&#32676;&#20307;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#27493;&#25972;&#21512;&#26469;&#33258;&#32676;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21046;&#23450;&#32508;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;FairDP&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#25928;&#30410;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FairDP&#30340;&#26032;&#22411;&#26426;&#21046;&#65292;&#26088;&#22312;&#21516;&#26102;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;(DP)&#21644;&#20844;&#24179;&#24615;&#12290;FairDP&#36890;&#36807;&#29420;&#31435;&#20026;&#19981;&#21516;&#30340;&#20010;&#20307;&#32676;&#20307;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20351;&#29992;&#32452;&#29305;&#23450;&#30340;&#21098;&#35009;&#39033;&#26469;&#35780;&#20272;&#21644;&#38480;&#21046;DP&#30340;&#24046;&#24322;&#24433;&#21709;&#30340;&#21516;&#26102;&#25805;&#20316;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35813;&#26426;&#21046;&#36880;&#27493;&#25972;&#21512;&#26469;&#33258;&#32676;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21046;&#23450;&#32508;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#24191;&#27867;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;FairDP&#30340;&#21151;&#25928;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#25928;&#30410;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces FairDP, a novel mechanism designed to simultaneously ensure differential privacy (DP) and fairness. FairDP operates by independently training models for distinct individual groups, using group-specific clipping terms to assess and bound the disparate impacts of DP. Throughout the training process, the mechanism progressively integrates knowledge from group models to formulate a comprehensive model that balances privacy, utility, and fairness in downstream tasks. Extensive theoretical and empirical analyses validate the efficacy of FairDP, demonstrating improved trade-offs between model utility, privacy, and fairness compared with existing methods.
&lt;/p&gt;</description></item><item><title>SMT 2.0&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20195;&#29702;&#27169;&#22411;&#24037;&#20855;&#21253;&#65292;&#24341;&#20837;&#20102;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#21644;&#23618;&#27425;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#26041;&#27861;&#12289;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;Kriging&#30340;&#26041;&#24046;&#21644;&#26680;&#23548;&#25968;&#26469;&#25913;&#36827;&#20102;SMT&#12290;</title><link>http://arxiv.org/abs/2305.13998</link><description>&lt;p&gt;
SMT 2.0&#65306;&#19968;&#20010;&#20851;&#27880;&#23618;&#27425;&#21644;&#28151;&#21512;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#30340;&#20195;&#29702;&#27169;&#22411;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes. (arXiv:2305.13998v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13998
&lt;/p&gt;
&lt;p&gt;
SMT 2.0&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20195;&#29702;&#27169;&#22411;&#24037;&#20855;&#21253;&#65292;&#24341;&#20837;&#20102;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#21644;&#23618;&#27425;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#26041;&#27861;&#12289;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;Kriging&#30340;&#26041;&#24046;&#21644;&#26680;&#23548;&#25968;&#26469;&#25913;&#36827;&#20102;SMT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Surrogate Modeling Toolbox (SMT)&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#12289;&#37319;&#26679;&#25216;&#26415;&#21644;&#19968;&#22871;&#31034;&#20363;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SMT 2.0&#65292;&#36825;&#26159;SMT&#30340;&#19968;&#20010;&#37325;&#35201;&#26032;&#29256;&#26412;&#65292;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#21319;&#32423;&#21644;&#26032;&#21151;&#33021;&#12290;&#36825;&#20010;&#29256;&#26412;&#22686;&#21152;&#20102;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#20195;&#29702;&#27169;&#22411;&#21644;&#23618;&#27425;&#21464;&#37327;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#31867;&#22411;&#30340;&#21464;&#37327;&#22312;&#22810;&#20010;&#20195;&#29702;&#24314;&#27169;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;SMT 2.0&#36824;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#26041;&#27861;&#12289;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;Kriging&#30340;&#26041;&#24046;&#21644;&#26680;&#23548;&#25968;&#26469;&#25913;&#36827;&#20102;SMT&#12290;&#36825;&#20010;&#29256;&#26412;&#36824;&#21253;&#25324;&#20102;&#22788;&#29702;&#24102;&#22122;&#22768;&#21644;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#25968;&#25454;&#30340;&#26032;&#20989;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SMT 2.0&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#23618;&#27425;&#21644;&#28151;&#21512;&#36755;&#20837;&#30340;&#24320;&#28304;&#20195;&#29702;&#24211;&#12290;&#36825;&#20010;&#24320;&#28304;&#36719;&#20214;&#37319;&#29992;New BSD&#35768;&#21487;&#35777;&#36827;&#34892;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license.
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21033;&#29992;&#31561;&#21387;&#31561;&#28201;&#27969;&#24471;&#21040;&#21513;&#24067;&#26031;&#33258;&#30001;&#33021;&#65292;&#24182;&#22312;&#21333;&#21407;&#23376;&#27700;&#30340;&#32467;&#26230;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13233</link><description>&lt;p&gt;
&#36890;&#36807;&#31561;&#21387;&#31561;&#28201;&#27969;&#33719;&#24471;&#21513;&#24067;&#26031;&#33258;&#30001;&#33021;
&lt;/p&gt;
&lt;p&gt;
Gibbs free energies via isobaric-isothermal flows. (arXiv:2305.13233v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13233
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21033;&#29992;&#31561;&#21387;&#31561;&#28201;&#27969;&#24471;&#21040;&#21513;&#24067;&#26031;&#33258;&#30001;&#33021;&#65292;&#24182;&#22312;&#21333;&#21407;&#23376;&#27700;&#30340;&#32467;&#26230;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21487;&#20174;&#31561;&#21387;&#31561;&#28201;&#65288;NPT&#65289;&#38598;&#21512;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#36817;&#20284;&#26041;&#27861;&#26469;&#24471;&#21040;&#23436;&#20840;&#28789;&#27963;&#30340;&#19977;&#26012;&#26230;&#31995;&#32479;&#30340;&#32852;&#21512;&#20998;&#24067;&#21644;&#31890;&#23376;&#22352;&#26631;&#20197;&#36798;&#21040;&#25152;&#38656;&#30340;&#20869;&#37096;&#21387;&#21147;&#12290;&#25105;&#20204;&#23545;&#21333;&#21407;&#23376;&#27700;&#22312;&#31435;&#26041;&#21644;&#20845;&#35282;&#20912;&#30456;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#19982;&#24050;&#24314;&#31435;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#21513;&#24067;&#26031;&#33258;&#30001;&#33021;&#21644;&#20854;&#20182;&#21487;&#35266;&#27979;&#37327;&#30340;&#32467;&#26524;&#23436;&#20840;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a machine-learning model based on normalizing flows that is trained to sample from the isobaric-isothermal (NPT) ensemble. In our approach, we approximate the joint distribution of a fully-flexible triclinic simulation box and particle coordinates to achieve a desired internal pressure. We test our model on monatomic water in the cubic and hexagonal ice phases and find excellent agreement of Gibbs free energies and other observables compared with established baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#22240;&#26524;&#20998;&#26512;&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#20844;&#24179;&#24615;&#21442;&#25968;&#19982;&#20854;&#20182;&#20851;&#38190;&#25351;&#26631;&#30340;&#26435;&#34913;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#29702;&#35299;&#21644;&#20915;&#31574;&#22522;&#30784;&#65292;&#24110;&#21161;&#24320;&#21457;&#32773;&#22312;&#25552;&#20379;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#26102;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2305.13057</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#22240;&#26524;&#20851;&#32852;&#26435;&#34913;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causality-Aided Trade-off Analysis for Machine Learning Fairness. (arXiv:2305.13057v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#22240;&#26524;&#20998;&#26512;&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#20844;&#24179;&#24615;&#21442;&#25968;&#19982;&#20854;&#20182;&#20851;&#38190;&#25351;&#26631;&#30340;&#26435;&#34913;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#29702;&#35299;&#21644;&#20915;&#31574;&#22522;&#30784;&#65292;&#24110;&#21161;&#24320;&#21457;&#32773;&#22312;&#25552;&#20379;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#26102;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#26041;&#38754;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#28044;&#29616;&#20986;&#26469;&#12290;&#23613;&#31649;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#25552;&#21319;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23545;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#26102;&#65292;&#22312;&#32771;&#34385;&#22240;&#32032;&#20043;&#38388;&#30340;&#26435;&#34913;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#29702;&#35299;&#12290;&#36825;&#31181;&#29702;&#35299;&#23545;&#20110;&#24320;&#21457;&#32773;&#20570;&#20986;&#20851;&#20110;&#25552;&#20379;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#26126;&#26234;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#23384;&#22312;&#22810;&#20010;&#20844;&#24179;&#24615;&#21442;&#25968;&#21644;&#20854;&#20182;&#20851;&#38190;&#25351;&#26631;&#65292;&#24182;&#19988;&#24444;&#27492;&#20043;&#38388;&#23384;&#22312;&#32806;&#21512;&#29978;&#33267;&#20914;&#31361;&#26102;&#65292;&#20998;&#26512;&#36825;&#20123;&#26435;&#34913;&#26159;&#26497;&#20854;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22240;&#26524;&#20998;&#26512;&#20316;&#20026;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#20998;&#26512;&#20844;&#24179;&#24615;&#21442;&#25968;&#19982;&#20854;&#20182;&#20851;&#38190;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#23454;&#38469;&#26377;&#25928;&#22320;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#39046;&#22495;&#29305;&#23450;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#20934;&#30830;&#30340;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#22522;&#20110;&#25104;&#29087;&#30340;&#22240;&#26524;&#25512;&#26029;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#26032;&#39062;&#30340;&#26435;&#34913;&#20998;&#26512;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an increasing interest in enhancing the fairness of machine learning (ML). Despite the growing number of fairness-improving methods, we lack a systematic understanding of the trade-offs among factors considered in the ML pipeline when fairness-improving methods are applied. This understanding is essential for developers to make informed decisions regarding the provision of fair ML services. Nonetheless, it is extremely difficult to analyze the trade-offs when there are multiple fairness parameters and other crucial metrics involved, coupled, and even in conflict with one another.  This paper uses causality analysis as a principled method for analyzing trade-offs between fairness parameters and other crucial metrics in ML pipelines. To ractically and effectively conduct causality analysis, we propose a set of domain-specific optimizations to facilitate accurate causal discovery and a unified, novel interface for trade-off analysis based on well-established causal inferenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36817;&#20284;&#31639;&#27861;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#26694;&#26550;DClEVerNet&#65292;&#21487;&#20197;&#20248;&#21270;&#22823;&#35268;&#27169;&#32593;&#32476;&#21270;&#30340;EV&#20805;&#30005;&#31449;&#30340;&#39044;&#32422;&#31649;&#29702;&#31243;&#24207;&#65292;&#26368;&#22823;&#21270;EV&#29992;&#25143;&#30340;&#24635;&#31119;&#21033;&#25910;&#30410;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#32593;&#32476;&#30340;&#21487;&#29992;&#21151;&#29575;&#23481;&#37327;&#21644;&#31449;&#28857;&#30340;&#20837;&#20303;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.11195</link><description>&lt;p&gt;
DClEVerNet: &#28145;&#24230;&#32452;&#21512;&#23398;&#20064;&#20248;&#21270;&#22823;&#35268;&#27169;&#32593;&#32476;&#21270;&#20805;&#30005;&#35774;&#26045;&#30340;&#39640;&#25928;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
DClEVerNet: Deep Combinatorial Learning for Efficient EV Charging Scheduling in Large-scale Networked Facilities. (arXiv:2305.11195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36817;&#20284;&#31639;&#27861;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#26694;&#26550;DClEVerNet&#65292;&#21487;&#20197;&#20248;&#21270;&#22823;&#35268;&#27169;&#32593;&#32476;&#21270;&#30340;EV&#20805;&#30005;&#31449;&#30340;&#39044;&#32422;&#31649;&#29702;&#31243;&#24207;&#65292;&#26368;&#22823;&#21270;EV&#29992;&#25143;&#30340;&#24635;&#31119;&#21033;&#25910;&#30410;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#32593;&#32476;&#30340;&#21487;&#29992;&#21151;&#29575;&#23481;&#37327;&#21644;&#31449;&#28857;&#30340;&#20837;&#20303;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20132;&#36890;&#30005;&#27668;&#21270;&#65292;&#30005;&#21160;&#27773;&#36710;&#65288;EV&#65289;&#30340;&#26222;&#21450;&#21487;&#33021;&#20250;&#26174;&#30528;&#22686;&#21152;&#37197;&#30005;&#32593;&#32476;&#30340;&#21387;&#21147;&#65292;&#23548;&#33268;&#20854;&#24615;&#33021;&#19979;&#38477;&#21644;&#31283;&#23450;&#24615;&#21463;&#21040;&#23041;&#32961;&#12290;&#20026;&#20102;&#20197;&#32463;&#27982;&#26377;&#25928;&#30340;&#26041;&#24335;&#23481;&#32435;&#36825;&#20123;&#26032;&#36127;&#36733;&#65292;&#29616;&#20195;&#30005;&#21147;&#32593;&#32476;&#38656;&#35201;&#21327;&#35843;&#25110;&#8220;&#26234;&#33021;&#8221;&#20805;&#30005;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#19979;&#20248;&#21270;EV&#20805;&#30005;&#35843;&#24230;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#22823;&#35268;&#27169;&#12289;&#32593;&#32476;&#21270;EV&#20805;&#30005;&#31449;&#30340;&#39044;&#32422;&#31649;&#29702;&#31243;&#24207;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26102;&#32806;&#21512;&#30340;&#20108;&#36827;&#21046;&#20248;&#21270;&#38382;&#39064;&#65292;&#26368;&#22823;&#21270;EV&#29992;&#25143;&#30340;&#24635;&#31119;&#21033;&#25910;&#30410;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#32593;&#32476;&#30340;&#21487;&#29992;&#21151;&#29575;&#23481;&#37327;&#21644;&#31449;&#28857;&#30340;&#20837;&#20303;&#38480;&#21046;&#12290;&#20026;&#20102;&#22312;&#20445;&#25345;&#39640;&#35299;&#20915;&#36136;&#37327;&#30340;&#21516;&#26102;&#22823;&#35268;&#27169;&#35299;&#20915;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36817;&#20284;&#31639;&#27861;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36755;&#20837;&#36755;&#20986;&#22788;&#29702;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the electrification of transportation, the rising uptake of electric vehicles (EVs) might stress distribution networks significantly, leaving their performance degraded and stability jeopardized. To accommodate these new loads cost-effectively, modern power grids require coordinated or ``smart'' charging strategies capable of optimizing EV charging scheduling in a scalable and efficient fashion. With this in view, the present work focuses on reservation management programs for large-scale, networked EV charging stations. We formulate a time-coupled binary optimization problem that maximizes EV users' total welfare gain while accounting for the network's available power capacity and stations' occupancy limits. To tackle the problem at scale while retaining high solution quality, a data-driven optimization framework combining techniques from the fields of Deep Learning and Approximation Algorithms is introduced. The framework's key ingredient is a novel input-output processing schem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;P2MPO&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#40065;&#26834;&#31163;&#32447;RL&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#21644;&#21452;&#37325;&#24754;&#35266;&#30340;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#65292;&#37319;&#29992;&#21452;&#37325;&#24754;&#35266;&#24615;&#21407;&#21017;&#20197;&#20811;&#26381;&#27169;&#22411;&#20559;&#31227;&#31561;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#35813;&#26694;&#26550;&#22312;&#25317;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#20855;&#22791;&#39640;&#25928;&#24615;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.09659</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40065;&#26834;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#21452;&#37325;&#24754;&#35266;&#24615;&#30340;&#36890;&#29992;&#31639;&#27861;&#21644;&#24378;&#20581;&#37096;&#20998;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage. (arXiv:2305.09659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;P2MPO&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#40065;&#26834;&#31163;&#32447;RL&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#21644;&#21452;&#37325;&#24754;&#35266;&#30340;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#65292;&#37319;&#29992;&#21452;&#37325;&#24754;&#35266;&#24615;&#21407;&#21017;&#20197;&#20811;&#26381;&#27169;&#22411;&#20559;&#31227;&#31561;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#35813;&#26694;&#26550;&#22312;&#25317;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#20855;&#22791;&#39640;&#25928;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#40065;&#26834;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;&#40065;&#26834;&#31163;&#32447;RL&#65289;&#65292;&#20854;&#26088;&#22312;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#32431;&#31929;&#22320;&#25214;&#21040;&#19968;&#20010;&#33021;&#22815;&#22312;&#25200;&#21160;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#26368;&#20248;&#24378;&#40065;&#26834;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;P2MPO&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#21644;&#21452;&#37325;&#24754;&#35266;&#30340;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#12290;&#21452;&#37325;&#24754;&#35266;&#24615;&#21407;&#21017;&#23545;&#20110;&#20811;&#26381;&#30001;&#34892;&#20026;&#31574;&#30053;&#21644;&#30446;&#26631;&#31574;&#30053;&#23478;&#26063;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#20197;&#21450;&#21517;&#20041;&#27169;&#22411;&#30340;&#25200;&#21160;&#25152;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23545;&#27169;&#22411;&#20272;&#35745;&#23376;&#20363;&#31243;&#36827;&#34892;&#19968;&#23450;&#20934;&#30830;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;P2MPO&#31639;&#27861;&#22312;&#25317;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#37096;&#20998;&#35206;&#30422;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distributionally robust offline reinforcement learning (robust offline RL), which seeks to find an optimal robust policy purely from an offline dataset that can perform well in perturbed environments. We propose a generic algorithm framework \underline{D}oubly \underline{P}essimistic \underline{M}odel-based \underline{P}olicy \underline{O}ptimization ($\texttt{P}^2\texttt{MPO}$) for robust offline RL, which features a novel combination of a flexible model estimation subroutine and a doubly pessimistic policy optimization step. The \emph{double pessimism} principle is crucial to overcome the distributional shift incurred by i) the mismatch between behavior policy and the family of target policies; and ii) the perturbation of the nominal model. Under certain accuracy assumptions on the model estimation subroutine, we show that $\texttt{P}^2\texttt{MPO}$ is provably efficient with \emph{robust partial coverage data}, which means that the offline dataset has good coverage of the d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#37327;&#21270;&#29702;&#35770;&#26041;&#27861;&#20248;&#21270;&#19981;&#36830;&#32493;&#21464;&#37327;&#30340;&#35774;&#35745;&#65292;&#20197;&#24179;&#34913;&#25928;&#24212;&#22823;&#23567;&#30340;&#22686;&#30410;&#21644;&#25439;&#22833;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08559</link><description>&lt;p&gt;
&#35774;&#35745;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Designing Discontinuities. (arXiv:2305.08559v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#37327;&#21270;&#29702;&#35770;&#26041;&#27861;&#20248;&#21270;&#19981;&#36830;&#32493;&#21464;&#37327;&#30340;&#35774;&#35745;&#65292;&#20197;&#24179;&#34913;&#25928;&#24212;&#22823;&#23567;&#30340;&#22686;&#30410;&#21644;&#25439;&#22833;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#36830;&#32493;&#24615;&#21487;&#20197;&#26159;&#30456;&#24403;&#20219;&#24847;&#30340;&#65292;&#20294;&#20063;&#20250;&#22312;&#31038;&#20250;&#31995;&#32479;&#20013;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#20107;&#23454;&#19978;&#65292;&#23427;&#20204;&#30340;&#20219;&#24847;&#24615;&#26159;&#20026;&#20160;&#20040;&#23427;&#20204;&#34987;&#29992;&#20110;&#25512;&#26029;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#35745;&#37327;&#32463;&#27982;&#23398;&#20013;&#30340;&#22238;&#24402;&#19981;&#36830;&#32493;&#24615;&#20551;&#23450;&#23384;&#22312;&#19968;&#20010;&#19981;&#36830;&#32493;&#30340;&#21464;&#37327;&#65292;&#23558;&#20154;&#21475;&#20998;&#25104;&#19981;&#21516;&#30340;&#37096;&#20998;&#65292;&#20197;&#20272;&#35745;&#32473;&#23450;&#29616;&#35937;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20026;&#32473;&#23450;&#30340;&#19981;&#36830;&#32493;&#21464;&#37327;&#35774;&#35745;&#20998;&#21306;&#20197;&#20248;&#21270;&#20197;&#21069;&#20351;&#29992;&#22238;&#24402;&#19981;&#36830;&#32493;&#24615;&#30740;&#31350;&#36807;&#30340;&#26576;&#31181;&#25928;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#29702;&#35770;&#26041;&#27861;&#26469;&#20248;&#21270;&#24863;&#20852;&#36259;&#30340;&#25928;&#26524;&#65292;&#39318;&#20808;&#23398;&#20064;&#32473;&#23450;&#19981;&#36830;&#32493;&#21464;&#37327;&#30340;&#22240;&#26524;&#25928;&#24212;&#22823;&#23567;&#65292;&#28982;&#21518;&#24212;&#29992;&#21160;&#24577;&#35268;&#21010;&#26469;&#20248;&#21270;&#19981;&#36830;&#32493;&#24615;&#30340;&#37327;&#21270;&#35774;&#35745;&#65292;&#20197;&#24179;&#34913;&#22686;&#30410;&#21644;&#25439;&#22833;&#30340;&#25928;&#24212;&#22823;&#23567;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#24418;&#25104;&#21160;&#24577;&#35268;&#21010;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discontinuities can be fairly arbitrary but also cause a significant impact on outcomes in social systems. Indeed, their arbitrariness is why they have been used to infer causal relationships among variables in numerous settings. Regression discontinuity from econometrics assumes the existence of a discontinuous variable that splits the population into distinct partitions to estimate the causal effects of a given phenomenon. Here we consider the design of partitions for a given discontinuous variable to optimize a certain effect previously studied using regression discontinuity. To do so, we propose a quantization-theoretic approach to optimize the effect of interest, first learning the causal effect size of a given discontinuous variable and then applying dynamic programming for optimal quantization design of discontinuities that balance the gain and loss in the effect size. We also develop a computationally-efficient reinforcement learning algorithm for the dynamic programming formul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24378;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#23545;&#20195;&#30721;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#20197;&#36866;&#24212;&#21464;&#21270;&#30340;&#36719;&#20214;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.04106</link><description>&lt;p&gt;
&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23545;&#20195;&#30721;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#20197;&#23454;&#29616;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code. (arXiv:2305.04106v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#23545;&#20195;&#30721;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#20197;&#36866;&#24212;&#21464;&#21270;&#30340;&#36719;&#20214;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20195;&#30721;&#20013;&#30340;&#26222;&#36941;&#25216;&#26415;&#65292;&#21033;&#29992;&#20004;&#38454;&#27573;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#33719;&#21462;&#20851;&#20110;&#20195;&#30721;&#30340;&#36890;&#29992;&#30693;&#35782;&#24182;&#19987;&#38376;&#20174;&#20107;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36719;&#20214;&#20195;&#30721;&#24211;&#30340;&#21160;&#24577;&#24615;&#23545;PLMs&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#26500;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#24378;&#35843;&#38656;&#35201;&#35843;&#25972;&#36866;&#24212;&#20195;&#30721;&#30340;PLMs&#65292;&#36866;&#24212;&#20998;&#24067;&#20250;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36719;&#20214;&#25968;&#25454;&#65292;&#36825;&#26159;&#20043;&#21069;&#30340;&#30740;&#31350;&#25152;&#24573;&#35270;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#21160;&#26426;&#26159;&#23558;PLM&#35270;&#20026;&#19968;&#20010;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#27169;&#22411;&#22312;&#24212;&#23545;&#28436;&#21270;&#30340;&#36719;&#20214;&#25968;&#25454;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have become a prevalent technique in deep learning for code, utilizing a two-stage pre-training and fine-tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks. However, the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of PLMs. In particular, world-realistic scenarios potentially lead to significant differences between the distribution of the pre-training and test data, i.e., distribution shift, resulting in a degradation of the PLM's performance on downstream tasks. In this paper, we stress the need for adapting PLMs of code to software data whose distribution changes over time, a crucial problem that has been overlooked in previous works. The motivation of this work is to consider the PLM in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario. Specifically, we design a scenario where the model 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#20307;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.14633</link><description>&lt;p&gt;
CVRecon: &#37325;&#26032;&#24605;&#32771;&#31070;&#32463;&#37325;&#24314;&#30340;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction. (arXiv:2304.14633v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14633
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#20307;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20351;&#29992;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#31070;&#32463;&#37325;&#24314;&#30340;&#36827;&#23637;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32570;&#20047;&#28145;&#24230;&#20449;&#24687;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20307;&#31215;&#30340;&#25216;&#26415;&#20165;&#27839;&#25972;&#20010;&#30456;&#26426;&#20809;&#32447;&#22797;&#21046;&#23545;&#35937;&#34920;&#38754;&#30340;2D&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#22797;&#21046;&#20250;&#22312;&#31354;&#27934;&#21644;&#36974;&#25377;&#31354;&#38388;&#20013;&#24341;&#20837;&#22122;&#22768;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;3D&#20960;&#20309;&#20307;&#25104;&#24418;&#26041;&#38754;&#20135;&#29983;&#25361;&#25112;&#12290;&#21463;&#20256;&#32479;&#22810;&#35270;&#35282;&#31435;&#20307;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#26088;&#22312;&#21033;&#29992;&#20195;&#20215;&#20307;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#20960;&#20309;&#29305;&#24449;&#34920;&#31034;&#27861;&#8212;&#8212;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#32534;&#30721;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#24674;&#22797;&#20102;&#28165;&#26224;&#30340;
&lt;/p&gt;
&lt;p&gt;
Recent advances in neural reconstruction using posed image sequences have made remarkable progress. However, due to the lack of depth information, existing volumetric-based techniques simply duplicate 2D image features of the object surface along the entire camera ray. We contend this duplication introduces noise in empty and occluded spaces, posing challenges for producing high-quality 3D geometry. Drawing inspiration from traditional multi-view stereo methods, we propose an end-to-end 3D neural reconstruction framework CVRecon, designed to exploit the rich geometric embedding in the cost volumes to facilitate 3D geometric feature learning. Furthermore, we present Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature representation that encodes view-dependent information with improved integrity and robustness. Through comprehensive experiments, we demonstrate that our approach significantly improves the reconstruction quality in various metrics and recovers clear
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; shard graph &#36827;&#34892;&#26426;&#22120;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#26368;&#23567;&#21270;&#36951;&#24536;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#22810;&#26679;&#25968;&#25454;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13169</link><description>&lt;p&gt;
SAFE: &#20351;&#29992; Shard Graphs &#36827;&#34892;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
SAFE: Machine Unlearning With Shard Graphs. (arXiv:2304.13169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; shard graph &#36827;&#34892;&#26426;&#22120;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#26368;&#23567;&#21270;&#36951;&#24536;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#22810;&#26679;&#25968;&#25454;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; Synergy Aware Forgetting Ensemble&#65288;SAFE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26368;&#23567;&#21270;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#28040;&#38500;&#35757;&#32451;&#26679;&#26412;&#24433;&#21709;&#30340;&#39044;&#26399;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#36866;&#24212;&#21508;&#31181;&#25968;&#25454;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;&#36825;&#20010;&#36807;&#31243;&#20063;&#34987;&#31216;&#20026;&#36873;&#25321;&#24615;&#36951;&#24536;&#25110;&#36951;&#24536;&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#20998;&#25104;&#30862;&#29255;&#65292;&#23545;&#27599;&#20010;&#30862;&#29255;&#36827;&#34892;&#23436;&#20840;&#29420;&#31435;&#30340;&#27169;&#22411;&#35757;&#32451;&#65292;&#28982;&#21518;&#23558;&#25152;&#24471;&#27169;&#22411;&#21512;&#25104;&#26469;&#36827;&#34892;&#30340;&#12290;&#22686;&#21152;&#30862;&#29255;&#30340;&#25968;&#37327;&#21487;&#20197;&#38477;&#20302;&#36951;&#24536;&#30340;&#39044;&#26399;&#25104;&#26412;&#65292;&#20294;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#25512;&#29702;&#25104;&#26412;&#65292;&#24182;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#26368;&#32456;&#20934;&#30830;&#24615;&#65292;&#22240;&#20026;&#29420;&#31435;&#30340;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#22833;&#21435;&#20102;&#26679;&#26412;&#20043;&#38388;&#30340;&#21327;&#21516;&#20449;&#24687;&#12290;SAFE &#24341;&#20837;&#20102; shard graph &#30340;&#27010;&#24565;&#65292;&#23427;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#20854;&#20182;&#30862;&#29255;&#20013;&#24341;&#20837;&#26377;&#38480;&#30340;&#20449;&#24687;&#65292;&#20197;&#29306;&#29298;&#19968;&#23450;&#30340;&#39044;&#26399;&#36951;&#24536;&#25104;&#26412;&#22686;&#21152;&#26126;&#26174;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20173;&#28982;&#23454;&#29616;&#23436;&#20840;&#28040;&#38500;&#27531;&#30041;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Synergy Aware Forgetting Ensemble (SAFE), a method to adapt large models on a diverse collection of data while minimizing the expected cost to remove the influence of training samples from the trained model. This process, also known as selective forgetting or unlearning, is often conducted by partitioning a dataset into shards, training fully independent models on each, then ensembling the resulting models. Increasing the number of shards reduces the expected cost to forget but at the same time it increases inference cost and reduces the final accuracy of the model since synergistic information between samples is lost during the independent model training. Rather than treating each shard as independent, SAFE introduces the notion of a shard graph, which allows incorporating limited information from other shards during training, trading off a modest increase in expected forgetting cost with a significant increase in accuracy, all while still attaining complete removal of resi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#22359;&#21270;&#26680;&#25506;&#27979;&#22120;&#30340;&#26080;&#26631;&#35760;&#26102;&#38388;&#20998;&#26512;&#65292;&#20854;&#33021;&#22815;&#21033;&#29992;&#21333;&#20010;&#25506;&#27979;&#22120;&#20869;&#37096;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#26377;&#24847;&#20041;&#21644;&#20934;&#30830;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.11930</link><description>&lt;p&gt;
&#26080;&#26631;&#35760;&#26102;&#38388;&#20998;&#26512;&#27169;&#22359;&#21270;&#26680;&#25506;&#27979;&#22120;&#30340;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Label-free timing analysis of modularized nuclear detectors with physics-constrained deep learning. (arXiv:2304.11930v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11930
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#22359;&#21270;&#26680;&#25506;&#27979;&#22120;&#30340;&#26080;&#26631;&#35760;&#26102;&#38388;&#20998;&#26512;&#65292;&#20854;&#33021;&#22815;&#21033;&#29992;&#21333;&#20010;&#25506;&#27979;&#22120;&#20869;&#37096;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#26377;&#24847;&#20041;&#21644;&#20934;&#30830;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#26102;&#38388;&#26159;&#26680;&#20202;&#22120;&#23398;&#20013;&#37325;&#35201;&#30340;&#35805;&#39064;&#65292;&#20855;&#26377;&#20174;&#39640;&#33021;&#29289;&#29702;&#21040;&#36752;&#23556;&#25104;&#20687;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#39640;&#36895;&#27169;&#25968;&#36716;&#25442;&#22120;&#36234;&#26469;&#36234;&#21457;&#23637;&#21644;&#26131;&#20110;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#26680;&#25506;&#27979;&#22120;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#20248;&#28857;&#20173;&#19981;&#30830;&#23450;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30456;&#20851;&#30340;&#26102;&#38388;&#31639;&#27861;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#29702;&#35299;&#21644;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#22359;&#21270;&#26680;&#25506;&#27979;&#22120;&#30340;&#26102;&#38388;&#20998;&#26512;&#65292;&#26080;&#38656;&#23545;&#20107;&#20214;&#25968;&#25454;&#36827;&#34892;&#26174;&#24335;&#30340;&#26631;&#35760;&#12290;&#36890;&#36807;&#21033;&#29992;&#21333;&#20010;&#25506;&#27979;&#22120;&#20869;&#37096;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24418;&#25104;&#19968;&#20010;&#26080;&#26631;&#35760;&#25439;&#22833;&#20989;&#25968;&#21644;&#19968;&#20010;&#32463;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#21644;&#20934;&#30830;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#25105;&#20204;&#20174;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#25152;&#38656;&#26041;&#27861;&#30340;&#26368;&#20248;&#20989;&#25968;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#31639;&#27861;&#26469;&#35757;&#32451;&#21644;&#26657;&#20934;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#22312;&#26680;&#33021;&#35889;&#23398;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulse timing is an important topic in nuclear instrumentation, with far-reaching applications from high energy physics to radiation imaging. While high-speed analog-to-digital converters become more and more developed and accessible, their potential uses and merits in nuclear detector signal processing are still uncertain, partially due to associated timing algorithms which are not fully understood and utilized. In this paper, we propose a novel method based on deep learning for timing analysis of modularized nuclear detectors without explicit needs of labelling event data. By taking advantage of the inner time correlation of individual detectors, a label-free loss function with a specially designed regularizer is formed to supervise the training of neural networks towards a meaningful and accurate mapping function. We mathematically demonstrate the existence of the optimal function desired by the method, and give a systematic algorithm for training and calibration of the model. The pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#22270;&#34701;&#21512;&#21450;&#20004;&#27493;&#24335;&#36801;&#31227;&#23398;&#20064;&#22686;&#24378;&#30340;&#33258;&#21160;&#20869;&#38236;&#32467;&#30707;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#32958;&#32467;&#30707;&#20998;&#31867;&#20934;&#30830;&#24230;&#36798;6%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.03193</link><description>&lt;p&gt;
&#37319;&#29992;&#22810;&#35270;&#22270;&#34701;&#21512;&#21450;&#20004;&#27493;&#24335;&#36801;&#31227;&#23398;&#20064;&#22686;&#24378;&#30340;&#33258;&#21160;&#20869;&#38236;&#32467;&#30707;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving automatic endoscopic stone recognition using a multi-view fusion approach enhanced with two-step transfer learning. (arXiv:2304.03193v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#22270;&#34701;&#21512;&#21450;&#20004;&#27493;&#24335;&#36801;&#31227;&#23398;&#20064;&#22686;&#24378;&#30340;&#33258;&#21160;&#20869;&#38236;&#32467;&#30707;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#32958;&#32467;&#30707;&#20998;&#31867;&#20934;&#30830;&#24230;&#36798;6%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#34701;&#21512;&#20174;&#19981;&#21516;&#35270;&#35282;&#33719;&#21462;&#30340;&#22270;&#20687;&#20449;&#24687;&#65292;&#26088;&#22312;&#20026;&#37492;&#23450;&#20869;&#38236;&#22270;&#20687;&#20013;&#25152;&#35265;&#30340;&#32958;&#32467;&#30707;&#31867;&#22411;&#20135;&#29983;&#26356;&#20855;&#21306;&#20998;&#24615;&#30340;&#29289;&#20307;&#29305;&#24449;&#12290;&#26412;&#27169;&#22411;&#36827;&#19968;&#27493;&#24212;&#29992;&#20102;&#20004;&#27493;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21644;&#27880;&#24847;&#21147;&#22359;&#26469;&#35843;&#25972;&#23398;&#20064;&#30340;&#29305;&#24449;&#22270;&#12290;&#28145;&#24230;&#29305;&#24449;&#34701;&#21512;&#31574;&#30053;&#23558;&#32958;&#32467;&#30707;&#20998;&#31867;&#20934;&#30830;&#24230;&#27604;&#21333;&#35270;&#22270;&#25552;&#21462;&#20027;&#24178;&#27169;&#22411;&#25552;&#39640;&#20102;6%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
This contribution presents a deep-learning method for extracting and fusing image information acquired from different viewpoints, with the aim to produce more discriminant object features for the identification of the type of kidney stones seen in endoscopic images. The model was further improved with a two-step transfer learning approach and by attention blocks to refine the learned feature maps. Deep feature fusion strategies improved the results of single view extraction backbone models by more than 6% in terms of accuracy of the kidney stones classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#22312;&#26550;&#26500;&#20013;&#21512;&#24182;&#20102;&#26230;&#20307;&#30340;&#21333;&#20803;&#26684;&#23545;&#31216;&#24615;&#65292;&#24182;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#22810;&#23380;&#32467;&#26500;&#65292;&#21487;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22810;&#23380;&#26230;&#20307;&#26448;&#26009;&#30340;&#21560;&#38468;&#28909;&#12290;</title><link>http://arxiv.org/abs/2304.01628</link><description>&lt;p&gt;
&#22810;&#23380;&#26230;&#24577;&#26448;&#26009;&#30340;&#31561;&#21464;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Equivariant Networks for Porous Crystalline Materials. (arXiv:2304.01628v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#22312;&#26550;&#26500;&#20013;&#21512;&#24182;&#20102;&#26230;&#20307;&#30340;&#21333;&#20803;&#26684;&#23545;&#31216;&#24615;&#65292;&#24182;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#22810;&#23380;&#32467;&#26500;&#65292;&#21487;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22810;&#23380;&#26230;&#20307;&#26448;&#26009;&#30340;&#21560;&#38468;&#28909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#39044;&#27979;&#22810;&#23380;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#20855;&#26377;&#21152;&#36895;&#24320;&#21457;&#26032;&#26448;&#26009;&#30340;&#39640;&#36890;&#37327;&#31579;&#36873;&#36807;&#31243;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#22240;&#20026;&#20351;&#29992;&#31532;&#19968;&#21407;&#29702;&#27169;&#22411;&#36827;&#34892;&#30340;&#27169;&#25311;&#24448;&#24448;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#24314;&#27169;&#36825;&#20123;&#26448;&#26009;&#65292;&#25105;&#20204;&#38656;&#35201;&#21033;&#29992;&#26230;&#20307;&#20013;&#23384;&#22312;&#30340;&#23545;&#31216;&#24615;&#65292;&#36825;&#20123;&#23545;&#31216;&#24615;&#30001;&#23427;&#20204;&#30340;&#31354;&#38388;&#32676;&#23450;&#20041;&#12290;&#29616;&#26377;&#30340;&#26230;&#20307;&#24615;&#36136;&#39044;&#27979;&#26041;&#27861;&#35201;&#20040;&#20855;&#26377;&#36807;&#20110;&#20005;&#26684;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#65292;&#35201;&#20040;&#20165;&#21253;&#25324;&#21333;&#20803;&#26684;&#20043;&#38388;&#30340;&#23545;&#31216;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#27809;&#26377;&#26126;&#30830;&#22320;&#24314;&#27169;&#26230;&#20307;&#30340;&#22810;&#23380;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#22312;&#20854;&#26550;&#26500;&#20013;&#21512;&#24182;&#20102;&#26230;&#20307;&#30340;&#21333;&#20803;&#26684;&#23545;&#31216;&#24615;&#65292;&#24182;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#22810;&#23380;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#19981;&#21516;&#26500;&#22411;&#30340;&#33707;&#23572;&#23450;&#30707;&#27832;&#30707;&#30340;CO$_2$&#21560;&#38468;&#28909;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26230;&#20307;&#24615;&#36136;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently predicting properties of porous crystalline materials has great potential to accelerate the high throughput screening process for developing new materials, as simulations carried out using first principles model are often computationally expensive. To effectively make use of Deep Learning methods to model these materials, we need to utilize the symmetries present in the crystals, which are defined by their space group. Existing methods for crystal property prediction either have symmetry constraints that are too restrictive or only incorporate symmetries between unit cells. In addition, these models do not explicitly model the porous structure of the crystal. In this paper, we develop a model which incorporates the symmetries of the unit cell of a crystal in its architecture and explicitly models the porous structure. We evaluate our model by predicting the heat of adsorption of CO$_2$ for different configurations of the mordenite zeolite. Our results confirm that our metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29702;&#35770;&#35777;&#26126;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#26159;&#38477;&#20302;Adam&#25110;AdamW&#33719;&#24471;&#36755;&#20986;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#30340;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#26041;&#38024;&#20026;Adam&#25110;AdamW&#20248;&#21270;&#31639;&#27861;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;Lipschitz&#24120;&#25968;&#36739;&#20302;&#19988;&#26368;&#22823;&#20540;&#36739;&#23567;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16464</link><description>&lt;p&gt;
Adam&#21644;AdamW&#20248;&#21270;&#22120;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#25928;&#24212;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers. (arXiv:2303.16464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#35777;&#26126;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#26159;&#38477;&#20302;Adam&#25110;AdamW&#33719;&#24471;&#36755;&#20986;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#30340;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#26041;&#38024;&#20026;Adam&#25110;AdamW&#20248;&#21270;&#31639;&#27861;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;Lipschitz&#24120;&#25968;&#36739;&#20302;&#19988;&#26368;&#22823;&#20540;&#36739;&#23567;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#19982;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#26159;&#38477;&#20302;Adam&#25110;AdamW&#33719;&#24471;&#36755;&#20986;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20316;&#20026;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#26102;&#20248;&#21270;&#31639;&#27861;&#20026;Adam&#25110;AdamW&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#26412;&#25991;&#36873;&#25321;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20154;&#33080;&#24180;&#40836;&#35780;&#20272;&#38382;&#39064;&#26469;&#35780;&#20272;&#29702;&#35770;&#30028;&#38480;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#65292;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20174;&#19981;&#21516;&#20998;&#24067;&#20013;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Lipschitz&#24120;&#25968;&#36739;&#20302;&#19988;&#26368;&#22823;&#20540;&#36739;&#23567;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;Adam&#25110;AdamW&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization performance of deep neural networks with regard to the optimization algorithm is one of the major concerns in machine learning. This performance can be affected by various factors. In this paper, we theoretically prove that the Lipschitz constant of a loss function is an important factor to diminish the generalization error of the output model obtained by Adam or AdamW. The results can be used as a guideline for choosing the loss function when the optimization algorithm is Adam or AdamW. In addition, to evaluate the theoretical bound in a practical setting, we choose the human age estimation problem in computer vision. For assessing the generalization better, the training and test datasets are drawn from different distributions. Our experimental evaluation shows that the loss function with lower Lipschitz constant and maximum value improves the generalization of the model trained by Adam or AdamW.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#36710;&#32852;&#32593;&#36890;&#20449;&#20013;&#30340;QoS&#39044;&#27979;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#37319;&#26679;&#36807;&#31243;&#12289;&#25968;&#25454;&#38598;&#29305;&#24449;&#20998;&#26512;&#12289;&#25968;&#25454;&#25286;&#20998;&#25928;&#26524;&#21644;&#25968;&#25454;&#21487;&#29992;&#24615;&#31561;&#26041;&#38754;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#25552;&#21319;&#26368;&#22823;&#21534;&#21520;&#37327;&#39044;&#27979;&#22312;&#27773;&#36710;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2302.11966</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#36710;&#32852;&#32593;&#36890;&#20449;&#20013;&#30340;QoS&#39044;&#27979;&#65306;&#25361;&#25112;&#19982;&#35299;&#20915;&#26041;&#26696;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for QoS Prediction in Vehicular Communication: Challenges and Solution Approaches. (arXiv:2302.11966v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#36710;&#32852;&#32593;&#36890;&#20449;&#20013;&#30340;QoS&#39044;&#27979;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#37319;&#26679;&#36807;&#31243;&#12289;&#25968;&#25454;&#38598;&#29305;&#24449;&#20998;&#26512;&#12289;&#25968;&#25454;&#25286;&#20998;&#25928;&#26524;&#21644;&#25968;&#25454;&#21487;&#29992;&#24615;&#31561;&#26041;&#38754;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#25552;&#21319;&#26368;&#22823;&#21534;&#21520;&#37327;&#39044;&#27979;&#22312;&#27773;&#36710;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#34562;&#31389;&#32593;&#32476;&#36827;&#21270;&#21040;&#31532;&#20845;&#20195;&#65292;&#26426;&#22120;&#23398;&#20064;&#34987;&#35270;&#20026;&#25552;&#21319;&#32593;&#32476;&#33021;&#21147;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#39044;&#27979;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32593;&#32476;&#21464;&#24471;&#20027;&#21160;&#12290;&#32593;&#32476;&#30340;&#20027;&#21160;&#34892;&#20026;&#21487;&#20197;&#29992;&#20110;&#32500;&#25345;&#29305;&#23450;&#30340;&#26381;&#21153;&#36136;&#37327;&#35201;&#27714;&#12290;&#26377;&#20102;&#39044;&#27979;&#24615;&#30340;&#26381;&#21153;&#36136;&#37327;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#26032;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#23588;&#20854;&#26159;&#22312;&#27773;&#36710;&#34892;&#19994;&#20013;&#65292;&#28041;&#21450;&#21040;&#23433;&#20840;&#21644;&#23089;&#20048;&#31561;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#25552;&#39640;&#26368;&#22823;&#21534;&#21520;&#37327;&#30340;&#39044;&#27979;&#65292;&#22914;&#27969;&#23186;&#20307;&#25110;&#39640;&#28165;&#22320;&#22270;&#24212;&#29992;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#65292;&#24378;&#35843;&#20102;&#19968;&#20123;&#19981;&#22826;&#34987;&#20851;&#27880;&#30340;&#26041;&#38754;&#65292;&#22914;&#35814;&#32454;&#30340;&#37319;&#26679;&#36807;&#31243;&#12289;&#25968;&#25454;&#38598;&#29305;&#24449;&#30340;&#28145;&#20837;&#20998;&#26512;&#12289;&#25552;&#20379;&#32467;&#26524;&#26102;&#30340;&#25968;&#25454;&#25286;&#20998;&#25928;&#26524;&#20197;&#21450;&#25968;&#25454;&#21487;&#29992;&#24615;&#12290;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#38754;&#23545;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
As cellular networks evolve towards the 6th generation, machine learning is seen as a key enabling technology to improve the capabilities of the network. Machine learning provides a methodology for predictive systems, which can make networks become proactive. This proactive behavior of the network can be leveraged to sustain, for example, a specific quality of service requirement. With predictive quality of service, a wide variety of new use cases, both safety- and entertainment-related, are emerging, especially in the automotive sector. Therefore, in this work, we consider maximum throughput prediction enhancing, for example, streaming or high-definition mapping applications. We discuss the entire machine learning workflow highlighting less regarded aspects such as the detailed sampling procedures, the in-depth analysis of the dataset characteristics, the effects of splits in the provided results, and the data availability. Reliable machine learning models need to face a lot of challe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#21452;&#37325;TCN-Attention&#32593;&#32476;&#30340;DTAAD&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#36890;&#36807;&#38598;&#25104;&#35774;&#35745;&#21644;&#24341;&#20837;&#32553;&#25918;&#26041;&#27861;&#21644;&#21453;&#39304;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#24555;&#36895;&#20934;&#30830;&#23450;&#20301;&#24322;&#24120;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25193;&#22823;&#20102;&#30456;&#20851;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2302.10753</link><description>&lt;p&gt;
DTAAD: &#21452;&#37325;TCN-Attention&#32593;&#32476;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DTAAD: Dual Tcn-Attention Networks for Anomaly Detection in Multivariate Time Series Data. (arXiv:2302.10753v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10753
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#21452;&#37325;TCN-Attention&#32593;&#32476;&#30340;DTAAD&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#36890;&#36807;&#38598;&#25104;&#35774;&#35745;&#21644;&#24341;&#20837;&#32553;&#25918;&#26041;&#27861;&#21644;&#21453;&#39304;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#24555;&#36895;&#20934;&#30830;&#23450;&#20301;&#24322;&#24120;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25193;&#22823;&#20102;&#30456;&#20851;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#33021;&#22815;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#26377;&#25928;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#23545;&#24403;&#20170;&#24037;&#19994;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#24322;&#24120;&#26631;&#31614;&#12289;&#25968;&#25454;&#30340;&#39640;&#32500;&#22797;&#26434;&#24615;&#12289;&#23454;&#38469;&#30828;&#20214;&#30340;&#20869;&#23384;&#29942;&#39048;&#20197;&#21450;&#24555;&#36895;&#25512;&#29702;&#30340;&#38656;&#27714;&#65292;&#24314;&#31435;&#19968;&#20010;&#33021;&#22815;&#36805;&#36895;&#20934;&#30830;&#23450;&#20301;&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#21452;&#37325;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#35786;&#26029;&#27169;&#22411;--DTAAD&#12290;&#25105;&#20204;&#30340;&#25972;&#20307;&#27169;&#22411;&#26159;&#19968;&#20010;&#38598;&#25104;&#35774;&#35745;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;AR&#65289;&#19982;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#24182;&#24341;&#20837;&#20102;&#32553;&#25918;&#26041;&#27861;&#21644;&#21453;&#39304;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#21644;&#25193;&#22823;&#30456;&#20851;&#24046;&#24322;&#12290;&#25105;&#20204;&#26500;&#24314;&#30340;&#21452;&#37325;TCN-Attention&#32593;&#32476;&#65288;DTA&#65289;&#22312;&#22522;&#20934;&#23454;&#39564;&#20013;&#20165;&#20351;&#29992;&#20102;&#21333;&#23618;Transformer&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection techniques enable effective anomaly detection and diagnosis in multi-variate time series data, which are of major significance for today's industrial applications. However, establishing an anomaly detection system that can be rapidly and accurately located is a challenging problem due to the lack of outlier tags, the high dimensional complexity of the data, memory bottlenecks in the actual hardware, and the need for fast reasoning. We have proposed an anomaly detection and diagnosis model -- DTAAD in this paper, based on Transformer, and Dual Temporal Convolutional Network(TCN). Our overall model will be an integrated design in which autoregressive model(AR) combines autoencoder(AE) structures, and scaling methods and feedback mechanisms are introduced to improve prediction accuracy and expand correlation differences. Constructed by us, the Dual TCN-Attention Network (DTA) only uses a single layer of Transformer encoder in our baseline experiment, that belongs to an u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20048;&#35266;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22312;Stochastically Extended Adversarial (SEA)&#27169;&#22411;&#20013;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#23545;&#20110;&#20984;&#21644;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20854;&#36951;&#25022;&#30028;&#38480;&#20026;O(sqrt(&#963;_{1:T}^2) + sqrt(&#931;_{1:T}^2))&#65292;&#23545;&#20110;&#24378;&#20984;&#21644;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20854;&#30028;&#38480;&#20026;O(sqrt(&#963;_{\max}^2) + sqrt(&#931;_{\max}^2))&#12290;</title><link>http://arxiv.org/abs/2302.04552</link><description>&lt;p&gt;
&#20048;&#35266;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#29992;&#20110;&#36830;&#25509;&#38543;&#26426;&#24615;&#21644;&#23545;&#25239;&#24615;&#22312;&#32447;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization. (arXiv:2302.04552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20048;&#35266;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22312;Stochastically Extended Adversarial (SEA)&#27169;&#22411;&#20013;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#23545;&#20110;&#20984;&#21644;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20854;&#36951;&#25022;&#30028;&#38480;&#20026;O(sqrt(&#963;_{1:T}^2) + sqrt(&#931;_{1:T}^2))&#65292;&#23545;&#20110;&#24378;&#20984;&#21644;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20854;&#30028;&#38480;&#20026;O(sqrt(&#963;_{\max}^2) + sqrt(&#931;_{\max}^2))&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sachs&#31561;&#20154;&#20171;&#32461;&#20102;Stochastically Extended Adversarial (SEA)&#27169;&#22411;&#65292;&#20316;&#20026;&#38543;&#26426;&#24615;&#21644;&#23545;&#25239;&#24615;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#25554;&#20540;&#26041;&#27861;&#12290;&#22312;&#20809;&#28369;&#26465;&#20214;&#19979;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#20048;&#35266;&#30340;Follow-the-Regularized-Leader (FTRL)&#31639;&#27861;&#30340;&#26399;&#26395;&#36951;&#25022;&#20381;&#36182;&#20110;&#20984;&#20989;&#25968;&#30340;&#32047;&#31215;&#38543;&#26426;&#26041;&#24046;&#21644;&#32047;&#31215;&#23545;&#25239;&#21464;&#21270;&#12290;&#23545;&#20110;&#24378;&#20984;&#20989;&#25968;&#65292;&#20182;&#20204;&#20063;&#32473;&#20986;&#20102;&#22522;&#20110;&#26368;&#22823;&#38543;&#26426;&#26041;&#24046;&#21644;&#26368;&#22823;&#23545;&#25239;&#21464;&#21270;&#30340;&#31245;&#24369;&#30028;&#38480;&#12290;&#21463;&#21040;&#20182;&#20204;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20048;&#35266;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22312;SEA&#27169;&#22411;&#20013;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#23545;&#20110;&#20984;&#19988;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#21363;O(sqrt(&#963;_{1:T}^2) + sqrt(&#931;_{1:T}^2))&#65292;&#32780;&#19981;&#38656;&#35201;&#20010;&#21035;&#20989;&#25968;&#30340;&#20984;&#24615;&#35201;&#27714;&#12290;&#23545;&#20110;&#24378;&#20984;&#19988;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;O(sqrt(&#963;_{\max}^2) + sqrt(&#931;_{\max}^2))&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastically Extended Adversarial (SEA) model is introduced by Sachs et al. [2022] as an interpolation between stochastic and adversarial online convex optimization. Under the smoothness condition, they demonstrate that the expected regret of optimistic follow-the-regularized-leader (FTRL) depends on the cumulative stochastic variance $\sigma_{1:T}^2$ and the cumulative adversarial variation $\Sigma_{1:T}^2$ for convex functions. They also provide a slightly weaker bound based on the maximal stochastic variance $\sigma_{\max}^2$ and the maximal adversarial variation $\Sigma_{\max}^2$ for strongly convex functions. Inspired by their work, we investigate the theoretical guarantees of optimistic online mirror descent (OMD) for the SEA model. For convex and smooth functions, we obtain the same $\mathcal{O}(\sqrt{\sigma_{1:T}^2}+\sqrt{\Sigma_{1:T}^2})$ regret bound, without the convexity requirement of individual functions. For strongly convex and smooth functions, we establish an $\mathc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;&#25110;&#26412;&#20307;&#23545;&#40784;&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#30340;Truveta Mapper&#26694;&#26550;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#26174;&#24335;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.09767</link><description>&lt;p&gt;
Truveta Mapper&#65306;&#19968;&#20010;&#38646;&#26679;&#26412;&#26412;&#20307;&#26144;&#23556;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09767
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;&#25110;&#26412;&#20307;&#23545;&#40784;&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#30340;Truveta Mapper&#26694;&#26550;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#26174;&#24335;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;(Ontology Matching, OM)&#25110;&#26412;&#20307;&#23545;&#40784;(Ontology Alignment, OA)&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#12290;&#23558;&#26412;&#20307;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#22312;&#28304;&#26412;&#20307;&#22270;&#20013;&#30340;&#33410;&#28857;&#21040;&#30446;&#26631;&#26412;&#20307;&#22270;&#20013;&#30340;&#36335;&#24452;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#25152;&#25552;&#20986;&#30340;Truveta Mapper (TM)&#26694;&#26550;&#21033;&#29992;&#22810;&#20219;&#21153;&#24207;&#21015;&#21040;&#24207;&#21015;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#22810;&#20219;&#21153;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#38544;&#21547;&#22320;&#23398;&#20064;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26080;&#38656;&#20219;&#20309;&#26174;&#24335;&#30340;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;&#36825;&#20063;&#20351;&#24471;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#27169;&#22411;&#20165;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#20869;&#37096;&#26412;&#20307;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#35813;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#26631;&#20934;&#22522;&#20934;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;Edit-Similarity&#21644;MINTE+&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task. Ontologies are represented as graphs, and the translation is performed from a node in the source ontology graph to a path in the target ontology graph. The proposed framework, Truveta Mapper (TM), leverages a multi-task sequence-to-sequence transformer model to perform alignment across multiple ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables the model to implicitly learn the relationship between different ontologies via transfer-learning without requiring any explicit cross-ontology manually labeled data. This also enables the formulated framework to outperform existing solutions for both runtime latency and alignment quality. The model is pre-trained and fine-tuned only on publicly available text corpus and inner-ontologies data. The proposed solution outperforms state-of-the-art approaches, Edit-Similari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#40657;&#30418;&#25915;&#20987;&#20113;&#26381;&#21153;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#30452;&#25509;&#24212;&#29992;&#20110;&#20113;API&#32780;&#19981;&#26159;&#26412;&#22320;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#21033;&#29992;&#36127;&#36733;&#24179;&#34913;&#23454;&#29616;&#20102;&#25915;&#20987;&#26102;&#38388;&#30340;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2210.16371</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40657;&#30418;&#25915;&#20987;&#20113;&#26381;&#21153;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Distributed Black-box Attack against Image Classification Cloud Services. (arXiv:2210.16371v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#40657;&#30418;&#25915;&#20987;&#20113;&#26381;&#21153;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#30452;&#25509;&#24212;&#29992;&#20110;&#20113;API&#32780;&#19981;&#26159;&#26412;&#22320;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#21033;&#29992;&#36127;&#36733;&#24179;&#34913;&#23454;&#29616;&#20102;&#25915;&#20987;&#26102;&#38388;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#21487;&#20197;&#27450;&#39575;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#22312;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#32467;&#26500;&#21644;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#21578;&#20102;&#36229;&#36807;95%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#26597;&#35810;&#27425;&#25968;&#23569;&#20110;1000&#27425;&#12290;&#28982;&#21518;&#20135;&#29983;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#40657;&#30418;&#25915;&#20987;&#26159;&#21542;&#24050;&#32463;&#25104;&#20026;&#20381;&#36182;&#20113;API&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#30495;&#27491;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#25104;&#21151;&#29575;&#21644;&#20943;&#23569;&#26597;&#35810;&#27425;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#40657;&#30418;&#25915;&#20987;&#20113;API&#32780;&#35328;&#65292;&#25915;&#20987;&#25152;&#38656;&#30340;&#26102;&#38388;&#20063;&#26159;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#23558;&#40657;&#30418;&#25915;&#20987;&#30452;&#25509;&#24212;&#29992;&#20110;&#20113;API&#65292;&#32780;&#19981;&#26159;&#26412;&#22320;&#27169;&#22411;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#22312;&#22270;&#20687;&#32534;&#30721;&#21644;&#39044;&#22788;&#29702;&#20043;&#21069;&#24212;&#29992;&#25200;&#21160;&#36896;&#25104;&#30340;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36127;&#36733;&#24179;&#34913;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#40657;&#30418;&#25915;&#20987;&#65292;&#21487;&#20197;&#23558;&#25915;&#20987;&#26102;&#38388;&#32553;&#30701;&#32422;&#20116;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box adversarial attacks can fool image classifiers into misclassifying images without requiring access to model structure and weights. Recent studies have reported attack success rates of over 95% with less than 1,000 queries. The question then arises of whether black-box attacks have become a real threat against IoT devices that rely on cloud APIs to achieve image classification. To shed some light on this, note that prior research has primarily focused on increasing the success rate and reducing the number of queries. However, another crucial factor for black-box attacks against cloud APIs is the time required to perform the attack. This paper applies black-box attacks directly to cloud APIs rather than to local models, thereby avoiding mistakes made in prior research that applied the perturbation before image encoding and pre-processing. Further, we exploit load balancing to enable distributed black-box attacks that can reduce the attack time by a factor of about five for both
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#29289;&#20307;&#22312;&#22270;&#20687;&#20013;&#30340;&#20301;&#32622;&#65292;&#24182;&#23558;&#35270;&#35273;&#30456;&#20851;&#37096;&#20998;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#20462;&#25913;&#65292;&#20351;&#27169;&#22411;&#29420;&#29305;&#22320;&#23398;&#20064;&#20102;&#35821;&#20041;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#25351;&#26631;&#34913;&#37327;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.09996</link><description>&lt;p&gt;
&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24863;&#30693;&#20998;&#32452;
&lt;/p&gt;
&lt;p&gt;
Perceptual Grouping in Contrastive Vision-Language Models. (arXiv:2210.09996v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#29289;&#20307;&#22312;&#22270;&#20687;&#20013;&#30340;&#20301;&#32622;&#65292;&#24182;&#23558;&#35270;&#35273;&#30456;&#20851;&#37096;&#20998;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#20462;&#25913;&#65292;&#20351;&#27169;&#22411;&#29420;&#29305;&#22320;&#23398;&#20064;&#20102;&#35821;&#20041;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#25351;&#26631;&#34913;&#37327;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#38646;&#26679;&#26412;&#22270;&#20687;&#35782;&#21035;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20102;&#39640;&#24230;&#35821;&#20041;&#20449;&#24687;&#30340;&#36890;&#29992;&#35270;&#35273;&#34920;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30701;&#35821;&#36827;&#34892;&#20219;&#24847;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#22270;&#20687;&#19981;&#20165;&#20165;&#26159;&#29702;&#35299;&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#20869;&#23481;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#20102;&#35299;&#36825;&#20123;&#20869;&#23481;&#25152;&#22312;&#30340;&#20301;&#32622;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#29702;&#35299;&#29289;&#20307;&#22312;&#22270;&#20687;&#20013;&#30340;&#20301;&#32622;&#65292;&#24182;&#23558;&#35270;&#35273;&#30456;&#20851;&#37096;&#20998;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#21644;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#30340;&#29616;&#20195;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#22312;&#26377;&#38480;&#30340;&#30446;&#26631;&#23450;&#20301;&#20449;&#24687;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#23567;&#30340;&#20462;&#25913;&#65292;&#20351;&#27169;&#22411;&#29420;&#29305;&#22320;&#23398;&#20064;&#20102;&#35821;&#20041;&#21644;&#31354;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#38646;&#26679;&#26412;&#22270;&#20687;&#35782;&#21035;&#12289;&#26080;&#30417;&#30563;&#33258;&#19979;&#32780;&#19978;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#35821;&#20041;&#20998;&#21106;&#20197;&#21450;&#40065;&#26834;&#30340;&#30446;&#26631;&#23450;&#20301;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in zero-shot image recognition suggest that vision-language models learn generic visual representations with a high degree of semantic information that may be arbitrarily probed with natural language phrases. Understanding an image, however, is not just about understanding what content resides within an image, but importantly, where that content resides. In this work we examine how well vision-language models are able to understand where objects reside within an image and group together visually related parts of the imagery. We demonstrate how contemporary vision and language representation learning models based on contrastive losses and large web-based data capture limited object localization information. We propose a minimal set of modifications that results in models that uniquely learn both semantic and spatial information. We measure this performance in terms of zero-shot image recognition, unsupervised bottom-up and top-down semantic segmentations, as well as robu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20998;&#20139;&#30340;&#20020;&#24202;&#35821;&#38899;&#35760;&#24405;&#30340;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;&#65292;&#21457;&#29616;&#39118;&#38505;&#19982;&#25628;&#32034;&#31354;&#38388;&#22823;&#23567;&#21644;&#35821;&#38899;&#35760;&#24405;&#30340;&#24615;&#36136;&#26377;&#20851;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#21487;&#20197;&#37325;&#26032;&#35782;&#21035;&#29305;&#23450;&#21442;&#19982;&#32773;&#12290;</title><link>http://arxiv.org/abs/2210.09975</link><description>&lt;p&gt;
&#20998;&#20139;&#30340;&#20020;&#24202;&#35821;&#38899;&#35760;&#24405;&#30340;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Risk of re-identification for shared clinical speech recordings. (arXiv:2210.09975v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20998;&#20139;&#30340;&#20020;&#24202;&#35821;&#38899;&#35760;&#24405;&#30340;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;&#65292;&#21457;&#29616;&#39118;&#38505;&#19982;&#25628;&#32034;&#31354;&#38388;&#22823;&#23567;&#21644;&#35821;&#38899;&#35760;&#24405;&#30340;&#24615;&#36136;&#26377;&#20851;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#21487;&#20197;&#37325;&#26032;&#35782;&#21035;&#29305;&#23450;&#21442;&#19982;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#21033;&#29992;&#22522;&#20110;&#35821;&#38899;&#30340;&#24037;&#20855;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#12289;&#32463;&#36807;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#12290;&#30001;&#20110;&#21046;&#20316;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#26114;&#36149;&#65292;&#22240;&#27492;&#23545;&#25968;&#25454;&#20849;&#20139;&#20135;&#29983;&#20102;&#22686;&#21152;&#30340;&#20852;&#36259;&#12290;&#30001;&#20110;&#35821;&#38899;&#21487;&#33021;&#35782;&#21035;&#35828;&#35805;&#32773;&#65288;&#21363;&#22768;&#32441;&#65289;&#65292;&#20998;&#20139;&#24405;&#38899;&#24341;&#36215;&#20102;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#65292;&#30740;&#31350;&#20102;&#26080;&#38656;&#21442;&#32771;&#20154;&#21475;&#32479;&#35745;&#25110;&#20803;&#25968;&#25454;&#30340;&#35821;&#38899;&#35760;&#24405;&#30340;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#39118;&#38505;&#19982;&#23545;&#25163;&#24517;&#39035;&#32771;&#34385;&#30340;&#27604;&#36739;&#27425;&#25968;&#65288;&#21363;&#25628;&#32034;&#31354;&#38388;&#65289;&#21576;&#21453;&#30456;&#20851;&#20851;&#31995;&#12290;&#23545;&#20110;&#36739;&#23567;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#39118;&#38505;&#36739;&#39640;&#65292;&#20294;&#38543;&#30528;&#25628;&#32034;&#31354;&#38388;&#30340;&#22686;&#22823;&#65288;&lt;1&#215;10^6&#27425;&#27604;&#36739;&#65289;&#65292;&#39118;&#38505;&#19979;&#38477;&#65288;&#31934;&#30830;&#24230;&gt;0.85&#65307; &gt;3&#215;10^6&#27425;&#27604;&#36739;&#26102;&#65292;&#31934;&#30830;&#24230;&lt;0.5&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#38899;&#35760;&#24405;&#30340;&#24615;&#36136;&#22914;&#20309;&#24433;&#21709;&#37325;&#26032;&#35782;&#21035;&#30340;&#39118;&#38505;&#65292;&#38750;&#36830;&#32493;&#35821;&#38899;&#65288;&#20363;&#22914;&#38899;&#32032;&#24310;&#38271;&#65289;&#26356;&#38590;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35828;&#35805;&#20154;&#35782;&#21035;&#31995;&#32479;&#21487;&#20197;&#29992;&#20110;&#37325;&#26032;&#35782;&#21035;&#29305;&#23450;&#21442;&#19982;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large, curated datasets are required to leverage speech-based tools in healthcare. These are costly to produce, resulting in increased interest in data sharing. As speech can potentially identify speakers (i.e., voiceprints), sharing recordings raises privacy concerns. We examine the re-identification risk for speech recordings, without reference to demographic or metadata, using a state-of-the-art speaker recognition system. We demonstrate that the risk is inversely related to the number of comparisons an adversary must consider, i.e., the search space. Risk is high for a small search space but drops as the search space grows ($precision &gt;0.85$ for $&lt;1*10^{6}$ comparisons, $precision &lt;0.5$ for $&gt;3*10^{6}$ comparisons). Next, we show that the nature of a speech recording influences re-identification risk, with non-connected speech (e.g., vowel prolongation) being harder to identify. Our findings suggest that speaker recognition systems can be used to re-identify participants in specifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;ISEE.U&#65292;&#29992;&#20110;&#19981;&#21487;&#39044;&#27979;&#30446;&#26631;&#30340;&#23450;&#20301;&#65292;&#26080;&#38656;&#21442;&#25968;&#35843;&#25972;&#19988;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#23616;&#37096;&#20272;&#35745;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#65292;&#27599;&#20010;&#33410;&#28857;&#35745;&#31639;&#26368;&#22823;&#21270;&#25972;&#20307;&#30446;&#26631;&#20301;&#32622;&#31934;&#24230;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#30446;&#26631;&#36816;&#21160;&#19981;&#35268;&#24459;&#26102;&#34920;&#29616;&#20248;&#31168;&#19988;&#35745;&#31639;&#26102;&#38388;&#26497;&#23569;&#12290;</title><link>http://arxiv.org/abs/2210.09107</link><description>&lt;p&gt;
ISEE.U&#65306;&#20855;&#26377;&#19981;&#21487;&#39044;&#27979;&#30446;&#26631;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20027;&#21160;&#30446;&#26631;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
ISEE.U: Distributed online active target localization with unpredictable targets. (arXiv:2210.09107v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;ISEE.U&#65292;&#29992;&#20110;&#19981;&#21487;&#39044;&#27979;&#30446;&#26631;&#30340;&#23450;&#20301;&#65292;&#26080;&#38656;&#21442;&#25968;&#35843;&#25972;&#19988;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#23616;&#37096;&#20272;&#35745;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#65292;&#27599;&#20010;&#33410;&#28857;&#35745;&#31639;&#26368;&#22823;&#21270;&#25972;&#20307;&#30446;&#26631;&#20301;&#32622;&#31934;&#24230;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#30446;&#26631;&#36816;&#21160;&#19981;&#35268;&#24459;&#26102;&#34920;&#29616;&#20248;&#31168;&#19988;&#35745;&#31639;&#26102;&#38388;&#26497;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;ISEE.U&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#33410;&#28857;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#12289;&#31616;&#21333;&#21644;&#24555;&#36895;&#30340;&#35745;&#31639;&#26469;&#23454;&#29616;&#30446;&#26631;&#23450;&#20301;&#65292;&#26080;&#38656;&#35843;&#25972;&#21442;&#25968;&#65292;&#19988;&#27599;&#20010;&#33410;&#28857;&#23545;&#30446;&#26631;&#20301;&#32622;&#30340;&#20272;&#35745;&#26399;&#26395;&#28176;&#36827;&#22320;&#31561;&#20110;&#20013;&#24515;&#21270;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#12290;ISEE.U&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#22122;&#22768;&#36317;&#31163;&#65292;&#23547;&#25214;&#26368;&#22823;&#21270;&#23450;&#20301;&#31934;&#24230;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#19981;&#23545;&#30446;&#26631;&#21160;&#24577;&#20570;&#20986;&#29305;&#23450;&#20551;&#35774;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38754;&#23545;&#19981;&#21487;&#39044;&#27979;&#30340;&#30446;&#26631;&#26102;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#27599;&#20010;&#33410;&#28857;&#36890;&#36807;&#23616;&#37096;&#20272;&#35745;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#35745;&#31639;&#26368;&#22823;&#21270;&#25972;&#20307;&#30446;&#26631;&#20301;&#32622;&#31934;&#24230;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#30446;&#26631;&#36816;&#21160;&#19981;&#36981;&#24490;&#39044;&#23450;&#36712;&#36857;&#26102;&#34920;&#29616;&#20248;&#31168;&#65292;&#29978;&#33267;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#36816;&#34892;&#22312;&#21333;&#20010;&#20013;&#22830;CPU&#26102;&#65292;&#35745;&#31639;&#26102;&#38388;&#20165;&#20026;&#20854;100&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses target localization with an online active learning algorithm defined by distributed, simple and fast computations at each node, with no parameters to tune and where the estimate of the target position at each agent is asymptotically equal in expectation to the centralized maximum-likelihood estimator. ISEE.U takes noisy distances at each agent and finds a control that maximizes localization accuracy. We do not assume specific target dynamics and, thus, our method is robust when facing unpredictable targets. Each agent computes the control that maximizes overall target position accuracy via a local estimate of the Fisher Information Matrix. We compared the proposed method with a state of the art algorithm outperforming it when the target movements do not follow a prescribed trajectory, with x100 less computation time, even when our method is running in one central CPU.
&lt;/p&gt;</description></item><item><title>CrowdGuard&#26159;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#25928;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26426;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#23545;&#20010;&#21035;&#27169;&#22411;&#30340;&#21453;&#39304;&#20998;&#26512;&#34892;&#20026;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2210.07714</link><description>&lt;p&gt;
CrowdGuard&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32852;&#37030;&#21518;&#38376;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CrowdGuard: Federated Backdoor Detection in Federated Learning. (arXiv:2210.07714v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07714
&lt;/p&gt;
&lt;p&gt;
CrowdGuard&#26159;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#25928;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26426;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#23545;&#20010;&#21035;&#27169;&#22411;&#30340;&#21453;&#39304;&#20998;&#26512;&#34892;&#20026;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#20998;&#20139;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#65288;&#25110;&#26377;&#38024;&#23545;&#24615;&#30340;&#27602;&#21270;&#65289;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#26159;&#30001;&#24694;&#24847;&#23458;&#25143;&#31471;&#21457;&#36215;&#30340;&#65292;&#20182;&#20204;&#35797;&#22270;&#36890;&#36807;&#24341;&#20837;&#29305;&#23450;&#34892;&#20026;&#21040;&#23398;&#20064;&#27169;&#22411;&#20013;&#26469;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#65292;&#36825;&#20123;&#34892;&#20026;&#21487;&#20197;&#30001;&#31934;&#24515;&#35774;&#35745;&#30340;&#36755;&#20837;&#35302;&#21457;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65306;&#23427;&#20204;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25110;&#32773;&#30001;&#20110;&#25490;&#38500;&#33391;&#24615;&#27169;&#22411;&#25110;&#28155;&#21152;&#22122;&#38899;&#32780;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#31934;&#24230;&#65292;&#26131;&#21463;&#21040;&#20855;&#26377;&#36866;&#24212;&#24615;&#38450;&#24481;&#24847;&#35782;&#30340;&#23545;&#25163;&#30340;&#25915;&#20987;&#65292;&#25110;&#35201;&#27714;&#26381;&#21153;&#22120;&#35775;&#38382;&#26412;&#22320;&#27169;&#22411;&#65292;&#20174;&#32780;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#25512;&#26029;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26426;&#21046;CrowdGuard&#65292;&#23427;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#23427;&#21033;&#29992;&#23458;&#25143;&#23545;&#20010;&#21035;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#20998;&#26512;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a promising approach enabling multiple clients to train Deep Neural Networks (DNNs) collaboratively without sharing their local training data. However, FL is susceptible to backdoor (or targeted poisoning) attacks. These attacks are initiated by malicious clients who seek to compromise the learning process by introducing specific behaviors into the learned model that can be triggered by carefully crafted inputs. Existing FL safeguards have various limitations: They are restricted to specific data distributions or reduce the global model accuracy due to excluding benign models or adding noise, are vulnerable to adaptive defense-aware adversaries, or require the server to access local models, allowing data inference attacks.  This paper presents a novel defense mechanism, CrowdGuard, that effectively mitigates backdoor attacks in FL and overcomes the deficiencies of existing techniques. It leverages clients' feedback on individual models, analyzes the behavior 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24179;&#34913;&#20256;&#25773;&#65288;EP&#65289;&#36827;&#34892;&#24207;&#21015;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;EP&#26159;&#19968;&#31181;&#26356;&#31526;&#21512;&#29983;&#29289;&#21487;&#20449;&#24615;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;&#25991;&#20013;&#21033;&#29992;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#36827;&#23637;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;EP&#30340;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#21160;&#24577;&#36755;&#20837;&#30340;&#38382;&#39064;&#65292;&#20026;&#22797;&#26434;&#24207;&#21015;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2209.09626</link><description>&lt;p&gt;
&#20351;&#29992;&#24179;&#34913;&#20256;&#25773;&#30340;&#24207;&#21015;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sequence Learning Using Equilibrium Propagation. (arXiv:2209.09626v4 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24179;&#34913;&#20256;&#25773;&#65288;EP&#65289;&#36827;&#34892;&#24207;&#21015;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;EP&#26159;&#19968;&#31181;&#26356;&#31526;&#21512;&#29983;&#29289;&#21487;&#20449;&#24615;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;&#25991;&#20013;&#21033;&#29992;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#36827;&#23637;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;EP&#30340;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#21160;&#24577;&#36755;&#20837;&#30340;&#38382;&#39064;&#65292;&#20026;&#22797;&#26434;&#24207;&#21015;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#34913;&#20256;&#25773;&#65288;EP&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#19988;&#26356;&#31526;&#21512;&#29983;&#29289;&#21487;&#20449;&#24615;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#26367;&#20195;&#20256;&#32479;&#30340;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#12290;EP&#30340;&#26377;&#25928;&#24615;&#28304;&#20110;&#23427;&#20165;&#20381;&#36182;&#20110;&#23616;&#37096;&#35745;&#31639;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#30340;&#20004;&#20010;&#38454;&#27573;&#20013;&#21482;&#38656;&#35201;&#19968;&#31181;&#35745;&#31639;&#21333;&#20803;&#65292;&#22240;&#27492;&#22312;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31561;&#39046;&#22495;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;EP&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#21463;&#33021;&#37327;&#20989;&#25968;&#25511;&#21046;&#65292;&#27169;&#22411;&#30340;&#20869;&#37096;&#29366;&#24577;&#38543;&#20043;&#25910;&#25947;&#21040;&#31283;&#23450;&#29366;&#24577;&#65292;&#36981;&#24490;&#30001;&#21516;&#19968;&#20989;&#25968;&#23450;&#20041;&#30340;&#29366;&#24577;&#36716;&#25442;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#23450;&#20041;&#65292;EP&#35201;&#27714;&#27169;&#22411;&#30340;&#36755;&#20837;&#65288;&#25910;&#25947;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#35757;&#32451;&#30340;&#20004;&#20010;&#38454;&#27573;&#20013;&#37117;&#26159;&#38745;&#24577;&#30340;&#12290;&#22240;&#27492;&#65292;&#19981;&#21487;&#33021;&#20351;&#29992;&#31867;&#20284;LSTM&#25110;GRU&#30340;&#26550;&#26500;&#35774;&#35745;&#22522;&#20110;EP&#30340;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;&#12290;&#26412;&#25991;&#21033;&#29992;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36827;&#19968;&#27493;&#29702;&#35299;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#24182;&#20026;&#22797;&#26434;&#24207;&#21015;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equilibrium Propagation (EP) is a powerful and more bio-plausible alternative to conventional learning frameworks such as backpropagation. The effectiveness of EP stems from the fact that it relies only on local computations and requires solely one kind of computational unit during both of its training phases, thereby enabling greater applicability in domains such as bio-inspired neuromorphic computing. The dynamics of the model in EP is governed by an energy function and the internal states of the model consequently converge to a steady state following the state transition rules defined by the same. However, by definition, EP requires the input to the model (a convergent RNN) to be static in both the phases of training. Thus it is not possible to design a model for sequence classification using EP with an LSTM or GRU like architecture. In this paper, we leverage recent developments in modern hopfield networks to further understand energy based models and develop solutions for complex 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#21644;&#27969;&#24418;&#23398;&#20064;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#26469;&#21457;&#29616;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#20445;&#23432;&#23450;&#24459;&#12290;&#36890;&#36807;&#27979;&#35797;&#22312;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#19978;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#30830;&#23450;&#20445;&#23432;&#23450;&#24459;&#30340;&#25968;&#37327;&#24182;&#25552;&#21462;&#20854;&#20540;&#65292;&#20026;&#20998;&#26512;&#31995;&#32479;&#21160;&#21147;&#23398;&#21644;&#26500;&#24314;&#31283;&#23450;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#38752;&#32780;&#35299;&#37322;&#24615;&#24378;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.14995</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#21644;&#27969;&#24418;&#23398;&#20064;&#21457;&#29616;&#20445;&#23432;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Discovering Conservation Laws using Optimal Transport and Manifold Learning. (arXiv:2208.14995v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#21644;&#27969;&#24418;&#23398;&#20064;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#26469;&#21457;&#29616;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#20445;&#23432;&#23450;&#24459;&#12290;&#36890;&#36807;&#27979;&#35797;&#22312;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#19978;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#30830;&#23450;&#20445;&#23432;&#23450;&#24459;&#30340;&#25968;&#37327;&#24182;&#25552;&#21462;&#20854;&#20540;&#65292;&#20026;&#20998;&#26512;&#31995;&#32479;&#21160;&#21147;&#23398;&#21644;&#26500;&#24314;&#31283;&#23450;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#38752;&#32780;&#35299;&#37322;&#24615;&#24378;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#23432;&#23450;&#24459;&#26159;&#29702;&#35299;&#12289;&#34920;&#24449;&#21644;&#24314;&#27169;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#20851;&#38190;&#29702;&#35770;&#21644;&#23454;&#38469;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#22797;&#26434;&#31995;&#32479;&#65292;&#24456;&#38590;&#30830;&#23450;&#30456;&#24212;&#30340;&#20445;&#23432;&#37327;&#65292;&#20174;&#32780;&#38590;&#20197;&#20998;&#26512;&#23427;&#20204;&#30340;&#21160;&#21147;&#23398;&#21644;&#26500;&#24314;&#31283;&#23450;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#24403;&#21069;&#21457;&#29616;&#20445;&#23432;&#23450;&#24459;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#35814;&#32454;&#30340;&#21160;&#21147;&#23398;&#20449;&#24687;&#25110;&#20381;&#36182;&#20110;&#40657;&#30418;&#21442;&#25968;&#21270;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#27969;&#24418;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#26041;&#27861;&#26469;&#21457;&#29616;&#20445;&#23432;&#37327;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29289;&#29702;&#31995;&#32479;&#19978;&#27979;&#35797;&#20102;&#36825;&#31181;&#26032;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#30830;&#23450;&#20445;&#23432;&#37327;&#30340;&#25968;&#37327;&#24182;&#25552;&#21462;&#23427;&#20204;&#30340;&#20540;&#12290;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;&#27969;&#24418;&#23398;&#20064;&#24037;&#20855;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#25509;&#30340;&#20960;&#20309;&#26041;&#27861;&#26469;&#30830;&#23450;&#20445;&#23432;&#23450;&#24459;&#65292;&#26082;&#31283;&#20581;&#21448;&#21487;&#35299;&#37322;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#21160;&#21147;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conservation laws are key theoretical and practical tools for understanding, characterizing, and modeling nonlinear dynamical systems. However, for many complex systems, the corresponding conserved quantities are difficult to identify, making it hard to analyze their dynamics and build stable predictive models. Current approaches for discovering conservation laws often depend on detailed dynamical information or rely on black box parametric deep learning methods. We instead reformulate this task as a manifold learning problem and propose a non-parametric approach for discovering conserved quantities. We test this new approach on a variety of physical systems and demonstrate that our method is able to both identify the number of conserved quantities and extract their values. Using tools from optimal transport theory and manifold learning, our proposed method provides a direct geometric approach to identifying conservation laws that is both robust and interpretable without requiring an e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Quantile Risk Minimization&#65288;QRM&#65289;&#26041;&#27861;&#23454;&#29616;&#21487;&#33021;&#30340;&#39046;&#22495;&#27867;&#21270;&#30340;&#27010;&#29575;&#24615;&#26694;&#26550;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#22120;&#39118;&#38505;&#20998;&#24067;&#22312;&#19981;&#21516;&#39046;&#22495;&#19978;&#30340;&#20998;&#20301;&#25968;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22312;&#27979;&#35797;&#26102;&#20197;&#39640;&#27010;&#29575;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2207.09944</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#20301;&#25968;&#39118;&#38505;&#26368;&#23567;&#21270;&#23454;&#29616;&#21487;&#33021;&#30340;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Probable Domain Generalization via Quantile Risk Minimization. (arXiv:2207.09944v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Quantile Risk Minimization&#65288;QRM&#65289;&#26041;&#27861;&#23454;&#29616;&#21487;&#33021;&#30340;&#39046;&#22495;&#27867;&#21270;&#30340;&#27010;&#29575;&#24615;&#26694;&#26550;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#22120;&#39118;&#38505;&#20998;&#24067;&#22312;&#19981;&#21516;&#39046;&#22495;&#19978;&#30340;&#20998;&#20301;&#25968;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22312;&#27979;&#35797;&#26102;&#20197;&#39640;&#27010;&#29575;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#30456;&#20851;&#35757;&#32451;&#39046;&#22495;&#30340;&#25968;&#25454;&#65292;&#23547;&#25214;&#22312;&#26410;&#35265;&#27979;&#35797;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#22120;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;DG&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#23545;&#21487;&#33021;&#30340;&#39046;&#22495;&#38598;&#21512;&#36827;&#34892;&#24179;&#22343;&#25110;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#22120;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#32780;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#22120;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#29575;&#24615;&#26694;&#26550;&#26469;&#36827;&#34892;DG&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#20197;&#39640;&#27010;&#29575;&#34920;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#21464;&#21270;&#24212;&#35813;&#33021;&#22815;&#21578;&#35785;&#25105;&#20204;&#27979;&#35797;&#26102;&#21487;&#33021;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35757;&#32451;&#21644;&#27979;&#35797;&#39046;&#22495;&#26126;&#30830;&#22320;&#35270;&#20026;&#20174;&#21516;&#19968;&#22522;&#30784;&#20803;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#33021;&#30340;DG&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Quantile Risk Minimization&#65288;QRM&#65289;&#30340;&#26032;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#22120;&#39118;&#38505;&#20998;&#24067;&#22312;&#39046;&#22495;&#19978;&#30340;&#945;-&#20998;&#20301;&#25968;&#65292;QRM&#21487;&#20197;&#23454;&#29616;&#27010;&#29575;&#19978;&#30340;DG&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) seeks predictors which perform well on unseen test distributions by leveraging data drawn from multiple related training distributions or domains. To achieve this, DG is commonly formulated as an average- or worst-case problem over the set of possible domains. However, predictors that perform well on average lack robustness while predictors that perform well in the worst case tend to be overly-conservative. To address this, we propose a new probabilistic framework for DG where the goal is to learn predictors that perform well with high probability. Our key idea is that distribution shifts seen during training should inform us of probable shifts at test time, which we realize by explicitly relating training and test domains as draws from the same underlying meta-distribution. To achieve probable DG, we propose a new optimization problem called Quantile Risk Minimization (QRM). By minimizing the $\alpha$-quantile of predictor's risk distribution over domains, Q
&lt;/p&gt;</description></item><item><title>AceIRL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#22312;&#19981;&#38656;&#35201;&#29615;&#22659;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30830;&#23450;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#25214;&#21040;&#20391;&#37325;&#20110;&#29615;&#22659;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#21306;&#22495;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2207.08645</link><description>&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#21160;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Active Exploration for Inverse Reinforcement Learning. (arXiv:2207.08645v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08645
&lt;/p&gt;
&lt;p&gt;
AceIRL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#22312;&#19981;&#38656;&#35201;&#29615;&#22659;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30830;&#23450;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#25214;&#21040;&#20391;&#37325;&#20110;&#29615;&#22659;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#21306;&#22495;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#30340;&#24378;&#22823;&#33539;&#24335;&#12290;&#35768;&#22810;IRL&#31639;&#27861;&#38656;&#35201;&#24050;&#30693;&#30340;&#36716;&#31227;&#27169;&#22411;&#65292;&#26377;&#26102;&#29978;&#33267;&#38656;&#35201;&#24050;&#30693;&#30340;&#19987;&#23478;&#31574;&#30053;&#65292;&#25110;&#32773;&#33267;&#23569;&#38656;&#35201;&#35775;&#38382;&#29983;&#25104;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#20551;&#35774;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#22826;&#24378;&#20102;&#65292;&#22240;&#20026;&#21482;&#33021;&#36890;&#36807;&#39034;&#24207;&#20132;&#20114;&#26469;&#35775;&#38382;&#29615;&#22659;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;IRL&#31639;&#27861;&#65306;&#20027;&#21160;&#25506;&#32034;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;AceIRL&#65289;&#65292;&#23427;&#20027;&#21160;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#21644;&#19987;&#23478;&#31574;&#30053;&#65292;&#24555;&#36895;&#23398;&#20064;&#19987;&#23478;&#30340;&#22870;&#21169;&#20989;&#25968;&#24182;&#35782;&#21035;&#20986;&#19968;&#20010;&#22909;&#30340;&#31574;&#30053;&#12290;AceIRL&#20351;&#29992;&#20808;&#21069;&#30340;&#35266;&#23519;&#32467;&#26524;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#26469;&#25429;&#25417;&#21487;&#34892;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#25214;&#21040;&#20391;&#37325;&#20110;&#29615;&#22659;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#21306;&#22495;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;AceIRL&#26159;&#31532;&#19968;&#31181;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#19988;&#19981;&#38656;&#35201;&#29615;&#22659;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#21160;IRL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a reward function from expert demonstrations. Many IRL algorithms require a known transition model and sometimes even a known expert policy, or they at least require access to a generative model. However, these assumptions are too strong for many real-world applications, where the environment can be accessed only through sequential interaction. We propose a novel IRL algorithm: Active exploration for Inverse Reinforcement Learning (AceIRL), which actively explores an unknown environment and expert policy to quickly learn the expert's reward function and identify a good policy. AceIRL uses previous observations to construct confidence intervals that capture plausible reward functions and find exploration policies that focus on the most informative regions of the environment. AceIRL is the first approach to active IRL with sample-complexity bounds that does not require a generative model of the environment. AceIRL 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRAug&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#21512;&#25104;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#26469;&#22686;&#24378;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#34701;&#21512;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2205.14900</link><description>&lt;p&gt;
FRAug: &#36890;&#36807;&#34920;&#31034;&#22686;&#24378;&#35299;&#20915;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#29305;&#24449;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FRAug: Tackling Federated Learning with Non-IID Features via Representation Augmentation. (arXiv:2205.14900v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRAug&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#21512;&#25104;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#26469;&#22686;&#24378;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#34701;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#20854;&#20013;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38598;&#20013;&#20854;&#26412;&#22320;&#25968;&#25454;&#65292;&#20174;&#32780;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#28041;&#21450;&#19981;&#21516;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36825;&#20250;&#25439;&#23475;&#23458;&#25143;&#31471;&#23545;&#26469;&#33258;&#21508;&#33258;&#25968;&#25454;&#20998;&#24067;&#30340;&#26410;&#35265;&#26679;&#26412;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#29305;&#24449;&#20559;&#31227;&#38382;&#39064;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#24067;&#65292;&#32780;&#26631;&#31614;&#20998;&#24067;&#30456;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Federated Representation Augmentation (FRAug)&#26469;&#35299;&#20915;&#36825;&#20010;&#23454;&#38469;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#21512;&#25104;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#65292;&#20197;&#22686;&#24378;&#36890;&#24120;&#36739;&#23567;&#30340;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26469;&#34701;&#21512;&#23458;&#25143;&#31471;&#20174;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#36825;&#20010;&#29983;&#25104;&#22120;&#21512;&#25104;&#23458;&#25143;&#26080;&#20851;&#30340;&#23884;&#20837;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a decentralized learning paradigm, in which multiple clients collaboratively train deep learning models without centralizing their local data, and hence preserve data privacy. Real-world applications usually involve a distribution shift across the datasets of the different clients, which hurts the generalization ability of the clients to unseen samples from their respective data distributions. In this work, we address the recently proposed feature shift problem where the clients have different feature distributions, while the label distribution is the same. We propose Federated Representation Augmentation (FRAug) to tackle this practical and challenging problem. Our approach generates synthetic client-specific samples in the embedding space to augment the usually small client datasets. For that, we train a shared generative model to fuse the clients knowledge learned from their different feature distributions. This generator synthesizes client-agnostic embedd
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#37327;&#23376;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#25918;&#22823;&#30028;&#38480;&#12290;&#39318;&#27425;&#23637;&#31034;&#20102;&#36890;&#36807;&#37327;&#23376;&#32534;&#30721;&#30340;&#32463;&#20856;&#25968;&#25454;&#38598;&#30340;&#31639;&#27861;&#25110;&#37327;&#23376;&#21551;&#21457;&#24335;&#32463;&#20856;&#37319;&#26679;&#32467;&#26524;&#33021;&#22815;&#25918;&#22823;&#24046;&#20998;&#38544;&#31169;&#12290;&#30740;&#31350;&#36824;&#35777;&#26126;&#20102;&#37327;&#23376;&#29256;&#26412;&#30340;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#36890;&#36807;&#28385;&#36275;&#28151;&#21512;&#26465;&#20214;&#30340;&#37327;&#23376;&#36890;&#36947;&#32452;&#21512;&#26469;&#25918;&#22823;&#12290;</title><link>http://arxiv.org/abs/2203.03604</link><description>&lt;p&gt;
&#37327;&#23376;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy Amplification in Quantum and Quantum-inspired Algorithms. (arXiv:2203.03604v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#37327;&#23376;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#25918;&#22823;&#30028;&#38480;&#12290;&#39318;&#27425;&#23637;&#31034;&#20102;&#36890;&#36807;&#37327;&#23376;&#32534;&#30721;&#30340;&#32463;&#20856;&#25968;&#25454;&#38598;&#30340;&#31639;&#27861;&#25110;&#37327;&#23376;&#21551;&#21457;&#24335;&#32463;&#20856;&#37319;&#26679;&#32467;&#26524;&#33021;&#22815;&#25918;&#22823;&#24046;&#20998;&#38544;&#31169;&#12290;&#30740;&#31350;&#36824;&#35777;&#26126;&#20102;&#37327;&#23376;&#29256;&#26412;&#30340;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#36890;&#36807;&#28385;&#36275;&#28151;&#21512;&#26465;&#20214;&#30340;&#37327;&#23376;&#36890;&#36947;&#32452;&#21512;&#26469;&#25918;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20026;&#22788;&#29702;&#20851;&#20110;n&#20010;&#29992;&#25143;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#30830;&#20445;&#36755;&#20986;&#23545;&#20219;&#20309;&#21333;&#20010;&#29992;&#25143;&#30340;&#20449;&#24687;&#27844;&#38706;&#26368;&#23567;&#21270;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#21644;&#36827;&#34892;&#23376;&#37319;&#26679;&#12289;&#27927;&#29260;&#12289;&#36845;&#20195;&#12289;&#28151;&#21512;&#21644;&#25193;&#25955;&#31561;&#22810;&#20010;&#36807;&#31243;&#26469;&#23454;&#29616;&#36825;&#31181;&#38544;&#31169;&#20445;&#25252;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#37327;&#23376;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#38544;&#31169;&#25918;&#22823;&#30028;&#38480;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#22312;&#37327;&#23376;&#32534;&#30721;&#30340;&#32463;&#20856;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#30340;&#31639;&#27861;&#25110;&#37327;&#23376;&#21551;&#21457;&#24335;&#32463;&#20856;&#37319;&#26679;&#30340;&#32467;&#26524;&#33021;&#22815;&#25918;&#22823;&#24046;&#20998;&#38544;&#31169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37327;&#23376;&#29256;&#26412;&#30340;&#24046;&#20998;&#38544;&#31169;&#36890;&#36807;&#28385;&#36275;&#19968;&#20123;&#28151;&#21512;&#26465;&#20214;&#65292;&#21487;&#20197;&#36890;&#36807;&#37327;&#23376;&#36890;&#36947;&#30340;&#32452;&#21512;&#26469;&#25918;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy provides a theoretical framework for processing a dataset about $n$ users, in a way that the output reveals a minimal information about any single user. Such notion of privacy is usually ensured by noise-adding mechanisms and amplified by several processes, including subsampling, shuffling, iteration, mixing and diffusion. In this work, we provide privacy amplification bounds for quantum and quantum-inspired algorithms. In particular, we show for the first time, that algorithms running on quantum encoding of a classical dataset or the outcomes of quantum-inspired classical sampling, amplify differential privacy. Moreover, we prove that a quantum version of differential privacy is amplified by the composition of quantum channels, provided that they satisfy some mixing conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#21644;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#22312;&#23616;&#37096;&#27169;&#22411;&#19979;&#36827;&#34892;&#20102;&#31561;&#20215;&#24314;&#31435;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#32467;&#26524;&#21040;&#37327;&#23376;&#39046;&#22495;&#12290;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#19979;&#25512;&#23548;&#20102;&#37327;&#23376;&#30456;&#23545;&#29109;&#30340;&#24378;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21463;&#38480;&#27979;&#37327;&#30340;&#19981;&#23545;&#31216;&#20551;&#35774;&#27979;&#35797;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#37327;&#23376;&#22810;&#26041;&#35745;&#31639;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2203.03591</link><description>&lt;p&gt;
&#37327;&#23376;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#21644;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Quantum Local Differential Privacy and Quantum Statistical Query Model. (arXiv:2203.03591v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#21644;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#22312;&#23616;&#37096;&#27169;&#22411;&#19979;&#36827;&#34892;&#20102;&#31561;&#20215;&#24314;&#31435;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#32467;&#26524;&#21040;&#37327;&#23376;&#39046;&#22495;&#12290;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#19979;&#25512;&#23548;&#20102;&#37327;&#23376;&#30456;&#23545;&#29109;&#30340;&#24378;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21463;&#38480;&#27979;&#37327;&#30340;&#19981;&#23545;&#31216;&#20551;&#35774;&#27979;&#35797;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#37327;&#23376;&#22810;&#26041;&#35745;&#31639;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#26377;&#38480;&#37327;&#23376;&#36164;&#28304;&#30340;&#23398;&#20064;&#32773;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#22312;&#24403;&#21069;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#20010;&#27169;&#22411;&#29305;&#21035;&#30456;&#20851;&#65292;&#22240;&#20026;&#21487;&#29992;&#30340;&#37327;&#23376;&#35774;&#22791;&#21463;&#21040;&#20005;&#37325;&#22122;&#22768;&#21644;&#38480;&#21046;&#37327;&#23376;&#23384;&#20648;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#30340;&#26694;&#26550;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22122;&#22768;&#21487;&#33021;&#26377;&#30410;&#20110;&#35745;&#31639;&#65292;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#32479;&#35745;&#23433;&#20840;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#21644;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#23558;&#19968;&#20010;&#33879;&#21517;&#30340;&#32463;&#20856;&#32467;&#26524;&#25512;&#24191;&#21040;&#37327;&#23376;&#35774;&#32622;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#19979;&#25512;&#23548;&#20986;&#20102;&#37327;&#23376;&#30456;&#23545;&#29109;&#30340;&#24378;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#65292;&#24182;&#23558;&#27492;&#32467;&#26524;&#24212;&#29992;&#20110;&#21463;&#38480;&#27979;&#37327;&#30340;&#19981;&#23545;&#31216;&#20551;&#35774;&#27979;&#35797;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#37327;&#23376;&#22810;&#26041;&#35745;&#31639;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum statistical queries provide a theoretical framework for investigating the computational power of a learner with limited quantum resources. This model is particularly relevant in the current context, where available quantum devices are subject to severe noise and have limited quantum memory. On the other hand, the framework of quantum differential privacy demonstrates that noise can, in some cases, benefit the computation, enhancing robustness and statistical security. In this work, we establish an equivalence between quantum statistical queries and quantum differential privacy in the local model, extending a celebrated classical result to the quantum setting. Furthermore, we derive strong data processing inequalities for the quantum relative entropy under local differential privacy and apply this result to the task of asymmetric hypothesis testing with restricted measurements. Finally, we consider the task of quantum multi-party computation under local differential privacy. As 
&lt;/p&gt;</description></item><item><title>&#33258;&#20027;&#35757;&#32451;&#26041;&#27861;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;&#20302;&#23494;&#24230;&#21306;&#22495;&#25214;&#21040;&#20915;&#31574;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#20998;&#25968;&#20316;&#20026;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#20998;&#37197;&#20266;&#26631;&#31614;&#65292;&#36845;&#20195;&#22320;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20016;&#23500;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2202.12040</link><description>&lt;p&gt;
&#33258;&#20027;&#35757;&#32451;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Self-Training: A Survey. (arXiv:2202.12040v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12040
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#35757;&#32451;&#26041;&#27861;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;&#20302;&#23494;&#24230;&#21306;&#22495;&#25214;&#21040;&#20915;&#31574;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#20998;&#25968;&#20316;&#20026;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#20998;&#37197;&#20266;&#26631;&#31614;&#65292;&#36845;&#20195;&#22320;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20016;&#23500;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#31639;&#27861;&#26088;&#22312;&#20174;&#23569;&#37327;&#26377;&#26631;&#31614;&#35266;&#27979;&#21644;&#22823;&#37327;&#26080;&#26631;&#31614;&#35266;&#27979;&#20013;&#23398;&#20064;&#39044;&#27979;&#20989;&#25968;&#12290;&#30001;&#20110;&#36825;&#20010;&#26694;&#26550;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#30456;&#20851;&#30340;&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#29616;&#26377;&#30340;&#25216;&#26415;&#20013;&#65292;&#33258;&#20027;&#35757;&#32451;&#26041;&#27861;&#22312;&#36817;&#24180;&#26469;&#26080;&#30097;&#24341;&#36215;&#20102;&#26356;&#22823;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#22312;&#20302;&#23494;&#24230;&#21306;&#22495;&#25214;&#21040;&#20915;&#31574;&#36793;&#30028;&#65292;&#32780;&#19981;&#23545;&#25968;&#25454;&#20998;&#24067;&#20316;&#20986;&#39069;&#22806;&#30340;&#20551;&#35774;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#26080;&#31526;&#21495;&#36755;&#20986;&#20998;&#25968;&#25110;&#20854;&#36793;&#30028;&#20316;&#20026;&#32622;&#20449;&#24230;&#30340;&#25351;&#26631;&#12290;&#33258;&#20027;&#35757;&#32451;&#31639;&#27861;&#30340;&#24037;&#20316;&#21407;&#29702;&#26159;&#36890;&#36807;&#32473;&#20855;&#26377;&#22823;&#20110;&#26576;&#20010;&#38408;&#20540;&#30340;&#36793;&#30028;&#30340;&#26080;&#26631;&#31614;&#35757;&#32451;&#26679;&#26412;&#20998;&#37197;&#20266;&#26631;&#31614;&#65292;&#36845;&#20195;&#22320;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20266;&#26631;&#35760;&#30340;&#31034;&#20363;&#26469;&#22686;&#24378;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19982;&#26377;&#26631;&#31614;&#35757;&#32451;&#38598;&#19968;&#36215;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised algorithms aim to learn prediction functions from a small set of labeled observations and a large set of unlabeled observations. Because this framework is relevant in many applications, they have received a lot of interest in both academia and industry. Among the existing techniques, self-training methods have undoubtedly attracted greater attention in recent years. These models are designed to find the decision boundary on low density regions without making additional assumptions about the data distribution, and use the unsigned output score of a learned classifier, or its margin, as an indicator of confidence. The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. The pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set. In this
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#20102;&#29616;&#26377;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24191;&#20041;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#12290;&#22312;&#27492;&#26694;&#26550;&#30340;&#25351;&#23548;&#19979;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#23618;&#36882;&#36827;&#30340;&#22810;&#35270;&#35282;&#32534;&#30721;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2109.02344</link><description>&lt;p&gt;
&#20449;&#24687;&#35770;&#24341;&#23548;&#30340;&#21551;&#21457;&#24335;&#28176;&#36827;&#24335;&#22810;&#35270;&#35282;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2109.02344v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.02344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#20102;&#29616;&#26377;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24191;&#20041;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#12290;&#22312;&#27492;&#26694;&#26550;&#30340;&#25351;&#23548;&#19979;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#23618;&#36882;&#36827;&#30340;&#22810;&#35270;&#35282;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#20174;&#20849;&#20139;&#19978;&#19979;&#25991;&#30340;&#22810;&#20010;&#35270;&#35282;&#20013;&#25429;&#25417;&#32508;&#21512;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30452;&#35266;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24212;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#65292;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#25104;&#23545;&#26041;&#24335;&#65292;&#20294;&#20173;&#28982;&#21487;&#25193;&#23637;&#65306;&#22312;&#23398;&#20064;&#35270;&#35282;&#20849;&#20139;&#34920;&#31034;&#26102;&#65292;&#19981;&#20250;&#36807;&#28388;&#29305;&#23450;&#20110;&#35270;&#35282;&#30340;&#22122;&#22768;&#65307;&#34394;&#20551;&#30340;&#36127;&#23545;&#65292;&#20854;&#20013;&#36127;&#39033;&#23454;&#38469;&#19978;&#19982;&#27491;&#39033;&#23646;&#20110;&#21516;&#19968;&#31867;&#65292;&#20197;&#21450;&#30495;&#23454;&#30340;&#36127;&#23545;&#34987;&#31561;&#21516;&#23545;&#24453;&#65307;&#22343;&#21248;&#22320;&#27979;&#37327;&#26415;&#35821;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21487;&#33021;&#24178;&#25200;&#20248;&#21270;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#38024;&#23545;&#24191;&#20041;&#33258;&#30417;&#30563;&#22810;&#35270;&#35282;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#36229;&#36807;&#20004;&#20010;&#35270;&#35282;&#30340;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#29616;&#26377;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#33539;&#24335;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24191;&#20041;&#22810;&#35270;&#35282;&#23398;&#20064;&#12290;&#22312;&#20854;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#20855;&#26377;&#19977;&#23618;&#36882;&#36827;&#30340;&#22810;&#35270;&#35282;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view representation learning captures comprehensive information from multiple views of a shared context. Recent works intuitively apply contrastive learning (CL) to learn representations, regarded as a pairwise manner, which is still scalable: view-specific noise is not filtered in learning view-shared representations; the fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated; and evenly measuring the similarities between terms might interfere with optimization. Importantly, few works research the theoretical framework of generalized self-supervised multi-view learning, especially for more than two views. To this end, we rethink the existing multi-view learning paradigm from the information theoretical perspective and then propose a novel information theoretical framework for generalized multi-view learning. Guided by it, we build a multi-view coding method with a three-tier progressive 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#21017;&#21270;&#26041;&#27861;&#65292;Contrast-Reg&#65292;&#36890;&#36807;&#24212;&#29992;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#26469;&#26657;&#20934;&#21644;&#25913;&#36827;&#22270;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2101.11525</link><description>&lt;p&gt;
&#26657;&#20934;&#21644;&#25913;&#36827;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Calibrating and Improving Graph Contrastive Learning. (arXiv:2101.11525v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.11525
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#21017;&#21270;&#26041;&#27861;&#65292;Contrast-Reg&#65292;&#36890;&#36807;&#24212;&#29992;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#26469;&#26657;&#20934;&#21644;&#25913;&#36827;&#22270;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30417;&#30563;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#19968;&#20123;&#23545;&#27604;&#23545;&#21487;&#33021;&#19982;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#30495;&#30456;&#30456;&#30683;&#30462;&#65292;&#22240;&#27492;&#22312;&#36825;&#20123;&#23545;&#27604;&#23545;&#19978;&#20943;&#23569;&#25439;&#22833;&#20250;&#19981;&#24076;&#26395;&#22320;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#23545;&#27604;&#23545;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#19982;&#30495;&#30456;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#23558;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#24212;&#29992;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#12290;ECE&#30340;&#20998;&#26512;&#28608;&#21457;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#21017;&#21270;&#26041;&#27861;&#65292;Contrast-Reg&#65292;&#20197;&#30830;&#20445;&#20943;&#23569;&#23545;&#27604;&#25439;&#22833;&#33021;&#22815;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20316;&#20026;&#25554;&#20214;&#24335;&#35268;&#21017;&#21270;&#22120;&#65292;Contrast-Reg&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#29616;&#26377;&#22270;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#26469;&#35777;&#26126;Contrast-Reg&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning algorithms have demonstrated remarkable success in various applications such as node classification, link prediction, and graph clustering. However, in unsupervised graph contrastive learning, some contrastive pairs may contradict the truths in downstream tasks and thus the decrease of losses on these pairs undesirably harms the performance in the downstream tasks. To assess the discrepancy between the prediction and the ground-truth in the downstream tasks for these contrastive pairs, we adapt the expected calibration error (ECE) to graph contrastive learning. The analysis of ECE motivates us to propose a novel regularization method, Contrast-Reg, to ensure that decreasing the contrastive loss leads to better performance in the downstream tasks. As a plug-in regularizer, Contrast-Reg effectively improves the performance of existing graph contrastive learning algorithms. We provide both theoretical and empirical results to demonstrate the effectiveness of Con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;MMD&#27491;&#21017;&#21270;&#30340;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#20984;&#35268;&#21010;&#29992;&#20110;&#20272;&#31639;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#35823;&#24046;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2011.05001</link><description>&lt;p&gt;
MMD&#27491;&#21017;&#21270;&#30340;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MMD-regularized Unbalanced Optimal Transport. (arXiv:2011.05001v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.05001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;MMD&#27491;&#21017;&#21270;&#30340;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#20984;&#35268;&#21010;&#29992;&#20110;&#20272;&#31639;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#35823;&#24046;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#65288;UOT&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#20351;&#29992;&#26368;&#22823;&#24179;&#22343;&#20559;&#24046;&#65288;MMD&#65289;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#36793;&#38469;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#22312;&#20110;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#30340;UOT&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;$\phi$-&#25955;&#24230;&#65288;&#20363;&#22914;KL&#65289;&#30340;&#27491;&#21017;&#21270;&#12290;MMD&#20316;&#20026;&#20114;&#34917;&#30340;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#23478;&#26063;&#20043;&#19968;&#65292;&#22312;UOT&#19978;&#30340;&#20316;&#29992;&#20284;&#20046;&#19981;&#22826;&#34987;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#65292;&#21033;&#29992;&#23427;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;MMD&#27491;&#21017;&#21270;&#30340;UOT&#65288;MMD-UOT&#65289;&#30340;&#29305;&#24615;&#12290;&#36825;&#31181;&#23545;&#20598;&#32467;&#26524;&#30340;&#19968;&#20010;&#26377;&#36259;&#32467;&#26524;&#26159;MMD-UOT&#35825;&#23548;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20063;&#23646;&#20110;IPM&#23478;&#26063;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#20984;&#35268;&#21010;&#65292;&#29992;&#20110;&#20272;&#31639;MMD-UOT&#21644;&#30456;&#24212;&#30340;&#37325;&#24515;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#20272;&#35745;&#37327;&#26159;&#19968;&#33268;&#30340;&#65292;&#32780;&#19988;&#20272;&#35745;&#35823;&#24046;&#20197;$\mathcal{O}(m^{-1/2})$&#30340;&#36895;&#29575;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the unbalanced optimal transport (UOT) problem, where the marginal constraints are enforced using Maximum Mean Discrepancy (MMD) regularization. Our study is motivated by the observation that existing works on UOT have mainly focused on regularization based on $\phi$-divergence (e.g., KL). The role of MMD, which belongs to the complementary family of integral probability metrics (IPMs), as a regularizer in the context of UOT seems to be less understood. Our main result is based on Fenchel duality, using which we are able to study the properties of MMD-regularized UOT (MMD-UOT). One interesting outcome of this duality result is that MMD-UOT induces a novel metric over measures, which again belongs to the IPM family. Further, we present finite-sample-based convex programs for estimating MMD-UOT and the corresponding barycenter. Under mild conditions, we prove that our convex-program-based estimators are consistent, and the estimation error decays at a rate $\mathcal{O}\left(m^{-
&lt;/p&gt;</description></item></channel></rss>