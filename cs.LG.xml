<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#24310;&#36831;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24310;&#36831;&#25928;&#24212;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01231</link><description>&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24310;&#36831;&#25928;&#24212;&#25581;&#31034;&#65306;&#22522;&#20110;&#26102;&#31354;&#24310;&#36831;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unveiling Delay Effects in Traffic Forecasting: A Perspective from Spatial-Temporal Delay Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#24310;&#36831;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24310;&#36831;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#20132;&#36890;&#35268;&#21010;&#21644;&#31649;&#29702;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#30740;&#31350;&#38382;&#39064;&#65292;&#20063;&#26159;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#30340;&#19968;&#20010;&#20856;&#22411;&#20363;&#23376;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#22312;&#25429;&#25417;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#26377;&#20004;&#20010;&#19981;&#21487;&#24573;&#35270;&#30340;&#38382;&#39064;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#22320;&#35299;&#20915;&#65306;1&#65289;GNNs&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#26159;&#21363;&#26102;&#30340;&#65292;&#32780;&#22312;&#29616;&#23454;&#20013;&#65292;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#31354;&#38388;&#28040;&#24687;&#20132;&#20114;&#21487;&#33021;&#23384;&#22312;&#24310;&#36831;&#12290;&#20132;&#36890;&#27969;&#37327;&#22312;&#19968;&#20010;&#33410;&#28857;&#19978;&#30340;&#21464;&#21270;&#38656;&#35201;&#20960;&#20998;&#38047;&#30340;&#26102;&#38388;&#24310;&#36831;&#65292;&#25165;&#33021;&#24433;&#21709;&#20854;&#30456;&#36830;&#30340;&#37051;&#23621;&#33410;&#28857;&#12290;2&#65289;&#20132;&#36890;&#29366;&#20917;&#19981;&#26029;&#21464;&#21270;&#12290;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#39044;&#27979;&#39057;&#29575;&#21487;&#33021;&#26681;&#25454;&#20855;&#20307;&#22330;&#26223;&#35201;&#27714;&#32780;&#21464;&#21270;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31163;&#25955;&#27169;&#22411;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#39044;&#27979;&#26102;&#38388;&#27573;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#24310;&#36831;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#26469;&#25429;&#25417;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24310;&#36831;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic flow forecasting is a fundamental research issue for transportation planning and management, which serves as a canonical and typical example of spatial-temporal predictions. In recent years, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) have achieved great success in capturing spatial-temporal correlations for traffic flow forecasting. Yet, two non-ignorable issues haven't been well solved: 1) The message passing in GNNs is immediate, while in reality the spatial message interactions among neighboring nodes can be delayed. The change of traffic flow at one node will take several minutes, i.e., time delay, to influence its connected neighbors. 2) Traffic conditions undergo continuous changes. The prediction frequency for traffic flow forecasting may vary based on specific scenario requirements. Most existing discretized models require retraining for each prediction horizon, restricting their applicability. To tackle the above issues, we propose a neural Spati
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#25237;&#24433;&#30340;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38170;&#22270;&#25237;&#24433;&#21040;&#26631;&#31614;&#31354;&#38388;&#65292;&#24182;&#23558;&#30697;&#38453;&#25237;&#24433;&#25193;&#23637;&#21040;&#24352;&#37327;&#25237;&#24433;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#31354;&#38388;&#32467;&#26500;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.16544</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#25237;&#24433;&#30340;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Label Learning Method Based on Tensor Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16544
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#25237;&#24433;&#30340;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38170;&#22270;&#25237;&#24433;&#21040;&#26631;&#31614;&#31354;&#38388;&#65292;&#24182;&#23558;&#30697;&#38453;&#25237;&#24433;&#25193;&#23637;&#21040;&#24352;&#37327;&#25237;&#24433;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#31354;&#38388;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38170;&#22270;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#22240;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#36991;&#20813;&#21518;&#22788;&#29702;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#38170;&#22270;&#30340;&#26041;&#27861;&#23398;&#20064;&#20855;&#26377;&#36830;&#25509;&#32452;&#20214;&#30340;&#20108;&#37096;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#21442;&#25968;&#26377;&#24456;&#39640;&#30340;&#35201;&#27714;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#20855;&#26377;&#28165;&#26224;&#36830;&#25509;&#32452;&#20214;&#30340;&#20108;&#37096;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#25237;&#24433;&#30340;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65288;LLMTP&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#27491;&#20132;&#25237;&#24433;&#30697;&#38453;&#23558;&#38170;&#22270;&#25237;&#24433;&#21040;&#26631;&#31614;&#31354;&#38388;&#65292;&#30452;&#25509;&#33719;&#24471;&#32858;&#31867;&#26631;&#31614;&#12290;&#32771;&#34385;&#21040;&#22312;&#19981;&#21516;&#35270;&#22270;&#20013;&#20998;&#21035;&#25237;&#24433;&#26102;&#65292;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#31354;&#38388;&#32467;&#26500;&#20449;&#24687;&#21487;&#33021;&#20250;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#34987;&#24573;&#30053;&#65292;&#25105;&#20204;&#23558;&#30697;&#38453;&#25237;&#24433;&#21464;&#25442;&#25193;&#23637;&#21040;&#24352;&#37327;&#25237;&#24433;&#65292;&#20197;&#20415;&#20805;&#20998;&#21033;&#29992;&#35270;&#22270;&#20043;&#38388;&#30340;&#31354;&#38388;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16544v1 Announce Type: new  Abstract: Multi-view clustering method based on anchor graph has been widely concerned due to its high efficiency and effectiveness. In order to avoid post-processing, most of the existing anchor graph-based methods learn bipartite graphs with connected components. However, such methods have high requirements on parameters, and in some cases it may not be possible to obtain bipartite graphs with clear connected components. To end this, we propose a label learning method based on tensor projection (LLMTP). Specifically, we project anchor graph into the label space through an orthogonal projection matrix to obtain cluster labels directly. Considering that the spatial structure information of multi-view data may be ignored to a certain extent when projected in different views separately, we extend the matrix projection transformation to tensor projection, so that the spatial structure information between views can be fully utilized. In addition, we i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#25511;&#21046;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20248;&#21270;&#31574;&#30053;&#26469;&#20943;&#23569;&#27969;&#20307;&#27169;&#25311;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.16543</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21152;&#36895;&#27969;&#20307;&#27169;&#25311;&#20013;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-based deep reinforcement learning for accelerated learning from flow simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#25511;&#21046;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20248;&#21270;&#31574;&#30053;&#26469;&#20943;&#23569;&#27969;&#20307;&#27169;&#25311;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#38381;&#29615;&#27969;&#25511;&#38382;&#39064;&#30340;&#25216;&#26415;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;&#29615;&#22659;&#21487;&#20197;&#20107;&#20808;&#31471;&#21040;&#31471;&#22320;&#20248;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#20026;&#23433;&#20840;&#20851;&#38190;&#30340;&#25511;&#21046;&#24212;&#29992;&#25552;&#20379;&#34394;&#25311;&#35797;&#39564;&#24179;&#21488;&#65292;&#24182;&#19988;&#21487;&#20197;&#28145;&#20837;&#29702;&#35299;&#25511;&#21046;&#26426;&#21046;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#25511;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#22312;&#20174;&#27969;&#27169;&#25311;&#20013;&#37319;&#26679;&#30340;&#36712;&#36857;&#21644;&#20174;&#29615;&#22659;&#27169;&#22411;&#38598;&#21512;&#20013;&#37319;&#26679;&#30340;&#36712;&#36857;&#20043;&#38388;&#20132;&#26367;&#20248;&#21270;&#31574;&#30053;&#12290;&#27169;&#22411;&#20026;&#22522;&#30784;&#23398;&#20064;&#38477;&#20302;&#20102;&#25972;&#20307;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16543v1 Announce Type: cross  Abstract: In recent years, deep reinforcement learning has emerged as a technique to solve closed-loop flow control problems. Employing simulation-based environments in reinforcement learning enables a priori end-to-end optimization of the control system, provides a virtual testbed for safety-critical control applications, and allows to gain a deep understanding of the control mechanisms. While reinforcement learning has been applied successfully in a number of rather simple flow control benchmarks, a major bottleneck toward real-world applications is the high computational cost and turnaround time of flow simulations. In this contribution, we demonstrate the benefits of model-based reinforcement learning for flow control applications. Specifically, we optimize the policy by alternating between trajectories sampled from flow simulations and trajectories sampled from an ensemble of environment models. The model-based learning reduces the overall 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMGR&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#20250;&#35805;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35805;&#25512;&#33616;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.16539</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#20250;&#35805;&#25512;&#33616;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models with Graphical Session-Based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMGR&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#20250;&#35805;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35805;&#25512;&#33616;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#25506;&#32034;&#65292;&#21033;&#29992;LLMs&#22312;&#25512;&#33616;&#31995;&#32479;&#19978;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;&#34429;&#28982;&#24320;&#21019;&#24615;&#30340;&#31574;&#30053;&#20027;&#35201;&#26159;&#23558;&#20256;&#32479;&#25512;&#33616;&#20219;&#21153;&#36716;&#21464;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25361;&#25112;&#65292;&#20294;&#22312;&#20250;&#35805;&#25512;&#33616;&#65288;SBR&#65289;&#39046;&#22495;&#30340;&#25506;&#32034;&#30456;&#23545;&#36739;&#23569;&#65292;&#22240;&#20026;&#20854;&#20855;&#20307;&#24615;&#12290;SBR&#20027;&#35201;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#20027;&#23548;&#65292;&#30001;&#20110;&#20854;&#25429;&#33719;&#30456;&#37051;&#34892;&#20026;&#20043;&#38388;&#30340;&#20869;&#22312;&#21644;&#26174;&#24615;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#32467;&#26524;&#12290;&#22270;&#30340;&#32467;&#26500;&#24615;&#36136;&#19982;&#33258;&#28982;&#35821;&#35328;&#30340;&#26412;&#36136;&#24418;&#25104;&#23545;&#27604;&#65292;&#20026;LLMs&#25552;&#20986;&#20102;&#37325;&#22823;&#30340;&#36866;&#24212;&#24615;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#20250;&#35805;&#25512;&#33616;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;LLMGR&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21644;&#35856;&#22320;&#24357;&#21512;&#19978;&#36848;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16539v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), various explorations have arisen to utilize LLMs capability of context understanding on recommender systems. While pioneering strategies have primarily transformed traditional recommendation tasks into challenges of natural language generation, there has been a relative scarcity of exploration in the domain of session-based recommendation (SBR) due to its specificity. SBR has been primarily dominated by Graph Neural Networks, which have achieved many successful outcomes due to their ability to capture both the implicit and explicit relationships between adjacent behaviors. The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs. In this paper, we introduce large language models with graphical Session-Based recommendation, named LLMGR, an effective framework that bridges the aforementioned gap by harmoniously 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#21457;&#29616;&#20154;&#24037;&#31896;&#24615;&#27169;&#22411;&#65292;&#26080;&#38656;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#39640;&#38454;&#23432;&#24658;&#23450;&#24459;&#27714;&#35299;&#22120;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.16517</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#19981;&#36830;&#32493;Galerkin&#36924;&#36817;&#23432;&#24658;&#23450;&#24459;&#30340;&#20154;&#24037;&#31896;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16517
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#21457;&#29616;&#20154;&#24037;&#31896;&#24615;&#27169;&#22411;&#65292;&#26080;&#38656;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#39640;&#38454;&#23432;&#24658;&#23450;&#24459;&#27714;&#35299;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#39640;&#38454;&#23432;&#24658;&#23450;&#24459;&#27714;&#35299;&#22120;&#25552;&#20379;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#19981;&#36830;&#32493;&#22788;&#38754;&#20020;Gibbs&#29616;&#35937;&#25361;&#25112;&#12290;&#20154;&#24037;&#31896;&#24615;&#26159;&#22522;&#20110;&#29289;&#29702;&#35265;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#38750;&#30417;&#30563;&#33539;&#24335;&#19979;&#30340;&#20154;&#24037;&#31896;&#24615;&#27169;&#22411;&#12290;&#35813;&#31639;&#27861;&#21463;&#24378;&#21270;&#23398;&#20064;&#21551;&#21457;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#23450;&#20041;&#20026;&#30456;&#23545;&#21442;&#32771;&#35299;&#30340;&#24046;&#24322;&#30340;&#25439;&#22833;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#21333;&#20803;&#26684;&#36880;&#20010;&#21333;&#20803;&#26684;&#25805;&#20316;&#65288;&#31896;&#24615;&#27169;&#22411;&#65289;&#12290;&#36825;&#20351;&#24471;&#33021;&#22815;&#36827;&#34892;&#26080;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#20854;&#25972;&#21512;&#21040;&#26368;&#20808;&#36827;&#30340;Runge-Kutta&#19981;&#36830;&#32493;Galerkin&#27714;&#35299;&#22120;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#22312;&#26631;&#37327;&#21644;&#30690;&#37327;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20960;&#20010;&#25968;&#20540;&#27979;&#35797;&#65292;&#22914;Burgers'&#21644;Euler&#30340;&#26041;&#31243;&#22312;&#19968;&#32500;&#21644;&#20108;&#32500;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16517v1 Announce Type: cross  Abstract: Finite element-based high-order solvers of conservation laws offer large accuracy but face challenges near discontinuities due to the Gibbs phenomenon. Artificial viscosity is a popular and effective solution to this problem based on physical insight. In this work, we present a physics-informed machine learning algorithm to automate the discovery of artificial viscosity models in a non-supervised paradigm. The algorithm is inspired by reinforcement learning and trains a neural network acting cell-by-cell (the viscosity model) by minimizing a loss defined as the difference with respect to a reference solution thanks to automatic differentiation. This enables a dataset-free training procedure. We prove that the algorithm is effective by integrating it into a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase several numerical tests on scalar and vectorial problems, such as Burgers' and Euler's equations in one and tw
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;GPHT&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20998;&#23618;Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#26500;&#24314;&#28151;&#21512;&#25968;&#25454;&#38598;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#25968;&#25454;&#38598;&#38480;&#21046;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#24573;&#35270;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.16516</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20998;&#23618;Transformer
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Hierarchical Transformer for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16516
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;GPHT&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20998;&#23618;Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#26500;&#24314;&#28151;&#21512;&#25968;&#25454;&#38598;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#25968;&#25454;&#38598;&#38480;&#21046;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#24573;&#35270;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#24341;&#20837;&#20808;&#36827;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20173;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#32570;&#38519;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21333;&#19968;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#21463;&#38480;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#24191;&#27867;&#37319;&#29992;&#19968;&#27493;&#29983;&#25104;&#27169;&#24335;&#65292;&#36825;&#38656;&#35201;&#23450;&#21046;&#21270;&#30340;&#39044;&#27979;&#22836;&#37096;&#65292;&#24573;&#30053;&#20102;&#36755;&#20986;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#36328;&#24230;&#35774;&#32622;&#19979;&#20250;&#23548;&#33268;&#22686;&#21152;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#39044;&#27979;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20998;&#23618;Transformer&#26550;&#26500;&#65292;&#21629;&#21517;&#20026;GPHT&#12290;GPHT&#20013;&#26377;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16516v1 Announce Type: new  Abstract: Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model's generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings.   To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset for pretraining our model, comprising various dataset
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21452;&#20048;&#35266;&#23398;&#20064;&#30340;Robbins-Monro&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#20219;&#21153;&#35843;&#24230;&#20013;&#22870;&#21169;&#21644;&#25104;&#26412;&#38590;&#20197;&#24314;&#27169;&#12289;&#20219;&#21153;&#21040;&#36798;&#20998;&#24067;&#19981;&#30830;&#23450;&#31561;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.16463</link><description>&lt;p&gt;
&#20351;&#29992;&#36172;&#21338;&#21453;&#39304;&#23398;&#20064;&#22312;&#32447;&#20219;&#21153;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning to Schedule Online Tasks with Bandit Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16463
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21452;&#20048;&#35266;&#23398;&#20064;&#30340;Robbins-Monro&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#20219;&#21153;&#35843;&#24230;&#20013;&#22870;&#21169;&#21644;&#25104;&#26412;&#38590;&#20197;&#24314;&#27169;&#12289;&#20219;&#21153;&#21040;&#36798;&#20998;&#24067;&#19981;&#30830;&#23450;&#31561;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#35745;&#31639;&#21644;&#20247;&#21253;&#20013;&#65292;&#22312;&#32447;&#20219;&#21153;&#35843;&#24230;&#23545;&#20110;&#20219;&#21153;&#23494;&#38598;&#22411;&#24212;&#29992;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20248;&#21270;&#30340;&#35843;&#24230;&#21487;&#20197;&#22312;&#26576;&#20123;&#20219;&#21153;&#21040;&#36798;&#20998;&#24067;&#19979;&#22686;&#24378;&#31995;&#32479;&#24615;&#33021;&#65292;&#36890;&#24120;&#36890;&#36807;&#22870;&#21169;&#25104;&#26412;&#27604;&#26469;&#34913;&#37327;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#21644;&#25104;&#26412;&#37117;&#20381;&#36182;&#20110;&#20219;&#21153;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#35780;&#20215;&#25351;&#26631;&#65289;&#19988;&#22312;&#23454;&#36341;&#20013;&#20445;&#25345;&#40657;&#30418;&#29366;&#24577;&#12290;&#36825;&#20351;&#24471;&#22870;&#21169;&#21644;&#25104;&#26412;&#38590;&#20197;&#24314;&#27169;&#65292;&#22240;&#27492;&#22312;&#20915;&#31574;&#20043;&#21069;&#26410;&#30693;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20219;&#21153;&#21040;&#36798;&#34892;&#20026;&#23545;&#20110;&#35832;&#22914;&#31995;&#32479;&#27874;&#21160;&#31561;&#22240;&#32032;&#20445;&#25345;&#25935;&#24863;&#65292;&#20808;&#21069;&#30340;&#20272;&#35745;&#25110;&#24120;&#35268;&#21040;&#36798;&#20998;&#24067;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#27850;&#26494;&#20998;&#24067;&#65289;&#21487;&#33021;&#22833;&#25928;&#12290;&#36825;&#24847;&#21619;&#30528;&#21478;&#19968;&#20010;&#23454;&#38469;&#20294;&#24120;&#34987;&#24573;&#35270;&#30340;&#25361;&#25112;&#65292;&#21363;&#19981;&#30830;&#23450;&#30340;&#20219;&#21153;&#21040;&#36798;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#22312;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#38745;&#24577;&#29615;&#22659;&#19979;&#26377;&#25928;&#35843;&#24230;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#20048;&#35266;&#23398;&#20064;&#30340;Robbins-Monro&#65288;DOL-RM&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16463v1 Announce Type: new  Abstract: Online task scheduling serves an integral role for task-intensive applications in cloud computing and crowdsourcing. Optimal scheduling can enhance system performance, typically measured by the reward-to-cost ratio, under some task arrival distribution. On one hand, both reward and cost are dependent on task context (e.g., evaluation metric) and remain black-box in practice. These render reward and cost hard to model thus unknown before decision making. On the other hand, task arrival behaviors remain sensitive to factors like unpredictable system fluctuation whereby a prior estimation or the conventional assumption of arrival distribution (e.g., Poisson) may fail. This implies another practical yet often neglected challenge, i.e., uncertain task arrival distribution. Towards effective scheduling under a stationary environment with various uncertainties, we propose a double-optimistic learning based Robbins-Monro (DOL-RM) algorithm. Spec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16442</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#37197;&#23545;&#27425;&#27169;&#27169;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;&#22823;&#20110;&#20869;&#23384;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#21462;&#20915;&#20110;&#23376;&#38598;&#36873;&#25321;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#30830;&#23450;&#19968;&#32452;&#37325;&#35201;&#21644;&#20195;&#34920;&#24615;&#30340;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#35777;&#20272;&#35745;&#36817;&#20284;&#20445;&#35777;&#30340;&#26032;&#39062;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16442v1 Announce Type: cross  Abstract: Many learning problems hinge on the fundamental problem of subset selection, i.e., identifying a subset of important and representative points. For example, selecting the most significant samples in ML training cannot only reduce training costs but also enhance model quality. Submodularity, a discrete analogue of convexity, is commonly used for solving subset selection problems. However, existing algorithms for optimizing submodular functions are sequential, and the prior distributed methods require at least one central machine to fit the target subset. In this paper, we relax the requirement of having a central machine for the target subset by proposing a novel distributed bounding algorithm with provable approximation guarantees. The algorithm iteratively bounds the minimum and maximum utility values to select high quality points and discard the unimportant ones. When bounding does not find the complete subset, we use a multi-round, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#21464;&#32479;&#35745;&#25439;&#22833;&#35757;&#32451;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#32570;&#22833;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.16435</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21464;&#32479;&#35745;&#25439;&#22833;&#35757;&#32451;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Implicit Generative Models via an Invariant Statistical Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16435
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#21464;&#32479;&#35745;&#25439;&#22833;&#35757;&#32451;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#32570;&#22833;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#23398;&#20064;&#20219;&#24847;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#38656;&#35201;&#36890;&#36807;&#23545;&#25239;&#24615;&#37492;&#21035;&#22120;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#32570;&#22833;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37492;&#21035;&#22120;&#30340;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#19968;&#32500;&#65288;1D&#65289;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#65292;&#38543;&#21518;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#20197;&#36866;&#24212;&#22810;&#21464;&#37327;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#27169;&#22411;&#26679;&#26412;&#32463;&#36807;&#36866;&#24403;&#36873;&#25321;&#30340;&#21464;&#25442;&#19982;&#22343;&#21248;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#24230;&#37327;&#65307;&#22240;&#27492;&#65292;&#23427;&#23545;&#25968;&#25454;&#30340;&#30495;&#23454;&#20998;&#24067;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#19968;&#32500;&#38543;&#26426;&#21464;&#37327;&#21046;&#23450;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20026;&#36817;&#20284;&#37325;&#21442;&#25968;&#21270;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16435v1 Announce Type: cross  Abstract: Implicit generative models have the capability to learn arbitrary complex data distributions. On the downside, training requires telling apart real data from artificially-generated ones using adversarial discriminators, leading to unstable training and mode-dropping issues. As reported by Zahee et al. (2017), even in the one-dimensional (1D) case, training a generative adversarial network (GAN) is challenging and often suboptimal. In this work, we develop a discriminator-free method for training one-dimensional (1D) generative implicit models and subsequently expand this method to accommodate multivariate cases. Our loss function is a discrepancy measure between a suitably chosen transformation of the model samples and a uniform distribution; hence, it is invariant with respect to the true distribution of the data. We first formulate our method for 1D random variables, providing an effective solution for approximate reparameterization 
&lt;/p&gt;</description></item><item><title>TOTEM&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20196;&#29260;&#21270;&#26550;&#26500;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#31163;&#25955;&#30690;&#37327;&#21270;&#34920;&#31034;&#23884;&#20837;&#19981;&#21516;&#39046;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#33021;&#22815;&#23454;&#29616;&#36890;&#29992;&#12289;&#36328;&#39046;&#22495;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.16412</link><description>&lt;p&gt;
TOTEM&#65306;&#29992;&#20110;&#19968;&#33324;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#20196;&#29260;&#21270;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16412
&lt;/p&gt;
&lt;p&gt;
TOTEM&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20196;&#29260;&#21270;&#26550;&#26500;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#31163;&#25955;&#30690;&#37327;&#21270;&#34920;&#31034;&#23884;&#20837;&#19981;&#21516;&#39046;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#33021;&#22815;&#23454;&#29616;&#36890;&#29992;&#12289;&#36328;&#39046;&#22495;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#33324;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#24320;&#22987;&#25506;&#32034;&#32479;&#19968;&#24314;&#27169;&#65292;&#20854;&#20013;&#19968;&#20010;&#36890;&#29992;&#30340;&#26550;&#26500;&#21487;&#20197;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#20174;&#19968;&#20010;&#20114;&#34917;&#30340;&#35282;&#24230;&#25509;&#36817;&#32479;&#19968;&#21270;&#65306;&#36328;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#32479;&#19968;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#31163;&#25955;&#12289;&#23398;&#20064;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#23545;&#21551;&#29992;&#36890;&#29992;&#12289;&#36328;&#39046;&#22495;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;TOTEM&#65292;&#21363;TOkenized Time Series EMbeddings&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26631;&#35760;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#30340;&#31163;&#25955;&#30690;&#37327;&#21270;&#34920;&#31034;&#23884;&#20837;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;TOTEM&#21487;&#20197;&#36328;&#22810;&#20010;&#20219;&#21153;&#21644;&#39046;&#22495;&#24037;&#20316;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#35843;&#25972;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;3&#20010;&#20219;&#21153;&#19978;&#30340;17&#20010;&#30495;&#23454;&#19990;&#30028;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#26469;&#30740;&#31350;TOTEM&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19987;&#23478;&#65288;&#21363;&#22312;&#27599;&#20010;&#39046;&#22495;&#35757;&#32451;&#27169;&#22411;&#65289;&#21644;&#36890;&#29992;&#65288;&#21363;&#35757;&#32451;&#65289;&#30340;TOTEM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16412v1 Announce Type: new  Abstract: The field of general time series analysis has recently begun to explore unified modeling, where a common architectural backbone can be retrained on a specific task for a specific dataset. In this work, we approach unification from a complementary vantage point: unification across tasks and domains. To this end, we explore the impact of discrete, learnt, time series data representations that enable generalist, cross-domain training. Our method, TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer architecture that embeds time series data from varying domains using a discrete vectorized representation learned in a self-supervised manner. TOTEM works across multiple tasks and domains with minimal to no tuning. We study the efficacy of TOTEM with an extensive evaluation on 17 real world time series datasets across 3 tasks. We evaluate both the specialist (i.e., training a model on each domain) and generalist (i.e., trainin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31283;&#23450;&#35757;&#32451;&#39640;&#32500;&#21464;&#20998;&#25512;&#26029;&#20013;&#27491;&#35268;&#21270;&#27969;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.16408</link><description>&lt;p&gt;
&#39640;&#32500;&#21464;&#20998;&#25512;&#26029;&#20013;&#27491;&#35268;&#21270;&#27969;&#30340;&#31283;&#23450;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stable Training of Normalizing Flows for High-dimensional Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16408
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31283;&#23450;&#35757;&#32451;&#39640;&#32500;&#21464;&#20998;&#25512;&#26029;&#20013;&#27491;&#35268;&#21270;&#27969;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27491;&#35268;&#21270;&#27969;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#22312;&#21462;&#20195;MCMC&#26041;&#27861;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;&#32806;&#21512;&#23618;&#65288;Real NVPs&#65289;&#30340;&#27491;&#35268;&#21270;&#27969;&#30001;&#20110;&#20854;&#33391;&#22909;&#30340;&#32463;&#39564;&#24615;&#33021;&#32780;&#32463;&#24120;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#35757;&#32451;&#29992;&#20110;&#36924;&#36817;&#39640;&#32500;&#21518;&#39564;&#20998;&#24067;&#30340;&#28145;&#23618;&#27491;&#35268;&#21270;&#27969;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#38543;&#26426;&#26799;&#24230;&#30340;&#39640;&#26041;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20808;&#21069;&#29992;&#20110;&#31283;&#23450;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#24046;&#30340;&#26041;&#27861;&#21487;&#33021;&#19981;&#36275;&#20197;&#23454;&#29616;Real NVPs&#30340;&#31283;&#23450;&#35757;&#32451;&#12290;&#25105;&#20204;&#30830;&#23450;&#38382;&#39064;&#30340;&#26681;&#28304;&#26159;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#26679;&#26412;&#36890;&#24120;&#21576;&#29616;&#24322;&#24120;&#39640;&#30340;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#32452;&#21512;&#65306;&#65288;1&#65289;&#23545;Real NVPs&#20013;&#30340;&#23610;&#24230;&#36827;&#34892;&#36719;&#38408;&#20540;&#22788;&#29702;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23545;&#26679;&#26412;&#36827;&#34892;&#21452;&#23556;&#36719;&#23545;&#25968;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16408v1 Announce Type: cross  Abstract: Variational inference with normalizing flows (NFs) is an increasingly popular alternative to MCMC methods. In particular, NFs based on coupling layers (Real NVPs) are frequently used due to their good empirical performance. In theory, increasing the depth of normalizing flows should lead to more accurate posterior approximations. However, in practice, training deep normalizing flows for approximating high-dimensional posterior distributions is often infeasible due to the high variance of the stochastic gradients. In this work, we show that previous methods for stabilizing the variance of stochastic gradient descent can be insufficient to achieve stable training of Real NVPs. As the source of the problem, we identify that, during training, samples often exhibit unusual high values. As a remedy, we propose a combination of two methods: (1) soft-thresholding of the scale in Real NVPs, and (2) a bijective soft log transformation of the sam
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#24067;&#23616;&#37319;&#26679;&#26041;&#27861;&#65292;Distributional Edge Layouts&#65288;DELs&#65289;&#65292;&#36890;&#36807;Langevin&#21160;&#21147;&#23398;&#21644;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#33021;&#22815;&#25429;&#33719;&#24191;&#27867;&#30340;&#33021;&#37327;&#20998;&#24067;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#26377;&#21161;&#20110;&#31616;&#21270;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.16402</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#24335;&#36793;&#24067;&#23616;&#30340;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Learning with Distributional Edge Layouts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16402
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#24067;&#23616;&#37319;&#26679;&#26041;&#27861;&#65292;Distributional Edge Layouts&#65288;DELs&#65289;&#65292;&#36890;&#36807;Langevin&#21160;&#21147;&#23398;&#21644;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#33021;&#22815;&#25429;&#33719;&#24191;&#27867;&#30340;&#33021;&#37327;&#20998;&#24067;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#26377;&#21161;&#20110;&#31616;&#21270;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#22312;&#26576;&#20123;&#25299;&#25169;&#24067;&#23616;&#19978;&#27839;&#30528;&#36793;&#22312;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#20256;&#36882;&#23616;&#37096;&#20449;&#24687;&#26469;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#22312;&#29616;&#20195;GNNs&#20013;&#65292;&#36825;&#20123;&#25299;&#25169;&#24067;&#23616;&#36890;&#24120;&#26159;&#25353;&#29031;&#30830;&#23450;&#24615;&#35745;&#31639;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;GNNs&#65289;&#25110;&#22312;&#21551;&#21457;&#24615;&#20551;&#35774;&#19979;&#26412;&#22320;&#37319;&#26679;&#65288;&#20363;&#22914;&#65292;GraphSage&#65289;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#36825;&#20123;&#24067;&#23616;&#21487;&#20197;&#36890;&#36807;Langevin&#21160;&#21147;&#23398;&#20840;&#23616;&#37319;&#26679;&#65292;&#36981;&#24490;&#37197;&#22791;&#26126;&#30830;&#29289;&#29702;&#33021;&#37327;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#20174;&#32780;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#26356;&#20855;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#19968;&#32452;&#37319;&#26679;/&#20248;&#21270;&#30340;&#24067;&#23616;&#21487;&#20197;&#25429;&#33719;&#24191;&#27867;&#30340;&#33021;&#37327;&#20998;&#24067;&#65292;&#24182;&#22312;WL-test&#20043;&#19978;&#24102;&#26469;&#39069;&#22806;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22240;&#27492;&#26377;&#21161;&#20110;&#31616;&#21270;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#36793;&#24067;&#23616;&#65288;DELs&#65289;&#20316;&#20026;&#21508;&#31181;GNNs&#30340;&#34917;&#20805;&#12290;DEL&#26159;&#19968;&#20010;&#19982;&#21518;&#32493;GNN&#21464;&#31181;&#26080;&#20851;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#65292;&#22240;&#27492;&#38750;&#24120;&#28789;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16402v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) learn from graph-structured data by passing local messages between neighboring nodes along edges on certain topological layouts. Typically, these topological layouts in modern GNNs are deterministically computed (e.g., attention-based GNNs) or locally sampled (e.g., GraphSage) under heuristic assumptions. In this paper, we for the first time pose that these layouts can be globally sampled via Langevin dynamics following Boltzmann distribution equipped with explicit physical energy, leading to higher feasibility in the physical world. We argue that such a collection of sampled/optimized layouts can capture the wide energy distribution and bring extra expressivity on top of WL-test, therefore easing downstream tasks. As such, we propose Distributional Edge Layouts (DELs) to serve as a complement to a variety of GNNs. DEL is a pre-processing strategy independent of subsequent GNN variants, thus being highly fl
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20132;&#21449;&#19968;&#33268;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.16388</link><description>&lt;p&gt;
&#20855;&#26377;&#20132;&#21449;&#19968;&#33268;$p$-&#20540;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16388
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#20013;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20132;&#21449;&#19968;&#33268;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#38752;&#12289;&#21487;&#20449;&#21644;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#23545;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#35201;&#27714;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25511;&#21046;&#31867;&#22411;I&#38169;&#35823;&#29575;($\alpha$)&#32780;&#21448;&#19981;&#25439;&#23475;&#31995;&#32479;&#30340;&#32479;&#35745;&#21151;&#29575;($1-\beta$)&#21487;&#20197;&#24314;&#31435;&#20449;&#20219;&#65292;&#24182;&#20943;&#23569;&#19982;&#20551;&#21457;&#29616;&#30456;&#20851;&#30340;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#24403;&#21518;&#32493;&#31243;&#24207;&#26114;&#36149;&#26102;&#12290;&#21033;&#29992;&#31526;&#21512;&#39044;&#27979;&#21407;&#21017;&#30340;&#26041;&#27861;&#26377;&#26395;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20026;&#24322;&#24120;&#26816;&#27979;&#25552;&#20379;&#30456;&#24212;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#35813;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;&#20132;&#21449;&#19968;&#33268;&#24322;&#24120;&#26816;&#27979;&#65292;&#24314;&#31435;&#22312;&#20026;&#39044;&#27979;&#20219;&#21153;&#35774;&#35745;&#30340;&#33879;&#21517;&#20132;&#21449;&#19968;&#33268;&#26041;&#27861;&#20043;&#19978;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#20182;&#22635;&#34917;&#20102;&#22312;&#24402;&#32435;&#19968;&#33268;&#24322;&#24120;&#26816;&#27979;&#29615;&#22659;&#20013;&#25193;&#23637;&#20808;&#21069;&#30740;&#31350;&#30340;&#33258;&#28982;&#30740;&#31350;&#31354;&#30333;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16388v1 Announce Type: cross  Abstract: Given the growing significance of reliable, trustworthy, and explainable machine learning, the requirement of uncertainty quantification for anomaly detection systems has become increasingly important. In this context, effectively controlling Type I error rates ($\alpha$) without compromising the statistical power ($1-\beta$) of these systems can build trust and reduce costs related to false discoveries, particularly when follow-up procedures are expensive. Leveraging the principles of conformal prediction emerges as a promising approach for providing respective statistical guarantees by calibrating a model's uncertainty. This work introduces a novel framework for anomaly detection, termed cross-conformal anomaly detection, building upon well-known cross-conformal methods designed for prediction tasks. With that, it addresses a natural research gap by extending previous works in the context of inductive conformal anomaly detection, rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26102;&#38388;&#22270;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;Simplified-Temporal-Graph-Network&#12290;</title><link>https://arxiv.org/abs/2402.16387</link><description>&lt;p&gt;
&#20851;&#20110;&#26102;&#38388;&#22270;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65306;&#29702;&#35770;&#35265;&#35299;&#19982;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26102;&#38388;&#22270;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;Simplified-Temporal-Graph-Network&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#23398;&#20064;&#65288;TGL&#65289;&#24050;&#25104;&#20026;&#19981;&#21516;&#30495;&#23454;&#24212;&#29992;&#20013;&#26222;&#36941;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#21487;&#20197;&#34920;&#31034;&#20026;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#22270;&#30340;&#39046;&#22495;&#12290;&#23613;&#31649;TGL&#22312;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#26377;&#38480;&#23485;&#36229;&#21442;&#25968;&#21270;&#26465;&#20214;&#19979;&#19981;&#21516;TGL&#31639;&#27861;&#65288;&#22914;&#22522;&#20110;GNN&#12289;&#22522;&#20110;RNN&#21644;&#22522;&#20110;&#20869;&#23384;&#30340;&#26041;&#27861;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;TGL&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#19982;GNN-/RNN- TGL&#26041;&#27861;&#20013;&#8220;&#23618;&#25968;/&#27493;&#25968;&#8221;&#20197;&#21450;&#29305;&#24449;-&#26631;&#31614;&#23545;&#40784;&#65288;FLA&#65289;&#20998;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20854;&#20013;FLA&#21487;&#29992;&#20316;&#34920;&#36798;&#33021;&#21147;&#30340;&#20195;&#29702;&#65292;&#24182;&#35299;&#37322;&#20102;&#22522;&#20110;&#20869;&#23384;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21270;&#26102;&#38388;&#22270;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16387v1 Announce Type: cross  Abstract: Temporal Graph Learning (TGL) has become a prevalent technique across diverse real-world applications, especially in domains where data can be represented as a graph and evolves over time. Although TGL has recently seen notable progress in algorithmic solutions, its theoretical foundations remain largely unexplored. This paper aims at bridging this gap by investigating the generalization ability of different TGL algorithms (e.g., GNN-based, RNN-based, and memory-based methods) under the finite-wide over-parameterized regime. We establish the connection between the generalization error of TGL algorithms and "the number of layers/steps" in the GNN-/RNN-based TGL methods and "the feature-label alignment (FLA) score", where FLA can be used as a proxy for the expressive power and explains the performance of memory-based methods. Guided by our theoretical analysis, we propose Simplified-Temporal-Graph-Network, which enjoys a small generaliza
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#32622;&#25442;&#30340;&#35268;&#33539;&#30456;&#20851;&#24615;&#30446;&#26631;&#23398;&#20064;&#34701;&#21512;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#22810;&#20010;&#35270;&#22270;&#30340;&#19968;&#33268;&#20266;&#26631;&#31614;&#26469;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#26377;&#25928;&#24615;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#36924;&#36817;&#30417;&#30563;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#34920;&#31034;&#65292;&#25552;&#20379;&#20102;&#30001;&#38169;&#35823;&#20266;&#26631;&#31614;&#27880;&#37322;&#24341;&#36215;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.16383</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Self Supervised Correlation-based Permutations for Multi-View Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16383
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#32622;&#25442;&#30340;&#35268;&#33539;&#30456;&#20851;&#24615;&#30446;&#26631;&#23398;&#20064;&#34701;&#21512;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#22810;&#20010;&#35270;&#22270;&#30340;&#19968;&#33268;&#20266;&#26631;&#31614;&#26469;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#26377;&#25928;&#24615;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#36924;&#36817;&#30417;&#30563;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#34920;&#31034;&#65292;&#25552;&#20379;&#20102;&#30001;&#38169;&#35823;&#20266;&#26631;&#31614;&#27880;&#37322;&#24341;&#36215;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#21487;&#20197;&#22686;&#24378;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#65292;&#21253;&#25324;&#32858;&#31867;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#65288;MVC&#65289;&#35299;&#20915;&#26041;&#26696;&#20165;&#38480;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#27425;&#20248;&#30340;&#19988;&#35745;&#31639;&#38656;&#27714;&#39640;&#30340;&#34920;&#31034;&#21644;&#32858;&#31867;&#20004;&#38454;&#27573;&#31243;&#24207;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#30340;&#36890;&#29992;&#25968;&#25454;&#65288;&#22270;&#20687;&#12289;&#34920;&#26684;&#31561;&#65289;&#30340;MVC&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#22522;&#20110;&#26032;&#39062;&#32622;&#25442;&#30340;&#35268;&#33539;&#30456;&#20851;&#24615;&#30446;&#26631;&#26469;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#34701;&#21512;&#25968;&#25454;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#36328;&#22810;&#20010;&#35270;&#22270;&#30340;&#19968;&#33268;&#20266;&#26631;&#31614;&#26469;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#12290;&#25105;&#20204;&#20351;&#29992;&#21313;&#20010;MVC&#22522;&#20934;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#36924;&#36817;&#20102;&#30417;&#30563;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#34920;&#31034;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30001;&#38169;&#35823;&#20266;&#26631;&#31614;&#27880;&#37322;&#24341;&#36215;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16383v1 Announce Type: new  Abstract: Fusing information from different modalities can enhance data analysis tasks, including clustering. However, existing multi-view clustering (MVC) solutions are limited to specific domains or rely on a suboptimal and computationally demanding two-stage procedure of representation and clustering. We propose an end-to-end deep learning-based MVC framework for general data (image, tabular, etc.). Our approach involves learning meaningful fused data representations with a novel permutation-based canonical correlation objective. Concurrently, we learn cluster assignments by identifying consistent pseudo-labels across multiple views. We demonstrate the effectiveness of our model using ten MVC benchmark datasets. Theoretically, we show that our model approximates the supervised linear discrimination analysis (LDA) representation. Additionally, we provide an error bound induced by false-pseudo label annotations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#29305;&#23450;&#30340;&#35821;&#38899;&#20998;&#24067;&#25972;&#21512;&#12289;&#33258;&#21160;&#21270;&#24405;&#21046;&#36807;&#31243;&#12289;&#33258;&#21160;&#21270;&#21644;&#20154;&#26426;&#21327;&#20316;&#30340;&#24405;&#38899;&#36136;&#37327;&#20445;&#35777;&#20197;&#21450;&#24405;&#38899;&#26684;&#24335;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.16380</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#36716;&#35821;&#38899;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#24320;&#28304;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#29305;&#23450;&#30340;&#35821;&#38899;&#20998;&#24067;&#25972;&#21512;&#12289;&#33258;&#21160;&#21270;&#24405;&#21046;&#36807;&#31243;&#12289;&#33258;&#21160;&#21270;&#21644;&#20154;&#26426;&#21327;&#20316;&#30340;&#24405;&#38899;&#36136;&#37327;&#20445;&#35777;&#20197;&#21450;&#24405;&#38899;&#26684;&#24335;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#23545;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#22522;&#20110;&#35821;&#38899;&#30340;&#25216;&#26415;&#12290;&#38543;&#30528;&#20869;&#23481;&#21019;&#20316;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20869;&#23481;&#65292;&#32763;&#35793;&#21644;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#24517;&#19981;&#21487;&#23569;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#23545;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#24613;&#20999;&#38656;&#27714;&#12290;&#26412;&#20316;&#21697;&#30340;&#36129;&#29486;&#22810;&#26041;&#38754;&#65292;&#21253;&#25324;&#65306;&#23558;&#35821;&#35328;&#29305;&#23450;&#30340;&#35821;&#38899;&#20998;&#24067;&#25972;&#21512;&#21040;&#26679;&#26412;&#36873;&#25321;&#20013;&#65292;&#33258;&#21160;&#21270;&#24405;&#21046;&#36807;&#31243;&#65292;&#33258;&#21160;&#21270;&#21644;&#20154;&#26426;&#21327;&#20316;&#30340;&#24405;&#38899;&#36136;&#37327;&#20445;&#35777;&#65292;&#20197;&#21450;&#22788;&#29702;&#24405;&#38899;&#20197;&#28385;&#36275;&#25351;&#23450;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16380v1 Announce Type: cross  Abstract: Data availability is crucial for advancing artificial intelligence applications, including voice-based technologies. As content creation, particularly in social media, experiences increasing demand, translation and text-to-speech (TTS) technologies have become essential tools. Notably, the performance of these TTS technologies is highly dependent on the quality of the training data, emphasizing the mutual dependence of data availability and technological progress. This paper introduces an end-to-end tool to generate high-quality datasets for text-to-speech (TTS) models to address this critical need for high-quality data. The contributions of this work are manifold and include: the integration of language-specific phoneme distribution into sample selection, automation of the recording process, automated and human-in-the-loop quality assurance of recordings, and processing of recordings to meet specified formats. The proposed application
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#35299;&#20915;&#22270;&#23398;&#20064;&#20013;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#30340;&#26368;&#26032;&#26041;&#27861;&#12289;&#31574;&#30053;&#21644;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.16374</link><description>&lt;p&gt;
&#22270;&#23398;&#20064;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30740;&#31350;&#65306;&#39046;&#22495;&#33258;&#36866;&#24212;&#12289;&#22806;&#37096;&#20998;&#24067;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#35299;&#20915;&#22270;&#23398;&#20064;&#20013;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#30340;&#26368;&#26032;&#26041;&#27861;&#12289;&#31574;&#30053;&#21644;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#19988;&#22240;&#20854;&#22312;&#24314;&#27169;&#30001;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#31034;&#30340;&#22797;&#26434;&#25968;&#25454;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20174;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#21040;&#25512;&#33616;&#31995;&#32479;&#12290;&#29616;&#23454;&#20013;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#36890;&#24120;&#38543;&#26102;&#38388;&#21160;&#24577;&#21464;&#21270;&#65292;&#33410;&#28857;&#23646;&#24615;&#21644;&#36793;&#32467;&#26500;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#22270;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#21463;&#21040;&#20998;&#24067;&#36716;&#31227;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#36716;&#31227;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#22270;&#23398;&#20064;&#26041;&#27861;&#22312;&#38477;&#20302;&#27867;&#21270;&#21644;&#36866;&#24212;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#32473;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#35299;&#20915;&#22270;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#30340;&#26368;&#26032;&#26041;&#27861;&#12289;&#31574;&#30053;&#21644;&#35265;&#35299;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16374v1 Announce Type: new  Abstract: Graph learning plays a pivotal role and has gained significant attention in various application scenarios, from social network analysis to recommendation systems, for its effectiveness in modeling complex data relations represented by graph structural data. In reality, the real-world graph data typically show dynamics over time, with changing node attributes and edge structure, leading to the severe graph data distribution shift issue. This issue is compounded by the diverse and complex nature of distribution shifts, which can significantly impact the performance of graph learning methods in degraded generalization and adaptation capabilities, posing a substantial challenge to their effectiveness. In this survey, we provide a comprehensive review and summary of the latest approaches, strategies, and insights that address distribution shifts within the context of graph learning. Concretely, according to the observability of distributions 
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#27491;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#65292;&#32780;&#27492;&#35843;&#26597;&#35770;&#25991;&#26088;&#22312;&#20840;&#38754;&#27010;&#36848;&#29983;&#25104;AI&#25193;&#25955;&#21644;&#20256;&#32479;&#27169;&#22411;&#30340;&#22522;&#26412;&#25216;&#26415;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16369</link><description>&lt;p&gt;
&#35270;&#35273;&#20013;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65306;&#27169;&#22411;&#12289;&#24230;&#37327;&#21644;&#24212;&#29992;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative AI in Vision: A Survey on Models, Metrics and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16369
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#27491;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#65292;&#32780;&#27492;&#35843;&#26597;&#35770;&#25991;&#26088;&#22312;&#20840;&#38754;&#27010;&#36848;&#29983;&#25104;AI&#25193;&#25955;&#21644;&#20256;&#32479;&#27169;&#22411;&#30340;&#22522;&#26412;&#25216;&#26415;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#27169;&#22411;&#36890;&#36807;&#23454;&#29616;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#21019;&#24314;&#65292;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;&#29983;&#25104;AI&#25193;&#25955;&#21644;&#20256;&#32479;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#30340;&#22522;&#26412;&#25216;&#26415;&#12289;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#20197;&#21450;&#23427;&#20204;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#21253;&#25324;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#21644;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#24314;&#27169;&#31561;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#22270;&#20687;&#20462;&#34917;&#21644;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#31561;&#22810;&#26679;&#21270;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#21644;&#25968;&#25454;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#32508;&#21512;&#29616;&#26377;&#30740;&#31350;&#24182;&#31361;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#20851;&#38190;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#19968;&#20221;&#20851;&#20110;&#29983;&#25104;AI&#22312;&#35270;&#35273;&#20013;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16369v1 Announce Type: cross  Abstract: Generative AI models have revolutionized various fields by enabling the creation of realistic and diverse data samples. Among these models, diffusion models have emerged as a powerful approach for generating high-quality images, text, and audio. This survey paper provides a comprehensive overview of generative AI diffusion and legacy models, focusing on their underlying techniques, applications across different domains, and their challenges. We delve into the theoretical foundations of diffusion models, including concepts such as denoising diffusion probabilistic models (DDPM) and score-based generative modeling. Furthermore, we explore the diverse applications of these models in text-to-image, image inpainting, and image super-resolution, along with others, showcasing their potential in creative tasks and data augmentation. By synthesizing existing research and highlighting critical advancements in this field, this survey aims to prov
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#33258;&#28982;&#31354;&#38388;&#25551;&#36848;&#36827;&#34892;&#22810;&#23610;&#24230;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#36890;&#36807;&#33719;&#30693;&#22320;&#22270;&#30693;&#35782;&#24471;&#21040;&#30340;&#25551;&#36848;&#33021;&#22815;&#25552;&#20379;&#29615;&#22659;&#30340;&#25972;&#20307;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.16364</link><description>&lt;p&gt;
&#20174;&#21738;&#37324;&#20986;&#21457;&#65311;&#26469;&#33258;&#33258;&#28982;&#31354;&#38388;&#25551;&#36848;&#20013;&#30340;&#22810;&#23610;&#24230;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16364
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#33258;&#28982;&#31354;&#38388;&#25551;&#36848;&#36827;&#34892;&#22810;&#23610;&#24230;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#36890;&#36807;&#33719;&#30693;&#22320;&#22270;&#30693;&#35782;&#24471;&#21040;&#30340;&#25551;&#36848;&#33021;&#22815;&#25552;&#20379;&#29615;&#22659;&#30340;&#25972;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29992;&#33258;&#28982;&#35821;&#35328;&#20256;&#36798;&#36335;&#32447;&#26102;&#65292;&#8220;&#33719;&#24471;&#30340;&#31354;&#38388;&#30693;&#35782;&#8221;&#27010;&#24565;&#23545;&#22320;&#29702;&#20449;&#24687;&#26816;&#32034;&#65288;GIR&#65289;&#21644;&#31354;&#38388;&#35748;&#30693;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23548;&#33322;&#30740;&#31350;&#32463;&#24120;&#24573;&#35270;&#36825;&#31181;&#33719;&#24471;&#30693;&#35782;&#23545;&#25991;&#26412;&#25551;&#36848;&#30340;&#24433;&#21709;&#12290;&#24403;&#21069;&#23548;&#33322;&#30740;&#31350;&#38598;&#20013;&#22312;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#26412;&#22320;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#8220;&#23427;&#23558;&#22312;&#24744;&#30340;&#21491;&#36793;&#8221;&#65289;&#65292;&#36825;&#20123;&#25551;&#36848;&#38656;&#35201;&#23545;&#20195;&#29702;&#20154;&#30340;&#26412;&#22320;&#30693;&#35273;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#22320;&#22270;&#33719;&#24471;&#30340;&#30693;&#35782;&#22522;&#30784;&#19978;&#30340;&#25551;&#36848;&#25552;&#20379;&#20102;&#29615;&#22659;&#30340;&#25972;&#20307;&#35270;&#22270;&#65292;&#24182;&#25429;&#25417;&#20102;&#20854;&#24635;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16364v1 Announce Type: new  Abstract: When communicating routes in natural language, the concept of {\em acquired spatial knowledge} is crucial for geographic information retrieval (GIR) and in spatial cognitive research. However, NLP navigation studies often overlook the impact of such acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., `it will be on your right') that require reasoning over the agent's local perception. These instructions are typically given as a sequence of steps, with each action-step explicitly mentioning and being followed by a landmark that the agent can use to verify they are on the right path (e.g., `turn right and then you will see...'). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure. These instructions (e.g., `it is south of Central Park and a block north of a police station') are 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;</title><link>https://arxiv.org/abs/2402.16359</link><description>&lt;p&gt;
&#21453;&#39304;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Feedback Efficient Online Fine-Tuning of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22270;&#20687;&#65292;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27169;&#25311;&#26368;&#22823;&#21270;&#26576;&#20123;&#23646;&#24615;&#30340;&#20998;&#24067;&#30340;&#37096;&#20998;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#29983;&#25104;&#20855;&#26377;&#39640;&#23457;&#32654;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#25110;&#20855;&#26377;&#39640;&#29983;&#29289;&#27963;&#24615;&#30340;&#20998;&#23376;&#12290;&#33258;&#28982;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#36825;&#35270;&#20026;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#19982;&#26576;&#20123;&#23646;&#24615;&#23545;&#24212;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21363;&#20351;&#21487;&#20197;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#30340;&#22312;&#32447;&#26597;&#35810;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#20063;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#23427;&#20204;&#22312;&#21021;&#22987;&#20998;&#24067;&#20013;&#30340;&#27010;&#29575;&#21487;&#33021;&#24456;&#20302;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#19981;&#21487;&#34892;&#30340;&#26679;&#26412;&#65292;&#29978;&#33267;&#27809;&#26377;&#23450;&#20041;&#33391;&#22909;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#19981;&#33258;&#28982;&#30340;&#22270;&#20687;&#25110;&#29289;&#29702;&#19978;&#19981;&#21487;&#33021;&#30340;&#20998;&#23376;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16359v1 Announce Type: cross  Abstract: Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that effi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16358</link><description>&lt;p&gt;
&#19968;&#20010;&#25972;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#29992;&#20110;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Integrated Data Processing Framework for Pretraining Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16358
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#32463;&#24120;&#38656;&#35201;&#25163;&#21160;&#20174;&#19981;&#21516;&#26469;&#28304;&#31574;&#21010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#27599;&#20010;&#25968;&#25454;&#23384;&#20648;&#24211;&#24320;&#21457;&#19987;&#38376;&#30340;&#25968;&#25454;&#28165;&#27927;&#27969;&#31243;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#36825;&#19968;&#36807;&#31243;&#37325;&#22797;&#32780;&#32321;&#29712;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#22788;&#29702;&#27169;&#22359;&#21253;&#25324;&#19968;&#31995;&#21015;&#19981;&#21516;&#31890;&#24230;&#27700;&#24179;&#30340;&#25805;&#20316;&#31526;&#65292;&#32780;&#20998;&#26512;&#27169;&#22359;&#25903;&#25345;&#23545;&#31934;&#28860;&#25968;&#25454;&#36827;&#34892;&#25506;&#26597;&#21644;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26131;&#20110;&#20351;&#29992;&#19988;&#39640;&#24230;&#28789;&#27963;&#12290;&#22312;&#36825;&#31687;&#28436;&#31034;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#24182;&#23637;&#31034;&#23427;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#19982;ChatGPT&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#31471;&#21040;&#31471;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16358v1 Announce Type: cross  Abstract: The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#24341;&#20837;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#26469;&#24341;&#23548;&#36825;&#19968;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#24182;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16354</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Language-guided Skill Learning with Temporal Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#24341;&#20837;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#26469;&#24341;&#23548;&#36825;&#19968;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#24182;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#21457;&#29616;&#25216;&#33021;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25552;&#20986;&#36712;&#36857;&#30340;&#21021;&#22987;&#20998;&#21106;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#20998;&#23618;&#21464;&#20998;&#25512;&#26029;&#26694;&#26550;&#23558;LLM&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#36890;&#36807;&#21512;&#24182;&#36712;&#36857;&#27573;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25511;&#21046;&#21387;&#32553;&#21644;&#21487;&#37325;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#30340;&#26032;&#36741;&#21161;&#30446;&#26631;&#65292;&#24110;&#21161;&#24341;&#23548;&#36825;&#31181;&#25216;&#33021;&#21457;&#29616;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;Agent&#33021;&#22815;&#21457;&#29616;&#26377;&#21161;&#20110;&#21152;&#36895;&#23398;&#20064;&#30340;&#25216;&#33021;&#65292;&#22312;BabyAI&#65288;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#23548;&#33322;&#29615;&#22659;&#65289;&#20197;&#21450;ALFRED&#65288;&#19968;&#20010;&#23478;&#24237;&#27169;&#25311;&#29615;&#22659;&#65289;&#30340;&#26032;&#38271;&#26399;&#20219;&#21153;&#20013;&#32988;&#36807;&#22522;&#32447;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16354v1 Announce Type: cross  Abstract: We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#37327;&#23376;&#24577;&#37325;&#26500;&#20013;&#30740;&#31350;&#20102;&#21487;&#20197;&#21516;&#26102;&#27979;&#37327;&#22810;&#20010;&#21103;&#26412;&#30340;&#24773;&#20917;&#65292;&#21457;&#29616;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#26410;&#30693;&#30340;$d$&#32500;&#24577;&#21040;&#36319;&#36394;&#36317;&#31163;$\epsilon$&#65292;&#38656;&#35201;&#24182;&#19988;&#36275;&#22815;&#30340;&#25335;&#36125;&#20026;$\widetilde{\Theta}(\frac{d^3}{\sqrt{t}\epsilon^2})$&#12290;</title><link>https://arxiv.org/abs/2402.16353</link><description>&lt;p&gt;
&#22312;&#37327;&#23376;&#24577;&#37325;&#26500;&#20013;&#37327;&#23376;&#32416;&#32544;&#21644;&#22797;&#21046;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26368;&#20248;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
An optimal tradeoff between entanglement and copy complexity for state tomography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#37327;&#23376;&#24577;&#37325;&#26500;&#20013;&#30740;&#31350;&#20102;&#21487;&#20197;&#21516;&#26102;&#27979;&#37327;&#22810;&#20010;&#21103;&#26412;&#30340;&#24773;&#20917;&#65292;&#21457;&#29616;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#26410;&#30693;&#30340;$d$&#32500;&#24577;&#21040;&#36319;&#36394;&#36317;&#31163;$\epsilon$&#65292;&#38656;&#35201;&#24182;&#19988;&#36275;&#22815;&#30340;&#25335;&#36125;&#20026;$\widetilde{\Theta}(\frac{d^3}{\sqrt{t}\epsilon^2})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24403;&#20195;&#37327;&#23376;&#35774;&#22791;&#30340;&#23454;&#38469;&#32422;&#26463;&#22914;&#20309;&#24433;&#21709;&#37327;&#23376;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#23545;&#20110;&#32463;&#20856;&#38382;&#39064;&#8212;&#8212;&#24577;&#37325;&#26500;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20005;&#26684;&#21051;&#30011;&#20102;&#21482;&#33021;&#21516;&#26102;&#27979;&#37327;&#26410;&#30693;&#24577;&#30340;&#19968;&#20010;&#21103;&#26412;&#30340;&#20219;&#20309;&#21327;&#35758;&#30340;&#22797;&#21046;&#22797;&#26434;&#24615;&#65292;&#34920;&#26126;&#36825;&#31181;&#22797;&#26434;&#24615;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#27604;&#21487;&#20197;&#36827;&#34892;&#23436;&#20840;&#32416;&#32544;&#27979;&#37327;&#30340;&#24773;&#20917;&#26356;&#31967;&#12290;&#34429;&#28982;&#25105;&#20204;&#29616;&#22312;&#24050;&#32463;&#27604;&#36739;&#20840;&#38754;&#22320;&#20102;&#35299;&#20102;&#22312;&#36817;&#26399;&#21644;&#23481;&#38169;&#21306;&#22495;&#36827;&#34892;&#27492;&#31867;&#20219;&#21153;&#30340;&#36895;&#29575;&#65292;&#20294;&#30446;&#21069;&#25105;&#20204;&#36824;&#19981;&#22826;&#20102;&#35299;&#20854;&#20013;&#30340;&#26223;&#35937;&#26159;&#20160;&#20040;&#26679;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#20197;&#21516;&#26102;&#23545;$t$&#20010;&#21103;&#26412;&#36827;&#34892;&#27979;&#37327;&#30340;&#33258;&#28982;&#24773;&#22659;&#20013;&#30340;&#24577;&#37325;&#26500;&#12290;&#23545;&#20110;&#36275;&#22815;&#23567;&#30340;$\epsilon$&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#24847;$t \le d^2$&#65292;&#23398;&#20064;&#19968;&#20010;&#26410;&#30693;&#30340;$d$&#32500;&#24577;$\rho$&#21040;&#36319;&#36394;&#36317;&#31163;$\epsilon$&#38656;&#35201;&#24182;&#19988;&#36275;&#22815;&#30340;&#25335;&#36125;&#20026;$\widetilde{\Theta}(\frac{d^3}{\sqrt{t}\epsilon^2})$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16353v1 Announce Type: cross  Abstract: There has been significant interest in understanding how practical constraints on contemporary quantum devices impact the complexity of quantum learning. For the classic question of tomography, recent work tightly characterized the copy complexity for any protocol that can only measure one copy of the unknown state at a time, showing it is polynomially worse than if one can make fully-entangled measurements. While we now have a fairly complete picture of the rates for such tasks in the near-term and fault-tolerant regimes, it remains poorly understood what the landscape in between looks like.   In this work, we study tomography in the natural setting where one can make measurements of $t$ copies at a time. For sufficiently small $\epsilon$, we show that for any $t \le d^2$, $\widetilde{\Theta}(\frac{d^3}{\sqrt{t}\epsilon^2})$ copies are necessary and sufficient to learn an unknown $d$-dimensional state $\rho$ to trace distance $\epsilo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#25511;&#21046;&#29702;&#35770;&#25913;&#36827;&#20102;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;GAIL&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Controlled-GAIL&#8221;&#65288;C-GAIL&#65289;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;GAIL&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;MuJoCo&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.16349</link><description>&lt;p&gt;
C-GAIL: &#21033;&#29992;&#25511;&#21046;&#29702;&#35770;&#31283;&#23450;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#25511;&#21046;&#29702;&#35770;&#25913;&#36827;&#20102;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;GAIL&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Controlled-GAIL&#8221;&#65288;C-GAIL&#65289;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;GAIL&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;MuJoCo&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;GAIL&#65289;&#35757;&#32451;&#19968;&#20010;&#29983;&#25104;&#31574;&#30053;&#26469;&#27169;&#20223;&#19968;&#20010;&#28436;&#31034;&#32773;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#20248;&#21270;&#20174;&#31867;&#20284;GAN&#30340;&#37492;&#21035;&#22120;&#20013;&#23548;&#20986;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;GAIL&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#20854;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615; - &#23427;&#32487;&#25215;&#20102;GAN&#30340;&#22797;&#26434;&#35757;&#32451;&#21160;&#24577;&#65292;&#20197;&#21450;RL&#24341;&#20837;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25391;&#33633;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#31574;&#30053;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#25511;&#21046;&#29702;&#35770;&#21487;&#20197;&#24110;&#21161;GAN&#30340;&#35757;&#32451;&#25910;&#25947;&#12290;&#26412;&#25991;&#24310;&#20280;&#20102;&#36825;&#19968;&#32447;&#36335;&#30340;&#24037;&#20316;&#65292;&#23545;GAIL&#36827;&#34892;&#20102;&#25511;&#21046;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#19981;&#20165;&#23558;GAIL&#25512;&#21521;&#26399;&#26395;&#30340;&#22343;&#34913;&#28857;&#65292;&#36824;&#22312;&#8220;&#21333;&#27493;&#8221;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#28176;&#36817;&#31283;&#23450;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#8220;Controlled-GAIL&#8221;&#65288;C-GAIL&#65289;&#12290;&#22312;MuJoCo&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#21463;&#25511;&#21464;&#20307;&#33021;&#22815;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16349v1 Announce Type: new  Abstract: Generative Adversarial Imitation Learning (GAIL) trains a generative policy to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from a GAN-like discriminator. A major drawback of GAIL is its training instability - it inherits the complex training dynamics of GANs, and the distribution shift introduced by RL. This can cause oscillations during training, harming its sample efficiency and final policy performance. Recent work has shown that control theory can help with the convergence of a GAN's training. This paper extends this line of work, conducting a control-theoretic analysis of GAIL and deriving a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a 'one-step' setting. Based on this, we propose a practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled variant is able to speed up the rate of conve
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;PH&#21521;&#27744;&#21270;&#23618;&#27880;&#20837;&#20840;&#23616;&#25299;&#25169;&#19981;&#21464;&#24615;&#30340;&#26426;&#21046;&#26174;&#33879;&#25552;&#21319;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16346</link><description>&lt;p&gt;
&#29992;&#25345;&#20037;&#21516;&#35843;&#22686;&#24378;&#22270;&#27744;&#21270;
&lt;/p&gt;
&lt;p&gt;
Boosting Graph Pooling with Persistent Homology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16346
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;PH&#21521;&#27744;&#21270;&#23618;&#27880;&#20837;&#20840;&#23616;&#25299;&#25169;&#19981;&#21464;&#24615;&#30340;&#26426;&#21046;&#26174;&#33879;&#25552;&#21319;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#25345;&#20037;&#21516;&#35843;&#65288;PH&#65289;&#32435;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20197;&#20016;&#23500;&#34920;&#36798;&#33021;&#21147;&#30340;&#36235;&#21183;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#23558;PH&#29305;&#24449;&#25554;&#20837;GNN&#23618;&#24635;&#26159;&#24102;&#26469;&#36739;&#20302;&#21487;&#35299;&#37322;&#24615;&#30340;&#36793;&#38469;&#25913;&#36827;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;PH&#21521;&#27744;&#21270;&#23618;&#27880;&#20837;&#20840;&#23616;&#25299;&#25169;&#19981;&#21464;&#24615;&#65292;&#28789;&#24863;&#26469;&#33258;PH&#20013;&#30340;&#36807;&#28388;&#25805;&#20316;&#33258;&#28982;&#22320;&#20351;&#22270;&#27744;&#21270;&#20197;&#25130;&#26029;&#26041;&#24335;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#24335;&#19979;&#65292;&#31895;&#21270;&#22270;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#27839;&#30528;&#25345;&#20037;&#27744;&#21270;&#25299;&#25169;&#36827;&#34892;&#65292;&#20174;&#32780;&#25552;&#21319;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26426;&#21046;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#20960;&#20010;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#25345;&#32493;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#23637;&#31034;&#20102;&#20854;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16346v1 Announce Type: new  Abstract: Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#30340;&#31616;&#21333;&#38543;&#26426;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#30697;&#38453;&#20056;&#27861;&#23454;&#29616;&#39640;&#36136;&#37327;&#36924;&#36817;&#20272;&#35745;&#27010;&#29575;&#21644;&#27169;&#22411;&#25972;&#20307;&#24046;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16326</link><description>&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#30340;&#21487;&#35777;&#23454;&#20934;&#30830;&#24615;&#38543;&#26426;&#25277;&#26679;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Provably Accurate Randomized Sampling Algorithm for Logistic Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16326
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#30340;&#31616;&#21333;&#38543;&#26426;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#30697;&#38453;&#20056;&#27861;&#23454;&#29616;&#39640;&#36136;&#37327;&#36924;&#36817;&#20272;&#35745;&#27010;&#29575;&#21644;&#27169;&#22411;&#25972;&#20307;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#36923;&#36753;&#22238;&#24402;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#20108;&#20998;&#31867;&#20219;&#21153;&#30340;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#24403;&#35266;&#27979;&#25968;&#37327;&#36828;&#36828;&#36229;&#36807;&#39044;&#27979;&#21464;&#37327;&#25968;&#37327;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#30340;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#31639;&#27861;&#65292;&#20445;&#35777;&#39640;&#36136;&#37327;&#36924;&#36817;&#20272;&#35745;&#27010;&#29575;&#21644;&#27169;&#22411;&#25972;&#20307;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24314;&#31435;&#22312;&#20004;&#20010;&#31616;&#21333;&#30340;&#32467;&#26500;&#26465;&#20214;&#22522;&#30784;&#19978;&#65292;&#36825;&#20004;&#20010;&#26465;&#20214;&#21487;&#24402;&#32467;&#20026;&#38543;&#26426;&#30697;&#38453;&#20056;&#27861;&#65292;&#26159;&#38543;&#26426;&#21270;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#30340;&#22522;&#26412;&#19988;&#28145;&#20837;&#29702;&#35299;&#30340;&#22522;&#20803;&#12290;&#24403;&#21033;&#29992;&#26464;&#26438;&#20998;&#25968;&#23545;&#35266;&#27979;&#36827;&#34892;&#25277;&#26679;&#26102;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36923;&#36753;&#22238;&#24402;&#30340;&#20272;&#35745;&#27010;&#29575;&#23646;&#24615;&#65292;&#24182;&#35777;&#26126;&#20934;&#30830;&#36924;&#36817;&#21487;&#20197;&#36890;&#36807;&#36828;&#23567;&#20110;&#24635;&#35266;&#27979;&#25968;&#30340;&#26679;&#26412;&#23454;&#29616;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16326v1 Announce Type: cross  Abstract: In statistics and machine learning, logistic regression is a widely-used supervised learning technique primarily employed for binary classification tasks. When the number of observations greatly exceeds the number of predictor variables, we present a simple, randomized sampling-based algorithm for logistic regression problem that guarantees high-quality approximations to both the estimated probabilities and the overall discrepancy of the model. Our analysis builds upon two simple structural conditions that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized numerical linear algebra. We analyze the properties of estimated probabilities of logistic regression when leverage scores are used to sample observations, and prove that accurate approximations can be achieved with a sample whose size is much smaller than the total number of observations. To further validate our theoretical findi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#32422;$O(1/\epsilon)$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#30456;&#27604;&#20808;&#21069;&#25991;&#29486;&#20013;&#24050;&#26377;&#30340;$O(1/\epsilon^2)$&#26679;&#26412;&#22797;&#26434;&#24230;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.16324</link><description>&lt;p&gt;
&#23454;&#29616;&#32422;$O(1/\epsilon)$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#29992;&#20110;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained Markov Decision Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16324
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#32422;$O(1/\epsilon)$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#30456;&#27604;&#20808;&#21069;&#25991;&#29486;&#20013;&#24050;&#26377;&#30340;$O(1/\epsilon^2)$&#26679;&#26412;&#22797;&#26434;&#24230;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#39034;&#24207;&#23398;&#20064;&#21644;&#20915;&#31574;&#20013;&#28385;&#36275;&#23433;&#20840;&#24615;&#25110;&#36164;&#28304;&#32422;&#26463;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25317;&#26377;&#26377;&#38480;&#36164;&#28304;&#21644;&#26410;&#30693;&#36716;&#31227;&#27010;&#29575;&#30340;MDP&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#21462;&#19968;&#20010;&#34892;&#21160;&#65292;&#25910;&#38598;&#22870;&#21169;&#24182;&#28040;&#32791;&#19968;&#20123;&#36164;&#28304;&#65292;&#25152;&#26377;&#20551;&#35774;&#37117;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#38543;&#30528;&#26102;&#38388;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#20026;CMDP&#38382;&#39064;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#38382;&#39064;&#30456;&#20851;&#20445;&#35777;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#23545;&#25968;&#36951;&#25022;&#30028;&#38480;&#65292;&#36825;&#36716;&#21270;&#20026;$O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20854;&#20013;$\kappa$&#26159;&#19968;&#20010;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#20294;&#19982;$\epsilon$&#26080;&#20851;&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#25913;&#36827;&#20102;&#20808;&#21069;&#25991;&#29486;&#20013;&#38024;&#23545;CMDP&#38382;&#39064;&#24314;&#31435;&#30340;$O(1/\epsilon^2)$&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16324v1 Announce Type: new  Abstract: We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound, with $\kappa$ being a problem-dependent parameter, yet independent of $\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;VQScore&#65292;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#21487;&#21464;&#20998;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#36136;&#37327;&#24182;&#36827;&#34892;&#33258;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#35757;&#32451;&#65292;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#21644;&#26032;&#39062;&#30340;&#33258;&#33976;&#39311;&#26426;&#21046;&#25552;&#39640;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16321</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#28165;&#26224;&#35821;&#38899;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#19982;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16321
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;VQScore&#65292;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#21487;&#21464;&#20998;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#36136;&#37327;&#24182;&#36827;&#34892;&#33258;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#35757;&#32451;&#65292;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#21644;&#26032;&#39062;&#30340;&#33258;&#33976;&#39311;&#26426;&#21046;&#25552;&#39640;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#20174;&#20154;&#31867;&#21548;&#35273;&#19987;&#23478;&#35774;&#35745;&#36716;&#21464;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#30446;&#21069;&#22823;&#22810;&#25968;&#27169;&#22411;&#20381;&#36182;&#20110;&#30417;&#30563;&#23398;&#20064;&#32780;&#23548;&#33268;&#26631;&#31614;&#25910;&#38598;&#32791;&#26102;&#26114;&#36149;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;VQScore&#65292;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#21487;&#21464;&#20998;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#37327;&#21270;&#35823;&#24046;&#35780;&#20272;&#35821;&#38899;&#30340;&#33258;&#30417;&#30563;&#24230;&#37327;&#12290;VQ-VAE&#30340;&#35757;&#32451;&#20381;&#36182;&#20110;&#28165;&#26224;&#35821;&#38899;&#65292;&#22240;&#27492;&#24403;&#35821;&#38899;&#21463;&#21040;&#25197;&#26354;&#26102;&#21487;&#20197;&#39044;&#26399;&#21040;&#36739;&#22823;&#30340;&#37327;&#21270;&#35823;&#24046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#19982;&#30495;&#23454;&#36136;&#37327;&#20998;&#25968;&#30340;&#30456;&#20851;&#24615;&#65292;&#23558;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;&#27169;&#22411;&#35774;&#35745;&#20013;&#12290;&#21457;&#29616;&#30690;&#37327;&#37327;&#21270;&#26426;&#21046;&#20063;&#21487;&#20197;&#29992;&#20110;&#33258;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#27169;&#22411;&#35757;&#32451;&#12290;&#20026;&#20102;&#25552;&#39640;&#32534;&#30721;&#22120;&#23545;SE&#30340;&#40065;&#26834;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#33976;&#39311;&#26426;&#21046;&#32467;&#21512;&#23545;&#25239;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16321v1 Announce Type: cross  Abstract: Speech quality estimation has recently undergone a paradigm shift from human-hearing expert designs to machine-learning models. However, current models rely mainly on supervised learning, which is time-consuming and expensive for label collection. To solve this problem, we propose VQScore, a self-supervised metric for evaluating speech based on the quantization error of a vector-quantized-variational autoencoder (VQ-VAE). The training of VQ-VAE relies on clean speech; hence, large quantization errors can be expected when the speech is distorted. To further improve correlation with real quality scores, domain knowledge of speech processing is incorporated into the model design. We found that the vector quantization mechanism could also be used for self-supervised speech enhancement (SE) model training. To improve the robustness of the encoder for SE, a novel self-distillation mechanism combined with adversarial training is introduced. I
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#32852;&#37030;&#19978;&#19979;&#25991;&#32423;&#32852;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24322;&#27493;&#36890;&#20449;&#21644;&#32771;&#34385;&#24322;&#36136;&#29992;&#25143;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#19981;&#21516;&#20559;&#22909;&#30340;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#21270;&#25512;&#33616;&#65292;&#24182;&#32473;&#20986;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.16312</link><description>&lt;p&gt;
&#20855;&#26377;&#24322;&#27493;&#36890;&#20449;&#21644;&#24322;&#36136;&#29992;&#25143;&#30340;&#32852;&#37030;&#19978;&#19979;&#25991;&#32423;&#32852;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Federated Contextual Cascading Bandits with Asynchronous Communication and Heterogeneous Users
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#32852;&#37030;&#19978;&#19979;&#25991;&#32423;&#32852;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24322;&#27493;&#36890;&#20449;&#21644;&#32771;&#34385;&#24322;&#36136;&#29992;&#25143;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#19981;&#21516;&#20559;&#22909;&#30340;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#21270;&#25512;&#33616;&#65292;&#24182;&#32473;&#20986;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32852;&#37030;&#19978;&#19979;&#25991;&#32452;&#21512;&#32423;&#32852;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;$|\mathcal{U}|$&#20010;&#20195;&#29702;&#22312;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#19979;&#21512;&#20316;&#65292;&#20026;$|\mathcal{U}|$&#20010;&#23545;&#24212;&#30340;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#25512;&#33616;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#24322;&#27493;&#36890;&#20449;&#33539;&#24335;&#19979;&#30340;&#32852;&#37030;&#20195;&#29702;&#65292;&#26080;&#38656;&#24378;&#21046;&#21516;&#27493;&#65292;&#24182;&#19988;&#25152;&#26377;&#20195;&#29702;&#37117;&#29420;&#31435;&#20110;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#20197;&#21450;&#24322;&#36136;&#29992;&#25143;&#34892;&#20026;&#65292;&#20854;&#20013;&#29992;&#25143;&#21487;&#20197;&#34987;&#20998;&#20026;$J\le |\mathcal{U}|$&#20010;&#28508;&#22312;&#29992;&#25143;&#38598;&#32676;&#65292;&#27599;&#20010;&#38598;&#32676;&#23637;&#29616;&#20986;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#31934;&#24515;&#36890;&#20449;&#21327;&#35758;&#30340;UCB&#31867;&#22411;&#31639;&#27861;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23545;&#20110;p&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16312v1 Announce Type: cross  Abstract: We study the problem of federated contextual combinatorial cascading bandits, where $|\mathcal{U}|$ agents collaborate under the coordination of a central server to provide tailored recommendations to the $|\mathcal{U}|$ corresponding users. Existing works consider either a synchronous framework, necessitating full agent participation and global synchronization, or assume user homogeneity with identical behaviors. We overcome these limitations by considering (1) federated agents operating in an asynchronous communication paradigm, where no mandatory synchronization is required and all agents communicate independently with the server, (2) heterogeneous user behaviors, where users can be stratified into $J \le |\mathcal{U}|$ latent user clusters, each exhibiting distinct preferences. For this setting, we propose a UCB-type algorithm with delicate communication protocols. Through theoretical analysis, we give sub-linear regret bounds on p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.16310</link><description>&lt;p&gt;
REPLAY: &#23545;&#31232;&#30095;&#36712;&#36857;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#39044;&#27979;&#26159;&#26681;&#25454;&#21382;&#21490;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#26469;&#39044;&#27979;&#29992;&#25143;&#20301;&#32622;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#30340;&#22266;&#26377;&#31232;&#30095;&#38382;&#39064;&#65292;&#26102;&#31354;&#19978;&#19979;&#25991;&#34987;&#35777;&#26126;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#26159;&#23558;&#20301;&#32622;&#20043;&#38388;&#30340;&#26102;&#31354;&#36317;&#31163;&#32435;&#20837;&#21040;&#31227;&#21160;&#36712;&#36857;&#20013;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#25552;&#20379;&#32473;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#26469;&#23547;&#25214;&#26377;&#20449;&#24687;&#30340;&#36807;&#21435;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#26410;&#33021;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#20363;&#22914;&#65292;&#20154;&#31867;&#31227;&#21160;&#22312;&#26089;&#26216;&#36890;&#24120;&#27604;&#20854;&#20182;&#26102;&#38388;&#26356;&#26377;&#35268;&#24459;&#65307;&#36825;&#26263;&#31034;&#20102;&#23454;&#38469;&#26102;&#38388;&#25139;&#30340;&#26377;&#29992;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPLAY&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;RNN&#26550;&#26500;&#65292;&#26088;&#22312;&#25429;&#25417;&#26102;&#38388;&#21464;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#35268;&#24459;&#20197;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#22411;&#21453;&#28436;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#33258;&#30001;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32469;&#36807;&#20256;&#32479;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#30452;&#25509;&#20248;&#21270;&#22270;&#20687;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#65292;&#20026;&#25913;&#36827;&#22270;&#20687;&#29983;&#25104;&#25552;&#20379;&#20102;&#20851;&#38190;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.16305</link><description>&lt;p&gt;
&#23457;&#31295;&#21592;&#20063;&#21487;&#20197;&#21442;&#19982;&#65306;&#36890;&#36807;&#27169;&#22411;&#21453;&#28436;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#30340;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16305
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#21453;&#28436;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#33258;&#30001;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32469;&#36807;&#20256;&#32479;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#30452;&#25509;&#20248;&#21270;&#22270;&#20687;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#65292;&#20026;&#25913;&#36827;&#22270;&#20687;&#29983;&#25104;&#25552;&#20379;&#20102;&#20851;&#38190;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#24615;&#25361;&#25112;&#65292;&#21363;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#38590;&#20197;&#20005;&#26684;&#36981;&#23432;&#22797;&#26434;&#12289;&#22810;&#26041;&#38754;&#30340;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36825;&#19968;&#21327;&#35843;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#35266;&#28857;&#65292;&#23558;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;DPMs&#35270;&#20026;&#21453;&#36716;&#20808;&#36827;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#36890;&#36807;&#36825;&#31181;&#34920;&#36848;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#32469;&#36807;&#19982;DPMs&#30456;&#20851;&#30340;&#20256;&#32479;&#37319;&#26679;&#36807;&#31243;&#12290;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#22270;&#20687;&#65292;&#24182;&#22312;&#26377;&#36776;&#21035;&#21147;&#30340;VLMs&#30340;&#30417;&#30563;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#23454;&#29616;&#26356;&#22909;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#12290;&#20316;&#20026;&#27010;&#24565;&#30340;&#35777;&#26126;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;BLIP-2&#27169;&#22411;&#30340;&#27969;&#31243;&#65292;&#24182;&#30830;&#23450;&#20102;&#20960;&#20010;&#29992;&#20110;&#25913;&#36827;&#22270;&#20687;&#29983;&#25104;&#30340;&#20851;&#38190;&#35774;&#35745;&#12290;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#23450;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16305v1 Announce Type: cross  Abstract: As a dominant force in text-to-image generation tasks, Diffusion Probabilistic Models (DPMs) face a critical challenge in controllability, struggling to adhere strictly to complex, multi-faceted instructions. In this work, we aim to address this alignment challenge for conditional generation tasks. First, we provide an alternative view of state-of-the-art DPMs as a way of inverting advanced Vision-Language Models (VLMs). With this formulation, we naturally propose a training-free approach that bypasses the conventional sampling process associated with DPMs. By directly optimizing images with the supervision of discriminative VLMs, the proposed method can potentially achieve a better text-image alignment. As proof of concept, we demonstrate the pipeline with the pre-trained BLIP-2 model and identify several key designs for improved image generation. To further enhance the image fidelity, a Score Distillation Sampling module of Stable Di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#22270;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;GDPO&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20026;&#20219;&#24847;&#30446;&#26631;&#20248;&#21270;&#22270;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16302</link><description>&lt;p&gt;
&#22270;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph Diffusion Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22270;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;GDPO&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20026;&#20219;&#24847;&#30446;&#26631;&#20248;&#21270;&#22270;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#29305;&#23450;&#19979;&#28216;&#30446;&#26631;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36825;&#23545;&#20110;&#39046;&#22495;&#22914;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#22270;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36861;&#27714;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#22270;&#25193;&#25955;&#23384;&#22312;&#25361;&#25112;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;GDPO&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20026;&#20219;&#24847;&#65288;&#22914;&#38750;&#21487;&#24494;&#20998;&#65289;&#30446;&#26631;&#20248;&#21270;&#22270;&#25193;&#25955;&#27169;&#22411;&#12290;GDPO&#22522;&#20110;&#38024;&#23545;&#22270;&#25193;&#25955;&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#24613;&#20999;&#31574;&#30053;&#26799;&#24230;&#65292;&#36890;&#36807;&#35748;&#30495;&#20998;&#26512;&#24320;&#21457;&#65292;&#26377;&#26395;&#25552;&#39640;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GDPO&#22312;&#20855;&#26377;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30446;&#26631;&#30340;&#21508;&#31181;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/sail-sg/GDPO&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16302v1 Announce Type: cross  Abstract: Recent research has made significant progress in optimizing diffusion models for specific downstream objectives, which is an important pursuit in fields such as graph generation for drug design. However, directly applying these models to graph diffusion presents challenges, resulting in suboptimal performance. This paper introduces graph diffusion policy optimization (GDPO), a novel approach to optimize graph diffusion models for arbitrary (e.g., non-differentiable) objectives using reinforcement learning. GDPO is based on an eager policy gradient tailored for graph diffusion models, developed through meticulous analysis and promising improved performance. Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives. Code is available at https://github.com/sail-sg/GDPO.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#30340;&#32622;&#20449;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#24615;&#22238;&#24402;&#20013;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16300</link><description>&lt;p&gt;
Conformalized Selective Regression
&lt;/p&gt;
&lt;p&gt;
Conformalized Selective Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#30340;&#32622;&#20449;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#24615;&#22238;&#24402;&#20013;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#24635;&#26159;&#35201;&#25552;&#20379;&#39044;&#27979;&#65311;&#22312;&#36861;&#27714;&#26368;&#22823;&#39044;&#27979;&#24615;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#21487;&#38752;&#24615;&#21644;&#20844;&#24179;&#24615;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#12290;&#36873;&#25321;&#24615;&#22238;&#24402;&#65292;&#20063;&#31216;&#20026;&#8220;&#25298;&#32477;&#36873;&#39033;&#8221;&#65292;&#20801;&#35768;&#27169;&#22411;&#22312;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#25918;&#24323;&#39044;&#27979;&#12290;&#23613;&#31649;7&#21313;&#24180;&#21069;&#23601;&#26368;&#21021;&#25552;&#20986;&#20102;&#36873;&#25321;&#24615;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#29992;&#20110;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#20195;&#29702;&#65292;&#23588;&#20854;&#26159;&#26465;&#20214;&#26041;&#24046;&#12290;&#20294;&#36825;&#31181;&#20851;&#27880;&#24573;&#35270;&#20102;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#20026;&#22522;&#20110;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#30340;&#20010;&#21035;&#39044;&#27979;&#25552;&#20379;&#26377;&#26681;&#25454;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20415;&#36827;&#34892;&#24688;&#24403;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16300v1 Announce Type: new  Abstract: Should prediction models always deliver a prediction? In the pursuit of maximum predictive performance, critical considerations of reliability and fairness are often overshadowed, particularly when it comes to the role of uncertainty. Selective regression, also known as the "reject option," allows models to abstain from predictions in cases of considerable uncertainty. Initially proposed seven decades ago, approaches to selective regression have mostly focused on distribution-based proxies for measuring uncertainty, particularly conditional variance. However, this focus neglects the significant influence of model-specific biases on a model's performance. In this paper, we propose a novel approach to selective regression by leveraging conformal prediction, which provides grounded confidence measures for individual predictions based on model-specific biases. In addition, we propose a standardized evaluation framework to allow proper compar
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;DWHRec&#31639;&#27861;&#26469;&#35299;&#20915;&#38899;&#20048;&#25512;&#33616;&#20013;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#21152;&#26435;&#36229;&#22270;&#23884;&#20837;&#23398;&#20064;&#26469;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16299</link><description>&lt;p&gt;
&#23545;&#25239;&#31579;&#36873;&#27668;&#27873;&#65306;&#22522;&#20110;&#21152;&#26435;&#36229;&#22270;&#23884;&#20837;&#23398;&#20064;&#30340;&#38899;&#20048;&#25512;&#33616;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16299
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;DWHRec&#31639;&#27861;&#26469;&#35299;&#20915;&#38899;&#20048;&#25512;&#33616;&#20013;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#21152;&#26435;&#36229;&#22270;&#23884;&#20837;&#23398;&#20064;&#26469;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#29992;&#25143;&#26377;&#30528;&#21452;&#37325;&#20316;&#29992;&#65306;&#36807;&#28388;&#19981;&#21512;&#36866;&#25110;&#19981;&#21305;&#37197;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20934;&#30830;&#35782;&#21035;&#31526;&#21512;&#20854;&#20559;&#22909;&#30340;&#39033;&#30446;&#12290;&#35768;&#22810;&#25512;&#33616;&#31639;&#27861;&#26088;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20449;&#24687;&#38453;&#21015;&#65292;&#20197;&#28385;&#36275;&#20854;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#20010;&#24615;&#21270;&#21487;&#33021;&#20250;&#23558;&#29992;&#25143;&#38480;&#21046;&#22312;&#8220;&#31579;&#36873;&#27668;&#27873;&#8221;&#20013;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#33616;&#20013;&#33719;&#24471;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#26159;&#19968;&#39033;&#36843;&#20999;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#20197;&#38899;&#20048;&#25512;&#33616;&#20026;&#20363;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#26679;&#21270;&#21152;&#26435;&#36229;&#22270;&#38899;&#20048;&#25512;&#33616;&#31639;&#27861;&#65288;DWHRec&#65289;&#12290;&#22312;DWHRec&#31639;&#27861;&#20013;&#65292;&#29992;&#25143;&#21644;&#24050;&#21548;&#26354;&#30446;&#20043;&#38388;&#30340;&#21021;&#22987;&#36830;&#25509;&#30001;&#21152;&#26435;&#36229;&#22270;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#36824;&#23558;&#33402;&#26415;&#23478;&#12289;&#19987;&#36753;&#21644;&#26631;&#35760;&#19982;&#26354;&#30446;&#30340;&#20851;&#32852;&#20063;&#38468;&#21152;&#21040;&#36229;&#22270;&#20013;&#12290;&#20026;&#20102;&#25506;&#32034;&#29992;&#25143;&#30340;&#28508;&#22312;&#20559;&#22909;&#65292;&#19968;&#20010;&#36229;&#22270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16299v1 Announce Type: cross  Abstract: Recommender systems serve a dual purpose for users: sifting out inappropriate or mismatched information while accurately identifying items that align with their preferences. Numerous recommendation algorithms are designed to provide users with a personalized array of information tailored to their preferences. Nevertheless, excessive personalization can confine users within a "filter bubble". Consequently, achieving the right balance between accuracy and diversity in recommendations is a pressing concern. To address this challenge, exemplified by music recommendation, we introduce the Diversified Weighted Hypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm, the initial connections between users and listened tracks are represented by a weighted hypergraph. Simultaneously, associations between artists, albums and tags with tracks are also appended to the hypergraph. To explore users' latent preferences, a hypergrap
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#24179;&#31283;&#36716;&#31227;&#21160;&#24577;&#30340;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;Dirichlet Markov&#38142;&#21644;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26469;&#35299;&#20915;&#21407;&#26377;&#27169;&#22411;&#25429;&#25417;&#26102;&#21464;&#36716;&#31227;&#21160;&#24577;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.16297</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#24179;&#31283;&#36716;&#31227;&#21160;&#24577;&#30340;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16297
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#24179;&#31283;&#36716;&#31227;&#21160;&#24577;&#30340;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;Dirichlet Markov&#38142;&#21644;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26469;&#35299;&#20915;&#21407;&#26377;&#27169;&#22411;&#25429;&#25417;&#26102;&#21464;&#36716;&#31227;&#21160;&#24577;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#35745;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#22240;&#20854;&#33021;&#22815;&#25512;&#26029;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#32467;&#26500;&#21644;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#32780;&#22791;&#21463;&#37325;&#35270;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22788;&#29702;&#22024;&#26434;&#21644;&#19981;&#23436;&#25972;&#30340;&#35745;&#25968;&#25968;&#25454;&#12290;&#22312;&#36825;&#20123;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#65292;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;&#65288;PGDSs&#65289;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#35266;&#23519;&#21040;&#30340;&#35745;&#25968;&#24207;&#21015;&#24213;&#23618;&#21160;&#24577;&#30340;&#28436;&#21464;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#30340;PGDS&#22312;&#25429;&#25417;&#24120;&#35265;&#20110;&#23454;&#38469;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#21464;&#36716;&#31227;&#21160;&#24577;&#26041;&#38754;&#20173;&#26377;&#19981;&#36275;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24179;&#31283;PGDS&#65292;&#20801;&#35768;&#22522;&#30784;&#36716;&#31227;&#30697;&#38453;&#38543;&#26102;&#38388;&#28436;&#21464;&#65292;&#28436;&#21464;&#30340;&#36716;&#31227;&#30697;&#38453;&#30001;&#31934;&#24515;&#35774;&#35745;&#30340;Dirichlet Markov&#38142;&#24314;&#27169;&#12290;&#21033;&#29992;Dirichlet-Multinomial-Beta&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#20849;&#36717;&#19988;&#39640;&#25928;&#30340;Gibbs&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16297v1 Announce Type: cross  Abstract: Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with noisy and incomplete count data. Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences. However, the state-of-the-art PGDS still falls short in capturing the time-varying transition dynamics that are commonly observed in real-world count time series. To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by sophisticatedly-designed Dirichlet Markov chains. Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;AMS&#30005;&#30913;&#37327;&#33021;&#22120;&#36827;&#34892;&#36136;&#23376;&#32972;&#26223;&#25298;&#32477;&#30340;&#26032;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#39640;&#33021;&#27491;&#30005;&#23376;&#27979;&#37327;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16285</link><description>&lt;p&gt;
&#29992;AMS&#30005;&#30913;&#37327;&#33021;&#22120;&#27604;&#36739;&#25506;&#27979;&#36136;&#23376;&#32972;&#26223;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Deep Learning Models for Proton Background Rejection with the AMS Electromagnetic Calorimeter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16285
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;AMS&#30005;&#30913;&#37327;&#33021;&#22120;&#36827;&#34892;&#36136;&#23376;&#32972;&#26223;&#25298;&#32477;&#30340;&#26032;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#39640;&#33021;&#27491;&#30005;&#23376;&#27979;&#37327;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Alpha Magnetic Spectrometer (AMS)&#26159;&#19968;&#31181;&#39640;&#31934;&#24230;&#31890;&#23376;&#25506;&#27979;&#22120;&#65292;&#23433;&#35013;&#22312;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#65292;&#21253;&#21547;&#20845;&#31181;&#19981;&#21516;&#30340;&#27425;&#25506;&#27979;&#22120;&#12290;&#36807;&#28193;&#36752;&#23556;&#25506;&#27979;&#22120;&#21644;&#30005;&#30913;&#37327;&#33021;&#22120;(ECAL)&#29992;&#20110;&#23558;&#30005;&#23376;/&#27491;&#30005;&#23376;&#19982;&#20016;&#23500;&#30340;&#23431;&#23449;&#23556;&#32447;&#36136;&#23376;&#32972;&#26223;&#20998;&#24320;&#12290; AMS&#22312;&#31354;&#38388;&#20013;&#27979;&#37327;&#21040;&#30340;&#27491;&#30005;&#23376;&#36890;&#37327;&#26381;&#20174;&#24130;&#24459;&#65292;&#22312;25 GeV&#20197;&#19978;&#24847;&#22806;&#21464;&#36719;&#65292;&#28982;&#21518;&#22312;280 GeV&#20197;&#19978;&#21464;&#30828;&#12290;&#26377;&#20960;&#31181;&#29702;&#35770;&#27169;&#22411;&#35797;&#22270;&#35299;&#37322;&#36825;&#20123;&#29616;&#35937;&#65292;&#38656;&#35201;&#26356;&#32431;&#20928;&#30340;&#39640;&#33021;&#27491;&#30005;&#23376;&#27979;&#37327;&#26469;&#24110;&#21161;&#27979;&#35797;&#23427;&#20204;&#12290;&#30446;&#21069;&#29992;&#20110;&#22312;&#39640;&#33021;&#37327;&#19979;&#25298;&#32477;&#36136;&#23376;&#32972;&#26223;&#30340;&#26041;&#27861;&#28041;&#21450;&#20174;ECAL&#20013;&#22806;&#25512;&#28107;&#28020;&#29305;&#24449;&#65292;&#29992;&#20316;&#22686;&#24378;&#20915;&#31574;&#26641;&#21644;&#20284;&#28982;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;AMS ECAL&#36827;&#34892;&#31890;&#23376;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#28145;&#24230;&#23398;&#20064;(DL)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16285v1 Announce Type: cross  Abstract: The Alpha Magnetic Spectrometer (AMS) is a high-precision particle detector onboard the International Space Station containing six different subdetectors. The Transition Radiation Detector and Electromagnetic Calorimeter (ECAL) are used to separate electrons/positrons from the abundant cosmic-ray proton background.   The positron flux measured in space by AMS falls with a power law which unexpectedly softens above 25 GeV and then hardens above 280 GeV. Several theoretical models try to explain these phenomena, and a purer measurement of positrons at higher energies is needed to help test them. The currently used methods to reject the proton background at high energies involve extrapolating shower features from the ECAL to use as inputs for boosted decision tree and likelihood classifiers. We present a new approach for particle identification with the AMS ECAL using deep learning (DL). By taking the energy deposition within all the ECAL
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.16278</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#27880;&#37322;&#23884;&#20837;&#27169;&#22411;&#30340;&#26412;&#20307;&#21253;&#21547;&#20851;&#31995;&#39044;&#27979;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16278
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#34920;&#31034;&#23454;&#20307;&#30340;&#26412;&#20307;&#23884;&#20837;&#65292;&#29992;&#20110;&#26412;&#20307;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#26412;&#20307;&#23884;&#20837;&#26410;&#35299;&#20915;&#31867;&#20284;&#21644;&#23396;&#31435;&#23454;&#20307;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#26410;&#25552;&#21462;&#26412;&#20307;&#20013;&#27880;&#37322;&#20844;&#29702;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#30340;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65306;Inverted-index Matrix Embedding (InME) &#21644; Co-occurrence Matrix Embedding (CoME)&#12290;&#36825;&#20004;&#31181;&#23884;&#20837;&#36890;&#36807;&#27599;&#20010;&#21333;&#35789;&#22312;&#19968;&#32452;&#20844;&#29702;&#20013;&#20986;&#29616;&#30340;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#20844;&#29702;&#20013;&#21333;&#35789;&#30340;&#20849;&#29616;&#26469;&#25429;&#33719;&#27880;&#37322;&#20844;&#29702;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#12290;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#39044;&#27979;&#30340;&#36229;&#31867;&#19982;&#23376;&#31867;&#30456;&#20284;&#19988;&#23396;&#31435;&#20110;&#26412;&#20307;&#20013;&#30340;&#20854;&#20182;&#23454;&#20307;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20915;&#31574;&#20248;&#21270;CoPilot&#65288;DOCP&#65289;&#65292;&#24110;&#21161;&#20915;&#31574;&#32773;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#29702;&#35299;&#24182;&#35299;&#20915;&#19994;&#21153;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16269</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#21040;&#20915;&#31574;&#20248;&#21270;CoPilot&#65306;&#19968;&#39033;&#30740;&#31350;&#23459;&#35328;
&lt;/p&gt;
&lt;p&gt;
From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16269
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20915;&#31574;&#20248;&#21270;CoPilot&#65288;DOCP&#65289;&#65292;&#24110;&#21161;&#20915;&#31574;&#32773;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#29702;&#35299;&#24182;&#35299;&#20915;&#19994;&#21153;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22823;&#31616;&#21270;&#20026; real-world business problems &#21019;&#24314;&#20248;&#21270;&#27169;&#22411;&#19968;&#30452;&#26159;&#23558;&#25968;&#23398;&#20248;&#21270;&#26356;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#37325;&#35201;&#21830;&#19994;&#21644;&#31038;&#20250;&#20915;&#31574;&#30340;&#20027;&#35201;&#30446;&#26631;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#21450;&#26102;&#30340;&#26426;&#20250;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;LLMs&#21644;&#20248;&#21270;&#30340;&#20132;&#21449;&#28857;&#19978;&#24320;&#23637;&#30740;&#31350;&#65292;&#21019;&#24314;&#19968;&#20010;&#20915;&#31574;&#20248;&#21270;CoPilot&#65288;DOCP&#65289;- &#19968;&#20010;&#26088;&#22312;&#24110;&#21161;&#20219;&#20309;&#20915;&#31574;&#32773;&#30340;AI&#24037;&#20855;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26469;&#29702;&#35299;&#19994;&#21153;&#38382;&#39064;&#65292;&#38543;&#21518;&#21046;&#23450;&#21644;&#35299;&#20915;&#30456;&#24212;&#20248;&#21270;&#27169;&#22411;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;DOCP&#24895;&#26223;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#23454;&#26045;&#30340;&#20960;&#20010;&#22522;&#26412;&#35201;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#25991;&#29486;&#35843;&#26597;&#21644;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23454;&#39564;&#25551;&#36848;&#20102;&#29616;&#29366;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;a&#65289;LLMs&#24050;&#32463;&#25552;&#20379;&#20102;&#19982;DOCP&#30456;&#20851;&#30340;&#37325;&#22823;&#26032;&#21151;&#33021;&#65292;b&#65289;&#20027;&#35201;&#30740;&#31350; c
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16269v1 Announce Type: new  Abstract: Significantly simplifying the creation of optimization models for real-world business problems has long been a major goal in applying mathematical optimization more widely to important business and societal decisions. The recent capabilities of Large Language Models (LLMs) present a timely opportunity to achieve this goal. Therefore, we propose research at the intersection of LLMs and optimization to create a Decision Optimization CoPilot (DOCP) - an AI tool designed to assist any decision maker, interacting in natural language to grasp the business problem, subsequently formulating and solving the corresponding optimization model. This paper outlines our DOCP vision and identifies several fundamental requirements for its implementation. We describe the state of the art through a literature survey and experiments using ChatGPT. We show that a) LLMs already provide substantial novel capabilities relevant to a DOCP, and b) major research c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#30784;&#27169;&#22411;&#36879;&#26126;&#24230;&#25253;&#21578;&#65292;&#20511;&#37492;&#31038;&#20132;&#23186;&#20307;&#30340;&#36879;&#26126;&#24230;&#25253;&#21578;&#23454;&#36341;&#65292;&#30446;&#30340;&#22312;&#20110;&#22312;&#22522;&#30784;&#27169;&#22411;&#34892;&#19994;&#23578;&#26410;&#25104;&#29087;&#26102;&#21046;&#23450;&#36879;&#26126;&#24230;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2402.16268</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36879;&#26126;&#24230;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Foundation Model Transparency Reports
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#30784;&#27169;&#22411;&#36879;&#26126;&#24230;&#25253;&#21578;&#65292;&#20511;&#37492;&#31038;&#20132;&#23186;&#20307;&#30340;&#36879;&#26126;&#24230;&#25253;&#21578;&#23454;&#36341;&#65292;&#30446;&#30340;&#22312;&#20110;&#22312;&#22522;&#30784;&#27169;&#22411;&#34892;&#19994;&#23578;&#26410;&#25104;&#29087;&#26102;&#21046;&#23450;&#36879;&#26126;&#24230;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#20855;&#26377;&#24191;&#27867;&#31038;&#20250;&#24433;&#21709;&#30340;&#20851;&#38190;&#25968;&#23383;&#25216;&#26415;&#65292;&#38656;&#35201;&#36879;&#26126;&#24230;&#12290;&#20026;&#20102;&#35268;&#33539;&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#32773;&#24212;&#22914;&#20309;&#25552;&#20379;&#26377;&#20851;&#20854;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#36879;&#26126;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#30784;&#27169;&#22411;&#36879;&#26126;&#24230;&#25253;&#21578;&#65292;&#20511;&#37492;&#31038;&#20132;&#23186;&#20307;&#30340;&#36879;&#26126;&#24230;&#25253;&#21578;&#23454;&#36341;&#12290;&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#36879;&#26126;&#24230;&#25253;&#21578;&#26159;&#30001;&#22806;&#37096;&#23545;&#31038;&#20250;&#20260;&#23475;&#30340;&#25991;&#26723;&#20419;&#25104;&#30340;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#34892;&#19994;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#26102;&#20026;&#22522;&#30784;&#27169;&#22411;&#21046;&#23450;&#36879;&#26126;&#24230;&#25253;&#21578;&#12290;&#20026;&#35774;&#35745;&#25105;&#20204;&#30340;&#25253;&#21578;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;6&#26465;&#35774;&#35745;&#21407;&#21017;&#65292;&#32771;&#34385;&#20102;&#31038;&#20132;&#23186;&#20307;&#36879;&#26126;&#24230;&#25253;&#21578;&#30340;&#25104;&#21151;&#21644;&#19981;&#36275;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20351;&#25105;&#20204;&#30340;&#25253;&#21578;&#31995;&#32479;&#21270;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#22522;&#30784;&#27169;&#22411;&#36879;&#26126;&#24230;&#25351;&#25968;&#30340;100&#20010;&#36879;&#26126;&#24230;&#25351;&#26631;&#12290;&#26681;&#25454;&#36825;&#20123;&#25351;&#26631;&#65292;&#25105;&#20204;&#27979;&#37327;&#23427;&#20204;&#19982;&#20845;&#20010;&#20027;&#35201;&#36879;&#26126;&#24230;&#35201;&#27714;&#20013;&#21253;&#21547;&#30340;&#36879;&#26126;&#24230;&#35201;&#27714;&#30340;&#37325;&#21472;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16268v1 Announce Type: cross  Abstract: Foundation models are critical digital technologies with sweeping societal impact that necessitates transparency. To codify how foundation model developers should provide transparency about the development and deployment of their models, we propose Foundation Model Transparency Reports, drawing upon the transparency reporting practices in social media. While external documentation of societal harms prompted social media transparency reports, our objective is to institutionalize transparency reporting for foundation models while the industry is still nascent. To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting. To further schematize our reports, we draw upon the 100 transparency indicators from the Foundation Model Transparency Index. Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six promine
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#20013;&#21457;&#29616;&#24403;&#38754;&#23545;&#24322;&#26500;&#25968;&#25454;&#26102;&#65292;&#22240;&#23384;&#22312;&#26377;&#20559;&#30340;&#25237;&#24433;&#22836;&#23548;&#33268;&#30340;&#32852;&#37030;&#27169;&#22411;&#19981;&#21487;&#38752;&#24615;&#65292;&#25552;&#20986;&#20102;&#8220;&#32452;&#35013;&#25237;&#24433;&#22836;&#8221;&#65288;APH&#65289;&#26041;&#27861;&#20197;&#25552;&#39640;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16255</link><description>&lt;p&gt;
&#30041;&#24847;&#20320;&#30340;&#22836;&#37096;&#65306;&#32452;&#35013;&#25237;&#24433;&#22836;&#20197;&#25552;&#39640;&#32852;&#37030;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16255
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#21457;&#29616;&#24403;&#38754;&#23545;&#24322;&#26500;&#25968;&#25454;&#26102;&#65292;&#22240;&#23384;&#22312;&#26377;&#20559;&#30340;&#25237;&#24433;&#22836;&#23548;&#33268;&#30340;&#32852;&#37030;&#27169;&#22411;&#19981;&#21487;&#38752;&#24615;&#65292;&#25552;&#20986;&#20102;&#8220;&#32452;&#35013;&#25237;&#24433;&#22836;&#8221;&#65288;APH&#65289;&#26041;&#27861;&#20197;&#25552;&#39640;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#38754;&#23545;&#24322;&#26500;&#25968;&#25454;&#26102;&#36935;&#21040;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#21644;&#25910;&#25947;&#38382;&#39064;&#12290;&#23613;&#31649;&#22312;&#20943;&#36731;&#27492;&#31867;&#24433;&#21709;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#32852;&#37030;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#21364;&#34987;&#22823;&#22810;&#24573;&#35270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#36890;&#29992;&#21644;&#20010;&#24615;&#21270;&#32852;&#37030;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#25581;&#31034;&#20102;&#19968;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;\textbf{&#24403;&#38754;&#23545;&#24322;&#26500;&#25968;&#25454;&#26102;&#65292;&#32852;&#37030;&#27169;&#22411;&#34920;&#29616;&#20986;&#19981;&#21487;&#38752;&#24615;}&#65292;&#34920;&#29616;&#20026;&#23545;&#20998;&#24067;&#27979;&#35797;&#25968;&#25454;&#30340;&#36739;&#24046;&#26657;&#20934;&#21644;&#23545;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#20302;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#12290;&#36825;&#31181;&#19981;&#21487;&#38752;&#24615;&#20027;&#35201;&#24402;&#22240;&#20110;&#23384;&#22312;&#26377;&#20559;&#30340;&#25237;&#24433;&#22836;&#65292;&#36825;&#20123;&#25237;&#24433;&#22836;&#20026;&#32852;&#37030;&#27169;&#22411;&#24341;&#20837;&#20102;&#38169;&#35823;&#26657;&#20934;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;&#32852;&#37030;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#8220;&#32452;&#35013;&#25237;&#24433;&#22836;&#8221;&#65288;APH&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16255v1 Announce Type: cross  Abstract: Federated learning encounters substantial challenges with heterogeneous data, leading to performance degradation and convergence issues. While considerable progress has been achieved in mitigating such an impact, the reliability aspect of federated models has been largely disregarded. In this study, we conduct extensive experiments to investigate the reliability of both generic and personalized federated models. Our exploration uncovers a significant finding: \textbf{federated models exhibit unreliability when faced with heterogeneous data}, demonstrating poor calibration on in-distribution test data and low uncertainty levels on out-of-distribution data. This unreliability is primarily attributed to the presence of biased projection heads, which introduce miscalibration into the federated models. Inspired by this observation, we propose the "Assembled Projection Heads" (APH) method for enhancing the reliability of federated models. By
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#20064;&#24471;&#38382;&#39064;&#65288;CLAP&#65289;&#30340;&#26032;&#39062;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#22312;&#30446;&#26631;&#31038;&#21306;&#20013;&#20174;&#20114;&#21160;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#25918;&#23485;&#20102;Zero-Shot Coordination&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2402.16247</link><description>&lt;p&gt;
&#23398;&#20064;&#32763;&#35793;&#65306;&#24212;&#23545;&#21512;&#20316;&#35821;&#35328;&#20064;&#24471;&#30340;&#26032;&#20852;&#27807;&#36890;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16247
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#20064;&#24471;&#38382;&#39064;&#65288;CLAP&#65289;&#30340;&#26032;&#39062;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#22312;&#30446;&#26631;&#31038;&#21306;&#20013;&#20174;&#20114;&#21160;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#25918;&#23485;&#20102;Zero-Shot Coordination&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#20852;&#27807;&#36890;&#20013;&#65292;&#20195;&#29702;&#23398;&#20064;&#24444;&#27492;&#36827;&#34892;&#27807;&#36890;&#65292;&#20294;&#20182;&#20204;&#21046;&#23450;&#30340;&#21327;&#35758;&#26159;&#38024;&#23545;&#20182;&#20204;&#30340;&#35757;&#32451;&#32676;&#20307;&#30340;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#23548;&#33268;&#20102;&#23545;&#20110;&#23398;&#20064;&#23545;&#26410;&#22312;&#35757;&#32451;&#20013;&#36935;&#21040;&#30340;&#20195;&#29702;&#31283;&#20581;&#30340;&#27807;&#36890;&#31574;&#30053;&#30340;Zero-Shot Coordination&#65288;ZSC&#65289;&#30340;&#30740;&#31350;&#12290;&#20294;&#26159;&#65292;ZSC&#36890;&#24120;&#20551;&#35774;&#20851;&#20110;&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#20250;&#36935;&#21040;&#30340;&#20195;&#29702;&#30340;&#20808;&#21069;&#25968;&#25454;&#26159;&#26080;&#27861;&#33719;&#24471;&#30340;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#24517;&#35201;&#30340;&#26840;&#25163;&#38382;&#39064;&#65292;&#24182;&#25490;&#38500;&#20102;&#36890;&#36807;&#39044;&#20808;&#24314;&#31435;&#30340;&#32422;&#23450;&#36827;&#34892;&#27807;&#36890;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#20064;&#24471;&#38382;&#39064;&#65288;CLAP&#65289;&#30340;&#26032;&#39062;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#65292;&#22312;&#20854;&#20013;&#36890;&#36807;&#20801;&#35768;&#8220;&#21152;&#20837;&#32773;&#8221;&#20195;&#29702;&#20174;&#30446;&#26631;&#31038;&#21306;&#20869;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26469;&#25918;&#23485;&#20102;ZSC&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#35299;&#20915;CLAPs&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21644;&#26032;&#20852;&#27807;&#36890;&#30340;&#39044;&#35757;&#32451;&#21644;&#32763;&#35793;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16247v1 Announce Type: cross  Abstract: In Emergent Communication (EC) agents learn to communicate with one another, but the protocols that they develop are specialised to their training community. This observation led to research into Zero-Shot Coordination (ZSC) for learning communication strategies that are robust to agents not encountered during training. However, ZSC typically assumes that no prior data is available about the agents that will be encountered in the zero-shot setting. In many cases, this presents an unnecessarily hard problem and rules out communication via preestablished conventions. We propose a novel AI challenge called a Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions are relaxed by allowing a 'joiner' agent to learn from a dataset of interactions between agents in a target community. We propose and compare two methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication pretraining and Translation Learni
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#31163;&#25955;&#21270;&#30452;&#25509;&#22312;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#20013;&#24037;&#20316;&#30340;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#27963;&#36291;&#27700;&#24179;&#38598;&#20272;&#35745;&#31639;&#27861;</title><link>https://arxiv.org/abs/2402.16237</link><description>&lt;p&gt;
&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#20013;&#27963;&#36291;&#27700;&#24179;&#38598;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16237
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#31163;&#25955;&#21270;&#30452;&#25509;&#22312;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#20013;&#24037;&#20316;&#30340;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#27963;&#36291;&#27700;&#24179;&#38598;&#20272;&#35745;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#27700;&#24179;&#38598;&#20272;&#35745;&#65292;&#20854;&#30446;&#26631;&#26159;&#30830;&#23450;&#20989;&#25968;&#22495;&#20013;&#20989;&#25968;&#39640;&#20110;&#25110;&#20302;&#20110;&#32473;&#23450;&#38408;&#20540;&#30340;&#21306;&#22495;&#12290; &#24403;&#20989;&#25968;&#26159;&#40657;&#30418;&#19988;&#35780;&#20272;&#25104;&#26412;&#39640;&#26102;&#65292;&#38656;&#35201;&#22312;&#26368;&#23567;&#30340;&#20989;&#25968;&#35780;&#20272;&#38598;&#20013;&#25214;&#21040;&#27700;&#24179;&#38598;&#12290; &#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#20026;&#20855;&#26377;&#26377;&#38480;&#25968;&#25454;&#28857;&#38598;&#30340;&#31163;&#25955;&#25628;&#32034;&#31354;&#38388;&#65292;&#29992;&#20110;&#20989;&#25968;&#35780;&#20272;&#21644;&#20272;&#35745;&#27700;&#24179;&#38598;&#12290; &#24403;&#24212;&#29992;&#20110;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39318;&#20808;&#23545;&#31354;&#38388;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#36825;&#20250;&#23548;&#33268;&#32467;&#26524;&#19981;&#20339;&#65292;&#21516;&#26102;&#38656;&#35201;&#39640;&#35745;&#31639;&#26102;&#38388;&#12290; &#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#35774;&#23450;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#23545;&#29702;&#35770;&#25910;&#25947;&#30340;&#36866;&#24403;&#20445;&#35777;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#31639;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#31163;&#25955;&#21270;&#65292;&#21487;&#20197;&#30452;&#25509;&#22312;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#20013;&#24037;&#20316;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16237v1 Announce Type: cross  Abstract: A common problem encountered in many real-world applications is level set estimation where the goal is to determine the region in the function domain where the function is above or below a given threshold. When the function is black-box and expensive to evaluate, the level sets need to be found in a minimum set of function evaluations. Existing methods often assume a discrete search space with a finite set of data points for function evaluations and estimating the level sets. When applied to a continuous search space, these methods often need to first discretize the space which leads to poor results while needing high computational time. While some methods cater for the continuous setting, they still lack a proper guarantee for theoretical convergence. To address this problem, we propose a novel algorithm that does not need any discretization and can directly work in continuous search spaces. Our method suggests points by constructing 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;GARNNs&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#34880;&#31958;&#27700;&#24179;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#21464;&#37327;&#36129;&#29486;&#24635;&#32467;&#21644;&#29305;&#24449;&#22270;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.16230</link><description>&lt;p&gt;
GARNN: &#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#36890;&#36807;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#34880;&#31958;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;GARNNs&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#34880;&#31958;&#27700;&#24179;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#21464;&#37327;&#36129;&#29486;&#24635;&#32467;&#21644;&#29305;&#24449;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#39044;&#27979;&#26410;&#26469;&#34880;&#31958;&#65288;BG&#65289;&#27700;&#24179;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#34880;&#31958;&#31649;&#29702;&#65292;&#20174;&#32780;&#20943;&#23569;&#24182;&#21457;&#30151;&#65292;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22270;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;GARNNs&#65289;&#65292;&#29992;&#20110;&#24314;&#27169;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#65292;&#36890;&#36807;&#24635;&#32467;&#21464;&#37327;&#37325;&#35201;&#24615;&#35299;&#37322;&#21464;&#37327;&#36129;&#29486;&#65292;&#24182;&#36890;&#36807;&#22270;&#27880;&#24847;&#26426;&#21046;&#29983;&#25104;&#29305;&#24449;&#22270;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#20107;&#21518;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#19981;&#21516;&#20020;&#24202;&#22330;&#26223;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;GARNNs&#12290;&#19982;&#21313;&#20108;&#31181;&#20844;&#35748;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;GARNNs&#19981;&#20165;&#23454;&#29616;&#20102;BG&#39044;&#27979;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#36824;&#26356;&#20855;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16230v1 Announce Type: cross  Abstract: Accurate prediction of future blood glucose (BG) levels can effectively improve BG management for people living with diabetes, thereby reducing complications and improving quality of life. The state of the art of BG prediction has been achieved by leveraging advanced deep learning methods to model multi-modal data, i.e., sensor data and self-reported event data, organised as multi-variate time series (MTS). However, these methods are mostly regarded as ``black boxes'' and not entirely trusted by clinicians and patients. In this paper, we propose interpretable graph attentive recurrent neural networks (GARNNs) to model MTS, explaining variable contributions via summarizing variable importance and generating feature maps by graph attention mechanisms instead of post-hoc analysis. We evaluate GARNNs on four datasets, representing diverse clinical scenarios. Upon comparison with twelve well-established baseline methods, GARNNs not only ach
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;IR2&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#26597;&#35810;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50%&#12290;</title><link>https://arxiv.org/abs/2402.16200</link><description>&lt;p&gt;
IR2&#65306;&#20449;&#24687;&#27491;&#21017;&#21270;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
IR2: Information Regularization for Information Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16200
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;IR2&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#26597;&#35810;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22797;&#26434;&#26597;&#35810;&#65292;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IR2&#65292;&#21363;&#20449;&#24687;&#26816;&#32034;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#22797;&#26434;&#26597;&#35810;&#29305;&#24449;&#30340;&#19977;&#20010;&#26368;&#36817;&#30340;IR&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;DORIS-MAE&#12289;ArguAna&#21644;WhatsThatBook&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#19981;&#20165;&#22312;&#25152;&#32771;&#34385;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#21512;&#25104;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#33021;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50&#65285;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23558;&#19981;&#21516;&#38454;&#27573;&#30340;&#19977;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#8212;&#8212;&#36755;&#20837;&#12289;&#25552;&#31034;&#21644;&#36755;&#20986;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25506;&#32034;&#65292;&#27599;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#27809;&#26377;&#27491;&#21017;&#21270;&#30340;&#27169;&#22411;&#22343;&#25552;&#20379;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16200v1 Announce Type: cross  Abstract: Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regulariz
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#23436;&#25104;&#30495;&#23454;&#19990;&#30028;&#20195;&#30721;&#26102;&#30340;&#19977;&#31181;&#20844;&#20849;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#22312;&#32447;&#21644;&#31163;&#32447;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20026;&#33258;&#21160;&#20195;&#30721;&#34917;&#20840;&#30340;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.16197</link><description>&lt;p&gt;
&#29992;&#20110;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#23454;&#36341;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Language Models for Code Completion: A Practical Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16197
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#23436;&#25104;&#30495;&#23454;&#19990;&#30028;&#20195;&#30721;&#26102;&#30340;&#19977;&#31181;&#20844;&#20849;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#22312;&#32447;&#21644;&#31163;&#32447;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20026;&#33258;&#21160;&#20195;&#30721;&#34917;&#20840;&#30340;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#20195;&#30721;&#34917;&#20840;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#36825;&#20123;&#27169;&#22411;&#30340;&#35780;&#20272;&#24456;&#23569;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#23545;&#23436;&#25104;&#30495;&#23454;&#19990;&#30028;&#20195;&#30721;&#26102;&#30340;&#19977;&#31181;&#20844;&#20849;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;IDE&#25193;&#23637;&#24037;&#20855;&#65292;Code4Me&#65292;&#29992;&#20110;&#22312;&#32447;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#20174;1200&#22810;&#21517;&#29992;&#25143;&#37027;&#37324;&#25910;&#38598;&#20102;&#19968;&#24180;&#22810;&#30340;&#30495;&#23454;&#33258;&#21160;&#34917;&#20840;&#20351;&#29992;&#25968;&#25454;&#65292;&#20135;&#29983;&#20102;&#36229;&#36807;60&#19975;&#20010;&#26377;&#25928;&#30340;&#23436;&#25104;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20845;&#20010;&#26631;&#20934;&#25351;&#26631;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#21313;&#20108;&#31181;&#32534;&#31243;&#35821;&#35328;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23450;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;1690&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#23436;&#25104;&#35831;&#27714;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;&#36824;&#23545;&#27169;&#22411;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20351;&#29992;&#20004;&#31181;&#36974;&#34109;&#31574;&#30053;&#21644;&#22522;&#20934;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16197v1 Announce Type: cross  Abstract: Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies. Our findings suggest that while
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;OpenFOAM&#21644;SmartSim&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#21487;&#20280;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#24320;&#21457;CFD+ML&#31639;&#27861;&#65292;&#36890;&#36807;SmartSim&#23558;OpenFOAM&#30340;&#19981;&#21516;&#37096;&#20998;&#26377;&#25928;&#22320;&#19982;ML&#32806;&#21512;&#65292;&#21253;&#25324;&#39044;&#22788;&#29702;/&#21518;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#12289;&#27714;&#35299;&#22120;&#12289;&#20989;&#25968;&#23545;&#35937;&#21644;&#32593;&#26684;&#36816;&#21160;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.16196</link><description>&lt;p&gt;
&#32467;&#21512; OpenFOAM &#21644; SmartSim &#30340;&#26426;&#22120;&#23398;&#20064;&#19982;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Combining Machine Learning with Computational Fluid Dynamics using OpenFOAM and SmartSim
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16196
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;OpenFOAM&#21644;SmartSim&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#21487;&#20280;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#24320;&#21457;CFD+ML&#31639;&#27861;&#65292;&#36890;&#36807;SmartSim&#23558;OpenFOAM&#30340;&#19981;&#21516;&#37096;&#20998;&#26377;&#25928;&#22320;&#19982;ML&#32806;&#21512;&#65292;&#21253;&#25324;&#39044;&#22788;&#29702;/&#21518;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#12289;&#27714;&#35299;&#22120;&#12289;&#20989;&#25968;&#23545;&#35937;&#21644;&#32593;&#26684;&#36816;&#21160;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#19982;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;CFD&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#25913;&#36827;&#25216;&#26415;&#21644;&#33258;&#28982;&#31995;&#32479;&#30340;&#27169;&#25311;&#25171;&#24320;&#20102;&#35768;&#22810;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;CFD+ML&#31639;&#27861;&#38656;&#35201;&#22312;&#24322;&#26500;&#30828;&#20214;&#19978;&#20132;&#25442;&#25968;&#25454;&#12289;&#21516;&#27493;&#21644;&#35745;&#31639;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#30340;&#23454;&#29616;&#24322;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#21487;&#20280;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#24320;&#28304;&#36719;&#20214;OpenFOAM&#21644;SmartSim&#24320;&#21457;CFD+ML&#31639;&#27861;&#12290;SmartSim&#25552;&#20379;&#20102;&#19968;&#20010;&#32534;&#25490;&#22120;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#32534;&#31243;CFD+ML&#31639;&#27861;&#30340;&#36807;&#31243;&#65292;&#20197;&#21450;&#19968;&#20010;Redis&#25968;&#25454;&#24211;&#65292;&#30830;&#20445;ML&#21644;CFD&#23458;&#25143;&#31471;&#20043;&#38388;&#39640;&#24230;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#20132;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;SmartSim&#23558;OpenFOAM&#30340;&#19981;&#21516;&#37096;&#20998;&#26377;&#25928;&#22320;&#19982;ML&#32806;&#21512;&#65292;&#21253;&#25324;&#39044;&#22788;&#29702;/&#21518;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#12289;&#27714;&#35299;&#22120;&#12289;&#20989;&#25968;&#23545;&#35937;&#21644;&#32593;&#26684;&#36816;&#21160;&#27714;&#35299;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;OpenFOAM&#23376;&#27169;&#22359;&#65292;&#20854;&#20013;&#21253;&#21547;&#21487;&#29992;&#20316;&#36215;&#22987;&#28857;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16196v1 Announce Type: new  Abstract: Combining machine learning (ML) with computational fluid dynamics (CFD) opens many possibilities for improving simulations of technical and natural systems. However, CFD+ML algorithms require exchange of data, synchronization, and calculation on heterogeneous hardware, making their implementation for large-scale problems exceptionally challenging.   We provide an effective and scalable solution to developing CFD+ML algorithms using open source software OpenFOAM and SmartSim. SmartSim provides an Orchestrator that significantly simplifies the programming of CFD+ML algorithms and a Redis database that ensures highly scalable data exchange between ML and CFD clients. We show how to leverage SmartSim to effectively couple different segments of OpenFOAM with ML, including pre/post-processing applications, solvers, function objects, and mesh motion solvers. We additionally provide an OpenFOAM sub-module with examples that can be used as starti
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#34429;&#28982;&#20855;&#26377;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#31561;&#20248;&#28857;&#65292;&#20294;&#20063;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#23454;&#29992;&#25351;&#21335;&#20197;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.16187</link><description>&lt;p&gt;
&#21033;&#29992;&#20854;&#20248;&#21183;&#25915;&#20987;LLM&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Attacking LLM Watermarks by Exploiting Their Strengths
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16187
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#34429;&#28982;&#20855;&#26377;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#31561;&#20248;&#28857;&#65292;&#20294;&#20063;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#23454;&#29992;&#25351;&#21335;&#20197;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#22270;&#29255;&#33021;&#22815;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27700;&#21360;&#25216;&#26415;&#26088;&#22312;&#23558;&#20449;&#24687;&#23884;&#20837;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#20197;&#39564;&#35777;&#20854;&#26469;&#28304;&#65292;&#23545;&#20110;&#20943;&#23569;&#23545;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28389;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#26696;&#20173;&#28982;&#20196;&#20154;&#24847;&#22806;&#22320;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#20849;&#20139;&#30340;&#21487;&#21462;&#29305;&#24615;&#65292;&#20363;&#22914;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#65292;&#21453;&#36807;&#26469;&#21364;&#20351;&#36825;&#20123;&#31995;&#32479;&#23481;&#26131;&#36973;&#21463;&#21508;&#31181;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#24120;&#35265;&#27700;&#21360;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#20005;&#26684;&#30740;&#31350;&#28508;&#22312;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25915;&#20987;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#38450;&#24481;&#25514;&#26045;&#8212;&#8212;&#24314;&#31435;&#20102;&#19968;&#22871;&#23884;&#20837;&#21644;&#26816;&#27979;LLM&#27700;&#21360;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16187v1 Announce Type: cross  Abstract: Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#23485;&#39640;&#26031;&#36807;&#31243;&#26497;&#38480;&#20998;&#26512;&#38543;&#26426;&#21021;&#22987;&#21270;&#26102;&#35825;&#23548;&#38544;&#34255;&#36755;&#20986;&#31232;&#30095;&#34892;&#20026;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16184</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#35825;&#23548;&#28608;&#27963;&#21021;&#22987;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Network Initialization with Sparsity Inducing Activations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#23485;&#39640;&#26031;&#36807;&#31243;&#26497;&#38480;&#20998;&#26512;&#38543;&#26426;&#21021;&#22987;&#21270;&#26102;&#35825;&#23548;&#38544;&#34255;&#36755;&#20986;&#31232;&#30095;&#34892;&#20026;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#35825;&#23548;&#21644;&#21033;&#29992;&#31232;&#30095;&#28608;&#27963;&#26159;&#25913;&#21892;&#28145;&#24230;&#32593;&#32476;&#35745;&#31639;&#25928;&#29575;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#65292;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#24212;&#29992;&#33539;&#22260;&#30340;&#25193;&#22823;&#65292;&#36825;&#19968;&#28857;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#21033;&#29992;&#22823;&#23485;&#39640;&#26031;&#36807;&#31243;&#26497;&#38480;&#26469;&#20998;&#26512;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#26102;&#35825;&#23548;&#38544;&#34255;&#36755;&#20986;&#31232;&#30095;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20808;&#21069;&#26410;&#25253;&#36947;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24418;&#24335;&#65292;&#38024;&#23545;&#38544;&#34255;&#23618;&#31232;&#30095;&#21270;&#30340;&#20004;&#20010;&#26368;&#33258;&#28982;&#30340;&#20505;&#36873;&#32773;&#65306;&#31227;&#20301;ReLU&#65288;$\phi(x)=\max(0, x-\tau)$&#65292;&#20854;&#20013;$\tau\ge 0$&#65289;&#21644;&#36719;&#38408;&#20540;&#65288;$\phi(x)=0$&#65292;&#24403;$|x|\le\tau$&#26102;&#65292;$x-\text{sign}(x)\tau$&#65292;&#24403;$|x|&gt;\tau$&#26102;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#28608;&#27963;&#24133;&#24230;&#20462;&#21098;&#21040;&#30001;&#30456;&#20851;&#39640;&#26031;&#36807;&#31243;&#26041;&#24046;&#22270;&#30340;&#24418;&#29366;&#35268;&#23450;&#30340;&#27700;&#24179;&#19978;&#34987;&#20811;&#26381;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16184v1 Announce Type: new  Abstract: Inducing and leveraging sparse activations during training and inference is a promising avenue for improving the computational efficiency of deep networks, which is increasingly important as network sizes continue to grow and their application becomes more widespread. Here we use the large width Gaussian process limit to analyze the behaviour, at random initialization, of nonlinear activations that induce sparsity in the hidden outputs. A previously unreported form of training instability is proven for arguably two of the most natural candidates for hidden layer sparsification; those being a shifted ReLU ($\phi(x)=\max(0, x-\tau)$ for $\tau\ge 0$) and soft thresholding ($\phi(x)=0$ for $|x|\le\tau$ and $x-\text{sign}(x)\tau$ for $|x|&gt;\tau$). We show that this instability is overcome by clipping the nonlinear activation magnitude, at a level prescribed by the shape of the associated Gaussian process variance map. Numerical experiments ver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#30340;&#31574;&#30053;&#20808;&#39564;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16181</link><description>&lt;p&gt;
LLM&#22914;&#20309;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#65311;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
How Can LLM Guide RL? A Value-Based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#30340;&#31574;&#30053;&#20808;&#39564;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#25104;&#20026;&#36890;&#36807;&#25913;&#36827;&#26410;&#26469;&#30340;&#34892;&#21160;&#31574;&#30053;&#26469;&#35299;&#20915;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#30340;&#20107;&#23454;&#26631;&#20934;&#23454;&#36341;&#65292;&#20294;&#26159;RL&#31639;&#27861;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#20132;&#20114;&#26469;&#25910;&#38598;&#26377;&#29992;&#30340;&#21453;&#39304;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#25506;&#32034;&#21644;&#33258;&#25105;&#25913;&#36827;&#35268;&#21010;&#20219;&#21153;&#30340;&#33021;&#21147;&#19978;&#20173;&#23384;&#22312;&#19981;&#36275;&#65292;&#32570;&#20047;&#22522;&#20110;&#21453;&#39304;&#33258;&#20027;&#25913;&#36827;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#25552;&#20379;&#30340;&#31574;&#30053;&#20808;&#39564;&#22914;&#20309;&#22686;&#24378;RL&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;LINVIT&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;LLM&#24341;&#23548;&#20316;&#20026;&#20215;&#20540;&#22411;RL&#20013;&#30340;&#27491;&#21017;&#21270;&#22240;&#23376;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#23398;&#20064;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#65292;&#29305;&#21035;&#26159;&#24403;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16181v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback. However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement. On the other hand, recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback. Therefore, in this paper, we study how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms. Specifically, we develop an algorithm named LINVIT that incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#26080;&#20851;&#20844;&#24179;&#23398;&#20064;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;FedFaiREE&#65292;&#36866;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#20855;&#26377;&#23567;&#26679;&#26412;&#30340;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2402.16158</link><description>&lt;p&gt;
&#20998;&#24067;&#26080;&#20851;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#19982;&#23567;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Distribution-Free Fair Federated Learning with Small Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#26080;&#20851;&#20844;&#24179;&#23398;&#20064;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;FedFaiREE&#65292;&#36866;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#20855;&#26377;&#23567;&#26679;&#26412;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#35757;&#32451;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#36328;&#32676;&#20307;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29992;&#20110;&#30830;&#20445;&#20844;&#24179;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#20026;&#38598;&#20013;&#21270;&#25968;&#25454;&#29615;&#22659;&#35774;&#35745;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#26679;&#26412;&#21644;&#20998;&#24067;&#20551;&#35774;&#65292;&#24378;&#35843;&#20102;&#36843;&#20999;&#38656;&#35201;&#38024;&#23545;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#20998;&#24067;&#26080;&#20851;&#20445;&#35777;&#30340;&#21435;&#20013;&#24515;&#21270;&#21644;&#24322;&#26500;&#31995;&#32479;&#36827;&#34892;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#35843;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;FedFaiREE&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#23567;&#26679;&#26412;&#30340;&#20998;&#24067;&#26080;&#20851;&#20844;&#24179;&#23398;&#20064;&#30340;&#21518;&#22788;&#29702;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20363;&#22914;&#23458;&#25143;&#24322;&#36136;&#24615;&#12289;&#36890;&#20449;&#25104;&#26412;&#21644;&#23567;&#26679;&#26412;&#22823;&#23567;&#12290;&#25105;&#20204;&#20026;bot&#25552;&#20379;&#20005;&#26684;&#30340;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16158v1 Announce Type: cross  Abstract: As federated learning gains increasing importance in real-world applications due to its capacity for decentralized data training, addressing fairness concerns across demographic groups becomes critically important. However, most existing machine learning algorithms for ensuring fairness are designed for centralized data environments and generally require large-sample and distributional assumptions, underscoring the urgent need for fairness techniques adapted for decentralized and heterogeneous systems with finite-sample and distribution-free guarantees. To address this issue, this paper introduces FedFaiREE, a post-processing algorithm developed specifically for distribution-free fair learning in decentralized settings with small samples. Our approach accounts for unique challenges in decentralized environments, such as client heterogeneity, communication costs, and small sample sizes. We provide rigorous theoretical guarantees for bot
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20849;&#35782;&#23398;&#20064;&#36825;&#19968;&#20840;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#32467;&#21512;&#32463;&#20856;&#38598;&#25104;&#26041;&#27861;&#19982;&#20849;&#35782;&#21327;&#35758;&#65292;&#22312;&#20445;&#35777;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16157</link><description>&lt;p&gt;
&#20849;&#35782;&#23398;&#20064;&#65306;&#19968;&#31181;&#20840;&#26032;&#30340;&#20998;&#25955;&#24335;&#38598;&#25104;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Consensus learning: A novel decentralised ensemble learning paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16157
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20849;&#35782;&#23398;&#20064;&#36825;&#19968;&#20840;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#32467;&#21512;&#32463;&#20856;&#38598;&#25104;&#26041;&#27861;&#19982;&#20849;&#35782;&#21327;&#35758;&#65292;&#22312;&#20445;&#35777;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24191;&#27867;&#37319;&#29992;&#31361;&#26174;&#20102;&#20998;&#24067;&#24335;&#35745;&#31639;&#22312;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#19978;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;--&#20849;&#35782;&#23398;&#20064;--&#23558;&#32463;&#20856;&#38598;&#25104;&#26041;&#27861;&#19982;&#37096;&#32626;&#22312;&#23545;&#31561;&#31995;&#32479;&#20013;&#30340;&#20849;&#35782;&#21327;&#35758;&#30456;&#32467;&#21512;&#12290;&#36825;&#20123;&#31639;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#65292;&#21442;&#19982;&#32773;&#24320;&#21457;&#20182;&#20204;&#30340;&#27169;&#22411;&#24182;&#20026;&#20219;&#20309;&#26032;&#25968;&#25454;&#36755;&#20837;&#25552;&#20132;&#39044;&#27979;&#65307;&#20854;&#27425;&#65292;&#20010;&#20307;&#39044;&#27979;&#34987;&#29992;&#20316;&#36890;&#20449;&#38454;&#27573;&#30340;&#36755;&#20837;&#65292;&#35813;&#38454;&#27573;&#30001;&#20849;&#35782;&#21327;&#35758;&#25511;&#21046;&#12290;&#20849;&#35782;&#23398;&#20064;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#65292;&#21516;&#26102;&#20063;&#32487;&#25215;&#20102;&#22522;&#30784;&#20849;&#35782;&#26426;&#21046;&#23545;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#23433;&#20840;&#25514;&#26045;&#12290;&#25105;&#20204;&#20026;&#29305;&#23450;&#30340;&#20849;&#35782;&#21327;&#35758;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#23558;&#20849;&#35782;&#23398;&#20064;&#38598;&#25104;&#30340;&#24615;&#33021;&#19982;&#20013;&#24515;&#21270;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16157v1 Announce Type: new  Abstract: The widespread adoption of large-scale machine learning models in recent years highlights the need for distributed computing for efficiency and scalability. This work introduces a novel distributed machine learning paradigm -- \emph{consensus learning} -- which combines classical ensemble methods with consensus protocols deployed in peer-to-peer systems. These algorithms consist of two phases: first, participants develop their models and submit predictions for any new data inputs; second, the individual predictions are used as inputs for a communication phase, which is governed by a consensus protocol. Consensus learning ensures user data privacy, while also inheriting the safety measures against Byzantine attacks from the underlying consensus mechanism. We provide a detailed theoretical analysis for a particular consensus protocol and compare the performance of the consensus learning ensemble with centralised ensemble learning algorithm
&lt;/p&gt;</description></item><item><title>ChatMusician &#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#20869;&#22312;&#38899;&#20048;&#33021;&#21147;&#30340;&#24320;&#28304;LLM&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#20860;&#23481;&#30340;&#38899;&#20048;&#34920;&#31034;&#27861;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#20048;&#65292;&#34920;&#29616;&#20248;&#20110;GPT-4&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.16153</link><description>&lt;p&gt;
ChatMusician&#65306;&#29702;&#35299;&#21644;&#29983;&#25104;&#20855;&#26377;LLM&#30340;&#38899;&#20048;&#20869;&#22312;
&lt;/p&gt;
&lt;p&gt;
ChatMusician: Understanding and Generating Music Intrinsically with LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16153
&lt;/p&gt;
&lt;p&gt;
ChatMusician &#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#20869;&#22312;&#38899;&#20048;&#33021;&#21147;&#30340;&#24320;&#28304;LLM&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#20860;&#23481;&#30340;&#38899;&#20048;&#34920;&#31034;&#27861;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#20048;&#65292;&#34920;&#29616;&#20248;&#20110;GPT-4&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#33021;&#21147;&#23578;&#26410;&#25512;&#24191;&#21040;&#38899;&#20048;&#65292;&#20063;&#23601;&#26159;&#20154;&#31867;&#30340;&#21019;&#36896;&#24615;&#35821;&#35328;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ChatMusician&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#65292;&#38598;&#25104;&#20102;&#20869;&#22312;&#30340;&#38899;&#20048;&#33021;&#21147;&#12290;&#23427;&#22522;&#20110;&#23545;&#25991;&#26412;&#20860;&#23481;&#30340;&#38899;&#20048;&#34920;&#31034;&#27861;ABC&#35760;&#35889;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;LLaMA2&#65292;&#24182;&#19988;&#23558;&#38899;&#20048;&#35270;&#20026;&#31532;&#20108;&#35821;&#35328;&#12290;ChatMusician&#21487;&#20197;&#20351;&#29992;&#32431;&#25991;&#26412;&#26631;&#35760;&#22120;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#20048;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#22806;&#37096;&#22810;&#27169;&#24577;&#31070;&#32463;&#32467;&#26500;&#25110;&#26631;&#35760;&#22120;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36171;&#20104;&#38899;&#20048;&#33021;&#21147;&#24182;&#19981;&#20250;&#25439;&#23475;&#35821;&#35328;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#36798;&#21040;&#30053;&#39640;&#30340;MMLU&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#12289;&#21644;&#24358;&#12289;&#26059;&#24459;&#12289;&#20027;&#39064;&#12289;&#38899;&#20048;&#24418;&#24335;&#31561;&#21019;&#20316;&#32467;&#26500;&#33391;&#22909;&#12289;&#23436;&#25972;&#38271;&#24230;&#30340;&#38899;&#20048;&#65292;&#36229;&#36234;&#20102;GPT-4&#30340;&#22522;&#32447;&#12290;&#22312;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#30340;&#22823;&#23398;&#32423;&#38899;&#20048;&#29702;&#35299;&#22522;&#20934;&#19978;&#65292;MusicTheory
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16153v1 Announce Type: cross  Abstract: While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language. We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score. Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline. On our meticulously curated college-level music understanding benchmark, MusicTheory
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;VAE&#30340;&#26694;&#26550;&#65292;&#21487;&#32852;&#21512;&#23398;&#20064;&#19968;&#32452;&#30456;&#20851;&#20294;&#24322;&#26500;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#20197;&#21407;&#21017;&#24615;&#26041;&#24335;&#22788;&#29702;&#25552;&#21462;&#20849;&#20139;&#32467;&#26500;&#21644;&#35782;&#21035;&#20010;&#20307;&#29305;&#24615;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.16131</link><description>&lt;p&gt;
&#22522;&#20110;VAE&#30340;&#22810;&#23618;&#31070;&#32463;Granger-&#22240;&#26524;&#36830;&#25509;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16131
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;VAE&#30340;&#26694;&#26550;&#65292;&#21487;&#32852;&#21512;&#23398;&#20064;&#19968;&#32452;&#30456;&#20851;&#20294;&#24322;&#26500;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#20197;&#21407;&#21017;&#24615;&#26041;&#24335;&#22788;&#29702;&#25552;&#21462;&#20849;&#20139;&#32467;&#26500;&#21644;&#35782;&#21035;&#20010;&#20307;&#29305;&#24615;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Granger&#22240;&#26524;&#20851;&#31995;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#29992;&#20110;&#25429;&#25417;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#32452;&#20214;&#20043;&#38388;&#30340;&#20808;&#23548;-&#28382;&#21518;&#20851;&#31995;&#65292;&#29616;&#26377;&#25991;&#29486;&#30340;&#37325;&#28857;&#38598;&#20013;&#22312;&#21333;&#19968;&#21160;&#24577;&#31995;&#32479;&#19978;&#12290;&#22312;&#23439;&#35266;&#32463;&#27982;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#21487;&#20197;&#35775;&#38382;&#26469;&#33258;&#19968;&#32452;&#30456;&#20851;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;&#24863;&#20852;&#36259;&#30340;&#24314;&#27169;&#20219;&#21153;&#26159;&#25552;&#21462;&#23884;&#20837;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#20849;&#20139;&#20844;&#20849;&#32467;&#26500;&#65292;&#20197;&#21450;&#35782;&#21035;&#21508;&#33258;&#31995;&#32479;&#20013;&#30340;&#29305;&#36136;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#32852;&#21512;&#23398;&#20064;&#20102;&#19968;&#32452;&#30456;&#20851;&#20294;&#24322;&#26500;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#20197;&#21407;&#21017;&#24615;&#26041;&#24335;&#22788;&#29702;&#19978;&#36848;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20960;&#31181;&#21512;&#25104;&#25968;&#25454;&#35774;&#32622;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20026;&#21333;&#20010;&#21160;&#24577;&#31995;&#32479;&#35774;&#35745;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16131v1 Announce Type: new  Abstract: Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system. In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones. This paper introduces a Variational Autoencoder (VAE) based framework that jointly learns Granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way. The performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual s
&lt;/p&gt;</description></item><item><title>InstructEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#65292;&#36890;&#36807;&#31616;&#21333;&#25351;&#20196;&#20351;&#32534;&#36753;&#22120;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#32534;&#36753;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16123</link><description>&lt;p&gt;
InstructEdit&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
InstructEdit: Instruction-based Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16123
&lt;/p&gt;
&lt;p&gt;
InstructEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#65292;&#36890;&#36807;&#31616;&#21333;&#25351;&#20196;&#20351;&#32534;&#36753;&#22120;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#32534;&#36753;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#32780;&#19981;&#20250;&#23545;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#28040;&#26497;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#36328;&#20219;&#21153;&#30340;&#36890;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#35774;&#35745;&#19968;&#20010;&#29420;&#29305;&#30340;&#32534;&#36753;&#22120;&#65292;&#36825;&#26174;&#33879;&#38459;&#30861;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#30693;&#35782;&#32534;&#36753;&#20013;&#30340;&#22810;&#20219;&#21153;&#27867;&#21270;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#31216;&#20026;InstructEdit&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25351;&#20196;&#20419;&#36827;&#32534;&#36753;&#22120;&#21516;&#26102;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#21482;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#32534;&#36753;&#22120;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#26041;&#38754;&#34920;&#26126;&#65292;InstructEdit&#21487;&#20197;&#25552;&#39640;&#32534;&#36753;&#22120;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#22810;&#20219;&#21153;&#32534;&#36753;&#35774;&#32622;&#20013;&#24179;&#22343;&#25552;&#39640;&#21487;&#38752;&#24615;14.86%&#12290;&#27492;&#22806;&#65292;&#28041;&#21450;&#20445;&#30041;&#26410;&#35265;&#20219;&#21153;&#30340;&#23454;&#39564;&#35828;&#26126;&#65292;InstructEdi
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16123v1 Announce Type: cross  Abstract: Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#21517;&#20026;DeepForge&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38381;&#27169;&#28909;&#38203;&#36896;&#36807;&#31243;&#20013;&#36890;&#36807;&#20351;&#29992;&#34920;&#38754;&#28201;&#24230;&#27979;&#37327;&#39044;&#27979;&#24494;&#35266;&#32452;&#32455;&#30340;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;DeepForge&#33021;&#22815;&#20197;&#26497;&#20302;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#39044;&#27979;&#24494;&#35266;&#32452;&#32455;&#65292;&#24182;&#25506;&#32034;&#20102;&#20351;&#29992;MPC&#35843;&#25972;&#31561;&#24453;&#26102;&#38388;&#26469;&#23454;&#29616;&#22312;&#29305;&#23450;&#24037;&#20214;&#21306;&#22495;&#20869;&#36798;&#21040;&#30446;&#26631;&#26230;&#31890;&#23610;&#23544;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16119</link><description>&lt;p&gt;
&#21033;&#29992;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#28145;&#24230;&#38203;&#36896;&#65306;&#22312;&#37329;&#23646;&#25104;&#24418;&#20013;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#24494;&#35266;&#32452;&#32455;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DeepForge: Leveraging AI for Microstructural Control in Metal Forming via Model Predictive Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16119
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#21517;&#20026;DeepForge&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38381;&#27169;&#28909;&#38203;&#36896;&#36807;&#31243;&#20013;&#36890;&#36807;&#20351;&#29992;&#34920;&#38754;&#28201;&#24230;&#27979;&#37327;&#39044;&#27979;&#24494;&#35266;&#32452;&#32455;&#30340;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;DeepForge&#33021;&#22815;&#20197;&#26497;&#20302;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#39044;&#27979;&#24494;&#35266;&#32452;&#32455;&#65292;&#24182;&#25506;&#32034;&#20102;&#20351;&#29992;MPC&#35843;&#25972;&#31561;&#24453;&#26102;&#38388;&#26469;&#23454;&#29616;&#22312;&#29305;&#23450;&#24037;&#20214;&#21306;&#22495;&#20869;&#36798;&#21040;&#30446;&#26631;&#26230;&#31890;&#23610;&#23544;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38381;&#27169;&#28909;&#38203;&#36896;&#20013;&#23454;&#29616;&#24494;&#35266;&#32452;&#32455;&#25511;&#21046;&#65292;&#23558;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#19982;&#19968;&#31181;&#21517;&#20026;DeepForge&#30340;&#24320;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;DeepForge&#20351;&#29992;&#32467;&#21512;&#20102;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#26550;&#26500;&#12290;&#23427;&#20351;&#29992;&#24037;&#20214;&#34920;&#38754;&#28201;&#24230;&#27979;&#37327;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#39044;&#27979;&#38203;&#36896;&#36807;&#31243;&#20013;&#30340;&#24494;&#35266;&#32452;&#32455;&#21464;&#21270;&#12290;&#35770;&#25991;&#36824;&#35814;&#32454;&#20171;&#32461;&#20102;DeepForge&#30340;&#26550;&#26500;&#20197;&#21450;&#29992;&#20110;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#20803;&#27169;&#25311;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#19977;&#34892;&#31243;&#38203;&#36896;&#24037;&#33402;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DeepForge&#33021;&#22815;&#20197;0.4&#177;0.3%&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#39044;&#27979;&#24494;&#35266;&#32452;&#32455;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;MPC&#35843;&#25972;&#34892;&#31243;&#20043;&#38388;&#30340;&#31561;&#24453;&#26102;&#38388;&#65292;&#26377;&#25928;&#22320;&#25269;&#28040;&#28201;&#24230;&#25200;&#21160;&#65292;&#23454;&#29616;&#22312;&#24037;&#20214;&#30340;&#29305;&#23450;2D&#21306;&#22495;&#20869;&#23558;&#26230;&#31890;&#23610;&#23544;&#25511;&#21046;&#22312;35&#24494;&#31859;&#20197;&#19979;&#30340;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#32467;&#26524;&#32463;&#36807;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16119v1 Announce Type: new  Abstract: This study presents a novel method for microstructure control in closed die hot forging that combines Model Predictive Control (MPC) with a developed machine learning model called DeepForge. DeepForge uses an architecture that combines 1D convolutional neural networks and gated recurrent units. It uses surface temperature measurements of a workpiece as input to predict microstructure changes during forging. The paper also details DeepForge's architecture and the finite element simulation model used to generate the data set, using a three-stroke forging process. The results demonstrate DeepForge's ability to predict microstructure with a mean absolute error of 0.4$\pm$0.3%. In addition, the study explores the use of MPC to adjust inter-stroke wait times, effectively counteracting temperature disturbances to achieve a target grain size of less than 35 microns within a specific 2D region of the workpiece. These results are then verified exp
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#36825;&#19968;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#20849;&#20139;&#65292;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25269;&#24481;&#35266;&#27979;&#22122;&#22768;&#12290;</title><link>https://arxiv.org/abs/2402.16105</link><description>&lt;p&gt;
&#36890;&#30693;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Informed Meta-Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#36825;&#19968;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#20849;&#20139;&#65292;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25269;&#24481;&#35266;&#27979;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#30427;&#34892;&#30340;&#22024;&#26434;&#21644;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#31361;&#20986;&#30340;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#22320;&#34701;&#21512;&#20419;&#36827;&#25968;&#25454;&#25928;&#29575;&#21644;&#31283;&#20581;&#24615;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#20803;&#23398;&#20064;&#21644;&#36890;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#20004;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#26041;&#27861;&#12290;&#21069;&#32773;&#20381;&#36182;&#20110;&#19968;&#31181;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#26469;&#28304;&#65292;&#32780;&#21518;&#32773;&#21463;&#19987;&#23478;&#30693;&#35782;&#30340;&#24418;&#24335;&#21270;&#34920;&#31034;&#24341;&#23548;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#33539;&#24335;&#65292;&#36890;&#30693;&#20803;&#23398;&#20064;&#65292;&#26088;&#22312;&#23454;&#29616;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#36328;&#20219;&#21153;&#30693;&#35782;&#20849;&#20139;&#30340;&#20114;&#34917;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#19968;&#26694;&#26550;&#30340;&#20855;&#20307;&#23454;&#20363;--&#36890;&#30693;&#31070;&#32463;&#36807;&#31243;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35828;&#26126;&#24615;&#21644;&#26356;&#22823;&#35268;&#27169;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#22312;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25269;&#24481;&#35266;&#27979;&#22122;&#22768;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16105v1 Announce Type: new  Abstract: In noisy and low-data regimes prevalent in real-world applications, an outstanding challenge of machine learning lies in effectively incorporating inductive biases that promote data efficiency and robustness. Meta-learning and informed ML stand out as two approaches for incorporating prior knowledge into the ML pipeline. While the former relies on a purely data-driven source of priors, the latter is guided by a formal representation of expert knowledge. This paper introduces a novel hybrid paradigm, informed meta-learning, seeking complementarity in cross-task knowledge sharing of humans and machines. We establish the foundational components of informed meta-learning and present a concrete instantiation of this framework--the Informed Neural Process. Through a series of illustrative and larger-scale experiments, we demonstrate the potential benefits of informed meta-learning in improving data efficiency and robustness to observational no
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#20803;&#32032;&#32423;&#21035;&#32780;&#38750;&#20256;&#32479;&#30340;&#23618;&#32423;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#20197;&#36873;&#25321;&#20010;&#24615;&#21270;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.16091</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#21442;&#25968;&#36873;&#25321;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Network For Personalized Federated Learning Parameter Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16091
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#20803;&#32032;&#32423;&#21035;&#32780;&#38750;&#20256;&#32479;&#30340;&#23618;&#32423;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#20197;&#36873;&#25321;&#20010;&#24615;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#26102;&#24615;&#33021;&#19981;&#20339;&#20173;&#28982;&#26159;&#35813;&#39046;&#22495;&#26368;&#20026;&#32039;&#36843;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20559;&#31163;&#20256;&#32479;&#33539;&#24335;&#65292;&#20854;&#20013;&#25152;&#26377;&#23458;&#25143;&#31471;&#20351;&#29992;&#30456;&#21516;&#27169;&#22411;&#65292;&#32780;&#26159;&#21162;&#21147;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#21457;&#29616;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#24615;&#12290;&#19968;&#31181;&#26041;&#27861;&#28041;&#21450;&#20010;&#24615;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#23450;&#23618;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#24182;&#27809;&#26377;&#25552;&#20379;&#21487;&#38752;&#30340;&#29702;&#30001;&#65292;&#26377;&#20123;&#36873;&#25321;&#20102;&#23436;&#20840;&#19981;&#21516;&#19988;&#30456;&#20114;&#20914;&#31361;&#30340;&#20010;&#24615;&#21270;&#23618;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26356;&#36827;&#19968;&#27493;&#65292;&#25552;&#35758;&#22312;&#20803;&#32032;&#32423;&#21035;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#23618;&#32423;&#20010;&#24615;&#21270;&#12290;&#20026;&#20102;&#36873;&#25321;&#20010;&#24615;&#21270;&#21442;&#25968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20381;&#36182;&#23427;&#20204;&#25552;&#20379;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25351;&#23548;&#25105;&#20204;&#36873;&#25321;&#20010;&#24615;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16091v1 Announce Type: cross  Abstract: Federated learning's poor performance in the presence of heterogeneous data remains one of the most pressing issues in the field. Personalized federated learning departs from the conventional paradigm in which all clients employ the same model, instead striving to discover an individualized model for each client to address the heterogeneity in the data. One of such approach involves personalizing specific layers of neural networks. However, prior endeavors have not provided a dependable rationale, and some have selected personalized layers that are entirely distinct and conflicting. In this work, we take a step further by proposing personalization at the elemental level, rather than the traditional layer-level personalization. To select personalized parameters, we introduce Bayesian neural networks and rely on the uncertainty they offer to guide our selection of personalized parameters. Finally, we validate our algorithm's efficacy on 
&lt;/p&gt;</description></item><item><title>&#26080;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#30740;&#31350;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12289;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#20027;&#24178;&#26550;&#26500;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#20027;&#24178;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#36873;&#25321;&#23545;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2402.16090</link><description>&lt;p&gt;
&#26080;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#65306;&#28145;&#20837;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Key Design Choices in Source-Free Unsupervised Domain Adaptation: An In-depth Empirical Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16090
&lt;/p&gt;
&lt;p&gt;
&#26080;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#30740;&#31350;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12289;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#20027;&#24178;&#26550;&#26500;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#20027;&#24178;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#36873;&#25321;&#23545;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#26080;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20005;&#35880;&#30340;&#23454;&#35777;&#29702;&#35299;SF-UDA&#26041;&#27861;&#20013;&#22810;&#20010;&#20851;&#38190;&#35774;&#35745;&#22240;&#32032;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#12290;&#30740;&#31350;&#20174;&#23454;&#35777;&#35282;&#24230;&#26816;&#39564;&#20102;&#22810;&#31181;SF-UDA&#25216;&#26415;&#65292;&#35780;&#20272;&#20854;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#19968;&#33268;&#24615;&#12289;&#23545;&#29305;&#23450;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#22312;&#19981;&#21516;&#20027;&#24178;&#26550;&#26500;&#31995;&#21015;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20840;&#38754;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#31574;&#30053;&#65292;&#29305;&#21035;&#20851;&#27880;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#21450;&#24494;&#35843;&#23545;&#28304;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#24378;&#35843;&#20102;&#29616;&#26377;&#22522;&#20934;&#23454;&#36341;&#20013;&#30340;&#24046;&#36317;&#65292;&#24341;&#23548;SF-UDA&#30740;&#31350;&#26397;&#30528;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#26041;&#27861;&#21457;&#23637;&#12290;&#23427;&#24378;&#35843;&#20102;&#20027;&#24178;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#36873;&#25321;&#23545;SF-UDA&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16090v1 Announce Type: cross  Abstract: This study provides a comprehensive benchmark framework for Source-Free Unsupervised Domain Adaptation (SF-UDA) in image classification, aiming to achieve a rigorous empirical understanding of the complex relationships between multiple key design factors in SF-UDA methods. The study empirically examines a diverse set of SF-UDA techniques, assessing their consistency across datasets, sensitivity to specific hyperparameters, and applicability across different families of backbone architectures. Moreover, it exhaustively evaluates pre-training datasets and strategies, particularly focusing on both supervised and self-supervised methods, as well as the impact of fine-tuning on the source domain. Our analysis also highlights gaps in existing benchmark practices, guiding SF-UDA research towards more effective and general approaches. It emphasizes the importance of backbone architecture and pre-training dataset selection on SF-UDA performance
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#25552;&#20986;&#20102;Evolution Graph Fourier Transform(EFT)&#65292;&#39318;&#27425;&#23454;&#29616;&#22312;&#26102;&#38388;&#22270;&#19978;&#25429;&#25417;&#28436;&#21270;&#34920;&#31034;&#30340;&#21487;&#36870;&#35889;&#21464;&#25442;&#65292;&#36890;&#36807;&#20248;&#21270;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#20266;&#35889;&#26494;&#24347;&#26469;&#39640;&#25928;&#35745;&#31639;&#36716;&#25442;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.16078</link><description>&lt;p&gt;
&#36229;&#36234;&#26102;&#31354;&#34920;&#31034;&#65306;&#28436;&#21270;Fourier&#21464;&#25442;&#29992;&#20110;&#26102;&#38388;&#22270;
&lt;/p&gt;
&lt;p&gt;
Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16078
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#25552;&#20986;&#20102;Evolution Graph Fourier Transform(EFT)&#65292;&#39318;&#27425;&#23454;&#29616;&#22312;&#26102;&#38388;&#22270;&#19978;&#25429;&#25417;&#28436;&#21270;&#34920;&#31034;&#30340;&#21487;&#36870;&#35889;&#21464;&#25442;&#65292;&#36890;&#36807;&#20248;&#21270;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#20266;&#35889;&#26494;&#24347;&#26469;&#39640;&#25928;&#35745;&#31639;&#36716;&#25442;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Evolving Graph Fourier Transform (EFT)&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25429;&#25417;&#26102;&#38388;&#22270;&#28436;&#21464;&#34920;&#31034;&#30340;&#21487;&#36870;&#35889;&#21464;&#25442;&#12290;&#25105;&#20204;&#36890;&#36807;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#28436;&#21464;&#22270;&#35889;&#30340;&#19981;&#36275;&#26469;&#28608;&#21457;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#30001;&#20110;&#26102;&#38388;&#22240;&#32032;&#20197;&#21450;&#22270;&#39030;&#28857;&#22495;&#30340;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#23545;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20266;&#35889;&#26494;&#24347;&#65292;&#20351;&#21464;&#25442;&#36807;&#31243;&#20998;&#35299;&#65292;&#20174;&#32780;&#20351;&#20854;&#39640;&#24230;&#35745;&#31639;&#26377;&#25928;&#12290;EFT&#26041;&#27861;&#29087;&#32451;&#22320;&#25429;&#25417;&#20102;&#28436;&#21464;&#22270;&#30340;&#32467;&#26500;&#21644;&#20301;&#32622;&#23646;&#24615;&#65292;&#20351;&#20854;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#28436;&#21464;&#22270;&#19978;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#20316;&#20026;&#21442;&#32771;&#23454;&#26045;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;EFT&#26469;&#25429;&#25417;&#28436;&#21464;&#22270;&#35889;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16078v1 Announce Type: new  Abstract: We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the Laplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph's structural and positional properties, making it effective for downstream tasks on evolving graphs. Hence, as a reference implementation, we develop a simple neural model induced with EFT for capturing evolving graph spectra. We empirically validate our theoretical findings on a number of large-scale an
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#24120;&#29992;&#30340;&#32676;&#65292;&#30740;&#31350;&#25581;&#31034;&#20986;&#27809;&#26377;&#26377;&#25928;&#30340;&#21487;&#35745;&#31639;&#30340;&#26694;&#26550;&#36873;&#25321;&#33021;&#22815;&#20445;&#25345;&#34987;&#24179;&#22343;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#65292;&#20294;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21152;&#26435;&#26694;&#26550;&#36825;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.16077</link><description>&lt;p&gt;
&#31561;&#21464;&#26694;&#26550;&#19982;&#36830;&#32493;&#35268;&#33539;&#21270;&#30340;&#19981;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
Equivariant Frames and the Impossibility of Continuous Canonicalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16077
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24120;&#29992;&#30340;&#32676;&#65292;&#30740;&#31350;&#25581;&#31034;&#20986;&#27809;&#26377;&#26377;&#25928;&#30340;&#21487;&#35745;&#31639;&#30340;&#26694;&#26550;&#36873;&#25321;&#33021;&#22815;&#20445;&#25345;&#34987;&#24179;&#22343;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#65292;&#20294;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21152;&#26435;&#26694;&#26550;&#36825;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#33539;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#19982;&#26550;&#26500;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#24378;&#21046;&#20445;&#25345;&#31561;&#21464;&#24615;&#65292;&#36817;&#26399;&#24191;&#21463;&#20851;&#27880;&#30340;&#27867;&#21270;&#26041;&#27861;&#22914;&#26694;&#26550;&#24179;&#22343;&#21270;&#25104;&#20026;&#19968;&#31181;&#36731;&#37327;&#19988;&#28789;&#27963;&#30340;&#31561;&#21464;&#26550;&#26500;&#26367;&#20195;&#26041;&#26696;&#12290; &#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#27010;&#29575;&#26694;&#26550;&#33021;&#22815;&#24102;&#26469;&#23454;&#35777;&#25928;&#30410;&#65292;&#36825;&#20123;&#26694;&#26550;&#23398;&#20064;&#32676;&#20803;&#32032;&#19978;&#30340;&#21152;&#26435;&#20998;&#24067;&#12290; &#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#19968;&#29616;&#35937;&#30340;&#24378;&#26377;&#21147;&#29702;&#35770;&#35777;&#25454;&#65306;&#23545;&#20110;&#24120;&#29992;&#30340;&#32676;&#65292;&#27809;&#26377;&#26377;&#25928;&#30340;&#21487;&#35745;&#31639;&#30340;&#26694;&#26550;&#36873;&#25321;&#33021;&#22815;&#20445;&#25345;&#34987;&#24179;&#22343;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#12290; &#25442;&#21477;&#35805;&#35828;&#65292;&#38750;&#21152;&#26435;&#30340;&#26694;&#26550;&#24179;&#22343;&#21487;&#20197;&#23558;&#19968;&#20010;&#24179;&#28369;&#30340;&#12289;&#38750;&#23545;&#31216;&#30340;&#20989;&#25968;&#36716;&#21464;&#20026;&#19968;&#20010;&#19981;&#36830;&#32493;&#12289;&#23545;&#31216;&#30340;&#20989;&#25968;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22522;&#26412;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#24182;&#26500;&#24314;&#20102;\emph{&#21152;&#26435;}&#26694;&#26550;&#65292;&#25454;&#35777;&#26126;&#33021;&#22815;&#20445;&#25345;&#36830;&#32493;&#24615;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#39640;&#25928;&#36830;&#32493;&#30340;&#21152;&#26435;&#26694;&#26550;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16077v1 Announce Type: new  Abstract: Canonicalization provides an architecture-agnostic method for enforcing equivariance, with generalizations such as frame-averaging recently gaining prominence as a lightweight and flexible alternative to equivariant architectures. Recent works have found an empirical benefit to using probabilistic frames instead, which learn weighted distributions over group elements. In this work, we provide strong theoretical justification for this phenomenon: for commonly-used groups, there is no efficiently computable choice of frame that preserves continuity of the function being averaged. In other words, unweighted frame-averaging can turn a smooth, non-symmetric function into a discontinuous, symmetric function. To address this fundamental robustness problem, we formally define and construct \emph{weighted} frames, which provably preserve continuity, and demonstrate their utility by constructing efficient and continuous weighted frames for the act
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.16075</link><description>&lt;p&gt;
&#22522;&#20110;&#25554;&#20540;&#30340;&#31574;&#30053;&#25193;&#25955;&#30340;&#34892;&#20026;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Behavioral Refinement via Interpolant-based Policy Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16075
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#36807;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26469;&#27169;&#20223;&#34892;&#20026;&#12290;&#26368;&#36817;&#65292;&#25317;&#26377;&#24314;&#27169;&#39640;&#32500;&#24230;&#21644;&#22810;&#27169;&#24577;&#20998;&#24067;&#33021;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23558;&#21160;&#20316;&#65288;&#25110;&#29366;&#24577;&#65289;&#20174;&#26631;&#20934;&#39640;&#26031;&#22122;&#22768;&#20013;&#25193;&#25955;&#26469;&#22609;&#36896;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35201;&#23398;&#20064;&#30340;&#30446;&#26631;&#31574;&#30053;&#36890;&#24120;&#19982;&#39640;&#26031;&#20998;&#24067;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#31181;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#22312;&#20351;&#29992;&#23569;&#37327;&#25193;&#25955;&#27493;&#39588;&#65288;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65289;&#21644;&#26377;&#38480;&#25968;&#25454;&#19979;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#20174;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#24320;&#22987;&#65292;&#21487;&#20197;&#20351;&#25193;&#25955;&#26041;&#27861;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#12289;&#19968;&#31181;&#26032;&#26041;&#27861;&#21644;&#23454;&#35777;&#21457;&#29616;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#28304;&#31574;&#30053;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;BRIDGER&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16075v1 Announce Type: cross  Abstract: Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochast
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;&#65292;&#25552;&#39640;&#20102;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#36716;&#21270;&#29575;&#25552;&#21319;4.9&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.16073</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20960;&#20046;&#23454;&#26102;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16073
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#30456;&#20284;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#20449;&#24687;&#27969;&#65292;&#25552;&#39640;&#20102;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#36716;&#21270;&#29575;&#25552;&#21319;4.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#23884;&#20837;&#26469;&#32534;&#30721;&#29992;&#25143;&#21160;&#20316;&#21644;&#39033;&#30446;&#65292;&#28982;&#21518;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#26816;&#32034;&#65292;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#29992;&#25143;&#23884;&#20837;&#21487;&#33021;&#38480;&#21046;&#25152;&#25429;&#33719;&#30340;&#20852;&#36259;&#22810;&#26679;&#24615;&#65292;2&#65289;&#20445;&#25345;&#23427;&#20204;&#26368;&#26032;&#38656;&#35201;&#26114;&#36149;&#30340;&#23454;&#26102;&#22522;&#30784;&#35774;&#26045;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#38469;&#24037;&#19994;&#29615;&#22659;&#20013;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21160;&#24577;&#26356;&#26032;&#23458;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#27599;&#20004;&#20998;&#38047;&#32452;&#25104;&#19968;&#20010;&#20449;&#24687;&#27969;&#65292;&#21033;&#29992;&#39044;&#35745;&#31639;&#30340;&#23884;&#20837;&#21450;&#20854;&#21508;&#33258;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#33655;&#20848;&#21644;&#27604;&#21033;&#26102;&#26368;&#22823;&#30340;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20043;&#19968;Bol&#19978;&#27979;&#35797;&#24182;&#37096;&#32626;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#23458;&#25143;&#21442;&#19982;&#24230;&#21644;&#20307;&#39564;&#65292;&#23548;&#33268;&#36716;&#21270;&#29575;&#26174;&#33879;&#25552;&#39640;&#20102;4.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16073v1 Announce Type: cross  Abstract: In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#26631;&#35760;&#26144;&#23556;&#21040;&#20849;&#20139;&#23383;&#31526;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#38463;&#25289;&#20271;-&#24076;&#20271;&#26469;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21516;&#26102;&#34920;&#31034;&#20004;&#31181;&#35821;&#35328;&#30340;&#32479;&#19968;&#33050;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#27604;&#20110;&#20445;&#25345;&#21407;&#26377;&#33050;&#26412;&#30340;&#27169;&#22411;&#26377;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.16065</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#26631;&#35760;&#26144;&#23556;&#21040;&#20849;&#20139;&#23383;&#31526;&#31354;&#38388;&#26469;&#35757;&#32451;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16065
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26631;&#35760;&#26144;&#23556;&#21040;&#20849;&#20139;&#23383;&#31526;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#38463;&#25289;&#20271;-&#24076;&#20271;&#26469;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21516;&#26102;&#34920;&#31034;&#20004;&#31181;&#35821;&#35328;&#30340;&#32479;&#19968;&#33050;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#27604;&#20110;&#20445;&#25345;&#21407;&#26377;&#33050;&#26412;&#30340;&#27169;&#22411;&#26377;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#38463;&#25289;&#20271;&#25991;&#25991;&#26412;&#30340;&#38899;&#35793;&#29256;&#26412;&#22312;&#24076;&#20271;&#26469;&#35821;&#20013;&#35757;&#32451;&#20102;&#19968;&#20010;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#20004;&#31181;&#35821;&#35328;&#22312;&#21516;&#19968;&#33050;&#26412;&#20013;&#34920;&#31034;&#12290;&#37492;&#20110;&#38463;&#25289;&#20271;&#35821;&#21644;&#24076;&#20271;&#26469;&#35821;&#20043;&#38388;&#30340;&#24418;&#24577;&#23398;&#12289;&#32467;&#26500;&#30456;&#20284;&#24615;&#20197;&#21450;&#22823;&#37327;&#20849;&#21516;&#35789;&#28304;&#35789;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;&#32479;&#19968;&#33050;&#26412;&#34920;&#31034;&#20004;&#31181;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#36328;&#35821;&#35328;&#30693;&#35782;&#30340;&#26426;&#22120;&#32763;&#35793;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20445;&#25345;&#38463;&#25289;&#20271;&#25991;&#26412;&#22312;&#38463;&#25289;&#20271;&#33050;&#26412;&#20013;&#30340;&#23545;&#27604;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#38899;&#35793;&#27493;&#39588;&#30340;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#26041;&#38754;&#35757;&#32451;&#38598;&#22823;&#23567;&#32422;&#20026;&#20854;&#20182;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;60&#65285;&#65292;&#20294;&#22312;&#26426;&#22120;&#32763;&#35793;&#30340;&#20004;&#20010;&#26041;&#21521;&#19978;&#20284;&#20046;&#25552;&#20379;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16065v1 Announce Type: new  Abstract: We train a bilingual Arabic-Hebrew language model using a transliterated version of Arabic texts in Hebrew, to ensure both languages are represented in the same script. Given the morphological, structural similarities, and the extensive number of cognates shared among Arabic and Hebrew, we assess the performance of a language model that employs a unified script for both languages, on machine translation which requires cross-lingual knowledge. The results are promising: our model outperforms a contrasting model which keeps the Arabic texts in the Arabic script, demonstrating the efficacy of the transliteration step. Despite being trained on a dataset approximately 60% smaller than that of other existing language models, our model appears to deliver comparable performance in machine translation across both translation directions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23558;&#28145;&#39640;&#26031;&#36807;&#31243;&#25193;&#23637;&#21040;&#21253;&#21547;&#26799;&#24230;&#25968;&#25454;&#65292;&#29992;&#20110;&#22810;&#20445;&#30495;&#24314;&#27169;&#65292;&#33021;&#22815;&#25429;&#33719;&#19981;&#21516;&#20445;&#30495;&#25968;&#25454;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#21644;&#36755;&#20837;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.16059</link><description>&lt;p&gt;
&#26799;&#24230;&#22686;&#24378;&#28145;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#22810;&#20445;&#30495;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Gradient-enhanced deep Gaussian processes for multifidelity modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16059
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23558;&#28145;&#39640;&#26031;&#36807;&#31243;&#25193;&#23637;&#21040;&#21253;&#21547;&#26799;&#24230;&#25968;&#25454;&#65292;&#29992;&#20110;&#22810;&#20445;&#30495;&#24314;&#27169;&#65292;&#33021;&#22815;&#25429;&#33719;&#19981;&#21516;&#20445;&#30495;&#25968;&#25454;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#21644;&#36755;&#20837;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20445;&#30495;&#27169;&#22411;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#29983;&#25104;&#24213;&#23618;&#36807;&#31243;&#30340;&#21333;&#19968;&#36924;&#36817;&#22120;&#12290;&#23494;&#38598;&#30340;&#20302;&#20445;&#30495;&#26679;&#26412;&#29992;&#20110;&#20943;&#23569;&#25554;&#20540;&#35823;&#24046;&#65292;&#31232;&#30095;&#30340;&#39640;&#20445;&#30495;&#26679;&#26412;&#29992;&#20110;&#24357;&#34917;&#20302;&#20445;&#30495;&#26679;&#26412;&#20013;&#30340;&#20559;&#24046;&#25110;&#22122;&#38899;&#12290;&#26799;&#24230;&#22686;&#24378;&#30340;&#28145;&#39640;&#26031;&#36807;&#31243;&#23545;&#22810;&#20445;&#30495;&#24314;&#27169;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#19981;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#20851;&#38190;&#26159;&#33021;&#22815;&#25429;&#33719;&#19981;&#21516;&#20445;&#30495;&#25968;&#25454;&#20043;&#38388;&#38750;&#32447;&#24615;&#21644;&#36755;&#20837;&#30456;&#20851;&#30340;&#20851;&#31995;&#12290;&#35768;&#22810;&#25968;&#25454;&#38598;&#33258;&#28982;&#21253;&#21547;&#26799;&#24230;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#30001;&#19982;&#33258;&#21160;&#24494;&#20998;&#20860;&#23481;&#25110;&#20855;&#26377;&#20849;&#36717;&#35299;&#30340;&#35745;&#31639;&#27169;&#22411;&#29983;&#25104;&#26102;&#12290;&#26412;&#24037;&#20316;&#20027;&#35201;&#26159;&#23558;&#28145;&#39640;&#26031;&#36807;&#31243;&#25193;&#23637;&#20026;&#21253;&#21547;&#26799;&#24230;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20998;&#26512;&#27979;&#35797;&#38382;&#39064;&#21644;&#19968;&#20010;&#29616;&#23454;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#19978;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#36825;&#20004;&#20010;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#27668;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16059v1 Announce Type: cross  Abstract: Multifidelity models integrate data from multiple sources to produce a single approximator for the underlying process. Dense low-fidelity samples are used to reduce interpolation error, while sparse high-fidelity samples are used to compensate for bias or noise in the low-fidelity samples. Deep Gaussian processes (GPs) are attractive for multifidelity modelling as they are non-parametric, robust to overfitting, perform well for small datasets, and, critically, can capture nonlinear and input-dependent relationships between data of different fidelities. Many datasets naturally contain gradient data, especially when they are generated by computational models that are compatible with automatic differentiation or have adjoint solutions. Principally, this work extends deep GPs to incorporate gradient data. We demonstrate this method on an analytical test problem and a realistic partial differential equation problem, where we predict the aer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.16048</link><description>&lt;p&gt;
LLMs&#24102;&#26377;&#24605;&#32500;&#38142;&#26465;&#26159;&#38750;&#22240;&#26524;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
LLMs with Chain-of-Thought Are Non-Causal Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#26377;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;LLMs&#20013;&#27491;&#30830;&#31572;&#26696;&#36319;&#38543;&#19981;&#27491;&#30830;CoTs&#30340;&#39057;&#29575;&#21450;&#21453;&#20043;&#12290;&#25105;&#20204;&#37319;&#29992;&#22240;&#26524;&#20998;&#26512;&#26469;&#35780;&#20272;CoTs/&#25351;&#20196;&#19982;LLMs&#31572;&#26696;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25581;&#31034;LLMs&#36817;&#20284;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#12290;&#36890;&#36807;&#27604;&#36739;&#26263;&#31034;SCM&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;SCM&#65292;&#25105;&#20204;&#31361;&#26174;&#20102;LLM&#21644;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24433;&#21709;&#26263;&#31034;SCM&#22240;&#26524;&#32467;&#26500;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26174;&#33879;&#24433;&#21709;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;https://github.com/StevenZHB/CoT_Causal_Analysis&#21457;&#24067;&#20102;&#20195;&#30721;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16048v1 Announce Type: cross  Abstract: This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#31181;&#32676;&#24847;&#35782;&#20248;&#21270;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#20043;&#38388;&#24494;&#22937;&#30340;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16041</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#31181;&#32676;&#24847;&#35782;&#20248;&#21270;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;
&lt;/p&gt;
&lt;p&gt;
Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16041
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#31181;&#32676;&#24847;&#35782;&#20248;&#21270;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#20043;&#38388;&#24494;&#22937;&#30340;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#39118;&#38505;&#65292;&#22914;&#25220;&#34989;&#38382;&#39064;&#12289;&#35823;&#23548;&#24615;&#20449;&#24687;&#25110;&#24187;&#35273;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26159;&#38750;&#24120;&#32039;&#36843;&#21644;&#37325;&#35201;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;LLMs&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#21306;&#20998;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#24120;&#24120;&#38750;&#24120;&#24494;&#22937;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#21033;&#29992;\textit{&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;}&#65288;MMD&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;MMD&#21487;&#20197;&#24456;&#22909;&#22320;&#35782;&#21035;&#20998;&#24067;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;&#21508;&#31181;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23545;MMD&#36827;&#34892;&#35757;&#32451;&#23558;&#23548;&#33268;MMD&#30340;&#26041;&#24046;&#26174;&#33879;&#22686;&#21152;&#65292;&#22240;&#20026;&#19981;&#21516;LLMs&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21487;&#33021;&#21253;&#21547;\textit{&#22810;&#20010;&#25991;&#26412;&#32676;&#20307;}&#12290;&#36825;&#23558;&#20005;&#37325;&#25439;&#23475;MMD&#27979;&#37327;&#20998;&#24067;&#24046;&#24322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16041v1 Announce Type: new  Abstract: Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMs. In this paper, we seek to exploit \textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However, directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \textit{multiple text populations} due to various LLMs. This will severely impair MMD's ability to measure the diff
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#38382;&#31572;&#31995;&#32479;&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#23601;&#65292;&#23588;&#20854;&#26159;&#22312;&#32925;&#32454;&#32990;&#30284;&#30740;&#31350;&#20013;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.16038</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25913;&#36827;&#32925;&#32454;&#32990;&#30284;&#30740;&#31350;&#20013;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16038
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#38382;&#31572;&#31995;&#32479;&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#23601;&#65292;&#23588;&#20854;&#26159;&#22312;&#32925;&#32454;&#32990;&#30284;&#30740;&#31350;&#20013;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#21463;&#30410;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#21033;&#29992;&#35832;&#22914;GPU&#21644;TPU&#31561;&#24378;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20687;BERT&#21644;GPT-3&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#19979;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#21508;&#31181;&#20219;&#21153;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#65292;&#21253;&#25324;&#35821;&#20041;&#29702;&#35299;&#12289;&#26234;&#33021;&#20889;&#20316;&#21644;&#25512;&#29702;&#65292;&#20026;&#26356;&#36890;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;NLP&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26469;&#24357;&#21512;&#20154;&#19982;&#35745;&#31639;&#26426;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;NLP&#30340;&#24403;&#21069;&#26684;&#23616;&#21644;&#26410;&#26469;&#23637;&#26395;&#65292;&#37325;&#28857;&#25918;&#22312;&#36825;&#19968;&#39046;&#22495;&#20869;&#30340;&#38382;&#31572;&#31995;&#32479;&#19978;&#12290;&#20998;&#26512;&#20102;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#38382;&#31572;&#31995;&#32479;&#30340;&#23454;&#38469;&#26696;&#20363;&#21644;&#21457;&#23637;&#65292;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16038v1 Announce Type: cross  Abstract: In recent years, advancements in natural language processing (NLP) have been fueled by deep learning techniques, particularly through the utilization of powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3, trained on vast amounts of data, have revolutionized language understanding and generation. These pre-trained models serve as robust bases for various tasks including semantic understanding, intelligent writing, and reasoning, paving the way for a more generalized form of artificial intelligence. NLP, as a vital application of AI, aims to bridge the gap between humans and computers through natural language interaction. This paper delves into the current landscape and future prospects of large-scale model-based NLP, focusing on the question-answering systems within this domain. Practical cases and developments in artificial intelligence-driven question-answering systems are analyzed to foster further explora
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#20013;&#20851;&#20110;&#36710;&#36742;&#24847;&#22270;&#36712;&#36857;&#35782;&#21035;&#21644;&#39044;&#27979;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16036</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24847;&#22270;&#36712;&#36857;&#35782;&#21035;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Based Vehicle Intention Trajectory Recognition and Prediction for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#20013;&#20851;&#20110;&#36710;&#36742;&#24847;&#22270;&#36712;&#36857;&#35782;&#21035;&#21644;&#39044;&#27979;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20114;&#32852;&#32593;&#25216;&#26415;&#30340;&#25193;&#24352;&#21644;&#33258;&#21160;&#21270;&#30340;&#36827;&#27493;&#24341;&#36215;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#21253;&#25324;&#27779;&#23572;&#27779;&#12289;&#22868;&#39536;&#21644;&#29305;&#26031;&#25289;&#22312;&#20869;&#30340;&#20027;&#35201;&#27773;&#36710;&#21046;&#36896;&#21830;&#36880;&#27493;&#25512;&#20986;&#20102;&#20174;&#36741;&#21161;&#39550;&#39542;&#36710;&#36742;&#21040;&#21322;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20135;&#21697;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#26102;&#26399;&#20063;&#21457;&#29983;&#20102;&#20960;&#36215;&#28041;&#21450;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20132;&#36890;&#23433;&#20840;&#20107;&#25925;&#12290;&#20363;&#22914;&#65292;2016&#24180;3&#26376;&#65292;&#19968;&#36742;&#35895;&#27468;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#19982;&#19968;&#36742;&#20844;&#20132;&#36710;&#21457;&#29983;&#20102;&#36731;&#24494;&#30896;&#25758;&#12290;&#20107;&#25925;&#21457;&#29983;&#26102;&#65292;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#35797;&#22270;&#24182;&#20837;&#21491;&#36710;&#36947;&#65292;&#20294;&#26410;&#33021;&#21160;&#24577;&#21709;&#24212;&#36710;&#36947;&#21464;&#26356;&#36807;&#31243;&#20013;&#30340;&#23454;&#26102;&#29615;&#22659;&#20449;&#24687;&#12290;&#23427;&#38169;&#35823;&#22320;&#20551;&#35774;&#38752;&#36817;&#30340;&#20844;&#20132;&#36710;&#20250;&#20943;&#36895;&#36991;&#35753;&#65292;&#23548;&#33268;&#19982;&#20844;&#20132;&#36710;&#21457;&#29983;&#20302;&#36895;&#30896;&#25758;&#12290;&#36825;&#19968;&#20107;&#20214;&#20984;&#26174;&#20986;&#24403;&#21069;&#25216;&#26415;&#30340;&#19981;&#36275;&#21644;&#23433;&#20840;&#39038;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16036v1 Announce Type: cross  Abstract: In recent years, the expansion of internet technology and advancements in automation have brought significant attention to autonomous driving technology. Major automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have progressively introduced products ranging from assisted-driving vehicles to semi-autonomous vehicles. However, this period has also witnessed several traffic safety incidents involving self-driving vehicles. For instance, in March 2016, a Google self-driving car was involved in a minor collision with a bus. At the time of the accident, the autonomous vehicle was attempting to merge into the right lane but failed to dynamically respond to the real-time environmental information during the lane change. It incorrectly assumed that the approaching bus would slow down to avoid it, leading to a low-speed collision with the bus. This incident highlights the current technological shortcomings and safety concerns a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#36793;&#24418;&#38754;&#31215;&#30340;&#26032;&#22411;&#27491;&#20132;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#21306;&#20998;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#28151;&#21512;&#38750;&#21333;&#35843;&#32447;&#24615;&#25628;&#32034;&#26041;&#27861;&#22788;&#29702;&#27491;&#20132;&#32422;&#26463;&#24102;&#26469;&#30340;&#38750;&#20984;&#20248;&#21270;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#38477;&#32500;&#21644;&#25552;&#21319;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16026</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#20132;&#32422;&#26463;&#21644;&#22810;&#36793;&#24418;&#38754;&#31215;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Feature Selection Based on Orthogonal Constraints and Polygon Area
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16026
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#36793;&#24418;&#38754;&#31215;&#30340;&#26032;&#22411;&#27491;&#20132;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#21306;&#20998;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#28151;&#21512;&#38750;&#21333;&#35843;&#32447;&#24615;&#25628;&#32034;&#26041;&#27861;&#22788;&#29702;&#27491;&#20132;&#32422;&#26463;&#24102;&#26469;&#30340;&#38750;&#20984;&#20248;&#21270;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#38477;&#32500;&#21644;&#25552;&#21319;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#35780;&#20272;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#35782;&#21035;&#20219;&#21153;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#23376;&#38598;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#38477;&#32500;&#12290;&#30446;&#21069;&#65292;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#21306;&#20998;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#36793;&#24418;&#38754;&#31215;&#30340;&#26032;&#22411;&#27491;&#20132;&#22238;&#24402;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#30452;&#35266;&#25429;&#25417;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#21306;&#20998;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#37319;&#29992;&#28151;&#21512;&#38750;&#21333;&#35843;&#32447;&#24615;&#25628;&#32034;&#26041;&#27861;&#26469;&#39640;&#25928;&#22788;&#29702;&#27491;&#20132;&#32422;&#26463;&#24102;&#26469;&#30340;&#38750;&#20984;&#20248;&#21270;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#25429;&#25417;&#21306;&#20998;&#24615;&#20381;&#36182;&#20449;&#24687;&#65292;&#32780;&#19988;&#22312;&#38477;&#20302;&#29305;&#24449;&#32500;&#24230;&#21644;&#25552;&#21319;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16026v1 Announce Type: new  Abstract: The goal of feature selection is to choose the optimal subset of features for a recognition task by evaluating the importance of each feature, thereby achieving effective dimensionality reduction. Currently, proposed feature selection methods often overlook the discriminative dependencies between features and labels. To address this problem, this paper introduces a novel orthogonal regression model incorporating the area of a polygon. The model can intuitively capture the discriminative dependencies between features and labels. Additionally, this paper employs a hybrid non-monotone linear search method to efficiently tackle the non-convex optimization challenge posed by orthogonal constraints. Experimental results demonstrate that our approach not only effectively captures discriminative dependency information but also surpasses traditional methods in reducing feature dimensions and enhancing classification performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;HiGPT&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#24322;&#36136;&#22270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#27867;&#21270;&#38480;&#21046;&#21644;&#20998;&#24067;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16024</link><description>&lt;p&gt;
HiGPT&#65306;&#24322;&#36136;&#22270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HiGPT: Heterogeneous Graph Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16024
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;HiGPT&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#24322;&#36136;&#22270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#27867;&#21270;&#38480;&#21046;&#21644;&#20998;&#24067;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#23398;&#20064;&#26088;&#22312;&#25429;&#25417;&#24322;&#26500;&#22270;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#22810;&#26679;&#21270;&#20851;&#31995;&#35821;&#20041;&#65292;&#20197;&#33719;&#24471;&#33410;&#28857;&#21644;&#36793;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#12290;&#26368;&#36817;&#22312;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#32771;&#34385;&#20851;&#31995;&#30340;&#24322;&#36136;&#24615;&#24182;&#20351;&#29992;&#19987;&#38376;&#30340;&#28040;&#24687;&#20989;&#25968;&#21644;&#32858;&#21512;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26694;&#26550;&#22312;&#27867;&#21270;&#21040;&#19981;&#21516;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#26694;&#26550;&#37117;&#36981;&#24490;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#30340;&#8220;&#39044;&#35757;&#32451;&#8221;&#21644;&#8220;&#24494;&#35843;&#8221;&#33539;&#24335;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#36866;&#24212;&#26032;&#30340;&#21644;&#30475;&#19981;&#35265;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#23558;&#24322;&#36136;&#22270;&#27169;&#22411;&#27867;&#21270;&#20026;&#36866;&#24212;&#20855;&#26377;&#33410;&#28857;&#20196;&#29260;&#38598;&#21644;&#20851;&#31995;&#31867;&#22411;&#24322;&#36136;&#24615;&#20998;&#24067;&#21464;&#21270;&#30340;&#19981;&#21516;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#65311;&#8221;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16024v1 Announce Type: new  Abstract: Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges. Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets. Most of these frameworks follow the "pre-train" and "fine-tune" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: "Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?'' To tackle those challenges, we p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36880;&#27493;&#20171;&#32461;&#23454;&#29616;&#31616;&#21333;&#33258;&#21160;&#24494;&#20998;&#31995;&#32479;&#65292;&#22635;&#34917;&#20102;&#25945;&#23398;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#31616;&#21270;&#20102;&#25968;&#23398;&#27010;&#24565;&#21644;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.16020</link><description>&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#30340;&#20998;&#27493;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
A Step-by-step Introduction to the Implementation of Automatic Differentiation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36880;&#27493;&#20171;&#32461;&#23454;&#29616;&#31616;&#21333;&#33258;&#21160;&#24494;&#20998;&#31995;&#32479;&#65292;&#22635;&#34917;&#20102;&#25945;&#23398;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#31616;&#21270;&#20102;&#25968;&#23398;&#27010;&#24565;&#21644;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20010;&#20027;&#39064;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20248;&#31168;&#30340;&#35843;&#26597;&#65288;&#22914;Baydin&#31561;&#65292;2018&#24180;&#65289;&#24050;&#32463;&#21487;&#20197;&#28165;&#26224;&#22320;&#25551;&#36848;&#22522;&#26412;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#33258;&#21160;&#24494;&#20998;&#30340;&#22797;&#26434;&#23454;&#29616;&#29616;&#22312;&#26159;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#25945;&#25480;&#23398;&#29983;&#29616;&#26377;&#31995;&#32479;&#30340;&#23454;&#29616;&#26159;&#22256;&#38590;&#30340;&#65292;&#22914;&#26524;&#19981;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#23427;&#22826;&#22797;&#26434;&#20102;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#25945;&#23398;&#27490;&#27493;&#20110;&#22522;&#26412;&#27010;&#24565;&#65292;&#23398;&#29983;&#23558;&#26080;&#27861;&#24863;&#21463;&#21040;&#23454;&#29616;&#30340;&#23454;&#29616;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#22312;&#25945;&#23398;&#33258;&#21160;&#24494;&#20998;&#26102;&#32463;&#24120;&#25552;&#21450;&#35745;&#31639;&#22270;&#65292;&#20294;&#23398;&#29983;&#20204;&#24819;&#30693;&#36947;&#22914;&#20309;&#23454;&#29616;&#21644;&#20351;&#29992;&#23427;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36880;&#27493;&#20171;&#32461;&#23454;&#29616;&#31616;&#21333;&#33258;&#21160;&#24494;&#20998;&#31995;&#32479;&#26469;&#22635;&#34917;&#37096;&#20998;&#31354;&#30333;&#12290;&#25105;&#20204;&#31616;&#21270;&#20102;&#25968;&#23398;&#27010;&#24565;&#21644;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16020v1 Announce Type: new  Abstract: Automatic differentiation is a key component in deep learning. This topic is well studied and excellent surveys such as Baydin et al. (2018) have been available to clearly describe the basic concepts. Further, sophisticated implementations of automatic differentiation are now an important part of popular deep learning frameworks. However, it is difficult, if not impossible, to directly teach students the implementation of existing systems due to the complexity. On the other hand, if the teaching stops at the basic concept, students fail to sense the realization of an implementation. For example, we often mention the computational graph in teaching automatic differentiation, but students wonder how to implement and use it. In this document, we partially fill the gap by giving a step by step introduction of implementing a simple automatic differentiation system. We streamline the mathematical concepts and the implementation. Further, we gi
&lt;/p&gt;</description></item><item><title>&#23637;&#31034;&#33258;&#21160;&#24494;&#20998;&#22312;&#35745;&#31639;&#21644;&#25511;&#21046;&#38544;&#24335;&#32447;&#24615;&#31639;&#23376;&#39057;&#35889;&#20013;&#30340;&#26377;&#25928;&#24615;&#65307;&#25552;&#20379;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#19968;&#33324;&#21367;&#31215;&#23618;&#30340;&#35009;&#21098;&#26041;&#27861;&#65307;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#23618;&#19982;&#21367;&#31215;&#23618;&#32452;&#21512;&#30340;&#25928;&#26524;&#65307;&#36890;&#36807;&#27604;&#36739;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#24615;&#33021;&#34920;&#26126;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.16017</link><description>&lt;p&gt;
&#38544;&#24335;&#32447;&#24615;&#23618;&#30340;&#39057;&#35889;&#25552;&#21462;&#21644;&#35009;&#21098;
&lt;/p&gt;
&lt;p&gt;
Spectrum Extraction and Clipping for Implicitly Linear Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16017
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#33258;&#21160;&#24494;&#20998;&#22312;&#35745;&#31639;&#21644;&#25511;&#21046;&#38544;&#24335;&#32447;&#24615;&#31639;&#23376;&#39057;&#35889;&#20013;&#30340;&#26377;&#25928;&#24615;&#65307;&#25552;&#20379;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#19968;&#33324;&#21367;&#31215;&#23618;&#30340;&#35009;&#21098;&#26041;&#27861;&#65307;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#23618;&#19982;&#21367;&#31215;&#23618;&#32452;&#21512;&#30340;&#25928;&#26524;&#65307;&#36890;&#36807;&#27604;&#36739;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#24615;&#33021;&#34920;&#26126;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#21160;&#24494;&#20998;&#22312;&#39640;&#25928;&#20934;&#30830;&#35745;&#31639;&#21644;&#25511;&#21046;&#38544;&#24335;&#32447;&#24615;&#31639;&#23376;&#30340;&#39057;&#35889;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#26159;&#19968;&#31867;&#21253;&#25324;&#25152;&#26377;&#26631;&#20934;&#21367;&#31215;&#21644;&#20840;&#36830;&#25509;&#23618;&#30340;&#20016;&#23500;&#23618;&#31867;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#19968;&#33324;&#21367;&#31215;&#23618;&#30340;&#35009;&#21098;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#23548;&#33268;&#20043;&#21069;&#24037;&#20316;&#20013;&#27491;&#30830;&#24615;&#38382;&#39064;&#30340;&#34920;&#31034;&#38480;&#21046;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#23618;&#19982;&#21367;&#31215;&#23618;&#20018;&#32852;&#26102;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35009;&#21098;&#26041;&#27861;&#22914;&#20309;&#24212;&#29992;&#20110;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#23454;&#39564;&#34920;&#26126;&#23427;&#20204;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;&#20195;&#30721;&#38142;&#25509;https://github.com/Ali-E/FastClip&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16017v1 Announce Type: new  Abstract: We show the effectiveness of automatic differentiation in efficiently and correctly computing and controlling the spectrum of implicitly linear operators, a rich family of layer types including all standard convolutional and dense layers. We provide the first clipping method which is correct for general convolution layers, and illuminate the representational limitation that caused correctness issues in prior work. We study the effect of the batch normalization layers when concatenated with convolutional layers and show how our clipping method can be applied to their composition. By comparing the accuracy and performance of our algorithms to the state-of-the-art methods, using various experiments, we show they are more precise and efficient and lead to better generalization and adversarial robustness. We provide the code for using our methods at https://github.com/Ali-E/FastClip.
&lt;/p&gt;</description></item><item><title>OmniArch&#36890;&#36807;&#22810;&#29289;&#29702;&#23398;&#26102;&#31354;&#25968;&#25454;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#29289;&#29702;&#20449;&#24687;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#31185;&#23398;&#35745;&#31639;&#39046;&#22495;&#26500;&#24314;&#28789;&#27963;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24615;&#33021;&#12289;&#36866;&#24212;&#24615;&#21644;&#36870;&#38382;&#39064;&#27714;&#35299;&#26041;&#38754;&#21462;&#24471;&#31361;&#30772;&#65292;&#23637;&#29616;&#20102;AI&#23545;&#31185;&#23398;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16014</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#35745;&#31639;&#35268;&#27169;&#19978;&#26500;&#24314;&#28789;&#27963;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Building Flexible Machine Learning Models for Scientific Computing at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16014
&lt;/p&gt;
&lt;p&gt;
OmniArch&#36890;&#36807;&#22810;&#29289;&#29702;&#23398;&#26102;&#31354;&#25968;&#25454;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#29289;&#29702;&#20449;&#24687;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#31185;&#23398;&#35745;&#31639;&#39046;&#22495;&#26500;&#24314;&#28789;&#27963;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24615;&#33021;&#12289;&#36866;&#24212;&#24615;&#21644;&#36870;&#38382;&#39064;&#27714;&#35299;&#26041;&#38754;&#21462;&#24471;&#31361;&#30772;&#65292;&#23637;&#29616;&#20102;AI&#23545;&#31185;&#23398;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16014v1
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16014v1 Announce Type: cross  Abstract: Foundation models have revolutionized knowledge acquisition across domains, and our study introduces OmniArch, a paradigm-shifting approach designed for building foundation models in multi-physics scientific computing. OmniArch's pre-training involves a versatile pipeline that processes multi-physics spatio-temporal data, casting forward problem learning into scalable auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning (PIRL) technique during fine-tuning ensures alignment with physical laws. Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional adaptability to new physics via few-shot and zero-shot learning approaches. The model's representations further extend to inverse problem-solving, highlighting the transformative potential of AI-enabled Scientific Computing(AI4SC) foundation models for engineering ap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23545;&#27604;&#22270;&#23398;&#20064;&#65288;DCGL&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#32534;&#30721;&#22120;&#21644;GCN&#65292;&#22312;&#22788;&#29702;&#19968;&#33324;&#25968;&#25454;&#32858;&#31867;&#26102;&#24378;&#35843;&#20102;&#22270;&#32467;&#26500;&#21644;&#21407;&#22987;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.16012</link><description>&lt;p&gt;
&#20855;&#26377;&#38754;&#21521;&#32858;&#31867;&#30340;&#24341;&#23548;&#30340;&#28145;&#24230;&#23545;&#27604;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Contrastive Graph Learning with Clustering-Oriented Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16012
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23545;&#27604;&#22270;&#23398;&#20064;&#65288;DCGL&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#32534;&#30721;&#22120;&#21644;GCN&#65292;&#22312;&#22788;&#29702;&#19968;&#33324;&#25968;&#25454;&#32858;&#31867;&#26102;&#24378;&#35843;&#20102;&#22270;&#32467;&#26500;&#21644;&#21407;&#22987;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#25913;&#21892;&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#20026;&#20102;&#22788;&#29702;&#27809;&#26377;&#20808;&#39564;&#22270;&#30340;&#19968;&#33324;&#32858;&#31867;&#22330;&#26223;&#65292;&#36825;&#20123;&#27169;&#22411;&#20808;&#20272;&#35745;&#19968;&#20010;&#21021;&#22987;&#22270;&#65292;&#28982;&#21518;&#24212;&#29992;GCN&#12290;&#25991;&#29486;&#20013;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20851;&#27880;&#20110;&#21021;&#22987;&#22270;&#32780;&#24573;&#30053;&#20102;&#21407;&#22987;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#21487;&#36776;&#35782;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;&#20302;&#36136;&#37327;&#21021;&#22987;&#22270;&#30340;&#30772;&#22351;&#65307;&#35757;&#32451;&#36807;&#31243;&#32570;&#20047;&#26377;&#25928;&#30340;&#32858;&#31867;&#24341;&#23548;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23558;&#19982;&#32858;&#31867;&#26080;&#20851;&#30340;&#20449;&#24687;&#21512;&#24182;&#21040;&#23398;&#21040;&#30340;&#22270;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#32858;&#31867;&#30340;&#28145;&#24230;&#23545;&#27604;&#22270;&#23398;&#20064;&#65288;DCGL&#65289;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20266;&#23402;&#29983;&#32593;&#32476;&#65292;&#23558;&#33258;&#32534;&#30721;&#22120;&#19982;GCN&#30456;&#32467;&#21512;&#65292;&#20197;&#24378;&#35843;&#22270;&#32467;&#26500;&#21644;&#21407;&#22987;&#29305;&#24449;&#12290;&#22522;&#20110;&#27492;&#65292;&#29305;&#24449;&#32423;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16012v1 Announce Type: new  Abstract: Graph Convolutional Network (GCN) has exhibited remarkable potential in improving graph-based clustering. To handle the general clustering scenario without a prior graph, these models estimate an initial graph beforehand to apply GCN. Throughout the literature, we have witnessed that 1) most models focus on the initial graph while neglecting the original features. Therefore, the discriminability of the learned representation may be corrupted by a low-quality initial graph; 2) the training procedure lacks effective clustering guidance, which may lead to the incorporation of clustering-irrelevant information into the learned graph. To tackle these problems, the Deep Contrastive Graph Learning (DCGL) model is proposed for general data clustering. Specifically, we establish a pseudo-siamese network, which incorporates auto-encoder with GCN to emphasize both the graph structure and the original features. On this basis, feature-level contrasti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Jacobson&#26174;&#33879;&#24615;&#22320;&#22270;&#65288;JSM&#65289;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#35843;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#19981;&#21512;&#29702;&#30340;&#35748;&#30693;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#65288;&#20197;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20026;&#20363;&#65289;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16008</link><description>&lt;p&gt;
&#36890;&#36807;&#25513;&#30422;&#36755;&#20837;&#26799;&#24230;&#25581;&#31034;&#30196;&#21574;&#30151;&#26816;&#27979;&#65306;&#19968;&#31181;&#29992;&#20110;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#30830;&#24615;&#30340;JSM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to Model Interpretability and Precision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16008
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Jacobson&#26174;&#33879;&#24615;&#22320;&#22270;&#65288;JSM&#65289;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#35843;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#19981;&#21512;&#29702;&#30340;&#35748;&#30693;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#65288;&#20197;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20026;&#20363;&#65289;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#26174;&#33879;&#25913;&#21464;&#20102;&#25216;&#26415;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#31561;&#20851;&#38190;&#39046;&#22495;&#20013;&#26377;&#25928;&#24212;&#29992;&#23427;&#20204;&#38656;&#35201;&#26356;&#22810;&#65292;&#19981;&#20165;&#35201;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36824;&#35201;&#26377;&#21487;&#20449;&#36182;&#24615;&#12290;&#34429;&#28982;&#21487;&#35299;&#37322;&#24615;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#32463;&#24120;&#26080;&#27861;&#25581;&#31034;&#27169;&#22411;&#20197;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#25110;&#20598;&#28982;&#20851;&#32852;&#20570;&#20986;&#65288;&#19981;&#33021;&#27867;&#21270;&#30340;&#65289;&#27491;&#30830;&#39044;&#27979;&#30340;&#8220;&#26426;&#26234;&#27721;&#26031;&#8221;&#34892;&#20026;&#12290;&#21516;&#26679;&#65292;&#24403;&#21069;&#30340;&#20107;&#21518;XAI&#26041;&#27861;&#23481;&#26131;&#29983;&#25104;&#19981;&#21512;&#29702;&#30340;&#21453;&#20107;&#23454;&#31034;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#21019;&#26032;&#30340;&#8220;&#27169;&#22411;&#35843;&#35797;&#8221;&#26041;&#27861;&#26469;&#22788;&#29702;XAI&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38597;&#21508;&#27604;&#26174;&#33879;&#24615;&#22270;&#65288;JSM&#65289;&#23454;&#29616;&#12290;&#20026;&#20102;&#20351;&#38382;&#39064;&#20855;&#20307;&#21270;&#65292;&#25105;&#20204;&#36873;&#25321;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#35786;&#26029;&#20316;&#20026;&#29992;&#20363;&#65292;&#36825;&#28304;&#20110;&#20854;&#23545;&#20154;&#31867;&#29983;&#27963;&#30340;&#37325;&#22823;&#24433;&#21709;&#20197;&#21450;&#23545;&#26089;&#26399;&#26816;&#27979;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16008v1 Announce Type: cross  Abstract: The evolution of deep learning and artificial intelligence has significantly reshaped technological landscapes. However, their effective application in crucial sectors such as medicine demands more than just superior performance, but trustworthiness as well. While interpretability plays a pivotal role, existing explainable AI (XAI) approaches often do not reveal {\em Clever Hans} behavior where a model makes (ungeneralizable) correct predictions using spurious correlations or biases in data. Likewise, current post-hoc XAI methods are susceptible to generating unjustified counterfactual examples. In this paper, we approach XAI with an innovative {\em model debugging} methodology realized through Jacobian Saliency Map (JSM). To cast the problem into a concrete context, we employ Alzheimer's disease (AD) diagnosis as the use case, motivated by its significant impact on human lives and the formidable challenge in its early detection, stemm
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30528;&#21147;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25932;&#23545;&#25915;&#20987;&#38382;&#39064;&#19982;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#39046;&#22495;&#21516;&#21270;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16005</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#21516;&#21270;&#23454;&#29616;&#21307;&#23398;&#22270;&#20687;&#30340;&#25932;&#23545;&#31283;&#20581;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversarial-Robust Transfer Learning for Medical Imaging via Domain Assimilation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16005
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30528;&#21147;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25932;&#23545;&#25915;&#20987;&#38382;&#39064;&#19982;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#39046;&#22495;&#21516;&#21270;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#21033;&#29992;&#20854;&#28508;&#21147;&#26469;&#25581;&#31034;&#24739;&#32773;&#30340;&#20851;&#38190;&#35786;&#26029;&#29305;&#24449;&#12290;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#21307;&#23398;&#35786;&#26029;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#12289;&#26816;&#27979;&#21644;&#35782;&#21035;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#30142;&#30149;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#30340;&#26174;&#33879;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#20449;&#20219;&#38382;&#39064;&#12290;&#23545;&#21407;&#22987;&#22270;&#20687;&#24341;&#20837;&#24494;&#23567;&#25200;&#21160;&#20351;&#23545;&#25163;&#33021;&#22815;&#25805;&#32437;&#39044;&#27979;&#36755;&#20986;&#65292;&#23558;&#20854;&#37325;&#23450;&#21521;&#21040;&#20854;&#20182;&#30446;&#26631;&#25110;&#38750;&#30446;&#26631;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#21307;&#23398;&#22270;&#20687;&#31232;&#32570;&#65292;&#26500;&#25104;&#21487;&#38752;&#35757;&#32451;&#30340;&#29942;&#39048;&#65292;&#36825;&#23548;&#33268;&#24403;&#20195;&#31639;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#22823;&#37327;&#33258;&#28982;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411; -- &#19968;&#31181;&#31216;&#20026;&#36801;&#31227;&#23398;&#20064;&#30340;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16005v1 Announce Type: cross  Abstract: In the field of Medical Imaging, extensive research has been dedicated to leveraging its potential in uncovering critical diagnostic features in patients. Artificial Intelligence (AI)-driven medical diagnosis relies on sophisticated machine learning and deep learning models to analyze, detect, and identify diseases from medical images. Despite the remarkable performance of these models, characterized by high accuracy, they grapple with trustworthiness issues. The introduction of a subtle perturbation to the original image empowers adversaries to manipulate the prediction output, redirecting it to other targeted or untargeted classes. Furthermore, the scarcity of publicly available medical images, constituting a bottleneck for reliable training, has led contemporary algorithms to depend on pretrained models grounded on a large set of natural images -- a practice referred to as transfer learning. However, a significant {\em domain discre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;&#32534;&#30721;&#30340;&#21518;&#37327;&#23376;&#23494;&#30721;&#26041;&#27861;&#26144;&#23556;&#21040;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;PQC&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#12289;&#38543;&#26426;&#25200;&#21160;&#30340;&#23494;&#25991;&#21644;&#23494;&#25991;&#30340;&#22343;&#21248;&#20998;&#24067;&#22686;&#24378;&#23494;&#25991;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16002</link><description>&lt;p&gt;
&#21518;&#37327;&#23376;&#23494;&#30721;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Post-Quantum Cryptography Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16002
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;&#32534;&#30721;&#30340;&#21518;&#37327;&#23376;&#23494;&#30721;&#26041;&#27861;&#26144;&#23556;&#21040;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;PQC&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#12289;&#38543;&#26426;&#25200;&#21160;&#30340;&#23494;&#25991;&#21644;&#23494;&#25991;&#30340;&#22343;&#21248;&#20998;&#24067;&#22686;&#24378;&#23494;&#25991;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#37327;&#23376;&#35745;&#31639;&#26426;&#21644;Shor&#37327;&#23376;&#31639;&#27861;&#23545;&#24403;&#21069;&#20027;&#27969;&#30340;&#38750;&#23545;&#31216;&#21152;&#23494;&#26041;&#27861;&#65288;&#20363;&#22914;RSA&#21644;&#26925;&#22278;&#26354;&#32447;&#21152;&#23494;&#65288;ECC&#65289;&#65289;&#26500;&#25104;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#26500;&#24314;&#19968;&#31181;&#21518;&#37327;&#23376;&#23494;&#30721;&#65288;PQC&#65289;&#26041;&#27861;&#26469;&#25269;&#25239;&#37327;&#23376;&#35745;&#31639;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PQC&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#22522;&#20110;&#32534;&#30721;&#30340;PQC&#26041;&#27861;&#26144;&#23556;&#21040;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#12289;&#38543;&#26426;&#25200;&#21160;&#30340;&#23494;&#25991;&#20197;&#21450;&#23494;&#25991;&#30340;&#22343;&#21248;&#20998;&#24067;&#22686;&#24378;&#20102;&#23494;&#25991;&#30340;&#23433;&#20840;&#24615;&#12290;&#22312;&#23454;&#38469;&#23454;&#39564;&#20013;&#65292;&#26412;&#30740;&#31350;&#20197;&#34562;&#31389;&#32593;&#32476;&#20449;&#21495;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#24314;&#35758;&#30340;&#22522;&#20110;PQC&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#23494;&#25991;&#30340;&#22343;&#21248;&#20998;&#24067;&#36827;&#34892;&#21152;&#23494;&#21644;&#35299;&#23494;&#12290;&#26410;&#26469;&#65292;&#36825;&#31181;&#25552;&#20986;&#30340;&#22522;&#20110;PQC&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16002v1 Announce Type: cross  Abstract: In recent years, quantum computers and Shor quantum algorithm have posed a threat to current mainstream asymmetric cryptography methods (e.g. RSA and Elliptic Curve Cryptography (ECC)). Therefore, it is necessary to construct a Post-Quantum Cryptography (PQC) method to resist quantum computing attacks. Therefore, this study proposes a PQC-based neural network that maps a code-based PQC method to a neural network structure and enhances the security of ciphertexts with non-linear activation functions, random perturbation of ciphertexts, and uniform distribution of ciphertexts. In practical experiments, this study uses cellular network signals as a case study to demonstrate that encryption and decryption can be performed by the proposed PQC-based neural network with the uniform distribution of ciphertexts. In the future, the proposed PQC-based neural network could be applied to various applications.
&lt;/p&gt;</description></item><item><title>Cieran&#26159;&#19968;&#20010;&#20801;&#35768;&#25968;&#25454;&#20998;&#26512;&#24072;&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#35774;&#35745;&#22270;&#34920;&#26102;&#24555;&#36895;&#25214;&#21040;&#36136;&#37327;&#37197;&#33394;&#26041;&#26696;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#25490;&#24207;&#21644;&#21019;&#24314;&#26032;&#30340;&#37197;&#33394;&#26041;&#26696;&#65292;&#24110;&#21161;&#26032;&#25163;&#20998;&#26512;&#24072;&#23450;&#21046;&#37197;&#33394;&#26041;&#26696;&#20197;&#36866;&#24212;&#20854;&#25968;&#25454;&#32972;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.15997</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#29616;&#22330;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#35774;&#35745;&#39034;&#24207;&#37197;&#33394;&#26041;&#26696;&#30340;Cieran
&lt;/p&gt;
&lt;p&gt;
Cieran: Designing Sequential Colormaps via In-Situ Active Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15997
&lt;/p&gt;
&lt;p&gt;
Cieran&#26159;&#19968;&#20010;&#20801;&#35768;&#25968;&#25454;&#20998;&#26512;&#24072;&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#35774;&#35745;&#22270;&#34920;&#26102;&#24555;&#36895;&#25214;&#21040;&#36136;&#37327;&#37197;&#33394;&#26041;&#26696;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#25490;&#24207;&#21644;&#21019;&#24314;&#26032;&#30340;&#37197;&#33394;&#26041;&#26696;&#65292;&#24110;&#21161;&#26032;&#25163;&#20998;&#26512;&#24072;&#23450;&#21046;&#37197;&#33394;&#26041;&#26696;&#20197;&#36866;&#24212;&#20854;&#25968;&#25454;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#36136;&#30340;&#37197;&#33394;&#26041;&#26696;&#21487;&#20197;&#24110;&#21161;&#20256;&#36798;&#37325;&#35201;&#30340;&#25968;&#25454;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#35201;&#20026;&#29305;&#23450;&#24773;&#26223;&#25214;&#21040;&#30475;&#36215;&#26469;&#8220;&#24688;&#21040;&#22909;&#22788;&#8221;&#30340;&#32654;&#35266;&#37197;&#33394;&#26041;&#26696;&#65292;&#38656;&#35201;&#30456;&#24403;&#22810;&#30340;&#35774;&#35745;&#21644;&#25216;&#26415;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Cieran&#65292;&#36825;&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#20801;&#35768;&#20219;&#20309;&#25968;&#25454;&#20998;&#26512;&#24072;&#22312;&#35774;&#35745;Jupyter&#31508;&#35760;&#26412;&#20013;&#30340;&#22270;&#34920;&#26102;&#36805;&#36895;&#25214;&#21040;&#20248;&#36136;&#30340;&#37197;&#33394;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#37319;&#29992;&#20102;&#19968;&#31181;&#20027;&#21160;&#30340;&#20559;&#22909;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#20004;&#20004;&#27604;&#36739;&#26469;&#23545;&#19987;&#23478;&#35774;&#35745;&#30340;&#37197;&#33394;&#26041;&#26696;&#36827;&#34892;&#25490;&#24207;&#65292;&#20801;&#35768;&#22312;&#33394;&#24425;&#35774;&#35745;&#26041;&#38754;&#26159;&#26032;&#25163;&#30340;&#20998;&#26512;&#24072;&#26681;&#25454;&#20854;&#25968;&#25454;&#32972;&#26223;&#23450;&#21046;&#37197;&#33394;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#37197;&#33394;&#26041;&#26696;&#30340;&#35774;&#35745;&#35270;&#20026;&#22312;CIELAB&#39068;&#33394;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#19982;&#21313;&#20108;&#21517;&#31185;&#23398;&#23478;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Cieran&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#29992;&#25143;&#30340;&#20559;&#22909;&#26469;&#25490;&#21517;&#37197;&#33394;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#27169;&#22411;&#21019;&#24314;&#20102;&#26032;&#30340;&#20248;&#36136;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15997v1 Announce Type: cross  Abstract: Quality colormaps can help communicate important data patterns. However, finding an aesthetically pleasing colormap that looks "just right" for a given scenario requires significant design and technical expertise. We introduce Cieran, a tool that allows any data analyst to rapidly find quality colormaps while designing charts within Jupyter Notebooks. Our system employs an active preference learning paradigm to rank expert-designed colormaps and create new ones from pairwise comparisons, allowing analysts who are novices in color design to tailor colormaps to their data context. We accomplish this by treating colormap design as a path planning problem through the CIELAB colorspace with a context-specific reward model. In an evaluation with twelve scientists, we found that Cieran effectively modeled user preferences to rank colormaps and leveraged this model to create new quality designs. Our work shows the potential of active preferenc
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#23398;&#20064;&#22312;&#32500;&#24230;N&#20013;&#30340;$\omega(\log \log N)$&#20010;&#21322;&#31354;&#38388;&#29978;&#33267;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#26174;&#33879;&#32553;&#23567;&#20102;&#36825;&#19968;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.15995</link><description>&lt;p&gt;
&#25913;&#36827;&#23398;&#20064;&#21322;&#31354;&#38388;&#20132;&#38598;&#30340;&#22256;&#38590;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Improved Hardness Results for Learning Intersections of Halfspaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15995
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#23398;&#20064;&#22312;&#32500;&#24230;N&#20013;&#30340;$\omega(\log \log N)$&#20010;&#21322;&#31354;&#38388;&#29978;&#33267;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#26174;&#33879;&#32553;&#23567;&#20102;&#36825;&#19968;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#27491;&#30830;&#35774;&#32622;&#20013;&#23398;&#20064;&#21322;&#31354;&#38388;&#20132;&#38598;&#30340;&#24369;&#23398;&#20064;&#19979;&#30028;&#65292;&#36825;&#20123;&#19979;&#30028;&#38750;&#24120;&#24378;&#22823;&#65288;&#24182;&#19988;&#20196;&#20154;&#24778;&#35766;&#22320;&#31616;&#21333;&#65289;&#12290;&#20851;&#20110;&#36825;&#20010;&#38382;&#39064;&#30693;&#20043;&#29978;&#23569;&#12290;&#20363;&#22914;&#65292;&#29978;&#33267;&#19981;&#30693;&#36947;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#23398;&#20064;&#20165;&#20004;&#20010;&#21322;&#31354;&#38388;&#30340;&#20132;&#38598;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#33391;&#22909;&#24314;&#31435;&#30340;&#20551;&#35774;&#65288;&#22914;&#36817;&#20284;&#26368;&#22351;&#24773;&#20917;&#30340;&#26684;&#38382;&#39064;&#25110;Feige&#30340;3SAT&#20551;&#35774;&#30340;&#21464;&#20307;&#65289;&#30340;&#19979;&#30028;&#20165;&#23545;&#36229;&#23545;&#25968;&#20010;&#21322;&#31354;&#38388;&#30340;&#20132;&#38598;&#24050;&#30693;&#65288;&#25110;&#32773;&#30001;&#24050;&#26377;&#32467;&#26524;&#26263;&#31034;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15995v1 Announce Type: cross  Abstract: We show strong (and surprisingly simple) lower bounds for weakly learning intersections of halfspaces in the improper setting. Strikingly little is known about this problem. For instance, it is not even known if there is a polynomial-time algorithm for learning the intersection of only two halfspaces. On the other hand, lower bounds based on well-established assumptions (such as approximating worst-case lattice problems or variants of Feige's 3SAT hypothesis) are only known (or are implied by existing results) for the intersection of super-logarithmically many halfspaces [KS09,KS06,DSS16]. With intersections of fewer halfspaces being only ruled out under less standard assumptions [DV21] (such as the existence of local pseudo-random generators with large stretch). We significantly narrow this gap by showing that even learning $\omega(\log \log N)$ halfspaces in dimension $N$ takes super-polynomial time under standard assumptions on wors
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;DQN&#31639;&#27861;&#24341;&#20837;&#36164;&#20135;&#31649;&#29702;&#25237;&#36164;&#32452;&#21512;&#20013;&#65292;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;DRL&#31639;&#27861;&#22312;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#20013;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2402.15994</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#25968;&#23383;&#36164;&#20135;&#30340;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#21644;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Optimizing Portfolio Management and Risk Assessment in Digital Assets Using Deep Learning for Predictive Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15994
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;DQN&#31639;&#27861;&#24341;&#20837;&#36164;&#20135;&#31649;&#29702;&#25237;&#36164;&#32452;&#21512;&#20013;&#65292;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;DRL&#31639;&#27861;&#22312;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#38382;&#39064;&#24050;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#37327;&#21270;&#20132;&#26131;&#26041;&#27861;&#20173;&#23384;&#22312;&#19968;&#20123;&#21487;&#20197;&#25913;&#36827;&#30340;&#26041;&#38754;&#12290;&#26412;&#25991;&#23558;DQN&#31639;&#27861;&#20197;&#19968;&#31181;&#20840;&#26032;&#32780;&#30452;&#25509;&#30340;&#26041;&#24335;&#24341;&#20837;&#36164;&#20135;&#31649;&#29702;&#25237;&#36164;&#32452;&#21512;&#20013;&#65292;&#24615;&#33021;&#22823;&#22823;&#36229;&#36807;&#22522;&#20934;&#65292;&#20805;&#20998;&#35777;&#26126;&#20102;DRL&#31639;&#27861;&#22312;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15994v1 Announce Type: cross  Abstract: Portfolio management issues have been extensively studied in the field of artificial intelligence in recent years, but existing deep learning-based quantitative trading methods have some areas where they could be improved. First of all, the prediction mode of stocks is singular; often, only one trading expert is trained by a model, and the trading decision is solely based on the prediction results of the model. Secondly, the data source used by the model is relatively simple, and only considers the data of the stock itself, ignoring the impact of the whole market risk on the stock. In this paper, the DQN algorithm is introduced into asset management portfolios in a novel and straightforward way, and the performance greatly exceeds the benchmark, which fully proves the effectiveness of the DRL algorithm in portfolio management. This also inspires us to consider the complexity of financial problems, and the use of algorithms should be fu
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#25968;&#25454;&#30340;&#36793;&#32536;&#26234;&#33021;&#24212;&#29992;&#30340;S4&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24179;&#34913;&#25130;&#26029;&#25216;&#26415;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#36807;&#31243;&#21644;&#20248;&#21270;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#25351;&#26631;&#26469;&#36229;&#36234;&#20256;&#32479;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15993</link><description>&lt;p&gt;
&#20351;&#29992;&#24179;&#34913;&#25130;&#26029;&#25216;&#26415;&#23398;&#20064;&#24102;&#26377;&#23545;&#35282;&#29366;&#24577;&#31354;&#38388;&#23618;&#30340;S4&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning method for S4 with Diagonal State Space Layers using Balanced Truncation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15993
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#25968;&#25454;&#30340;&#36793;&#32536;&#26234;&#33021;&#24212;&#29992;&#30340;S4&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24179;&#34913;&#25130;&#26029;&#25216;&#26415;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#36807;&#31243;&#21644;&#20248;&#21270;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#25351;&#26631;&#26469;&#36229;&#36234;&#20256;&#32479;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#65288;S4&#65289;&#27169;&#22411;&#24182;&#19988;&#21152;&#20837;&#20102;&#23545;&#35282;&#29366;&#24577;&#31354;&#38388;&#65288;DSS&#65289;&#23618;&#65292;&#36825;&#31181;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#36793;&#32536;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#38271;&#24207;&#21015;&#25968;&#25454;&#65292;&#21253;&#25324;&#20256;&#24863;&#22120;&#25968;&#25454;&#20998;&#26512;&#21644;&#23454;&#26102;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24179;&#34913;&#25130;&#26029;&#25216;&#26415;&#65292;&#22312;&#25511;&#21046;&#29702;&#35770;&#20013;&#24456;&#24120;&#35265;&#65292;&#29305;&#21035;&#24212;&#29992;&#20110;DSS&#23618;&#20197;&#38477;&#20302;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;S4&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;Skew-HiPPo&#21021;&#22987;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#35757;&#32451;&#30340;&#24102;&#26377;DSS&#23618;&#30340;S4&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#25351;&#26631;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#26174;&#31034;&#19968;&#20010;&#31215;&#26497;&#30340;&#30456;&#20851;&#24615;&#65306;&#21407;&#22987;&#27169;&#22411;&#20013;&#30340;&#26356;&#39640;&#20934;&#30830;&#24230;&#19968;&#33268;&#23548;&#33268;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#22686;&#21152;&#65292;&#36825;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15993v1 Announce Type: new  Abstract: We introduce a novel learning method for Structured State Space Sequence (S4) models incorporating Diagonal State Space (DSS) layers, tailored for processing long-sequence data in edge intelligence applications, including sensor data analysis and real-time analytics. This method utilizes the balanced truncation technique, prevalent in control theory, applied specifically to DSS layers to reduce computational costs during inference. By leveraging parameters from the reduced model, we refine the initialization process of S4 models, outperforming the widely used Skew-HiPPo initialization in terms of performance. Numerical experiments demonstrate that our trained S4 models with DSS layers surpass conventionally trained models in accuracy and efficiency metrics. Furthermore, our observations reveal a positive correlation: higher accuracy in the original model consistently leads to increased accuracy in models trained using our method, suggest
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#25512;&#25991;&#20197;&#30830;&#23450;&#23458;&#25143;&#28385;&#24847;&#27700;&#24179;&#65292;&#26377;&#21161;&#20110;&#31616;&#21270;&#30740;&#31350;&#25104;&#21315;&#19978;&#19975;&#26465;&#25512;&#25991;&#24182;&#25913;&#36827;&#33322;&#31354;&#20844;&#21496;&#26381;&#21153;&#30340;&#32321;&#29712;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.15992</link><description>&lt;p&gt;
&#20174;&#22810;&#20010;&#25512;&#25991;&#21442;&#25968;&#20013;&#26816;&#27979;&#23458;&#25143;&#28385;&#24847;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Approach to Detect Customer Satisfaction From Multiple Tweet Parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15992
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#25512;&#25991;&#20197;&#30830;&#23450;&#23458;&#25143;&#28385;&#24847;&#27700;&#24179;&#65292;&#26377;&#21161;&#20110;&#31616;&#21270;&#30740;&#31350;&#25104;&#21315;&#19978;&#19975;&#26465;&#25512;&#25991;&#24182;&#25913;&#36827;&#33322;&#31354;&#20844;&#21496;&#26381;&#21153;&#30340;&#32321;&#29712;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20114;&#32852;&#32593;&#25216;&#26415;&#24471;&#20197;&#21457;&#23637;&#20197;&#26469;&#65292;&#23458;&#25143;&#28385;&#24847;&#24230;&#24050;&#25104;&#20026;&#20225;&#19994;&#21457;&#23637;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#22312;&#32447;&#24179;&#21488;&#24050;&#25104;&#20026;&#20998;&#20139;&#35780;&#35770;&#30340;&#20027;&#35201;&#22330;&#25152;&#20043;&#19968;&#12290;Twitter&#26159;&#20854;&#20013;&#20043;&#19968;&#65292;&#23458;&#25143;&#32463;&#24120;&#22312;&#35813;&#24179;&#21488;&#19978;&#21457;&#24067;&#20182;&#20204;&#30340;&#24819;&#27861;&#12290;&#22312;&#36825;&#20123;&#24179;&#21488;&#19978;&#23545;&#33322;&#29677;&#30340;&#35780;&#35770;&#24050;&#25104;&#20026;&#33322;&#31354;&#20844;&#21496;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#31215;&#26497;&#30340;&#35780;&#35770;&#21487;&#20197;&#24110;&#21161;&#20844;&#21496;&#22686;&#38271;&#65292;&#32780;&#28040;&#26497;&#30340;&#35780;&#35770;&#21017;&#21487;&#33021;&#36805;&#36895;&#30772;&#22351;&#20854;&#25910;&#20837;&#21644;&#22768;&#35465;&#12290;&#22240;&#27492;&#65292;&#23545;&#33322;&#31354;&#20844;&#21496;&#26469;&#35828;&#65292;&#23457;&#26597;&#23458;&#25143;&#30340;&#21453;&#39304;&#21644;&#20307;&#39564;&#12289;&#25913;&#36827;&#20854;&#26381;&#21153;&#20197;&#20445;&#25345;&#31454;&#20105;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#25104;&#21315;&#19978;&#19975;&#26465;&#25512;&#25991;&#24182;&#20998;&#26512;&#23427;&#20204;&#20197;&#30830;&#23450;&#23458;&#25143;&#28385;&#24847;&#24230;&#26159;&#19968;&#39033;&#30456;&#24403;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#25512;&#25991;&#20197;&#30830;&#23450;&#23458;&#25143;&#28385;&#24847;&#27700;&#24179;&#21487;&#20197;&#31616;&#21270;&#36825;&#19968;&#36153;&#26102;&#30340;&#36807;&#31243;&#12290;&#24050;&#32463;&#26377;&#19968;&#20123;&#20851;&#20110;&#36825;&#31181;&#31574;&#30053;&#30340;&#24037;&#20316;&#21487;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#20351;&#29992;&#26426;&#22120;&#23398;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15992v1 Announce Type: cross  Abstract: Since internet technologies have advanced, one of the primary factors in company development is customer happiness. Online platforms have become prominent places for sharing reviews. Twitter is one of these platforms where customers frequently post their thoughts. Reviews of flights on these platforms have become a concern for the airline business. A positive review can help the company grow, while a negative one can quickly ruin its revenue and reputation. So it's vital for airline businesses to examine the feedback and experiences of their customers and enhance their services to remain competitive. But studying thousands of tweets and analyzing them to find the satisfaction of the customer is quite a difficult task. This tedious process can be made easier by using a machine learning approach to analyze tweets to determine client satisfaction levels. Some work has already been done on this strategy to automate the procedure using mach
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20844;&#24179;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;FairGAD&#65289;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#26469;&#33258;Reddit&#21644;Twitter&#30340;&#20004;&#20010;&#26032;&#22270;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#25991;&#29486;&#22312;&#27492;&#38382;&#39064;&#19978;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.15988</link><description>&lt;p&gt;
&#26397;&#21521;&#20844;&#24179;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#65306;&#38382;&#39064;&#12289;&#26032;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15988
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20844;&#24179;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;FairGAD&#65289;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#26469;&#33258;Reddit&#21644;Twitter&#30340;&#20004;&#20010;&#26032;&#22270;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#25991;&#29486;&#22312;&#27492;&#38382;&#39064;&#19978;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;FairGAD&#65289;&#38382;&#39064;&#26088;&#22312;&#22312;&#36755;&#20837;&#22270;&#20013;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#33410;&#28857;&#65292;&#21516;&#26102;&#30830;&#20445;&#20844;&#24179;&#24615;&#65292;&#36991;&#20813;&#38024;&#23545;&#26469;&#33258;&#25935;&#24863;&#20122;&#32452;&#22914;&#24615;&#21035;&#25110;&#25919;&#27835;&#20542;&#21521;&#30340;&#20010;&#20307;&#30340;&#20559;&#35265;&#39044;&#27979;&#12290;&#22270;&#20013;&#30340;&#20844;&#24179;&#24615;&#22312;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#23588;&#20026;&#20851;&#38190;&#65292;&#27604;&#22914;&#22312;&#25628;&#32034;/&#25490;&#21517;&#31995;&#32479;&#20013;&#36827;&#34892;&#30340;&#35823;&#20449;&#24687;&#26816;&#27979;&#65292;&#20915;&#31574;&#32467;&#26524;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#24433;&#21709;&#20010;&#20154;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25991;&#29486;&#27809;&#26377;&#20840;&#38754;&#35752;&#35770;&#36825;&#20010;&#38382;&#39064;&#65292;&#20063;&#27809;&#26377;&#25552;&#20379;&#36148;&#36817;&#23454;&#38469;&#22270;&#32467;&#26500;&#12289;&#24322;&#24120;&#26631;&#31614;&#21644;&#25935;&#24863;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;FairGAD&#30740;&#31350;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FairGAD&#38382;&#39064;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#20110;&#20840;&#29699;&#30693;&#21517;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;Reddit&#21644;Twitter&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15988v1 Announce Type: cross  Abstract: The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect anomalous nodes in an input graph while ensuring fairness and avoiding biased predictions against individuals from sensitive subgroups such as gender or political leanings. Fairness in graphs is particularly crucial in anomaly detection areas such as misinformation detection in search/ranking systems, where decision outcomes can significantly affect individuals. However, the current literature does not comprehensively discuss this problem, nor does it provide realistic datasets that encompass actual graph structures, anomaly labels, and sensitive attributes for research in FairGAD. To bridge this gap, we introduce a formal definition of the FairGAD problem and present two novel graph datasets constructed from the globally prominent social media platforms Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges associated with 9,000 and 47,000 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;HuBERT&#23454;&#29616;&#20102;&#23545;&#29356;&#21483;&#22768;&#30340;&#22768;&#32032;&#26631;&#31614;&#20998;&#31867;&#21644;&#35789;&#27719;&#35782;&#21035;&#65292;&#21457;&#29616;&#20102;&#20855;&#26377;&#26174;&#33879;&#22768;&#23398;&#19968;&#33268;&#24615;&#30340;&#29356;&#35789;&#27719;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;Web&#31995;&#32479;&#26631;&#35760;&#29399;&#21483;&#22768;&#20013;&#30340;&#22768;&#32032;n-gram&#12290;</title><link>https://arxiv.org/abs/2402.15985</link><description>&lt;p&gt;
&#20351;&#29992;HuBERT&#36827;&#34892;&#23545;&#29356;&#35821;&#35328;&#30340;&#35821;&#38899;&#21644;&#35789;&#27719;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Phonetic and Lexical Discovery of a Canine Language using HuBERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15985
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;HuBERT&#23454;&#29616;&#20102;&#23545;&#29356;&#21483;&#22768;&#30340;&#22768;&#32032;&#26631;&#31614;&#20998;&#31867;&#21644;&#35789;&#27719;&#35782;&#21035;&#65292;&#21457;&#29616;&#20102;&#20855;&#26377;&#26174;&#33879;&#22768;&#23398;&#19968;&#33268;&#24615;&#30340;&#29356;&#35789;&#27719;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;Web&#31995;&#32479;&#26631;&#35760;&#29399;&#21483;&#22768;&#20013;&#30340;&#22768;&#32032;n-gram&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#29356;&#21483;&#22768;&#20013;&#28508;&#22312;&#27807;&#36890;&#27169;&#24335;&#30340;&#24320;&#21019;&#24615;&#25506;&#32034;&#65292;&#24182;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#35821;&#35328;&#20998;&#26512;&#38556;&#30861;&#65292;&#22823;&#37327;&#20381;&#36182;&#20110;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#21644;&#26377;&#38480;&#25968;&#25454;&#38598;&#26469;&#21457;&#29616;&#29399;&#30340;&#21483;&#22768;&#20013;&#30340;&#22768;&#38899;&#21333;&#20803;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;HuBERT&#23454;&#29616;&#20102;&#22768;&#32032;&#26631;&#31614;&#30340;&#20934;&#30830;&#20998;&#31867;&#65292;&#24182;&#35782;&#21035;&#20102;&#26263;&#31034;&#29356;&#21483;&#22768;&#20013;&#19968;&#31181;&#22522;&#30784;&#35789;&#27719;&#30340;&#22768;&#38899;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#30830;&#23450;&#30340;&#29356;&#35789;&#27719;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#22768;&#23398;&#19968;&#33268;&#24615;&#65292;&#35206;&#30422;&#20102;&#25152;&#26377;&#35266;&#23519;&#21040;&#30340;&#29356;&#21483;&#22768;&#24207;&#21015;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#29356;&#21483;&#22768;&#26631;&#35760;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#29992;&#25143;&#19978;&#20256;&#30340;&#29399;&#21483;&#22768;&#20013;&#31361;&#20986;&#26174;&#31034;&#35789;&#27719;&#20013;&#23384;&#22312;&#30340;&#22768;&#32032;n-gram&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15985v1 Announce Type: cross  Abstract: This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. We present a self-supervised approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. We further develop a web-based dog vocalization labeling system. This system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#34920;&#36798;&#24335;&#23548;&#20986;&#23574;&#23792;&#21464;&#25442;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#25551;&#36848;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.15984</link><description>&lt;p&gt;
&#29992;&#32479;&#19968;&#30340;&#20613;&#37324;&#21494;&#20999;&#29255;&#26041;&#27861;&#23548;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#28145;&#24230;-2&#31070;&#32463;&#32593;&#32476;&#30340;&#23574;&#23792;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15984
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#34920;&#36798;&#24335;&#23548;&#20986;&#23574;&#23792;&#21464;&#25442;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#25551;&#36848;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26102;&#65292;&#30740;&#31350;&#21442;&#25968;&#20998;&#24067;&#27604;&#30740;&#31350;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#26356;&#23481;&#26131;&#12290;&#23574;&#23792;&#21464;&#25442;&#26159;&#19968;&#20010;&#20266;&#36870;&#31639;&#23376;&#65292;&#23558;&#32473;&#23450;&#20989;&#25968; $f$ &#26144;&#23556;&#21040;&#21442;&#25968;&#20998;&#24067; $\gamma$&#65292;&#20351;&#24471;&#32593;&#32476; $\mathtt{NN}[\gamma]$ &#33021;&#22815;&#37325;&#29616; $f$&#65292;&#21363; $\mathtt{NN}[\gamma]=f$&#12290;&#22312;&#27431;&#27663;&#31354;&#38388;&#19978;&#30340;&#28145;&#24230;-2&#20840;&#36830;&#25509;&#32593;&#32476;&#20013;&#65292;&#24050;&#21457;&#29616;&#20102;&#23574;&#23792;&#21464;&#25442;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#25551;&#36848;&#21442;&#25968;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#31181;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23578;&#19981;&#30693;&#36947;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20613;&#37324;&#21494;&#34920;&#36798;&#24335;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#23548;&#21508;&#31181;&#29616;&#20195;&#32593;&#32476;&#30340;&#23574;&#23792;&#21464;&#25442;&#65292;&#20363;&#22914;&#26377;&#38480;&#22495; $\mathbb{F}_p$ &#19978;&#30340;&#32593;&#32476;&#12289;&#25277;&#35937;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388; $\mathcal{H}$ &#19978;&#30340;&#32676;&#21367;&#31215;&#32593;&#32476;&#65292;&#20197;&#21450;&#38750;&#32039;&#33268;&#23545;&#31216;&#30340;&#20840;&#36830;&#25509;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15984v1 Announce Type: new  Abstract: To investigate neural network parameters, it is easier to study the distribution of parameters than to study the parameters in each neuron. The ridgelet transform is a pseudo-inverse operator that maps a given function $f$ to the parameter distribution $\gamma$ so that a network $\mathtt{NN}[\gamma]$ reproduces $f$, i.e. $\mathtt{NN}[\gamma]=f$. For depth-2 fully-connected networks on a Euclidean space, the ridgelet transform has been discovered up to the closed-form expression, thus we could describe how the parameters are distributed. However, for a variety of modern neural network architectures, the closed-form expression has not been known. In this paper, we explain a systematic method using Fourier expressions to derive ridgelet transforms for a variety of modern networks such as networks on finite fields $\mathbb{F}_p$, group convolutional networks on abstract Hilbert space $\mathcal{H}$, fully-connected networks on noncompact symm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#20284;&#28982;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#36125;&#21494;&#26031;&#36793;&#32536;&#20284;&#28982;&#21644;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#31232;&#30095;&#21270;&#65292;&#24182;&#37319;&#29992;&#33258;&#21160;&#22885;&#21345;&#22982;&#21059;&#20992;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26435;&#37325;&#21066;&#20943;&#12290;</title><link>https://arxiv.org/abs/2402.15978</link><description>&lt;p&gt;
&#20351;&#29992;&#22885;&#21345;&#22982;&#21059;&#20992;&#21066;&#20943;&#26435;&#37325;&#65306;&#20351;&#29992;&#36793;&#32536;&#20284;&#28982;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks Using the Marginal Likelihood
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#20284;&#28982;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#36125;&#21494;&#26031;&#36793;&#32536;&#20284;&#28982;&#21644;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#31232;&#30095;&#21270;&#65292;&#24182;&#37319;&#29992;&#33258;&#21160;&#22885;&#21345;&#22982;&#21059;&#20992;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26435;&#37325;&#21066;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#21270;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#65292;&#21487;&#20197;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#35768;&#22810;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21464;&#24471;&#36807;&#22823;&#20197;&#33267;&#26080;&#27861;&#30452;&#25509;&#37096;&#32626;&#22312;&#28040;&#36153;&#31867;&#30828;&#20214;&#30340;&#26102;&#20195;&#12290;&#34429;&#28982;&#24456;&#22810;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#19981;&#21516;&#30340;&#26435;&#37325;&#21098;&#26525;&#20934;&#21017;&#19978;&#65292;&#20294;&#32593;&#32476;&#30340;&#24635;&#20307;&#31232;&#30095;&#24615;&#65292;&#21363;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21098;&#26525;&#30340;&#33021;&#21147;&#65292;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#36793;&#32536;&#20284;&#28982;&#37327;&#65288;Marginal likelihood&#65289;&#30340;&#31232;&#30095;&#24615;&#65288;SpaM&#65289;&#65292;&#19968;&#20010;&#31232;&#30095;&#21270;&#26694;&#26550;&#65292;&#37325;&#28857;&#24378;&#35843;&#20351;&#29992;&#36125;&#21494;&#26031;&#36793;&#32536;&#20284;&#28982;&#19982;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#31232;&#30095;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#33258;&#21160;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#65292;&#36873;&#25321;&#26368;&#24819;&#35201;&#21066;&#20943;&#30340;&#27169;&#22411;&#65292;&#20197;&#20381;&#28982;&#33021;&#22815;&#24456;&#22909;&#22320;&#35299;&#37322;&#25968;&#25454;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#32467;&#26500;&#21270;&#36824;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#31232;&#30095;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#20013;&#20351;&#29992;&#30340;&#39044;&#35745;&#31639;&#21518;&#39564;&#40657;&#22622;&#36817;&#20284;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15978v1 Announce Type: new  Abstract: Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to na\"ively deploy on consumer hardware. While much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM), a pruning framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. In addition, we demonstrate that the pre-computed posterior Hessian approximation used in the Laplace approximation
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#36890;&#20449;&#12289;&#24863;&#30693;&#21644;&#35745;&#31639;&#30340;&#20219;&#21153;&#21368;&#36733;&#33539;&#24335;&#65292;&#26088;&#22312;&#20248;&#21270;&#35745;&#31639;&#27169;&#24335;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#38477;&#20302;&#36164;&#28304;&#28040;&#32791;&#25104;&#26412;&#65292;&#24182;&#20445;&#35777;&#20219;&#21153;&#30340;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2402.15972</link><description>&lt;p&gt;
&#32467;&#26500;&#30693;&#35782;&#39537;&#21160;&#30340;&#20803;&#23398;&#20064;&#29992;&#20110;&#36710;&#32852;&#32593;&#20013;&#38598;&#25104;&#36890;&#20449;&#12289;&#24863;&#30693;&#21644;&#35745;&#31639;&#30340;&#20219;&#21153;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
Structural Knowledge-Driven Meta-Learning for Task Offloading in Vehicular Networks with Integrated Communications, Sensing and Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15972
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#36890;&#20449;&#12289;&#24863;&#30693;&#21644;&#35745;&#31639;&#30340;&#20219;&#21153;&#21368;&#36733;&#33539;&#24335;&#65292;&#26088;&#22312;&#20248;&#21270;&#35745;&#31639;&#27169;&#24335;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#38477;&#20302;&#36164;&#28304;&#28040;&#32791;&#25104;&#26412;&#65292;&#24182;&#20445;&#35777;&#20219;&#21153;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21368;&#36733;&#26159;&#28385;&#36275;&#36710;&#36733;&#24212;&#29992;&#30340;&#20005;&#26684;&#35745;&#31639;&#38656;&#27714;&#21644;&#23545;&#24310;&#36831;&#25935;&#24863;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#36710;&#36733;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#21368;&#36733;&#33539;&#24335;&#65292;&#21363;&#38598;&#25104;&#36890;&#20449;&#12289;&#24863;&#30693;&#21644;&#35745;&#31639;&#65288;I-CSC&#65289;&#65292;&#36890;&#36807;&#36335;&#20391;&#21333;&#20803;&#65288;RSU&#65289;&#24863;&#30693;&#30340;&#29615;&#22659;&#25968;&#25454;&#30452;&#25509;&#29992;&#20110;&#35745;&#31639;&#65292;&#36710;&#36742;&#21487;&#20197;&#36873;&#25321;&#23558;&#24863;&#30693;&#25968;&#25454;&#19978;&#20256;&#21040;RSU&#25110;&#22312;&#21368;&#36733;&#36807;&#31243;&#20013;&#21521;RSU&#21457;&#36865;&#35745;&#31639;&#25351;&#20196;&#12290;&#36890;&#36807;&#20248;&#21270;&#35745;&#31639;&#27169;&#24335;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;I-CSC&#30340;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#24102;&#26469;&#30340;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#35777;&#27599;&#20010;&#20219;&#21153;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15972v1 Announce Type: new  Abstract: Task offloading is a potential solution to satisfy the strict requirements of computation-intensive and latency-sensitive vehicular applications due to the limited onboard computing resources. However, the overwhelming upload traffic may lead to unacceptable uploading time. To tackle this issue, for tasks taking environmental data as input, the data perceived by roadside units (RSU) equipped with several sensors can be directly exploited for computation, resulting in a novel task offloading paradigm with integrated communications, sensing and computing (I-CSC). With this paradigm, vehicles can select to upload their sensed data to RSUs or transmit computing instructions to RSUs during the offloading. By optimizing the computation mode and network resources, in this paper, we investigate an I-CSC-based task offloading problem to reduce the cost caused by resource consumption while guaranteeing the latency of each task. Although this non-c
&lt;/p&gt;</description></item><item><title>CoDream&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#21327;&#20316;&#20248;&#21270;&#25968;&#25454;&#26469;&#20132;&#25442;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#26550;&#26500;&#26080;&#20851;&#12289;&#36890;&#20449;&#19981;&#21463;&#27169;&#22411;&#22823;&#23567;&#24433;&#21709;&#12289;&#20860;&#23481;&#23433;&#20840;&#32858;&#21512;&#30340;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.15968</link><description>&lt;p&gt;
CoDream&#65306;&#20351;&#29992;&#24322;&#26500;&#27169;&#22411;&#20132;&#25442;&#26790;&#24819;&#32780;&#19981;&#26159;&#27169;&#22411;&#36827;&#34892;&#32852;&#21512;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15968
&lt;/p&gt;
&lt;p&gt;
CoDream&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#21327;&#20316;&#20248;&#21270;&#25968;&#25454;&#26469;&#20132;&#25442;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#26550;&#26500;&#26080;&#20851;&#12289;&#36890;&#20449;&#19981;&#21463;&#27169;&#22411;&#22823;&#23567;&#24433;&#21709;&#12289;&#20860;&#23481;&#23433;&#20840;&#32858;&#21512;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#32858;&#21512;&#27169;&#22411;&#21442;&#25968;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#25955;&#25968;&#25454;&#19978;&#30340;&#21327;&#20316;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32858;&#21512;&#27169;&#22411;&#20135;&#29983;&#30340;&#8220;&#30693;&#35782;&#8221;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#21442;&#25968;&#26469;&#25193;&#23637;&#36825;&#19968;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; \codream &#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#23458;&#25143;&#31471;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#20351;&#29992;&#32852;&#21512;&#20248;&#21270;&#26469;&#21327;&#20316;&#20248;&#21270;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#25968;&#25454;&#65292;&#31867;&#20284;&#20110;&#22312;FL&#20013;&#20248;&#21270;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#32852;&#21512;&#20248;&#21270;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#25429;&#33719;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#30340;&#29305;&#24615;&#12290;&#22312;&#25968;&#25454;&#31354;&#38388;&#20849;&#20139;&#30693;&#35782;&#20855;&#26377;&#35768;&#22810;&#22909;&#22788;&#65306;&#65288;1&#65289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#21363;&#19981;&#21516;&#23458;&#25143;&#31471;&#21487;&#20197;&#20855;&#26377;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#65307;&#65288;2&#65289;&#36890;&#20449;&#19981;&#21463;&#27169;&#22411;&#22823;&#23567;&#24433;&#21709;&#65292;&#28040;&#38500;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65307;&#65288;3&#65289;&#19982;&#23433;&#20840;&#32858;&#21512;&#20860;&#23481;&#65292;&#22240;&#27492;&#21487;&#39044;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15968v1 Announce Type: cross  Abstract: Federated Learning (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters. Our approach extends this concept by aggregating "knowledge" derived from models, instead of model parameters. We present a novel framework called \codream, where clients collaboratively optimize randomly initialized data using federated optimization in the input data space, similar to how randomly initialized model parameters are optimized in FL. Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution. Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus pre
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23618;&#27425;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#27773;&#36710;&#21046;&#36896;&#36807;&#31243;&#30340;&#33021;&#37327;&#28040;&#32791;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#21487;&#23454;&#29616;&#26356;&#22909;&#30340;&#25805;&#20316;&#21487;&#35265;&#24615;&#21644;&#35782;&#21035;&#33410;&#33021;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.15962</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26500;&#24314;&#30340;&#23618;&#27425;&#21270;&#33021;&#37327;&#29305;&#24449;&#23545;&#27773;&#36710;&#21046;&#36896;&#36807;&#31243;&#36827;&#34892;&#25805;&#20316;&#21487;&#35265;&#24615;&#21644;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Hierarchical energy signatures using machine learning for operational visibility and diagnostics in automotive manufacturing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15962
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23618;&#27425;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#27773;&#36710;&#21046;&#36896;&#36807;&#31243;&#30340;&#33021;&#37327;&#28040;&#32791;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#21487;&#23454;&#29616;&#26356;&#22909;&#30340;&#25805;&#20316;&#21487;&#35265;&#24615;&#21644;&#35782;&#21035;&#33410;&#33021;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#36896;&#19994;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#21253;&#21547;&#20102;&#25805;&#20316;&#21487;&#35265;&#24615;&#21644;&#35786;&#26029;&#25152;&#38656;&#30340;&#37325;&#35201;&#36807;&#31243;&#29305;&#24449;&#12290;&#36825;&#20123;&#29305;&#24449;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#65292;&#20174;&#26376;&#24230;&#21040;&#20122;&#31186;&#32423;&#20998;&#36776;&#29575;&#19981;&#31561;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#27773;&#36710;&#21046;&#36896;&#36807;&#31243;&#30340;&#29305;&#24449;&#20174;&#27833;&#28422;&#36710;&#38388;&#30005;&#21147;&#28040;&#32791;&#25968;&#25454;&#20013;&#65292;&#28085;&#30422;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#65288;&#21608;&#24230;&#21644;&#26085;&#24230;&#65289;&#12290;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20197;&#21450;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#32467;&#21512;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#29992;&#20110;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#19987;&#19994;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#20102;&#25152;&#24320;&#21457;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25805;&#20316;&#21487;&#35265;&#24615;&#21644;&#35782;&#21035;&#33410;&#33021;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15962v1 Announce Type: new  Abstract: Manufacturing energy consumption data contains important process signatures required for operational visibility and diagnostics. These signatures may be of different temporal scales, ranging from monthly to sub-second resolutions. We introduce a hierarchical machine learning approach to identify automotive process signatures from paint shop electricity consumption data at varying temporal scales (weekly and daily). A Multi-Layer Perceptron (MLP), a Convolutional Neural Network (CNN), and Principal Component Analysis (PCA) combined with Logistic Regression (LR) are used for the analysis. We validate the utility of the developed algorithms with subject matter experts for (i) better operational visibility, and (ii) identifying energy saving opportunities.
&lt;/p&gt;</description></item><item><title>&#28145;&#20837;&#30740;&#31350;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20957;&#32858;&#29616;&#35937;&#21644;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#33258;&#21457;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#22797;&#26434;&#24615;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#26377;&#25928;&#21160;&#21147;&#23398;&#30340;&#29190;&#28856;&#24615;&#36136;&#21644;&#20957;&#32858;&#21457;&#29983;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.15958</link><description>&lt;p&gt;
&#35770;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#65306;&#21021;&#22987;&#20957;&#32858;
&lt;/p&gt;
&lt;p&gt;
On the dynamics of three-layer neural networks: initial condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15958
&lt;/p&gt;
&lt;p&gt;
&#28145;&#20837;&#30740;&#31350;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20957;&#32858;&#29616;&#35937;&#21644;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#33258;&#21457;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#22797;&#26434;&#24615;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#26377;&#25928;&#21160;&#21147;&#23398;&#30340;&#29190;&#28856;&#24615;&#36136;&#21644;&#20957;&#32858;&#21457;&#29983;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#21644;&#29702;&#35770;&#30740;&#31350;&#26174;&#31034;&#65292;&#24403;&#21021;&#22987;&#21270;&#20026;&#23567;&#20540;&#26102;&#65292;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#26435;&#37325;&#20250;&#25910;&#25947;&#21040;&#23396;&#31435;&#30340;&#26041;&#21521;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#20957;&#32858;&#65292;&#34920;&#26126;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#33258;&#21457;&#22320;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#12290; &#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#20957;&#32858;&#29616;&#35937;&#32972;&#21518;&#30340;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#19982;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36827;&#34892;&#21306;&#20998;&#12290; &#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26377;&#25928;&#21160;&#21147;&#23398;&#30340;&#29190;&#28856;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#21457;&#29983;&#20957;&#32858;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#36825;&#20123;&#21457;&#29616;&#24471;&#21040;&#20102;&#23454;&#39564;&#32467;&#26524;&#30340;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20957;&#32858;&#19982;&#28145;&#24230;&#30697;&#38453;&#22240;&#23376;&#20998;&#35299;&#20013;&#35266;&#23519;&#21040;&#30340;&#20302;&#31209;&#20559;&#24046;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15958v1 Announce Type: new  Abstract: Empirical and theoretical works show that the input weights of two-layer neural networks, when initialized with small values, converge towards isolated orientations. This phenomenon, referred to as condensation, indicates that the gradient descent methods tend to spontaneously reduce the complexity of neural networks during the training process. In this work, we elucidate the mechanisms behind the condensation phenomena occurring in the training of three-layer neural networks and distinguish it from the training of two-layer neural networks. Through rigorous theoretical analysis, we establish the blow-up property of effective dynamics and present a sufficient condition for the occurrence of condensation, findings that are substantiated by experimental results. Additionally, we explore the association between condensation and the low-rank bias observed in deep matrix factorization.
&lt;/p&gt;</description></item><item><title>DynaMITE-RL &#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#30340;&#28508;&#22312;&#20449;&#24687;&#12289;&#20250;&#35805;&#25513;&#30721;&#21644;&#20808;&#39564;&#28508;&#22312;&#26465;&#20214;&#31561;&#20851;&#38190;&#20462;&#25913;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26356;&#20248;&#24322;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25512;&#29702;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15957</link><description>&lt;p&gt;
DynaMITE-RL: &#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#29992;&#20110;&#25913;&#36827;&#26102;&#38388;&#20803;&#20803;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15957
&lt;/p&gt;
&lt;p&gt;
DynaMITE-RL &#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#30340;&#28508;&#22312;&#20449;&#24687;&#12289;&#20250;&#35805;&#25513;&#30721;&#21644;&#20808;&#39564;&#28508;&#22312;&#26465;&#20214;&#31561;&#20851;&#38190;&#20462;&#25913;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26356;&#20248;&#24322;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25512;&#29702;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;DynaMITE-RL&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#24378;&#21270;&#23398;&#20064;(meta-RL)&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28508;&#22312;&#29366;&#24577;&#20197;&#19981;&#21516;&#36895;&#29575;&#28436;&#21464;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#36817;&#20284;&#25512;&#29702;&#12290;&#25105;&#20204;&#24314;&#27169;&#21095;&#38598;&#20250;&#35805; - &#21095;&#38598;&#30340;&#37096;&#20998;&#65292;&#22312;&#36825;&#20123;&#37096;&#20998;&#20013;&#65292;&#28508;&#22312;&#29366;&#24577;&#26159;&#22266;&#23450;&#30340; - &#24182;&#23545;&#29616;&#26377;&#30340;meta-RL&#26041;&#27861;&#25552;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#20462;&#25913;&#65306;&#21095;&#38598;&#20869;&#37096;&#28508;&#22312;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#65292;&#20250;&#35805;&#25513;&#30721;&#21644;&#20808;&#39564;&#28508;&#22312;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#39046;&#22495;&#23637;&#31034;&#20102;&#36825;&#20123;&#20462;&#25913;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#31163;&#25955;Gridworld&#29615;&#22659;&#21040;&#36830;&#32493;&#25511;&#21046;&#21644;&#27169;&#25311;&#26426;&#22120;&#20154;&#36741;&#21161;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;DynaMITE-RL&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#25512;&#29702;&#32467;&#26524;&#26041;&#38754;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15957v1 Announce Type: new  Abstract: We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to approximate inference in environments where the latent state evolves at varying rates. We model episode sessions - parts of the episode where the latent state is fixed - and propose three key modifications to existing meta-RL methods: consistency of latent information within sessions, session masking, and prior latent conditioning. We demonstrate the importance of these modifications in various domains, ranging from discrete Gridworld environments to continuous-control and simulated robot assistive tasks, demonstrating that DynaMITE-RL significantly outperforms state-of-the-art baselines in sample efficiency and inference returns.
&lt;/p&gt;</description></item><item><title>GreenLLaMA&#26159;&#19968;&#31181;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35299;&#27602;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#24179;&#21488;&#35821;&#26009;&#24211;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15951</link><description>&lt;p&gt;
GreenLLaMA: &#19968;&#31181;&#24102;&#26377;&#35299;&#37322;&#30340;&#35299;&#27602;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GreenLLaMA: A Framework for Detoxification with Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15951
&lt;/p&gt;
&lt;p&gt;
GreenLLaMA&#26159;&#19968;&#31181;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35299;&#27602;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#24179;&#21488;&#35821;&#26009;&#24211;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#35299;&#27602;&#30340;&#30740;&#31350;&#24037;&#20316;&#20998;&#25955;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#65292;&#22240;&#20026;&#23427;&#20204;&#24182;&#27809;&#26377;&#28085;&#30422;&#21040;&#30495;&#23454;&#22330;&#26223;&#20013;&#25152;&#38656;&#30340;&#25152;&#26377;&#35299;&#27602;&#26041;&#38754;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#24320;&#21457;&#35299;&#27602;&#27169;&#22411;&#30340;&#20219;&#21153;&#23616;&#38480;&#22312;&#20165;&#35265;&#36807;&#30340;&#24179;&#21488;&#23376;&#38598;&#19978;&#65292;&#27809;&#26377;&#25506;&#35752;&#27169;&#22411;&#22312;&#26410;&#30693;&#24179;&#21488;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#24037;&#20316;&#27809;&#26377;&#35299;&#20915;&#19981;&#21487;&#35299;&#27602;&#24615;&#36825;&#19968;&#29616;&#35937;&#65292;&#21363;&#27602;&#24615;&#25991;&#26412;&#26080;&#27861;&#22312;&#19981;&#25913;&#21464;&#21547;&#20041;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35299;&#27602;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GreenLLaMA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35299;&#27602;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#36731;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#36328;&#24179;&#21488;&#20266;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#24212;&#29992;&#22810;&#27493;&#25968;&#25454;&#22788;&#29702;&#21644;&#29983;&#25104;&#31574;&#30053;&#21033;&#29992;ChatGPT&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36328;&#24179;&#21488;&#35821;&#26009;&#24211;&#35757;&#32451;&#19968;&#22871;&#35299;&#27602;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#27602;&#27169;&#22411;&#20248;&#20110;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15951v1 Announce Type: cross  Abstract: Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose GreenLLaMA, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated par
&lt;/p&gt;</description></item><item><title>&#20248;&#21270;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#32447;&#24615;&#31995;&#32479;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;RMINRES&#26041;&#27861;&#22312;Python&#21644;PyTorch&#20013;&#30340;&#24212;&#29992;</title><link>https://arxiv.org/abs/2402.15941</link><description>&lt;p&gt;
&#22312;Python&#20013;&#23454;&#29616;&#29992;&#20110;&#32447;&#24615;&#31995;&#32479;&#30340;&#22238;&#25910;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implementing Recycling Methods for Linear Systems in Python with an Application to Multiple Objective Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15941
&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#32447;&#24615;&#31995;&#32479;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;RMINRES&#26041;&#27861;&#22312;Python&#21644;PyTorch&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#24085;&#32023;&#22270;&#21069;&#27839;&#26102;&#65292;&#32447;&#24615;&#31995;&#32479;&#30340;&#24207;&#21015;&#20986;&#29616;&#22312;&#39044;&#27979;&#26657;&#27491;&#26041;&#27861;&#20013;&#12290;&#19982;&#20002;&#24323;&#35299;&#20915;&#19968;&#20010;&#31995;&#32479;&#26102;&#29983;&#25104;&#30340;&#20449;&#24687;&#19981;&#21516;&#65292;&#37325;&#26032;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#21487;&#33021;&#23545;&#21518;&#32493;&#31995;&#32479;&#26377;&#21033;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#24120;&#35265;&#30340;&#22238;&#25910;&#26041;&#27861;&#26469;&#20943;&#23569;&#35299;&#20915;&#32447;&#24615;&#31995;&#32479;&#26102;&#30340;&#24635;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22238;&#25910;&#26368;&#23567;&#27531;&#24046;&#65288;RMINRES&#65289;&#26041;&#27861;&#20197;&#21450;&#31995;&#25968;&#30697;&#38453;&#20043;&#38388;&#30340;&#26144;&#23556;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;&#36825;&#20123;&#26041;&#27861;&#23436;&#20840;&#25972;&#21512;&#21040;Enouen&#31561;&#20154;&#65288;2022&#24180;&#65289;&#20351;&#29992;&#30340;&#36719;&#20214;&#20013;&#65292;&#24517;&#39035;&#22312;Python&#21644;PyTorch&#20013;&#37117;&#26377;&#27599;&#20010;&#26041;&#27861;&#30340;&#21487;&#29992;&#29256;&#26412;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#35745;&#31639;&#36825;&#20123;&#22238;&#25910;&#31574;&#30053;&#30340;&#39640;&#25928;Python&#23454;&#29616;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65288;&#20197;&#21450;&#19968;&#20123;&#27491;&#22312;&#36827;&#34892;&#20013;&#30340;&#65289;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#22312;Python&#21644;PyTorch&#20013;&#23454;&#29616;RMINRES&#24182;&#23558;&#20854;&#28155;&#21152;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15941v1 Announce Type: cross  Abstract: Sequences of linear systems arise in the predictor-corrector method when computing the Pareto front for multi-objective optimization. Rather than discarding information generated when solving one system, it may be advantageous to recycle information for subsequent systems. To accomplish this, we seek to reduce the overall cost of computation when solving linear systems using common recycling methods. In this work, we assessed the performance of recycling minimum residual (RMINRES) method along with a map between coefficient matrices. For these methods to be fully integrated into the software used in Enouen et al. (2022), there must be working version of each in both Python and PyTorch. Herein, we discuss the challenges we encountered and solutions undertaken (and some ongoing) when computing efficient Python implementations of these recycling strategies. The goal of this project was to implement RMINRES in Python and PyTorch and add it
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#26102;&#31354;&#23398;&#20064;&#32593;&#32476;&#65288;DeepSSL&#65289;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26102;&#31354;&#20808;&#39564;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22270;&#20687;&#37325;&#24314;&#26041;&#26696;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.15939</link><description>&lt;p&gt;
&#28145;&#24230;&#21487;&#20998;&#31163;&#26102;&#31354;&#23398;&#20064;&#29992;&#20110;&#24555;&#36895;&#21160;&#24577;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Deep Separable Spatiotemporal Learning for Fast Dynamic Cardiac MRI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15939
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#26102;&#31354;&#23398;&#20064;&#32593;&#32476;&#65288;DeepSSL&#65289;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26102;&#31354;&#20808;&#39564;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22270;&#20687;&#37325;&#24314;&#26041;&#26696;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22312;&#24515;&#33039;&#35786;&#26029;&#20013;&#36215;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#23454;&#29616;&#24555;&#36895;&#25104;&#20687;&#65292;k-&#31354;&#38388;&#25968;&#25454;&#21487;&#20197;&#36827;&#34892;&#27424;&#37319;&#26679;&#65292;&#20294;&#22270;&#20687;&#37325;&#24314;&#38754;&#20020;&#30528;&#39640;&#32500;&#22788;&#29702;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38477;&#32500;&#21487;&#20998;&#31163;&#23398;&#20064;&#26041;&#26696;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#25968;&#25454;&#26497;&#20026;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#34920;&#29616;&#21331;&#36234;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#20854;&#19982;&#26102;&#31354;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#21487;&#20998;&#31163;&#26102;&#31354;&#23398;&#20064;&#32593;&#32476;&#65288;DeepSSL&#65289;&#65292;&#35813;&#32593;&#32476;&#23637;&#24320;&#20102;&#19968;&#20010;&#20855;&#26377;&#26102;&#38388;&#20302;&#31209;&#24615;&#21644;&#31354;&#38388;&#31232;&#30095;&#24615;&#30340;&#37325;&#24314;&#27169;&#22411;&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;&#20013;&#38388;&#36755;&#20986;&#34987;&#21487;&#35270;&#21270;&#65292;&#20197;&#25552;&#20379;&#26377;&#20851;&#32593;&#32476;&#34892;&#20026;&#30340;&#35265;&#35299;&#24182;&#22686;&#24378;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#23545;&#24515;&#33039;&#30701;&#29255;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;DeepSSL&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15939v1 Announce Type: cross  Abstract: Dynamic magnetic resonance imaging (MRI) plays an indispensable role in cardiac diagnosis. To enable fast imaging, the k-space data can be undersampled but the image reconstruction poses a great challenge of high-dimensional processing. This challenge leads to necessitate extensive training data in many deep learning reconstruction methods. This work proposes a novel and efficient approach, leveraging a dimension-reduced separable learning scheme that excels even with highly limited training data. We further integrate it with spatiotemporal priors to develop a Deep Separable Spatiotemporal Learning network (DeepSSL), which unrolls an iteration process of a reconstruction model with both temporal low-rankness and spatial sparsity. Intermediate outputs are visualized to provide insights into the network's behavior and enhance its interpretability. Extensive results on cardiac cine datasets show that the proposed DeepSSL is superior to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15938</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#25110;&#35760;&#24518;&#65306;&#25968;&#25454;&#27745;&#26579;&#19982;&#21487;&#20449;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#30340;&#35828;&#27861;&#36890;&#24120;&#26159;&#36890;&#36807;&#22312;&#24320;&#25918;&#33719;&#21462;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#26469;&#25903;&#25345;&#30340;&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24222;&#22823;&#35268;&#27169;&#21644;&#24191;&#27867;&#26469;&#28304;&#65292;&#23427;&#21487;&#33021;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#21253;&#21547;&#27979;&#35797;&#25968;&#25454;&#65292;&#23548;&#33268;LLMs&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#20197;&#21450;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#23545;&#20110;LLMs&#26469;&#35828;&#26816;&#27979;&#21644;&#20943;&#36731;&#25968;&#25454;&#27745;&#26579;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CDD&#65292;&#21363;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;CDD&#12290;CDD&#20165;&#38656;&#35201;&#37319;&#26679;&#25991;&#26412;&#26469;&#26816;&#27979;&#25968;&#25454;&#27745;&#26579;&#65292;&#36890;&#36807;&#35782;&#21035;LLMs&#36755;&#20986;&#20998;&#24067;&#30340;&#23792;&#20540;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#35780;&#20272;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;TED&#65306;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38382;&#39064;&#26465;&#20214;&#30340;2D&#35270;&#22270;&#36873;&#25321;&#36807;&#31243;&#21644;&#21452;&#20998;&#25903;Transformer&#32467;&#26500;&#65292;&#23558;2D&#30693;&#35782;&#25972;&#21512;&#21040;3D-VQA&#31995;&#32479;&#20013;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;3D&#35270;&#35273;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15933</link><description>&lt;p&gt;
&#36328;&#36234;2D&#21644;3D&#35270;&#35273;&#38382;&#31572;&#20043;&#38388;&#30340;&#40511;&#27807;&#65306;&#19968;&#31181;&#29992;&#20110;3D VQA&#30340;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15933
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38382;&#39064;&#26465;&#20214;&#30340;2D&#35270;&#22270;&#36873;&#25321;&#36807;&#31243;&#21644;&#21452;&#20998;&#25903;Transformer&#32467;&#26500;&#65292;&#23558;2D&#30693;&#35782;&#25972;&#21512;&#21040;3D-VQA&#31995;&#32479;&#20013;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;3D&#35270;&#35273;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;3D&#35270;&#35273;&#38382;&#31572;&#65288;3D VQA&#65289;&#20013;&#65292;&#20805;&#20998;&#27880;&#37322;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#26377;&#38480;&#30340;&#35270;&#35273;&#20869;&#23481;&#22810;&#26679;&#24615;&#38459;&#30861;&#20102;&#23545;&#26032;&#39062;&#22330;&#26223;&#21644;3D&#27010;&#24565;&#30340;&#27867;&#21270;&#65288;&#22914;ScanQA&#21644;SQA&#25968;&#25454;&#38598;&#20165;&#21033;&#29992;&#20102;&#32422;800&#20010;&#22330;&#26223;&#65289;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#34917;&#20805;2D&#20449;&#24687;&#26469;&#36741;&#21161;3D&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#65306;&#23427;&#20204;&#35201;&#20040;&#20351;&#29992;&#24341;&#20837;&#36807;&#20110;&#22797;&#26434;&#19988;&#26377;&#26102;&#19982;&#38382;&#39064;&#26080;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#30340;&#33258;&#19978;&#32780;&#19979;&#30340;2D&#35270;&#22270;&#65292;&#35201;&#20040;&#20381;&#38752;&#26469;&#33258;2D VLM&#30340;&#20840;&#23616;&#32858;&#21512;&#22330;&#26223;/&#22270;&#20687;&#32423;&#34920;&#31034;&#65292;&#20174;&#32780;&#20002;&#22833;&#20102;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#35821;&#35328;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;2D&#35270;&#22270;&#36873;&#25321;&#36807;&#31243;&#65292;&#20934;&#30830;&#22320;&#25351;&#20986;&#20102;&#20851;&#38190;&#35270;&#35273;&#32447;&#32034;&#30340;&#35821;&#20041;&#30456;&#20851;2D&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21452;&#20998;&#25903;Transformer&#32467;&#26500;&#23558;&#36825;&#31181;2D&#30693;&#35782;&#25972;&#21512;&#21040;3D-VQA&#31995;&#32479;&#20013;&#12290;&#36825;&#31181;&#32467;&#26500;&#37319;&#29992;&#20102;&#21452;Transformer&#35774;&#35745;&#65292;&#32039;&#20945;&#22320;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15933v1 Announce Type: cross  Abstract: In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combine
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;RLlib-IMPALA&#26694;&#26550;&#30340;&#21487;&#25193;&#23637;Volt-VAR&#20248;&#21270;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#20998;&#24067;&#24335;&#35745;&#31639;&#21152;&#36895;&#20102;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;VVO&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.15932</link><description>&lt;p&gt;
&#20351;&#29992;RLlib-IMPALA&#26694;&#26550;&#30340;&#21487;&#25193;&#23637;Volt-VAR&#20248;&#21270;&#65306;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Volt-VAR Optimization using RLlib-IMPALA Framework: A Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15932
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;RLlib-IMPALA&#26694;&#26550;&#30340;&#21487;&#25193;&#23637;Volt-VAR&#20248;&#21270;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#20998;&#24067;&#24335;&#35745;&#31639;&#21152;&#36895;&#20102;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;VVO&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#21147;&#31995;&#32479;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;Volt-VAR&#20248;&#21270;(VVO)&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#21487;&#20877;&#29983;&#33021;&#28304;&#19981;&#26029;&#34701;&#20837;&#20854;&#20013;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;VVO&#26041;&#27861;&#22312;&#24222;&#22823;&#19988;&#21160;&#24577;&#21464;&#21270;&#30340;&#30005;&#21147;&#31995;&#32479;&#20013;&#24120;&#24120;&#21463;&#21040;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#28508;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#20855;&#20307;&#21033;&#29992;&#20102;&#37325;&#35201;&#24615;&#21152;&#26435;Actor-Learner&#26550;&#26500;(IMPALA)&#31639;&#27861;&#65292;&#24182;&#22312;RAY&#24179;&#21488;&#19978;&#25191;&#34892;&#12290;&#36825;&#20010;&#26694;&#26550;&#24314;&#31435;&#22312;RLlib&#20043;&#19978;&#65292;RLlib&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#19994;&#26631;&#20934;&#65292;&#24039;&#22937;&#22320;&#21033;&#29992;&#20102;RAY&#25552;&#20379;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#33021;&#21147;&#21644;&#20808;&#36827;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21151;&#33021;&#12290;&#36825;&#20010;&#35774;&#35745;&#26174;&#33879;&#21152;&#24555;&#20102;VVO&#35299;&#31354;&#38388;&#20013;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20294;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15932v1 Announce Type: new  Abstract: In the rapidly evolving domain of electrical power systems, the Volt-VAR optimization (VVO) is increasingly critical, especially with the burgeoning integration of renewable energy sources. Traditional approaches to learning-based VVO in expansive and dynamically changing power systems are often hindered by computational complexities. To address this challenge, our research presents a novel framework that harnesses the potential of Deep Reinforcement Learning (DRL), specifically utilizing the Importance Weighted Actor-Learner Architecture (IMPALA) algorithm, executed on the RAY platform. This framework, built upon RLlib-an industry-standard in Reinforcement Learning-ingeniously capitalizes on the distributed computing capabilities and advanced hyperparameter tuning offered by RAY. This design significantly expedites the exploration and exploitation phases in the VVO solution space. Our empirical results demonstrate that our approach not 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#29992;&#20110;&#27491;&#24335;&#35748;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#35777;&#20070;&#23450;&#37327;&#21270;&#19988;&#21253;&#21547;&#39640;&#32622;&#20449;&#24230;&#30340;&#27010;&#29575;&#30028;&#38480;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#25552;&#39640;&#65292;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15929</link><description>&lt;p&gt;
QuaCer-C&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#23450;&#37327;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#29992;&#20110;&#27491;&#24335;&#35748;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#35777;&#20070;&#23450;&#37327;&#21270;&#19988;&#21253;&#21547;&#39640;&#32622;&#20449;&#24230;&#30340;&#27010;&#29575;&#30028;&#38480;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#25552;&#39640;&#65292;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30740;&#31350;&#24182;&#26410;&#23545;LLMs&#30340;&#34920;&#29616;&#25552;&#20379;&#27491;&#24335;&#30340;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#25105;&#20204;&#22312;&#27492;&#23545;&#30693;&#21517;LLMs&#30340;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#27491;&#24335;&#35748;&#35777;&#12290;&#25105;&#20204;&#30340;&#35777;&#20070;&#26159;&#23450;&#37327;&#30340; - &#23427;&#20204;&#21253;&#25324;&#23545;&#30446;&#26631;LLM&#22312;&#20219;&#20309;&#30456;&#20851;&#30693;&#35782;&#29702;&#35299;&#25552;&#31034;&#19978;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#27010;&#29575;&#30340;&#39640;&#32622;&#20449;&#24230;&#32039;&#23494;&#30028;&#38480;&#12290;&#25105;&#20204;&#38024;&#23545;Llama&#12289;Vicuna&#21644;Mistral LLMs&#30340;&#35777;&#20070;&#34920;&#26126;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#38543;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#24182;&#19988;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15929v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive performance on several benchmarks. However, traditional studies do not provide formal guarantees on the performance of LLMs. In this work, we propose a novel certification framework for LLM, QuaCer-C, wherein we formally certify the knowledge-comprehension capabilities of popular LLMs. Our certificates are quantitative - they consist of high-confidence, tight bounds on the probability that the target LLM gives the correct answer on any relevant knowledge comprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs indicate that the knowledge comprehension capability improves with an increase in the number of parameters and that the Mistral model is less performant than the rest in this evaluation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#34920;&#26126;&#23545;&#20110;&#20855;&#26377;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#30340;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#65292;&#35774;&#32622;&#19968;&#20010;&#24658;&#23450;&#20294;&#36739;&#22823;&#30340;&#27493;&#38271;&#65292;&#22312;&#21021;&#22987;&#38663;&#33633;&#21518;&#21487;&#20197;&#23454;&#29616;&#36739;&#24555;&#30340;&#25910;&#25947;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#27493;&#39588;&#21518;&#21487;&#20197;&#36798;&#21040;&#21152;&#36895;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#21160;&#37327;&#25110;&#21464;&#27493;&#38271;&#35843;&#24230;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.15926</link><description>&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#30340;&#22823;&#27493;&#26799;&#24230;&#19979;&#38477;&#65306;&#25439;&#22833;&#30340;&#38750;&#21333;&#35843;&#24615;&#25552;&#39640;&#20102;&#20248;&#21270;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15926
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#34920;&#26126;&#23545;&#20110;&#20855;&#26377;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#30340;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#65292;&#35774;&#32622;&#19968;&#20010;&#24658;&#23450;&#20294;&#36739;&#22823;&#30340;&#27493;&#38271;&#65292;&#22312;&#21021;&#22987;&#38663;&#33633;&#21518;&#21487;&#20197;&#23454;&#29616;&#36739;&#24555;&#30340;&#25910;&#25947;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#27493;&#39588;&#21518;&#21487;&#20197;&#36798;&#21040;&#21152;&#36895;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#21160;&#37327;&#25110;&#21464;&#27493;&#38271;&#35843;&#24230;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#19982;&#20855;&#26377;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#30340;&#36923;&#36753;&#22238;&#24402;&#32467;&#21512;&#20351;&#29992;&#30340;&#24658;&#23450;&#27493;&#38271;&#24773;&#20917;&#65292;&#20854;&#20013;&#24658;&#23450;&#27493;&#38271;$\eta$&#38750;&#24120;&#22823;&#65292;&#20197;&#33267;&#20110;&#25439;&#22833;&#22312;&#21021;&#22987;&#38454;&#27573;&#20250;&#38663;&#33633;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GD&#22312;$\mathcal{O}(\eta)$&#27493;&#20869;&#36805;&#36895;&#36864;&#20986;&#36825;&#31181;&#21021;&#22987;&#38663;&#33633;&#38454;&#27573;&#65292;&#24182;&#22312;&#39069;&#22806;&#30340;$t$&#27493;&#20043;&#21518;&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{\mathcal{O}}(1 / (\eta t) )$&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#65292;&#32473;&#23450;$T$&#27493;&#30340;&#39044;&#31639;&#65292;&#20351;&#29992;&#31215;&#26497;&#30340;&#27493;&#38271;$\eta:= \Theta( T)$&#65292;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#21160;&#37327;&#25110;&#21464;&#27493;&#38271;&#35843;&#24230;&#22120;&#65292;GD&#21487;&#20197;&#23454;&#29616;&#19968;&#20010;$\tilde{\mathcal{O}}(1/T^2)$&#30340;&#21152;&#36895;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#25216;&#26415;&#22810;&#25165;&#22810;&#33402;&#65292;&#36824;&#21487;&#20197;&#22788;&#29702;&#19968;&#33324;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#65288;&#20854;&#20013;&#38656;&#35201;&#25351;&#25968;&#23614;&#37096;&#26469;&#23454;&#29616;$\tilde{\mathcal{O}}(1/T^2)$&#30340;&#21152;&#36895;&#65289;&#12289;&#31070;&#32463;&#20999;&#32447;&#26680;&#21306;&#22495;&#30340;&#38750;&#32447;&#24615;&#39044;&#27979;&#22120;&#65292;&#20197;&#21450;&#20855;&#26377;&#22823;&#27493;&#38271;&#30340;&#22312;&#32447;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15926v1 Announce Type: new  Abstract: We consider gradient descent (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\eta$ is so large that the loss initially oscillates. We show that GD exits this initial oscillatory phase rapidly -- in $\mathcal{O}(\eta)$ steps -- and subsequently achieves an $\tilde{\mathcal{O}}(1 / (\eta t) )$ convergence rate after $t$ additional steps. Our results imply that, given a budget of $T$ steps, GD can achieve an accelerated loss of $\tilde{\mathcal{O}}(1/T^2)$ with an aggressive stepsize $\eta:= \Theta( T)$, without any use of momentum or variable stepsize schedulers. Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\tilde{\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the neural tangent kernel regime, and online stochastic gradient descent (SGD) with a large stepsize, under sui
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#22312;&#23454;&#26102;&#20998;&#26512;&#20013;&#39044;&#27979;&#30005;&#23376;&#31454;&#25216;&#27604;&#36187;&#32467;&#26524;&#65292;&#20165;&#21033;&#29992;&#29609;&#23478;&#29983;&#21629;&#20540;&#25351;&#26631;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#65292;&#36739;&#20043;&#20256;&#32479;&#26041;&#27861;&#21644;Transformer&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15923</link><description>&lt;p&gt;
&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#22312;&#30005;&#23376;&#31454;&#25216;&#20013;&#39044;&#27979;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Predicting Outcomes in Video Games with Long Short Term Memory Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15923
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#22312;&#23454;&#26102;&#20998;&#26512;&#20013;&#39044;&#27979;&#30005;&#23376;&#31454;&#25216;&#27604;&#36187;&#32467;&#26524;&#65292;&#20165;&#21033;&#29992;&#29609;&#23478;&#29983;&#21629;&#20540;&#25351;&#26631;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#65292;&#36739;&#20043;&#20256;&#32479;&#26041;&#27861;&#21644;Transformer&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#30005;&#23376;&#31454;&#25216;&#20013;&#30340;&#32988;&#32773;&#65292;&#22312;&#23454;&#26102;&#20998;&#26512;&#20013;&#20855;&#26377;&#28508;&#21147;&#36827;&#19968;&#27493;&#21560;&#24341;&#35266;&#30475;&#20027;&#35201;&#38182;&#26631;&#36187;&#30340;&#35266;&#20247;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28216;&#25103;&#20013;&#28041;&#21450;&#19981;&#21516;&#29609;&#23478;&#31574;&#30053;&#21644;&#20915;&#31574;&#30340;&#19981;&#21487;&#39044;&#27979;&#21464;&#37327;&#65292;&#36827;&#34892;&#36825;&#31181;&#23454;&#26102;&#39044;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23454;&#26102;&#39044;&#27979;&#32988;&#21033;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#35270;&#39057;&#28216;&#25103;&#27604;&#36187;&#20013;&#35266;&#20247;&#30340;&#21442;&#19982;&#24230;&#12290;&#25105;&#20204;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTMs&#65289;&#30340;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#20165;&#20351;&#29992;&#27599;&#20010;&#29609;&#23478;&#30340;&#29983;&#21629;&#20540;&#25351;&#26631;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#26469;&#39640;&#25928;&#39044;&#27979;&#32988;&#36127;&#32467;&#26524;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#22312;&#32463;&#20856;&#30340;&#20004;&#20154;&#34903;&#26426;&#28216;&#25103;&#12298;&#36229;&#32423;&#34903;&#38712;II Turbo&#12299;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#26032;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65307;&#21363;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25214;&#21040;&#30340;Transformer&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#24076;&#26395;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15923v1 Announce Type: cross  Abstract: Forecasting winners in E-sports with real-time analytics has the potential to further engage audiences watching major tournament events. However, making such real-time predictions is challenging due to unpredictable variables within the game involving diverse player strategies and decision-making. Our work attempts to enhance audience engagement within video game tournaments by introducing a real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs) based approach enables efficient predictions of win-lose outcomes by only using the health indicator of each player as a time series. As a proof of concept, we evaluate our model's performance within a classic, two-player arcade game, Super Street Fighter II Turbo. We also benchmark our method against state of the art methods for time series forecasting; i.e. Transformer models found in large language models (LLMs). Finally, we open-source our data set and code in hopes
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25513;&#30721;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;GNN&#22312;&#25311;&#21512;&#27700;&#31995;&#32479;&#21183;&#33021;&#34920;&#38754;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#19978;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#20351;&#29992;&#20854;&#20182;&#39044;&#35757;&#32451;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15921</link><description>&lt;p&gt;
&#31070;&#32463;&#21183;&#39044;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Pretraining Strategy for Neural Potentials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15921
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25513;&#30721;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;GNN&#22312;&#25311;&#21512;&#27700;&#31995;&#32479;&#21183;&#33021;&#34920;&#38754;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#19978;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#20351;&#29992;&#20854;&#20182;&#39044;&#35757;&#32451;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#25513;&#30721;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#25311;&#21512;&#21183;&#33021;&#34920;&#38754;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#27700;&#31995;&#32479;&#20013;&#12290;GNNs&#36890;&#36807;&#20174;&#20998;&#23376;&#20013;&#24674;&#22797;&#19982;&#25513;&#30721;&#30340;&#21407;&#23376;&#30456;&#20851;&#30340;&#31354;&#38388;&#20449;&#24687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#21407;&#23376;&#21147;&#22330;&#19978;&#36827;&#34892;&#36716;&#31227;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#36825;&#31181;&#39044;&#35757;&#32451;&#65292;GNNs&#23398;&#20064;&#20102;&#20851;&#20110;&#20998;&#23376;&#31995;&#32479;&#30340;&#32467;&#26500;&#21644;&#28508;&#22312;&#29289;&#29702;&#20449;&#24687;&#30340;&#26377;&#24847;&#20041;&#20808;&#39564;&#65292;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#26159;&#26377;&#29992;&#30340;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#20351;&#29992;&#20854;&#20182;&#39044;&#35757;&#32451;&#25216;&#26415;&#65288;&#22914;&#21435;&#22122;&#65289;&#30340;GNNs&#65292;&#22312;&#31934;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#19978;&#30340;&#25552;&#39640;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#36866;&#29992;&#20110;&#20197;&#33021;&#37327;&#20026;&#20013;&#24515;&#21644;&#20197;&#21147;&#20026;&#20013;&#24515;&#30340;GNNs&#12290;&#36825;&#31181;&#26041;&#27861;&#23637;&#31034;&#20102;&#23427;&#22312;&#25311;&#21512;&#20998;&#23376;&#21147;&#22330;&#26041;&#38754;&#25552;&#39640;&#20102;GNNs&#30340;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15921v1 Announce Type: new  Abstract: We propose a mask pretraining method for Graph Neural Networks (GNNs) to improve their performance on fitting potential energy surfaces, particularly in water systems. GNNs are pretrained by recovering spatial information related to masked-out atoms from molecules, then transferred and finetuned on atomic forcefields. Through such pretraining, GNNs learn meaningful prior about structural and underlying physical information of molecule systems that are useful for downstream tasks. From comprehensive experiments and ablation studies, we show that the proposed method improves the accuracy and convergence speed compared to GNNs trained from scratch or using other pretraining techniques such as denoising. On the other hand, our pretraining method is suitable for both energy-centric and force-centric GNNs. This approach showcases its potential to enhance the performance and data efficiency of GNNs in fitting molecular force fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23467;&#39048;&#30284;&#32454;&#32990;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;CNNs&#36827;&#34892;&#24494;&#35843;&#24182;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#26368;&#23567;&#21270;&#35823;&#20998;&#31867;&#25104;&#26412;&#65292;&#36798;&#21040;&#20102;97.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.15905</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#23545;&#27604;&#21644;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#29992;&#20110;&#23467;&#39048;&#30284;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23467;&#39048;&#30284;&#32454;&#32990;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;CNNs&#36827;&#34892;&#24494;&#35843;&#24182;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#26368;&#23567;&#21270;&#35823;&#20998;&#31867;&#25104;&#26412;&#65292;&#36798;&#21040;&#20102;97.29%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#23545;&#23467;&#39048;&#30284;&#32454;&#32990;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#24494;&#35843;&#20102;&#20116;&#20010;&#39044;&#35757;&#32451;&#30340;CNNs&#65292;&#24182;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#26356;&#39640;&#30456;&#20851;&#25104;&#26412;&#25110;&#37325;&#35201;&#24615;&#30340;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#26469;&#26368;&#23567;&#21270;&#24635;&#30340;&#35823;&#20998;&#31867;&#25104;&#26412;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20351;&#27169;&#22411;&#26356;&#25797;&#38271;&#25429;&#25417;&#37325;&#35201;&#29305;&#24449;&#21644;&#27169;&#24335;&#12290;&#23545;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;SIPaKMeD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102;97.29%&#30340;&#20934;&#30830;&#29575;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#31995;&#32479;&#26356;&#21152;&#21487;&#20449;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#31181;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35299;&#37322;&#27169;&#22411;&#26159;&#22914;&#20309;&#20570;&#20986;&#20855;&#20307;&#20915;&#31574;&#30340;&#12290;&#31995;&#32479;&#30340;&#23454;&#29616;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040; - https://github.com/isha-67/CervicalCancerStudy.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15905v1 Announce Type: cross  Abstract: This paper proposes an efficient system for classifying cervical cancer cells using pre-trained convolutional neural networks (CNNs). We first fine-tune five pre-trained CNNs and minimize the overall cost of misclassification by prioritizing accuracy for certain classes that have higher associated costs or importance. To further enhance the performance of the models, supervised contrastive learning is included to make the models more adept at capturing important features and patterns. Extensive experimentation are conducted to evaluate the proposed system on the SIPaKMeD dataset. The experimental results demonstrate the effectiveness of the developed system, achieving an accuracy of 97.29%. To make our system more trustworthy, we have employed several explainable AI techniques to interpret how the models reached a specific decision. The implementation of the system can be found at - https://github.com/isha-67/CervicalCancerStudy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ESFL&#65289;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#31471;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#27169;&#22411;&#24182;&#32771;&#34385;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#65292;&#20849;&#21516;&#20248;&#21270;&#29992;&#25143;&#31471;&#24037;&#20316;&#37327;&#21644;&#26381;&#21153;&#22120;&#31471;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.15903</link><description>&lt;p&gt;
&#39640;&#25928;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#26080;&#32447;&#35774;&#22791;&#19978;
&lt;/p&gt;
&lt;p&gt;
ESFL: Efficient Split Federated Learning over Resource-Constrained Heterogeneous Wireless Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ESFL&#65289;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#31471;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#27169;&#22411;&#24182;&#32771;&#34385;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#65292;&#20849;&#21516;&#20248;&#21270;&#29992;&#25143;&#31471;&#24037;&#20316;&#37327;&#21644;&#26381;&#21153;&#22120;&#31471;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#65288;&#20998;&#24067;&#24335;&#35774;&#22791;&#65289;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#35774;&#22791;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#30340;&#36164;&#28304;&#26159;&#19968;&#20010;&#26497;&#20855;&#21560;&#24341;&#21147;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ESFL&#65289;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20013;&#22830;&#26381;&#21153;&#22120;&#22312;&#20855;&#26377;&#24322;&#26500;&#31471;&#35774;&#22791;&#65288;EDs&#65289;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19979;&#30340;&#24378;&#22823;&#35745;&#31639;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;EDs&#20043;&#38388;&#23558;&#27169;&#22411;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#20849;&#21516;&#20248;&#21270;&#29992;&#25143;&#31471;&#24037;&#20316;&#37327;&#21644;&#26381;&#21153;&#22120;&#31471;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;&#25105;&#20204;&#23558;&#25972;&#20010;&#20248;&#21270;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#65292;&#36825;&#26159;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36845;&#20195;&#26041;&#27861;&#26469;&#26377;&#25928;&#22320;&#33719;&#24471;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15903v1 Announce Type: cross  Abstract: Federated learning (FL) allows multiple parties (distributed devices) to train a machine learning model without sharing raw data. How to effectively and efficiently utilize the resources on devices and the central server is a highly interesting yet challenging problem. In this paper, we propose an efficient split federated learning algorithm (ESFL) to take full advantage of the powerful computing capabilities at a central server under a split federated learning framework with heterogeneous end devices (EDs). By splitting the model into different submodels between the server and EDs, our approach jointly optimizes user-side workload and server-side computing resource allocation by considering users' heterogeneity. We formulate the whole optimization problem as a mixed-integer non-linear program, which is an NP-hard problem, and develop an iterative approach to obtain an approximate solution efficiently. Extensive simulations have been c
&lt;/p&gt;</description></item><item><title>ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15898</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Information-based Transductive Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15898
&lt;/p&gt;
&lt;p&gt;
ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20027;&#21160;&#23398;&#20064;&#25512;&#24191;&#21040;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#37319;&#26679;&#21463;&#38480;&#20110;&#21487;&#35775;&#38382;&#22495;&#30340;&#24773;&#20917;&#65292;&#32780;&#39044;&#27979;&#30446;&#26631;&#21487;&#33021;&#20301;&#20110;&#36825;&#20010;&#22495;&#20043;&#22806;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ITL&#65292;&#21363;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ITL&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#26368;&#23567;&#21487;&#33021;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;ITL&#65306;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;ITL&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#23433;&#20840;RL&#25511;&#21046;&#31574;&#30053;&#21644;&#35782;&#21035;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15893</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#24182;&#34892;&#23398;&#20064;&#31574;&#30053;&#21644;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15893
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#23433;&#20840;RL&#25511;&#21046;&#31574;&#30053;&#21644;&#35782;&#21035;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#37096;&#32626;RL&#31574;&#30053;&#38754;&#20020;&#30528;&#30830;&#20445;&#23433;&#20840;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#23433;&#20840;RL&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#23558;&#39044;&#23450;&#20041;&#30340;&#23433;&#20840;&#32422;&#26463;&#32435;&#20837;&#21040;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#23545;&#39044;&#23450;&#20041;&#23433;&#20840;&#32422;&#26463;&#30340;&#20381;&#36182;&#22312;&#23433;&#20840;&#25511;&#21046;RL&#20219;&#21153;&#20013;&#20855;&#26377;&#38480;&#21046;&#65292;&#22240;&#20026;&#36825;&#20123;&#32422;&#26463;&#21487;&#33021;&#26080;&#27861;&#24471;&#21040;&#25110;&#19981;&#22815;&#36866;&#24212;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#23398;&#20064;&#23433;&#20840;&#30340;RL&#25511;&#21046;&#31574;&#30053;&#24182;&#30830;&#23450;&#32473;&#23450;&#29615;&#22659;&#30340;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;pSTL&#65289;&#23433;&#20840;&#35268;&#33539;&#21644;&#19968;&#20010;&#23567;&#30340;&#21021;&#22987;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#20219;&#21153;&#65292;&#24039;&#22937;&#22320;&#23558;&#21463;&#38480;&#31574;&#30053;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15893v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has revolutionized decision-making across a wide range of domains over the past few decades. Yet, deploying RL policies in real-world scenarios presents the crucial challenge of ensuring safety. Traditional safe RL approaches have predominantly focused on incorporating predefined safety constraints into the policy learning process. However, this reliance on predefined safety constraints poses limitations in dynamic and unpredictable real-world settings where such constraints may not be available or sufficiently adaptable. Bridging this gap, we propose a novel approach that concurrently learns a safe RL control policy and identifies the unknown safety constraint parameters of a given environment. Initializing with a parametric signal temporal logic (pSTL) safety specification and a small initial labeled dataset, we frame the problem as a bilevel optimization task, intricately integrating constrained policy op
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;Bayesian&#32479;&#35745;&#23884;&#20837;&#21040;&#26356;&#24191;&#27867;&#30340;&#20915;&#31574;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#32479;&#35745;&#28216;&#25103;&#20316;&#20026;&#32479;&#19968;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#39057;&#29575;&#27966;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#23567;&#21518;&#24724;&#20934;&#21017;&#20316;&#20026;&#20915;&#31574;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15892</link><description>&lt;p&gt;
&#32479;&#35745;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Statistical Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;Bayesian&#32479;&#35745;&#23884;&#20837;&#21040;&#26356;&#24191;&#27867;&#30340;&#20915;&#31574;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#32479;&#35745;&#28216;&#25103;&#20316;&#20026;&#32479;&#19968;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#39057;&#29575;&#27966;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#23567;&#21518;&#24724;&#20934;&#21017;&#20316;&#20026;&#20915;&#31574;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23545;&#20960;&#31181;&#20856;&#22411;&#30340;&#28216;&#25103;&#36827;&#34892;&#20102;&#25968;&#23398;&#25506;&#32034;&#65292;&#20854;&#20013;&#33258;&#28982;&#28044;&#29616;&#20102;&#32479;&#35745;&#23398;&#21644;&#27010;&#29575;&#35770;&#30340;&#26680;&#24515;&#27010;&#24565;&#12290;&#36825;&#20123;&#28216;&#25103;&#21253;&#25324;&#36153;&#33293;&#23572;&#28216;&#25103;&#21644;&#36125;&#21494;&#26031;&#28216;&#25103;&#65292;&#23427;&#20204;&#20998;&#21035;&#19982;&#39057;&#29575;&#27966;&#32479;&#35745;&#23398;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#30456;&#20851;&#12290;&#38543;&#21518;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#31867;&#22411;&#30340;&#28216;&#25103;&#65292;&#31216;&#20026;&#32479;&#35745;&#28216;&#25103;&#65292;&#22312;&#20854;&#20013;&#21487;&#20197;&#35774;&#32622;&#19968;&#20010;&#36827;&#19968;&#27493;&#30340;&#21442;&#25968;&#65292;&#21363;&#29609;&#23478;&#30340;&#30456;&#23545;&#39118;&#38505;&#21388;&#24694;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36153;&#33293;&#23572;&#28216;&#25103;&#21644;&#36125;&#21494;&#26031;&#28216;&#25103;&#21487;&#20197;&#34987;&#35270;&#20026;&#32479;&#35745;&#28216;&#25103;&#30340;&#26497;&#38480;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#32479;&#35745;&#28216;&#25103;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#39057;&#29575;&#27966;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21746;&#23398;&#26694;&#26550;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#26368;&#23567;&#21518;&#24724;&#20934;&#21017;&#65292;&#20316;&#20026;&#20915;&#31574;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15892v1 Announce Type: cross  Abstract: This work contains the mathematical exploration of a few prototypical games in which central concepts from statistics and probability theory naturally emerge. The first two kinds of games are termed Fisher and Bayesian games, which are connected to Frequentist and Bayesian statistics, respectively. Later, a more general type of game is introduced, termed Statistical game, in which a further parameter, the players' relative risk aversion, can be set. In this work, we show that Fisher and Bayesian games can be viewed as limiting cases of Statistical games. Therefore, Statistical games can be viewed as a unified framework, incorporating both Frequentist and Bayesian statistics. Furthermore, a philosophical framework is (re-)presented -- often referred to as minimax regret criterion -- as a general approach to decision making.   The main motivation for this work was to embed Bayesian statistics into a broader decision-making framework, whe
&lt;/p&gt;</description></item><item><title>FENs&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#19988;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#22788;&#29702;&#24207;&#21015;&#65292;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#35757;&#32451;&#22823;&#33268;&#32447;&#24615;&#25968;&#37327;&#30340;&#24120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.15883</link><description>&lt;p&gt;
&#34701;&#21512;&#32534;&#30721;&#22120;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fusion Encoder Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15883
&lt;/p&gt;
&lt;p&gt;
FENs&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#19988;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#22788;&#29702;&#24207;&#21015;&#65292;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#35757;&#32451;&#22823;&#33268;&#32447;&#24615;&#25968;&#37327;&#30340;&#24120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34701;&#21512;&#32534;&#30721;&#22120;&#32593;&#32476;&#65288;FENs&#65289;&#30340;&#31639;&#27861;&#31867;&#65306;&#29992;&#20110;&#21019;&#24314;&#23558;&#22266;&#23450;&#38271;&#24230;&#24207;&#21015;&#26144;&#23556;&#21040;&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#29983;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#20165;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#65288;&#20943;&#36731;&#25968;&#25454;&#22312;&#32593;&#32476;&#20013;&#20256;&#25773;&#26102;&#30340;&#36864;&#21270;&#65289;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#22788;&#29702;&#24207;&#21015;&#65288;&#25110;&#32773;&#22312;&#20855;&#26377;&#32447;&#24615;&#22788;&#29702;&#22120;&#25968;&#37327;&#30340;&#23545;&#25968;&#26102;&#38388;&#20869;&#65289;&#12290;FENs&#30340;&#20851;&#38190;&#23646;&#24615;&#26159;&#23427;&#20204;&#36890;&#36807;&#35757;&#32451;&#22823;&#33268;&#32447;&#24615;&#25968;&#37327;&#30340;&#24120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#32593;&#32476;&#20855;&#26377;&#24120;&#28145;&#24230;&#24847;&#21619;&#30528;&#21453;&#21521;&#20256;&#25773;&#25928;&#26524;&#33391;&#22909;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#30446;&#21069;FENs&#30340;&#24615;&#33021;&#20165;&#20165;&#26159;&#25512;&#27979;&#65292;&#22240;&#20026;&#25105;&#20204;&#23578;&#26410;&#23454;&#29616;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15883v1 Announce Type: new  Abstract: In this paper we present fusion encoder networks (FENs): a class of algorithms for creating neural networks that map fixed-length sequences to outputs. The resulting neural network has only logarithmic depth (alleviating the degradation of data as it propagates through the network) and can process sequences in linear time (or in logarithmic time with a linear number of processors). The crucial property of FENs is that they learn by training a quasi-linear number of constant-depth neural networks in parallel. The fact that these networks are constant depth means that backpropagation works well. We note that currently the performance of FENs is only conjectured as we are yet to implement them.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#25311;&#33647;&#20998;&#23376;&#65292;&#30456;&#27604;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#31454;&#20105;&#24615;&#65292;&#33021;&#22815;&#35299;&#20915;&#23545;&#26144;&#24322;&#26500;&#20307;&#38382;&#39064;&#65292;&#36827;&#32780;&#32771;&#34385;&#25152;&#26377;&#20998;&#23376;&#20960;&#20309;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.15864</link><description>&lt;p&gt;
&#22522;&#20110;&#22330;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Field-based Molecule Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15864
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#25311;&#33647;&#20998;&#23376;&#65292;&#30456;&#27604;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#31454;&#20105;&#24615;&#65292;&#33021;&#22815;&#35299;&#20915;&#23545;&#26144;&#24322;&#26500;&#20307;&#38382;&#39064;&#65292;&#36827;&#32780;&#32771;&#34385;&#25152;&#26377;&#20998;&#23376;&#20960;&#20309;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;FMG&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25311;&#33647;&#20998;&#23376;&#30340;&#22522;&#20110;&#22330;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#22914;&#20309;&#30456;&#27604;&#26222;&#36941;&#20351;&#29992;&#30340;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#20248;&#21183;&#65292;&#24182;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#23376;&#31283;&#23450;&#24615;&#29983;&#25104;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#20809;&#23398;&#24322;&#26500;&#20307;&#65288;&#23545;&#26144;&#24322;&#26500;&#20307;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#21069;&#34987;&#24573;&#30053;&#30340;&#23545;&#20110;&#33647;&#29289;&#23433;&#20840;&#24615;&#21644;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#20998;&#23376;&#23646;&#24615;&#65292;&#24182;&#22240;&#27492;&#32771;&#34385;&#20102;&#25152;&#26377;&#20998;&#23376;&#20960;&#20309;&#26041;&#38754;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#26159;&#23545;&#19968;&#32452;&#21464;&#25442;&#19981;&#21464;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#26144;&#24322;&#26500;&#20307;&#25104;&#23545;&#23384;&#22312;&#65292;&#23548;&#33268;&#23427;&#20204;&#23545;&#20998;&#23376;&#30340;R&#21644;S&#26500;&#22411;&#20445;&#25345;&#19981;&#21464;&#65292;&#32780;&#25105;&#20204;&#30340;&#22522;&#20110;&#22330;&#30340;&#29983;&#25104;&#27169;&#22411;&#25429;&#25417;&#20102;&#36825;&#19968;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15864v1 Announce Type: new  Abstract: This work introduces FMG, a field-based model for drug-like molecule generation. We show how the flexibility of this method provides crucial advantages over the prevalent, point-cloud based methods, and achieves competitive molecular stability generation. We tackle optical isomerism (enantiomers), a previously omitted molecular property that is crucial for drug safety and effectiveness, and thus account for all molecular geometry aspects. We demonstrate how previous methods are invariant to a group of transformations that includes enantiomer pairs, leading them invariant to the molecular R and S configurations, while our field-based generative model captures this property.
&lt;/p&gt;</description></item><item><title>&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19982;&#21028;&#21035;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#26512;&#21644;&#35299;&#20915;LLMs&#23545;&#36755;&#20837;&#25552;&#31034;&#20013;&#19981;&#21516;&#31867;&#22411;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;</title><link>https://arxiv.org/abs/2402.15833</link><description>&lt;p&gt;
&#38024;&#23545;&#40065;&#26834;&#24615;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#25200;&#21160;&#19968;&#33268;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt Perturbation Consistency Learning for Robust Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15833
&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19982;&#21028;&#21035;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#26512;&#21644;&#35299;&#20915;LLMs&#23545;&#36755;&#20837;&#25552;&#31034;&#20013;&#19981;&#21516;&#31867;&#22411;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#24635;&#32467;&#31561;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#24847;&#22270;&#20998;&#31867;&#21644;&#27133;&#22635;&#20805;&#65288;IC-SF&#65289;&#31561;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#19978;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26174;&#33879;&#33853;&#21518;&#20110;&#21028;&#21035;&#27169;&#22411;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#36275;&#22815;&#22823;&#30340;LLMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#20135;&#29983;&#19982;&#21028;&#21035;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;IC-SF&#24615;&#33021;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#31995;&#32479;&#20998;&#26512;&#20102;&#36825;&#20123;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30001;&#20110;&#19977;&#31181;&#19981;&#21516;&#32780;&#30456;&#20851;&#30340;&#36755;&#20837;&#25200;&#21160; - &#21516;&#38899;&#35789;&#12289;&#21516;&#20041;&#35789;&#21644;&#37322;&#20041; - &#23548;&#33268;&#30340;&#24615;&#33021;&#24694;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32531;&#35299;&#26041;&#27861;&#65292;&#21363;&#25552;&#31034;&#25200;&#21160;&#19968;&#33268;&#24615;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15833v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated impressive performance on a number of natural language processing tasks, such as question answering and text summarization. However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of LLMs to various perturbations in the input prompts. The contributions of this paper are three-fold. First, we show that fine-tuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models. Next, we systematically analyze the performance deterioration of those fine-tuned models due to three distinct yet relevant types of input perturbations - oronyms, synonyms, and paraphrasing. Finally, we propose an efficient mitigation approach, Prompt Perturbatio
&lt;/p&gt;</description></item><item><title>&#20195;&#29702;&#36890;&#36807;&#36777;&#35770;&#22411;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#21487;&#36777;&#26126;&#31574;&#30053;&#65292;&#20197;&#25903;&#25345;&#35777;&#25454;&#35777;&#26126;&#20915;&#31574;&#30340;&#21512;&#29702;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15826</link><description>&lt;p&gt;
&#29992;&#20110;&#21512;&#29702;&#24207;&#36143;&#20915;&#31574;&#30340;&#22870;&#21169;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Reward Design for Justifiable Sequential Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15826
&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#36890;&#36807;&#36777;&#35770;&#22411;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#21487;&#36777;&#26126;&#31574;&#30053;&#65292;&#20197;&#25903;&#25345;&#35777;&#25454;&#35777;&#26126;&#20915;&#31574;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20195;&#29702;&#36171;&#20104;&#33021;&#22815;&#20351;&#29992;&#25903;&#25345;&#35777;&#25454;&#26469;&#35777;&#26126;&#20915;&#31574;&#30340;&#33021;&#21147;&#26159;&#36127;&#36131;&#20219;&#20915;&#31574;&#30340;&#22522;&#30707;&#12290;&#27492;&#22806;&#65292;&#30830;&#20445;&#36825;&#20123;&#35777;&#26126;&#19982;&#20154;&#31867;&#26399;&#26395;&#21644;&#31038;&#20250;&#35268;&#33539;&#19968;&#33268;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#65292;&#27604;&#22914;&#21307;&#30103;&#20445;&#20581;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36777;&#35770;&#22411;&#22870;&#21169;&#27169;&#22411;&#65292;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#20854;&#20013;&#38646;&#21644;&#36777;&#35770;&#28216;&#25103;&#30340;&#32467;&#26524;&#37327;&#21270;&#20102;&#22312;&#29305;&#23450;&#29366;&#24577;&#19979;&#30340;&#20915;&#31574;&#30340;&#21512;&#29702;&#24615;&#12290;&#28982;&#21518;&#20351;&#29992;&#35813;&#22870;&#21169;&#27169;&#22411;&#26469;&#35757;&#32451;&#19968;&#20010;&#21487;&#20197;&#26356;&#23481;&#26131;&#22320;&#19982;&#25903;&#25345;&#35777;&#25454;&#30456;&#21360;&#35777;&#30340;&#21487;&#36777;&#26126;&#31574;&#30053;&#12290;&#22312;&#36777;&#35770;&#28216;&#25103;&#20013;&#65292;&#20004;&#20010;&#36777;&#25252;&#24615;&#20195;&#29702;&#36718;&#27969;&#25552;&#20379;&#25903;&#25345;&#35777;&#25454;&#65292;&#25903;&#25345;&#20004;&#20010;&#31454;&#20105;&#24615;&#20915;&#31574;&#12290;&#22312;&#25552;&#20379;&#30340;&#35777;&#25454;&#26465;&#20214;&#19979;&#65292;&#19968;&#20010;&#20154;&#31867;&#27861;&#23448;&#30340;&#20195;&#29702;&#35780;&#20272;&#21738;&#20010;&#20915;&#31574;&#26356;&#21512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#22788;&#26041;&#31574;&#30053;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15826v1 Announce Type: cross  Abstract: Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;iDMIR&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#22240;&#26524;&#26426;&#21046;&#30340;&#20559;&#35265;&#28040;&#38500;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#20811;&#26381;&#20256;&#32479;&#22522;&#20110;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#27969;&#34892;&#24230;&#21644;&#25277;&#26679;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15819</link><description>&lt;p&gt;
&#28040;&#38500;&#20559;&#35265;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Debiased Model-based Interactive Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;iDMIR&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#22240;&#26524;&#26426;&#21046;&#30340;&#20559;&#35265;&#28040;&#38500;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#20811;&#26381;&#20256;&#32479;&#22522;&#20110;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#27969;&#34892;&#24230;&#21644;&#25277;&#26679;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#26597;&#35810;&#19990;&#30028;&#27169;&#22411;&#26469;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#65292;&#20294;&#20174;&#21382;&#21490;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#24456;&#23481;&#26131;&#21463;&#21040;&#20559;&#35265;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#22914;&#27969;&#34892;&#24230;&#20559;&#35265;&#21644;&#25277;&#26679;&#20559;&#35265;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#28040;&#38500;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;1&#65289;&#24573;&#30053;&#26102;&#38388;&#21464;&#21270;&#27969;&#34892;&#24230;&#30340;&#21160;&#24577;&#20250;&#23548;&#33268;&#23545;&#39033;&#30446;&#30340;&#38169;&#35823;&#37325;&#26032;&#21152;&#26435;&#12290; 2&#65289;&#23558;&#26410;&#30693;&#26679;&#26412;&#35270;&#20026;&#36127;&#26679;&#26412;&#22312;&#36127;&#37319;&#26679;&#20013;&#20250;&#23548;&#33268;&#25277;&#26679;&#20559;&#35265;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20004;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#35782;&#21035;&#21435;&#20559;&#35265;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#27169;&#22411;&#65288;&#31616;&#31216;iDMIR&#65289;&#12290;&#22312;iDMIR&#20013;&#65292;&#38024;&#23545;&#31532;&#19968;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22240;&#26524;&#26426;&#21046;&#30340;&#20559;&#35265;&#28040;&#38500;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#65292;&#20445;&#35777;&#20102;&#37492;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15819v1 Announce Type: cross  Abstract: Existing model-based interactive recommendation systems are trained by querying a world model to capture the user preference, but learning the world model from historical logged data will easily suffer from bias issues such as popularity bias and sampling bias. This is why some debiased methods have been proposed recently. However, two essential drawbacks still remain: 1) ignoring the dynamics of the time-varying popularity results in a false reweighting of items. 2) taking the unknown samples as negative samples in negative sampling results in the sampling bias. To overcome these two drawbacks, we develop a model called \textbf{i}dentifiable \textbf{D}ebiased \textbf{M}odel-based \textbf{I}nteractive \textbf{R}ecommendation (\textbf{iDMIR} in short). In iDMIR, for the first drawback, we devise a debiased causal world model based on the causal mechanism of the time-varying recommendation generation process with identification guarantee
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558; U-net &#21644; GAN &#32467;&#21512;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20855;&#26377;&#22810;&#23610;&#24230;&#29305;&#24615;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21019;&#26032;&#24615;&#22320;&#26500;&#24314;&#20102;&#22810;&#23610;&#24230;&#36890;&#36947;&#32858;&#21512;&#27169;&#22359;&#12289;&#22810;&#23610;&#24230;&#20998;&#23618;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#21367;&#31215;&#22359;&#27880;&#24847;&#26426;&#21046;</title><link>https://arxiv.org/abs/2402.15815</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26448;&#26009;&#24494;&#35266;&#32467;&#26500;3D&#37325;&#24314;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Generative Machine Learning Model for Material Microstructure 3D Reconstruction and Performance Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15815
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558; U-net &#21644; GAN &#32467;&#21512;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20855;&#26377;&#22810;&#23610;&#24230;&#29305;&#24615;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21019;&#26032;&#24615;&#22320;&#26500;&#24314;&#20102;&#22810;&#23610;&#24230;&#36890;&#36947;&#32858;&#21512;&#27169;&#22359;&#12289;&#22810;&#23610;&#24230;&#20998;&#23618;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#21367;&#31215;&#22359;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;2D&#20999;&#29255;&#37325;&#24314;3D&#24494;&#35266;&#32467;&#26500;&#34987;&#35748;&#20026;&#22312;&#39044;&#27979;&#26448;&#26009;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#29289;&#29702;&#24615;&#36136;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#20174;2D&#21040;3D&#30340;&#32500;&#24230;&#25193;&#23637;&#20174;&#24403;&#21069;&#25216;&#26415;&#35282;&#24230;&#30475;&#34987;&#35270;&#20026;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#36870;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#35768;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#27169;&#22411;&#36807;&#20110;&#31616;&#21270;&#12289;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#26679;&#26412;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38590;&#20197;&#23454;&#29616;&#27169;&#22411;&#25910;&#25947;&#12290;&#37492;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558; U-net &#30340;&#22810;&#23610;&#24230;&#29305;&#24615;&#21644; GAN &#30340;&#29983;&#25104;&#33021;&#21147;&#32467;&#21512;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;&#22522;&#20110;&#27492;&#65292;&#21019;&#26032;&#24615;&#22320;&#26500;&#24314;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36890;&#36947;&#32858;&#21512;&#27169;&#22359;&#12289;&#22810;&#23610;&#24230;&#20998;&#23618;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#21367;&#31215;&#22359;&#27880;&#24847;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15815v1 Announce Type: new  Abstract: The reconstruction of 3D microstructures from 2D slices is considered to hold significant value in predicting the spatial structure and physical properties of materials.The dimensional extension from 2D to 3D is viewed as a highly challenging inverse problem from the current technological perspective.Recently,methods based on generative adversarial networks have garnered widespread attention.However,they are still hampered by numerous limitations,including oversimplified models,a requirement for a substantial number of training samples,and difficulties in achieving model convergence during training.In light of this,a novel generative model that integrates the multiscale properties of U-net with and the generative capabilities of GAN has been proposed.Based on this,the innovative construction of a multi-scale channel aggregation module,a multi-scale hierarchical feature aggregation module and a convolutional block attention mechanism can 
&lt;/p&gt;</description></item><item><title>RNN&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#34920;&#31034;&#26356;&#22823;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#26377;&#30028;&#22534;&#26632;&#21644;&#24191;&#20041;&#22534;&#26632;&#26356;&#26032;&#20989;&#25968;&#65292;&#31867;&#20284;&#20110;&#20445;&#30041;&#22266;&#23450;&#25968;&#37327;&#31526;&#21495;&#35760;&#24518;&#24182;&#20351;&#29992;&#31616;&#21333;&#26356;&#26032;&#26426;&#21046;&#30340;&#33258;&#21160;&#26426;&#12290;</title><link>https://arxiv.org/abs/2402.15814</link><description>&lt;p&gt;
RNN&#35821;&#35328;&#27169;&#22411;&#24402;&#32435;&#20559;&#24046;&#30340;&#19968;&#20010;&#29702;&#35770;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Result on the Inductive Bias of RNN Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15814
&lt;/p&gt;
&lt;p&gt;
RNN&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#34920;&#31034;&#26356;&#22823;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#26377;&#30028;&#22534;&#26632;&#21644;&#24191;&#20041;&#22534;&#26632;&#26356;&#26032;&#20989;&#25968;&#65292;&#31867;&#20284;&#20110;&#20445;&#30041;&#22266;&#23450;&#25968;&#37327;&#31526;&#21495;&#35760;&#24518;&#24182;&#20351;&#29992;&#31616;&#21333;&#26356;&#26032;&#26426;&#21046;&#30340;&#33258;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;Hewitt&#31561;&#20154;&#65288;2020&#65289;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#20316;&#20026;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#32463;&#39564;&#25104;&#21151;&#21487;&#33021;&#24615;&#30340;&#19968;&#20010;&#35299;&#37322;&#12290; &#23427;&#26174;&#31034;RNNs&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#22312;&#20154;&#31867;&#35821;&#35328;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#26377;&#30028;&#20998;&#23618;&#32467;&#26500;&#12290; &#36825;&#34920;&#26126;RNNs&#30340;&#25104;&#21151;&#21487;&#33021;&#19982;&#23427;&#20204;&#24314;&#27169;&#23618;&#27425;&#32467;&#26500;&#30340;&#33021;&#21147;&#26377;&#20851;&#12290; &#28982;&#32780;&#65292;&#23545;Hewitt&#31561;&#20154;&#65288;2020&#65289;&#26500;&#36896;&#30340;&#26356;&#35814;&#32454;&#26816;&#26597;&#34920;&#26126;&#65292;&#23427;&#19981;&#38480;&#20110;&#20998;&#23618;LMs&#65292;&#36825;&#24341;&#20986;&#20102;RNNs&#21487;&#20197;&#26377;&#25928;&#34920;&#31034;&#21738;&#20123;\emph{&#20854;&#20182;&#31867;&#22411;} LMs&#30340;&#38382;&#39064;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#27010;&#25324;&#20182;&#20204;&#30340;&#26500;&#36896;&#20197;&#23637;&#31034;RNNs&#21487;&#20197;&#26377;&#25928;&#34920;&#31034;&#26356;&#22823;&#31867;&#21035;&#30340;LMs&#65306;&#21487;&#20197;&#36890;&#36807;&#24102;&#26377;&#26377;&#30028;&#22534;&#26632;&#21644;&#24191;&#20041;&#22534;&#26632;&#26356;&#26032;&#20989;&#25968;&#30340;&#19979;&#25512;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#37027;&#20123;&#12290; &#36825;&#31867;&#20284;&#20110;&#19968;&#20010;&#20445;&#30041;&#22266;&#23450;&#25968;&#37327;&#31526;&#21495;&#35760;&#24518;&#24182;&#20351;&#29992;&#31616;&#21333;&#26356;&#26032;&#26426;&#21046;&#26356;&#26032;&#35760;&#24518;&#30340;&#33258;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15814v1 Announce Type: new  Abstract: Recent work by Hewitt et al. (2020) provides a possible interpretation of the empirical success of recurrent neural networks (RNNs) as language models (LMs).   It shows that RNNs can efficiently represent bounded hierarchical structures that are prevalent in human language.   This suggests that RNNs' success might be linked to their ability to model hierarchy.   However, a closer inspection of Hewitt et al.'s (2020) construction shows that it is not limited to hierarchical LMs, posing the question of what \emph{other classes} of LMs can be efficiently represented by RNNs.   To this end, we generalize their construction to show that RNNs can efficiently represent a larger class of LMs: Those that can be represented by a pushdown automaton with a bounded stack and a generalized stack update function.   This is analogous to an automaton that keeps a memory of a fixed number of symbols and updates the memory with a simple update mechanism.  
&lt;/p&gt;</description></item><item><title>OAG-Bench&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#23398;&#26415;&#22270;&#30340;&#20840;&#38754;&#12289;&#22810;&#26041;&#38754;&#21644;&#31934;&#32454;&#21270;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;</title><link>https://arxiv.org/abs/2402.15810</link><description>&lt;p&gt;
OAG-Bench&#65306;&#38754;&#21521;&#23398;&#26415;&#22270;&#25366;&#25496;&#30340;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15810
&lt;/p&gt;
&lt;p&gt;
OAG-Bench&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#23398;&#26415;&#22270;&#30340;&#20840;&#38754;&#12289;&#22810;&#26041;&#38754;&#21644;&#31934;&#32454;&#21270;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31185;&#23398;&#25991;&#29486;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#22810;&#21151;&#33021;&#30340;&#23398;&#26415;&#30693;&#35782;&#26381;&#21153;&#36234;&#26469;&#36234;&#20381;&#36182;&#20840;&#38754;&#30340;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;&#23613;&#31649;&#20844;&#24320;&#23398;&#26415;&#22270;&#12289;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#24050;&#32463;&#26377;&#20102;&#65292;&#20294;&#36825;&#20123;&#36164;&#28304;&#36890;&#24120;&#22312;&#22810;&#26041;&#38754;&#21644;&#32454;&#31890;&#24230;&#27880;&#37322;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#21463;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#31867;&#22411;&#21644;&#39046;&#22495;&#65292;&#25110;&#32773;&#32570;&#20047;&#30495;&#23454;&#23398;&#26415;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24320;&#25918;&#23398;&#26415;&#22270;&#65288;OAG&#65289;&#30340;&#20840;&#38754;&#12289;&#22810;&#26041;&#38754;&#21644;&#31934;&#32454;&#21270;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;OAG-Bench&#12290;OAG-Bench&#28085;&#30422;&#20102;10&#20010;&#20219;&#21153;&#65292;20&#20010;&#25968;&#25454;&#38598;&#65292;70+&#20010;&#22522;&#20934;&#21644;120+&#20010;&#25130;&#33267;&#30446;&#21069;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#38024;&#23545;&#26576;&#20123;&#20219;&#21153;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#27880;&#37322;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#19968;&#22871;&#25968;&#25454;&#39044;&#22788;&#29702;&#20195;&#30721;&#12289;&#31639;&#27861;&#23454;&#29616;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#20419;&#36827;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36825;&#26679;&#30340;&#20808;&#36827;&#31639;&#27861;&#20063;&#20250;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#21463;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15810v1 Announce Type: cross  Abstract: With the rapid proliferation of scientific literature, versatile academic knowledge services increasingly rely on comprehensive academic graph mining. Despite the availability of public academic graphs, benchmarks, and datasets, these resources often fall short in multi-aspect and fine-grained annotations, are constrained to specific task types and domains, or lack underlying real academic graphs. In this paper, we present OAG-Bench, a comprehensive, multi-aspect, and fine-grained human-curated benchmark based on the Open Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines, and 120+ experimental results to date. We propose new data annotation strategies for certain tasks and offer a suite of data pre-processing codes, algorithm implementations, and standardized evaluation protocols to facilitate academic graph mining. Extensive experiments reveal that even advanced algorithms like large language models (LLMs) en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20449;&#24687;&#35770;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#22320;&#27719;&#24635;&#29616;&#26377;&#25506;&#27979;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#28040;&#38500;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.15808</link><description>&lt;p&gt;
&#22810;&#33218;&#25915;&#20987;&#30340;&#26368;&#20339;&#38646;&#23556;&#20987;&#25506;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Optimal Zero-Shot Detector for Multi-Armed Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20449;&#24687;&#35770;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#22320;&#27719;&#24635;&#29616;&#26377;&#25506;&#27979;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#28040;&#38500;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24694;&#24847;&#21442;&#19982;&#32773;&#37319;&#29992;&#22810;&#33218;&#25915;&#20987;&#31574;&#30053;&#25805;&#32437;&#25968;&#25454;&#26679;&#26412;&#30340;&#24773;&#20917;&#65292;&#20026;&#20854;&#25552;&#20379;&#20102;&#21508;&#31181;&#26041;&#24335;&#21521;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;&#22122;&#38899;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#26816;&#27979;&#20219;&#20309;&#23545;&#36755;&#20837;&#30340;&#26356;&#25913;&#26469;&#20445;&#25252;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#38450;&#24481;&#31574;&#30053;&#20013;&#26497;&#24230;&#35880;&#24910;&#65292;&#25805;&#20316;&#22312;&#38450;&#23432;&#32773;&#25317;&#26377;&#20449;&#24687;&#26126;&#26174;&#23569;&#20110;&#25915;&#20987;&#32773;&#30340;&#29615;&#22659;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38450;&#23432;&#32773;&#26080;&#27861;&#21033;&#29992;&#20219;&#20309;&#25968;&#25454;&#26679;&#26412;&#26469;&#35757;&#32451;&#38450;&#24481;&#27169;&#22411;&#25110;&#39564;&#35777;&#20449;&#36947;&#30340;&#23436;&#25972;&#24615;&#12290;&#30456;&#21453;&#65292;&#38450;&#23432;&#32773;&#23436;&#20840;&#20381;&#36182;&#19968;&#32452;&#29616;&#25104;&#30340;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#25506;&#27979;&#22120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20449;&#24687;&#35770;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#22320;&#27719;&#24635;&#36825;&#20123;&#25506;&#27979;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#20351;&#29992;&#26696;&#20363;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15808v1 Announce Type: cross  Abstract: This paper explores a scenario in which a malicious actor employs a multi-armed attack strategy to manipulate data samples, offering them various avenues to introduce noise into the dataset. Our central objective is to protect the data by detecting any alterations to the input. We approach this defensive strategy with utmost caution, operating in an environment where the defender possesses significantly less information compared to the attacker. Specifically, the defender is unable to utilize any data samples for training a defense model or verifying the integrity of the channel. Instead, the defender relies exclusively on a set of pre-existing detectors readily available ``off the shelf''. To tackle this challenge, we derive an innovative information-theoretic defense approach that optimally aggregates the decisions made by these detectors, eliminating the need for any training data. We further explore a practical use-case scenario fo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12289;&#31163;&#31574;&#30053;&#23398;&#20064;&#21644;&#33258;&#20030;&#30340;&#8220;&#33268;&#21629;&#19977;&#36830;&#8221;&#22330;&#26223;&#20013;&#30340;&#22810;&#27493;TD&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#37319;&#26679;&#26102;&#38388;&#36328;&#24230; n &#36275;&#22815;&#22823;&#26102;&#36825;&#20123;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#26377;&#24847;&#20041;&#30340;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.15781</link><description>&lt;p&gt;
&#23545;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#31163;&#31574;&#30053;&#22810;&#27493;TD&#23398;&#20064;&#31639;&#27861;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15781
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12289;&#31163;&#31574;&#30053;&#23398;&#20064;&#21644;&#33258;&#20030;&#30340;&#8220;&#33268;&#21629;&#19977;&#36830;&#8221;&#22330;&#26223;&#20013;&#30340;&#22810;&#27493;TD&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#37319;&#26679;&#26102;&#38388;&#36328;&#24230; n &#36275;&#22815;&#22823;&#26102;&#36825;&#20123;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#26377;&#24847;&#20041;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12289;&#31163;&#31574;&#30053;&#23398;&#20064;&#21644;&#33258;&#20030;&#30340;&#8220;&#33268;&#21629;&#19977;&#36830;&#8221;&#22330;&#26223;&#20013;&#30340;&#22810;&#27493;TD&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#37319;&#26679;&#26102;&#38388;&#36328;&#24230; n &#36275;&#22815;&#22823;&#26102;&#65292;n&#27493;TD&#23398;&#20064;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#35299;&#12290;&#35813;&#35770;&#25991;&#20998;&#20026;&#20004;&#37096;&#20998;&#12290;&#31532;&#19968;&#37096;&#20998;&#20840;&#38754;&#30740;&#31350;&#20102;&#27169;&#22411;&#22522;&#30784;&#30830;&#23450;&#24615;&#31639;&#27861;&#30340;&#22522;&#26412;&#24615;&#36136;&#65292;&#21253;&#25324;&#25237;&#24433;&#20540;&#36845;&#20195;&#12289;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21644;&#25511;&#21046;&#29702;&#35770;&#26041;&#27861;&#65292;&#23427;&#20204;&#21487;&#20197;&#34987;&#35270;&#20026;&#21407;&#22411;&#30830;&#23450;&#24615;&#31639;&#27861;&#65292;&#23545;&#20110;&#29702;&#35299;&#21644;&#21457;&#23637;&#26080;&#27169;&#22411;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403; n &#36275;&#22815;&#22823;&#26102;&#65292;&#36825;&#20123;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#26377;&#24847;&#20041;&#30340;&#35299;&#12290;&#26681;&#25454;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;n&#27493;TD&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15781v1 Announce Type: cross  Abstract: This paper analyzes multi-step TD-learning algorithms within the `deadly triad' scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that n-step TD-learning algorithms converge to a solution as the sampling horizon n increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, and the control theoretic approach, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when n is sufficiently large. Based on these findings, two n-step TD-learning algorithms are proposed and analyzed, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#32943;&#23450;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#24120;&#35265;&#30340;&#23433;&#20840;&#32422;&#26463;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15776</link><description>&lt;p&gt;
&#21463;&#38480;&#21046;MDP&#20013;&#30340;&#30495;&#27491;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Truly No-Regret Learning in Constrained MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#32943;&#23450;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#24120;&#35265;&#30340;&#23433;&#20840;&#32422;&#26463;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24314;&#27169;&#23433;&#20840;&#32422;&#26463;&#30340;&#24120;&#35265;&#26041;&#24335;&#12290;&#30446;&#21069;&#29992;&#20110;&#39640;&#25928;&#35299;&#20915;CMDPs&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#12290;&#23545;&#20110;&#36825;&#20123;&#31639;&#27861;&#65292;&#25152;&#26377;&#24403;&#21069;&#24050;&#30693;&#30340;&#21518;&#24724;&#30028;&#37117;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#8212;&#8212;&#21487;&#20197;&#36890;&#36807;&#22312;&#19968;&#20010;&#22238;&#21512;&#20013;&#30340;&#32422;&#26463;&#36829;&#21453;&#26469;&#29992;&#20005;&#26684;&#30340;&#32422;&#26463;&#28385;&#36275;&#22312;&#21478;&#19968;&#20010;&#22238;&#21512;&#20013;&#12290;&#36825;&#20351;&#24471;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#19981;&#23433;&#20840;&#65292;&#22240;&#20026;&#23427;&#20165;&#20445;&#35777;&#26368;&#32456;&#65288;&#28151;&#21512;&#65289;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#20445;&#35777;&#23433;&#20840;&#12290;&#27491;&#22914;Efroni&#31561;&#20154;&#65288;2020&#24180;&#65289;&#25351;&#20986;&#30340;&#65292;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#21487;&#35777;&#26126;&#22320;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31532;&#19968;&#20010;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20851;&#20110;&#27491;&#21017;&#21270;&#21407;&#22987;-&#23545;&#20598;&#26041;&#26696;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#24615;&#36890;&#29992;&#21270;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#19978;&#12290;&#22522;&#20110;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21407;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15776v1 Announce Type: new  Abstract: Constrained Markov decision processes (CMDPs) are a common way to model safety constraints in reinforcement learning. State-of-the-art methods for efficiently solving CMDPs are based on primal-dual algorithms. For these algorithms, all currently known regret bounds allow for error cancellations -- one can compensate for a constraint violation in one round with a strict constraint satisfaction in another. This makes the online learning process unsafe since it only guarantees safety for the final (mixture) policy but not during learning. As Efroni et al. (2020) pointed out, it is an open question whether primal-dual algorithms can provably achieve sublinear regret if we do not allow error cancellations. In this paper, we give the first affirmative answer. We first generalize a result on last-iterate convergence of regularized primal-dual schemes to CMDPs with multiple constraints. Building upon this insight, we propose a model-based primal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#20027;&#21160;&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#26377;&#25928;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#26597;&#35810;&#29983;&#25104;&#26102;&#38388;&#30701;&#24182;&#21487;&#24182;&#34892;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15757</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Batch Active Learning of Reward Functions from Human Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#20027;&#21160;&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#26377;&#25928;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#26597;&#35810;&#29983;&#25104;&#26102;&#38388;&#30701;&#24182;&#21487;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#29983;&#25104;&#21644;&#26631;&#35760;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24448;&#24448;&#25104;&#26412;&#39640;&#26114;&#12290;&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26159;&#19968;&#20010;&#27010;&#24565;&#65292;&#36890;&#36807;&#21521;&#29992;&#25143;&#25552;&#20986;&#20559;&#22909;&#38382;&#39064;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#26631;&#35760;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#26032;&#31639;&#27861;&#65292;&#25209;&#37327;&#20027;&#21160;&#22522;&#20110;&#20559;&#22909;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#25968;&#25454;&#26679;&#26412;&#26377;&#25928;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#30701;&#30340;&#26597;&#35810;&#29983;&#25104;&#26102;&#38388;&#65292;&#24182;&#20445;&#25345;&#21487;&#24182;&#34892;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25209;&#37327;&#29983;&#25104;&#21644;&#20960;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#27169;&#25311;&#20013;&#20171;&#32461;&#20102;&#19968;&#20123;&#26426;&#22120;&#20154;&#23398;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#20165;&#38656;&#35201;&#23569;&#37327;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15757v1 Announce Type: cross  Abstract: Data generation and labeling are often expensive in robot learning. Preference-based learning is a concept that enables reliable labeling by querying users with preference questions. Active querying methods are commonly employed in preference-based learning to generate more informative data at the expense of parallelization and computation time. In this paper, we develop a set of novel algorithms, batch active preference-based learning methods, that enable efficient learning of reward functions using as few data samples as possible while still having short query generation times and also retaining parallelizability. We introduce a method based on determinantal point processes (DPP) for active batch generation and several heuristic-based alternatives. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that ar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;MeZO&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#30446;&#26631;</title><link>https://arxiv.org/abs/2402.15751</link><description>&lt;p&gt;
&#31232;&#30095;MeZO&#65306;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;MeZO&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#36890;&#24120;&#20250;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#20013;&#30340;&#21453;&#21521;&#20256;&#25773;&#32780;&#23548;&#33268;&#20869;&#23384;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#39640;&#25928;&#21033;&#29992;&#23384;&#20648;&#22120;&#30340;&#38646;&#38454;&#65288;MeZO&#65289;&#20248;&#21270;&#22120;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21482;&#38656;&#35201;&#21069;&#21521;&#20256;&#36882;&#65292;&#20351;&#20854;&#26356;&#31526;&#21512;&#20869;&#23384;&#21451;&#22909;&#24615;&#12290;&#28982;&#32780;&#65292;&#38646;&#38454;&#20248;&#21270;&#20013;&#26799;&#24230;&#20272;&#35745;&#30340;&#36136;&#37327;&#24448;&#24448;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#32500;&#25968;&#65292;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19982;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26631;&#20934;&#24494;&#35843;&#30456;&#27604;&#65292;MeZO&#20173;&#28982;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#21463;&#21040;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;MeZO&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#20165;&#23558;ZO&#24212;&#29992;&#20110;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#36873;&#25321;&#26041;&#26696;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15751v1 Announce Type: cross  Abstract: While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, the quality of gradient estimates in zeroth order optimization often depends on the data dimensionality, potentially explaining why MeZO still exhibits significant performance drops compared to standard fine-tuning across various tasks. Inspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Spar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20302;&#31209;&#29615;&#22659;&#20013;&#20855;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#36172;&#24466;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#31574;&#30053;&#35780;&#20272;&#12289;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#24182;&#19988;&#22312;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#31574;&#30053;&#35780;&#20272;&#26041;&#38754;&#30340;&#31639;&#27861;&#20960;&#20046;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.15739</link><description>&lt;p&gt;
&#20302;&#31209;&#36172;&#24466;&#36890;&#36807;&#32039;&#32477;&#23545;&#21040;&#26080;&#31351;&#22855;&#24322;&#23376;&#31354;&#38388;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15739
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20302;&#31209;&#29615;&#22659;&#20013;&#20855;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#36172;&#24466;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#31574;&#30053;&#35780;&#20272;&#12289;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#24182;&#19988;&#22312;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#31574;&#30053;&#35780;&#20272;&#26041;&#38754;&#30340;&#31639;&#27861;&#20960;&#20046;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#30340;&#24773;&#22659;&#36172;&#24466;&#38382;&#39064;&#65292;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#22914;&#26524;&#36873;&#25321;&#20102;(&#24773;&#22659;&#65292;&#21160;&#20316;)&#23545;$(i,j)\in [m]\times [n]$&#65292;&#23398;&#20064;&#32773;&#20250;&#35266;&#23519;&#19968;&#20010;&#26410;&#30693;&#20302;&#31209;&#22870;&#21169;&#30697;&#38453;&#30340;$(i,j)$-th&#20837;&#21475;&#30340;&#22024;&#26434;&#26679;&#26412;&#12290;&#36830;&#32493;&#30340;&#24773;&#22659;&#20197;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26041;&#24335;&#38543;&#26426;&#29983;&#25104;&#24182;&#36879;&#38706;&#32473;&#23398;&#20064;&#32773;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#36172;&#24466;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#29992;&#20110;&#31574;&#30053;&#35780;&#20272;&#12289;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;&#23545;&#20110;&#31574;&#30053;&#35780;&#20272;&#21644;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20960;&#20046;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;&#20363;&#22914;&#65292;&#20026;&#20102;&#20197;&#33267;&#23569;$1-\delta$&#30340;&#27010;&#29575;&#36820;&#22238;&#19968;&#20010;$\varepsilon$-&#26368;&#20339;&#31574;&#30053;&#65292;&#36890;&#24120;&#38656;&#35201;&#30340;&#26679;&#26412;&#25968;&#22823;&#33268;&#25353;&#29031;${m+n\over \varepsilon^2}\log(1/\delta)$&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#20139;&#26377;&#30340;&#26497;&#23567;&#26497;&#22823;&#20445;&#35777;&#25353;&#29031;$r^{7/4}(m+n)^{3/4}\sqrt{T}$&#32553;&#25918;&#65292;&#36825;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#25152;&#26377;&#25552;&#20986;&#30340;&#31639;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15739v1 Announce Type: new  Abstract: We study contextual bandits with low-rank structure where, in each round, if the (context, arm) pair $(i,j)\in [m]\times [n]$ is selected, the learner observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward matrix. Successive contexts are generated randomly in an i.i.d. manner and are revealed to the learner. For such bandits, we present efficient algorithms for policy evaluation, best policy identification and regret minimization. For policy evaluation and best policy identification, we show that our algorithms are nearly minimax optimal. For instance, the number of samples required to return an $\varepsilon$-optimal policy with probability at least $1-\delta$ typically scales as ${m+n\over \varepsilon^2}\log(1/\delta)$. Our regret minimization algorithm enjoys minimax guarantees scaling as $r^{7/4}(m+n)^{3/4}\sqrt{T}$, which improves over existing algorithms. All the proposed algorithms consist of two phases: they
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;PDE&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#22806;&#22495;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15734</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;PDE&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#22806;&#22495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#35265;&#35777;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19982;&#29289;&#29702;&#39046;&#22495;&#29305;&#23450;&#27934;&#23519;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#31185;&#23398;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#23494;&#38598;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;PDE&#25968;&#25454;&#12290; &#36825;&#37325;&#26032;&#24341;&#20837;&#20102;&#23545;&#26114;&#36149;&#30340;&#25968;&#20540;PDE&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#65292;&#37096;&#20998;&#21066;&#24369;&#20102;&#36991;&#20813;&#36825;&#20123;&#26114;&#36149;&#27169;&#25311;&#30340;&#21407;&#22987;&#30446;&#26631;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#23547;&#27714;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29992;&#20110;PDE&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290; &#20026;&#20102;&#20943;&#23569;&#23545;&#24102;&#26377;&#27169;&#25311;&#35299;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#37325;&#26500;&#30340;&#20195;&#29702;&#20219;&#21153;&#22312;&#26410;&#26631;&#35760;&#30340;PDE&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#36816;&#31639;&#31526;&#12290; &#20026;&#20102;&#25552;&#39640;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24110;&#21161;&#31070;&#32463;&#36816;&#31639;&#31526;&#28789;&#27963;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#25110;&#35774;&#35745;&#12290; &#22312;&#21508;&#31181;PD&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15734v1 Announce Type: new  Abstract: Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insight for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining and in-context learning methods for PDE operator learning. To reduce the need for training data with simulated solutions, we pretrain neural operators on unlabeled PDE data using reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging in-context learning methods, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PD
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;ArEEG_Chars&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.15733</link><description>&lt;p&gt;
ArEEG_Chars: &#29992;&#20110;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#35774;&#24819;&#35821;&#38899;&#35782;&#21035;&#30340;&#38463;&#25289;&#20271;&#23383;&#31526;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15733
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;ArEEG_Chars&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#26159;&#36817;&#24180;&#26469;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21487;&#20197;&#24110;&#21161;&#30251;&#30186;&#24739;&#32773;&#25913;&#21892;&#29983;&#27963;&#12290;&#26377;&#20960;&#39033;&#30740;&#31350;&#33258;&#21160;&#23558;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#20998;&#31867;&#20026;&#33521;&#25991;&#23383;&#31526;&#21644;&#21333;&#35789;&#12290;&#38463;&#25289;&#20271;&#35821;&#26159;&#19990;&#30028;&#19978;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#35821;&#35328;&#20043;&#19968;&#12290;&#28982;&#32780;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#38024;&#23545;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;&#65292;&#24182;&#21629;&#21517;&#20026;ArEEG_Chars&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;ArEEG_Chars&#36827;&#34892;&#20102;&#22810;&#39033;&#23454;&#39564;&#12290;&#22312;&#20351;&#29992;LSTM&#26102;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#12290;ArEEG_Chars&#25968;&#25454;&#38598;&#23558;&#23545;&#30740;&#31350;&#20154;&#21592;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15733v1 Announce Type: cross  Abstract: Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;Dynamic Dataset Generator&#65288;DDG&#65289;&#26469;&#35299;&#20915;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#32858;&#31867;&#26102;&#32570;&#20047;&#22810;&#26679;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#29616;&#23454;&#24615;&#21160;&#24577;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24110;&#21161;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15731</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#32858;&#31867;&#65306;&#20855;&#26377;&#24322;&#36136;&#24615;&#21464;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Clustering in Dynamic Environments: A Framework for Benchmark Dataset Generation With Heterogeneous Changes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;Dynamic Dataset Generator&#65288;DDG&#65289;&#26469;&#35299;&#20915;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#32858;&#31867;&#26102;&#32570;&#20047;&#22810;&#26679;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#29616;&#23454;&#24615;&#21160;&#24577;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24110;&#21161;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#32858;&#31867;&#26159;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#20174;&#23454;&#26102;&#25968;&#25454;&#20998;&#26512;&#21644;&#22312;&#32447;&#26080;&#30417;&#30563;&#23398;&#20064;&#21040;&#21160;&#24577;&#35774;&#26045;&#23450;&#20301;&#38382;&#39064;&#12290;&#23613;&#31649;&#20803;&#21551;&#21457;&#24335;&#22312;&#38745;&#24577;&#32858;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36319;&#36394;&#26368;&#20339;&#32858;&#31867;&#35299;&#20915;&#26041;&#26696;&#25110;&#22312;&#26102;&#38388;&#19978;&#31283;&#20581;&#22320;&#36827;&#34892;&#32858;&#31867;&#30340;&#24212;&#29992;&#20173;&#28982;&#24456;&#23569;&#25506;&#35752;&#12290;&#36825;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#32570;&#20047;&#20855;&#26377;&#22810;&#26679;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#29616;&#23454;&#24615;&#21160;&#24577;&#29305;&#24449;&#30340;&#21160;&#24577;&#25968;&#25454;&#38598;&#65292;&#38459;&#30861;&#20102;&#22312;&#21508;&#31181;&#21160;&#24577;&#22330;&#26223;&#20013;&#23545;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#31995;&#32479;&#24615;&#24615;&#33021;&#35780;&#20272;&#12290;&#36825;&#31181;&#32570;&#38519;&#23548;&#33268;&#25105;&#20204;&#23545;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#35774;&#35745;&#32858;&#31867;&#31639;&#27861;&#30340;&#29702;&#35299;&#21644;&#33021;&#21147;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#21160;&#24577;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65288;DDG&#65289;&#12290;DDG&#20855;&#26377;&#22810;&#20010;&#21160;&#24577;&#39640;&#26031;&#32452;&#20214;&#65292;&#38598;&#25104;&#20102;&#19968;&#31995;&#21015;&#24322;&#36136;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15731v1 Announce Type: new  Abstract: Clustering in dynamic environments is of increasing importance, with broad applications ranging from real-time data analysis and online unsupervised learning to dynamic facility location problems. While meta-heuristics have shown promising effectiveness in static clustering tasks, their application for tracking optimal clustering solutions or robust clustering over time in dynamic environments remains largely underexplored. This is partly due to a lack of dynamic datasets with diverse, controllable, and realistic dynamic characteristics, hindering systematic performance evaluations of clustering algorithms in various dynamic scenarios. This deficiency leads to a gap in our understanding and capability to effectively design algorithms for clustering in dynamic environments. To bridge this gap, this paper introduces the Dynamic Dataset Generator (DDG). DDG features multiple dynamic Gaussian components integrated with a range of heterogeneo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22260;&#32469;&#22312;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#23545;&#32570;&#22833;&#24615;&#36827;&#34892;&#20010;&#24615;&#21270;&#34920;&#31034;&#23637;&#24320;&#65292;&#20026;&#30495;&#27491;&#20010;&#24615;&#21270;&#30340;&#39044;&#27979;&#24314;&#27169;&#25552;&#20379;&#20102;&#26032;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.15730</link><description>&lt;p&gt;
&#20102;&#35299;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#24615;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Understanding Missingness in Time-series Electronic Health Records for Individualized Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15730
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22260;&#32469;&#22312;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#23545;&#32570;&#22833;&#24615;&#36827;&#34892;&#20010;&#24615;&#21270;&#34920;&#31034;&#23637;&#24320;&#65292;&#20026;&#30495;&#27491;&#20010;&#24615;&#21270;&#30340;&#39044;&#27979;&#24314;&#27169;&#25552;&#20379;&#20102;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24314;&#31435;&#20010;&#24615;&#21270;&#21307;&#23398;&#24212;&#29992;&#30340;&#20852;&#36259;&#36880;&#28176;&#22686;&#21152;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#20851;&#20110;&#20010;&#24615;&#21270;&#21307;&#23398;&#30340;&#30740;&#31350;&#25552;&#20986;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#20174;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#20013;&#34920;&#31034;&#32570;&#22833;&#24615;&#24182;&#23398;&#20064;&#32570;&#22833;&#24615;&#27169;&#24335;&#12290;&#23545;&#32570;&#22833;&#24615;&#34920;&#31034;&#32570;&#20047;&#20010;&#24615;&#21270;&#20851;&#27880;&#65292;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22312;&#30495;&#27491;&#20010;&#24615;&#21270;&#26041;&#21521;&#19978;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#31616;&#35201;&#20132;&#27969;&#31361;&#20986;&#20102;&#32570;&#22833;&#24615;&#27169;&#24335;&#30340;&#26032;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#30495;&#23454;&#26696;&#20363;&#21644;EHR&#20013;&#32570;&#22833;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#24037;&#20316;&#30340;&#35265;&#35299;&#26088;&#22312;&#24357;&#21512;&#29702;&#35770;&#20551;&#35774;&#19982;&#23454;&#38469;&#35266;&#23519;&#20043;&#38388;&#22312;&#23454;&#38469;EHR&#20013;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#20026;&#25506;&#32034;&#26356;&#22909;&#30340;&#39044;&#27979;&#24314;&#27169;&#34920;&#31034;&#26041;&#21521;&#25171;&#24320;&#26032;&#30340;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15730v1 Announce Type: new  Abstract: With the widespread of machine learning models for healthcare applications, there is increased interest in building applications for personalized medicine. Despite the plethora of proposed research for personalized medicine, very few focus on representing missingness and learning from the missingness patterns in time-series Electronic Health Records (EHR) data. The lack of focus on missingness representation in an individualized way limits the full utilization of machine learning applications towards true personalization. In this brief communication, we highlight new insights into patterns of missingness with real-world examples and implications of missingness in EHRs. The insights in this work aim to bridge the gap between theoretical assumptions and practical observations in real-world EHRs. We hope this work will open new doors for exploring directions for better representation in predictive modelling for true personalization.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#31639;&#23376;&#26159;&#36817;&#20284;&#38750;&#32447;&#24615;&#31639;&#23376;&#26144;&#23556;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#39640;&#25928;&#26367;&#20195;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26597;&#35810;&#20219;&#21153;&#65292;&#23588;&#20854;&#22312;&#27809;&#26377;&#25968;&#23398;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.15715</link><description>&lt;p&gt;
&#25805;&#20316;&#31526;&#23398;&#20064;&#65306;&#31639;&#27861;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Operator Learning: Algorithms and Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15715
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#26159;&#36817;&#20284;&#38750;&#32447;&#24615;&#31639;&#23376;&#26144;&#23556;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#39640;&#25928;&#26367;&#20195;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26597;&#35810;&#20219;&#21153;&#65292;&#23588;&#20854;&#22312;&#27809;&#26377;&#25968;&#23398;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25805;&#20316;&#31526;&#23398;&#20064;&#25351;&#30340;&#26159;&#23558;&#26426;&#22120;&#23398;&#20064;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#36817;&#20284;&#65288;&#36890;&#24120;&#26159;&#38750;&#32447;&#24615;&#30340;&#65289;&#22312;&#33539;&#25968;&#31354;&#38388;&#20013;&#26144;&#23556;&#30340;&#31639;&#23376;&#65292;&#36825;&#20123;&#31639;&#23376;&#36890;&#24120;&#26469;&#33258;&#20110;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#34920;&#36798;&#30340;&#29289;&#29702;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#31639;&#23376;&#20316;&#20026;&#39640;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#22312;&#35768;&#22810;&#26597;&#35810;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20197;&#34917;&#20805;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#30001;&#20110;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#23427;&#20204;&#36824;&#21487;&#20197;&#22312;&#25968;&#23398;&#19978;&#26080;&#27861;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#21457;&#29616;&#12290;&#36825;&#31687;&#32508;&#36848;&#20027;&#35201;&#20851;&#27880;&#31070;&#32463;&#31639;&#23376;&#65292;&#24314;&#31435;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#23450;&#20041;&#20989;&#25968;&#36817;&#20284;&#26041;&#38754;&#30340;&#25104;&#21151;&#22522;&#30784;&#19978;&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;&#31070;&#32463;&#31639;&#23376;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#19981;&#23436;&#25972;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#24635;&#32467;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#21644;&#25105;&#20204;&#29702;&#35770;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15715v1 Announce Type: new  Abstract: Operator learning refers to the application of ideas from machine learning to approximate (typically nonlinear) operators mapping between Banach spaces of functions. Such operators often arise from physical models expressed in terms of partial differential equations (PDEs). In this context, such approximate operators hold great potential as efficient surrogate models to complement traditional numerical methods in many-query tasks. Being data-driven, they also enable model discovery when a mathematical description in terms of a PDE is not available. This review focuses primarily on neural operators, built on the success of deep neural networks in the approximation of functions defined on finite dimensional Euclidean spaces. Empirically, neural operators have shown success in a variety of applications, but our theoretical understanding remains incomplete. This review article summarizes recent progress and the current state of our theoretic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32479;&#35745;&#20998;&#26512;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;Wasserstein&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#20869;&#22312;&#20302;&#32500;&#25968;&#25454;&#30340;&#29305;&#24615;&#19982;&#23616;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.15710</link><description>&lt;p&gt;
&#23545;&#20869;&#22312;&#20302;&#32500;&#25968;&#25454;&#30340;Wasserstein&#33258;&#32534;&#30721;&#22120;&#30340;&#32479;&#35745;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32479;&#35745;&#20998;&#26512;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;Wasserstein&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#20869;&#22312;&#20302;&#32500;&#25968;&#25454;&#30340;&#29305;&#24615;&#19982;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(Variational Autoencoders, VAEs)&#22312;&#30740;&#31350;&#20154;&#21592;&#20013;&#24191;&#21463;&#27426;&#36814;&#65292;&#34987;&#35748;&#20026;&#26159;&#29702;&#35299;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#26410;&#30693;&#20998;&#24067;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36825;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#37096;&#20998;&#28304;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#37096;&#20998;&#28304;&#20110;&#20854;&#33021;&#22815;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;Wasserstein&#33258;&#32534;&#30721;&#22120;(WAEs)&#26159;VAEs&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#19981;&#20165;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#32780;&#19988;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20854;&#32479;&#35745;&#20445;&#35777;&#30340;&#20998;&#26512;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#30001;&#20110;WAEs&#25152;&#24212;&#29992;&#30340;&#25968;&#25454;&#20998;&#24067;&#65288;&#20363;&#22914;&#33258;&#28982;&#22270;&#20687;&#65289;&#36890;&#24120;&#34987;&#35748;&#20026;&#22312;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20855;&#26377;&#20302;&#32500;&#32467;&#26500;&#65292;&#32780;&#24403;&#21069;&#30340;&#29702;&#35770;&#24182;&#26410;&#20805;&#20998;&#32771;&#34385;&#36825;&#19968;&#28857;&#65292;&#23548;&#33268;&#24050;&#30693;&#30340;&#30028;&#38480;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#24357;&#21512;WAEs&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;WAEs...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15710v1 Announce Type: new  Abstract: Variational Autoencoders (VAEs) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. This popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature representations in the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to not only improve model efficiency but also interpretability. However, there has been limited focus on analyzing their statistical guarantees. The matter is further complicated by the fact that the data distributions to which WAEs are applied - such as natural images - are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. To bridge the gap between the theory and practice of WAEs, in this paper, we show that WAEs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#33021;&#25214;&#21040;&#19968;&#20010;&#19982;&#26368;&#20248;&#31574;&#30053;&#31454;&#20105;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#20026;&#22312;&#20165;&#26377;&#23569;&#37327;&#26679;&#26412;&#19979;&#36827;&#34892;&#21487;&#38752;&#20915;&#31574;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.15703</link><description>&lt;p&gt;
&#26159;&#21542;&#21487;&#20197;&#20165;&#20973;&#26377;&#38480;&#26679;&#26412;&#36827;&#34892;&#31163;&#32447;&#20915;&#31574;&#65311;&#36890;&#36807;&#20449;&#20219;&#21306;&#22495;&#22686;&#24378;&#22312;&#25968;&#25454;&#31232;&#32570;&#36172;&#21338;&#26426;&#20013;&#21487;&#38752;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#33021;&#25214;&#21040;&#19968;&#20010;&#19982;&#26368;&#20248;&#31574;&#30053;&#31454;&#20105;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#20026;&#22312;&#20165;&#26377;&#23569;&#37327;&#26679;&#26412;&#19979;&#36827;&#34892;&#21487;&#38752;&#20915;&#31574;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#21482;&#21253;&#21547;&#27599;&#20010;&#33218;&#30340;&#21333;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#33021;&#20174;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#38382;&#39064;&#20013;&#23398;&#21040;&#20160;&#20040;&#65311;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21363;&#20351;&#22312;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#30340;&#29615;&#22659;&#20013;&#65292;&#20173;&#28982;&#21487;&#33021;&#25214;&#21040;&#19968;&#20010;&#19982;&#26368;&#20248;&#31574;&#30053;&#31454;&#20105;&#30340;&#31574;&#30053;&#12290;&#36825;&#20026;&#22312;&#24517;&#39035;&#20165;&#20381;&#38752;&#23569;&#25968;&#26679;&#26412;&#20570;&#20986;&#20851;&#38190;&#20915;&#31574;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#21487;&#38752;&#30340;&#20915;&#31574;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;\emph{&#38543;&#26426;&#31574;&#30053;&#23545;&#20110;&#31163;&#32447;&#20915;&#31574;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#30830;&#23450;&#24615;&#31574;&#30053;}&#12290;&#19987;&#27880;&#20110;&#31163;&#32447;&#22810;&#33218;&#32769;&#34382;&#26426;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#20449;&#20219;&#21306;&#22495;&#30340;&#38543;&#26426;&#31574;&#30053;&#22686;&#24378;&#65288;TRUST&#65289;&#30340;&#31639;&#27861;&#65292;&#36825;&#19982;&#20027;&#23548;&#24615;&#20215;&#20540;&#20026;&#22522;&#30784;&#30340;&#36739;&#20302;&#32622;&#20449;&#19979;&#30028;&#26041;&#27861;&#26377;&#24456;&#22823;&#19981;&#21516;&#12290;&#20854;&#35774;&#35745;&#24471;&#30410;&#20110;&#23450;&#20301;&#35268;&#24459;&#12289;&#20020;&#30028;&#21322;&#24452;&#21644;&#30456;&#23545;&#24754;&#35266;&#20027;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;L&#30340;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15703v1 Announce Type: cross  Abstract: What can an agent learn in a stochastic Multi-Armed Bandit (MAB) problem from a dataset that contains just a single sample for each arm? Surprisingly, in this work, we demonstrate that even in such a data-starved setting it may still be possible to find a policy competitive with the optimal one. This paves the way to reliable decision-making in settings where critical decisions must be made by relying only on a handful of samples.   Our analysis reveals that \emph{stochastic policies can be substantially better} than deterministic ones for offline decision-making. Focusing on offline multi-armed bandits, we design an algorithm called Trust Region of Uncertainty for Stochastic policy enhancemenT (TRUST) which is quite different from the predominant value-based lower confidence bound approach. Its design is enabled by localization laws, critical radii, and relative pessimism. We prove that its sample complexity is comparable to that of L
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#32534;&#30721;&#20851;&#31995;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;ICD&#32534;&#30721;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#30456;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15700</link><description>&lt;p&gt;
&#12298;CoRelation: &#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#32534;&#30721;&#20851;&#31995;&#23398;&#20064;&#25552;&#21319;&#33258;&#21160;ICD&#32534;&#30721;&#12299;
&lt;/p&gt;
&lt;p&gt;
CoRelation: Boosting Automatic ICD Coding Through Contextualized Code Relation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15700
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#32534;&#30721;&#20851;&#31995;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;ICD&#32534;&#30721;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#30456;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD&#65289;&#32534;&#30721;&#22312;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#20197;&#20415;&#27491;&#30830;&#35760;&#24405;&#21644;&#35745;&#36153;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25552;&#21319;&#33258;&#21160;ICD&#32534;&#30721;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#21521;&#26159;&#23545;ICD&#32534;&#30721;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#23545;ICD&#32534;&#30721;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#24314;&#27169;&#19981;&#36275;&#65292;&#36890;&#24120;&#24573;&#35270;&#20102;&#20020;&#24202;&#35760;&#24405;&#20013;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#21363;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#21644;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;ICD&#32534;&#30721;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#32771;&#34385;&#20020;&#24202;&#35760;&#24405;&#19978;&#19979;&#25991;&#30340;&#20381;&#36182;&#23398;&#20064;&#33539;&#24335;&#65292;&#23545;&#24314;&#27169;&#25152;&#26377;&#21487;&#33021;&#30340;&#32534;&#30721;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#20844;&#20849;ICD&#32534;&#30721;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15700v1 Announce Type: cross  Abstract: Automatic International Classification of Diseases (ICD) coding plays a crucial role in the extraction of relevant information from clinical notes for proper recording and billing. One of the most important directions for boosting the performance of automatic ICD coding is modeling ICD code relations. However, current methods insufficiently model the intricate relationships among ICD codes and often overlook the importance of context in clinical notes. In this paper, we propose a novel approach, a contextualized and flexible framework, to enhance the learning of ICD code representations. Our approach, unlike existing methods, employs a dependent learning paradigm that considers the context of clinical notes in modeling all possible code relations. We evaluate our approach on six public ICD coding datasets and the experimental results demonstrate the effectiveness of our approach compared to state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#20132;&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#20419;&#36827;&#29983;&#25104;&#26356;&#21152;&#31616;&#21270;&#30340;&#21152;&#27861;&#35268;&#21017;&#38598;&#21512;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15691</link><description>&lt;p&gt;
&#27491;&#20132;&#26799;&#24230;&#25552;&#21319;&#29992;&#20110;&#31616;&#21270;&#30340;&#21152;&#27861;&#35268;&#21017;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15691
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#20132;&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#20419;&#36827;&#29983;&#25104;&#26356;&#21152;&#31616;&#21270;&#30340;&#21152;&#27861;&#35268;&#21017;&#38598;&#21512;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#35268;&#21017;&#30340;&#26799;&#24230;&#25552;&#21319;&#26159;&#19968;&#31181;&#23398;&#20064;&#28508;&#22312;&#21487;&#35299;&#37322;&#19988;&#20934;&#30830;&#30340;&#27010;&#29575;&#27169;&#22411;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#21487;&#35299;&#37322;&#24615;&#38656;&#35201;&#38480;&#21046;&#29983;&#25104;&#30340;&#35268;&#21017;&#25968;&#37327;&#21644;&#22823;&#23567;&#65292;&#29616;&#26377;&#30340;&#25552;&#21319;&#21464;&#20307;&#24182;&#38750;&#20026;&#27492;&#30446;&#30340;&#32780;&#35774;&#35745;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#30446;&#26631;&#20989;&#25968;&#34913;&#37327;&#20102;&#39118;&#38505;&#26799;&#24230;&#21521;&#37327;&#19982;&#26465;&#20214;&#36755;&#20986;&#21521;&#37327;&#22312;&#24050;&#36873;&#25321;&#26465;&#20214;&#30340;&#27491;&#20132;&#34917;&#19978;&#30340;&#25237;&#24433;&#30340;&#22841;&#35282;&#65292;&#20174;&#32780;&#27491;&#30830;&#36924;&#36817;&#23558;&#39118;&#38505;&#26799;&#24230;&#26412;&#36523;&#28155;&#21152;&#21040;&#27169;&#22411;&#30340;&#29702;&#24819;&#26356;&#26032;&#65292;&#24182;&#20542;&#21521;&#20110;&#21253;&#25324;&#26356;&#19968;&#33324;&#19988;&#26356;&#30701;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15691v1 Announce Type: new  Abstract: Gradient boosting of prediction rules is an efficient approach to learn potentially interpretable yet accurate probabilistic models. However, actual interpretability requires to limit the number and size of the generated rules, and existing boosting variants are not designed for this purpose. Though corrective boosting refits all rule weights in each iteration to minimise prediction risk, the included rule conditions tend to be sub-optimal, because commonly used objective functions fail to anticipate this refitting. Here, we address this issue by a new objective function that measures the angle between the risk gradient vector and the projection of the condition output vector onto the orthogonal complement of the already selected conditions. This approach correctly approximate the ideal update of adding the risk gradient itself to the model and favours the inclusion of more general and thus shorter rules. As we demonstrate using a wide r
&lt;/p&gt;</description></item><item><title>&#26080;&#38170;&#32858;&#31867;&#26041;&#27861;AFCAGF&#36890;&#36807;&#23398;&#20064;&#38170;&#22270;&#24182;&#20248;&#21270;&#25104;&#23545;&#26679;&#26412;&#36317;&#31163;&#65292;&#36991;&#20813;&#20102;&#38170;&#28857;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#30340;&#38656;&#35201;&#65292;&#25552;&#21319;&#20102;&#32858;&#31867;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15688</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#22270;&#22240;&#23376;&#20998;&#35299;&#30340;&#26080;&#38170;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Anchor-free Clustering based on Anchor Graph Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15688
&lt;/p&gt;
&lt;p&gt;
&#26080;&#38170;&#32858;&#31867;&#26041;&#27861;AFCAGF&#36890;&#36807;&#23398;&#20064;&#38170;&#22270;&#24182;&#20248;&#21270;&#25104;&#23545;&#26679;&#26412;&#36317;&#31163;&#65292;&#36991;&#20813;&#20102;&#38170;&#28857;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#30340;&#38656;&#35201;&#65292;&#25552;&#21319;&#20102;&#32858;&#31867;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38170;&#28857;&#26041;&#27861;&#26159;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#32858;&#31867;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#38454;&#27573;&#65306;&#36873;&#25321;&#38170;&#28857;&#21644;&#26500;&#24314;&#38170;&#22270;&#12290;&#36825;&#31181;&#20108;&#20998;&#20197;&#21450;&#38170;&#28857;&#30340;&#21021;&#22987;&#21270;&#26174;&#33879;&#24433;&#21709;&#31639;&#27861;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#38170;&#22270;&#22240;&#23376;&#20998;&#35299;&#30340;&#26080;&#38170;&#32858;&#31867;&#65288;AFCAGF&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;AFCAGF&#22312;&#23398;&#20064;&#38170;&#22270;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#21482;&#38656;&#35201;&#35745;&#31639;&#26679;&#26412;&#20043;&#38388;&#30340;&#25104;&#23545;&#36317;&#31163;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#26126;&#30830;&#36873;&#25321;&#38170;&#28857;&#30340;&#24517;&#35201;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#27169;&#31946;k&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#65288;FKM&#65289;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#65292;&#26080;&#38656;&#21021;&#22987;&#21270;&#32858;&#31867;&#20013;&#24515;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#35760;&#24518;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15688v1 Announce Type: new  Abstract: Anchor-based methods are a pivotal approach in handling clustering of large-scale data. However, these methods typically entail two distinct stages: selecting anchor points and constructing an anchor graph. This bifurcation, along with the initialization of anchor points, significantly influences the overall performance of the algorithm. To mitigate these issues, we introduce a novel method termed Anchor-free Clustering based on Anchor Graph Factorization (AFCAGF). AFCAGF innovates in learning the anchor graph, requiring only the computation of pairwise distances between samples. This process, achievable through straightforward optimization, circumvents the necessity for explicit selection of anchor points. More concretely, our approach enhances the Fuzzy k-means clustering algorithm (FKM), introducing a new manifold learning technique that obviates the need for initializing cluster centers. Additionally, we evolve the concept of the mem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35780;&#20272;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#19979;&#28216;&#20219;&#21153;&#36873;&#25321;&#24433;&#21709;&#30340;&#26032;&#35266;&#28857;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#22686;&#24378;&#22411;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.15680</link><description>&lt;p&gt;
&#20811;&#26381;&#22270;&#23545;&#27604;&#23398;&#20064;&#35780;&#20272;&#20013;&#30340;&#38519;&#38449;&#65306;&#36208;&#21521;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Overcoming Pitfalls in Graph Contrastive Learning Evaluation: Toward Comprehensive Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35780;&#20272;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#19979;&#28216;&#20219;&#21153;&#36873;&#25321;&#24433;&#21709;&#30340;&#26032;&#35266;&#28857;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#22686;&#24378;&#22411;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15680v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20852;&#36215;&#20351;&#24471;&#22270;&#23398;&#20064;&#31038;&#21306;&#23545;&#20854;&#20135;&#29983;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#32780;&#33258;&#30417;&#30563;&#23398;&#20064;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#31181;&#28909;&#24773;&#23548;&#33268;&#20102;&#35768;&#22810;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20854;&#30446;&#26631;&#37117;&#26159;&#21019;&#24314;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#22270;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;GCL&#26041;&#27861;&#30340;&#35780;&#20272;&#26631;&#20934;&#23384;&#22312;&#32570;&#38519;&#65292;&#22240;&#20026;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#20381;&#36182;&#21333;&#19968;&#30340;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20123;&#32570;&#38519;&#21487;&#33021;&#20351;&#35780;&#20272;&#20559;&#31163;&#39044;&#26399;&#30446;&#26631;&#65292;&#28508;&#22312;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#35770;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24443;&#24213;&#30740;&#31350;&#20102;&#36825;&#20123;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35266;&#28857;&#65292;&#21363;GCL&#26041;&#27861;&#22914;&#20309;&#21463;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#36873;&#25321;&#29992;&#20110;&#35780;&#20272;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22686;&#24378;&#22411;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15680v1 Announce Type: new  Abstract: The rise of self-supervised learning, which operates without the need for labeled data, has garnered significant interest within the graph learning community. This enthusiasm has led to the development of numerous Graph Contrastive Learning (GCL) techniques, all aiming to create a versatile graph encoder that leverages the wealth of unlabeled data for various downstream tasks. However, the current evaluation standards for GCL approaches are flawed due to the need for extensive hyper-parameter tuning during pre-training and the reliance on a single downstream task for assessment. These flaws can skew the evaluation away from the intended goals, potentially leading to misleading conclusions. In our paper, we thoroughly examine these shortcomings and offer fresh perspectives on how GCL methods are affected by hyper-parameter choices and the choice of downstream tasks for their evaluation. Additionally, we introduce an enhanced evaluation fr
&lt;/p&gt;</description></item><item><title>sDBSCAN&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#36827;&#34892;&#39640;&#32500;&#23494;&#24230;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15679</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#30340;&#21487;&#25193;&#23637;&#23494;&#24230;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Scalable Density-based Clustering with Random Projections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15679
&lt;/p&gt;
&lt;p&gt;
sDBSCAN&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#36827;&#34892;&#39640;&#32500;&#23494;&#24230;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;sDBSCAN&#30340;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20351;&#29992;&#20313;&#24358;&#36317;&#31163;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#23494;&#24230;&#32858;&#31867;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#30340;&#20445;&#37051;&#29305;&#24615;&#65292;sDBSCAN&#33021;&#22815;&#24555;&#36895;&#35782;&#21035;&#26680;&#24515;&#28857;&#21450;&#20854;&#37051;&#22495;&#65292;&#36825;&#26159;&#23494;&#24230;&#32858;&#31867;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;sDBSCAN&#22312;&#36739;&#36731;&#30340;&#26465;&#20214;&#19979;&#20197;&#39640;&#27010;&#29575;&#36755;&#20986;&#31867;&#20284;&#20110;DBSCAN&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;sDBSCAN&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;sOPTICS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#20114;&#24335;&#25506;&#32034;&#20869;&#22312;&#32858;&#31867;&#32467;&#26500;&#30340;&#21487;&#25193;&#23637;OPTICS&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#38543;&#26426;&#26680;&#29305;&#24449;&#23558;sDBSCAN&#21644;sOPTICS&#25193;&#23637;&#21040;L2&#12289;L1&#12289;$\chi^2$&#21644;Jensen-Shannon&#36317;&#31163;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;sDBSCAN&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#19978;&#27604;&#35768;&#22810;&#20854;&#20182;&#32858;&#31867;&#31639;&#27861;&#26174;&#33879;&#26356;&#24555;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#65292;sDBSCAN&#21644;sOPTICS&#22312;&#20960;&#20998;&#38047;&#20869;&#36816;&#34892;&#65292;&#32780;scikit-learn&#30340;&#23545;&#24212;&#31639;&#27861;&#38656;&#35201;&#25968;&#23567;&#26102;&#25110;&#30001;&#20110;&#20869;&#23384;&#19981;&#36275;&#32780;&#26080;&#27861;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15679v1 Announce Type: new  Abstract: We present sDBSCAN, a scalable density-based clustering algorithm in high dimensions with cosine distance. Utilizing the neighborhood-preserving property of random projections, sDBSCAN can quickly identify core points and their neighborhoods, the primary hurdle of density-based clustering. Theoretically, sDBSCAN outputs a clustering structure similar to DBSCAN under mild conditions with high probability. To further facilitate sDBSCAN, we present sOPTICS, a scalable OPTICS for interactive exploration of the intrinsic clustering structure. We also extend sDBSCAN and sOPTICS to L2, L1, $\chi^2$, and Jensen-Shannon distances via random kernel features. Empirically, sDBSCAN is significantly faster and provides higher accuracy than many other clustering algorithms on real-world million-point data sets. On these data sets, sDBSCAN and sOPTICS run in a few minutes, while the scikit-learn's counterparts demand several hours or cannot run due to m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#25913;&#36827;&#22312;&#32447;&#23458;&#25143;&#26381;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23458;&#25143;&#38382;&#39064;&#39044;&#27979;&#26631;&#31614;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#28040;&#38500;&#20010;&#21035;&#27169;&#22411;&#35757;&#32451;&#21644;&#32500;&#25252;&#30340;&#38656;&#27714;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#24320;&#21457;&#21608;&#26399;&#21644;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.15666</link><description>&lt;p&gt;
&#22312;&#32447;&#23458;&#25143;&#26381;&#21153;&#20013;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Universal Model in Online Customer Service
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#25913;&#36827;&#22312;&#32447;&#23458;&#25143;&#26381;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23458;&#25143;&#38382;&#39064;&#39044;&#27979;&#26631;&#31614;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#28040;&#38500;&#20010;&#21035;&#27169;&#22411;&#35757;&#32451;&#21644;&#32500;&#25252;&#30340;&#38656;&#27714;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#24320;&#21457;&#21608;&#26399;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#22312; typcial &#21830;&#19994;&#22330;&#26223;&#20013;&#24120;&#24120;&#38656;&#35201;&#25968;&#26376;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#24615;&#33021;&#30340;&#19968;&#33268;&#24615;&#24182;&#32771;&#34385;&#21040;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#23450;&#26399;&#30340;&#37325;&#26032;&#35757;&#32451;&#26159;&#24517;&#38656;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30005;&#23376;&#21830;&#21153;&#22312;&#32447;&#23458;&#25143;&#26381;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23458;&#25143;&#38382;&#39064;&#39044;&#27979;&#26631;&#31614;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#23545;&#35805;&#20013;&#26631;&#35760;&#23458;&#25143;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#38382;&#39064;&#21450;&#30456;&#24212;&#26631;&#31614;&#30340;&#23384;&#20648;&#24211;&#12290;&#24403;&#23458;&#25143;&#35831;&#27714;&#24110;&#21161;&#26102;&#65292;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22312;&#23384;&#20648;&#24211;&#20013;&#25628;&#32034;&#30456;&#20284;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#32479;&#35745;&#20998;&#26512;&#26469;&#39044;&#27979;&#30456;&#24212;&#30340;&#26631;&#31614;&#12290;&#36890;&#36807;&#28040;&#38500;&#20010;&#21035;&#27169;&#22411;&#35757;&#32451;&#21644;&#32500;&#25252;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#27169;&#22411;&#24320;&#21457;&#21608;&#26399;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15666v1 Announce Type: cross  Abstract: Building machine learning models can be a time-consuming process that often takes several months to implement in typical business scenarios. To ensure consistent model performance and account for variations in data distribution, regular retraining is necessary. This paper introduces a solution for improving online customer service in e-commerce by presenting a universal model for predict-ing labels based on customer questions, without requiring training. Our novel approach involves using machine learning techniques to tag customer questions in transcripts and create a repository of questions and corresponding labels. When a customer requests assistance, an information retrieval model searches the repository for similar questions, and statistical analysis is used to predict the corresponding label. By eliminating the need for individual model training and maintenance, our approach reduces both the model development cycle and costs. The 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#24072;&#29983;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#23458;&#25143;&#32852;&#31995;&#30340;&#22797;&#26434;&#24615;&#24182;&#23558;&#20854;&#24341;&#23548;&#21040;&#21512;&#36866;&#30340;&#20195;&#29702;&#21830;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#20307;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;AUC&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.15665</link><description>&lt;p&gt;
&#22312;&#26234;&#33021;&#36335;&#30001;&#20013;&#30340;&#24072;&#29983;&#23398;&#20064;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Teacher-Student Learning on Complexity in Intelligent Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15665
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#24072;&#29983;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#23458;&#25143;&#32852;&#31995;&#30340;&#22797;&#26434;&#24615;&#24182;&#23558;&#20854;&#24341;&#23548;&#21040;&#21512;&#36866;&#30340;&#20195;&#29702;&#21830;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#20307;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;AUC&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#26381;&#21153;&#36890;&#24120;&#26159;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#20013;&#26368;&#32791;&#26102;&#30340;&#29615;&#33410;&#65292;&#27599;&#27425;&#32852;&#31995;&#36890;&#24120;&#38656;&#35201;&#33457;&#36153;10-15&#20998;&#38047;&#12290;&#26377;&#25928;&#22320;&#23558;&#23458;&#25143;&#24341;&#23548;&#21040;&#21512;&#36866;&#30340;&#20195;&#29702;&#21830;&#65292;&#36991;&#20813;&#36716;&#25509;&#26159;&#30005;&#23376;&#21830;&#21153;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#23458;&#25143;&#32852;&#31995;&#30340;&#22797;&#26434;&#24615;&#24182;&#30456;&#24212;&#22320;&#23558;&#20854;&#24341;&#23548;&#21040;&#21512;&#36866;&#30340;&#20195;&#29702;&#21830;&#12290;&#35813;&#26694;&#26550;&#20998;&#20026;&#20004;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#24072;&#20613;&#27169;&#22411;&#65292;&#26681;&#25454;&#32852;&#31995;&#21518;&#30340;&#23545;&#35805;&#25991;&#26412;&#35780;&#20998;&#26469;&#35780;&#20272;&#32852;&#31995;&#30340;&#22797;&#26434;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24072;&#20613;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#26631;&#27880;&#32773;&#65292;&#20026;&#35757;&#32451;&#19968;&#20010;&#21482;&#26681;&#25454;&#32852;&#31995;&#21069;&#25968;&#25454;&#39044;&#27979;&#22797;&#26434;&#24615;&#30340;&#23398;&#29983;&#27169;&#22411;&#25552;&#20379;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#26694;&#26550;&#26159;&#25104;&#21151;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#23458;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#22797;&#26434;&#24615;AUC&#30340;&#26377;&#29992;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#22312;&#32479;&#35745;&#27700;&#24179;&#19978;&#35780;&#20272;&#23458;&#25143;&#26381;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15665v1 Announce Type: cross  Abstract: Customer service is often the most time-consuming aspect for e-commerce websites, with each contact typically taking 10-15 minutes. Effectively routing customers to appropriate agents without transfers is therefore crucial for e-commerce success. To this end, we have developed a machine learning framework that predicts the complexity of customer contacts and routes them to appropriate agents accordingly. The framework consists of two parts. First, we train a teacher model to score the complexity of a contact based on the post-contact transcripts. Then, we use the teacher model as a data annotator to provide labels to train a student model that predicts the complexity based on pre-contact data only. Our experiments show that such a framework is successful and can significantly improve customer experience. We also propose a useful metric called complexity AUC that evaluates the effectiveness of customer service at a statistical level.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21322;&#32447;&#24615;&#31070;&#32463;&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#27979;&#21644;&#26657;&#27491;&#25805;&#20316;&#23454;&#29616;&#20102;&#23545;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#26102;&#31354;PDE&#30340;&#35299;&#36827;&#34892;&#22788;&#29702;&#19982;&#25968;&#25454;&#21516;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15656</link><description>&lt;p&gt;
&#23398;&#20064;&#21322;&#32447;&#24615;&#31070;&#32463;&#31639;&#23376;&#65306;&#39044;&#27979;&#21644;&#25968;&#25454;&#21516;&#21270;&#30340;&#32479;&#19968;&#36882;&#24402;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning Semilinear Neural Operators : A Unified Recursive Framework For Prediction And Data Assimilation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15656
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21322;&#32447;&#24615;&#31070;&#32463;&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#27979;&#21644;&#26657;&#27491;&#25805;&#20316;&#23454;&#29616;&#20102;&#23545;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#26102;&#31354;PDE&#30340;&#35299;&#36827;&#34892;&#22788;&#29702;&#19982;&#25968;&#25454;&#21516;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#31639;&#23376;&#65288;NOs&#65289;&#29702;&#35770;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#35745;&#31639;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25551;&#36848;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#35299;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#24403;&#21069;&#22522;&#20110;NO&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22788;&#29702;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#26102;&#31354;PDE&#26102;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#21069;&#30340;NO&#29702;&#35770;&#27809;&#26377;&#25552;&#20986;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#20197;&#20415;&#26681;&#25454;&#31232;&#30095;&#37319;&#26679;&#30340;&#22024;&#26434;&#27979;&#37327;&#26377;&#25928;&#22320;&#32416;&#27491;PDE&#35299;&#30340;&#28436;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#29366;&#24577;&#31354;&#38388;&#26041;&#27861;&#26469;&#35745;&#31639;&#26080;&#38480;&#32500;&#21322;&#32447;&#24615;PDE&#30340;&#35299;&#31639;&#23376;&#12290;&#21033;&#29992;&#21322;&#32447;&#24615;PDE&#30340;&#32467;&#26500;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#38750;&#32447;&#24615;&#35266;&#27979;&#32773;&#29702;&#35770;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#36882;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#27979;&#21644;&#26657;&#27491;&#25805;&#20316;&#65292;&#20801;&#35768;&#21516;&#26102;&#36827;&#34892;&#39044;&#27979;&#21644;&#25968;&#25454;&#21516;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15656v1 Announce Type: cross  Abstract: Recent advances in the theory of Neural Operators (NOs) have enabled fast and accurate computation of the solutions to complex systems described by partial differential equations (PDEs). Despite their great success, current NO-based solutions face important challenges when dealing with spatio-temporal PDEs over long time scales. Specifically, the current theory of NOs does not present a systematic framework to perform data assimilation and efficiently correct the evolution of PDE solutions over time based on sparsely sampled noisy measurements. In this paper, we propose a learning-based state-space approach to compute the solution operators to infinite-dimensional semilinear PDEs. Exploiting the structure of semilinear PDEs and the theory of nonlinear observers in function spaces, we develop a flexible recursive method that allows for both prediction and data assimilation by combining prediction and correction operations. The proposed 
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23450;&#20041;&#32852;&#31995;&#22797;&#26434;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;AI&#19987;&#23478;&#27169;&#22411;&#26469;&#35780;&#20272;&#23458;&#25143;&#38382;&#39064;&#22797;&#26434;&#24615;&#65292;&#36991;&#20813;&#20102;&#20154;&#24037;&#26631;&#27880;&#30340;&#26102;&#38388;&#21644;&#37329;&#38065;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.15655</link><description>&lt;p&gt;
&#23458;&#25143;&#26381;&#21153;&#20013;&#30340;&#32852;&#31995;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Contact Complexity in Customer Service
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15655
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23450;&#20041;&#32852;&#31995;&#22797;&#26434;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;AI&#19987;&#23478;&#27169;&#22411;&#26469;&#35780;&#20272;&#23458;&#25143;&#38382;&#39064;&#22797;&#26434;&#24615;&#65292;&#36991;&#20813;&#20102;&#20154;&#24037;&#26631;&#27880;&#30340;&#26102;&#38388;&#21644;&#37329;&#38065;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#31995;&#23458;&#25143;&#26381;&#21153;&#25903;&#25345;&#30340;&#23458;&#25143;&#21487;&#33021;&#38754;&#20020;&#21508;&#31181;&#22797;&#26434;&#31243;&#24230;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#23558;&#39640;&#22797;&#26434;&#24615;&#30340;&#32852;&#31995;&#36335;&#30001;&#21040;&#21021;&#32423;&#20195;&#29702;&#21487;&#33021;&#23548;&#33268;&#22810;&#27425;&#36716;&#25509;&#25110;&#37325;&#22797;&#32852;&#31995;&#65292;&#32780;&#23558;&#20302;&#22797;&#26434;&#24615;&#30340;&#32852;&#31995;&#36335;&#30001;&#21040;&#39640;&#32423;&#20195;&#29702;&#21487;&#33021;&#20250;&#20351;&#20182;&#20204;&#30340;&#19987;&#19994;&#24110;&#21161;&#23481;&#37327;&#21463;&#21040;&#21387;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#23458;&#25143;&#38382;&#39064;&#22797;&#26434;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23450;&#20041;&#32852;&#31995;&#30340;&#22797;&#26434;&#24615;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#20010;&#39640;&#24230;&#25277;&#35937;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#32463;&#39564;&#20195;&#29702;&#36827;&#34892;&#22522;&#20110;&#20849;&#35782;&#30340;&#25968;&#25454;&#27880;&#37322;&#26159;&#19968;&#20010;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#21644;&#37329;&#38065;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23450;&#20041;&#32852;&#31995;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#65292;&#32780;&#26159;&#35757;&#32451;&#19968;&#31181;AI&#19987;&#23478;&#27169;&#22411;&#26469;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#24182;&#26681;&#25454;&#32852;&#31995;&#30340;&#34892;&#20026;&#35780;&#20272;&#27599;&#20010;&#32852;&#31995;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15655v1 Announce Type: cross  Abstract: Customers who reach out for customer service support may face a range of issues that vary in complexity. Routing high-complexity contacts to junior agents can lead to multiple transfers or repeated contacts, while directing low-complexity contacts to senior agents can strain their capacity to assist customers who need professional help. To tackle this, a machine learning model that accurately predicts the complexity of customer issues is highly desirable. However, defining the complexity of a contact is a difficult task as it is a highly abstract concept. While consensus-based data annotation by experienced agents is a possible solution, it is time-consuming and costly. To overcome these challenges, we have developed a novel machine learning approach to define contact complexity. Instead of relying on human annotation, we trained an AI expert model to mimic the behavior of agents and evaluate each contact's complexity based on how the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#25233;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#25913;&#36827;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#22522;&#20934;&#32447;&#30456;&#24403;&#30340;&#20219;&#21153;&#22870;&#21169;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.15650</link><description>&lt;p&gt;
&#20855;&#26377;&#30446;&#26631;&#25233;&#21046;&#30340;&#22810;&#32422;&#26463;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Constraint Safe RL with Objective Suppression for Safety-Critical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#25233;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#25913;&#36827;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#22522;&#20934;&#32447;&#30456;&#24403;&#30340;&#20219;&#21153;&#22870;&#21169;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38750;&#24120;&#24120;&#35265;&#65292;&#20294;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#30446;&#26631;&#25233;&#21046;&#65292;&#26681;&#25454;&#23433;&#20840;&#35780;&#21028;&#22120;&#33258;&#36866;&#24212;&#22320;&#25233;&#21046;&#20219;&#21153;&#22870;&#21169;&#26368;&#22823;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#23545;&#30446;&#26631;&#25233;&#21046;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19968;&#20010;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20219;&#20309;&#38169;&#35823;&#30340;&#34892;&#20026;&#37117;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#26174;&#33879;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#21305;&#37197;&#25105;&#20204;&#30340;&#22522;&#20934;&#32447;&#25152;&#36798;&#21040;&#30340;&#20219;&#21153;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15650v1 Announce Type: cross  Abstract: Safe reinforcement learning tasks with multiple constraints are a challenging domain despite being very common in the real world. To address this challenge, we propose Objective Suppression, a novel method that adaptively suppresses the task reward maximizing objectives according to a safety critic. We benchmark Objective Suppression in two multi-constraint safety domains, including an autonomous driving domain where any incorrect behavior can lead to disastrous consequences. Empirically, we demonstrate that our proposed method, when combined with existing safe RL algorithms, can match the task reward achieved by our baselines with significantly fewer constraint violations.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20248;&#21270;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25928;&#29992;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;FairGrad&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#25439;&#22833;&#36882;&#20943;&#30340;&#20844;&#24179;&#20248;&#21270;&#65292;&#21516;&#26102;&#20855;&#26377;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.15638</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Fair Resource Allocation in Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15638
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20248;&#21270;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25928;&#29992;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;FairGrad&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#25439;&#22833;&#36882;&#20943;&#30340;&#20844;&#24179;&#20248;&#21270;&#65292;&#21516;&#26102;&#20855;&#26377;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21487;&#20197;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#30693;&#35782;&#65292;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;MTL&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#23384;&#22312;&#20914;&#31361;&#30340;&#26799;&#24230;&#65292;&#36825;&#21487;&#33021;&#38459;&#30861;&#26576;&#20123;&#20219;&#21153;&#30340;&#20844;&#24179;&#20248;&#21270;&#65292;&#20174;&#32780;&#22952;&#30861;MTL&#23454;&#29616;&#26356;&#22909;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#21463;&#36890;&#20449;&#32593;&#32476;&#20013;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;MTL&#30340;&#20248;&#21270;&#23450;&#24335;&#20026;&#19968;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#22312;&#19981;&#21516;&#30340;&#20844;&#24179;&#24230;&#37327;&#19979;&#26368;&#22823;&#21270;&#36328;&#20219;&#21153;&#30340;&#25439;&#22833;&#36882;&#20943;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MTL&#20248;&#21270;&#26041;&#27861;FairGrad&#12290;FairGrad&#19981;&#20165;&#21487;&#20197;&#28789;&#27963;&#22320;&#24378;&#35843;&#26576;&#20123;&#20219;&#21153;&#65292;&#32780;&#19988;&#21487;&#20197;&#23454;&#29616;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#26799;&#24230;&#35843;&#25972;&#26041;&#27861;&#20013;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15638v1 Announce Type: new  Abstract: By jointly learning multiple tasks, multi-task learning (MTL) can leverage the shared knowledge across tasks, resulting in improved data efficiency and generalization performance. However, a major challenge in MTL lies in the presence of conflicting gradients, which can hinder the fair optimization of some tasks and subsequently impede MTL's ability to achieve better overall performance. Inspired by fair resource allocation in communication networks, we formulate the optimization of MTL as a utility maximization problem, where the loss decreases across tasks are maximized under different fairness measurements. To solve this problem, we propose FairGrad, a novel MTL optimization method. FairGrad not only enables flexible emphasis on certain tasks but also achieves a theoretical convergence guarantee. Extensive experiments demonstrate that our method can achieve state-of-the-art performance among gradient manipulation methods on a suite of
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36830;&#32493;&#30340;&#25805;&#20316;&#21592;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;Jerk&#27491;&#21017;&#21270;&#32435;&#20837;&#21387;&#32553;&#28508;&#22312;&#31354;&#38388;&#30340;&#23398;&#20064;&#20013;&#65292;&#20419;&#36827;&#20102;&#28508;&#22312;&#31354;&#38388;&#21160;&#24577;&#30340;&#24179;&#28369;&#24615;&#21644;&#31232;&#30095;&#24615;</title><link>https://arxiv.org/abs/2402.15636</link><description>&lt;p&gt;
&#25805;&#20316;&#21592;&#23398;&#20064;&#20013;&#20809;&#28369;&#31232;&#30095;&#30340;&#28508;&#22312;&#21160;&#24577;&#19982;Jerk&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Smooth and Sparse Latent Dynamics in Operator Learning with Jerk Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15636
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36830;&#32493;&#30340;&#25805;&#20316;&#21592;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;Jerk&#27491;&#21017;&#21270;&#32435;&#20837;&#21387;&#32553;&#28508;&#22312;&#31354;&#38388;&#30340;&#23398;&#20064;&#20013;&#65292;&#20419;&#36827;&#20102;&#28508;&#22312;&#31354;&#38388;&#21160;&#24577;&#30340;&#24179;&#28369;&#24615;&#21644;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15636v1 Announce Type: new  Abstract: &#26102;&#31354;&#24314;&#27169;&#23545;&#20110;&#29702;&#35299;&#36328;&#36234;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#23398;&#31185;&#30340;&#22797;&#26434;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#31995;&#32479;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#65292;&#25511;&#21046;&#26041;&#31243;&#36890;&#24120;&#19981;&#23436;&#20840;&#24050;&#30693;&#25110;&#22312;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#12290;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#38477;&#38454;&#27169;&#22411;&#65288;ROMs&#65289;&#36890;&#36807;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#26469;&#25552;&#20379;&#19968;&#31181;&#24555;&#36895;&#20934;&#30830;&#30340;&#26102;&#31354;&#39044;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26500;&#24314;&#28508;&#22312;&#31354;&#38388;&#26102;&#36890;&#24120;&#24573;&#30053;&#30456;&#37051;&#24555;&#29031;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#36825;&#23548;&#33268;&#21387;&#32553;&#27425;&#20248;&#12289;&#38191;&#40831;&#29366;&#30340;&#28508;&#22312;&#36712;&#36857;&#20197;&#21450;&#38543;&#26102;&#38388;&#26377;&#38480;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#32493;&#30340;&#25805;&#20316;&#21592;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;Jerk&#27491;&#21017;&#21270;&#32435;&#20837;&#21040;&#21387;&#32553;&#28508;&#22312;&#31354;&#38388;&#30340;&#23398;&#20064;&#20013;&#12290;&#36825;&#31181;Jerk&#27491;&#21017;&#21270;&#20419;&#36827;&#20102;&#28508;&#22312;&#31354;&#38388;&#21160;&#24577;&#30340;&#24179;&#28369;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15636v1 Announce Type: new  Abstract: Spatiotemporal modeling is critical for understanding complex systems across various scientific and engineering disciplines, but governing equations are often not fully known or computationally intractable due to inherent system complexity. Data-driven reduced-order models (ROMs) offer a promising approach for fast and accurate spatiotemporal forecasting by computing solutions in a compressed latent space. However, these models often neglect temporal correlations between consecutive snapshots when constructing the latent space, leading to suboptimal compression, jagged latent trajectories, and limited extrapolation ability over time. To address these issues, this paper introduces a continuous operator learning framework that incorporates jerk regularization into the learning of the compressed latent space. This jerk regularization promotes smoothness and sparsity of latent space dynamics, which not only yields enhanced accuracy and conve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#26001;&#28857;&#22122;&#22768;&#23384;&#22312;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#35065;&#34955;&#24335;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#65288;Bagged-DIP&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#19982;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#38598;&#25104;&#65292;&#20197;&#21450;&#36890;&#36807;&#22312;&#36845;&#20195;&#20013;&#20351;&#29992;Newton-Schulz&#31639;&#27861;&#26469;&#20943;&#23569;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.15635</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#26001;&#28857;&#22122;&#22768;&#23384;&#22312;&#24773;&#20917;&#19979;&#24674;&#22797;&#22270;&#20687;&#30340;&#35065;&#34955;&#24335;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Bagged Deep Image Prior for Recovering Images in the Presence of Speckle Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15635
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#26001;&#28857;&#22122;&#22768;&#23384;&#22312;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#35065;&#34955;&#24335;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#65288;Bagged-DIP&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#19982;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#38598;&#25104;&#65292;&#20197;&#21450;&#36890;&#36807;&#22312;&#36845;&#20195;&#20013;&#20351;&#29992;Newton-Schulz&#31639;&#27861;&#26469;&#20943;&#23569;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22522;&#20110;&#20284;&#28982;&#30340;&#26041;&#27861;&#22312;&#26001;&#28857;&#65288;&#20056;&#24615;&#65289;&#22122;&#22768;&#24433;&#21709;&#19979;&#20174;&#22810;&#32452;&#27979;&#37327;&#20013;&#24674;&#22797;&#22797;&#26434;&#20540;&#20449;&#21495;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#21253;&#25324;&#24314;&#31435;&#22312;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#20551;&#35774;&#19979;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#31532;&#19968;&#20010;&#29702;&#35770;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#25429;&#25417;&#20102;MSE&#19982;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;&#35266;&#27979;&#27425;&#25968;&#12289;&#20449;&#21495;&#32500;&#24230;&#21644;&#27599;&#27425;&#35266;&#27979;&#30340;&#27979;&#37327;&#27425;&#25968;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#31639;&#27861;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35065;&#34955;&#24335;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#65288;Bagged-DIP&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#19982;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#38598;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;PGD&#30340;&#36845;&#20195;&#20013;&#20351;&#29992;Newton-Schulz&#31639;&#27861;&#35745;&#31639;&#30697;&#38453;&#36870;&#65292;&#20174;&#32780;&#38477;&#20302;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15635v1 Announce Type: cross  Abstract: We investigate both the theoretical and algorithmic aspects of likelihood-based methods for recovering a complex-valued signal from multiple sets of measurements, referred to as looks, affected by speckle (multiplicative) noise. Our theoretical contributions include establishing the first existing theoretical upper bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under the deep image prior hypothesis. Our theoretical results capture the dependence of MSE upon the number of parameters in the deep image prior, the number of looks, the signal dimension, and the number of measurements per look. On the algorithmic side, we introduce the concept of bagged Deep Image Priors (Bagged-DIP) and integrate them with projected gradient descent. Furthermore, we show how employing Newton-Schulz algorithm for calculating matrix inverses within the iterations of PGD reduces the computational complexity of the algorithm. We will 
&lt;/p&gt;</description></item><item><title>MegaScale&#39033;&#30446;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#36229;&#36807;10,000&#20010;GPU&#35268;&#27169;&#19978;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#20135;&#31995;&#32479;&#65292;&#36890;&#36807;&#20840;&#26632;&#26041;&#27861;&#21327;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#32452;&#20214;&#26469;&#35299;&#20915;&#35757;&#32451;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15627</link><description>&lt;p&gt;
MegaScale: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25193;&#23637;&#21040;&#36229;&#36807;10,000&#20010;GPU
&lt;/p&gt;
&lt;p&gt;
MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15627
&lt;/p&gt;
&lt;p&gt;
MegaScale&#39033;&#30446;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#36229;&#36807;10,000&#20010;GPU&#35268;&#27169;&#19978;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#20135;&#31995;&#32479;&#65292;&#36890;&#36807;&#20840;&#26632;&#26041;&#27861;&#21327;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#32452;&#20214;&#26469;&#35299;&#20915;&#35757;&#32451;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#24037;&#31243;&#32463;&#39564;&#65292;&#26500;&#24314;&#21644;&#37096;&#32626;&#20102;MegaScale&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#36229;&#36807;10,000&#20010;GPU&#35268;&#27169;&#19978;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#20135;&#31995;&#32479;&#12290;&#22312;&#36825;&#31181;&#35268;&#27169;&#19978;&#35757;&#32451;LLMs&#20250;&#32473;&#35757;&#32451;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#24102;&#26469;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#31181;&#20840;&#26632;&#26041;&#27861;&#65292;&#36328;&#27169;&#22411;&#22359;&#21644;&#20248;&#21270;&#22120;&#35774;&#35745;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#37325;&#21472;&#12289;&#31639;&#23376;&#20248;&#21270;&#12289;&#25968;&#25454;&#31649;&#36947;&#21644;&#32593;&#32476;&#24615;&#33021;&#35843;&#20248;&#65292;&#21327;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#32452;&#20214;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#39640;&#25928;&#29575;&#65288;&#21363;&#31283;&#23450;&#24615;&#65289;&#26159;&#22312;&#29983;&#20135;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#32771;&#34385;&#65292;&#32771;&#34385;&#21040;LLM&#35757;&#32451;&#20316;&#19994;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#12290;&#35768;&#22810;&#33392;&#38590;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#21482;&#26377;&#22312;&#22823;&#35268;&#27169;&#19979;&#25165;&#20250;&#20986;&#29616;&#65292;&#28145;&#20837;&#30340;&#21487;&#35266;&#23519;&#24615;&#26159;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#35786;&#26029;&#24037;&#20855;&#65292;&#29992;&#20110;&#30417;&#25511;&#31995;&#32479;&#32452;&#20214;&#21644;&#28145;&#23618;&#22534;&#26632;&#20013;&#30340;&#20107;&#20214;&#65292;&#35782;&#21035;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#24471;&#20986;&#26377;&#25928;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15627v1 Announce Type: new  Abstract: We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effectiv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MissNODAGS&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#24490;&#29615;&#22240;&#26524;&#22270;&#65292;&#36890;&#36807;&#20132;&#26367;&#26367;&#34917;&#32570;&#22833;&#25968;&#25454;&#21644;&#26368;&#22823;&#21270;&#21487;&#35265;&#25968;&#25454;&#37096;&#20998;&#30340;&#39044;&#26399;&#23545;&#25968;&#20284;&#28982;&#26469;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.15625</link><description>&lt;p&gt;
&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#23398;&#20064;&#24490;&#29615;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Cyclic Causal Models from Incomplete Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MissNODAGS&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#24490;&#29615;&#22240;&#26524;&#22270;&#65292;&#36890;&#36807;&#20132;&#26367;&#26367;&#34917;&#32570;&#22833;&#25968;&#25454;&#21644;&#26368;&#22823;&#21270;&#21487;&#35265;&#25968;&#25454;&#37096;&#20998;&#30340;&#39044;&#26399;&#23545;&#25968;&#20284;&#28982;&#26469;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#23398;&#20064;&#26159;&#32479;&#35745;&#23398;&#21644;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#26410;&#35265;&#27835;&#30103;&#23545;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#37117;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#20551;&#35774;&#65306;(i) &#28508;&#22312;&#22270;&#26159;&#26080;&#29615;&#30340;&#65292;(ii) &#21487;&#29992;&#25968;&#25454;&#26159;&#23436;&#25972;&#30340;&#12290;&#36825;&#20123;&#20551;&#35774;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#31995;&#32479;&#21253;&#21547;&#21453;&#39304;&#29615;&#36335;&#65288;&#20363;&#22914;&#29983;&#29289;&#31995;&#32479;&#65289;&#65292;&#23454;&#38469;&#24773;&#20917;&#32463;&#24120;&#28041;&#21450;&#32570;&#22833;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MissNODAGS&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#24490;&#29615;&#22240;&#26524;&#22270;&#12290;&#22312;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#19979;&#65292;MissNODAGS&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#22312;&#26367;&#34917;&#32570;&#22833;&#25968;&#25454;&#19982;&#26368;&#22823;&#21270;&#25968;&#25454;&#21487;&#35265;&#37096;&#20998;&#30340;&#39044;&#26399;&#23545;&#25968;&#20284;&#28982;&#20043;&#38388;&#20132;&#26367;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#36981;&#24490;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26694;&#26550;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15625v1 Announce Type: cross  Abstract: Causal learning is a fundamental problem in statistics and science, offering insights into predicting the effects of unseen treatments on a system. Despite recent advances in this topic, most existing causal discovery algorithms operate under two key assumptions: (i) the underlying graph is acyclic, and (ii) the available data is complete. These assumptions can be problematic as many real-world systems contain feedback loops (e.g., biological systems), and practical scenarios frequently involve missing data. In this work, we propose a novel framework, named MissNODAGS, for learning cyclic causal graphs from partially missing data. Under the additive noise model, MissNODAGS learns the causal graph by alternating between imputing the missing data and maximizing the expected log-likelihood of the visible part of the data in each training step, following the principles of the expectation-maximization (EM) framework. Through synthetic exper
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20197;&#20154;&#31867;&#21487;&#35835;&#25991;&#26412;&#34920;&#31034;&#30340;&#29992;&#25143;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;Language-based Factorization Model (LFM)&#65292;&#22312;&#20919;&#21551;&#21160;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.15623</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#20559;&#22909;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language-Based User Profiles for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15623
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20197;&#20154;&#31867;&#21487;&#35835;&#25991;&#26412;&#34920;&#31034;&#30340;&#29992;&#25143;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;Language-based Factorization Model (LFM)&#65292;&#22312;&#20919;&#21551;&#21160;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;&#25512;&#33616;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#20998;&#35299;&#65289;&#23558;&#29992;&#25143;&#20559;&#22909;&#34920;&#31034;&#20026;&#39640;&#32500;&#21521;&#37327;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#21521;&#37327;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#22312;&#20919;&#21551;&#21160;&#29615;&#22659;&#19979;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#20197;&#20154;&#31867;&#21487;&#35835;&#25991;&#26412;&#34920;&#31034;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#22240;&#23376;&#20998;&#35299;&#27169;&#22411;&#65288;LFM&#65289;&#65292;&#23427;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#22343;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#32534;&#30721;&#22120;LLM&#20174;&#29992;&#25143;&#30340;&#35780;&#20998;&#21382;&#21490;&#29983;&#25104;&#29992;&#25143;&#20852;&#36259;&#30340;&#31616;&#27905;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#35299;&#30721;&#22120;LLM&#20351;&#29992;&#36825;&#20010;&#31616;&#35201;&#25551;&#36848;&#26469;&#23436;&#25104;&#39044;&#27979;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;MovieLens&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LFM&#26041;&#27861;&#65292;&#23558;&#20854;&#19982;&#30697;&#38453;&#20998;&#35299;&#21644;&#30452;&#25509;&#20174;&#29992;&#25143;&#35780;&#20998;&#21382;&#21490;&#39044;&#27979;&#30340;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#20919;&#21551;&#21160;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15623v1 Announce Type: new  Abstract: Most conventional recommendation methods (e.g., matrix factorization) represent user profiles as high-dimensional vectors. Unfortunately, these vectors lack interpretability and steerability, and often perform poorly in cold-start settings. To address these shortcomings, we explore the use of user profiles that are represented as human-readable text. We propose the Language-based Factorization Model (LFM), which is essentially an encoder/decoder model where both the encoder and the decoder are large language models (LLMs). The encoder LLM generates a compact natural-language profile of the user's interests from the user's rating history. The decoder LLM uses this summary profile to complete predictive downstream tasks. We evaluate our LFM approach on the MovieLens dataset, comparing it against matrix factorization and an LLM model that directly predicts from the user's rating history. In cold-start settings, we find that our method can h
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;LLMs&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#26631;&#35760;&#25968;&#25454;&#33719;&#21462;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.15613</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#34920;&#31034;&#23454;&#29616;&#22312;NLP&#20013;&#30340;&#39640;&#25928;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Active Learning in NLP via Pretrained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15613
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;LLMs&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#26631;&#35760;&#25968;&#25454;&#33719;&#21462;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24494;&#35843;&#29616;&#22312;&#26159;&#25991;&#26412;&#20998;&#31867;&#20013;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#33021;&#30475;&#21040;&#12290;&#24403;&#26631;&#35760;&#25991;&#26723;&#31232;&#32570;&#26102;&#65292;&#20027;&#21160;&#23398;&#20064;&#26377;&#21161;&#20110;&#33410;&#30465;&#27880;&#37322;&#24037;&#20316;&#65292;&#20294;&#38656;&#35201;&#22312;&#27599;&#27425;&#33719;&#21462;&#36845;&#20195;&#26102;&#37325;&#26032;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#20351;&#29992;LLMs&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#36825;&#19968;&#36807;&#31243;&#65292;&#19968;&#26086;&#33719;&#24471;&#25152;&#38656;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23601;&#21487;&#20197;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#24120;&#35265;&#30340;&#25991;&#26412;&#20998;&#31867;&#22522;&#20934;&#19978;&#39564;&#35777;&#65292;&#20197;&#39044;&#35757;&#32451;&#30340;BERT&#21644;RoBERTa&#20316;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#20135;&#29983;&#20102;&#19982;&#22312;&#25972;&#20010;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#24494;&#35843;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#24320;&#38144;&#38477;&#20302;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31243;&#24207;&#33719;&#21462;&#30340;&#25968;&#25454;&#21487;&#20197;&#36328;&#39044;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#27867;&#21270;&#65292;&#20174;&#32780;&#21487;&#20197;&#28789;&#27963;&#36873;&#25321;&#26368;&#32456;&#27169;&#22411;&#25110;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15613v1 Announce Type: cross  Abstract: Fine-tuning Large Language Models (LLMs) is now a common approach for text classification in a wide range of applications. When labeled documents are scarce, active learning helps save annotation efforts but requires retraining of massive models on each acquisition iteration. We drastically expedite this process by using pretrained representations of LLMs within the active learning loop and, once the desired amount of labeled data is acquired, fine-tuning that or even a different pretrained LLM on this labeled data to achieve the best performance. As verified on common text classification benchmarks with pretrained BERT and RoBERTa as the backbone, our strategy yields similar performance to fine-tuning all the way through the active learning loop but is orders of magnitude less computationally expensive. The data acquired with our procedure generalizes across pretrained networks, allowing flexibility in choosing the final model or upda
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;/&#30636;&#38388;&#39537;&#21160;&#30340;&#20004;&#31181;&#26367;&#20195;MPC&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#22823;&#35268;&#27169;&#31890;&#23376;&#31995;&#32479;&#30340;&#24555;&#36895;&#12289;&#23454;&#26102;&#21453;&#39304;&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.15611</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;/&#30636;&#38388;&#39537;&#21160;&#30340;&#38598;&#20307;&#21160;&#24577;&#24555;&#36895;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data/moment-driven approaches for fast predictive control of collective dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15611
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;/&#30636;&#38388;&#39537;&#21160;&#30340;&#20004;&#31181;&#26367;&#20195;MPC&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#22823;&#35268;&#27169;&#31890;&#23376;&#31995;&#32479;&#30340;&#24555;&#36895;&#12289;&#23454;&#26102;&#21453;&#39304;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26694;&#26550;&#20013;&#23457;&#26597;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#31890;&#23376;&#31995;&#32479;&#30340;&#21453;&#39304;&#25511;&#21046;&#21512;&#25104;&#12290;&#38598;&#20307;&#21160;&#24577;&#30340;&#39640;&#32500;&#29305;&#24615;&#38459;&#30861;&#20102;&#20256;&#32479;MPC&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#31639;&#27861;&#22522;&#20110;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#24555;&#36895;&#22312;&#32447;&#21160;&#24577;&#20248;&#21270;&#12290;&#25552;&#20986;&#20102;&#20004;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#31163;&#32447;&#36817;&#20284;&#26368;&#20248;&#21453;&#39304;&#23450;&#24459;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#23457;&#26597;&#20102;&#19968;&#31181;&#22522;&#20110;&#31890;&#23376;&#38598;&#21512;&#30340;&#23439;&#35266;&#37327;&#30340;&#39034;&#24207;&#32447;&#24615;&#21270;&#21160;&#24577;&#30340;&#36807;&#31243;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#36991;&#24320;&#20102;&#22312;&#32447;&#27714;&#35299;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#31890;&#23376;&#31995;&#32479;&#30340;&#24555;&#36895;&#12289;&#23454;&#26102;&#30340;&#21453;&#39304;&#21512;&#25104;&#12290;&#25968;&#20540;&#23454;&#39564;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15611v1 Announce Type: cross  Abstract: Feedback control synthesis for large-scale particle systems is reviewed in the framework of model predictive control (MPC). The high-dimensional character of collective dynamics hampers the performance of traditional MPC algorithms based on fast online dynamic optimization at every time step. Two alternatives to MPC are proposed. First, the use of supervised learning techniques for the offline approximation of optimal feedback laws is discussed. Then, a procedure based on sequential linearization of the dynamics based on macroscopic quantities of the particle ensemble is reviewed. Both approaches circumvent the online solution of optimal control problems enabling fast, real-time, feedback synthesis for large-scale particle systems. Numerical experiments assess the performance of the proposed algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;12&#20010;&#26376;&#32047;&#35745;&#20135;&#37327;&#30340;&#26377;&#25928;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#27833;&#20117;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15608</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27833;&#20117;&#24615;&#33021;&#20248;&#21270;&#23436;&#20117;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Based Completions Sequencing for Well Performance Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15608
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;12&#20010;&#26376;&#32047;&#35745;&#20135;&#37327;&#30340;&#26377;&#25928;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#27833;&#20117;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20934;&#30830;&#30340;&#30000;&#37326;&#21457;&#23637;&#21442;&#25968;&#20197;&#20248;&#21270;&#38271;&#26399;&#27833;&#30000;&#20135;&#37327;&#38656;&#35201;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#22240;&#20026;&#27833;&#20117;&#24320;&#21457;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#20272;&#31639;&#38271;&#26399;&#27833;&#20117;&#20135;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20256;&#32479;&#19978;&#65292;&#30707;&#27833;&#21644;&#22825;&#28982;&#27668;&#20844;&#21496;&#20351;&#29992;&#22266;&#26377;&#35745;&#31639;&#26114;&#36149;&#30340;&#27169;&#25311;&#36719;&#20214;&#26469;&#39044;&#27979;&#20135;&#37327;&#12290;&#22240;&#27492;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26368;&#36817;&#34987;&#25991;&#29486;&#20013;&#21033;&#29992;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20248;&#21270;&#23436;&#20117;&#21457;&#23637;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#22686;&#24378;&#23436;&#20117;&#29615;&#22659;&#12290;&#35813;&#39033;&#30446;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#33021;&#22815;&#38598;&#25104;&#22810;&#32500;&#39044;&#27979;&#21464;&#37327;&#25928;&#24212;&#65288;&#21363;&#23436;&#20117;&#29615;&#22659;&#65289;&#20197;&#20934;&#30830;&#39044;&#27979;12&#20010;&#26376;&#32047;&#35745;&#20135;&#37327;&#30340;&#26377;&#25928;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15608v1 Announce Type: new  Abstract: Establishing accurate field development parameters to optimize long-term oil production takes time and effort due to the complexity of oil well development, and the uncertainty in estimating long-term well production. Traditionally, oil and gas companies use simulation software that are inherently computationally expensive to forecast production. Thus, machine learning approaches are recently utilized in literature as an efficient alternative to optimize well developments by enhancing completion conditions. The primary goal of this project is to develop effective machine-learning models that can integrate the effects of multidimensional predictive variables (i.e., completion conditions) to predict 12-Month Cumulative Production accurately.   Three predictive regression machine learning models are implemented for predicting 12-month cumulative oil production: Random Forest, Gradient Boosting, and Long Short-Term Memory Models. All three m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20855;&#26377;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#21644;&#38750;&#32447;&#24615;MLP&#30340;Transformers&#30340;&#35757;&#32451;&#21160;&#24577;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#30340;ICL&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.15607</link><description>&lt;p&gt;
&#35757;&#32451;&#38750;&#32447;&#24615;Transformer&#36827;&#34892;&#39640;&#25928;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#29702;&#35770;&#23398;&#20064;&#21644;&#27867;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Training Nonlinear Transformers for Efficient In-Context Learning: A Theoretical Learning and Generalization Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20855;&#26377;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#21644;&#38750;&#32447;&#24615;MLP&#30340;Transformers&#30340;&#35757;&#32451;&#21160;&#24577;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#30340;ICL&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36890;&#36807;&#31616;&#21333;&#22320;&#22686;&#21152;&#26597;&#35810;&#19982;&#26469;&#33258;&#35813;&#20219;&#21153;&#30340;&#19968;&#20123;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#26469;&#24494;&#35843;&#12290;&#23613;&#31649;&#22312;&#23454;&#35777;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20998;&#26512;Transformers&#20013;&#38750;&#20984;&#35757;&#32451;&#38382;&#39064;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#22914;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#65292;&#35757;&#32451;Transformer&#20197;&#23454;&#29616;ICL&#21450;&#30456;&#24212;&#30340;ICL&#23481;&#37327;&#30340;&#26426;&#21046;&#22823;&#22810;&#19981;&#20026;&#20154;&#30693;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20855;&#26377;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#21644;&#38750;&#32447;&#24615;MLP&#30340;Transformers&#30340;&#35757;&#32451;&#21160;&#24577;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#30340;ICL&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#32452;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#36825;&#20123;&#20219;&#21153;&#23376;&#38598;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;Transformers&#65292;&#24182;&#37327;&#21270;&#21508;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15607v1 Announce Type: new  Abstract: Transformer-based large language models have displayed impressive in-context learning capabilities, where a pre-trained model can handle new tasks without fine-tuning by simply augmenting the query with some input-output examples from that task. Despite the empirical success, the mechanics of how to train a Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive due to the technical challenges of analyzing the nonconvex training problems resulting from the nonlinear self-attention and nonlinear activation in Transformers. To the best of our knowledge, this paper provides the first theoretical analysis of the training dynamics of Transformers with nonlinear self-attention and nonlinear MLP, together with the ICL generalization capability of the resulting model. Focusing on a group of binary classification tasks, we train Transformers using data from a subset of these tasks and quantify the impact of various factors
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#19982;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#20108;&#20803;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#25216;&#26415;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#24341;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#38544;&#31169;&#24615;&#33021;&#21644;&#25928;&#29992;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.15603</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20844;&#24179;&#20108;&#20803;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Fair Binary Classifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#19982;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#20108;&#20803;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#25216;&#26415;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#24341;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#38544;&#31169;&#24615;&#33021;&#21644;&#25928;&#29992;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#20108;&#20803;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#32806;&#25216;&#26415;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#20165;&#20855;&#26377;&#20844;&#24179;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#12290;&#35813;&#31639;&#27861;&#25509;&#21463;&#38024;&#23545;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#28385;&#36275;&#32479;&#35745;&#24179;&#34913;&#30340;&#21333;&#19968;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#35813;&#31639;&#27861;&#20197;&#32435;&#20837;&#24046;&#20998;&#38544;&#31169;&#12290;&#26368;&#32456;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20445;&#35777;&#26041;&#38754;&#24471;&#21040;&#20102;&#20005;&#26684;&#26816;&#39564;&#12290;&#23545;Adult&#21644;&#20449;&#29992;&#21345;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20844;&#24179;&#24615;&#20445;&#35777;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30456;&#21516;&#27700;&#24179;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15603v1 Announce Type: new  Abstract: In this work, we investigate binary classification under the constraints of both differential privacy and fairness. We first propose an algorithm based on the decoupling technique for learning a classifier with only fairness guarantee. This algorithm takes in classifiers trained on different demographic groups and generates a single classifier satisfying statistical parity. We then refine this algorithm to incorporate differential privacy. The performance of the final algorithm is rigorously examined in terms of privacy, fairness, and utility guarantees. Empirical evaluations conducted on the Adult and Credit Card datasets illustrate that our algorithm outperforms the state-of-the-art in terms of fairness guarantees, while maintaining the same level of privacy and utility.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#20855;&#26377;&#26497;&#23567;&#22343;&#26041;&#35823;&#24046;&#65292;&#21487;&#20197;&#33719;&#24471;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#31361;&#30772;&#20102;&#20165;&#20570;&#27425;&#39640;&#26031;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.15602</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26497;&#23567;&#21270;&#26368;&#20248;&#24615;&#65306;&#36229;&#36234;&#23494;&#24230;&#19979;&#30028;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#20855;&#26377;&#26497;&#23567;&#22343;&#26041;&#35823;&#24046;&#65292;&#21487;&#20197;&#33719;&#24471;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#31361;&#30772;&#20102;&#20165;&#20570;&#27425;&#39640;&#26031;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#38750;&#21442;&#25968;&#32479;&#35745;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#22823;&#26679;&#26412;&#22330;&#26223;&#19979;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#25277;&#26679;&#30340;&#28176;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#26680;&#30340;&#24471;&#20998;&#20272;&#35745;&#22120;&#21487;&#20197;&#23454;&#29616;&#23545; $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$ &#30340;&#24471;&#20998;&#20989;&#25968;&#30340;&#26368;&#20248;&#22343;&#26041;&#35823;&#24046;&#20026; $\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$&#65292;&#20854;&#20013; $n$ &#21644; $d$ &#20998;&#21035;&#20195;&#34920;&#26679;&#26412;&#22823;&#23567;&#21644;&#32500;&#24230;&#65292;$t$ &#22312;&#19978;&#19979;&#21463;&#21040; $n$ &#30340;&#22810;&#39033;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988; $p_0$ &#26159;&#20219;&#24847;&#27425;&#20122;&#39640;&#26031;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#36825;&#23548;&#33268;&#22312;&#20165;&#36827;&#34892;&#27425;&#39640;&#26031;&#20551;&#35774;&#26102;&#65292;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#20026; $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$&#12290;&#22914;&#26524;&#27492;&#22806;&#65292;$p_0$ &#23646;&#20110; $\beta\le 2$ &#30340; $\beta$-Sobolev&#31354;&#38388;&#30340;&#38750;&#21442;&#25968;&#26063;&#65292;&#36890;&#36807;&#37319;&#29992;&#26089;&#20572;&#31574;&#30053;&#65292;&#25105;&#20204;&#24471;&#21040;&#35813;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#30340;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#20026; $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15602v1 Announce Type: cross  Abstract: We study the asymptotic error of score-based diffusion model sampling in large-sample scenarios from a non-parametric statistics perspective. We show that a kernel-based score estimator achieves an optimal mean square error of $\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$ for the score function of $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$, where $n$ and $d$ represent the sample size and the dimension, $t$ is bounded above and below by polynomials of $n$, and $p_0$ is an arbitrary sub-Gaussian distribution. As a consequence, this yields an $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$ upper bound for the total variation error of the distribution of the sample generated by the diffusion model under a mere sub-Gaussian assumption. If in addition, $p_0$ belongs to the nonparametric family of the $\beta$-Sobolev space with $\beta\le 2$, by adopting an early stopping strategy, we obtain that the diffusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;HJB&#31639;&#23376;&#30340;&#38543;&#26426;&#31995;&#32479;&#31070;&#32463;&#26368;&#20248;&#25511;&#21046;&#22120;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#20915;&#39640;&#32500;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15592</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;HJB&#31639;&#23376;&#30340;&#38543;&#26426;&#31995;&#32479;&#31070;&#32463;&#26368;&#20248;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural optimal controller for stochastic systems via pathwise HJB operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;HJB&#31639;&#23376;&#30340;&#38543;&#26426;&#31995;&#32479;&#31070;&#32463;&#26368;&#20248;&#25511;&#21046;&#22120;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#20915;&#39640;&#32500;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#22522;&#20110;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#21644;&#21160;&#24577;&#35268;&#21010;&#65292;&#20026;&#39640;&#32500;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#24320;&#21457;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#19982;&#20381;&#36182;&#20110;Hamilton-Jacobi-Bellman&#65288;HJB&#65289;&#26041;&#31243;&#35299;&#30340;&#27010;&#29575;&#34920;&#31034;&#30340;&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19982;HJB&#26041;&#31243;&#30456;&#20851;&#30340;&#36335;&#24452;&#31639;&#23376;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#20041;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#26368;&#20248;&#25511;&#21046;&#26159;&#21542;&#20855;&#26377;&#26174;&#24335;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#25968;&#20540;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#25130;&#26029;&#12289;&#36817;&#20284;&#21644;&#20248;&#21270;&#35823;&#24046;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#20102;&#35823;&#24046;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#19978;&#23637;&#31034;&#25968;&#20540;&#32467;&#26524;&#65292;&#35828;&#26126;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15592v1 Announce Type: cross  Abstract: The aim of this work is to develop deep learning-based algorithms for high-dimensional stochastic control problems based on physics-informed learning and dynamic programming. Unlike classical deep learning-based methods relying on a probabilistic representation of the solution to the Hamilton--Jacobi--Bellman (HJB) equation, we introduce a pathwise operator associated with the HJB equation so that we can define a problem of physics-informed learning. According to whether the optimal control has an explicit representation, two numerical methods are proposed to solve the physics-informed learning problem. We provide an error analysis on how the truncation, approximation and optimization errors affect the accuracy of these methods. Numerical results on various applications are presented to illustrate the performance of the proposed algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#26469;&#28608;&#21457;&#19977;&#31181;&#27969;&#34892;LLM&#65292;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.15589</link><description>&lt;p&gt;
&#20174;&#23398;&#26415;&#25163;&#31295;&#30340;&#21516;&#34892;&#35780;&#23457;&#21465;&#20107;&#20013;&#35201;&#27714;LLMs&#25776;&#20889;&#20803;&#35780;&#35770;&#33609;&#26696;
&lt;/p&gt;
&lt;p&gt;
Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#26469;&#28608;&#21457;&#19977;&#31181;&#27969;&#34892;LLM&#65292;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#20294;&#20063;&#26368;&#32321;&#37325;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#25776;&#20889;&#20803;&#35780;&#35770;&#65292;&#36825;&#28041;&#21450;&#26681;&#25454;&#22810;&#20301;&#19987;&#23478;&#30340;&#21516;&#34892;&#35780;&#23457;&#21465;&#20107;&#29702;&#35299;&#23398;&#26415;&#25163;&#31295;&#30340;&#26680;&#24515;&#36129;&#29486;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#19987;&#23478;&#22810;&#35270;&#35282;&#30340;&#30475;&#27861;&#24635;&#32467;&#20026;&#31616;&#27905;&#30340;&#25972;&#20307;&#27010;&#36848;&#12290;&#37492;&#20110;&#29983;&#25104;&#22411;AI&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#37325;&#22823;&#21457;&#23637;&#65292;&#25105;&#20204;&#26377;&#20805;&#20998;&#30340;&#29702;&#30001;&#28145;&#20837;&#30740;&#31350;LLMs&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#29615;&#22659;&#20013;&#29983;&#25104;&#36825;&#31181;&#20803;&#35780;&#35770;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19977;&#31181;&#27969;&#34892;&#30340;LLM&#65292;&#21363;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#25191;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;TELeR&#20998;&#31867;&#27861;&#20197;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#20419;&#20351;&#23427;&#20204;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20803;&#35780;&#35770;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15589v1 Announce Type: cross  Abstract: One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves understanding the core contributions, strengths, and weaknesses of a scholarly manuscript based on peer-review narratives from multiple experts and then summarizing those multiple experts' perspectives into a concise holistic overview. Given the latest major developments in generative AI, especially Large Language Models (LLMs), it is very compelling to rigorously study the utility of LLMs in generating such meta-reviews in an academic peer-review setting. In this paper, we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to automatically generate meta-reviews by prompting them with different types/levels of prompts based on the recently proposed TELeR taxonomy. Finally, we perform a detailed qualitative study of the meta-reviews generated by the LLMs and summarize our findings and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#21487;&#23398;&#20064;&#26102;&#38388;&#23610;&#24230;&#21442;&#25968;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#39057;&#29575;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#65292;&#24182;&#30740;&#31350;&#20102;&#20004;&#31181;&#23545;&#25239;&#28151;&#21472;&#25928;&#24212;&#30340;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#36895;&#24230;&#24555;33%&#12290;</title><link>https://arxiv.org/abs/2402.15584</link><description>&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
State Space Models for Event Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15584
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#21487;&#23398;&#20064;&#26102;&#38388;&#23610;&#24230;&#21442;&#25968;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#39057;&#29575;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#65292;&#24182;&#30740;&#31350;&#20102;&#20004;&#31181;&#23545;&#25239;&#28151;&#21472;&#25928;&#24212;&#30340;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#36895;&#24230;&#24555;33%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22788;&#29702;&#20107;&#20214;&#30456;&#26426;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39318;&#20808;&#23558;&#19968;&#27573;&#26102;&#38388;&#20869;&#30340;&#20107;&#20214;&#36716;&#25442;&#20026;&#31264;&#23494;&#30340;&#32593;&#26684;&#29366;&#36755;&#20837;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#22312;&#37096;&#32626;&#25512;&#26029;&#39057;&#29575;&#39640;&#20110;&#23427;&#20204;&#35757;&#32451;&#26102;&#30340;&#39057;&#29575;&#65288;&#21363;&#26102;&#38388;&#31383;&#21475;&#36739;&#23567;&#65289;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#21487;&#23398;&#20064;&#26102;&#38388;&#23610;&#24230;&#21442;&#25968;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#36825;&#31181;&#35774;&#35745;&#36866;&#24212;&#19981;&#21516;&#39057;&#29575;&#32780;&#26080;&#38656;&#22312;&#19981;&#21516;&#39057;&#29575;&#19979;&#37325;&#26032;&#35757;&#32451;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#23545;&#25239;&#28151;&#21472;&#25928;&#24212;&#30340;&#31574;&#30053;&#65292;&#24403;&#22312;&#26356;&#39640;&#39057;&#29575;&#19979;&#37096;&#32626;&#27169;&#22411;&#26102;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23545;&#25239;&#22522;&#20110;RNN&#21644;Transformer&#26550;&#26500;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#21253;&#25324;Gen1&#21644;1 Mpx&#20107;&#20214;&#30456;&#26426;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;SSM&#30340;&#27169;&#22411;&#35757;&#32451;&#36895;&#24230;&#24555;33%&#65292;&#21516;&#26102;&#20063;&#34920;&#29616;&#20986;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15584v1 Announce Type: cross  Abstract: Today, state-of-the-art deep neural networks that process event-camera data first convert a temporal window of events into dense, grid-like input representations. As such, they exhibit poor generalizability when deployed at higher inference frequencies (i.e., smaller temporal windows) than the ones they were trained on. We address this challenge by introducing state-space models (SSMs) with learnable timescale parameters to event-based vision. This design adapts to varying frequencies without the need to retrain the network at different frequencies. Additionally, we investigate two strategies to counteract aliasing effects when deploying the model at higher frequencies. We comprehensively evaluate our approach against existing methods based on RNN and Transformer architectures across various benchmarks, including Gen1 and 1 Mpx event camera datasets. Our results demonstrate that SSM-based models train 33% faster and also exhibit minima
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Cohere3D&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#26399;&#36755;&#20837;&#24207;&#21015;&#20013;&#23398;&#20064;&#19968;&#33268;&#23454;&#20363;&#34920;&#24449;&#65292;&#26377;&#21161;&#20110;&#35270;&#35273;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#22810;&#24103;&#23454;&#20363;&#32423;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.15583</link><description>&lt;p&gt;
Cohere3D&#65306;&#21033;&#29992;&#26102;&#38388;&#19968;&#33268;&#24615;&#36827;&#34892;&#35270;&#35273;&#33258;&#21160;&#39550;&#39542;&#26080;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cohere3D: Exploiting Temporal Coherence for Unsupervised Representation Learning of Vision-based Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15583
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Cohere3D&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#26399;&#36755;&#20837;&#24207;&#21015;&#20013;&#23398;&#20064;&#19968;&#33268;&#23454;&#20363;&#34920;&#24449;&#65292;&#26377;&#21161;&#20110;&#35270;&#35273;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#22810;&#24103;&#23454;&#20363;&#32423;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#20687;&#32570;&#20047;&#28145;&#24230;&#32447;&#32034;&#65292;&#23545;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24863;&#30693;&#12289;&#39044;&#27979;&#21644;&#35268;&#21010;&#25104;&#21151;&#65292;&#22810;&#24103;&#36755;&#20837;&#38750;&#24120;&#37325;&#35201;&#12290;&#19981;&#21516;&#35282;&#24230;&#30340;&#35266;&#23519;&#20351;&#24471;&#33021;&#22815;&#20174;&#19981;&#21516;&#36755;&#20837;&#24103;&#20013;&#35782;&#21035;&#30456;&#21516;&#23454;&#20363;&#26469;&#24674;&#22797;3D&#23545;&#35937;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#21160;&#24577;&#24615;&#23548;&#33268;&#30456;&#26426;&#22312;&#19981;&#21516;&#26102;&#38388;&#27493;&#38271;&#25429;&#33719;&#30340;&#27599;&#20010;&#23454;&#20363;&#30340;&#22806;&#35266;&#21644;&#24418;&#29366;&#21457;&#29983;&#26174;&#30528;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;Cohere3D&#65292;&#29992;&#20110;&#23398;&#20064;&#38271;&#26399;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#19968;&#33268;&#23454;&#20363;&#34920;&#24449;&#65292;&#33021;&#22815;&#25269;&#25239;&#36317;&#31163;&#21644;&#35270;&#35282;&#21464;&#21270;&#12290;&#25152;&#23398;&#30340;&#34920;&#24449;&#26377;&#21161;&#20110;&#19979;&#28216;&#20219;&#21153;&#20013;&#36328;&#22810;&#20010;&#36755;&#20837;&#24103;&#30340;&#23454;&#20363;&#32423;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#21033;&#29992;&#26469;&#33258;LiDAR&#20256;&#24863;&#22120;&#30340;&#21407;&#22987;&#28857;&#20113;&#26469;&#26500;&#24314;&#38271;&#26399;&#26102;&#38388;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15583v1 Announce Type: cross  Abstract: Due to the lack of depth cues in images, multi-frame inputs are important for the success of vision-based perception, prediction, and planning in autonomous driving. Observations from different angles enable the recovery of 3D object states from 2D image inputs if we can identify the same instance in different input frames. However, the dynamic nature of autonomous driving scenes leads to significant changes in the appearance and shape of each instance captured by the camera at different time steps. To this end, we propose a novel contrastive learning algorithm, Cohere3D, to learn coherent instance representations in a long-term input sequence robust to the change in distance and perspective. The learned representation aids in instance-level correspondence across multiple input frames in downstream tasks. In the pretraining stage, the raw point clouds from LiDAR sensors are utilized to construct the long-term temporal correspondence fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#22871;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#29992;&#20110;&#22810;&#38899;&#39640;&#20272;&#35745;&#65292;&#35757;&#32451;&#23436;&#20840;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;&#22810;&#38899;&#39640;&#26174;&#33879;&#22270;&#65292;&#26080;&#38656;&#24494;&#35843;</title><link>https://arxiv.org/abs/2402.15569</link><description>&lt;p&gt;
&#23454;&#29616;&#23436;&#20840;&#33258;&#30417;&#30563;&#30340;&#22810;&#38899;&#39640;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Toward Fully Self-Supervised Multi-Pitch Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#22871;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#29992;&#20110;&#22810;&#38899;&#39640;&#20272;&#35745;&#65292;&#35757;&#32451;&#23436;&#20840;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;&#22810;&#38899;&#39640;&#26174;&#33879;&#22270;&#65292;&#26080;&#38656;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38899;&#39640;&#20272;&#35745;&#26159;&#19968;&#20010;&#25345;&#32493;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#28041;&#21450;&#26816;&#27979;&#19982;&#22810;&#20048;&#22120;&#28151;&#38899;&#20013;&#21516;&#26102;&#21457;&#29983;&#30340;&#38899;&#20048;&#20107;&#20214;&#30456;&#20851;&#30340;&#38899;&#39640;&#27963;&#21160;&#12290;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#22312;&#20219;&#21153;&#30340;&#36739;&#31364;&#29305;&#24449;&#25551;&#36848;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#21463;&#21040;&#32570;&#20047;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#24102;&#26377;&#22810;&#38899;&#39640;&#26631;&#27880;&#30340;&#22797;&#38899;&#38899;&#20048;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#29992;&#20110;&#22810;&#38899;&#39640;&#20272;&#35745;&#65292;&#36825;&#20123;&#30446;&#26631;&#40723;&#21169;&#25903;&#25345;&#22260;&#32469;&#35856;&#27874;&#20013;&#24515;&#12289;&#23545;&#38899;&#33394;&#36716;&#25442;&#19981;&#21464;&#20197;&#21450;&#23545;&#20960;&#20309;&#36716;&#25442;&#31561;&#21464;&#25442;&#31561;&#29305;&#24615;&#12290;&#36825;&#20123;&#30446;&#26631;&#36275;&#20197;&#35757;&#32451;&#23436;&#20840;&#22522;&#20110;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#30452;&#25509;&#29983;&#25104;&#22810;&#38899;&#39640;&#26174;&#33879;&#22270;&#65292;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#12290;&#23613;&#31649;&#20165;&#22312;&#19968;&#32452;&#21512;&#25104;&#21333;&#38899;&#39057;&#26679;&#26412;&#19978;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#23436;&#20840;&#33258;&#30417;&#30563;&#26694;&#26550;&#20855;&#26377;&#27867;&#21270;&#21040; po
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15569v1 Announce Type: cross  Abstract: Multi-pitch estimation is a decades-long research problem involving the detection of pitch activity associated with concurrent musical events within multi-instrument mixtures. Supervised learning techniques have demonstrated solid performance on more narrow characterizations of the task, but suffer from limitations concerning the shortage of large-scale and diverse polyphonic music datasets with multi-pitch annotations. We present a suite of self-supervised learning objectives for multi-pitch estimation, which encourage the concentration of support around harmonics, invariance to timbral transformations, and equivariance to geometric transformations. These objectives are sufficient to train an entirely convolutional autoencoder to produce multi-pitch salience-grams directly, without any fine-tuning. Despite training exclusively on a collection of synthetic single-note audio samples, our fully self-supervised framework generalizes to po
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#36890;&#29992;&#25919;&#31574;&#65292;&#20197;&#25429;&#33719;&#22810;&#26679;&#21270;&#12289;&#26368;&#20248;&#12289;&#38271;&#26102;&#22495;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.15567</link><description>&lt;p&gt;
&#20855;&#26377;&#24076;&#23572;&#20271;&#29305;&#34920;&#31034;&#30340;&#22522;&#30784;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Foundation Policies with Hilbert Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#36890;&#29992;&#25919;&#31574;&#65292;&#20197;&#25429;&#33719;&#22810;&#26679;&#21270;&#12289;&#26368;&#20248;&#12289;&#38271;&#26102;&#22495;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15567v1 &#36827;&#34892;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#20363;&#22914;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#65292;&#24050;&#32463;&#20351;&#24471;&#21487;&#20197;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#25214;&#21040;&#19968;&#20010;&#30495;&#27491;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30446;&#26631;&#20197;&#33719;&#21462;&#36890;&#29992;&#25919;&#31574;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#23454;&#29616;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;RL&#65292;&#22522;&#20110;&#35832;&#22914;&#22522;&#20110;&#30446;&#26631;&#30340;RL&#12289;&#34892;&#20026;&#20811;&#38534;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#23398;&#20064;&#31561;&#21407;&#21017;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#21457;&#29616;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#12289;&#38656;&#35201;&#39640;&#36136;&#37327;&#28436;&#31034;&#25968;&#25454;&#25110;&#32570;&#20047;&#26126;&#30830;&#30340;&#25552;&#31034;&#25110;&#36866;&#24212;&#26426;&#21046;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#33021;&#22815;&#25429;&#25417;&#22810;&#26679;&#21270;&#12289;&#26368;&#20248;&#12289;&#38271;&#26102;&#22495;&#34892;&#20026;&#30340;&#36890;&#29992;&#25919;&#31574;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#33719;&#21462;&#36825;&#20123;&#34892;&#20026;&#65292;&#20197;&#20415;&#23427;&#20204;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15567v1 Announce Type: cross  Abstract: Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear prompting or adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly ada
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;AI&#31639;&#27861;&#20174;&#26410;&#35265;&#25968;&#25454;&#28304;&#19978;&#35780;&#20272;&#26102;&#20986;&#29616;&#38169;&#35823;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#30382;&#32932;&#30149;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#25110;&#22270;&#20687;&#25429;&#25417;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#19968;&#27867;&#21270;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.15566</link><description>&lt;p&gt;
&#36890;&#36807;&#35843;&#25972;&#20020;&#24202;&#35774;&#32622;&#20013;&#30382;&#32932;&#30149;&#20998;&#24067;&#24046;&#24322;&#26469;&#32553;&#23567;&#20154;&#24037;&#26234;&#33021;&#27867;&#21270;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Closing the AI generalization gap by adjusting for dermatology condition distribution differences across clinical settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;AI&#31639;&#27861;&#20174;&#26410;&#35265;&#25968;&#25454;&#28304;&#19978;&#35780;&#20272;&#26102;&#20986;&#29616;&#38169;&#35823;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#30382;&#32932;&#30149;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#25110;&#22270;&#20687;&#25429;&#25417;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#19968;&#27867;&#21270;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#22312;&#20174;&#20020;&#24202;&#29031;&#29255;&#20013;&#23545;&#30382;&#32932;&#30149;&#30151;&#36827;&#34892;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23384;&#22312;&#35768;&#22810;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#20851;&#20110;&#36825;&#20123;&#31639;&#27861;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#20102;&#35299;&#29978;&#23569;&#12290;&#29702;&#35299;&#24182;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#23558;&#26377;&#21161;&#20110;&#24320;&#21457;&#20986;&#21487;&#20197;&#22312;&#21508;&#31181;&#20020;&#24202;&#29615;&#22659;&#20013;&#36741;&#21161;&#35786;&#26029;&#30382;&#32932;&#30149;&#30151;&#30340;&#36890;&#29992;&#22411;AI&#12290;&#22312;&#36825;&#39033;&#22238;&#39038;&#24615;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30382;&#32932;&#30149;&#20998;&#24067;&#30340;&#24046;&#24322;&#25165;&#26159;&#23548;&#33268;&#24403;&#19968;&#20010;AI&#31639;&#27861;&#22312;&#26469;&#33258;&#20808;&#21069;&#26410;&#35265;&#25968;&#25454;&#30340;&#28304;&#19978;&#35780;&#20272;&#26102;&#20986;&#29616;&#38169;&#35823;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#32780;&#19981;&#26159;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#25110;&#22270;&#20687;&#25429;&#25417;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#27493;&#39588;&#26469;&#32553;&#23567;&#36825;&#19968;&#27867;&#21270;&#24046;&#36317;&#65292;&#38656;&#35201;&#20851;&#20110;&#26032;&#28304;&#25968;&#25454;&#30340;&#26356;&#22810;&#20449;&#24687;&#65292;&#33539;&#22260;&#20174;&#30142;&#30149;&#30340;&#20998;&#24067;&#21040;&#23500;&#38598;&#25968;&#25454;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15566v1 Announce Type: cross  Abstract: Recently, there has been great progress in the ability of artificial intelligence (AI) algorithms to classify dermatological conditions from clinical photographs. However, little is known about the robustness of these algorithms in real-world settings where several factors can lead to a loss of generalizability. Understanding and overcoming these limitations will permit the development of generalizable AI that can aid in the diagnosis of skin conditions across a variety of clinical settings. In this retrospective study, we demonstrate that differences in skin condition distribution, rather than in demographics or image capture mode are the main source of errors when an AI algorithm is evaluated on data from a previously unseen source. We demonstrate a series of steps to close this generalization gap, requiring progressively more information about the new source, ranging from the condition distribution to training data enriched for data
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20803;&#33258;&#36866;&#24212;&#22238;&#24402;&#26679;&#26465;&#30340;&#20844;&#24179;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#34701;&#20837;&#20102;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.15561</link><description>&lt;p&gt;
&#20445;&#35777;&#20844;&#24179;&#21644;&#36879;&#26126;&#24615;&#30340;&#20844;&#24179;&#22810;&#20803;&#33258;&#36866;&#24212;&#22238;&#24402;&#26679;&#26465;
&lt;/p&gt;
&lt;p&gt;
Fair Multivariate Adaptive Regression Splines for Ensuring Equity and Transparency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20803;&#33258;&#36866;&#24212;&#22238;&#24402;&#26679;&#26465;&#30340;&#20844;&#24179;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#34701;&#20837;&#20102;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#25945;&#32946;&#65292;&#20197;&#25351;&#23548;&#20915;&#31574;&#24182;&#25913;&#21892;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#39044;&#27979;&#27169;&#22411;&#26159;&#19987;&#26377;&#30340;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#26080;&#27861;&#35780;&#20272;&#25110;&#20462;&#25913;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#38382;&#36131;&#21046;&#21644;&#36947;&#24503;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#23545;&#20351;&#29992;&#23427;&#20204;&#30340;&#23448;&#21592;&#32780;&#35328;&#26159;&#19981;&#36879;&#26126;&#21644;&#38590;&#20197;&#29702;&#35299;&#30340;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#20449;&#20219;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#27169;&#22411;&#21487;&#33021;&#24341;&#20837;&#25110;&#21152;&#21095;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#65292;&#23601;&#20687;&#22312;&#31038;&#20250;&#30340;&#35768;&#22810;&#39046;&#22495;&#19968;&#26679;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#25552;&#20986;&#19968;&#31181;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#20844;&#24179;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#36731;&#26494;&#34987;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#37319;&#32435;&#21644;&#35843;&#25972;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20803;&#33258;&#36866;&#24212;&#22238;&#24402;&#26679;&#26465;&#65288;MARS&#65289;&#30340;&#20844;&#24179;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#34701;&#20837;&#20102;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15561v1 Announce Type: new  Abstract: Predictive analytics is widely used in various domains, including education, to inform decision-making and improve outcomes. However, many predictive models are proprietary and inaccessible for evaluation or modification by researchers and practitioners, limiting their accountability and ethical design. Moreover, predictive models are often opaque and incomprehensible to the officials who use them, reducing their trust and utility. Furthermore, predictive models may introduce or exacerbate bias and inequity, as they have done in many sectors of society. Therefore, there is a need for transparent, interpretable, and fair predictive models that can be easily adopted and adapted by different stakeholders. In this paper, we propose a fair predictive model based on multivariate adaptive regression splines(MARS) that incorporates fairness measures in the learning process. MARS is a non-parametric regression model that performs feature selectio
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#24310;&#36831;&#27867;&#21270;&#21644;&#24310;&#36831;&#40065;&#26834;&#24615;&#29616;&#35937;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#22522;&#20110;&#26032;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#24230;&#37327;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.15555</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24635;&#26159;&#29702;&#35299;&#24182;&#19988;&#36825;&#23601;&#26159;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Deep Networks Always Grok and Here is Why
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15555
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#24310;&#36831;&#27867;&#21270;&#21644;&#24310;&#36831;&#40065;&#26834;&#24615;&#29616;&#35937;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#22522;&#20110;&#26032;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#24230;&#37327;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#65292;&#25110;&#32773;&#24310;&#36831;&#27867;&#21270;&#65292;&#26159;&#25351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#36798;&#21040;&#25509;&#36817;&#20110;&#38646;&#30340;&#35757;&#32451;&#35823;&#24046;&#21518;&#24456;&#38271;&#26102;&#38388;&#20869;&#25165;&#21457;&#29983;&#27867;&#21270;&#30340;&#29616;&#35937;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25253;&#21578;&#20102;&#22312;&#29305;&#23450;&#21463;&#25511;&#29615;&#22659;&#20013;&#20986;&#29616;grokking&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#20351;&#29992;&#22823;&#33539;&#25968;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;DNN&#25110;&#32773;&#22312;&#31639;&#27861;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;transformers&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;grokking&#23454;&#38469;&#19978;&#26356;&#21152;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#24191;&#27867;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#21576;&#29616;&#65292;&#20363;&#22914;&#22312;CIFAR10&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25110;&#32773;&#22312;Imagenette&#19978;&#35757;&#32451;&#30340;Resnet&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24310;&#36831;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;&#65292;&#21363;DNN&#22312;&#25554;&#20540;&#21644;/&#25110;&#27867;&#21270;&#20043;&#21518;&#23545;&#25239;&#31034;&#20363;&#36827;&#34892;&#29702;&#35299;&#24182;&#21464;&#24471;&#40065;&#26834;&#12290;&#25105;&#20204;&#38024;&#23545;DNN&#30340;&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#25552;&#20986;&#20102;&#20986;&#29616;&#24310;&#36831;&#27867;&#21270;&#21644;&#24310;&#36831;&#40065;&#26834;&#24615;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#34913;&#37327;&#20102;DNN&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#30340;&#22797;&#26434;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15555v1 Announce Type: cross  Abstract: Grokking, or delayed generalization, is a phenomenon where generalization in a deep neural network (DNN) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in specific controlled settings, such as DNNs initialized with large-norm parameters or transformers trained on algorithmic datasets. We demonstrate that grokking is actually much more widespread and materializes in a wide range of practical settings, such as training of a convolutional neural network (CNN) on CIFAR10 or a Resnet on Imagenette. We introduce the new concept of delayed robustness, whereby a DNN groks adversarial examples and becomes robust, long after interpolation and/or generalization. We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on a new measure of the local complexity of a DNN's input-output mapping. Our local complexity measures the d
&lt;/p&gt;</description></item><item><title>HiMAP &#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#20280;&#32553;&#26041;&#27861;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#24341;&#23548;&#30340;&#27169;&#20223;&#23398;&#20064;&#22312;&#20998;&#25955;&#24335;&#35757;&#32451;&#20013;&#23545;&#29992;&#25143;&#26234;&#20307;&#36335;&#24452;&#35268;&#21010;&#36827;&#34892;&#20102;&#25913;&#36827;</title><link>https://arxiv.org/abs/2402.15546</link><description>&lt;p&gt;
HiMAP&#65306;&#29992;&#20110;&#22823;&#35268;&#27169;&#22810;&#26234;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#23398;&#20064;&#21551;&#21457;&#24335;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent Pathfinding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15546
&lt;/p&gt;
&lt;p&gt;
HiMAP &#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#20280;&#32553;&#26041;&#27861;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#24341;&#23548;&#30340;&#27169;&#20223;&#23398;&#20064;&#22312;&#20998;&#25955;&#24335;&#35757;&#32451;&#20013;&#23545;&#29992;&#25143;&#26234;&#20307;&#36335;&#24452;&#35268;&#21010;&#36827;&#34892;&#20102;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#26234;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#22312;&#22810;&#20010;&#39046;&#22495;&#37117;&#23384;&#22312;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#38543;&#30528;&#22823;&#37327;&#33258;&#27835;&#20307;&#21516;&#26102;&#25805;&#20316;&#32780;&#22686;&#38271;&#65292;&#39640;&#25928;&#21644;&#36991;&#20813;&#30896;&#25758;&#30340;&#21327;&#35843;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#31639;&#27861;&#22312;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#26469;&#35299;&#20915;MAPF&#30340;&#22797;&#26434;&#24615;&#65307;&#28982;&#32780;&#65292;&#23427;&#20063;&#34987;&#21457;&#29616;&#22312;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#31934;&#32454;&#30340;&#23454;&#26045;&#12289;&#28459;&#38271;&#30340;&#35757;&#32451;&#65292;&#32780;&#19988;&#36890;&#24120;&#34920;&#29616;&#20986;&#19981;&#31283;&#23450;&#30340;&#25910;&#25947;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Heuristics-Informed Multi-Agent Pathfinding&#65288;HiMAP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#20280;&#32553;&#26041;&#27861;&#65292;&#37319;&#29992;&#20197;&#21551;&#21457;&#24335;&#24341;&#23548;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#24335;&#26469;&#36827;&#34892;&#20998;&#25955;&#24335;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#23567;&#35268;&#27169;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#19968;&#20010;&#21551;&#21457;&#24335;&#31574;&#30053;&#20316;&#20026;&#25945;&#24072;&#65292;&#23558;&#27599;&#20010;&#21333;&#20010;&#26234;&#20307;&#35266;&#27979;&#20449;&#24687;&#26144;&#23556;&#21040;&#34892;&#21160;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15546v1 Announce Type: cross  Abstract: Large-scale multi-agent pathfinding (MAPF) presents significant challenges in several areas. As systems grow in complexity with a multitude of autonomous agents operating simultaneously, efficient and collision-free coordination becomes paramount. Traditional algorithms often fall short in scalability, especially in intricate scenarios. Reinforcement Learning (RL) has shown potential to address the intricacies of MAPF; however, it has also been shown to struggle with scalability, demanding intricate implementation, lengthy training, and often exhibiting unstable convergence, limiting its practical application. In this paper, we introduce Heuristics-Informed Multi-Agent Pathfinding (HiMAP), a novel scalable approach that employs imitation learning with heuristic guidance in a decentralized manner. We train on small-scale instances using a heuristic policy as a teacher that maps each single agent observation information to an action prob
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#38598;&#25104;&#38754;&#20020;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#36793;&#32536;&#35745;&#31639;&#25506;&#32034;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15542</link><description>&lt;p&gt;
&#27969;&#24335;&#29289;&#32852;&#32593;&#25968;&#25454;&#19982;&#37327;&#23376;&#36793;&#32536;&#65306;&#19968;&#20010;&#32463;&#20856;/&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Streaming IoT Data and the Quantum Edge: A Classic/Quantum Machine Learning Use Case
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15542
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#38598;&#25104;&#38754;&#20020;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#36793;&#32536;&#35745;&#31639;&#25506;&#32034;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21518;&#25705;&#23572;&#26102;&#20195;&#30340;&#26469;&#20020;&#65292;&#31185;&#23398;&#30028;&#38754;&#20020;&#30528;&#35299;&#20915;&#24403;&#21069;&#25968;&#25454;&#23494;&#38598;&#22411;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#38656;&#27714;&#25361;&#25112;&#65292;&#36825;&#20123;&#24212;&#29992;&#26159;&#20998;&#24067;&#24335;&#35745;&#31639;&#20013;&#32039;&#24613;&#25968;&#25454;&#20998;&#26512;&#30340;&#22522;&#30707;&#12290;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#26159;&#28385;&#36275;&#32039;&#24613;&#25968;&#25454;&#20998;&#26512;&#38656;&#27714;&#22686;&#21152;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#28508;&#22312;&#30340;&#29702;&#35770;&#21152;&#36895;&#21644;&#22686;&#21152;&#30340;&#31354;&#38388;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;(1)&#20174;&#32463;&#20856;&#21040;&#37327;&#23376;&#39046;&#22495;&#30340;&#25968;&#25454;&#32534;&#30721;&#12289;(2)&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#20197;&#21450;(3)&#37327;&#23376;&#30828;&#20214;&#38598;&#25104;&#21040;&#20998;&#24067;&#24335;&#35745;&#31639;&#20013;&#30340;&#36830;&#32493;&#20307;&#31561;&#25361;&#25112;&#38480;&#21046;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#32039;&#24613;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#37319;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36793;&#32536;&#35745;&#31639;&#29992;&#20110;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#21040;&#20998;&#24067;&#24335;&#35745;&#31639;&#36830;&#32493;&#20307;&#20013;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#20027;&#35201;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#25968;&#25454;&#32534;&#30721;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15542v1 Announce Type: cross  Abstract: With the advent of the Post-Moore era, the scientific community is faced with the challenge of addressing the demands of current data-intensive machine learning applications, which are the cornerstone of urgent analytics in distributed computing. Quantum machine learning could be a solution for the increasing demand of urgent analytics, providing potential theoretical speedups and increased space efficiency. However, challenges such as (1) the encoding of data from the classical to the quantum domain, (2) hyperparameter tuning, and (3) the integration of quantum hardware into a distributed computing continuum limit the adoption of quantum machine learning for urgent analytics. In this work, we investigate the use of Edge computing for the integration of quantum machine learning into a distributed computing continuum, identifying the main challenges and possible solutions. Furthermore, exploring the data encoding and hyperparameter tuni
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15537</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Performance of ChatGPT for Spam Email Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#37038;&#20214;&#32487;&#32493;&#26159;&#19987;&#19994;&#21644;&#21830;&#19994;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#36890;&#20449;&#23186;&#20171;&#12290;&#28982;&#32780;&#65292;&#22403;&#22334;&#37038;&#20214;&#30340;&#26222;&#21450;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#25200;&#20081;&#20102;&#20182;&#20204;&#30340;&#26085;&#24120;&#24037;&#20316;&#24182;&#38477;&#20302;&#20102;&#29983;&#20135;&#29575;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20869;&#23481;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#36807;&#28388;&#22403;&#22334;&#37038;&#20214;&#23545;&#32593;&#32476;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#65292;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#35780;&#20272;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;ChatGPT&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#38656;&#35201;&#25552;&#31034;&#35828;&#26126;&#21644;&#23569;&#37327;&#31034;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15537v1 Announce Type: cross  Abstract: Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction and a few demonstrations. We also investigate how the t
&lt;/p&gt;</description></item><item><title>DiCoM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#20803;&#27010;&#24565;&#65292;&#26377;&#25928;&#34920;&#31034;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#65292;&#20197;&#24212;&#23545;&#21307;&#23398;&#25104;&#20687;&#39044;&#35757;&#32451;&#20013;&#19982;&#33258;&#28982;&#22270;&#20687;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15534</link><description>&lt;p&gt;
DiCoM -- &#22810;&#20803;&#27010;&#24565;&#24314;&#27169;&#20197;&#22686;&#24378;&#33016;&#37096;X&#23556;&#32447;&#30740;&#31350;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
DiCoM -- Diverse Concept Modeling towards Enhancing Generalizability in Chest X-Ray Studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15534
&lt;/p&gt;
&lt;p&gt;
DiCoM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#20803;&#27010;&#24565;&#65292;&#26377;&#25928;&#34920;&#31034;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#65292;&#20197;&#24212;&#23545;&#21307;&#23398;&#25104;&#20687;&#39044;&#35757;&#32451;&#20013;&#19982;&#33258;&#28982;&#22270;&#20687;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#20020;&#24202;&#25104;&#20687;&#27169;&#24577;&#65292;&#22312;&#21508;&#31181;&#32954;&#37096;&#21644;&#24515;&#33039;&#30456;&#20851;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;&#20381;&#36182;&#25918;&#23556;&#23398;&#35835;&#29255;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#20020;&#24202;&#35786;&#26029;&#24037;&#20855;&#35774;&#35745;&#31574;&#30053;&#38656;&#35201;&#39640;&#36136;&#37327;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#32988;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20195;&#34920;&#20102;&#35813;&#39046;&#22495;&#30340;&#37325;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#25104;&#20687;&#39044;&#35757;&#32451;&#19982;&#33258;&#28982;&#22270;&#20687;&#65288;&#20363;&#22914;ImageNet&#65289;&#30340;&#39044;&#35757;&#32451;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19981;&#21516;&#65292;&#22240;&#20026;&#20020;&#24202;&#22270;&#20687;&#20855;&#26377;&#29420;&#29305;&#23646;&#24615;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#20803;&#27010;&#24565;&#24314;&#27169;&#65288;DiCoM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#20102;&#23398;&#29983;&#25945;&#24072;&#26694;&#26550;&#26469;&#23398;&#20064;&#22810;&#20803;&#27010;&#24565;&#65292;&#20174;&#32780;&#26377;&#25928;&#34920;&#31034;CXR&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15534v1 Announce Type: cross  Abstract: Chest X-Ray (CXR) is a widely used clinical imaging modality and has a pivotal role in the diagnosis and prognosis of various lung and heart related conditions. Conventional automated clinical diagnostic tool design strategies relying on radiology reads and supervised learning, entail the cumbersome requirement of high quality annotated training data. To address this challenge, self-supervised pre-training has proven to outperform supervised pre-training in numerous downstream vision tasks, representing a significant breakthrough in the field. However, medical imaging pre-training significantly differs from pre-training with natural images (e.g., ImageNet) due to unique attributes of clinical images. In this context, we introduce Diverse Concept Modeling (DiCoM), a novel self-supervised training paradigm that leverages a student teacher framework for learning diverse concepts and hence effective representation of the CXR data. Hence, e
&lt;/p&gt;</description></item><item><title>Chain-of-Specificity (CoS)&#26159;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#36880;&#27493;&#31934;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36755;&#20837;&#25351;&#20196;&#20013;&#36845;&#20195;&#24378;&#35843;&#29305;&#23450;&#32422;&#26463;&#65292;&#35299;&#38145;&#30693;&#35782;&#24182;&#25913;&#36827;&#22238;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.15526</link><description>&lt;p&gt;
Chain-of-Specificity: &#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#36880;&#27493;&#31934;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Specificity: An Iteratively Refining Method for Eliciting Knowledge from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15526
&lt;/p&gt;
&lt;p&gt;
Chain-of-Specificity (CoS)&#26159;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#36880;&#27493;&#31934;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36755;&#20837;&#25351;&#20196;&#20013;&#36845;&#20195;&#24378;&#35843;&#29305;&#23450;&#32422;&#26463;&#65292;&#35299;&#38145;&#30693;&#35782;&#24182;&#25913;&#36827;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#24341;&#20154;&#30633;&#30446;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#33021;&#22815;&#29983;&#25104;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#26377;&#26102;&#38590;&#20197;&#36981;&#24490;&#20855;&#20307;&#30340;&#32422;&#26463;(&#22914;&#22312;&#29305;&#23450;&#22320;&#28857;&#25110;&#29305;&#23450;&#26102;&#38388;)&#65292;&#29978;&#33267;&#26377;&#26102;&#20250;&#24573;&#30053;&#36825;&#20123;&#32422;&#26463;&#65292;&#23548;&#33268;&#22238;&#24212;&#35201;&#20040;&#22826;&#31548;&#32479;&#65292;&#35201;&#20040;&#19981;&#22815;&#28385;&#24847;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Chain-of-Specificity (CoS)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CoS&#36845;&#20195;&#22320;&#24378;&#35843;&#36755;&#20837;&#25351;&#20196;&#20013;&#30340;&#20855;&#20307;&#32422;&#26463;&#65292;&#35299;&#38145;LLMs&#20869;&#37096;&#30340;&#30693;&#35782;&#65292;&#24182;&#31934;&#21270;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15526v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit remarkable generative capabilities, enabling the generation of valuable information. Despite these advancements, previous research found that LLMs sometimes struggle with adhering to specific constraints (e.g., in specific place or at specific time), at times even overlooking them, which leads to responses that are either too generic or not fully satisfactory. Existing approaches attempted to address this issue by decomposing or rewriting input instructions, yet they fall short in adequately emphasizing specific constraints and in unlocking the underlying knowledge (e.g., programming within the context of software development). In response, this paper proposes a simple yet effective method named Chain-of-Specificity (CoS). Specifically, CoS iteratively emphasizes the specific constraints in the input instructions, unlocks knowledge within LLMs, and refines responses. Experiments conducted on publicly 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20462;&#21098;&#20844;&#24335;&#30340;&#37096;&#20998;&#65292;&#21152;&#36895;&#26522;&#20030;&#26368;&#23567;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;&#65292;&#26080;&#38656;&#25968;&#25454;&#26631;&#35760;&#65292;&#20063;&#26080;&#38656;&#26469;&#33258;&#30446;&#26631;&#24212;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15524</link><description>&lt;p&gt;
&#22270;&#21098;&#26525;&#29992;&#20110;&#26522;&#20030;&#26368;&#23567;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;
&lt;/p&gt;
&lt;p&gt;
Graph Pruning for Enumeration of Minimal Unsatisfiable Subsets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15524
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20462;&#21098;&#20844;&#24335;&#30340;&#37096;&#20998;&#65292;&#21152;&#36895;&#26522;&#20030;&#26368;&#23567;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;&#65292;&#26080;&#38656;&#25968;&#25454;&#26631;&#35760;&#65292;&#20063;&#26080;&#38656;&#26469;&#33258;&#30446;&#26631;&#24212;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#20108;&#36827;&#21046;&#32422;&#26463;&#30340;&#26368;&#23567;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;&#65288;MUSes&#65289;&#26159;&#36229;&#32422;&#26463;&#31995;&#32479;&#19981;&#21487;&#34892;&#24615;&#20998;&#26512;&#20013;&#30340;&#24120;&#35265;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38382;&#39064;&#30340;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#65292;&#26522;&#20030;MUSes&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#23545;&#20844;&#24335;&#36827;&#34892;&#20462;&#21098;&#20197;&#21152;&#36895;MUS&#26522;&#20030;&#12290;&#25105;&#20204;&#23558;&#20844;&#24335;&#34920;&#31034;&#20026;&#22270;&#65292;&#28982;&#21518;&#24320;&#21457;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#24212;&#35813;&#20462;&#21098;&#20844;&#24335;&#30340;&#21738;&#19968;&#37096;&#20998;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#36890;&#36807;&#20165;&#26816;&#26597;&#20462;&#21098;&#21518;&#30340;&#20844;&#24335;&#30340;&#21487;&#28385;&#36275;&#24615;&#26469;&#36827;&#34892;&#25968;&#25454;&#26631;&#35760;&#12290;&#23427;&#29978;&#33267;&#19981;&#38656;&#35201;&#26469;&#33258;&#30446;&#26631;&#24212;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#20026;&#23427;&#23545;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#22806;&#25512;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#29616;&#26377;&#30340;MUS&#26522;&#20030;&#22120;&#32467;&#21512;&#65292;&#24182;&#39564;&#35777;&#20854;&#22312;&#21253;&#25324;&#19968;&#32452;&#36229;&#20986;&#25105;&#20204;&#35757;&#32451;&#20998;&#24067;&#33539;&#22260;&#30340;&#23454;&#38469;&#38382;&#39064;&#22312;&#20869;&#30340;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15524v1 Announce Type: new  Abstract: Finding Minimal Unsatisfiable Subsets (MUSes) of binary constraints is a common problem in infeasibility analysis of over-constrained systems. However, because of the exponential search space of the problem, enumerating MUSes is extremely time-consuming in real applications. In this work, we propose to prune formulas using a learned model to speed up MUS enumeration. We represent formulas as graphs and then develop a graph-based learning model to predict which part of the formula should be pruned. Importantly, our algorithm does not require data labeling by only checking the satisfiability of pruned formulas. It does not even require training data from the target application because it extrapolates to data with different distributions. In our experiments we combine our algorithm with existing MUS enumerators and validate its effectiveness in multiple benchmarks including a set of real-world problems outside our training distribution. The
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HKD-SHO&#30340;&#28151;&#21512;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#65292;&#23558;&#22522;&#20110;&#30693;&#35782;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#26377;&#30410;&#22320;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#35299;&#20915;&#20102;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#20013;&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15521</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#28151;&#21512;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479; - HKD-SHO
&lt;/p&gt;
&lt;p&gt;
HKD-SHO: A hybrid smart home system based on knowledge-based and data-driven services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15521
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HKD-SHO&#30340;&#28151;&#21512;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#65292;&#23558;&#22522;&#20110;&#30693;&#35782;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#26377;&#30410;&#22320;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#35299;&#20915;&#20102;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#20013;&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26234;&#33021;&#23478;&#23621;&#36890;&#36807;&#35774;&#32622;&#21508;&#31181;&#26381;&#21153;&#26469;&#23454;&#29616;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#21019;&#24314;&#26234;&#33021;&#23478;&#23621;&#26381;&#21153;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#20026;&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23621;&#27665;&#30340;&#25163;&#21160;&#36755;&#20837;&#65292;&#22914;&#26524;&#25152;&#20851;&#27880;&#29615;&#22659;&#29366;&#24577;&#30340;&#29289;&#29702;&#29616;&#35937;&#22797;&#26434;&#65292;&#32780;&#19988;&#23621;&#27665;&#19981;&#30693;&#36947;&#22914;&#20309;&#35843;&#25972;&#30456;&#20851;&#25191;&#34892;&#22120;&#20197;&#23454;&#29616;&#26381;&#21153;&#30417;&#35270;&#30340;&#29366;&#24577;&#30340;&#30446;&#26631;&#20540;&#65292;&#21017;&#21487;&#33021;&#20250;&#21464;&#24471;&#22797;&#26434;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23601;&#20687;&#40657;&#21283;&#23376;&#19968;&#26679;&#65292;&#26080;&#27861;&#21521;&#23621;&#27665;&#23637;&#31034;&#22312;&#21738;&#20123;&#24773;&#20917;&#19979;&#26576;&#20123;&#26381;&#21153;&#25552;&#20986;&#20102;&#26576;&#20123;&#25191;&#34892;&#22120;&#29366;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HKD-SHO&#65288;&#22522;&#20110;&#28151;&#21512;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#26234;&#33021;&#23478;&#23621;&#31995;&#32479;&#65289;&#30340;&#28151;&#21512;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15521v1 Announce Type: new  Abstract: A smart home is realized by setting up various services. Several methods have been proposed to create smart home services, which can be divided into knowledge-based and data-driven approaches. However, knowledge-based approaches usually require manual input from the inhabitant, which can be complicated if the physical phenomena of the concerned environment states are complex, and the inhabitant does not know how to adjust related actuators to achieve the target values of the states monitored by services. Moreover, machine learning-based data-driven approaches that we are interested in are like black boxes and cannot show the inhabitant in which situations certain services proposed certain actuators' states. To solve these problems, we propose a hybrid system called HKD-SHO (Hybrid Knowledge-based and Data-driven services based Smart HOme system), where knowledge-based and machine learning-based data-driven services are profitably integra
&lt;/p&gt;</description></item><item><title>GLA-Grad&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#26696;&#65292;&#21033;&#29992;&#26684;&#37324;&#33452;-&#26519;&#31639;&#27861;&#65288;GLA&#65289;&#22312;&#27874;&#24418;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#27599;&#19968;&#27493;&#20013;&#26469;&#26368;&#23567;&#21270;&#26465;&#20214;&#38169;&#35823;&#65292;&#25552;&#39640;&#22122;&#22768;&#25193;&#25955;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15516</link><description>&lt;p&gt;
GLA-Grad&#65306;&#19968;&#31181;&#26684;&#37324;&#33452;-&#26519;&#25193;&#23637;&#27874;&#24418;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GLA-Grad: A Griffin-Lim Extended Waveform Generation Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15516
&lt;/p&gt;
&lt;p&gt;
GLA-Grad&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#26696;&#65292;&#21033;&#29992;&#26684;&#37324;&#33452;-&#26519;&#31639;&#27861;&#65288;GLA&#65289;&#22312;&#27874;&#24418;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#27599;&#19968;&#27493;&#20013;&#26469;&#26368;&#23567;&#21270;&#26465;&#20214;&#38169;&#35823;&#65292;&#25552;&#39640;&#22122;&#22768;&#25193;&#25955;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#21040;&#20154;&#20204;&#20851;&#27880;&#65292;&#29992;&#20110;&#21508;&#31181;&#20449;&#21495;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#35821;&#38899;&#25110;&#38899;&#20048;&#21512;&#25104;&#12290;WaveGrad&#26159;&#19968;&#20010;&#25104;&#21151;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#26377;&#26465;&#20214;&#22320;&#20351;&#29992;&#26757;&#23572;&#22768;&#35889;&#26469;&#25351;&#23548;&#25193;&#25955;&#36807;&#31243;&#65292;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#38899;&#39057;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#26696;GLA-Grad&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#26465;&#20214;&#38169;&#35823;&#65292;&#25552;&#39640;&#22122;&#22768;&#25193;&#25955;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#35813;&#26041;&#26696;&#22312;&#27599;&#19968;&#27493;&#26222;&#36890;&#25193;&#25955;&#36807;&#31243;&#20013;&#24341;&#20837;&#26684;&#37324;&#33452;-&#26519;&#31639;&#27861;&#65288;GLA&#65289;&#31561;&#30456;&#20301;&#24674;&#22797;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15516v1 Announce Type: cross  Abstract: Diffusion models are receiving a growing interest for a variety of signal generation tasks such as speech or music synthesis. WaveGrad, for example, is a successful diffusion model that conditionally uses the mel spectrogram to guide a diffusion process for the generation of high-fidelity audio. However, such models face important challenges concerning the noise diffusion process for training and inference, and they have difficulty generating high-quality speech for speakers that were not seen during training. With the aim of minimizing the conditioning error and increasing the efficiency of the noise diffusion process, we propose in this paper a new scheme called GLA-Grad, which consists in introducing a phase recovery algorithm such as the Griffin-Lim algorithm (GLA) at each step of the regular diffusion process. Furthermore, it can be directly applied to an already-trained waveform generation model, without additional training or fi
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#28966;&#34385;&#21644;&#21387;&#21147;&#30456;&#20851;&#30340;&#29983;&#29702;&#29305;&#24449;&#22312;&#20854;&#20182;&#39640;&#28608;&#27963;&#24773;&#32490;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15513</link><description>&lt;p&gt;
&#30740;&#31350;&#28966;&#34385;&#30340;&#29983;&#29702;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Investigating the Generalizability of Physiological Characteristics of Anxiety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15513
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#28966;&#34385;&#21644;&#21387;&#21147;&#30456;&#20851;&#30340;&#29983;&#29702;&#29305;&#24449;&#22312;&#20854;&#20182;&#39640;&#28608;&#27963;&#24773;&#32490;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#29983;&#29702;&#20449;&#21495;&#26816;&#27979;&#28966;&#34385;&#21644;&#21387;&#21147;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;ML&#27169;&#22411;&#26159;&#21542;&#27491;&#22312;&#23398;&#20064;&#19982;&#21387;&#21147;&#29305;&#23450;&#30456;&#20851;&#30340;&#29983;&#29702;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#27495;&#20041;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24050;&#34987;&#35777;&#26126;&#19982;&#28966;&#34385;&#21644;&#21387;&#21147;&#30456;&#20851;&#30340;&#29983;&#29702;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#36866;&#24212;&#39640;&#28608;&#27963;&#24773;&#32490;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#20174;&#20197;&#19979;&#19977;&#20010;&#25968;&#25454;&#38598;&#25552;&#21462;&#30340;&#29305;&#24449;&#65306;&#28966;&#34385;&#38454;&#27573;&#25968;&#25454;&#38598;&#65288;APD&#65289;&#65292;&#21487;&#31359;&#25140;&#21387;&#21147;&#21644;&#24773;&#24863;&#26816;&#27979;&#65288;WESAD&#65289;&#65292;&#20197;&#21450;&#24773;&#32490;&#36830;&#32493;&#26631;&#27880;&#20449;&#21495;&#65288;CASE&#65289;&#25968;&#25454;&#38598;&#30340;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#21644;&#30382;&#30005;&#65288;EDA&#65289;&#20449;&#21495;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#32479;&#35745;&#22238;&#24402;&#20998;&#26512;&#65292;&#20197;&#21450;&#22312;&#35821;&#26009;&#24211;&#20869;&#37096;&#12289;&#36328;&#35821;&#26009;&#24211;&#21644;&#30041;&#19968;&#35821;&#26009;&#24211;&#22806;&#30340;&#20132;&#21449;&#39564;&#35777;&#65292;&#20102;&#35299;&#36825;&#20123;&#29305;&#24449;&#26159;&#29305;&#23450;&#20110;&#28966;&#34385;&#36824;&#26159;&#26222;&#36941;&#20110;&#20854;&#20182;&#39640;&#28608;&#27963;&#24773;&#32490;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15513v1 Announce Type: cross  Abstract: Recent works have demonstrated the effectiveness of machine learning (ML) techniques in detecting anxiety and stress using physiological signals, but it is unclear whether ML models are learning physiological features specific to stress. To address this ambiguity, we evaluated the generalizability of physiological features that have been shown to be correlated with anxiety and stress to high-arousal emotions. Specifically, we examine features extracted from electrocardiogram (ECG) and electrodermal (EDA) signals from the following three datasets: Anxiety Phases Dataset (APD), Wearable Stress and Affect Detection (WESAD), and the Continuously Annotated Signals of Emotion (CASE) dataset. We aim to understand whether these features are specific to anxiety or general to other high-arousal emotions through a statistical regression analysis, in addition to a within-corpus, cross-corpus, and leave-one-corpus-out cross-validation across instan
&lt;/p&gt;</description></item><item><title>AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15506</link><description>&lt;p&gt;
AgentOhana&#65306;&#20026;&#26377;&#25928;&#26234;&#33021;&#20307;&#23398;&#20064;&#35774;&#35745;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15506
&lt;/p&gt;
&lt;p&gt;
AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24341;&#36215;&#20102;&#37325;&#22823;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#36827;&#34892;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20855;&#26377;&#22810;&#36718;&#36712;&#36857;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#30340;&#24322;&#26500;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;AgentOhana&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;AgentOhana&#20174;&#19981;&#21516;&#29615;&#22659;&#20013;&#32858;&#21512;&#26234;&#33021;&#20307;&#36712;&#36857;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24773;&#26223;&#12290;&#23427;&#31934;&#24515;&#22320;&#23558;&#36825;&#20123;&#36712;&#36857;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#26684;&#24335;&#20013;&#65292;&#31616;&#21270;&#20102;&#20026;&#26234;&#33021;&#20307;&#35757;&#32451;&#20248;&#21270;&#30340;&#36890;&#29992;&#25968;&#25454;&#21152;&#36733;&#22120;&#30340;&#21019;&#24314;&#12290;&#36890;&#36807;&#25968;&#25454;&#32479;&#19968;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#27700;&#32447;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#35774;&#22791;&#20043;&#38388;&#30340;&#29420;&#31435;&#38543;&#26426;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;xLAM-v0.1&#65292;&#19968;&#20010;&#22823;&#21160;&#20316;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#24110;&#21161;&#24357;&#34917;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.15313</link><description>&lt;p&gt;
ArabianGPT&#65306;&#22522;&#20110;&#21407;&#29983;&#38463;&#25289;&#20271;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ArabianGPT: Native Arabic GPT-based Large Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#24110;&#21161;&#24357;&#34917;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#21644;&#25289;&#19969;&#35821;&#20026;&#20027;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20027;&#23548;&#22320;&#20301;&#23548;&#33268;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#26174;&#33879;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#32780;&#25104;&#12290;&#36825;&#20123;&#27169;&#22411;&#21253;&#25324;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#65292;&#19982;&#38463;&#25289;&#20271;&#35821;&#30340;&#24494;&#22937;&#35821;&#35328;&#29305;&#24449;&#30456;&#22865;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15313v1 Announce Type: cross  Abstract: The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#38544;&#24335;&#25195;&#25551;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#36830;&#32493;&#34920;&#31034;&#20219;&#24847;&#36816;&#21160;&#65292;&#24182;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#36895;&#24230;&#21644;&#20960;&#20309;&#30896;&#25758;&#26816;&#26597;&#30340;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.15281</link><description>&lt;p&gt;
&#31070;&#32463;&#38544;&#24335;&#25195;&#25551;&#20307;&#27169;&#22411;&#29992;&#20110;&#24555;&#36895;&#30896;&#25758;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Swept Volume Models for Fast Collision Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15281
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#38544;&#24335;&#25195;&#25551;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#36830;&#32493;&#34920;&#31034;&#20219;&#24847;&#36816;&#21160;&#65292;&#24182;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#36895;&#24230;&#21644;&#20960;&#20309;&#30896;&#25758;&#26816;&#26597;&#30340;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30896;&#25758;&#26816;&#27979;&#26159;&#36816;&#21160;&#35268;&#21010;&#20013;&#26368;&#32791;&#26102;&#30340;&#25805;&#20316;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#25506;&#32034;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21152;&#36895;&#30896;&#25758;&#26816;&#27979;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#20391;&#37325;&#20110;&#21033;&#29992;&#26426;&#22120;&#20154;&#20960;&#20309;&#20307;&#25110;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#25195;&#25551;&#20307;&#30340;&#31070;&#32463;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#38544;&#24335;&#25195;&#25551;&#20307;&#27169;&#22411;&#65292;&#39318;&#27425;&#36830;&#32493;&#34920;&#31034;&#30001;&#36215;&#22987;&#21644;&#30446;&#26631;&#37197;&#32622;&#21442;&#25968;&#21270;&#30340;&#20219;&#24847;&#36816;&#21160;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24555;&#36895;&#35745;&#31639;&#20219;&#21153;&#31354;&#38388;&#20013;&#20219;&#24847;&#28857;&#21040;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#35745;&#31639;&#30340;&#36895;&#24230;&#19982;&#20960;&#20309;&#30896;&#25758;&#26816;&#26597;&#30340;&#24378;&#22823;&#20934;&#30830;&#24615;&#20445;&#35777;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15281v1 Announce Type: cross  Abstract: Collision detection is one of the most time-consuming operations during motion planning. Thus, there is an increasing interest in exploring machine learning techniques to speed up collision detection and sampling-based motion planning. A recent line of research focuses on utilizing neural signed distance functions of either the robot geometry or the swept volume of the robot motion. Building on this, we present a novel neural implicit swept volume model that is the first to continuously represent arbitrary motions parameterized by their start and goal configurations. This allows to quickly compute signed distances for any point in the task space to the robot motion. Further, we present an algorithm combining the speed of the deep learning-based signed distance computations with the strong accuracy guarantees of geometric collision checkers. We validate our approach in simulated and real-world robotic experiments, and demonstrate that i
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#32479;&#19968;&#35299;&#37322;&#20102;Grokking&#12289;&#21452;&#37325;&#19979;&#38477;&#21644;&#26032;&#20852;&#33021;&#21147;&#36825;&#19977;&#31181;&#29616;&#35937;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#35760;&#24518;&#21644;&#27867;&#21270;&#30005;&#36335;&#20043;&#38388;&#30340;&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.15175</link><description>&lt;p&gt;
&#32479;&#19968;&#29702;&#35299;Grokking&#12289;&#21452;&#38477;&#21644;&#26032;&#20852;&#33021;&#21147;&#65306;&#26469;&#33258;&#30005;&#36335;&#31454;&#20105;&#35270;&#35282;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15175
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#32479;&#19968;&#35299;&#37322;&#20102;Grokking&#12289;&#21452;&#37325;&#19979;&#38477;&#21644;&#26032;&#20852;&#33021;&#21147;&#36825;&#19977;&#31181;&#29616;&#35937;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#35760;&#24518;&#21644;&#27867;&#21270;&#30005;&#36335;&#20043;&#38388;&#30340;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#20123;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#22914;Grokking&#12289;&#21452;&#37325;&#19979;&#38477;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#36825;&#20123;&#25361;&#25112;&#20102;&#20154;&#31867;&#30340;&#30452;&#35273;&#65292;&#24182;&#23545;&#31070;&#32463;&#27169;&#22411;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#19977;&#31181;&#29616;&#35937;&#30340;&#32479;&#19968;&#35266;&#28857;&#65292;&#37325;&#28857;&#20851;&#27880;&#35760;&#24518;&#21644;&#27867;&#21270;&#30005;&#36335;&#20043;&#38388;&#30340;&#31454;&#20105;&#12290;&#36825;&#31181;&#26041;&#27861;&#26368;&#21021;&#29992;&#20110;&#35299;&#37322;Grokking&#65292;&#25105;&#20204;&#22312;&#24037;&#20316;&#20013;&#23558;&#20854;&#25193;&#23637;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21246;&#21202;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#27599;&#31181;&#37117;&#21462;&#20915;&#20110;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#30340;&#19981;&#21516;&#32452;&#21512;&#12290;&#21033;&#29992;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;&#21452;&#37325;&#19979;&#38477;&#29616;&#35937;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#20110;&#20854;&#21457;&#29983;&#30340;&#21487;&#39564;&#35777;&#39044;&#27979;&#65292;&#22343;&#24471;&#21040;&#25105;&#20204;&#23454;&#39564;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15175v1 Announce Type: new  Abstract: Recent studies have uncovered intriguing phenomena in deep learning, such as grokking, double descent, and emergent abilities in large language models, which challenge human intuition and are crucial for a deeper understanding of neural models. In this paper, we present a comprehensive framework that provides a unified view of these three phenomena, focusing on the competition between memorization and generalization circuits. This approach, initially employed to explain grokking, is extended in our work to encompass a wider range of model sizes and training data volumes. Our framework delineates four distinct training dynamics, each depending on varying combinations of model size and training data quantity. Utilizing this framework, we provide a detailed analysis of the double descent phenomenon and propose two verifiable predictions regarding its occurrence, both substantiated by our experimental results. Moreover, we expand our framewo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064; CuPUL &#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14948</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14948
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064; CuPUL &#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#36828;&#31243;&#30417;&#30563;&#65288;DS-NER&#65289;&#26694;&#26550;&#19979;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#26631;&#31614;&#36136;&#37327;&#21463;&#21040;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#22914;&#20551;&#38451;&#24615;&#12289;&#20551;&#38452;&#24615;&#21644;&#27491;&#21521;&#31867;&#22411;&#38169;&#35823;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#20102;&#24403;&#21069;DS-NER&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;QTL&#30340;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#31526;&#21512;&#39044;&#26399;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#26222;&#36941;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#65288;CuPUL&#65289;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31574;&#30053;&#24615;&#22320;&#20174;&#8220;&#26131;&#8221;&#21644;&#26356;&#28165;&#27905;&#30340;&#26679;&#26412;&#24320;&#22987;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#22024;&#26434;&#26679;&#26412;&#30340;&#38887;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#31361;&#20986;&#20102;CuPUL&#20943;&#23569;&#22024;&#26434;&#26631;&#31614;&#24433;&#21709;&#24182;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14948v1 Announce Type: new  Abstract: This paper delves into Named Entity Recognition (NER) under the framework of Distant Supervision (DS-NER), where the main challenge lies in the compromised quality of labels due to inherent errors such as false positives, false negatives, and positive type errors. We critically assess the efficacy of current DS-NER methodologies using a real-world benchmark dataset named QTL, revealing that their performance often does not meet expectations. To tackle the prevalent issue of label noise, we introduce a simple yet effective approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which strategically starts on "easy" and cleaner samples during the training process to enhance model resilience to noisy samples. Our empirical results highlight the capability of CuPUL to significantly reduce the impact of noisy labels and outperform existing methods.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#23545;&#24615;&#33021;&#32780;&#38750;&#20219;&#21153;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#8220;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#8221;&#65292;&#21487;&#24110;&#21161;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#24182;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.14890</link><description>&lt;p&gt;
Vygotsky Distance: &#29992;&#20110;&#22522;&#20934;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vygotsky Distance: Measure for Benchmark Task Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14890
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#23545;&#24615;&#33021;&#32780;&#38750;&#20219;&#21153;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#8220;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#8221;&#65292;&#21487;&#24110;&#21161;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#24182;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#24037;&#20855;&#21644;&#23454;&#36341;&#31639;&#27861;&#26469;&#35745;&#31639;&#22522;&#20934;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#31216;&#20043;&#20026;"&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;"&#12290;&#36825;&#31181;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22522;&#20110;&#8220;&#23398;&#29983;&#8221;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#30340;&#30456;&#23545;&#34920;&#29616;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#20219;&#21153;&#26412;&#36523;&#30340;&#23646;&#24615;&#12290;&#22914;&#26524;&#20004;&#20010;&#20219;&#21153;&#22312;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#19978;&#24444;&#27492;&#25509;&#36817;&#65292;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978; tend to have similar relative performance&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20102;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14890v1 Announce Type: cross  Abstract: Evaluation plays a significant role in modern natural language processing. Most modern NLP benchmarks consist of arbitrary sets of tasks that neither guarantee any generalization potential for the model once applied outside the test set nor try to minimize the resource consumption needed for model evaluation. This paper presents a theoretical instrument and a practical algorithm to calculate similarity between benchmark tasks, we call this similarity measure "Vygotsky distance". The core idea of this similarity measure is that it is based on relative performance of the "students" on a given task, rather that on the properties of the task itself. If two tasks are close to each other in terms of Vygotsky distance the models tend to have similar relative performance on them. Thus knowing Vygotsky distance between tasks one can significantly reduce the number of evaluation tasks while maintaining a high validation quality. Experiments on v
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;DLM&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;</title><link>https://arxiv.org/abs/2402.14807</link><description>&lt;p&gt;
&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#20013;&#21160;&#24577;&#19981;&#23433;&#38745;&#22810;&#33218;&#32769;&#34382;&#26426;&#20219;&#21153;&#30340;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;&#65288;DLM&#65289;
&lt;/p&gt;
&lt;p&gt;
A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;DLM&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#38477;&#20302;&#23381;&#20135;&#22919;&#27515;&#20129;&#29575;&#30340;&#21162;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39044;&#38450;&#20445;&#20581;&#35745;&#21010;&#65292;&#21521;&#39640;&#39118;&#38505;&#20154;&#32676;&#20256;&#25773;&#37325;&#35201;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DLM&#65306;&#19968;&#31181;&#29992;&#20110;RMAB&#30340;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14807v1 Announce Type: cross  Abstract: Efforts to reduce maternal mortality rate, a key UN Sustainable Development target (SDG Target 3.1), rely largely on preventative care programs to spread critical health information to high-risk populations. These programs face two important challenges: efficiently allocating limited health resources to large beneficiary populations, and adapting to evolving policy priorities. While prior works in restless multi-armed bandit (RMAB) demonstrated success in public health allocation tasks, they lack flexibility to adapt to evolving policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept, automated planners in various domains, including robotic control and navigation. In this paper, we propose DLM: a Decision Language Model for RMABs. To enable dynamic fine-tuning of RMAB policies for challenging public health settings using human-language commands, we propose using LLMs as automated planners to (1) interpret hu
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#37325;&#26032;&#23457;&#35270;REINFORCE&#39118;&#26684;&#20248;&#21270;&#23545;&#20110;&#23398;&#20064;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#31616;&#21270;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14740</link><description>&lt;p&gt;
&#22238;&#24402;&#22522;&#30784;: &#37325;&#26032;&#23457;&#35270;LLMs&#20013;&#23398;&#20064;&#20154;&#31867;&#21453;&#39304;&#30340;REINFORCE&#39118;&#26684;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14740
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#37325;&#26032;&#23457;&#35270;REINFORCE&#39118;&#26684;&#20248;&#21270;&#23545;&#20110;&#23398;&#20064;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#31616;&#21270;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14740v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: AI&#23545;&#40784;&#34987;&#35270;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#33267;&#20851;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290; \textsc{Proximal Policy Optimization} (PPO)&#24050;&#34987;&#26368;&#26032;&#25991;&#29486;&#23450;&#20301;&#20026;RLHF&#20013;RL&#37096;&#20998;&#30340;&#20856;&#33539;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#26082;&#28041;&#21450;&#39640;&#35745;&#31639;&#25104;&#26412;&#21448;&#28041;&#21450;&#25935;&#24863;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#25105;&#20204;&#35748;&#20026;&#35302;&#21457;&#20102;PPO&#21457;&#23637;&#30340;&#22823;&#22810;&#25968;&#21160;&#26426;&#21407;&#21017;&#22312;RLHF&#20013;&#24182;&#38750;&#23454;&#36341;&#19978;&#30340;&#20851;&#27880;&#37325;&#28857;&#65292;&#24182;&#25552;&#20513;&#19968;&#31181;&#26356;&#23569;&#35745;&#31639;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20445;&#25345;&#29978;&#33267;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;RL&#30340;&#29615;&#22659;&#20013;&#26681;&#25454;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#30340;\textit{&#20844;&#24335;}&#12290;&#20197;&#31616;&#21333;&#24615;&#20026;&#25351;&#23548;&#21407;&#21017;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PPO&#30340;&#35768;&#22810;&#32452;&#20214;&#22312;RLHF&#29615;&#22659;&#20013;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#24182;&#19988;&#36828;&#36828;&#26356;&#31616;&#21333;&#30340;REINFORCE&#39118;&#26684;&#30340;&#20248;&#21270;&#21464;&#20307;&#34920;&#29616;&#20986;&#27604;PPO&#21644;&#26032;&#25552;&#20986;&#30340;&#8220;RL-free&#8221;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14740v1 Announce Type: new  Abstract: AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. \textsc{Proximal Policy Optimization} (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the \textit{formulation} of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed "RL-free" methods such a
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.14609</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Complex Qeury Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14609
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#31572;&#26696;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#25191;&#34892;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;&#30340;&#33021;&#21147;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22522;&#20110;&#22270;&#25512;&#29702;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#21644;&#36923;&#36753;&#26597;&#35810;&#34920;&#31034;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25214;&#21040;&#36923;&#36753;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#26597;&#35810;&#21333;&#20010;&#30693;&#35782;&#22270;&#35889;&#19978;&#65292;&#24182;&#19981;&#33021;&#24212;&#29992;&#20110;&#22810;&#20010;&#22270;&#24418;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#20849;&#20139;&#24102;&#26377;&#25935;&#24863;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#21487;&#33021;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#65292;&#20351;&#24471;&#20849;&#20139;&#21644;&#26500;&#24314;&#19968;&#20010;&#32858;&#21512;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#25512;&#29702;&#20197;&#26816;&#32034;&#26597;&#35810;&#31572;&#26696;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#19981;&#28165;&#26970;&#22914;&#20309;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#26597;&#35810;&#12290;&#19968;&#20010;&#23454;&#20307;&#21487;&#33021;&#28041;&#21450;&#21040;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#23545;&#20110;&#21457;&#29616;&#30693;&#35782;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14609v1 Announce Type: cross  Abstract: Complex logical query answering is a challenging task in knowledge graphs (KGs) that has been widely studied. The ability to perform complex logical reasoning is essential and supports various graph reasoning-based downstream tasks, such as search engines. Recent approaches are proposed to represent KG entities and logical queries into embedding vectors and find answers to logical queries from the KGs. However, existing proposed methods mainly focus on querying a single KG and cannot be applied to multiple graphs. In addition, directly sharing KGs with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated KG for reasoning to retrieve query answers. Thus, it remains unknown how to answer queries on multi-source KGs. An entity can be involved in various knowledge graphs and reasoning on multiple KGs and answering complex queries on multi-source KGs is important in discovering knowledge 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#23567;&#23454;&#20363;&#19978;&#20445;&#35777;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#20063;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#20934;&#30830;&#24230;&#30340;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14332</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#21040;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#30340;&#23610;&#23544;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14332
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#23567;&#23454;&#20363;&#19978;&#20445;&#35777;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#20063;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#20934;&#30830;&#24230;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#25105;&#20204;&#20250;&#24471;&#21040;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#35201;&#26377;&#25928;&#22320;&#36873;&#25321;&#35201;&#20351;&#29992;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26377;&#19968;&#20010;&#26410;&#30693;&#30340;&#22522;&#20934;&#32858;&#31867;&#65292;&#25105;&#20204;&#21482;&#33021;&#36890;&#36807;&#26114;&#36149;&#30340;oracle&#26597;&#35810;&#26469;&#35775;&#38382;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#32858;&#31867;&#31639;&#27861;&#30340;&#36755;&#20986;&#23558;&#19982;&#22522;&#26412;&#20107;&#23454;&#32467;&#26500;&#19978;&#25509;&#36817;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32858;&#31867;&#31639;&#27861;&#20934;&#30830;&#24615;&#30340;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23450;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#25105;&#20204;&#21487;&#20197;&#65288;1&#65289;&#23545;&#22823;&#35268;&#27169;&#32858;&#31867;&#23454;&#20363;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#65288;2&#65289;&#22312;&#36739;&#23567;&#23454;&#20363;&#19978;&#35780;&#20272;&#19968;&#32452;&#20505;&#36873;&#31639;&#27861;&#65292;&#65288;3&#65289;&#20445;&#35777;&#22312;&#23567;&#23454;&#20363;&#19978;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#20026;&#19977;&#31181;&#32463;&#20856;&#32858;&#31867;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#23610;&#23544;&#27867;&#21270;&#20445;&#35777;&#65306;&#21333;&#38142;&#25509;&#12289;k-means++&#21644;Gonzalez&#30340;k&#20013;&#24515;&#21551;&#21457;&#24335;&#65288;&#19968;&#31181;&#24179;&#28369;&#30340;&#21464;&#31181;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14332v1 Announce Type: new  Abstract: In clustering algorithm selection, we are given a massive dataset and must efficiently select which clustering algorithm to use. We study this problem in a semi-supervised setting, with an unknown ground-truth clustering that we can only access through expensive oracle queries. Ideally, the clustering algorithm's output will be structurally close to the ground truth. We approach this problem by introducing a notion of size generalization for clustering algorithm accuracy. We identify conditions under which we can (1) subsample the massive clustering instance, (2) evaluate a set of candidate algorithms on the smaller instance, and (3) guarantee that the algorithm with the best accuracy on the small instance will have the best accuracy on the original big instance. We provide theoretical size generalization guarantees for three classic clustering algorithms: single-linkage, k-means++, and (a smoothed variant of) Gonzalez's k-centers heuris
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35270;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27010;&#29575;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#36866;&#37197;&#20559;&#24046;&#26657;&#27491;&#20219;&#21153;&#65292;&#21487;&#20934;&#30830;&#26657;&#27491;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14169</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#20559;&#24046;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
A Temporal Bias Correction using a Machine Learning Attention model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35270;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27010;&#29575;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#36866;&#37197;&#20559;&#24046;&#26657;&#27491;&#20219;&#21153;&#65292;&#21487;&#20934;&#30830;&#26657;&#27491;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#27169;&#22411;&#22312;&#19982;&#30495;&#23454;&#19990;&#30028;&#35266;&#27979;&#25968;&#25454;&#30456;&#27604;&#23384;&#22312;&#20559;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#24433;&#21709;&#30740;&#31350;&#20043;&#21069;&#36827;&#34892;&#26657;&#20934;&#12290;&#20351;&#26657;&#20934;&#25104;&#20026;&#21487;&#33021;&#30340;&#32479;&#35745;&#26041;&#27861;&#38598;&#21512;&#34987;&#31216;&#20026;&#20559;&#24046;&#26657;&#27491;&#65288;BC&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;BC&#26041;&#27861;&#22312;&#35843;&#25972;&#26102;&#38388;&#20559;&#24046;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#30053;&#20102;&#36830;&#32493;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65288;&#22914;&#28909;&#28010;&#25345;&#32493;&#26102;&#38388;&#21644;&#39057;&#29575;&#65289;&#26080;&#27861;&#20934;&#30830;&#26657;&#27491;&#65292;&#36825;&#20351;&#24471;&#22312;&#36825;&#20123;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BC&#26041;&#27861;&#26469;&#26657;&#27491;&#26102;&#38388;&#20559;&#24046;&#12290;&#36825;&#24471;&#30410;&#20110;&#23558;BC&#37325;&#26032;&#26500;&#24819;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#24182;&#23558;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27010;&#29575;&#20851;&#27880;&#27169;&#22411;&#35843;&#25972;&#21040;BC&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#23612;&#26085;&#21033;&#20122;&#38463;&#24067;&#36158;&#30340;&#28909;&#28010;&#25345;&#32493;&#26102;&#38388;&#32479;&#35745;&#26696;&#20363;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14169v1 Announce Type: new  Abstract: Climate models are biased with respect to real world observations and usually need to be calibrated prior to impact studies. The suite of statistical methods that enable such calibrations is called bias correction (BC). However, current BC methods struggle to adjust for temporal biases, because they disregard the dependence between consecutive time-points. As a result, climate statistics with long-range temporal properties, such as heatwave duration and frequency, cannot be corrected accurately, making it more difficult to produce reliable impact studies on such climate statistics. In this paper, we offer a novel BC methodology to correct for temporal biases. This is made possible by i) re-thinking BC as a probability model rather than an algorithmic procedure, and ii) adapting state-of-the-art machine-learning (ML) probabilistic attention models to fit the BC task. With a case study of heatwave duration statistics in Abuja, Nigeria, and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#30340;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#30456;&#24212;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.13929</link><description>&lt;p&gt;
SDXL-Lightning: &#28176;&#36827;&#24335;&#23545;&#25239;&#24615;&#25193;&#25955;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
SDXL-Lightning: Progressive Adversarial Diffusion Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13929
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#30340;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#30456;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;SDXL&#30340;&#19968;&#27493;/&#20960;&#27493;1024&#20687;&#32032;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#36136;&#37327;&#21644;&#27169;&#24335;&#35206;&#30422;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#29702;&#35770;&#20998;&#26512;&#12289;&#21028;&#21035;&#22120;&#35774;&#35745;&#12289;&#27169;&#22411;&#20844;&#24335;&#21644;&#35757;&#32451;&#25216;&#24039;&#12290;&#25105;&#20204;&#20197;LoRA&#21644;&#23436;&#25972;UNet&#26435;&#37325;&#30340;&#24418;&#24335;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#33976;&#39311;SDXL-Lightning&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13929v1 Announce Type: cross  Abstract: We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#20113;&#36827;&#34892;&#20998;&#21106;&#65292;&#26088;&#22312;&#25552;&#39640;&#21355;&#26143;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24212;&#29992;&#20110;&#29615;&#22659;&#30417;&#27979;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.13918</link><description>&lt;p&gt;
BenchCloudVision: &#20113;&#26816;&#27979;&#21644;&#20998;&#21106;&#20013;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#20113;&#36827;&#34892;&#20998;&#21106;&#65292;&#26088;&#22312;&#25552;&#39640;&#21355;&#26143;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24212;&#29992;&#20110;&#29615;&#22659;&#30417;&#27979;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37197;&#22791;&#20809;&#23398;&#20256;&#24863;&#22120;&#30340;&#21355;&#26143;&#25429;&#33719;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20026;&#21508;&#31181;&#29615;&#22659;&#29616;&#35937;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#36965;&#24863;&#20013;&#19968;&#20123;&#25361;&#25112;&#30340;&#30740;&#31350;&#28608;&#22686;&#65292;&#20174;&#19981;&#21516;&#26223;&#35266;&#20013;&#30340;&#27700;&#26816;&#27979;&#21040;&#23665;&#22320;&#21644;&#22320;&#24418;&#30340;&#20998;&#21106;&#12290;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#21355;&#26143;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;&#23588;&#20854;&#26159;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#37325;&#28857;&#25918;&#22312;&#24320;&#21457;&#20934;&#30830;&#30340;&#27700;&#20307;&#26816;&#27979;&#12289;&#38634;&#21644;&#20113;&#30340;&#26041;&#27861;&#19978;&#65292;&#36825;&#20123;&#23545;&#29615;&#22659;&#30417;&#27979;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#19987;&#27880;&#20110;&#20174;&#36965;&#24863;&#22270;&#20687;&#20013;&#20998;&#21106;&#20113;&#12290;&#30001;&#20110;&#20809;&#23398;&#20256;&#24863;&#22120;&#24212;&#29992;&#20013;&#20113;&#30340;&#23384;&#22312;&#65292;&#20934;&#30830;&#30340;&#36965;&#24863;&#25968;&#25454;&#20998;&#26512;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#20135;&#21697;&#65288;&#22914;&#24212;&#29992;&#21644;&#30740;&#31350;&#65289;&#30340;&#36136;&#37327;&#30452;&#25509;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13918v1 Announce Type: cross  Abstract: Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directl
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13777</link><description>&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65306;&#25945;&#31243;&#12289;&#35843;&#26597;&#21644;&#26410;&#26469;&#26041;&#21521;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13777
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#20174;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#26041;&#38754;&#12290;&#31867;&#20284;&#22320;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#20063;&#38656;&#35201;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#29983;&#25104;&#20989;&#25968;&#20316;&#20026;&#31574;&#30053;&#25110;&#25919;&#31574;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#22240;&#27492;&#19981;&#21516;&#20998;&#25903;&#30340;&#21457;&#23637;&#30456;&#23545;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#24212;&#29992;&#26041;&#38754;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#24402;&#19968;&#21270;&#27969;&#12289;&#21464;&#21387;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
&lt;/p&gt;</description></item><item><title>HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;</title><link>https://arxiv.org/abs/2402.12656</link><description>&lt;p&gt;
HyperMoE: &#36890;&#36807;&#19987;&#23478;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#23454;&#29616;&#26356;&#22909;&#30340;&#19987;&#23478;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12656
&lt;/p&gt;
&lt;p&gt;
HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;(MoE)&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#23558;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#36335;&#30001;&#21040;&#29305;&#23450;&#30340;&#19987;&#23478;&#23376;&#38598;&#36827;&#34892;&#22788;&#29702;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#19987;&#23478;&#30693;&#35782;&#30340;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#38754;&#20020;&#25361;&#25112;&#65306;&#36890;&#36807;&#22686;&#21152;&#23545;&#19987;&#23478;&#30693;&#35782;&#30340;&#20351;&#29992;&#26469;&#22686;&#24378;&#24615;&#33021;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#31232;&#30095;&#24230;&#20943;&#23569;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#30683;&#30462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperMoE&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;Hypernetworks&#20043;&#19978;&#30340;&#26032;&#39062;MoE&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;MoE&#30340;&#35745;&#31639;&#36807;&#31243;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#27010;&#24565;&#36827;&#34892;&#20102;&#38598;&#25104;&#12290;&#22522;&#20110;&#26410;&#36873;&#25321;&#19987;&#23478;&#20449;&#24687;&#29983;&#25104;&#30340;&#29305;&#23450;&#27169;&#22359;&#20316;&#20026;&#34917;&#20805;&#20449;&#24687;&#65292;&#20801;&#35768;&#26410;&#34987;&#36873;&#20013;&#30340;&#19987;&#23478;&#30340;&#30693;&#35782;&#22312;&#20445;&#25345;&#36873;&#25321;&#31232;&#30095;&#24615;&#30340;&#21516;&#26102;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12656v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#23618;&#36125;&#21494;&#26031;&#32479;&#35745;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20854;&#20013;&#24320;&#21457;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#26469;&#24179;&#34913;&#21033;&#29992;&#26377;&#38480;&#26412;&#22320;&#25968;&#25454;&#21644;&#21327;&#20316;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.12537</link><description>&lt;p&gt;
&#38024;&#23545;&#20010;&#24615;&#21270;&#32852;&#37030;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#23618;&#36125;&#21494;&#26031;&#32479;&#35745;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20854;&#20013;&#24320;&#21457;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#26469;&#24179;&#34913;&#21033;&#29992;&#26377;&#38480;&#26412;&#22320;&#25968;&#25454;&#21644;&#21327;&#20316;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#26412;&#22320;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#20854;&#20419;&#20351;&#20010;&#24615;&#21270;&#31639;&#27861;&#38024;&#23545;&#26412;&#22320;&#25968;&#25454;&#32479;&#35745;&#37327;&#36827;&#34892;&#23450;&#21046;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#38024;&#23545;&#20010;&#24615;&#21270;&#30417;&#30563;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20294;&#36890;&#36807;&#20010;&#24615;&#21270;&#26080;&#30417;&#30563;&#23398;&#20064;&#21457;&#29616;&#26412;&#22320;&#25968;&#25454;&#30340;&#32467;&#26500;&#21364;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#23618;&#27425;&#36125;&#21494;&#26031;&#32479;&#35745;&#26694;&#26550;&#21551;&#21160;&#20102;&#23545;&#36825;&#31181;&#20010;&#24615;&#21270;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#20248;&#21270;&#26631;&#20934;&#30340;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#21463;&#21551;&#21457;&#20110;&#23618;&#27425;&#36125;&#21494;&#26031;&#32479;&#35745;&#26694;&#26550;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#65292;&#21457;&#29616;&#20102;&#21033;&#29992;&#26377;&#38480;&#26412;&#22320;&#25968;&#25454;&#21644;&#21327;&#20316;&#20449;&#24687;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#36825;&#39033;&#24037;&#20316;&#65306;&#20010;&#24615;&#21270;&#38477;&#32500;&#21644;&#20010;&#24615;&#21270;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#24320;&#21457;&#20102;&#25910;&#25947;&#20998;&#26512;&#65292;&#36825;&#20123;&#20998;&#26512;&#23637;&#31034;&#20102;&#23545;&#38382;&#39064;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;&#24322;&#36136;&#24615;&#65289;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12537v1 Announce Type: new  Abstract: Statistical heterogeneity of clients' local data is an important characteristic in federated learning, motivating personalized algorithms tailored to the local data statistics. Though there has been a plethora of algorithms proposed for personalized supervised learning, discovering the structure of local data through personalized unsupervised learning is less explored. We initiate a systematic study of such personalized unsupervised learning by developing algorithms based on optimization criteria inspired by a hierarchical Bayesian statistical framework. We develop adaptive algorithms that discover the balance between using limited local data and collaborative information. We do this in the context of two unsupervised learning tasks: personalized dimensionality reduction and personalized diffusion models. We develop convergence analyses for our adaptive algorithms which illustrate the dependence on problem parameters (e.g., heterogeneity
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12177</link><description>&lt;p&gt;
Mafin: &#29992;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#26469;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12177
&lt;/p&gt;
&lt;p&gt;
Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;RAG&#20013;&#30340;&#26816;&#32034;&#38454;&#27573;&#36890;&#24120;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#36716;&#25442;&#20026;&#21521;&#37327;&#20197;&#25429;&#33719;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#26102;&#65292;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20165;&#33021;&#20174;&#40657;&#30418;&#27169;&#22411;&#33719;&#21462;&#23884;&#20837;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#65288;Mafin&#65289;--&#19968;&#31181;&#36890;&#36807;&#29992;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Mafin&#20165;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22686;&#24378;&#27169;&#22411;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40657;&#30418;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
&lt;/p&gt;</description></item><item><title>Leaky ReLU&#21442;&#25968;$\alpha=-1$&#22312;&#35757;&#32451;&#35823;&#24046;&#21644;&#27867;&#21270;&#35823;&#24046;&#30028;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.11942</link><description>&lt;p&gt;
Leaky ReLU&#23545;&#36229;&#21442;&#25968;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#27867;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The effect of Leaky ReLUs on the training and generalization of overparameterized networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11942
&lt;/p&gt;
&lt;p&gt;
Leaky ReLU&#21442;&#25968;$\alpha=-1$&#22312;&#35757;&#32451;&#35823;&#24046;&#21644;&#27867;&#21270;&#35823;&#24046;&#30028;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#21508;&#31181;&#27844;&#28431;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20989;&#25968;&#30340;&#36229;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#35757;&#32451;&#21644;&#27867;&#21270;&#35823;&#24046;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20180;&#32454;&#22320;&#23545;&#36825;&#20123;NNs&#30340;&#35757;&#32451;&#35823;&#24046;&#30340;&#25910;&#25947;&#36895;&#29575;&#21644;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#19978;&#30028;&#20272;&#35745;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#30028;&#38480;&#23545;Leaky ReLU&#21442;&#25968;$\alpha$&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;$\alpha=-1$&#65292;&#23545;&#24212;&#20110;&#32477;&#23545;&#20540;&#28608;&#27963;&#20989;&#25968;&#65292;&#23545;&#20110;&#35757;&#32451;&#35823;&#24046;&#30028;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#29305;&#23450;&#35774;&#32622;&#20013;&#65292;&#36825;&#20063;&#26159;&#27867;&#21270;&#35823;&#24046;&#30028;&#30340;&#26368;&#20248;&#36873;&#25321;&#12290;&#25968;&#20540;&#23454;&#39564;&#22312;&#23454;&#36341;&#20013;&#25903;&#25345;&#20102;&#29702;&#35770;&#24341;&#23548;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11942v1 Announce Type: new  Abstract: We investigate the training and generalization errors of overparameterized neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU) functions. More specifically, we carefully upper bound both the convergence rate of the training error and the generalization error of such NNs and investigate the dependence of these bounds on the Leaky ReLU parameter, $\alpha$. We show that $\alpha =-1$, which corresponds to the absolute value activation function, is optimal for the training error bound. Furthermore, in special settings, it is also optimal for the generalization error bound. Numerical experiments empirically support the practical choices guided by the theory.
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.11793</link><description>&lt;p&gt;
&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative Kaleidoscopic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11793
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#65288;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#26550;&#26500;&#65289;&#34920;&#29616;&#20986;&#8220;&#36807;&#24230;&#27867;&#21270;&#8221;&#29616;&#35937;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#37027;&#20123;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#30475;&#21040;&#30340;&#36755;&#20837;&#30340;&#36755;&#20986;&#20540;&#34987;&#26144;&#23556;&#21040;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#36755;&#20986;&#33539;&#22260;&#38468;&#36817;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#20102;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#65292;&#36825;&#31181;&#25928;&#24212;&#22312;&#22686;&#21152;&#23618;&#25968;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#28145;&#24230;&#26102;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#28145;&#23618;ReLU&#32593;&#32476;&#30340;&#36825;&#19968;&#29305;&#24615;&#26469;&#35774;&#35745;&#19968;&#20010;&#25968;&#25454;&#38598;&#19975;&#33457;&#31570;&#65292;&#31216;&#20026;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#22914;&#26524;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#23558;&#36755;&#20837; $x\in\mathbb{R}^D$ &#26144;&#23556;&#21040;&#33258;&#36523; $f_\mathcal{N}(x)\rightarrow x$&#65292;&#37027;&#20040;&#8220;&#19975;&#33457;&#31570;&#37319;&#26679;&#8221;&#36807;&#31243;&#23558;&#20174;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768; $z\in\mathbb{R}^D$ &#24320;&#22987;&#65292;&#24182;&#36882;&#24402;&#22320;&#24212;&#29992; $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$&#12290;&#32463;&#36807;&#29123;&#28903;&#26399;&#21518;&#65292;&#25105;&#20204;&#24320;&#22987;&#35266;&#23519;&#26469;&#33258;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#28145;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;LLM&#24494;&#35843;&#20013;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#25193;&#23637;&#20102;&#23545;&#19981;&#21516;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#30340;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.11592</link><description>&lt;p&gt;
&#37325;&#26032;&#25506;&#35752;&#38646;&#38454;&#20248;&#21270;&#22312;&#20869;&#23384;&#39640;&#25928;LLM&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#20010;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;LLM&#24494;&#35843;&#20013;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#25193;&#23637;&#20102;&#23545;&#19981;&#21516;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#20351;&#29992;SGD&#21644;Adam&#31561;&#19968;&#38454;&#65288;FO&#65289;&#20248;&#21270;&#22120;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#20307;&#31215;&#30340;&#22686;&#38271;&#65292;&#30001;&#20110;FO&#26799;&#24230;&#35745;&#31639;&#30340;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#24102;&#26469;&#30340;&#24040;&#22823;&#20869;&#23384;&#24320;&#38144;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#20869;&#23384;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#35774;&#22791;&#31471;&#35757;&#32451;&#31561;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21521;&#19981;&#20351;&#29992;BP&#30340;&#38646;&#38454;&#65288;ZO&#65289;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;LLM&#24494;&#35843;&#36807;&#31243;&#20013;&#38477;&#20302;&#20869;&#23384;&#25104;&#26412;&#65292;&#26500;&#24314;&#22312;MeZO&#25552;&#20986;&#30340;&#27010;&#24565;&#22522;&#30784;&#19978;&#12290;&#19982;&#20256;&#32479;&#30340;ZO&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#25506;&#32034;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#12289;&#39318;&#27425;&#25512;&#20986;&#30340;&#22522;&#20934;&#30740;&#31350;&#36328;&#36234;&#20116;&#20010;LLM&#31995;&#21015;&#65288;Roberta&#65292;OPT&#65292;LLaMA&#65292;Vicuna&#65292;Mistral&#65289;&#65292;&#19977;&#31181;&#20219;&#21153;&#22797;&#26434;&#24615;&#21644;&#20116;&#31181;&#24494;&#35843;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11592v1 Announce Type: new  Abstract: In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#31283;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#31283;&#23450;&#24615;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#20102;&#19977;&#31181;&#31283;&#23450;&#24615;&#31867;&#22411;&#21644;&#19968;&#22871;&#20840;&#38754;&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.11404</link><description>&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Stability of Deep Learning Latent Feature Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11404
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#31283;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#31283;&#23450;&#24615;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#20102;&#19977;&#31181;&#31283;&#23450;&#24615;&#31867;&#22411;&#21644;&#19968;&#22871;&#20840;&#38754;&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#38598;&#22312;&#21508;&#20010;&#23398;&#31185;&#30340;&#32479;&#35745;&#24314;&#27169;&#20013;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#38477;&#32500;&#26041;&#27861;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20197;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#25552;&#28860;&#20851;&#38190;&#29305;&#24449;&#30340;&#33021;&#21147;&#32780;&#33879;&#31216;&#65292;&#36890;&#36807;&#38477;&#32500;&#30340;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#23454;&#29616;&#24314;&#27169;&#12289;&#21487;&#35270;&#21270;&#21644;&#21387;&#32553;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21040;&#22320;&#29699;&#31185;&#23398;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#28508;&#22312;&#31354;&#38388;&#30340;&#31283;&#23450;&#24615;&#65292;&#30830;&#20445;&#21518;&#32493;&#20998;&#26512;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#31283;&#23450;&#24615;&#34987;&#23450;&#20041;&#20026;&#28508;&#22312;&#31354;&#38388;&#23545;&#20110;&#24494;&#23567;&#25968;&#25454;&#12289;&#35757;&#32451;&#23454;&#29616;&#21644;&#21442;&#25968;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#26159;&#33267;&#20851;&#37325;&#35201;&#21364;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#30028;&#23450;&#20102;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#19977;&#31181;&#31283;&#23450;&#24615;&#31867;&#22411;&#65292;&#26679;&#26412;&#31283;&#23450;&#24615;&#12289;&#32467;&#26500;&#31283;&#23450;&#24615;&#21644;&#25512;&#26029;&#31283;&#23450;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#22871;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11404v1 Announce Type: new  Abstract: High-dimensional datasets present substantial challenges in statistical modeling across various disciplines, necessitating effective dimensionality reduction methods. Deep learning approaches, notable for their capacity to distill essential features from complex data, facilitate modeling, visualization, and compression through reduced dimensionality latent feature spaces, have wide applications from bioinformatics to earth sciences. This study introduces a novel workflow to evaluate the stability of these latent spaces, ensuring consistency and reliability in subsequent analyses. Stability, defined as the invariance of latent spaces to minor data, training realizations, and parameter perturbations, is crucial yet often overlooked.   Our proposed methodology delineates three stability types, sample, structural, and inferential, within latent spaces, and introduces a suite of metrics for comprehensive evaluation. We implement this workflow
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#38041;&#38043;&#30719;&#23454;&#39564;&#20013;&#25552;&#21462;&#22810;&#20010;&#22522;&#26412;&#26448;&#26009;&#21442;&#25968;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#21322;&#23548;&#20307;&#20248;&#21270;</title><link>https://arxiv.org/abs/2402.11101</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#20174;&#38041;&#38043;&#30719;&#23454;&#39564;&#20013;&#25552;&#21462;&#22522;&#20110;&#29289;&#29702;&#30340;&#26448;&#26009;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Physics-based material parameters extraction from perovskite experiments via Bayesian optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11101
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#38041;&#38043;&#30719;&#23454;&#39564;&#20013;&#25552;&#21462;&#22810;&#20010;&#22522;&#26412;&#26448;&#26009;&#21442;&#25968;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#21322;&#23548;&#20307;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23450;&#37327;&#23454;&#39564;&#20998;&#26512;&#20013;&#25552;&#21462;&#26448;&#26009;&#21442;&#25968;&#30340;&#33021;&#21147;&#23545;&#20110;&#21512;&#29702;&#35774;&#35745;&#21644;&#29702;&#35770;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29702;&#35770;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#26448;&#26009;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#20998;&#26512;&#30340;&#38590;&#24230;&#26174;&#30528;&#22686;&#21152;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#30636;&#24577;&#20809;&#33268;&#21457;&#20809;&#23454;&#39564;&#20013;&#25552;&#21462;&#19968;&#20010;&#26377;&#26426;&#37329;&#23646;&#38041;&#38043;&#30719;&#21322;&#23548;&#20307;&#30340;8&#20010;&#22522;&#26412;&#26448;&#26009;&#21442;&#25968;&#65292;&#22522;&#20110;&#19968;&#20010;&#21253;&#25324;&#36733;&#27969;&#23376;&#28418;&#31227;&#25193;&#25955;&#21644;&#21160;&#24577;&#32570;&#38519;&#21344;&#25454;&#30340;&#22797;&#26434;&#20840;&#29289;&#29702;&#27169;&#22411;&#12290;&#28909;&#38477;&#35299;&#30340;&#19968;&#20010;&#31034;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#25530;&#26434;&#27987;&#24230;&#21644;&#36733;&#27969;&#23376;&#36801;&#31227;&#29575;&#30340;&#21464;&#21270;&#20027;&#23548;&#65292;&#32780;&#32570;&#38519;&#33021;&#32423;&#20960;&#20046;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#20010;&#24179;&#21488;&#21487;&#20197;&#26041;&#20415;&#22320;&#24212;&#29992;&#20110;&#20854;&#20182;&#23454;&#39564;&#25110;&#23454;&#39564;&#32452;&#21512;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#21322;&#23548;&#20307;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11101v1 Announce Type: cross  Abstract: The ability to extract material parameters from quantitative experimental analysis is essential for rational design and theory advancement. However, the difficulty of this analysis increases significantly with the complexity of the theoretical model and the number of material parameters. Here we use Bayesian optimization to develop an analysis platform that can extract up to 8 fundamental material parameters of an organometallic perovskite semiconductor from a transient photoluminescence experiment, based on a complex full physics model that includes drift-diffusion of carriers and dynamic defect occupation. An example study of thermal degradation reveals that changes in doping concentration and carrier mobility dominate, while the defect energy level remains nearly unchanged. This platform can be conveniently applied to other experiments or to combinations of experiments, accelerating materials discovery and optimization of semiconduc
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#27867;&#21270;&#25928;&#26524;&#21462;&#20915;&#20110;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#21644;&#20154;&#32676;&#20013;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#26679;&#26412;&#36739;&#23569;&#30340;&#21307;&#38498;&#21644;&#29305;&#23450;&#20154;&#32676;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10965</link><description>&lt;p&gt;
&#21307;&#30103;AI&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65306;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10965
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#27867;&#21270;&#25928;&#26524;&#21462;&#20915;&#20110;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#21644;&#20154;&#32676;&#20013;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#26679;&#26412;&#36739;&#23569;&#30340;&#21307;&#38498;&#21644;&#29305;&#23450;&#20154;&#32676;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20026;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#26426;&#36935;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#12289;&#20020;&#24202;&#20915;&#31574;&#20197;&#21450;&#25552;&#21319;&#21307;&#24072;&#21644;&#31649;&#29702;&#20154;&#21592;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#21147;&#37325;&#35201;&#21462;&#20915;&#20110;&#23427;&#20204;&#22312;&#20020;&#24202;&#29615;&#22659;&#21644;&#20154;&#32676;&#20013;&#26377;&#25928;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#22312;&#26089;&#26399;&#24320;&#21457;&#20013;&#32463;&#24120;&#34987;&#20302;&#20272;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#25361;&#25112;&#30340;&#21407;&#22240;&#24182;&#21046;&#23450;&#32531;&#35299;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ClinicLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312; [HOSPITAL] &#30340;&#20020;&#24202;&#31508;&#35760;&#19978;&#35757;&#32451;&#30340;LLM&#27169;&#22411;&#65292;&#23545;&#20854;&#22312;30&#22825;&#20840;&#22240;&#32032;&#20877;&#20837;&#38498;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20998;&#26512;&#65292;&#20851;&#27880;&#36328;&#21307;&#38498;&#21644;&#24739;&#32773;&#29305;&#24449;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26679;&#26412;&#36739;&#23569;&#30340;&#21307;&#38498;&#12289;&#25919;&#24220;&#21644;&#26410;&#25351;&#23450;&#20445;&#38505;&#30340;&#24739;&#32773;&#12289;&#32769;&#24180;&#20154;&#20197;&#21450;&#39640;&#20849;&#30149;&#24615;&#24739;&#32773;&#20013;&#65292;&#27867;&#21270;&#25928;&#26524;&#36739;&#24046;&#12290;&#20026;&#20102;&#20102;&#35299;&#27867;&#21270;&#19981;&#24432;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26679;&#26412;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10965v1 Announce Type: new  Abstract: Advances in large language models (LLMs) provide new opportunities in healthcare for improved patient care, clinical decision-making, and enhancement of physician and administrator workflows. However, the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development. To better understand reasons for these challenges and inform mitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s clinical notes, analyzing its performance on 30-day all-cause readmission prediction focusing on variability across hospitals and patient characteristics. We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities. To understand reasons for lack of generalization, we investigated sample sizes 
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10877</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#26234;&#33021;&#20307;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust agents learn causal world models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10877
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#30452;&#26377;&#20154;&#20551;&#35774;&#22240;&#26524;&#25512;&#29702;&#22312;&#24378;&#20581;&#19988;&#20855;&#26377;&#36890;&#29992;&#26234;&#33021;&#20013;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#65292;&#28982;&#32780;&#19981;&#28165;&#26970;&#26234;&#33021;&#20307;&#26159;&#21542;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#25110;&#32773;&#20854;&#20182;&#24402;&#32435;&#20559;&#24046;&#26159;&#21542;&#36275;&#22815;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#34920;&#26126;&#20219;&#20309;&#33021;&#22815;&#22312;&#22823;&#37327;&#20998;&#24067;&#36716;&#21464;&#19979;&#28385;&#36275;&#21518;&#24724;&#30028;&#38480;&#30340;&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#36817;&#20284;&#22240;&#26524;&#27169;&#22411;&#65292;&#23545;&#20110;&#20248;&#21270;&#26234;&#33021;&#20307;&#26469;&#35828;&#65292;&#35813;&#36817;&#20284;&#27169;&#22411;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24037;&#19994;&#32423;&#22522;&#20934;TimeSeriesBench&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#22312;&#35757;&#32451;&#33539;&#24335;&#12289;&#22312;&#32447;&#26816;&#27979;&#33539;&#24335;&#21644;&#35780;&#20272;&#26631;&#20934;&#26041;&#38754;&#19982;&#23454;&#38469;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.10802</link><description>&lt;p&gt;
TimeSeriesBench&#65306;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24037;&#19994;&#32423;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10802
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24037;&#19994;&#32423;&#22522;&#20934;TimeSeriesBench&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#22312;&#35757;&#32451;&#33539;&#24335;&#12289;&#22312;&#32447;&#26816;&#27979;&#33539;&#24335;&#21644;&#35780;&#20272;&#26631;&#20934;&#26041;&#38754;&#19982;&#23454;&#38469;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#21644;&#35268;&#27169;&#30340;&#34067;&#24310;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#19982;&#23454;&#38469;&#24037;&#19994;&#31995;&#32479;&#30340;&#38656;&#27714;&#30456;&#27604;&#65292;&#29616;&#26377;&#31639;&#27861;&#22312;&#35757;&#32451;&#33539;&#24335;&#12289;&#22312;&#32447;&#26816;&#27979;&#33539;&#24335;&#21644;&#35780;&#20272;&#26631;&#20934;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#24403;&#21069;&#31639;&#27861;&#36890;&#24120;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#26102;&#38388;&#24207;&#21015;&#35757;&#32451;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#65292;&#28982;&#32780;&#22312;&#20855;&#26377;&#25968;&#20197;&#19975;&#35745;&#26354;&#32447;&#30340;&#22823;&#35268;&#27169;&#22312;&#32447;&#31995;&#32479;&#20013;&#65292;&#32500;&#25252;&#36825;&#20040;&#22810;&#27169;&#22411;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20165;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#26469;&#26816;&#27979;&#24322;&#24120;&#30340;&#24615;&#33021;&#23578;&#19981;&#26126;&#30830;&#12290;&#22823;&#22810;&#25968;TSAD&#27169;&#22411;&#37117;&#26159;&#22312;&#26102;&#38388;&#24207;&#21015;&#30340;&#21382;&#21490;&#37096;&#20998;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20854;&#26410;&#26469;&#37096;&#20998;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#65292;&#32463;&#24120;&#37096;&#32626;&#21644;&#21319;&#32423;&#31995;&#32479;&#65292;&#27599;&#22825;&#37117;&#20250;&#20986;&#29616;&#26032;&#30340;&#12289;&#20197;&#21069;&#27809;&#26377;&#35265;&#36807;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#25152;&#35757;&#32451;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;&#26032;&#26102;&#38388;&#24207;&#21015;&#30340;&#24615;&#33021;&#20063;&#19981;&#26126;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10802v1 Announce Type: new  Abstract: Driven by the proliferation of real-world application scenarios and scales, time series anomaly detection (TSAD) has attracted considerable scholarly and industrial interest. However, existing algorithms exhibit a gap in terms of training paradigm, online detection paradigm, and evaluation criteria when compared to the actual needs of real-world industrial systems. Firstly, current algorithms typically train a specific model for each individual time series. In a large-scale online system with tens of thousands of curves, maintaining such a multitude of models is impractical. The performance of using merely one single unified model to detect anomalies remains unknown. Secondly, most TSAD models are trained on the historical part of a time series and are tested on its future segment. In distributed systems, however, there are frequent system deployments and upgrades, with new, previously unseen time series emerging daily. The performance o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;DDPM&#21453;&#36716;&#36827;&#34892;&#38899;&#39057;&#20449;&#21495;&#30340;&#38646;&#26679;&#26412;&#32534;&#36753;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#25991;&#26412;&#30340;&#32534;&#36753;&#21644;&#26080;&#30417;&#30563;&#21457;&#29616;&#32534;&#36753;&#26041;&#21521;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#38899;&#20048;&#20449;&#21495;&#20013;&#23637;&#29616;&#20102;&#22810;&#26679;&#30340;&#38899;&#20048;&#20852;&#36259;&#20462;&#25913;&#12290;</title><link>https://arxiv.org/abs/2402.10009</link><description>&lt;p&gt;
&#20351;&#29992;DDPM&#21453;&#36716;&#36827;&#34892;&#38646;&#26679;&#26412;&#26080;&#30417;&#30563;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;DDPM&#21453;&#36716;&#36827;&#34892;&#38899;&#39057;&#20449;&#21495;&#30340;&#38646;&#26679;&#26412;&#32534;&#36753;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#25991;&#26412;&#30340;&#32534;&#36753;&#21644;&#26080;&#30417;&#30563;&#21457;&#29616;&#32534;&#36753;&#26041;&#21521;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#38899;&#20048;&#20449;&#21495;&#20013;&#23637;&#29616;&#20102;&#22810;&#26679;&#30340;&#38899;&#20048;&#20852;&#36259;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#32534;&#36753;&#24050;&#32463;&#22312;&#22270;&#20687;&#39046;&#22495;&#21462;&#24471;&#20102;&#36805;&#29467;&#30340;&#21457;&#23637;&#65292;&#20294;&#22312;&#38899;&#39057;&#39046;&#22495;&#23578;&#26410;&#20986;&#29616;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#22522;&#20110;DDPM&#21453;&#36716;&#30340;&#38899;&#39057;&#20449;&#21495;&#38646;&#26679;&#26412;&#32534;&#36753;&#25216;&#26415;&#12290;&#31532;&#19968;&#31181;&#26159;&#20174;&#22270;&#20687;&#39046;&#22495;&#37319;&#29992;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#22522;&#20110;&#25991;&#26412;&#36827;&#34892;&#32534;&#36753;&#12290;&#31532;&#20108;&#31181;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#21457;&#29616;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#32534;&#36753;&#26041;&#21521;&#12290;&#24403;&#24212;&#29992;&#20110;&#38899;&#20048;&#20449;&#21495;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23637;&#29616;&#20986;&#19968;&#31995;&#21015;&#20855;&#26377;&#38899;&#20048;&#20852;&#36259;&#30340;&#20462;&#25913;&#65292;&#20174;&#25511;&#21046;&#29305;&#23450;&#20048;&#22120;&#30340;&#21442;&#19982;&#21040;&#23545;&#26059;&#24459;&#36827;&#34892;&#21363;&#20852;&#28436;&#22863;&#12290;&#31034;&#20363;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#20363;&#23376;&#39029;&#38754;&#20013;&#25214;&#21040;&#65306;https://hilamanor.github.io/AudioEditing/ &#65292;&#20195;&#30721;&#21487;&#20197;&#22312; https://github.com/hilamanor/AudioEditing/ &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10009v1 Announce Type: cross  Abstract: Editing signals using large pre-trained models, in a zero-shot manner, has recently seen rapid advancements in the image domain. However, this wave has yet to reach the audio domain. In this paper, we explore two zero-shot editing techniques for audio signals, which use DDPM inversion on pre-trained diffusion models. The first, adopted from the image domain, allows text-based editing. The second, is a novel approach for discovering semantically meaningful editing directions without supervision. When applied to music signals, this method exposes a range of musically interesting modifications, from controlling the participation of specific instruments to improvisations on the melody. Samples can be found on our examples page in https://hilamanor.github.io/AudioEditing/ and code can be found in https://github.com/hilamanor/AudioEditing/ .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;Transformer&#26550;&#26500;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#31354;&#38388;&#21463;&#21040;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;Transformer&#23545;&#25935;&#24863;&#20989;&#25968;&#30340;&#22256;&#38590;&#12290;&#36825;&#19968;&#29702;&#35770;&#32479;&#19968;&#20102;&#20851;&#20110;Transformer&#23398;&#20064;&#33021;&#21147;&#21644;&#20559;&#35265;&#30340;&#24191;&#27867;&#35266;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.09963</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;Transformer&#23545;&#25935;&#24863;&#20989;&#25968;&#22256;&#38590;?
&lt;/p&gt;
&lt;p&gt;
Why are Sensitive Functions Hard for Transformers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;Transformer&#26550;&#26500;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#31354;&#38388;&#21463;&#21040;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;Transformer&#23545;&#25935;&#24863;&#20989;&#25968;&#30340;&#22256;&#38590;&#12290;&#36825;&#19968;&#29702;&#35770;&#32479;&#19968;&#20102;&#20851;&#20110;Transformer&#23398;&#20064;&#33021;&#21147;&#21644;&#20559;&#35265;&#30340;&#24191;&#27867;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#65292;Transformer&#23384;&#22312;&#19968;&#31995;&#21015;&#30340;&#23398;&#20064;&#20559;&#35265;&#21644;&#38480;&#21046;&#65292;&#22914;&#22312;&#23398;&#20064;&#35745;&#31639;&#31616;&#21333;&#24418;&#24335;&#35821;&#35328;&#65288;&#22914;PARITY&#65289;&#26102;&#30340;&#25345;&#20037;&#22256;&#38590;&#65292;&#20197;&#21450;&#23545;&#20302;&#38454;&#20989;&#25968;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#29702;&#35770;&#35201;&#20040;&#36807;&#24230;&#39044;&#27979;&#65292;&#35201;&#20040;&#20302;&#20272;&#20102;&#23454;&#38469;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;Transformer&#26550;&#26500;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#31354;&#38388;&#21463;&#21040;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#38480;&#21046;&#65306;&#36755;&#20986;&#23545;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#22810;&#20010;&#37096;&#20998;&#25935;&#24863;&#30340;Transformer&#23384;&#22312;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#23396;&#31435;&#28857;&#65292;&#23548;&#33268;&#27867;&#21270;&#20013;&#30340;&#20302;&#25935;&#24863;&#24615;&#20559;&#24046;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#35813;&#29702;&#35770;&#32479;&#19968;&#20102;&#20851;&#20110;Transformer&#23398;&#20064;&#33021;&#21147;&#21644;&#20559;&#35265;&#30340;&#24191;&#27867;&#35266;&#23519;&#65292;&#22914;&#23427;&#20204;&#23545;&#20302;&#25935;&#24863;&#24615;&#21644;&#20302;&#38454;&#30340;&#27867;&#21270;&#20559;&#24046;&#65292;&#20197;&#21450;&#22312;&#38271;&#24230;&#27867;&#21270;&#19978;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09963v1 Announce Type: new  Abstract: Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalizati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#21338;&#24328;&#20013;&#36827;&#34892;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#31639;&#27861;&#19982;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.09456</link><description>&lt;p&gt;
&#26410;&#30693;&#21338;&#24328;&#20013;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#29992;&#20110;&#26080;&#36951;&#25022;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimistic Thompson Sampling for No-Regret Learning in Unknown Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#21338;&#24328;&#20013;&#36827;&#34892;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#31639;&#27861;&#19982;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28041;&#21450;&#22810;&#20010;&#20915;&#31574;&#32773;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#21487;&#20197;&#24314;&#27169;&#20026;&#19968;&#20010;&#20855;&#26377;&#37096;&#20998;&#35266;&#27979;&#30340;&#26410;&#30693;&#21338;&#24328;&#12290;&#20026;&#20102;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#21644;&#22810;&#26426;&#26500;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#27748;&#26222;&#26862;&#25277;&#26679;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22914;&#20132;&#36890;&#36335;&#30001;&#21644;&#38647;&#36798;&#24863;&#30693;&#20013;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#19982;&#22522;&#20934;&#31639;&#27861;&#30456;&#27604;&#65292;&#20943;&#23569;&#20102;&#21313;&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23545;&#22870;&#21169;&#32467;&#26500;&#26377;&#19968;&#23450;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36951;&#25022;&#30028;&#38480;&#20165;&#23545;&#24635;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21576;&#23545;&#25968;&#20381;&#36182;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21644;&#39046;&#22495;&#20869;&#29616;&#26377;&#30340;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#26159;&#19968;&#39033;&#26032;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09456v1 Announce Type: cross  Abstract: Many real-world problems involving multiple decision-makers can be modeled as an unknown game characterized by partial observations. Addressing the challenges posed by partial information and the curse of multi-agency, we developed Thompson sampling-type algorithms, leveraging information about opponent's action and reward structures. Our approach significantly reduces experimental budgets, achieving a more than tenfold reduction compared to baseline algorithms in practical applications like traffic routing and radar sensing. We demonstrate that, under certain assumptions about the reward structure, the regret bound exhibits merely a logarithmic dependence on the total action space size, effectively mitigating the curse of multi-agency. Additionally, this research introduces the Optimism-then-NoRegret framework, a novel contribution that integrates both our proposed methodologies and existing algorithms in the field.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.09283</link><description>&lt;p&gt;
&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#24212;&#29992;&#20013;&#24050;&#32463;&#24456;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#22238;&#22797;&#30340;&#39118;&#38505;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#24182;&#28608;&#21457;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#27492;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#25688;&#35201;&#65292;&#22686;&#36827;&#23545;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#37325;&#35201;&#35838;&#39064;&#12290;&#20026;&#20102;&#26041;&#20415;&#21442;&#32771;&#65292;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#23545;&#25152;&#26377;&#22312;&#27492;&#35843;&#26597;&#20013;&#25552;&#21040;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/niconi19/LLM-conversation-safety&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#23545;&#20110;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#30446;&#26631;&#27169;&#24335;&#30340;&#21516;&#24577;&#35745;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#21147;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.08595</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#65306;&#20851;&#20110;&#22522;&#30784;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Homomorphism Counts for Graph Neural Networks: All About That Basis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#23545;&#20110;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#30446;&#26631;&#27169;&#24335;&#30340;&#21516;&#24577;&#35745;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#21147;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#23398;&#20064;&#22270;&#19978;&#19981;&#21464;&#20989;&#25968;&#30340;&#26550;&#26500;&#12290;&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#36136;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20123;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#19982;&#20854;&#34920;&#36798;&#33021;&#21147;&#30456;&#20851;&#30340;&#38480;&#21046;&#12290;&#23427;&#20204;&#26080;&#27861;&#35745;&#25968;&#22270;&#20013;&#30340;&#26576;&#20123;&#27169;&#24335;&#65288;&#20363;&#22914;&#24490;&#29615;&#65289;&#26159;&#36825;&#20123;&#38480;&#21046;&#30340;&#26680;&#24515;&#65292;&#22240;&#20026;&#35768;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#20989;&#25968;&#20381;&#36182;&#20110;&#35745;&#25968;&#36825;&#20123;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20004;&#31181;&#31361;&#20986;&#30340;&#33539;&#20363;&#26088;&#22312;&#36890;&#36807;&#20016;&#23500;&#22270;&#29305;&#24449;&#30340;&#23376;&#22270;&#25110;&#21516;&#24577;&#27169;&#24335;&#35745;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#37117;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#20027;&#24352;&#37319;&#29992;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#27169;&#24335;&#30340;&#8220;&#22522;&#30784;&#8221;&#20013;&#30340;&#21516;&#24577;&#35745;&#25968;&#32435;&#20837;&#32771;&#34385;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20135;&#29983;&#20102;&#26356;&#21152;&#34920;&#36798;&#21147;&#30340;&#26550;&#26500;&#65292;&#32780;&#19981;&#20250;&#24102;&#26469;&#20219;&#20309;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#29702;&#35770;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are architectures for learning invariant functions over graphs. A large body of work has investigated the properties of graph neural networks and identified several limitations, particularly pertaining to their expressive power. Their inability to count certain patterns (e.g., cycles) in a graph lies at the heart of such limitations, since many functions to be learned rely on the ability of counting such patterns. Two prominent paradigms aim to address this limitation by enriching the graph features with subgraph or homomorphism pattern counts. In this work, we show that both of these approaches are sub-optimal in a certain sense and argue for a more fine-grained approach, which incorporates the homomorphism counts of all structures in the "basis" of the target pattern. This yields strictly more expressive architectures without incurring any additional overhead in terms of computational complexity compared to existing approaches. We prove a series of theoretical r
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08277</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#21644;&#24378;&#22823;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#19987;&#23478;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#24544;&#23454;&#21644;&#21487;&#36861;&#36394;&#30340;&#31572;&#26696;&#30340;&#36827;&#27493;&#23545;&#20110;&#21508;&#31181;&#30740;&#31350;&#21644;&#23454;&#36341;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#21487;&#38752;&#30340;&#26469;&#28304;&#25552;&#20379;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#22312;&#20351;&#29992;LLM&#26102;&#24050;&#32463;&#35777;&#26126;&#22312;&#24341;&#29992;&#27491;&#30830;&#30340;&#26469;&#28304;&#65288;&#26469;&#28304;&#36136;&#37327;&#65289;&#21644;&#20934;&#30830;&#22320;&#34920;&#31034;&#26469;&#28304;&#20013;&#30340;&#20449;&#24687;&#65288;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65289;&#26041;&#38754;&#24037;&#20316;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;LLM&#65292;&#20197;&#25552;&#39640;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#22120;&#65292;&#21487;&#20197;&#22823;&#35268;&#27169;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#23545;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;%&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#35780;&#20272;&#30340;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#35780;&#20272;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#21253;&#25324;&#26641;&#32467;&#26500;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#35805;&#30340;&#30495;&#30456;&#12290;</title><link>https://arxiv.org/abs/2402.07281</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#26641;&#32467;&#26500;&#26041;&#27861;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21542;&#36229;&#36234;&#28145;&#24230;&#23398;&#20064;&#65311;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;&#12299;
&lt;/p&gt;
&lt;p&gt;
Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#21253;&#25324;&#26641;&#32467;&#26500;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#35805;&#30340;&#30495;&#30456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30830;&#20445;&#26381;&#21153;&#36830;&#32493;&#24615;&#26102;&#65292;&#22797;&#26434;&#30340;&#20851;&#38190;&#20219;&#21153;&#31995;&#32479;&#20013;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#24322;&#24120;&#20107;&#20214;&#34987;&#35748;&#20026;&#26159;&#32597;&#35265;&#20107;&#20214;&#65292;&#22240;&#27492;&#20174;&#25805;&#20316;&#25968;&#25454;&#20013;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#38754;&#20020;&#30528;&#31867;&#21035;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;&#35770;&#25991;&#36890;&#36807;&#23545;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#20844;&#27491;&#27604;&#36739;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#21253;&#25324;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;&#21508;&#31181;&#22522;&#20110;&#26641;&#32467;&#26500;&#30340;&#26041;&#27861;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#24322;&#24120;&#28857;&#26816;&#27979;&#26041;&#27861;&#12290;&#35770;&#25991;&#20351;&#29992;&#20102;104&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21644;&#23569;&#25968;&#19987;&#26377;&#30340;&#24037;&#19994;&#31995;&#32479;&#25968;&#25454;&#38598;&#65292;&#22686;&#24378;&#20102;&#30740;&#31350;&#30340;&#22810;&#26679;&#24615;&#65292;&#20351;&#31639;&#27861;&#24615;&#33021;&#30340;&#35780;&#20272;&#26356;&#21152;&#30495;&#23454;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23454;&#38469;&#22330;&#26223;&#30340;&#36866;&#24212;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#35770;&#25991;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#35805;&#30340;&#30495;&#30456;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of anomalous situations for complex mission-critical systems holds paramount importance when their service continuity needs to be ensured. A major challenge in detecting anomalies from the operational data arises due to the imbalanced class distribution problem since the anomalies are supposed to be rare events. This paper evaluates a diverse array of machine learning-based anomaly detection algorithms through a comprehensive benchmark study. The paper contributes significantly by conducting an unbiased comparison of various anomaly detection algorithms, spanning classical machine learning including various tree-based approaches to deep learning and outlier detection methods. The inclusion of 104 publicly available and a few proprietary industrial systems datasets enhances the diversity of the study, allowing for a more realistic evaluation of algorithm performance and emphasizing the importance of adaptability to real-world scenarios. The paper dispels the deep learning myth
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.05391</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05391
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#25512;&#21160;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#35821;&#20041;&#32593;&#32476;&#31038;&#21306;&#23545;&#22810;&#27169;&#24577;&#32500;&#24230;&#30340;&#25506;&#32034;&#20026;&#21019;&#26032;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;300&#22810;&#31687;&#25991;&#31456;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#24863;&#30693;&#30740;&#31350;&#65306;&#20197;&#30693;&#35782;&#22270;&#35889;&#25903;&#25345;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;KG&#39537;&#21160;&#22810;&#27169;&#24577;&#65288;KG4MM&#65289;&#23398;&#20064;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MM4KG&#65289;&#39046;&#22495;&#12290;&#25105;&#20204;&#20174;&#23450;&#20041;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#65292;&#28982;&#21518;&#25506;&#32034;&#23427;&#20204;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#31867;&#21035;&#65306;KG&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#22914;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#31361;&#20986;&#20102;&#20855;&#20307;&#30340;&#30740;&#31350;&#36712;&#36857;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#12289;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#20986;&#36827;&#34892;&#30456;&#20851;&#30740;&#31350;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;cu
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#20026;&#31227;&#21160;&#35774;&#22791;&#21644;&#20113;&#20043;&#38388;&#30340;&#35745;&#31639;&#65292;&#20197;&#20943;&#36731;&#31227;&#21160;&#35774;&#22791;&#30340;&#36127;&#25285;&#65292;&#24182;&#20248;&#21270;&#20113;&#31471;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;</title><link>https://arxiv.org/abs/2402.04880</link><description>&lt;p&gt;
&#32467;&#21512;&#20113;&#35745;&#31639;&#19982;&#31227;&#21160;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Combining Cloud and Mobile Computing for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04880
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#20026;&#31227;&#21160;&#35774;&#22791;&#21644;&#20113;&#20043;&#38388;&#30340;&#35745;&#31639;&#65292;&#20197;&#20943;&#36731;&#31227;&#21160;&#35774;&#22791;&#30340;&#36127;&#25285;&#65292;&#24182;&#20248;&#21270;&#20113;&#31471;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31227;&#21160;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#27491;&#22312;&#22686;&#21152;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#23567;&#20063;&#22312;&#22686;&#38271;&#12290;&#36825;&#31181;&#36235;&#21183;&#32473;&#31227;&#21160;&#35774;&#22791;&#24102;&#26469;&#20102;&#38382;&#39064;&#65292;&#22914;&#20869;&#23384;&#23481;&#37327;&#21644;&#30005;&#27744;&#23551;&#21629;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#35768;&#22810;&#26381;&#21153;&#65288;&#22914;ChatGPT&#21644;Midjourney&#65289;&#22312;&#20113;&#20013;&#36816;&#34892;&#25152;&#26377;&#30340;&#25512;&#29702;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#28789;&#27963;&#24615;&#21644;&#32454;&#31890;&#24230;&#30340;&#20219;&#21153;&#20998;&#37197;&#26356;&#21487;&#21462;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#20998;&#21106;&#35270;&#20026;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#35745;&#31639;&#20998;&#21106;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#20113;&#20043;&#38388;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#30340;&#35745;&#31639;&#23494;&#38598;&#37096;&#20998;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20998;&#21106;&#19981;&#20165;&#20943;&#23569;&#20102;&#29992;&#25143;&#31561;&#24453;&#26102;&#38388;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#32454;&#31890;&#24230;&#35843;&#25972;&#26469;&#20248;&#21270;&#20113;&#31471;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35843;&#24230;&#22120;&#65292;&#25910;&#38598;&#32593;&#32476;&#36136;&#37327;&#12289;&#23458;&#25143;&#31471;&#35774;&#22791;&#33021;&#21147;&#21644;&#20316;&#19994;&#35201;&#27714;&#30340;&#20449;&#24687;&#65292;&#20570;&#20986;&#20915;&#31574;&#20197;&#23454;&#29616;&#22312;&#21508;&#31181;&#35774;&#22791;&#19978;&#30340;&#19968;&#33268;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the computing power of mobile devices is increasing, machine learning models are also growing in size. This trend creates problems for mobile devices due to limitations like their memory capacity and battery life. While many services, like ChatGPT and Midjourney, run all the inferences in the cloud, we believe a flexible and fine-grained task distribution is more desirable. In this work, we consider model segmentation as a solution to improving the user experience, dividing the computation between mobile devices and the cloud in a way that offloads the compute-heavy portion of the model while minimizing the data transfer required. We show that the division not only reduces the wait time for users but can also be fine-tuned to optimize the workloads of the cloud. To achieve that, we design a scheduler that collects information about network quality, client device capability, and job requirements, making decisions to achieve consistent performance across a range of devices while
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.04875</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Provable Length and Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#26356;&#38271;&#24207;&#21015;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#20196;&#29260;&#32452;&#21512;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#26159;&#37325;&#35201;&#30340;&#38750;&#20998;&#24067;&#21270;&#27867;&#21270;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#26550;&#26500;&#20013;&#65292;&#26397;&#30528;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#26681;&#25454;&#26550;&#26500;&#30340;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#30340;&#24517;&#35201;&#24615;&#65292;&#20363;&#22914;&#19982;&#30495;&#23454;&#34920;&#31034;&#20855;&#26377;&#32447;&#24615;&#25110;&#25490;&#21015;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#21487;&#24494;&#21270;&#65292;dSGP4&#23454;&#29616;&#20102;&#36712;&#36947;&#20256;&#25773;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#21355;&#26143;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#20256;&#25773;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.04830</link><description>&lt;p&gt;
&#32553;&#23567;SGP4&#21644;&#39640;&#31934;&#24230;&#20256;&#25773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#36890;&#36807;&#21487;&#24494;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#21487;&#24494;&#21270;&#65292;dSGP4&#23454;&#29616;&#20102;&#36712;&#36947;&#20256;&#25773;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#21355;&#26143;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#20256;&#25773;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21270;&#30340;&#31532;&#22235;&#32423;&#25668;&#21160;(SGP4)&#36712;&#36947;&#20256;&#25773;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#24555;&#36895;&#21487;&#38752;&#22320;&#39044;&#27979;&#22320;&#29699;&#36712;&#36947;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#12290;&#23613;&#31649;&#19981;&#26029;&#25913;&#36827;&#65292;SGP&#27169;&#22411;&#20173;&#28982;&#32570;&#20047;&#25968;&#20540;&#20256;&#25773;&#22120;&#30340;&#31934;&#24230;&#65292;&#21518;&#32773;&#30340;&#35823;&#24046;&#26174;&#33879;&#36739;&#23567;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#26032;&#22411;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#20351;SGP4&#21487;&#24494;&#21270;&#65292;dSGP4&#20415;&#20110;&#36827;&#34892;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33322;&#22825;&#22120;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#36716;&#25442;&#12289;&#29366;&#24577;&#36716;&#31227;&#30697;&#38453;&#35745;&#31639;&#21644;&#21327;&#26041;&#24046;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;dSGP4&#30340;PyTorch&#23454;&#29616;&#20801;&#35768;&#22312;&#25209;&#37327;&#30340;TLE&#65288;&#20004;&#34892;&#21442;&#25968;&#65289;&#38598;&#19978;&#36827;&#34892;&#23604;&#23596;&#30340;&#24182;&#34892;&#36712;&#36947;&#20256;&#25773;&#65292;&#21033;&#29992;CPU&#12289;GPU&#21644;&#20998;&#24067;&#24335;&#39044;&#27979;&#21355;&#26143;&#20301;&#32622;&#30340;&#39640;&#32423;&#30828;&#20214;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;dSGP4&#30340;&#21487;&#24494;&#24615;&#20351;&#20854;&#33021;&#19982;&#27169;&#24335;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Simplified General Perturbations 4 (SGP4) orbital propagation method is widely used for predicting the positions and velocities of Earth-orbiting objects rapidly and reliably. Despite continuous refinement, SGP models still lack the precision of numerical propagators, which offer significantly smaller errors. This study presents dSGP4, a novel differentiable version of SGP4 implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates various space-related applications, including spacecraft orbit determination, state conversion, covariance transformation, state transition matrix computation, and covariance propagation. Additionally, dSGP4's PyTorch implementation allows for embarrassingly parallel orbital propagation across batches of Two-Line Element Sets (TLEs), leveraging the computational power of CPUs, GPUs, and advanced hardware for distributed prediction of satellite positions at future times. Furthermore, dSGP4's differentiability enables integration with mode
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#20869;&#23384;&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35299;&#20915;&#20102;&#35757;&#32451;&#20013;&#30340;&#26102;&#38388;&#38388;&#26029;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#20869;&#23384;&#27169;&#22359;&#25552;&#21462;&#21644;&#35760;&#24518;&#38271;&#26399;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;MDGNNs&#22312;&#22788;&#29702;&#22823;&#30340;&#26102;&#38388;&#25209;&#37327;&#26102;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04284</link><description>&lt;p&gt;
PRES: &#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#20869;&#23384;&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04284
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#20869;&#23384;&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35299;&#20915;&#20102;&#35757;&#32451;&#20013;&#30340;&#26102;&#38388;&#38388;&#26029;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#20869;&#23384;&#27169;&#22359;&#25552;&#21462;&#21644;&#35760;&#24518;&#38271;&#26399;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;MDGNNs&#22312;&#22788;&#29702;&#22823;&#30340;&#26102;&#38388;&#25209;&#37327;&#26102;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20869;&#23384;&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MDGNNs&#65289;&#26159;&#19968;&#31867;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#20869;&#23384;&#27169;&#22359;&#25552;&#21462;&#12289;&#25552;&#28860;&#21644;&#35760;&#24518;&#38271;&#26399;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#30456;&#27604;&#20110;&#26080;&#20869;&#23384;&#30340;&#23545;&#24212;&#29289;&#65292;&#34920;&#29616;&#20986;&#26356;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;MDGNNs&#38754;&#20020;&#30528;&#22788;&#29702;&#32416;&#32467;&#30340;&#26102;&#38388;&#21644;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#23545;&#25968;&#25454;&#24207;&#21015;&#36827;&#34892;&#39034;&#24207;&#21644;&#26102;&#38388;&#39034;&#24207;&#30340;&#22788;&#29702;&#65292;&#20197;&#25429;&#25417;&#20934;&#30830;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#22312;&#25209;&#37327;&#35757;&#32451;&#20013;&#65292;&#21516;&#19968;&#25209;&#27425;&#20869;&#30340;&#26102;&#38388;&#25968;&#25454;&#28857;&#23558;&#34987;&#24182;&#34892;&#22788;&#29702;&#65292;&#32780;&#23427;&#20204;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#23558;&#34987;&#24573;&#35270;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#26102;&#38388;&#38388;&#26029;&#65292;&#38480;&#21046;&#20102;&#26377;&#25928;&#30340;&#26102;&#38388;&#25209;&#37327;&#22823;&#23567;&#65292;&#38480;&#21046;&#20102;&#25968;&#25454;&#30340;&#24182;&#34892;&#24615;&#65292;&#24182;&#38477;&#20302;&#20102;MDGNNs&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;MDGNNs&#30340;&#22823;&#35268;&#27169;&#39640;&#25928;&#35757;&#32451;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#22823;&#30340;&#26102;&#38388;&#25209;&#37327;&#22823;&#23567;&#19979;&#35757;&#32451;MDGNNs&#26102;&#30340;&#26102;&#38388;&#38388;&#26029;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#29702;&#35770;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory-based Dynamic Graph Neural Networks (MDGNNs) are a family of dynamic graph neural networks that leverage a memory module to extract, distill, and memorize long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, training MDGNNs faces the challenge of handling entangled temporal and structural dependencies, requiring sequential and chronological processing of data sequences to capture accurate temporal patterns. During the batch training, the temporal data points within the same batch will be processed in parallel, while their temporal dependencies are neglected. This issue is referred to as temporal discontinuity and restricts the effective temporal batch size, limiting data parallelism and reducing MDGNNs' flexibility in industrial applications. This paper studies the efficient training of MDGNNs at scale, focusing on the temporal discontinuity in training MDGNNs with large temporal batch sizes. We first conduct a theoretic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RevOrder&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;nD&#20056;&#20197;1D&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#36816;&#31639;&#12290;&#32463;&#36807;&#20840;&#38754;&#27979;&#35797;&#65292;RevOrder&#22312;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#25968;&#26102;&#12290;&#22312;GSM8K&#25968;&#23398;&#20219;&#21153;&#20013;&#24212;&#29992;RevOrder&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#38169;&#35823;&#29575;&#24182;&#25552;&#39640;&#20102;&#24635;&#20307;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.03822</link><description>&lt;p&gt;
RevOrder&#65306;&#19968;&#31181;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RevOrder: A Novel Method for Enhanced Arithmetic in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RevOrder&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;nD&#20056;&#20197;1D&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#36816;&#31639;&#12290;&#32463;&#36807;&#20840;&#38754;&#27979;&#35797;&#65292;RevOrder&#22312;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#25968;&#26102;&#12290;&#22312;GSM8K&#25968;&#23398;&#20219;&#21153;&#20013;&#24212;&#29992;RevOrder&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#38169;&#35823;&#29575;&#24182;&#25552;&#39640;&#20102;&#24635;&#20307;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RevOrder&#65292;&#19968;&#31181;&#26088;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;n&#20301;&#25968;&#20056;&#20197;1&#20301;&#25968;&#65288;nD&#20056;&#20197;1D&#65289;&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39034;&#24207;&#20013;&#38388;&#25968;&#23383;&#30340;&#25968;&#37327; (CSID)&#65292;&#36825;&#26159;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#31181;&#35780;&#20272;&#26041;&#31243;&#22797;&#26434;&#24615;&#30340;&#26032;&#24230;&#37327;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#27979;&#35797;&#65292;RevOrder&#19981;&#20165;&#22312;&#22522;&#26412;&#30340;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#19988;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20256;&#32479;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#30340;&#22823;&#25968;&#24773;&#20917;&#19979;&#12290;RevOrder&#30340;&#23454;&#29616;&#23545;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#37117;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#27492;&#22806;&#65292;&#23558;RevOrder&#24212;&#29992;&#20110;&#23545;GSM8K&#25968;&#23398;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;LLaMA2-7B&#27169;&#22411;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#23558;&#26041;&#31243;&#35745;&#31639;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;46%&#65292;&#23558;&#24635;&#20307;&#24471;&#20998;&#20174;41.6&#25552;&#21319;&#21040;44.4&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03780</link><description>&lt;p&gt;
&#25581;&#31034;&#23459;&#20256;&#65306;&#22522;&#20110;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#35821;&#35328;&#21450;&#20854;&#25991;&#20307;&#29305;&#24449;&#12290;&#25552;&#20986;&#20102;PPN&#25968;&#25454;&#38598;&#65292;&#21363;&#23459;&#20256;&#24615;&#20266;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26426;&#26500;&#30830;&#23450;&#30340;&#23459;&#20256;&#26469;&#28304;&#32593;&#31449;&#19978;&#30340;&#26032;&#38395;&#25991;&#31456;&#32452;&#25104;&#12290;&#20174;&#35813;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#20102;&#19968;&#37096;&#20998;&#26679;&#26412;&#19982;&#26469;&#33258;&#24120;&#35268;&#27861;&#22269;&#26032;&#38395;&#30340;&#25991;&#31456;&#28151;&#21512;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;URL&#36827;&#34892;&#20102;&#25513;&#30422;&#65292;&#20197;&#36827;&#34892;&#20154;&#31867;&#27880;&#37322;&#23454;&#39564;&#65292;&#20351;&#29992;11&#20010;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#20004;&#31181;&#31867;&#22411;&#30340;&#26032;&#38395;&#32440;&#23545;&#27599;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#35782;&#21035;&#27880;&#37322;&#32773;&#20351;&#29992;&#30340;&#32447;&#32034;&#65292;&#24182;&#19982;&#26426;&#22120;&#20998;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;VAGO&#20998;&#26512;&#22120;&#36827;&#34892;&#36766;&#36848;&#27169;&#31946;&#21644;&#20027;&#35266;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#29992;TF-IDF&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#21450;&#22235;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;&#20004;&#20010;&#22522;&#20110;RoBERTa&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#21477;&#27861;&#30340;CATS&#65292;&#20197;&#21450;&#32467;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#19968;&#20010;XGBoost&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;&#31639;&#27861;&#65288;D-SOBA&#65289;&#65292;&#39318;&#27425;&#38416;&#26126;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#22312;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03167</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;: &#26080;&#29615;&#31639;&#27861;&#26356;&#26032;&#21644;&#30636;&#24577;&#36845;&#20195;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;&#31639;&#27861;&#65288;D-SOBA&#65289;&#65292;&#39318;&#27425;&#38416;&#26126;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#22312;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21452;&#32423;&#20248;&#21270;&#65288;SBO&#65289;&#22312;&#22788;&#29702;&#23884;&#22871;&#32467;&#26500;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#20351;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;SBO&#65292;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#20316;&#20026;&#26377;&#25928;&#30340;&#33539;&#20363;&#20986;&#29616;&#65292;&#20854;&#20013;&#33410;&#28857;&#19982;&#30452;&#25509;&#30456;&#37051;&#33410;&#28857;&#36827;&#34892;&#36890;&#20449;&#65292;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21644;&#22686;&#24378;&#31639;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21435;&#20013;&#24515;&#21270;SBO&#31639;&#27861;&#38754;&#20020;&#25361;&#25112;&#65292;&#21253;&#25324;&#26114;&#36149;&#30340;&#20869;&#37096;&#24490;&#29615;&#26356;&#26032;&#21644;&#23545;&#32593;&#32476;&#25299;&#25169;&#12289;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#23884;&#22871;&#21452;&#32423;&#31639;&#27861;&#32467;&#26500;&#30340;&#24433;&#21709;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;SBO&#65288;D-SOBA&#65289;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#30636;&#24577;&#36845;&#20195;&#22797;&#26434;&#24615;&#65292;&#39318;&#27425;&#28548;&#28165;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, current decentralized SBO algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. In this paper, we introduce a single-loop decentralized SBO (D-SOBA) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and transien
&lt;/p&gt;</description></item><item><title>Spin&#26159;&#19968;&#20010;GPU&#21152;&#36895;&#30340;&#22810;&#26041;&#35745;&#31639;(MPC)&#26694;&#26550;&#65292;&#25903;&#25345;&#22810;&#20010;&#35745;&#31639;&#26041;&#21644;&#19981;&#35802;&#23454;&#22810;&#25968;&#23545;&#25239;&#35774;&#32622;&#12290;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#20851;&#38190;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#20248;&#21270;&#21327;&#35758;&#65292;&#24182;&#36827;&#34892;&#20102;&#38024;&#23545;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#30340;&#26032;&#39062;&#20248;&#21270;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#23433;&#20840;&#30340;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.02320</link><description>&lt;p&gt;
Spin: &#19968;&#31181;&#20855;&#22791;GPU&#21152;&#36895;&#30340;&#39640;&#25928;&#23433;&#20840;&#35745;&#31639;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Spin: An Efficient Secure Computation Framework with GPU Acceleration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02320
&lt;/p&gt;
&lt;p&gt;
Spin&#26159;&#19968;&#20010;GPU&#21152;&#36895;&#30340;&#22810;&#26041;&#35745;&#31639;(MPC)&#26694;&#26550;&#65292;&#25903;&#25345;&#22810;&#20010;&#35745;&#31639;&#26041;&#21644;&#19981;&#35802;&#23454;&#22810;&#25968;&#23545;&#25239;&#35774;&#32622;&#12290;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#20851;&#38190;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#20248;&#21270;&#21327;&#35758;&#65292;&#24182;&#36827;&#34892;&#20102;&#38024;&#23545;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#30340;&#26032;&#39062;&#20248;&#21270;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#23433;&#20840;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#23545;&#20110;&#22810;&#26041;&#35745;&#31639;&#65288;MPC&#65289;&#26694;&#26550;&#20173;&#28982;&#26159;&#25361;&#25112;&#12290;Spin&#26159;&#19968;&#20010;&#25903;&#25345;&#22810;&#20010;&#35745;&#31639;&#26041;&#21644;&#19981;&#35802;&#23454;&#22810;&#25968;&#23545;&#25239;&#35774;&#32622;&#30340;GPU&#21152;&#36895;&#30340;MPC&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#20851;&#38190;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#20248;&#21270;&#21327;&#35758;&#65292;&#20197;&#21450;&#38024;&#23545;Transformer&#27169;&#22411;&#30340;&#22522;&#26412;&#21333;&#20803;&#27880;&#24847;&#21147;&#30340;&#20960;&#31181;&#26032;&#39062;&#20248;&#21270;&#65292;&#20351;Spin&#33021;&#22815;&#22312;&#19981;&#29306;&#29298;&#23433;&#20840;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#38750;&#24120;&#35268;CNN&#35757;&#32451;&#21644;Transformer&#25512;&#26029;&#12290;&#22312;&#21518;&#31471;&#23618;&#38754;&#65292;Spin&#21033;&#29992;GPU&#12289;CPU&#21644;RDMA&#21551;&#29992;&#30340;&#26234;&#33021;&#32593;&#32476;&#21345;&#36827;&#34892;&#21152;&#36895;&#12290;&#20840;&#38754;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Spin&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#38754;&#27604;&#26368;&#20808;&#36827;&#25216;&#26415;&#24555;&#20004;&#20493;&#12290;&#23545;&#20110;&#20855;&#26377;1890&#19975;&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#30340;&#25512;&#26029;&#65292;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#29305;&#23450;&#20248;&#21270;&#20351;Spin&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#29575;&#12289;&#26356;&#23569;&#30340;&#36890;&#20449;&#21644;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accuracy and efficiency remain challenges for multi-party computation (MPC) frameworks. Spin is a GPU-accelerated MPC framework that supports multiple computation parties and a dishonest majority adversarial setup. We propose optimized protocols for non-linear functions that are critical for machine learning, as well as several novel optimizations specific to attention that is the fundamental unit of Transformer models, allowing Spin to perform non-trivial CNNs training and Transformer inference without sacrificing security. At the backend level, Spin leverages GPU, CPU, and RDMA-enabled smart network cards for acceleration. Comprehensive evaluations demonstrate that Spin can be up to $2\times$ faster than the state-of-the-art for deep neural network training. For inference on a Transformer model with 18.9 million parameters, our attention-specific optimizations enable Spin to achieve better efficiency, less communication, and better accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.02229</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Vanilla Bayesian Optimization Performs Great in High Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#32500;&#38382;&#39064;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#36719;&#32907;&#12290;&#21463;&#21040;&#32500;&#24230;&#22122;&#38899;&#30340;&#21050;&#28608;&#65292;&#35768;&#22810;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#23545;&#30446;&#26631;&#24212;&#29992;&#21508;&#31181;&#31616;&#21270;&#20551;&#35774;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#23548;&#33268;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#19981;&#36866;&#29992;&#30340;&#36864;&#21270;&#29616;&#35937;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#26469;&#24212;&#23545;&#36825;&#20123;&#36864;&#21270;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#20013;&#20856;&#22411;&#20808;&#39564;&#20551;&#35774;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#23545;&#30446;&#26631;&#26045;&#21152;&#32467;&#26500;&#24615;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23558;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;&#21487;&#31649;&#29702;&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20462;&#25913;&#26041;&#27861;&#8212;&#8212;&#36890;&#36807;&#32500;&#24230;&#23545;&#39640;&#26031;&#36807;&#31243;&#38271;&#24230;&#20808;&#39564;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#8212;&#8212;&#25581;&#31034;&#20102;&#26631;&#20934;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#26126;&#30830;&#34920;&#26126;&#20854;&#25928;&#26524;&#36828;&#36828;&#36229;&#20986;&#20197;&#24448;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#20174;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#22478;&#24066;&#23398;&#20064;&#24182;&#39044;&#27979;&#20854;&#20182;&#22478;&#24066;&#30340;&#20132;&#36890;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.00397</link><description>&lt;p&gt;
&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00397
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#20174;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#22478;&#24066;&#23398;&#20064;&#24182;&#39044;&#27979;&#20854;&#20182;&#22478;&#24066;&#30340;&#20132;&#36890;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#23545;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21487;&#20197;&#24110;&#21161;&#39640;&#25928;&#20998;&#37197;&#36164;&#28304;&#21644;&#26377;&#25928;&#25511;&#21046;&#20132;&#36890;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#25928;&#24615;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#20016;&#23500;&#30340;&#20132;&#36890;&#25968;&#25454;&#65292;&#32780;&#35768;&#22810;&#22478;&#24066;&#30001;&#20110;&#35774;&#22791;&#25903;&#25345;&#26377;&#38480;&#32780;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#65292;&#36825;&#23545;&#20132;&#36890;&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#37492;&#20110;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#35266;&#23519;&#65306;&#20132;&#36890;&#27169;&#24335;&#22312;&#19981;&#21516;&#22478;&#24066;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#20851;&#38190;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#65288;MTPB&#65289;&#12290;&#20027;&#35201;&#19978;&#65292;MTPB&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20016;&#23500;&#30340;&#28304;&#22478;&#24066;&#21551;&#21160;&#20854;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#33719;&#21462;&#20840;&#38754;&#30340;&#20132;&#36890;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20808;&#36827;&#30340;&#32858;&#31867;&#25216;&#26415;&#20174;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#20013;&#31995;&#32479;&#29983;&#25104;&#19968;&#20010;&#22810;&#23610;&#24230;&#20132;&#36890;&#27169;&#24335;&#24211;&#12290;&#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20934;&#30830;&#30340;&#20132;&#36890;&#27169;&#24335;&#26816;&#32034;&#26426;&#21046;&#36827;&#34892;&#36328;&#22478;&#24066;&#30340;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control. However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting. Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities. Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank (MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process. Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge. Next, th
&lt;/p&gt;</description></item><item><title>FraiLT&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#36882;&#24402;&#26041;&#27861;&#21644;&#36845;&#20195;&#32534;&#30721;&#65292;&#23454;&#29616;&#20102;&#22312;&#32039;&#20945;&#24418;&#24335;&#19979;&#36798;&#21040;&#36739;&#22823;&#27169;&#22411;&#30340;&#35299;&#37322;&#28145;&#24230;&#65292;&#22312;&#24615;&#33021;&#34920;&#29616;&#19978;&#20248;&#20110;&#36739;&#22823;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#39640;&#25928;&#21644;&#21487;&#35775;&#38382;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.11626</link><description>&lt;p&gt;
&#33258;&#30001;&#38271;&#24605;&#21464;&#21387;&#22120;&#65288;FraiLT&#65289;
&lt;/p&gt;
&lt;p&gt;
Freely Long-Thinking Transformer (FraiLT)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11626
&lt;/p&gt;
&lt;p&gt;
FraiLT&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#36882;&#24402;&#26041;&#27861;&#21644;&#36845;&#20195;&#32534;&#30721;&#65292;&#23454;&#29616;&#20102;&#22312;&#32039;&#20945;&#24418;&#24335;&#19979;&#36798;&#21040;&#36739;&#22823;&#27169;&#22411;&#30340;&#35299;&#37322;&#28145;&#24230;&#65292;&#22312;&#24615;&#33021;&#34920;&#29616;&#19978;&#20248;&#20110;&#36739;&#22823;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#39640;&#25928;&#21644;&#21487;&#35775;&#38382;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30001;&#38271;&#24605;&#21464;&#21387;&#22120;&#65288;FraiLT&#65289;&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#26088;&#22312;&#22686;&#24378;&#22788;&#29702;&#33021;&#21147;&#32780;&#19981;&#22686;&#21152;&#35268;&#27169;&#12290;&#23427;&#37319;&#29992;&#36882;&#24402;&#26041;&#27861;&#65292;&#22810;&#27425;&#36845;&#20195;&#23376;&#23618;&#65292;&#24182;&#24341;&#20837;&#36845;&#20195;&#32534;&#30721;&#20197;&#22312;&#36825;&#20123;&#21608;&#26399;&#20013;&#20445;&#25345;&#24847;&#35782;&#12290;&#36845;&#20195;&#32534;&#30721;&#20351;FraiLT&#33021;&#22815;&#20197;&#32039;&#20945;&#30340;&#24418;&#24335;&#23454;&#29616;&#36739;&#22823;&#27169;&#22411;&#30340;&#35299;&#37322;&#28145;&#24230;&#12290;&#22312;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;FraiLT&#20248;&#20110;&#36739;&#22823;&#30340;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#30340;&#21516;&#26102;&#25552;&#20379;&#39640;&#36136;&#37327;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#20195;&#34920;&#20102;&#26356;&#39640;&#25928;&#21644;&#21487;&#35775;&#38382;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11626v2 Announce Type: replace-cross  Abstract: Freely Long-Thinking Transformer (FraiLT) is an improved transformer model designed to enhance processing capabilities without scaling up size. It utilizes a recursive approach, iterating over a subset of layers multiple times, and introduces iteration encodings to maintain awareness across these cycles. Iteration encoding allows FraiLT to achieve the interpretive depth of larger models in a compact form. When evaluated on a synthetic story dataset, FraiLT outperformed larger models, showcasing its ability to deliver high-quality performance while reducing memory demands. This model represents a step forward towards more efficient and accessible language models.
&lt;/p&gt;</description></item><item><title>GD-CAF&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38477;&#27700;&#39044;&#25253;&#20316;&#20026;&#19968;&#20010;&#26102;&#31354;&#22270;&#24207;&#21015;&#39044;&#25253;&#38382;&#39064;&#65292;&#21033;&#29992;&#22270;&#24418;&#21452;&#27969;&#21367;&#31215;&#27880;&#24847;&#21147;&#34701;&#21512;&#26469;&#23398;&#20064;&#21382;&#21490;&#38477;&#27700;&#22270;&#24182;&#22312;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#19978;&#39044;&#27979;&#26410;&#26469;&#30340;&#38477;&#27700;&#12290;</title><link>https://arxiv.org/abs/2401.07958</link><description>&lt;p&gt;
GD-CAF&#65306;&#29992;&#20110;&#38477;&#27700;&#39044;&#25253;&#30340;&#22270;&#24418;&#21452;&#27969;&#21367;&#31215;&#27880;&#24847;&#21147;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
GD-CAF: Graph Dual-stream Convolutional Attention Fusion for Precipitation Nowcasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07958
&lt;/p&gt;
&lt;p&gt;
GD-CAF&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38477;&#27700;&#39044;&#25253;&#20316;&#20026;&#19968;&#20010;&#26102;&#31354;&#22270;&#24207;&#21015;&#39044;&#25253;&#38382;&#39064;&#65292;&#21033;&#29992;&#22270;&#24418;&#21452;&#27969;&#21367;&#31215;&#27880;&#24847;&#21147;&#34701;&#21512;&#26469;&#23398;&#20064;&#21382;&#21490;&#38477;&#27700;&#22270;&#24182;&#22312;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#19978;&#39044;&#27979;&#26410;&#26469;&#30340;&#38477;&#27700;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#38477;&#27700;&#39044;&#25253;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#27946;&#27700;&#39044;&#27979;&#12289;&#28798;&#23475;&#31649;&#29702;&#12289;&#20248;&#21270;&#20892;&#19994;&#27963;&#21160;&#12289;&#31649;&#29702;&#20132;&#36890;&#36335;&#32447;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#26412;&#25991;&#23558;&#38477;&#27700;&#39044;&#25253;&#24418;&#24335;&#21270;&#20026;&#26102;&#31354;&#22270;&#24207;&#21015;&#39044;&#25253;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24418;&#21452;&#27969;&#21367;&#31215;&#27880;&#24847;&#21147;&#34701;&#21512;&#65288;GD-CAF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#21382;&#21490;&#38477;&#27700;&#22270;&#30340;&#26102;&#31354;&#22270;&#20013;&#23398;&#20064;&#65292;&#24182;&#39044;&#27979;&#26410;&#26469;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#30340;&#38477;&#27700;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07958v2 Announce Type: replace  Abstract: Accurate precipitation nowcasting is essential for various applications, including flood prediction, disaster management, optimizing agricultural activities, managing transportation routes and renewable energy. While several studies have addressed this challenging task from a sequence-to-sequence perspective, most of them have focused on a single area without considering the existing correlation between multiple disjoint regions. In this paper, we formulate precipitation nowcasting as a spatiotemporal graph sequence nowcasting problem. In particular, we introduce Graph Dual-stream Convolutional Attention Fusion (GD-CAF), a novel approach designed to learn from historical spatiotemporal graph of precipitation maps and nowcast future time step ahead precipitation at different spatial locations. GD-CAF consists of spatio-temporal convolutional attention as well as gated fusion modules which are equipped with depthwise-separable convolut
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;MoE-Mamba&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;Mamba&#21644;&#22522;&#20934;Transformer-MoE&#65292;&#36798;&#21040;&#20102;&#19982;Mamba&#30456;&#21516;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35757;&#32451;&#27493;&#39588;&#20943;&#23569;&#20102;2.35&#20493;&#12290;</title><link>https://arxiv.org/abs/2401.04081</link><description>&lt;p&gt;
MoE-Mamba: &#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.04081
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;MoE-Mamba&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;Mamba&#21644;&#22522;&#20934;Transformer-MoE&#65292;&#36798;&#21040;&#20102;&#19982;Mamba&#30456;&#21516;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35757;&#32451;&#27493;&#39588;&#20943;&#23569;&#20102;2.35&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#24050;&#32463;&#25104;&#20026;&#39034;&#24207;&#24314;&#27169;&#39046;&#22495;&#30340;&#20005;&#32899;&#31454;&#20105;&#32773;&#65292;&#25361;&#25112;&#20102;Transformer&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#24320;&#25918;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#35201;&#21457;&#25496;SSMs&#22312;&#25193;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#23427;&#20204;&#24212;&#35813;&#19982;MoE&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;Mamba&#19978;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#36817;&#22522;&#20110;SSM&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MoE-Mamba&#22312;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20248;&#20110;Mamba&#21644;&#22522;&#20934;Transformer-MoE&#12290;&#29305;&#21035;&#22320;&#65292;MoE-Mamba&#22312;&#26356;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#20013;&#36798;&#21040;&#19982;Mamba&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;Mamba&#30456;&#23545;&#20110;Transformer&#30340;&#25512;&#29702;&#24615;&#33021;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.04081v2 Announce Type: replace-cross  Abstract: State Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging the dominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved Transformer-based Large Language Models, including recent state-of-the-art open models. We propose that to unlock the potential of SSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a recent SSM-based model that achieves remarkable performance. Our model, MoE-Mamba, outperforms both Mamba and baseline Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in $2.35\times$ fewer training steps while preserving the inference performance gains of Mamba against Transformer.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AllSpark&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#32852;&#21512;&#35299;&#37322;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.00546</link><description>&lt;p&gt;
AllSpark: &#19968;&#20010;&#20855;&#26377;&#21313;&#19977;&#31181;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with Thirteen Modalities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00546
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AllSpark&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#32852;&#21512;&#35299;&#37322;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#30001;&#20110;&#21508;&#31181;&#26102;&#31354;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#39640;&#24230;&#24322;&#36136;&#24615;&#65292;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#30340;&#32852;&#21512;&#35299;&#37322;&#19968;&#30452;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20957;&#32858;&#21147;&#21644;&#33258;&#27835;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#32780;&#38543;&#30528;&#27169;&#24577;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#24179;&#34913;&#34920;&#29616;&#20986;&#36880;&#28176;&#38750;&#32447;&#24615;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#20316;&#20026;&#21442;&#32771;&#26694;&#26550;&#65288;LaRF&#65289;&#65292;&#36825;&#26159;&#26500;&#24314;&#22810;&#27169;&#24577;&#32479;&#19968;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#21017;&#65292;&#26088;&#22312;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#21462;&#24471;&#20957;&#32858;&#21147;&#21644;&#33258;&#27835;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AllSpark&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#21253;&#25324;1D&#65288;&#25991;&#26412;&#65292;&#20195;&#30721;&#65289;&#65292;2D&#65288;RGB&#65292;&#32418;&#22806;&#32447;&#65292;SAR&#65292;&#22810;&#20809;&#35889;&#65292;&#39640;&#20809;&#35889;&#65292;&#34920;&#26684;&#65292;&#22270;&#34920;&#65292;&#36712;&#36857;&#65292;&#26012;&#35282;&#25668;&#24433;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00546v2 Announce Type: replace  Abstract: For a long time, due to the high heterogeneity in structure and semantics among various spatiotemporal modal data, the joint interpretation of multimodal spatiotemporal data has been an extremely challenging problem. The primary challenge resides in striking a trade-off between the cohesion and autonomy of diverse modalities, and this trade-off exhibits a progressively nonlinear nature as the number of modalities expands. We introduce the Language as Reference Framework (LaRF), a fundamental principle for constructing a multimodal unified model, aiming to strike a trade-off between the cohesion and autonomy among different modalities. We propose a multimodal spatiotemporal general artificial intelligence model, called AllSpark. Our model integrates thirteen different modalities into a unified framework, including 1D (text, code), 2D (RGB, infrared, SAR, multispectral, hyperspectral, tables, graphs, trajectory, oblique photography), a
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.16427</link><description>&lt;p&gt;
&#29420;&#31435;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning to Embed Time Series Patches Independently
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16427
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25513;&#30721;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21551;&#21457;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#39318;&#20808;&#23558;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#22359;&#22788;&#29702;&#24182;&#37096;&#20998;&#25513;&#30422;&#65292;&#28982;&#21518;&#35757;&#32451;Transformer&#27169;&#22411;&#36890;&#36807;&#20174;&#26410;&#25513;&#30422;&#30340;&#22359;&#39044;&#27979;&#34987;&#25513;&#30422;&#22359;&#26469;&#25429;&#25417;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#25429;&#25417;&#36825;&#31181;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21487;&#33021;&#19981;&#26159;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#31574;&#30053;&#65307;&#30456;&#21453;&#65292;&#29420;&#31435;&#23398;&#20064;&#23884;&#20837;&#29255;&#27573;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;1&#65289;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#65292;&#33258;&#21160;&#23558;&#27599;&#20010;&#22359;&#36827;&#34892;&#32534;&#30721;&#32780;&#19981;&#26597;&#30475;&#20854;&#20182;&#22359;&#65292;&#20197;&#21450;2&#65289;&#29420;&#33258;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;&#31616;&#21333;&#22359;&#24335;MLP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#26377;&#25928;&#22320;&#20998;&#23618;&#25429;&#33719;&#30456;&#37051;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16427v2 Announce Type: replace-cross  Abstract: Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCLT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#36719;&#23545;&#27604;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24573;&#30053;&#22266;&#26377;&#30456;&#20851;&#24615;&#25152;&#23548;&#33268;&#30340;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.16424</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#36719;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Soft Contrastive Learning for Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCLT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#36719;&#23545;&#27604;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24573;&#30053;&#22266;&#26377;&#30456;&#20851;&#24615;&#25152;&#23548;&#33268;&#30340;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23545;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#34920;&#31034;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30456;&#20284;&#30340;&#23454;&#20363;&#25110;&#30456;&#37051;&#26102;&#38388;&#25139;&#30340;&#20540;&#36827;&#34892;&#23545;&#27604;&#20250;&#24573;&#30053;&#23427;&#20204;&#22266;&#26377;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SoftCLT&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#36719;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#12290;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#20174;&#38646;&#21040;&#19968;&#30340;&#36719;&#36171;&#20540;&#30340;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#23545;&#27604;&#25439;&#22833;&#26469;&#23454;&#29616;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20026;1)&#22522;&#20110;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#23450;&#20041;&#20102;&#23454;&#20363;&#32423;&#23545;&#27604;&#25439;&#22833;&#30340;&#36719;&#36171;&#20540;&#65292;&#24182;&#20026;2)&#22522;&#20110;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#20102;&#26102;&#38388;&#32423;&#23545;&#27604;&#25439;&#22833;&#12290;SoftCLT&#26159;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#27809;&#26377;&#36807;&#22810;&#22797;&#26434;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16424v2 Announce Type: replace-cross  Abstract: Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose SoftCLT, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps. SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experi
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;&#34920;&#31034;&#21644;&#24494;&#35843;&#36866;&#37197;&#22120;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;RepairLLaMA&#21487;&#20026;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#38169;&#35823;&#20135;&#29983;&#39640;&#25928;&#30340;&#36866;&#37197;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.15698</link><description>&lt;p&gt;
RepairLLaMA&#65306;&#39640;&#25928;&#34920;&#31034;&#21644;&#24494;&#35843;&#36866;&#37197;&#22120;&#29992;&#20110;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15698
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#34920;&#31034;&#21644;&#24494;&#35843;&#36866;&#37197;&#22120;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;RepairLLaMA&#21487;&#20026;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#38169;&#35823;&#20135;&#29983;&#39640;&#25928;&#30340;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#65288;APR&#65289;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24050;&#26377;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#23545;&#20110;&#31243;&#24207;&#20462;&#22797;&#36827;&#34892;LLMs&#30340;&#24494;&#35843;&#26159;&#26368;&#36817;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#65292;&#26377;&#35768;&#22810;&#26410;&#34987;&#25506;&#32034;&#30340;&#32500;&#24230;&#12290;&#29616;&#26377;&#24037;&#20316;&#22823;&#22810;&#20351;&#29992;&#31616;&#21333;&#30340;&#20195;&#30721;&#34920;&#31034;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#33021;&#22815;&#24494;&#35843;&#26356;&#22823;&#22411;LLMs&#30340;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#23616;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepairLLaMA&#65292;&#19968;&#20010;&#32467;&#21512;&#20102;1&#65289;&#29992;&#20110;APR&#30340;&#20195;&#30721;&#34920;&#31034;&#21644;2&#65289;&#26368;&#20808;&#36827;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;LLM&#24494;&#35843;&#25216;&#26415;LoRA&#30340;&#26032;&#22411;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;RepairLLaMA&#20135;&#29983;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#8220;&#31243;&#24207;&#20462;&#22797;&#36866;&#37197;&#22120;&#8221;&#65292;&#29992;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#20855;&#26377;&#31243;&#24207;&#20462;&#22797;&#29305;&#23450;&#20195;&#30721;&#34920;&#31034;&#30340;&#24494;&#35843;&#36866;&#37197;&#22120;&#20351;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#26377;&#24847;&#20041;&#30340;&#20462;&#22797;&#20449;&#21495;&#12290;&#20854;&#27425;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26377;&#21161;&#20110;&#24494;&#35843;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15698v2 Announce Type: replace-cross  Abstract: Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tunes LLMs with naive code representations and is fundamentally limited in its ability to fine-tune larger LLMs. To address this problem, we propose RepairLLaMA, a novel program repair approach that combines 1) code representations for APR and 2) the state-of-the-art parameter-efficient LLM fine-tuning technique called LoRA. This results in RepairLLaMA producing a highly effective `program repair adapter' for fixing bugs with language models. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals. Second, parameter-efficient fine-tuning helps fine-tun
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36890;&#36807;&#24191;&#25773;&#26597;&#35810;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#39640;&#25928;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#20026;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#23450;&#21046;&#30340;Sigmoid Trick&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#32467;&#21512;&#22312;KILT&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.12430</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#29992;&#20110;&#24555;&#36895;&#21644;&#25913;&#36827;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12430
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36890;&#36807;&#24191;&#25773;&#26597;&#35810;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#39640;&#25928;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#20026;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#23450;&#21046;&#30340;Sigmoid Trick&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#32467;&#21512;&#22312;KILT&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;RAG&#26041;&#27861;&#20013;&#65292;&#37325;&#26032;&#25490;&#24207;&#22120;&#22312;&#25552;&#21319;&#26816;&#32034;&#20934;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#33021;&#22815;&#25581;&#31034;&#27599;&#23545;&#26597;&#35810;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#38656;&#35201;&#21453;&#22797;&#23545;&#26597;&#35810;&#21644;&#22823;&#37327;&#38271;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#23548;&#33268;&#20102;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#38480;&#21046;&#20102;&#26816;&#32034;&#25991;&#26412;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20934;&#30830;&#24615;&#12290;&#20316;&#20026;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;&#24191;&#25773;&#26597;&#35810;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#39640;&#25928;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#36895;&#24230;&#25552;&#39640;20&#20493;&#33267;40&#20493;&#65292;&#36229;&#36807;&#22522;&#20934;&#36890;&#36947;&#37325;&#26032;&#25490;&#24207;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Sigmoid Trick&#65292;&#19968;&#31181;&#20026;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#23450;&#21046;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23558;&#36825;&#20004;&#31181;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#22312;&#20174;KILT&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#39564;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#32463;&#39564;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12430v3 Announce Type: replace-cross  Abstract: In recent RAG approaches, rerankers play a pivotal role in refining retrieval accuracy with the ability of revealing logical relations for each pair of query and text. However, existing rerankers are required to repeatedly encode the query and a large number of long retrieved text. This results in high computational costs and limits the number of retrieved text, hindering accuracy. As a remedy of the problem, we introduce the Efficient Title Reranker via Broadcasting Query Encoder, a novel technique for title reranking that achieves a 20x-40x speedup over the vanilla passage reranker. Furthermore, we introduce Sigmoid Trick, a novel loss function customized for title reranking. Combining both techniques, we empirically validated their effectiveness, achieving state-of-the-art results on all four datasets we experimented with from the KILT knowledge benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#19982;&#22522;&#20110;&#22870;&#21169;&#27169;&#22411;&#30340;&#25919;&#31574;&#20248;&#21270;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#36229;&#20986;&#20559;&#22909;&#25968;&#25454;&#23545;&#25919;&#31574;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#23454;&#39564;&#35777;&#23454;&#20013;&#65292;RMB-PO+&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2312.10584</link><description>&lt;p&gt;
RLHF&#20013;&#30340;&#25919;&#31574;&#20248;&#21270;&#65306;&#36229;&#20986;&#20559;&#22909;&#25968;&#25454;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization in RLHF: The Impact of Out-of-preference Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#19982;&#22522;&#20110;&#22870;&#21169;&#27169;&#22411;&#30340;&#25919;&#31574;&#20248;&#21270;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#36229;&#20986;&#20559;&#22909;&#25968;&#25454;&#23545;&#25919;&#31574;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#23454;&#39564;&#35777;&#23454;&#20013;&#65292;RMB-PO+&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#26041;&#27861;&#65306;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270; (DPO) &#21644;&#22522;&#20110;&#22870;&#21169;&#27169;&#22411;&#30340;&#25919;&#31574;&#20248;&#21270; (RMB-PO)&#12290;&#36824;&#32771;&#34385;&#20102;RMB-PO&#30340;&#21464;&#20307;&#65292;&#31216;&#20026;RMB-PO+&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20559;&#22909;&#25968;&#25454;&#26126;&#30830;&#25110;&#38544;&#24335;&#22320;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#22312;&#29992;&#20110;&#25919;&#31574;&#20248;&#21270;&#30340;&#25968;&#25454;&#20013;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#35299;&#38145;&#22870;&#21169;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#19982;DPO&#30456;&#27604;&#65292;RMB-PO&#36824;&#20351;&#29992;&#31574;&#30053;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#32780;RMB-PO+&#36827;&#19968;&#27493;&#21033;&#29992;&#26032;&#30340;&#26080;&#20559;&#22909;&#25968;&#25454;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#36229;&#20986;&#20559;&#22909;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#21463;&#25511;&#21644;&#21512;&#25104;&#23454;&#39564;&#36827;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;DPO&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;RMB-PO+&#34920;&#29616;&#26368;&#20339;&#12290;&#29305;&#21035;&#26159;&#65292;&#21363;&#20351;&#20026;&#31574;&#30053;&#27169;&#22411;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#29992;&#20805;&#20998;&#30340;&#25919;&#31574;&#20248;&#21270;&#36827;&#34892;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10584v2 Announce Type: replace  Abstract: Aligning intelligent agents with human preferences and values is important. This paper examines two popular alignment methods: Direct Preference Optimization (DPO) and Reward-Model-Based Policy Optimization (RMB-PO). A variant of RMB-PO, referred to as RMB-PO+ is also considered. These methods, either explicitly or implicitly, learn a reward model from preference data and differ in the data used for policy optimization to unlock the generalization ability of the reward model. In particular, compared with DPO, RMB-PO additionally uses policy-generated data, and RMB-PO+ further leverages new, preference-free data. We examine the impact of such out-of-preference data. Our study, conducted through controlled and synthetic experiments, demonstrates that DPO performs poorly, whereas RMB-PO+ performs the best. In particular, even when providing the policy model with a good feature representation, we find that policy optimization with adequa
&lt;/p&gt;</description></item><item><title>MaxK-GNN&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;MaxK&#38750;&#32447;&#24615;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22402;&#30452;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.08656</link><description>&lt;p&gt;
MaxK-GNN: &#25506;&#32034;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#29702;&#35770;&#36895;&#24230;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08656
&lt;/p&gt;
&lt;p&gt;
MaxK-GNN&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;MaxK&#38750;&#32447;&#24615;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22402;&#30452;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21152;&#36895;&#26041;&#38754;&#65292;GPU&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#24179;&#21488;&#12290; GPU&#22312;GNN&#19978;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#24037;&#20316;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20869;&#23384;&#35775;&#38382;&#19981;&#35268;&#21017;&#65292;&#23548;&#33268;&#30828;&#20214;&#21033;&#29992;&#19981;&#20805;&#20998;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20363;&#22914;PyG&#12289;DGL&#19982;cuSPARSE&#65292;&#20197;&#21450;GNNAdvisor&#26694;&#26550;&#37096;&#20998;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#20869;&#23384;&#27969;&#37327;&#20173;&#28982;&#24456;&#26174;&#33879;&#12290; &#25105;&#20204;&#35748;&#20026;&#65292;&#21482;&#26377;&#36890;&#36807;&#31639;&#27861;&#19982;&#31995;&#32479;&#21019;&#26032;&#30340;&#22402;&#30452;&#20248;&#21270;&#25165;&#33021;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#26159;&#23558;&#21152;&#36895;&#20248;&#21270;&#35270;&#20026;&#8220;&#20107;&#21518;&#24605;&#32771;&#8221;&#65288;&#21363;&#65288;i&#65289;&#32473;&#23450;GNN&#31639;&#27861;&#65292;&#35774;&#35745;&#21152;&#36895;&#22120;&#65292;&#25110;&#65288;ii&#65289;&#32473;&#23450;&#30828;&#20214;&#65292;&#20027;&#35201;&#20248;&#21270;GNN&#31639;&#27861;&#65289;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;MaxK-GNN&#65292;&#19968;&#31181;&#38598;&#25104;&#31639;&#27861;&#19982;&#31995;&#32479;&#21019;&#26032;&#30340;&#20808;&#36827;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#12290; &#65288;i&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;MaxK&#38750;&#32447;&#24615;&#24182;&#25552;&#20379;&#20102;MaxK&#38750;&#32447;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08656v3 Announce Type: replace-cross  Abstract: In the acceleration of deep neural network training, the GPU has become the mainstream platform. GPUs face substantial challenges on GNNs, such as workload imbalance and memory access irregularities, leading to underutilized hardware. Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks partially address these challenges but memory traffic is still significant.   We argue that drastic performance improvements can only be achieved by the vertical optimization of algorithm and system innovations, rather than treating the speedup optimization as an "after-thought" (i.e., (i) given a GNN algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing the GNN algorithm). In this paper, we present MaxK-GNN, an advanced high-performance GPU training system integrating algorithm and system innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical analysis of MaxK nonlinearity as
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;GPT-4V(ision)&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23545;&#27604;&#20102;&#20854;&#19982;CLIP&#12289;LLaVA&#21644;Gemini&#31561;&#30693;&#21517;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2312.07424</link><description>&lt;p&gt;
GPT-4V(ision)&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#36866;&#24212;&#24615;&#22914;&#20309;&#65311;&#21021;&#27493;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;GPT-4V(ision)&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23545;&#27604;&#20102;&#20854;&#19982;CLIP&#12289;LLaVA&#21644;Gemini&#31561;&#30693;&#21517;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#38024;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#27867;&#21270;&#33021;&#21147;&#8212;&#8212;&#21363;&#37096;&#32626;&#26465;&#20214;&#19982;&#35757;&#32451;&#22330;&#26223;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#8212;&#8212;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#35832;&#22914;&#27668;&#20505;&#24314;&#27169;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#39046;&#22495;&#12290;&#22522;&#20110;&#24191;&#27867;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#22810;&#26679;&#24615;&#32780;&#21306;&#21035;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#36866;&#24212;&#33021;&#21147;&#30340;&#22686;&#21152;&#20852;&#36259;&#12290;GPT-4V(ision)&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#20844;&#24320;&#33719;&#21462;&#30340;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#24322;&#24120;&#26816;&#27979;&#12289;&#35270;&#39057;&#29702;&#35299;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#21307;&#23398;&#35786;&#26029;&#31561;&#26041;&#38754;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#31283;&#20581;&#24615;&#20173;&#28982;&#36739;&#23569;&#34987;&#25506;&#31350;&#12290;&#38024;&#23545;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23545;GPT-4V&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#19982;CLIP&#12289;LLaVA&#21644;Gemini&#31561;&#30693;&#21517;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07424v3 Announce Type: replace-cross  Abstract: In machine learning, generalization against distribution shifts -- where deployment conditions diverge from the training scenarios -- is crucial, particularly in fields like climate modeling, biomedicine, and autonomous driving. The emergence of foundation models, distinguished by their extensive pretraining and task versatility, has led to an increased interest in their adaptability to distribution shifts. GPT-4V(ision) acts as the most advanced publicly accessible multimodal foundation model, with extensive applications across various domains, including anomaly detection, video understanding, image generation, and medical diagnosis. However, its robustness against data distributions remains largely underexplored. Addressing this gap, this study rigorously evaluates GPT-4V's adaptability and generalization capabilities in dynamic environments, benchmarking against prominent models like CLIP, LLaVA, and Gemini. We delve into GP
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;RMIA&#65289;&#65292;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24314;&#27169;&#21644;&#26356;&#39640;&#30340;&#27979;&#35797;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2312.03262</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#39640;&#21151;&#29575;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Low-Cost High-Power Membership Inference Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;RMIA&#65289;&#65292;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24314;&#27169;&#21644;&#26356;&#39640;&#30340;&#27979;&#35797;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#26088;&#22312;&#26816;&#27979;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#20351;&#29992;&#12290;&#26368;&#36817;&#19968;&#20123;&#24378;&#22823;&#30340;&#25915;&#20987;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#20351;&#23427;&#20204;&#23545;&#20110;&#23454;&#38469;&#30340;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;RMIA&#65289;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#27169;&#22411;&#30340;&#24635;&#20307;&#25968;&#25454;&#21644;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20284;&#28982;&#27604;&#26816;&#39564;&#20013;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;&#38646;&#20551;&#35774;&#35774;&#32622;&#65292;&#24182;&#26377;&#25928;&#22320;&#21033;&#29992;&#26469;&#33258;&#24635;&#20307;&#30340;&#21442;&#32771;&#27169;&#22411;&#21644;&#21442;&#32771;&#25968;&#25454;&#26679;&#26412;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#30495;&#27491;&#29575;&#65288;true-positive rate&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#39640;&#30340;&#27979;&#35797;&#33021;&#21147;&#65292;&#25972;&#20010;TPR-FPR&#26354;&#32447;&#37117;&#20855;&#22791;&#36825;&#31181;&#20248;&#21183;&#65292;&#21363;&#20351;&#22312;&#26497;&#20302;&#30340;&#35823;&#25253;&#29575;&#19979;&#65288;&#20302;&#33267;0&#65289;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#35745;&#31639;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03262v2 Announce Type: replace-cross  Abstract: Membership inference attacks (MIA) aim to detect if a particular data point was used in training a machine learning model. Recent strong attacks have high computational costs and inconsistent performance under varying conditions, rendering them unreliable for practical privacy risk assessment. We design a novel, efficient, and robust membership inference attack (RMIA) which accurately differentiates between population data and training data of a model, with minimal computational overhead. We achieve this by a more accurate modeling of the null hypothesis setting in our likelihood ratio tests, and effectively leveraging both reference models and reference data samples from the population. Our algorithm exhibits superior test power (true-positive rate) compared to prior methods, throughout the TPR-FPR curve including at extremely low false-positive rates (as low as 0). Under computation constraints, where only a limited number of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#23398;&#20064;&#22810;&#22270;&#32467;&#26500;&#30340;&#21019;&#26032;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#20013;&#23384;&#22312;&#30340;&#21382;&#21490;&#20381;&#36182;&#21644;&#26410;&#26469;&#36235;&#21183;&#21453;&#26144;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.03004</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#22270;&#32467;&#26500;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-graph Structure for Temporal Knowledge Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03004
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#23398;&#20064;&#22810;&#22270;&#32467;&#26500;&#30340;&#21019;&#26032;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#20013;&#23384;&#22312;&#30340;&#21382;&#21490;&#20381;&#36182;&#21644;&#26410;&#26469;&#36235;&#21183;&#21453;&#26144;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#65306;&#23558;&#22522;&#20110;&#21382;&#21490;&#24555;&#29031;&#30340;&#26410;&#26469;&#20107;&#20214;&#39044;&#27979;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#25512;&#29702;&#31216;&#20026;&#22806;&#25512;&#65292;&#24050;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#12290;&#30001;&#20110;&#20854;&#26497;&#31471;&#30340;&#22810;&#26679;&#24615;&#21644;&#31354;&#38388;&#19982;&#26102;&#38388;&#30456;&#20851;&#24615;&#30340;&#21464;&#21270;&#65292;TKG&#25512;&#29702;&#21576;&#29616;&#20986;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#26377;&#25928;&#25429;&#33719;&#20107;&#23454;&#20043;&#38388;&#30340;&#24182;&#21457;&#32467;&#26500;&#21644;&#28436;&#21464;&#20132;&#20114;&#20316;&#29992;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;TKG&#30340;&#22810;&#31181;&#20869;&#22312;&#34920;&#36798;&#35821;&#20041;&#24418;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#36328;&#22810;&#20010;&#26102;&#38388;&#25139;&#30340;&#23454;&#20307;&#30456;&#20851;&#24615;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#21608;&#26399;&#24615;&#12290;&#36825;&#31181;&#38480;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#20805;&#20998;&#21453;&#26144;&#21382;&#21490;&#20381;&#36182;&#20851;&#31995;&#21644;&#26410;&#26469;&#36235;&#21183;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#23398;&#20064;&#22810;&#22270;&#32467;&#26500;&#65288;LMS&#65289;&#30340;&#21019;&#26032;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03004v2 Announce Type: replace-cross  Abstract: Temporal Knowledge Graph (TKG) reasoning that forecasts future events based on historical snapshots distributed over timestamps is denoted as extrapolation and has gained significant attention. Owing to its extreme versatility and variation in spatial and temporal correlations, TKG reasoning presents a challenging task, demanding efficient capture of concurrent structures and evolutional interactions among facts. While existing methods have made strides in this direction, they still fall short of harnessing the diverse forms of intrinsic expressive semantics of TKGs, which encompass entity correlations across multiple timestamps and periodicity of temporal information. This limitation constrains their ability to thoroughly reflect historical dependencies and future trends. In response to these drawbacks, this paper proposes an innovative reasoning approach that focuses on Learning Multi-graph Structure (LMS). Concretely, it com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#20219;&#20309;&#27491;&#21017;&#21270;&#30340;&#22788;&#29702;&#20013;&#26041;&#27861;&#36716;&#21270;&#20026;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#20844;&#24179;&#38169;&#35823;&#26435;&#34913;&#65292;&#24182;&#19988;&#21487;&#33021;&#25552;&#39640;&#20043;&#21069;&#26041;&#27861;&#30340;&#25928;&#21147;</title><link>https://arxiv.org/abs/2312.02592</link><description>&lt;p&gt;
FRAPP'E&#65306;&#19968;&#20010;&#29992;&#20110;&#21518;&#22788;&#29702;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FRAPP\'E: A Group Fairness Framework for Post-Processing Everything
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02592
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#20219;&#20309;&#27491;&#21017;&#21270;&#30340;&#22788;&#29702;&#20013;&#26041;&#27861;&#36716;&#21270;&#20026;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#20844;&#24179;&#38169;&#35823;&#26435;&#34913;&#65292;&#24182;&#19988;&#21487;&#33021;&#25552;&#39640;&#20043;&#21069;&#26041;&#27861;&#30340;&#25928;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22788;&#29702;&#20013;&#23454;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#20844;&#24179;&#38169;&#35823;&#26435;&#34913;&#65292;&#20294;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#22788;&#29702;&#26041;&#27861;&#26080;&#27861;&#22312;&#35768;&#22810;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#25110;&#26080;&#27861;&#35775;&#38382;&#39044;&#27979;&#27169;&#22411;&#35757;&#32451;&#31649;&#36947;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#21518;&#22788;&#29702;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#19987;&#20026;&#29305;&#23450;&#38382;&#39064;&#35774;&#32622;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#32780;&#35774;&#35745;&#65292;&#22240;&#27492;&#19981;&#22914;&#22788;&#29702;&#20013;&#26041;&#27861;&#24191;&#27867;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#27491;&#21017;&#21270;&#30340;&#22788;&#29702;&#20013;&#26041;&#27861;&#36716;&#21270;&#20026;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#36825;&#19968;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33719;&#24471;&#26356;&#24191;&#27867;&#38382;&#39064;&#35774;&#32622;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#36884;&#24452;&#65292;&#36828;&#36229;&#36807;&#20197;&#21069;&#30340;&#21518;&#22788;&#29702;&#25991;&#29486;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20445;&#30041;&#20102;&#22788;&#29702;&#20013;&#23454;&#29616;&#30340;&#33391;&#22909;&#20844;&#24179;&#38169;&#35823;&#26435;&#34913;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#39640;&#20808;&#21069;&#26041;&#27861;&#30340;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02592v2 Announce Type: replace  Abstract: Despite achieving promising fairness-error trade-offs, in-processing mitigation techniques for group fairness cannot be employed in numerous practical applications with limited computation resources or no access to the training pipeline of the prediction model. In these situations, post-processing is a viable alternative. However, current methods are tailored to specific problem settings and fairness definitions and hence, are not as broadly applicable as in-processing. In this work, we propose a framework that turns any regularized in-processing method into a post-processing approach. This procedure prescribes a way to obtain post-processing techniques for a much broader range of problem settings than the prior post-processing literature. We show theoretically and through extensive experiments that our framework preserves the good fairness-error trade-offs achieved with in-processing and can improve over the effectiveness of prior p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#28857;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#28436;&#31034;&#21644;&#36880;&#28857;&#20559;&#22909;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20559;&#22909;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#24615;&#33021;&#27425;&#20248;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.02554</link><description>&lt;p&gt;
ULMA&#65306;&#20154;&#31867;&#28436;&#31034;&#21644;&#36880;&#28857;&#20559;&#22909;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02554
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#28857;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#28436;&#31034;&#21644;&#36880;&#28857;&#20559;&#22909;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20559;&#22909;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#24615;&#33021;&#27425;&#20248;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#26399;&#26395;&#65288;&#20363;&#22914;&#65292;&#26377;&#30410;&#21644;&#26080;&#23475;&#65289;&#23545;&#40784;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36843;&#20999;&#25361;&#25112;&#12290;&#20856;&#22411;&#30340;&#23545;&#40784;&#36807;&#31243;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#21644;&#20559;&#22909;&#23398;&#20064;&#12290;&#22823;&#22810;&#25968;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;RLHF&#21644;DPO&#65289;&#20381;&#36182;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#65292;&#36825;&#24182;&#19981;&#20805;&#20998;&#22320;&#35299;&#20915;&#20154;&#31867;&#21453;&#39304;&#26159;&#36880;&#28857;&#30340;&#24773;&#20917;&#65292;&#23548;&#33268;&#28508;&#22312;&#20449;&#24687;&#20002;&#22833;&#21644;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36880;&#28857;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65292;&#19968;&#31181;&#26088;&#22312;&#26377;&#25928;&#21033;&#29992;&#36880;&#28857;&#21453;&#39304;&#30340;&#26032;&#39062;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#25581;&#31034;&#20102;&#30417;&#30563;&#24494;&#35843;&#21644;&#36880;&#28857;&#20559;&#22909;&#23398;&#20064;&#20043;&#38388;&#30340;&#26032;&#39062;&#32852;&#31995;&#65292;&#26368;&#32456;&#24418;&#25104;&#20102;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#23545;&#40784;&#19982;&#20154;&#31867;&#28436;&#31034;&#21644;&#36880;&#28857;&#20559;&#22909;&#32479;&#19968;&#30340;&#21333;&#27493;&#26041;&#27861;&#12290;&#22312;&#36880;&#28857;&#20559;&#22909;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02554v2 Announce Type: replace-cross  Abstract: Aligning language models to human expectations, e.g., being helpful and harmless, has become a pressing challenge for large language models. A typical alignment procedure consists of supervised fine-tuning and preference learning. Most preference learning methods, such as RLHF and DPO, depend on pairwise preference data, which inadequately address scenarios where human feedback is point-wise, leading to potential information loss and suboptimal performance. Addressing this gap, we introduce Point-wise Direct Preference Optimization, a novel preference learning method designed to harness point-wise feedback effectively. Our work also uncovers a novel connection between supervised fine-tuning and point-wise preference learning, culminating in Unified Language Model Alignment, a single-step method that unifies the alignment with human demonstrations and point-wise preferences. Extensive experiments on point-wise preference dataset
&lt;/p&gt;</description></item><item><title>xTrimoGene&#26159;&#19968;&#31181;&#38024;&#23545;scRNA-seq&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#23545;&#31216;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#31232;&#30095;&#29305;&#24615;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20351;&#24471;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#35757;&#32451;&#26368;&#22823;&#30340;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2311.15156</link><description>&lt;p&gt;
xTrimoGene&#65306;&#29992;&#20110;&#21333;&#32454;&#32990;RNA-Seq&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#34920;&#31034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15156
&lt;/p&gt;
&lt;p&gt;
xTrimoGene&#26159;&#19968;&#31181;&#38024;&#23545;scRNA-seq&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#23545;&#31216;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#31232;&#30095;&#29305;&#24615;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20351;&#24471;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#35757;&#32451;&#26368;&#22823;&#30340;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36890;&#37327;&#27979;&#24207;&#25216;&#26415;&#30340;&#36827;&#27493;&#24050;&#32463;&#22312;&#21333;&#32454;&#32990;&#27700;&#24179;&#19978;&#27979;&#37327;&#22522;&#22240;&#34920;&#36798;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#21487;&#20844;&#24320;&#33719;&#21462;&#30340;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#65288;scRNA-seq&#65289;&#25968;&#25454;&#37327;&#24050;&#32463;&#36229;&#36807;&#20102;5000&#19975;&#26465;&#20154;&#31867;&#35760;&#24405;&#65292;&#27599;&#26465;&#35760;&#24405;&#27979;&#37327;&#20102;2&#19975;&#20010;&#22522;&#22240;&#12290;&#36825;&#31361;&#20986;&#20102;&#23545;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#38656;&#27714;&#65292;&#28982;&#32780;&#20256;&#32479;&#30340;Transformer&#26550;&#26500;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#26041;&#38754;&#37117;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;scRNA-seq&#25968;&#25454;&#30340;&#38750;&#23545;&#31216;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#65292;&#31216;&#20026;xTrimoGene$^\alpha$&#65288;&#25110;&#31616;&#31216;&#20026;xTrimoGene&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#25968;&#25454;&#30340;&#31232;&#30095;&#29305;&#24615;&#26469;&#25193;&#23637;&#39044;&#35757;&#32451;&#12290;xTrimoGene&#30340;&#21487;&#25193;&#23637;&#35774;&#35745;&#23558;FLOPs&#38477;&#20302;&#20102;&#19968;&#20010;&#21040;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19982;&#20256;&#32479;Transformer&#30456;&#27604;&#20173;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#26368;&#22823;&#30340;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15156v2 Announce Type: replace-cross  Abstract: Advances in high-throughput sequencing technology have led to significant progress in measuring gene expressions at the single-cell level. The amount of publicly available single-cell RNA-seq (scRNA-seq) data is already surpassing 50M records for humans with each record measuring 20,000 genes. This highlights the need for unsupervised representation learning to fully ingest these data, yet classical transformer architectures are prohibitive to train on such data in terms of both computation and memory. To address this challenge, we propose a novel asymmetric encoder-decoder transformer for scRNA-seq data, called xTrimoGene$^\alpha$ (or xTrimoGene for short), which leverages the sparse characteristic of the data to scale up the pre-training. This scalable design of xTrimoGene reduces FLOPs by one to two orders of magnitude compared to classical transformers while maintaining high accuracy, enabling us to train the largest transf
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;IL&#21644;&#40065;&#26834;MPC&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Tube-NeRF&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;NeRFs&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#65292;&#36890;&#36807;&#31649;&#30340;&#29305;&#24615;&#36873;&#25321;&#30456;&#20851;&#35270;&#22270;&#65292;&#39640;&#25928;&#35745;&#31639;&#23545;&#24212;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35270;&#35273;&#23548;&#21521;&#31574;&#30053;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.14153</link><description>&lt;p&gt;
Tube-NeRF&#65306;&#20351;&#29992;Tube-Guided&#25968;&#25454;&#22686;&#24378;&#21644;NeRFs&#20174;MPC&#36827;&#34892;&#35270;&#36816;&#21160;&#31574;&#30053;&#30340;&#39640;&#25928;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tube-NeRF: Efficient Imitation Learning of Visuomotor Policies from MPC using Tube-Guided Data Augmentation and NeRFs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14153
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;IL&#21644;&#40065;&#26834;MPC&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Tube-NeRF&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;NeRFs&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#65292;&#36890;&#36807;&#31649;&#30340;&#29305;&#24615;&#36873;&#25321;&#30456;&#20851;&#35270;&#22270;&#65292;&#39640;&#25928;&#35745;&#31639;&#23545;&#24212;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35270;&#35273;&#23548;&#21521;&#31574;&#30053;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21487;&#20197;&#20174;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65288;MPC&#65289;&#20013;&#35757;&#32451;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24863;&#30693;&#21160;&#20316;&#31574;&#30053;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38271;&#25110;&#40065;&#26834;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;IL&#19982;&#19968;&#31181;&#32771;&#34385;&#36807;&#31243;&#21644;&#20256;&#24863;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;MPC&#30340;&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#31574;&#30053;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#25552;&#20986;&#30340;DA&#26041;&#27861;&#21517;&#20026;Tube-NeRF&#65292;&#21033;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#29983;&#25104;&#26032;&#39062;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#21033;&#29992;&#40065;&#26834;MPC&#30340;&#24615;&#36136;&#65288;&#31649;&#65289;&#36873;&#25321;&#30456;&#20851;&#35270;&#22270;&#65292;&#24182;&#26377;&#25928;&#35745;&#31639;&#30456;&#24212;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#23558;&#26041;&#27861;&#37327;&#36523;&#23450;&#21046;&#20026;&#22810;&#26059;&#32764;&#19978;&#30340;&#23450;&#20301;&#21644;&#36712;&#36857;&#36319;&#36394;&#20219;&#21153;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20165;&#20351;&#29992;&#26426;&#36733;&#25668;&#20687;&#26426;&#22270;&#20687;&#20316;&#20026;&#27700;&#24179;&#20301;&#32622;&#21807;&#19968;&#26469;&#28304;&#30340;&#35270;&#21160;&#20316;&#31574;&#30053;&#26469;&#29983;&#25104;&#25511;&#21046;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14153v2 Announce Type: replace-cross  Abstract: Imitation learning (IL) can train computationally-efficient sensorimotor policies from a resource-intensive Model Predictive Controller (MPC), but it often requires many samples, leading to long training times or limited robustness. To address these issues, we combine IL with a variant of robust MPC that accounts for process and sensing uncertainties, and we design a data augmentation (DA) strategy that enables efficient learning of vision-based policies. The proposed DA method, named Tube-NeRF, leverages Neural Radiance Fields (NeRFs) to generate novel synthetic images, and uses properties of the robust MPC (the tube) to select relevant views and to efficiently compute the corresponding actions. We tailor our approach to the task of localization and trajectory tracking on a multirotor, by learning a visuomotor policy that generates control actions using images from the onboard camera as only source of horizontal position. Nume
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#65292;&#20174;&#39044;&#35757;&#32451;&#21040;&#25512;&#26029;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2311.12351</link><description>&lt;p&gt;
&#22312;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#36827;Transformer&#26550;&#26500;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#65292;&#20174;&#39044;&#35757;&#32451;&#21040;&#25512;&#26029;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#24212;&#29992;&#20110;&#30693;&#35782;&#24211;&#12289;&#20154;&#26426;&#30028;&#38754;&#21644;&#21160;&#24577;&#20195;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#65292;&#26631;&#24535;&#30528;&#36808;&#21521;&#36798;&#21040;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#19968;&#22823;&#27493;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LLMs&#20027;&#35201;&#26159;&#22312;&#30701;&#25991;&#26412;&#29255;&#27573;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36825;&#21361;&#21450;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#39057;&#32321;&#36935;&#21040;&#30340;&#38271;&#19978;&#19979;&#25991;&#25552;&#31034;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#23545;&#26368;&#36817;&#22312;&#26088;&#22312;&#22686;&#24378;LLMs&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#22522;&#20110;Transformer&#30340;LLM&#26550;&#26500;&#30340;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#28085;&#30422;&#20102;&#25972;&#20010;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#65292;&#20174;&#39044;&#35757;&#32451;&#21040;&#25512;&#26029;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38416;&#36848;&#24182;&#20998;&#26512;&#20102;&#24403;&#21069;&#22522;&#20110;Transformer&#27169;&#22411;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;Transformer&#26550;&#26500;&#21319;&#32423;&#30340;&#20998;&#31867;&#21644;&#26223;&#35266;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12351v2 Announce Type: replace  Abstract: Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investi
&lt;/p&gt;</description></item><item><title>&#30417;&#25511;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#26159;&#37325;&#35201;&#30340;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#22240;&#26524;&#38236;&#22836;&#23548;&#33322;&#35299;&#20915;&#26377;&#25928;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.11463</link><description>&lt;p&gt;
&#35774;&#35745;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30417;&#25511;&#31574;&#30053;&#65306;&#36890;&#36807;&#22240;&#26524;&#38236;&#22836;&#23548;&#33322;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Designing monitoring strategies for deployed machine learning algorithms: navigating performativity through a causal lens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11463
&lt;/p&gt;
&lt;p&gt;
&#30417;&#25511;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#26159;&#37325;&#35201;&#30340;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#22240;&#26524;&#38236;&#22836;&#23548;&#33322;&#35299;&#20915;&#26377;&#25928;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#31995;&#32479;&#37096;&#32626;&#21518;&#65292;&#30417;&#25511;&#20854;&#24615;&#33021;&#23545;&#20110;&#30830;&#20445;&#31639;&#27861;&#38271;&#26399;&#23433;&#20840;&#26377;&#25928;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;ML&#31639;&#27861;&#19982;&#20854;&#29615;&#22659;&#20114;&#21160;&#26102;&#65292;&#31639;&#27861;&#21487;&#33021;&#24433;&#21709;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#65292;&#24182;&#22312;&#35780;&#20272;&#20854;&#29420;&#31435;&#24615;&#33021;&#26102;&#25104;&#20026;&#20027;&#35201;&#20559;&#35265;&#28304;&#65292;&#36825;&#19968;&#38382;&#39064;&#34987;&#31216;&#20026;&#26377;&#25928;&#24615;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#22312;&#26377;&#25928;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#39564;&#35777;&#27169;&#22411;&#65292;&#20294;&#22312;&#26377;&#25928;&#24615;&#23384;&#22312;&#30340;&#29615;&#22659;&#20013;&#30417;&#25511;&#27169;&#22411;&#30340;&#24037;&#20316;&#21364;&#24456;&#23569;&#12290;&#19982;&#27169;&#22411;&#39564;&#35777;&#35774;&#32622;&#19981;&#21516;&#65292;&#23545;&#20110;&#35201;&#30417;&#25511;&#21738;&#20123;&#24615;&#33021;&#25351;&#26631;&#27809;&#26377;&#24456;&#22810;&#19968;&#33268;&#24615;&#12290;&#19981;&#21516;&#30340;&#30417;&#25511;&#26631;&#20934;&#20250;&#24433;&#21709;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21487;&#36776;&#35782;&#24615;&#25152;&#38656;&#30340;&#20551;&#35774;&#65292;&#20197;&#21450;&#26816;&#27979;&#36895;&#24230;&#12290;&#24403;&#36825;&#19968;&#36873;&#25321;&#36827;&#19968;&#27493;&#19982;&#20351;&#29992;&#35266;&#23519;&#24615;&#19982;&#19981;&#24179;&#31561;&#24615;&#30340;&#20915;&#23450;&#30456;&#32467;&#21512;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11463v2 Announce Type: replace  Abstract: After a machine learning (ML)-based system is deployed, monitoring its performance is important to ensure the safety and effectiveness of the algorithm over time. When an ML algorithm interacts with its environment, the algorithm can affect the data-generating mechanism and be a major source of bias when evaluating its standalone performance, an issue known as performativity. Although prior work has shown how to validate models in the presence of performativity using causal inference techniques, there has been little work on how to monitor models in the presence of performativity. Unlike the setting of model validation, there is much less agreement on which performance metrics to monitor. Different monitoring criteria impact how interpretable the resulting test statistic is, what assumptions are needed for identifiability, and the speed of detection. When this choice is further coupled with the decision to use observational versus in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745; (MultiNPE) &#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#25972;&#21512;&#19981;&#21516;&#26469;&#28304;&#30340;&#24322;&#26500;&#25968;&#25454;&#65292;&#22312;&#27169;&#25311;&#25512;&#29702;&#20013;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#21442;&#25968;&#30340;&#20934;&#30830;&#25512;&#26029;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.10671</link><description>&lt;p&gt;
&#22810;&#27169;&#25311;&#25512;&#29702;&#30340;&#28145;&#24230;&#34701;&#21512;&#65306;&#28145;&#24230;&#34701;&#21512;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#25311;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10671
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745; (MultiNPE) &#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#25972;&#21512;&#19981;&#21516;&#26469;&#28304;&#30340;&#24322;&#26500;&#25968;&#25454;&#65292;&#22312;&#27169;&#25311;&#25512;&#29702;&#20013;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#21442;&#25968;&#30340;&#20934;&#30830;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#22810;&#27169;&#24577;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;(MultiNPE)&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#25311;&#25512;&#29702;&#20013;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#21463;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#23427;&#36171;&#20104;&#30740;&#31350;&#20154;&#21592;&#20998;&#26512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#24182;&#25512;&#26029;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#21442;&#25968;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;MultiNPE&#21046;&#23450;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65288;&#26089;&#26399;&#12289;&#21518;&#26399;&#12289;&#28151;&#21512;&#65289;&#65292;&#24182;&#22312;&#19977;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#39564;&#20013;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;MultiNPE&#19981;&#20165;&#22312;&#21442;&#32771;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#19968;&#25968;&#25454;&#28304;&#22522;&#32447;&#65292;&#36824;&#22312;&#31070;&#32463;&#31185;&#23398;&#21644;&#24515;&#33039;&#30149;&#23398;&#30340;&#31185;&#23398;&#27169;&#22411;&#25512;&#29702;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#25104;&#32489;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#23545;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#21518;&#26399;&#21644;&#28151;&#21512;&#34701;&#21512;&#25216;&#26415;&#25104;&#20026;&#22810;&#27169;&#24577;&#27169;&#25311;&#25512;&#29702;&#23454;&#38469;&#24212;&#29992;&#30340;&#39318;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10671v2 Announce Type: replace-cross  Abstract: We present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy. We formulate multimodal fusion approaches for \hbox{MultiNPE} (early, late, hybrid) and evaluate their performance in three challenging experiments. MultiNPE not only outperforms single-source baselines on a reference task, but also achieves superior inference on scientific models from neuroscience and cardiology. We systematically investigate the impact of partially missing data on the different fusion strategies. Across our experiments, late and hybrid fusion techniques emerge as the methods of choice for practical applications of multimodal simulation-based infere
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#26102;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#31574;&#30053;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ever&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.09114</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#26102;&#39564;&#35777;&#21644;&#32416;&#27491;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09114
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#26102;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#31574;&#30053;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ever&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29983;&#25104;&#27969;&#30021;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#36935;&#21040;&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#26500;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#20110;&#38750;&#22522;&#20110;&#26816;&#32034;&#30340;&#29983;&#25104;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#20013;&#65292;&#29616;&#26377;&#30340;&#20107;&#21518;&#32416;&#27491;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#35299;&#20915;&#8220;&#28378;&#38634;&#29699;&#8221;&#38382;&#39064;&#23548;&#33268;&#30340;&#32047;&#31215;&#34394;&#26500;&#38169;&#35823;&#65292;&#29305;&#21035;&#26159;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ever&#8221;&#30340;&#26032;&#26041;&#27861;&#12290;Ever&#37319;&#29992;&#23454;&#26102;&#12289;&#36880;&#27493;&#30340;&#29983;&#25104;&#21644;&#34394;&#26500;&#32416;&#27491;&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#31561;&#21040;&#29983;&#25104;&#36807;&#31243;&#32467;&#26463;&#25165;&#32416;&#27491;&#34394;&#26500;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#26816;&#27979;&#21644;&#32416;&#27491;&#34394;&#26500;&#12290;&#19982;&#22522;&#20110;&#26816;&#32034;&#21644;&#38750;&#22522;&#20110;&#26816;&#32034;&#30340;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09114v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the "snowballing" issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based basel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.09101</link><description>&lt;p&gt;
&#26397;&#21521;&#22810;&#27493;&#25512;&#29702;&#30340;&#31572;&#26696;&#26657;&#20934;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified View of Answer Calibration for Multi-Step Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#25193;&#23637;&#20102;&#25913;&#36827;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#24120;&#23558;&#22810;&#27493;&#25512;&#29702;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#36335;&#24452;&#29983;&#25104;&#20197;&#29983;&#25104;&#25512;&#29702;&#36335;&#24452;&#65307;&#21644;&#31572;&#26696;&#26657;&#20934;&#21518;&#22788;&#29702;&#25512;&#29702;&#36335;&#24452;&#20197;&#33719;&#24471;&#26368;&#32456;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#32570;&#20047;&#23545;&#19981;&#21516;&#31572;&#26696;&#26657;&#20934;&#26041;&#27861;&#30340;&#31995;&#32479;&#20998;&#26512;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#22810;&#36335;&#24452;&#19978;&#30340;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21487;&#33021;&#21551;&#31034;&#20248;&#21270;&#22810;&#27493;&#25512;&#29702;&#31995;&#32479;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09101v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: path generation to generate the reasoning path(s); and answer calibration post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for opti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#22312;&#28436;&#35762;&#25991;&#26412;&#20013;&#21306;&#20998;&#21457;&#35328;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2311.07564</link><description>&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#33021;&#21542;&#21306;&#20998;&#28436;&#35762;&#25991;&#26412;&#20013;&#30340;&#21457;&#35328;&#20154;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#22312;&#28436;&#35762;&#25991;&#26412;&#20013;&#21306;&#20998;&#21457;&#35328;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#39564;&#35777;&#26159;&#30830;&#23450;&#20004;&#20010;&#19981;&#21516;&#20070;&#38754;&#26679;&#26412;&#26159;&#21542;&#21516;&#23646;&#19968;&#20316;&#32773;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#28041;&#21450;&#23545;&#20070;&#38754;&#25991;&#26412;&#30340;&#24402;&#22240;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36716;&#24405;&#28436;&#35762;&#30340;&#24402;&#23646;&#38382;&#39064;&#65292;&#36825;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#65292;&#35768;&#22810;&#25991;&#20307;&#29305;&#24449;&#65292;&#22914;&#26631;&#28857;&#21644;&#22823;&#20889;&#65292;&#22312;&#36825;&#31181;&#24773;&#22659;&#19979;&#24182;&#19981;&#20855;&#22791;&#20449;&#24687;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36716;&#24405;&#30340;&#28436;&#35762;&#21576;&#29616;&#20854;&#20182;&#27169;&#24335;&#65292;&#22914;&#22635;&#20805;&#35789;&#21644;&#22238;&#24212;&#24615;&#22768;&#38899;&#65288;&#20363;&#22914;&#8220;&#21999;&#8221;&#65292;&#8220;&#21999;&#65292;&#21999;&#8221;&#65289;&#65292;&#36825;&#20123;&#21487;&#33021;&#26159;&#19981;&#21516;&#21457;&#35328;&#20154;&#30340;&#29305;&#24449;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;&#20026;&#20102;&#38480;&#21046;&#21457;&#35328;&#20154;&#19982;&#35805;&#39064;&#20043;&#38388;&#30340;&#34394;&#20551;&#20851;&#32852;&#65292;&#25105;&#20204;&#20351;&#29992;&#20250;&#35805;&#25552;&#31034;&#21644;&#21442;&#19982;&#21516;&#19968;&#23545;&#35805;&#30340;&#21457;&#35328;&#20154;&#26500;&#24314;&#19981;&#21516;&#38590;&#24230;&#30340;&#39564;&#35777;&#35797;&#39564;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#22312;&#36825;&#19968;&#26032;&#22522;&#20934;&#19978;&#24314;&#31435;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07564v2 Announce Type: replace  Abstract: Authorship verification is the task of determining if two distinct writing samples share the same author and is typically concerned with the attribution of written text. In this paper, we explore the attribution of transcribed speech, which poses novel challenges. The main challenge is that many stylistic features, such as punctuation and capitalization, are not informative in this setting. On the other hand, transcribed speech exhibits other patterns, such as filler words and backchannels (e.g., 'um', 'uh-huh'), which may be characteristic of different speakers. We propose a new benchmark for speaker attribution focused on conversational speech transcripts. To limit spurious associations of speakers with topic, we employ both conversation prompts and speakers participating in the same conversation to construct verification trials of varying difficulties. We establish the state of the art on this new benchmark by comparing a suite of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;(RawHFL)&#65292;&#29992;&#20110;&#26080;&#32447;&#32593;&#32476;&#35270;&#39057;&#32531;&#23384;&#65292;&#36890;&#36807;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#65292;&#20197;&#25913;&#21892;&#22238;&#20256;&#27969;&#37327;&#25317;&#22622;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#65292;&#36890;&#36807;&#20248;&#21270;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#12289;&#26412;&#22320;&#35757;&#32451;&#36718;&#27425;&#21644;CPU&#39057;&#29575;&#65292;&#20197;&#26368;&#23567;&#21270;&#19968;&#20010;&#21152;&#26435;&#25928;&#29992;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;RawHFL&#30340;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2311.06918</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#35270;&#39057;&#32531;&#23384;&#30340;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Resource-Aware Hierarchical Federated Learning for Video Caching in Wireless Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;(RawHFL)&#65292;&#29992;&#20110;&#26080;&#32447;&#32593;&#32476;&#35270;&#39057;&#32531;&#23384;&#65292;&#36890;&#36807;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#65292;&#20197;&#25913;&#21892;&#22238;&#20256;&#27969;&#37327;&#25317;&#22622;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#65292;&#36890;&#36807;&#20248;&#21270;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#12289;&#26412;&#22320;&#35757;&#32451;&#36718;&#27425;&#21644;CPU&#39057;&#29575;&#65292;&#20197;&#26368;&#23567;&#21270;&#19968;&#20010;&#21152;&#26435;&#25928;&#29992;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;RawHFL&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#32531;&#23384;&#36890;&#36807;&#26412;&#22320;&#23384;&#20648;&#29992;&#25143;&#39057;&#32321;&#35831;&#27714;&#30340;&#28909;&#38376;&#20869;&#23481;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22238;&#20256;&#27969;&#37327;&#25317;&#22622;&#38382;&#39064;&#12290;&#20026;&#20102;&#23398;&#20064;&#29992;&#25143;&#38656;&#27714;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#32780;&#20445;&#25252;&#38544;&#31169;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;(RawHFL)&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#12290;&#32771;&#34385;&#21040;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20840;&#23616;&#26799;&#24230;&#33539;&#25968;&#30340;&#19978;&#30028;&#65292;&#35813;&#33539;&#25968;&#21462;&#20915;&#20110;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#36718;&#27425;&#21644;&#26080;&#32447;&#38142;&#36335;&#19978;&#32047;&#31215;&#26799;&#24230;&#30340;&#25104;&#21151;&#25509;&#25910;&#12290;&#22312;&#24310;&#36831;&#12289;&#33021;&#32791;&#21644;&#26080;&#32447;&#36164;&#28304;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#20248;&#21270;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#12289;&#26412;&#22320;&#35757;&#32451;&#36718;&#27425;&#21644;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;(CPU)&#39057;&#29575;&#65292;&#20197;&#26368;&#23567;&#21270;&#19968;&#20010;&#21152;&#26435;&#25928;&#29992;&#20989;&#25968;&#65292;&#20174;&#32780;&#20419;&#36827;RawHFL&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video caching can significantly improve backhaul traffic congestion by locally storing the popular content that users frequently request. A privacy-preserving method is desirable to learn how users' demands change over time. As such, this paper proposes a novel resource-aware hierarchical federated learning (RawHFL) solution to predict users' future content requests under the realistic assumptions that content requests are sporadic and users' datasets can only be updated based on the requested content's information. Considering a partial client participation case, we first derive the upper bound of the global gradient norm that depends on the clients' local training rounds and the successful reception of their accumulated gradients over the wireless links. Under delay, energy and radio resource constraints, we then optimize client selection and their local rounds and central processing unit (CPU) frequencies to minimize a weighted utility function that facilitates RawHFL's convergence 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#35299;&#30721;&#22120;&#20013;&#30340;&#21508;&#21521;&#24322;&#24615;&#21576;&#38047;&#29366;&#26354;&#32447;&#65292;&#26368;&#39640;&#21508;&#21521;&#24322;&#24615;&#27987;&#24230;&#22312;&#20013;&#38388;&#23618;&#65292;&#19982;&#32534;&#30721;&#22120;&#20013;&#26356;&#22343;&#21248;&#20998;&#24067;&#30340;&#21508;&#21521;&#24322;&#24615;&#19981;&#21516;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#30340;&#20869;&#22312;&#32500;&#24230;&#22312;&#35757;&#32451;&#21021;&#26399;&#22686;&#21152;&#65292;&#38543;&#21518;&#22312;&#35757;&#32451;&#26411;&#26399;&#20986;&#29616;&#21387;&#32553;&#65292;&#34920;&#26126;&#26356;&#32039;&#20945;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>https://arxiv.org/abs/2311.05928</link><description>&lt;p&gt;
&#23398;&#20064;&#30340;&#24418;&#29366;&#65306;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#21508;&#21521;&#24322;&#24615;&#21644;&#20869;&#22312;&#32500;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#35299;&#30721;&#22120;&#20013;&#30340;&#21508;&#21521;&#24322;&#24615;&#21576;&#38047;&#29366;&#26354;&#32447;&#65292;&#26368;&#39640;&#21508;&#21521;&#24322;&#24615;&#27987;&#24230;&#22312;&#20013;&#38388;&#23618;&#65292;&#19982;&#32534;&#30721;&#22120;&#20013;&#26356;&#22343;&#21248;&#20998;&#24067;&#30340;&#21508;&#21521;&#24322;&#24615;&#19981;&#21516;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#30340;&#20869;&#22312;&#32500;&#24230;&#22312;&#35757;&#32451;&#21021;&#26399;&#22686;&#21152;&#65292;&#38543;&#21518;&#22312;&#35757;&#32451;&#26411;&#26399;&#20986;&#29616;&#21387;&#32553;&#65292;&#34920;&#26126;&#26356;&#32039;&#20945;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;Transformer&#26550;&#26500;&#20013;&#23884;&#20837;&#30340;&#21508;&#21521;&#24322;&#24615;&#21160;&#24577;&#21644;&#20869;&#22312;&#32500;&#24230;&#23637;&#24320;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;Transformer&#35299;&#30721;&#22120;&#20013;&#30340;&#21508;&#21521;&#24322;&#24615;&#37197;&#32622;&#21576;&#29616;&#20986;&#26126;&#26174;&#30340;&#38047;&#29366;&#26354;&#32447;&#65292;&#20855;&#26377;&#26368;&#39640;&#30340;&#21508;&#21521;&#24322;&#24615;&#27987;&#24230;&#22312;&#20013;&#38388;&#23618;&#12290;&#36825;&#31181;&#27169;&#24335;&#19982;&#32534;&#30721;&#22120;&#20013;&#35266;&#23519;&#21040;&#30340;&#26356;&#22343;&#21248;&#20998;&#24067;&#30340;&#21508;&#21521;&#24322;&#24615;&#26377;&#25152;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#23884;&#20837;&#30340;&#20869;&#22312;&#32500;&#24230;&#22312;&#35757;&#32451;&#30340;&#21021;&#22987;&#38454;&#27573;&#22686;&#21152;&#65292;&#34920;&#26126;&#21521;&#26356;&#39640;&#32500;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;&#28982;&#21518;&#22312;&#35757;&#32451;&#26411;&#23614;&#20986;&#29616;&#21521;&#26356;&#20302;&#32500;&#24230;&#30340;&#21387;&#32553;&#38454;&#27573;&#65292;&#26263;&#31034;&#30528;&#23545;&#26356;&#32039;&#20945;&#34920;&#31034;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#29702;&#35299;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23884;&#20837;&#23646;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05928v2 Announce Type: replace-cross  Abstract: In this study, we present an investigation into the anisotropy dynamics and intrinsic dimension of embeddings in transformer architectures, focusing on the dichotomy between encoders and decoders. Our findings reveal that the anisotropy profile in transformer decoders exhibits a distinct bell-shaped curve, with the highest anisotropy concentrations in the middle layers. This pattern diverges from the more uniformly distributed anisotropy observed in encoders. In addition, we found that the intrinsic dimension of embeddings increases in the initial phases of training, indicating an expansion into higher-dimensional space. Which is then followed by a compression phase towards the end of training with dimensionality decrease, suggesting a refinement into more compact representations. Our results provide fresh insights to the understanding of encoders and decoders embedding properties.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;Meta-Continual Active Learning&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#35299;&#20915;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.03732</link><description>&lt;p&gt;
&#23398;&#20064;&#23398;&#20064;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#25345;&#20037;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn for Few-shot Continual Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;Meta-Continual Active Learning&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#35299;&#20915;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#30830;&#20445;&#35299;&#20915;&#20808;&#21069;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#23637;&#31034;&#23545;&#26032;&#39046;&#22495;&#30340;&#21487;&#22609;&#24615;&#12290;&#26368;&#36817;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#23616;&#38480;&#20110;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#65288;CAL&#65289;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#65292;&#20294;&#26410;&#26631;&#35760;&#25968;&#25454;&#20805;&#36275;&#65292;&#20294;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20803;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#26469;&#35299;&#20915;&#20219;&#21153;&#20043;&#38388;&#30340;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#23569;&#26679;&#26412;CAL&#35774;&#32622;&#20013;&#19981;&#21516;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39640;&#20934;&#30830;&#24615;&#19982;&#36879;&#26126;&#24615;&#30340;&#29627;&#29827;&#31665;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#24418;&#29366;&#20989;&#25968;&#24635;&#32467;&#29305;&#24449;&#25928;&#24212;&#65292;&#26377;&#25928;&#26144;&#23556;&#39118;&#21147;&#36755;&#20986;&#19982;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#20016;&#23500;&#20102;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#21644;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2310.18629</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#24314;&#27169;&#65306;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#29627;&#29827;&#31665;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explainable Modeling for Wind Power Forecasting: A Glass-Box Approach with High Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18629
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39640;&#20934;&#30830;&#24615;&#19982;&#36879;&#26126;&#24615;&#30340;&#29627;&#29827;&#31665;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#24418;&#29366;&#20989;&#25968;&#24635;&#32467;&#29305;&#24449;&#25928;&#24212;&#65292;&#26377;&#25928;&#26144;&#23556;&#39118;&#21147;&#36755;&#20986;&#19982;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#20016;&#23500;&#20102;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#21644;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#20013;&#21462;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#36890;&#24120;&#34987;&#35270;&#20026;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#40657;&#30418;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39640;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24615;&#30340;&#29627;&#29827;&#31665;&#26041;&#27861;&#65292;&#29992;&#20110;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#26500;&#24314;&#24418;&#29366;&#20989;&#25968;&#24635;&#32467;&#29305;&#24449;&#25928;&#24212;&#65292;&#26377;&#25928;&#22320;&#26144;&#23556;&#39118;&#21147;&#36755;&#20986;&#19982;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21512;&#24182;&#20132;&#20114;&#39033;&#65292;&#20016;&#23500;&#20102;&#39044;&#27979;&#27169;&#22411;&#65292;&#24039;&#22937;&#22320;&#25429;&#33719;&#20102;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#21644;&#21327;&#21516;&#20316;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;&#29627;&#29827;&#31665;&#26041;&#27861;&#30340;&#21152;&#27861;&#24615;&#36136;&#30830;&#20445;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#29627;&#29827;&#31665;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#20174;&#20840;&#23616;&#21644;&#23454;&#20363;&#35282;&#24230;&#35299;&#37322;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#22823;&#22810;&#25968;&#22522;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18629v2 Announce Type: replace  Abstract: Machine learning models (e.g., neural networks) achieve high accuracy in wind power forecasting, but they are usually regarded as black boxes that lack interpretability. To address this issue, the paper proposes a glass-box approach that combines high accuracy with transparency for wind power forecasting. Specifically, the core is to sum up the feature effects by constructing shape functions, which effectively map the intricate non-linear relationships between wind power output and input features. Furthermore, the forecasting model is enriched by incorporating interaction terms that adeptly capture interdependencies and synergies among the input features. The additive nature of the proposed glass-box approach ensures its interpretability. Simulation results show that the proposed glass-box approach effectively interprets the results of wind power forecasting from both global and instance perspectives. Besides, it outperforms most ben
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#23581;&#35797;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#8220;&#20114;&#34917;&#8221;&#30693;&#35782;&#30340;&#20256;&#36882;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2310.17653</link><description>&lt;p&gt;
&#20851;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#21644;&#21069;&#26223;&#30340;&#36890;&#29992;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.17653
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#23581;&#35797;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#8220;&#20114;&#34917;&#8221;&#30693;&#35782;&#30340;&#20256;&#36882;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#38656;&#35201;&#20851;&#20110;&#26550;&#26500;&#12289;&#25968;&#25454;&#22686;&#24378;&#25110;&#20248;&#21270;&#31561;&#21508;&#31181;&#35774;&#35745;&#20915;&#31574;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#35757;&#32451;&#21464;&#21270;&#23548;&#33268;&#32593;&#32476;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#29420;&#29305;&#30340;&#29305;&#24449;&#38598;&#12290;&#21033;&#29992;&#21253;&#25324;&#25968;&#21315;&#20010;&#27169;&#22411;&#22312;&#35832;&#22914;ImageNet&#20043;&#31867;&#30340;&#32463;&#20856;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20844;&#20849;&#27169;&#22411;&#24211;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23545;&#20110;&#20219;&#24847;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#37197;&#23545;&#65292;&#19968;&#20010;&#27169;&#22411;&#25552;&#21462;&#20986;&#21478;&#19968;&#20010;&#27169;&#22411;&#20013;&#19981;&#21487;&#29992;&#30340;&#37325;&#35201;&#25968;&#25454;&#32972;&#26223;--&#32780;&#36825;&#19982;&#25972;&#20307;&#24615;&#33021;&#26080;&#20851;&#12290;&#37492;&#20110;&#20219;&#20309;&#20219;&#24847;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#37197;&#23545;&#20197;&#21450;&#27809;&#26377;&#22806;&#37096;&#25490;&#21517;&#65288;&#22914;&#29420;&#31435;&#27979;&#35797;&#38598;&#65292;&#20363;&#22914;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#65289;&#65292;&#25105;&#20204;&#25506;&#35752;&#26159;&#21542;&#21487;&#33021;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20174;&#19968;&#20010;&#27169;&#22411;&#36716;&#31227;&#36825;&#31181;&#8220;&#20114;&#34917;&#8221;&#30693;&#35782;&#21040;&#21478;&#19968;&#20010;&#27169;&#22411;--&#36825;&#19968;&#20219;&#21153;&#23588;&#20854;&#22256;&#38590;&#65292;&#22240;&#20026;&#26356;&#24378;&#22823;&#12289;&#24615;&#33021;&#30456;&#21516;&#25110;&#26356;&#24369;&#30340;&#27169;&#22411;&#20013;&#21487;&#33021;&#21253;&#21547;&#39069;&#22806;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20419;&#36827;&#24378;&#22823;&#30340;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.17653v2 Announce Type: replace  Abstract: Training deep networks requires various design decisions regarding for instance their architecture, data augmentation, or optimization. In this work, we find these training variations to result in networks learning unique feature sets from the data. Using public model libraries comprising thousands of models trained on canonical datasets like ImageNet, we observe that for arbitrary pairings of pretrained models, one model extracts significant data context unavailable in the other -- independent of overall performance. Given any arbitrary pairing of pretrained models and no external rankings (such as separate test sets, e.g. due to data privacy), we investigate if it is possible to transfer such "complementary" knowledge from one model to another without performance degradation -- a task made particularly difficult as additional knowledge can be contained in stronger, equiperformant or weaker models. Yet facilitating robust transfer i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22320;&#38754;&#28857;&#39068;&#33394;&#21270;&#65288;GPC&#65289;&#30340;&#21019;&#26032;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25945;&#23548;&#27169;&#22411;&#30528;&#33394;LiDAR&#28857;&#20113;&#65292;&#25552;&#20379;&#23453;&#36149;&#30340;&#35821;&#20041;&#32447;&#32034;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.14592</link><description>&lt;p&gt;
&#36890;&#36807;&#39068;&#33394;&#21270;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;LiDAR&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pre-Training LiDAR-Based 3D Object Detectors Through Colorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22320;&#38754;&#28857;&#39068;&#33394;&#21270;&#65288;GPC&#65289;&#30340;&#21019;&#26032;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25945;&#23548;&#27169;&#22411;&#30528;&#33394;LiDAR&#28857;&#20113;&#65292;&#25552;&#20379;&#23453;&#36149;&#30340;&#35821;&#20041;&#32447;&#32034;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#29702;&#35299;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;LiDAR&#28857;&#20113;&#65292;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;Grounded Point Colorization (GPC)&#65292;&#36890;&#36807;&#25945;&#23548;&#27169;&#22411;&#30528;&#33394;LiDAR&#28857;&#20113;&#26469;&#24357;&#21512;&#25968;&#25454;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20026;&#20854;&#25552;&#20379;&#23453;&#36149;&#30340;&#35821;&#20041;&#32447;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#39068;&#33394;&#21464;&#21270;&#21644;&#36873;&#25321;&#20559;&#24046;&#24341;&#36215;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#30528;&#33394;&#36807;&#31243;&#20013;&#25552;&#20379;&#22320;&#38754;&#30495;&#23454;&#39068;&#33394;&#20316;&#20026;&#25552;&#31034;&#65292;&#23558;&#39068;&#33394;&#20316;&#20026;&#8220;&#19978;&#19979;&#25991;&#8221;&#26469;&#21152;&#20197;&#32771;&#34385;&#12290;&#22312;KITTI&#21644;Waymo&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;GPC&#30340;&#26174;&#33879;&#26377;&#25928;&#24615;&#12290;&#21363;&#20351;&#26159;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;GPC&#20063;&#26174;&#30528;&#25552;&#39640;&#20102;&#24494;&#35843;&#24615;&#33021;&#65307;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20165;&#20351;&#29992;KITTI&#25968;&#25454;&#38598;&#30340;20%&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;GPC&#30340;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;&#25972;&#20010;&#25968;&#25454;&#38598;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#20026;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;&#24341;&#20837;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14592v2 Announce Type: replace-cross  Abstract: Accurate 3D object detection and understanding for self-driving cars heavily relies on LiDAR point clouds, necessitating large amounts of labeled data to train. In this work, we introduce an innovative pre-training approach, Grounded Point Colorization (GPC), to bridge the gap between data and labels by teaching the model to colorize LiDAR point clouds, equipping it with valuable semantic cues. To tackle challenges arising from color variations and selection bias, we incorporate color as "context" by providing ground-truth colors as hints during colorization. Experimental results on the KITTI and Waymo datasets demonstrate GPC's remarkable effectiveness. Even with limited labeled data, GPC significantly improves fine-tuning performance; notably, on just 20% of the KITTI dataset, GPC outperforms training from scratch with the entire dataset. In sum, we introduce a fresh perspective on pre-training for 3D object detection, aligni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#30740;&#31350;&#38646;&#27425;&#25552;&#31034;&#21644;&#23569;&#37327;&#19978;&#19979;&#25991;&#25552;&#31034;&#31561;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#19982;&#24403;&#21069;BLI&#26041;&#27861;&#30456;&#27604;&#65292;&#24182;&#22914;&#20309;&#36827;&#34892;&#34917;&#20805;&#12290;</title><link>https://arxiv.org/abs/2310.13995</link><description>&lt;p&gt;
&#20851;&#20110;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
On Bilingual Lexicon Induction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.13995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#30740;&#31350;&#38646;&#27425;&#25552;&#31034;&#21644;&#23569;&#37327;&#19978;&#19979;&#25991;&#25552;&#31034;&#31561;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#19982;&#24403;&#21069;BLI&#26041;&#27861;&#30456;&#27604;&#65292;&#24182;&#22914;&#20309;&#36827;&#34892;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;&#65288;BLI&#65289;&#26159;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#65292;&#30446;&#21069;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#28982;&#20381;&#36182;&#20110;&#35745;&#31639;&#36328;&#35821;&#35328;&#21333;&#35789;&#34920;&#31034;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#33539;&#24335;&#36716;&#21464;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#26032;&#19968;&#20195;LLMs&#22312;&#21452;&#35821;&#35789;&#27719;&#24320;&#21457;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#20419;&#20351;&#21644;&#24494;&#35843;&#22810;&#35821;&#35328;LLMs&#65288;mLLMs&#65289;&#20197;&#36827;&#34892;BLI&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#19982;&#24403;&#21069;BLI&#26041;&#27861;&#30456;&#27604;&#22914;&#20309;&#20197;&#21450;&#22914;&#20309;&#34917;&#20805;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;1&#65289;&#29992;&#20110;&#26080;&#30417;&#30563;BLI&#30340;&#38646;&#27425;&#25552;&#31034;&#21644;2&#65289;&#20351;&#29992;&#19968;&#32452;&#31181;&#23376;&#32763;&#35793;&#23545;&#36827;&#34892;&#23569;&#37327;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#22343;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;LLM&#24494;&#35843;&#65292;&#20197;&#21450;3&#65289;&#23545;&#36739;&#23567;LLMs&#36827;&#34892;&#26631;&#20934;BLI&#23548;&#21521;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#19981;&#21516;&#22823;&#23567;&#65288;&#20174;0.3B&#21040;13B&#21442;&#25968;&#65289;&#30340;18&#20010;&#24320;&#28304;&#25991;&#26412;&#23545;&#25991;&#26412;mLLMs&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#28085;&#30422;&#20004;&#20010;&#26631;&#20934;BLI&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13995v2 Announce Type: replace-cross  Abstract: Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a rang
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#20984;&#30446;&#26631;&#21644;&#20984;&#32422;&#26463;&#30340;&#26368;&#22351;&#24773;&#20917;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2310.10117</link><description>&lt;p&gt;
&#20855;&#26377;&#20984;&#20840;&#23616;&#21644;&#23616;&#37096;&#32422;&#26463;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Convex Global and Local Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#20984;&#30446;&#26631;&#21644;&#20984;&#32422;&#26463;&#30340;&#26368;&#22351;&#24773;&#20917;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#38382;&#39064;&#37117;&#24102;&#26377;&#32422;&#26463;&#65292;&#20854;&#24212;&#29992;&#39046;&#22495;&#28041;&#21450;&#26080;&#27861;&#19982;&#20182;&#20154;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#25935;&#24863;&#25968;&#25454;&#65292;&#20363;&#22914;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#12290;&#22312;&#36825;&#31181;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#21327;&#20316;&#23398;&#20064;&#28041;&#21450;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#29992;&#20110;&#24102;&#32422;&#26463;&#30340;ML&#38382;&#39064;&#65292;&#25110;&#31616;&#31216;&#24102;&#32422;&#26463;&#30340;FL&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;FL&#25216;&#26415;&#24471;&#21040;&#20102;&#24191;&#27867;&#21457;&#23637;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#20165;&#22788;&#29702;&#26080;&#32422;&#26463;&#30340;FL&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#35299;&#20915;&#24102;&#32422;&#26463;FL&#38382;&#39064;&#30340;&#36890;&#29992;&#31639;&#27861;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#36817;&#31471;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#65288;AL&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21463;&#38480;&#21046;ML&#38382;&#39064;&#30340;&#26032;FL&#31639;&#27861;&#12290;&#20551;&#35774;&#20984;&#30446;&#26631;&#21644;&#20984;&#32422;&#26463;&#20197;&#21450;&#20854;&#20182;&#19968;&#20123;&#28201;&#21644;&#26465;&#20214;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26368;&#22351;&#24773;&#20917;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10117v2 Announce Type: replace  Abstract: In practice, many machine learning (ML) problems come with constraints, and their applied domains involve distributed sensitive data that cannot be shared with others, e.g., in healthcare. Collaborative learning in such practical scenarios entails federated learning (FL) for ML problems with constraints, or FL with constraints for short. Despite the extensive developments of FL techniques in recent years, these techniques only deal with unconstrained FL problems. To fill this gap, we take the first step toward building a general algorithmic framework for solving FL problems with constraints. In particular, we propose a new FL algorithm for constrained ML problems based on the proximal augmented Lagrangian (AL) method. Assuming convex objective and convex constraints plus other mild conditions, we establish the worst-case complexity of the proposed algorithm. Our numerical experiments show the effectiveness of our algorithm in perform
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.02174</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#30340;&#35810;&#38382;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#20013;&#30340;&#25671;&#25670;
&lt;/p&gt;
&lt;p&gt;
Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02174
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24448;&#24448;&#22312;&#20854;&#21028;&#26029;&#19978;&#25671;&#25670;&#19981;&#23450;&#65292;&#21363;&#20351;&#21407;&#22987;&#21028;&#26029;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#25671;&#25670;&#23545;&#20110;&#29983;&#25104;&#21487;&#38752;&#22238;&#22797;&#21644;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#20197;&#21450;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#30830;&#35748;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#29992;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#30340;&#26694;&#26550;Unwavering-FQ&#65292;&#36890;&#36807;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#20445;&#25345;&#20854;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#22686;&#24378;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#36341;&#23454;&#39564;&#21644;&#29702;&#35770;&#27934;&#23519;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#65292;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.02124</link><description>&lt;p&gt;
&#25506;&#32034;LLM&#20195;&#29702;&#30340;&#21327;&#20316;&#26426;&#21046;&#65306;&#31038;&#20250;&#24515;&#29702;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02124
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#36341;&#23454;&#39564;&#21644;&#29702;&#35770;&#27934;&#23519;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#65292;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#31038;&#20250;&#29615;&#22659;&#20013;&#65292;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#36825;&#20123;NLP&#31995;&#32479;&#33021;&#21542;&#27169;&#20223;&#31867;&#20154;&#31867;&#30340;&#21327;&#20316;&#26234;&#33021;&#65292;&#22312;&#30001;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32452;&#25104;&#30340;&#22810;&#20195;&#29702;&#31038;&#20250;&#20013;&#65311;&#26412;&#25991;&#36890;&#36807;&#23558;&#23454;&#36341;&#23454;&#39564;&#19982;&#29702;&#35770;&#35266;&#28857;&#30456;&#32467;&#21512;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#30001;LLM&#20195;&#29702;&#32452;&#25104;&#30340;&#29420;&#29305;&#8220;&#31038;&#20250;&#8221;&#65292;&#27599;&#20010;&#20195;&#29702;&#20197;&#29305;&#23450;&#30340;&#8220;&#29305;&#36136;&#8221;&#65288;&#38543;&#21644;&#25110;&#36807;&#20110;&#33258;&#20449;&#65289;&#20026;&#29305;&#24449;&#65292;&#24182;&#19982;&#19981;&#21516;&#30340;&#8220;&#24605;&#32500;&#27169;&#24335;&#8221;&#65288;&#36777;&#35770;&#25110;&#21453;&#24605;&#65289;&#23637;&#24320;&#21327;&#20316;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#36825;&#20123;&#22810;&#20195;&#29702;&#31038;&#20250;&#65292;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#19981;&#20165;&#32988;&#36807;&#20808;&#21069;&#39030;&#23574;&#26041;&#27861;&#65292;&#32780;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#65288;&#20351;&#29992;&#26356;&#23569;&#30340;API&#20196;&#29260;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#35828;&#26126;LLM&#20195;&#29702;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02124v2 Announce Type: replace-cross  Abstract: As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents mani
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Galvatron-BMW&#65292;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#34913;&#20869;&#23384;&#24037;&#20316;&#36127;&#36733;&#20248;&#21270;&#65292;&#38598;&#25104;&#22810;&#20010;&#24182;&#34892;&#32500;&#24230;&#24182;&#33258;&#21160;&#35782;&#21035;&#26368;&#26377;&#25928;&#30340;&#28151;&#21512;&#24182;&#34892;&#31574;&#30053;&#65292;&#36890;&#36807;&#20915;&#31574;&#26641;&#26041;&#27861;&#21644;&#21160;&#24577;&#35268;&#21010;&#25628;&#32034;&#31639;&#27861;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#30340;&#35757;&#32451;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2307.02031</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#34913;&#20869;&#23384;&#24037;&#20316;&#36127;&#36733;&#20248;&#21270;&#25913;&#36827;&#33258;&#21160;&#24182;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Improving Automatic Parallel Training via Balanced Memory Workload Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.02031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Galvatron-BMW&#65292;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#34913;&#20869;&#23384;&#24037;&#20316;&#36127;&#36733;&#20248;&#21270;&#65292;&#38598;&#25104;&#22810;&#20010;&#24182;&#34892;&#32500;&#24230;&#24182;&#33258;&#21160;&#35782;&#21035;&#26368;&#26377;&#25928;&#30340;&#28151;&#21512;&#24182;&#34892;&#31574;&#30053;&#65292;&#36890;&#36807;&#20915;&#31574;&#26641;&#26041;&#27861;&#21644;&#21160;&#24577;&#35268;&#21010;&#25628;&#32034;&#31639;&#27861;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#30340;&#35757;&#32451;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#24050;&#25104;&#20026;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#39046;&#20808;&#26041;&#27861;&#65292;&#20026;&#39640;&#32423;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;(DL)&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24182;&#34892;&#36873;&#39033;&#30340;&#20016;&#23500;&#24615;&#65292;&#36328;&#22810;&#20010;GPU&#26377;&#25928;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;DL&#31995;&#32479;&#35201;&#20040;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#20998;&#24067;&#24335;&#35757;&#32451;&#35745;&#21010;&#65292;&#35201;&#20040;&#23558;&#24182;&#34892;&#32452;&#21512;&#38480;&#21046;&#22312;&#32422;&#26463;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Galvatron-BMW&#65292;&#19968;&#20010;&#38598;&#25104;&#22810;&#20010;&#20027;&#27969;&#24182;&#34892;&#32500;&#24230;&#24182;&#33258;&#21160;&#30830;&#23450;&#26368;&#26377;&#25928;&#28151;&#21512;&#24182;&#34892;&#31574;&#30053;&#30340;&#26032;&#22411;&#31995;&#32479;&#26694;&#26550;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#36941;&#21382;&#36825;&#20010;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#30452;&#35266;&#35265;&#35299;&#30340;&#20915;&#31574;&#26641;&#26041;&#27861;&#36827;&#34892;&#20998;&#35299;&#21644;&#20462;&#21098;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#25628;&#32034;&#31639;&#27861;&#25512;&#20986;&#26368;&#20248;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.02031v2 Announce Type: replace  Abstract: Transformer models have emerged as the leading approach for achieving state-of-the-art performance across various application domains, serving as the foundation for advanced large-scale deep learning (DL) models. However, efficiently training these models across multiple GPUs remains a complex challenge due to the abundance of parallelism options. Existing DL systems either require manual efforts to design distributed training plans or limit parallelism combinations to a constrained search space. In this paper, we present Galvatron-BMW, a novel system framework that integrates multiple prevalent parallelism dimensions and automatically identifies the most efficient hybrid parallelism strategy. To effectively navigate this vast search space, we employ a decision tree approach for decomposition and pruning based on intuitive insights. We further utilize a dynamic programming search algorithm to derive the optimal plan. Moreover, to imp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#20462;&#22797;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#30830;&#35748;&#20559;&#35265;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#27010;&#24565;&#19982;&#65288;&#20122;&#31526;&#21495;&#65289;&#35299;&#37322;&#20043;&#38388;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#20041;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2307.00897</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#20462;&#22797;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#30830;&#35748;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fixing confirmation bias in feature attribution methods via semantic match
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#20462;&#22797;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#30830;&#35748;&#20559;&#35265;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#27010;&#24565;&#19982;&#65288;&#20122;&#31526;&#21495;&#65289;&#35299;&#37322;&#20043;&#38388;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#20041;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#26512;&#40657;&#30418;&#27169;&#22411;&#22797;&#26434;&#34892;&#20026;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19968;&#20123;&#23398;&#32773;&#25351;&#20986;&#36825;&#31867;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65306;&#23427;&#20204;&#19981;&#33021;&#21487;&#38752;&#22320;&#29992;&#20154;&#31867;&#27010;&#24565;&#36827;&#34892;&#35299;&#37322;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#20165;&#20165;&#21487;&#35270;&#21270;&#19968;&#31995;&#21015;&#29305;&#24449;&#36129;&#29486;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#26080;&#27861;&#24471;&#20986;&#20851;&#20110;&#27169;&#22411;&#20869;&#37096;&#34920;&#31034;&#30340;&#32467;&#35770;&#65292;&#32780;&#30830;&#35748;&#20559;&#35265;&#21487;&#33021;&#20250;&#35753;&#29992;&#25143;&#20135;&#29983;&#20851;&#20110;&#27169;&#22411;&#34892;&#20026;&#30340;&#38169;&#35823;&#20449;&#24565;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#39564;&#35777;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#20551;&#35774;&#26159;&#21542;&#24471;&#21040;&#20102;&#29305;&#24449;&#24402;&#22240;&#30340;&#30830;&#35748;&#12290;&#36825;&#23601;&#26159;&#25105;&#20204;&#25152;&#35828;&#30340;&#20154;&#31867;&#27010;&#24565;&#19982;&#65288;&#20122;&#31526;&#21495;&#65289;&#35299;&#37322;&#20043;&#38388;&#30340;&#8220;&#35821;&#20041;&#21305;&#37197;&#8221;&#12290;&#22312; Cin\`a&#31561;&#20154;[2023]&#25552;&#20986;&#30340;&#27010;&#24565;&#26694;&#26550;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#22312;&#23454;&#36341;&#20013;&#35780;&#20272;&#35821;&#20041;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36825;&#19968;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00897v2 Announce Type: replace-cross  Abstract: Feature attribution methods have become a staple method to disentangle the complex behavior of black box models. Despite their success, some scholars have argued that such methods suffer from a serious flaw: they do not allow a reliable interpretation in terms of human concepts. Simply put, visualizing an array of feature contributions is not enough for humans to conclude something about a model's internal representations, and confirmation bias can trick users into false beliefs about model behavior. We argue that a structured approach is required to test whether our hypotheses on the model are confirmed by the feature attributions. This is what we call the "semantic match" between human concepts and (sub-symbolic) explanations. Building on the conceptual framework put forward in Cin\`a et al. [2023], we propose a structured approach to evaluate semantic match in practice. We showcase the procedure in a suite of experiments spa
&lt;/p&gt;</description></item><item><title>&#22312;&#22806;&#22495;&#26816;&#27979;&#24615;&#33021;&#20013;&#65292;&#22810;&#26679;&#24615;&#23545;&#20110;&#24322;&#24120;&#26679;&#26412;&#30340;&#37319;&#26679;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DOS&#30340;&#22810;&#26679;&#21270;&#24322;&#24120;&#26679;&#26412;&#37319;&#26679;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2306.02031</link><description>&lt;p&gt;
DOS&#65306;&#22810;&#26679;&#21270;&#24322;&#24120;&#26679;&#26412;&#37319;&#26679;&#29992;&#20110;&#22806;&#22495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DOS: Diverse Outlier Sampling for Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02031
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22806;&#22495;&#26816;&#27979;&#24615;&#33021;&#20013;&#65292;&#22810;&#26679;&#24615;&#23545;&#20110;&#24322;&#24120;&#26679;&#26412;&#30340;&#37319;&#26679;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DOS&#30340;&#22810;&#26679;&#21270;&#24322;&#24120;&#26679;&#26412;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#37096;&#32626;&#22312;&#24320;&#25918;&#19990;&#30028;&#26102;&#65292;&#23545;&#22806;&#22495;&#36755;&#20837;&#32473;&#20986;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#36890;&#24120;&#20250;&#21033;&#29992;&#26367;&#20195;&#24322;&#24120;&#25968;&#25454;&#38598;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#27169;&#22411;&#36827;&#34892;&#35268;&#33539;&#21270;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#35774;&#35745;&#24322;&#24120;&#25968;&#25454;&#38598;&#37319;&#26679;&#31574;&#30053;&#26102;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20165;&#22522;&#20110;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#36873;&#25321;&#30340;&#22806;&#22495;&#26679;&#26412;&#21487;&#33021;&#20559;&#21521;&#20110;&#26576;&#20123;&#31867;&#22411;&#65292;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#25972;&#20010;&#24322;&#24120;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#22810;&#26679;&#24615;&#23545;&#20110;&#37319;&#26679;&#22806;&#22495;&#26679;&#26412;&#20197;&#25552;&#21319;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DOS&#65288;&#22810;&#26679;&#21270;&#24322;&#24120;&#26679;&#26412;&#37319;&#26679;&#65289;&#30340;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#36873;&#25321;&#22810;&#26679;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#23545;&#26631;&#20934;&#21270;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#28982;&#21518;&#20174;&#27599;&#20010;&#31751;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#24322;&#24120;&#26679;&#26412;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02031v2 Announce Type: replace  Abstract: Modern neural networks are known to give overconfident prediction for out-of-distribution inputs when deployed in the open world. It is common practice to leverage a surrogate outlier dataset to regularize the model during training, and recent studies emphasize the role of uncertainty in designing the sampling strategy for outlier dataset. However, the OOD samples selected solely based on predictive uncertainty can be biased towards certain types, which may fail to capture the full outlier distribution. In this work, we empirically show that diversity is critical in sampling outliers for OOD detection performance. Motivated by the observation, we propose a straightforward and novel sampling strategy named DOS (Diverse Outlier Sampling) to select diverse and informative outliers. Specifically, we cluster the normalized features at each iteration, and the most informative outlier from each cluster is selected for model training with ab
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21644;&#28857;&#20113;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#22312;&#39640;&#31890;&#24230;&#25506;&#27979;&#22120;&#20013;&#20934;&#30830;&#24555;&#36895;&#22320;&#27169;&#25311;&#31890;&#23376;&#31751;&#65292;&#20026;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#25552;&#20379;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;</title><link>https://arxiv.org/abs/2305.04847</link><description>&lt;p&gt;
CaloClouds: &#24555;&#36895;&#20960;&#20309;&#29420;&#31435;&#12289;&#39640;&#31890;&#24230;&#37327;&#33021;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
CaloClouds: Fast Geometry-Independent Highly-Granular Calorimeter Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.04847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21644;&#28857;&#20113;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#22312;&#39640;&#31890;&#24230;&#25506;&#27979;&#22120;&#20013;&#20934;&#30830;&#24555;&#36895;&#22320;&#27169;&#25311;&#31890;&#23376;&#31751;&#65292;&#20026;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#25552;&#20379;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#31890;&#24230;&#25506;&#27979;&#22120;&#20013;&#27169;&#25311;&#31890;&#23376;&#31751;&#26159;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#21069;&#27839;&#12290;&#20351;&#29992;&#29983;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#21644;&#36895;&#24230;&#23558;&#20351;&#20854;&#33021;&#22815;&#22686;&#24378;&#20256;&#32479;&#27169;&#25311;&#65292;&#32531;&#35299;&#20027;&#35201;&#30340;&#35745;&#31639;&#32422;&#26463;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#27425;&#30452;&#25509;&#22312;3D&#31354;&#38388;&#20013;&#29983;&#25104;&#23569;&#37327;&#33021;&#37327;&#27785;&#31215;&#30340;&#31354;&#38388;&#28857;&#30340;&#28857;&#20113;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#22266;&#23450;&#32593;&#26684;&#32467;&#26500;&#65292;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#36825;&#35201;&#24402;&#21151;&#20110;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#65306;i)&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#25913;&#36827;&#65292;&#25105;&#20204;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20809;&#23376;&#31751;&#20316;&#20026;&#39640;&#22522;&#25968;&#30340;&#28857;&#20113;&#12290;ii)&#36825;&#20123;&#39640;&#36798;6000&#20010;&#31354;&#38388;&#28857;&#30340;&#28857;&#20113;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#20960;&#20309;&#26080;&#20851;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#20174;&#26368;&#21021;&#26368;&#39640;&#20998;&#36776;&#29575;&#39640;&#36798;40000&#20010;Geant4&#27493;&#39588;&#30340;&#28857;&#20113;&#20013;&#36827;&#34892;&#19979;&#37319;&#26679;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.04847v2 Announce Type: replace-cross  Abstract: Simulating showers of particles in highly-granular detectors is a key frontier in the application of machine learning to particle physics. Achieving high accuracy and speed with generative machine learning models would enable them to augment traditional simulations and alleviate a major computing constraint. This work achieves a major breakthrough in this task by, for the first time, directly generating a point cloud of a few thousand space points with energy depositions in the detector in 3D space without relying on a fixed-grid structure. This is made possible by two key innovations: i) Using recent improvements in generative modeling we apply a diffusion model to generate photon showers as high-cardinality point clouds. ii) These point clouds of up to $6,000$ space points are largely geometry-independent as they are down-sampled from initial even higher-resolution point clouds of up to $40,000$ so-called Geant4 steps. We sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20132;&#20114;&#26041;&#24335;&#35775;&#38382;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#26465;&#20214;&#20998;&#24067;&#26679;&#26412;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;HMM&#30340;&#39640;&#25928;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#32780;&#32469;&#36807;&#20102;&#20854;&#23494;&#30721;&#23398;&#22256;&#38590;&#24615;&#12290;</title><link>https://arxiv.org/abs/2302.14753</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#26679;&#26412;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Hidden Markov Models Using Conditional Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.14753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20132;&#20114;&#26041;&#24335;&#35775;&#38382;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#26465;&#20214;&#20998;&#24067;&#26679;&#26412;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;HMM&#30340;&#39640;&#25928;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#32780;&#32469;&#36807;&#20102;&#20854;&#23494;&#30721;&#23398;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#23398;&#20064;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMM&#65289;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#34429;&#28982;HMM&#26159;&#39034;&#24207;&#21644;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#20294;&#22312;&#26631;&#20934;&#35774;&#32622;&#19979;&#65292;&#21363;&#23545;&#35266;&#27979;&#24207;&#21015;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#26679;&#26412;&#20855;&#26377;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#36215;&#26469;&#26159;&#20855;&#26377;&#23494;&#30721;&#23398;&#22256;&#38590;&#24615;&#30340;&#12290;&#26412;&#25991;&#20559;&#31163;&#20102;&#36825;&#19968;&#35774;&#23450;&#65292;&#32771;&#34385;&#20102;&#19968;&#31181;&#20132;&#20114;&#35775;&#38382;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#31639;&#27861;&#21487;&#20197;&#26597;&#35810;HMM&#30340;&#26465;&#20214;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;HMM&#30340;&#20132;&#20114;&#35775;&#38382;&#21487;&#20197;&#23454;&#29616;&#35745;&#31639;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#32780;&#32469;&#36807;&#23494;&#30721;&#23398;&#22256;&#38590;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#23398;&#20064;HMM&#30340;&#39640;&#25928;&#31639;&#27861;&#65306;&#65288;a&#65289;&#19968;&#31181;&#26356;&#23481;&#26131;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#21487;&#20197;&#26597;&#35810;&#20934;&#30830;&#26465;&#20214;&#27010;&#29575;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;&#22810;&#39033;&#24335;&#27425;&#26597;&#35810;&#65292;&#20197;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#20013;&#36817;&#20284;&#20219;&#20309;HMM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.14753v2 Announce Type: replace-cross  Abstract: This paper is concerned with the computational complexity of learning the Hidden Markov Model (HMM). Although HMMs are some of the most widely used tools in sequential and time series modeling, they are cryptographically hard to learn in the standard setting where one has access to i.i.d. samples of observation sequences. In this paper, we depart from this setup and consider an interactive access model, in which the algorithm can query for samples from the conditional distributions of the HMMs. We show that interactive access to the HMM enables computationally efficient learning algorithms, thereby bypassing cryptographic hardness. Specifically, we obtain efficient algorithms for learning HMMs in two settings:   (a) An easier setting where we have query access to the exact conditional probabilities. Here our algorithm runs in polynomial time and makes polynomially many queries to approximate any HMM in total variation distance.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26102;&#38388;&#24310;&#36831;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#26631;&#28857;&#20462;&#22797;&#30340;&#39640;&#25928;&#38598;&#25104;&#26041;&#27861;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20339;&#27169;&#22411;&#25552;&#39640;&#20102;1.0&#20010;F1&#20998;&#25968;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#26356;&#23569;&#30340;&#25512;&#29702;&#32593;&#32476;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2302.13376</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#24310;&#36831;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#22810;&#27169;&#24577;&#26631;&#28857;&#20462;&#22797;&#30340;&#39640;&#25928;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Ensemble for Multimodal Punctuation Restoration using Time-Delay Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.13376
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#24310;&#36831;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#26631;&#28857;&#20462;&#22797;&#30340;&#39640;&#25928;&#38598;&#25104;&#26041;&#27861;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20339;&#27169;&#22411;&#25552;&#39640;&#20102;1.0&#20010;F1&#20998;&#25968;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#26356;&#23569;&#30340;&#25512;&#29702;&#32593;&#32476;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#28857;&#20462;&#22797;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#21518;&#22788;&#29702;&#36807;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#23545;&#27169;&#22411;&#25928;&#29575;&#30340;&#35201;&#27714;&#26159;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EfficientPunct&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#26102;&#38388;&#24310;&#36831;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21040;&#21313;&#20998;&#20043;&#19968;&#30340;&#25512;&#29702;&#32593;&#32476;&#21442;&#25968;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#27169;&#22411;1.0&#20010;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#31616;&#21270;&#20102;&#35821;&#38899;&#35782;&#21035;&#22120;&#65292;&#20197;&#26377;&#25928;&#22320;&#36755;&#20986;&#29992;&#20110;&#26631;&#28857;&#20462;&#22797;&#30340;&#38544;&#34255;&#23618;&#22768;&#23398;&#23884;&#20837;&#65292;&#21516;&#26102;&#20351;&#29992;BERT&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#25991;&#26412;&#23884;&#20837;&#12290;&#36890;&#36807;&#20351;&#29992;&#24378;&#21046;&#23545;&#40784;&#21644;&#26102;&#38388;&#21367;&#31215;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#24517;&#35201;&#24615;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;EfficientPunct&#36890;&#36807;&#19968;&#20010;&#38598;&#25104;&#26041;&#27861;&#65292;&#23545;BERT&#32431;&#31929;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#30340;&#26435;&#37325;&#30053;&#39640;&#20110;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#39044;&#27979;&#65292;&#26641;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://githu
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.13376v2 Announce Type: replace  Abstract: Punctuation restoration plays an essential role in the post-processing procedure of automatic speech recognition, but model efficiency is a key requirement for this task. To that end, we present EfficientPunct, an ensemble method with a multimodal time-delay neural network that outperforms the current best model by 1.0 F1 points, using less than a tenth of its inference network parameters. We streamline a speech recognizer to efficiently output hidden layer acoustic embeddings for punctuation restoration, as well as BERT to extract meaningful text embeddings. By using forced alignment and temporal convolutions, we eliminate the need for attention-based fusion, greatly increasing computational efficiency and raising performance. EfficientPunct sets a new state of the art with an ensemble that weights BERT's purely language-based predictions slightly more than the multimodal network's predictions. Our code is available at https://githu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30830;&#35748;&#20102;&#20613;&#37324;&#21494;&#32423;&#25968;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21704;&#23494;&#39039;&#32534;&#30721;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#20613;&#37324;&#21494;&#31995;&#25968;&#30340;&#30830;&#23450;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2302.00105</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20613;&#37324;&#21494;&#32423;&#25968;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Fourier series weight in quantum machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30830;&#35748;&#20102;&#20613;&#37324;&#21494;&#32423;&#25968;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21704;&#23494;&#39039;&#32534;&#30721;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#20613;&#37324;&#21494;&#31995;&#25968;&#30340;&#30830;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#30830;&#35748;&#20613;&#37324;&#21494;&#32423;&#25968;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#27169;&#22411;&#12289;&#27979;&#35797;&#21644;&#28436;&#31034;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#21704;&#23494;&#39039;&#32534;&#30721;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#24494;&#22937;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#35282;&#25554;&#20540;&#12289;&#20108;&#36827;&#21046;&#21644;&#22810;&#31867;&#20998;&#31867;&#22120;&#20197;&#21450;&#37327;&#23376;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30830;&#23450;&#20613;&#37324;&#21494;&#31995;&#25968;&#30340;&#26041;&#22359;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;Pennylane&#26694;&#26550;&#25191;&#34892;&#21644;&#27979;&#35797;&#20102;&#25152;&#26377;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.00105v2 Announce Type: replace-cross  Abstract: In this work, we aim to confirm the impact of the Fourier series on the quantum machine learning model. We will propose models, tests, and demonstrations to achieve this objective. We designed a quantum machine learning leveraged on the Hamiltonian encoding. With a subtle change, we performed the trigonometric interpolation, binary and multiclass classifier, and a quantum signal processing application. We also proposed a block diagram of determining approximately the Fourier coefficient based on quantum machine learning. We performed and tested all the proposed models using the Pennylane framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20351;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#19987;&#27880;&#20110;&#23569;&#25968;&#26679;&#26412;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2301.12334</link><description>&lt;p&gt;
&#19981;&#20559;&#19981;&#20506;&#65306;&#23569;&#25968;&#26063;&#32676;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Don't Play Favorites: Minority Guidance for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.12334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20351;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#19987;&#27880;&#20110;&#23569;&#25968;&#26679;&#26412;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#23569;&#25968;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;&#23569;&#25968;&#26679;&#26412;&#26159;&#20301;&#20110;&#25968;&#25454;&#27969;&#24418;&#20302;&#23494;&#24230;&#21306;&#22495;&#30340;&#23454;&#20363;&#12290;&#29983;&#25104;&#36275;&#22815;&#25968;&#37327;&#30340;&#36825;&#31181;&#23569;&#25968;&#26679;&#26412;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#21253;&#21547;&#25968;&#25454;&#30340;&#19968;&#20123;&#29420;&#29305;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#20284;&#28982;&#24615;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#20256;&#32479;&#29983;&#25104;&#36807;&#31243;&#20027;&#35201;&#20135;&#29983;&#22823;&#22810;&#25968;&#26679;&#26412;&#65288;&#20301;&#20110;&#27969;&#24418;&#39640;&#23494;&#24230;&#21306;&#22495;&#65289;&#65292;&#20351;&#33258;&#36523;&#23545;&#23569;&#25968;&#29983;&#25104;&#20219;&#21153;&#26080;&#25928;&#19988;&#32791;&#26102;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#19987;&#27880;&#20110;&#23569;&#25968;&#26679;&#26412;&#12290;&#39318;&#20808;&#24378;&#35843; Tweedie &#30340;&#38477;&#22122;&#20844;&#24335;&#23545;&#22823;&#22810;&#25968;&#26679;&#26412;&#20135;&#29983;&#26377;&#21033;&#32467;&#26524;&#12290;&#36825;&#19968;&#35266;&#23519;&#28608;&#21169;&#25105;&#20204;&#24341;&#20837;&#25551;&#36848;&#32473;&#23450;&#26679;&#26412;&#29420;&#29305;&#24615;&#30340;&#24230;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#22909;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.12334v2 Announce Type: replace-cross  Abstract: We explore the problem of generating minority samples using diffusion models. The minority samples are instances that lie on low-density regions of a data manifold. Generating a sufficient number of such minority instances is important, since they often contain some unique attributes of the data. However, the conventional generation process of the diffusion models mostly yields majority samples (that lie on high-density regions of the manifold) due to their high likelihoods, making themselves ineffective and time-consuming for the minority generating task. In this work, we present a novel framework that can make the generation process of the diffusion models focus on the minority samples. We first highlight that Tweedie's denoising formula yields favorable results for majority samples. The observation motivates us to introduce a metric that describes the uniqueness of a given sample. To address the inherent preference of the di
&lt;/p&gt;</description></item><item><title>CEDAS&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#26080;&#20559;&#21387;&#32553;&#36816;&#31639;&#31526;&#19979;&#20855;&#26377;&#19982;&#38598;&#20013;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#26368;&#30701;&#30340;&#30636;&#24577;&#26102;&#38388;&#65292;&#23545;&#20809;&#28369;&#24378;&#20984;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#37117;&#36866;&#29992;&#12290;</title><link>https://arxiv.org/abs/2301.05872</link><description>&lt;p&gt;
CEDAS&#65306;&#19968;&#31181;&#20855;&#26377;&#25913;&#36827;&#25910;&#25947;&#24615;&#30340;&#21387;&#32553;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#27861;
&lt;/p&gt;
&lt;p&gt;
CEDAS: A Compressed Decentralized Stochastic Gradient Method with Improved Convergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.05872
&lt;/p&gt;
&lt;p&gt;
CEDAS&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#26080;&#20559;&#21387;&#32553;&#36816;&#31639;&#31526;&#19979;&#20855;&#26377;&#19982;&#38598;&#20013;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#26368;&#30701;&#30340;&#30636;&#24577;&#26102;&#38388;&#65292;&#23545;&#20809;&#28369;&#24378;&#20984;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#37117;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#36890;&#20449;&#21463;&#38480;&#29615;&#22659;&#19979;&#35299;&#20915;&#22810;&#20195;&#29702;&#32593;&#32476;&#19978;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20855;&#26377;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#21387;&#32553;&#31934;&#30830;&#25193;&#25955;&#65288;CEDAS&#65289;&#8221;&#30340;&#21387;&#32553;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#20559;&#21387;&#32553;&#36816;&#31639;&#31526;&#19979;&#28176;&#36817;&#22320;&#23454;&#29616;&#20102;&#19982;&#38598;&#20013;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#20809;&#28369;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#20809;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;CEDAS&#36804;&#20170;&#20026;&#27490;&#20197;&#20854;&#26368;&#30701;&#30340;&#30636;&#24577;&#26102;&#38388;&#65288;&#20851;&#20110;&#22270;&#30340;&#29305;&#24615;&#65289;&#23454;&#29616;&#20102;&#19982;&#38598;&#20013;&#24335;SGD&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#22312;&#20809;&#28369;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#19979;&#34920;&#29616;&#20026;$\mathcal{O}(n{C^3}/(1-\lambda_2)^{2})$&#65292;&#22312;&#20809;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#19979;&#34920;&#29616;&#20026;$\mathcal{O}(n^3{C^6}/(1-\lambda_2)^4)$&#65292;&#20854;&#20013;$(1-\lambda_2)$&#34920;&#31034;&#35889;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.05872v2 Announce Type: replace-cross  Abstract: In this paper, we consider solving the distributed optimization problem over a multi-agent network under the communication restricted setting. We study a compressed decentralized stochastic gradient method, termed ``compressed exact diffusion with adaptive stepsizes (CEDAS)", and show the method asymptotically achieves comparable convergence rate as centralized { stochastic gradient descent (SGD)} for both smooth strongly convex objective functions and smooth nonconvex objective functions under unbiased compression operators. In particular, to our knowledge, CEDAS enjoys so far the shortest transient time (with respect to the graph specifics) for achieving the convergence rate of centralized SGD, which behaves as $\mathcal{O}(n{C^3}/(1-\lambda_2)^{2})$ under smooth strongly convex objective functions, and $\mathcal{O}(n^3{C^6}/(1-\lambda_2)^4)$ under smooth nonconvex objective functions, where $(1-\lambda_2)$ denotes the spectr
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22312;CT&#32925;&#33039;&#19978;&#26816;&#27979;&#32925;&#32420;&#32500;&#21270;&#30340;&#25918;&#23556;&#32452;&#23398;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#26524;&#21457;&#29616;&#38750;&#22686;&#24378;CT&#22312;&#24179;&#22343;AUC&#20540;&#19978;&#20248;&#20110;&#22686;&#24378;CT&#12290;</title><link>https://arxiv.org/abs/2211.14396</link><description>&lt;p&gt;
&#20351;&#29992;&#25918;&#23556;&#32452;&#23398;&#22312;CT&#22270;&#20687;&#19978;&#36827;&#34892;&#38750;&#20405;&#20837;&#24615;&#32925;&#32420;&#32500;&#21270;&#31579;&#26597;
&lt;/p&gt;
&lt;p&gt;
Non-invasive Liver Fibrosis Screening on CT Images using Radiomics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.14396
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22312;CT&#32925;&#33039;&#19978;&#26816;&#27979;&#32925;&#32420;&#32500;&#21270;&#30340;&#25918;&#23556;&#32452;&#23398;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#26524;&#21457;&#29616;&#38750;&#22686;&#24378;CT&#22312;&#24179;&#22343;AUC&#20540;&#19978;&#20248;&#20110;&#22686;&#24378;CT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#24320;&#21457;&#24182;&#35780;&#20272;&#19968;&#31181;&#29992;&#20110;&#22312;CT&#32925;&#33039;&#19978;&#26816;&#27979;&#32925;&#32420;&#32500;&#21270;&#30340;&#25918;&#23556;&#32452;&#23398;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26041;&#27861;&#65306;&#38024;&#23545;&#36825;&#39033;&#22238;&#39038;&#24615;&#30340;&#21333;&#20013;&#24515;&#30740;&#31350;&#65292;&#20174;&#25509;&#21463;&#21516;&#26102;&#32925;&#27963;&#26816;&#21644;CT&#26816;&#26597;&#30340;&#24739;&#32773;&#30340;CT&#22270;&#20687;&#19978;&#25552;&#21462;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#12290;&#22522;&#20110;&#38543;&#26426;&#25918;&#32622;&#30340;ROI&#30340;&#24179;&#22343;&#27979;&#35797;&#19979;ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65292;&#30830;&#23450;&#20102;&#23545;&#27604;&#24230;&#12289;&#24402;&#19968;&#21270;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#32452;&#21512;&#12290;&#20855;&#26377;&#26368;&#39640;AUC&#30340;&#32452;&#21512;&#21644;&#36873;&#23450;&#30340;&#29305;&#24449;&#34987;&#29992;&#26469;&#24320;&#21457;&#26368;&#32456;&#30340;&#32925;&#32420;&#32500;&#21270;&#31579;&#26597;&#27169;&#22411;&#12290;&#32467;&#26524;&#65306;&#30740;&#31350;&#21253;&#25324;101&#21517;&#30007;&#24615;&#21644;68&#21517;&#22899;&#24615;&#24739;&#32773;&#65288;&#24179;&#22343;&#24180;&#40836;=51.2&#23681;&#65292;&#177;14.7[&#26631;&#20934;&#24046;]&#65289;&#12290;&#22312;&#25152;&#26377;&#32452;&#21512;&#30340;AUC&#24179;&#22343;&#20540;&#20013;&#65292;&#38750;&#22686;&#24378;CT&#65288;AUC&#65292;0.6100&#65307;95% CI&#65306;0.5897&#65292;0.6303&#65289;&#20248;&#20110;&#22686;&#24378;CT&#65288;AUC&#65292;0.5680&#65307;95% CI
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.14396v2 Announce Type: replace-cross  Abstract: Objectives: To develop and evaluate a radiomics machine learning model for detecting liver fibrosis on CT of the liver.   Methods: For this retrospective, single-centre study, radiomic features were extracted from Regions of Interest (ROIs) on CT images of patients who underwent simultaneous liver biopsy and CT examinations. Combinations of contrast, normalization, machine learning model, and feature selection method were determined based on their mean test Area Under the Receiver Operating Characteristic curve (AUC) on randomly placed ROIs. The combination and selected features with the highest AUC were used to develop a final liver fibrosis screening model.   Results: The study included 101 male and 68 female patients (mean age = 51.2 years $\pm$ 14.7 [SD]). When averaging the AUC across all combinations, non-contrast enhanced (NC) CT (AUC, 0.6100; 95% CI: 0.5897, 0.6303) outperformed contrast-enhanced CT (AUC, 0.5680; 95% CI
&lt;/p&gt;</description></item><item><title>&#29699;&#38754;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20272;&#35745;&#33041;&#24494;&#32467;&#26500;&#21442;&#25968;&#26041;&#38754;&#20855;&#26377;&#27604;&#24120;&#35268;&#25216;&#26415;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#23569;&#30340;&#26059;&#36716;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2211.09887</link><description>&lt;p&gt;
&#29699;&#38754;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25913;&#21892;&#25193;&#25955;MRI&#25968;&#25454;&#20013;&#23545;&#33041;&#24494;&#32467;&#26500;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Spherical convolutional neural networks can improve brain microstructure estimation from diffusion MRI data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.09887
&lt;/p&gt;
&lt;p&gt;
&#29699;&#38754;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20272;&#35745;&#33041;&#24494;&#32467;&#26500;&#21442;&#25968;&#26041;&#38754;&#20855;&#26377;&#27604;&#24120;&#35268;&#25216;&#26415;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#23569;&#30340;&#26059;&#36716;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#23545;&#22823;&#33041;&#32452;&#32455;&#30340;&#24494;&#32467;&#26500;&#29305;&#24615;&#26377;&#24456;&#39640;&#30340;&#25935;&#24863;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#27979;&#37327;&#20449;&#21495;&#20013;&#20272;&#35745;&#20020;&#24202;&#21644;&#31185;&#23398;&#30456;&#20851;&#30340;&#24494;&#32467;&#26500;&#29305;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#36870;&#38382;&#39064;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#26377;&#21161;&#20110;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#26059;&#36716;&#19981;&#21464;&#29699;&#38754;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#21487;&#20197;&#25913;&#21892;&#24494;&#32467;&#26500;&#21442;&#25968;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#29699;&#38754;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20174;&#39640;&#25928;&#27169;&#25311;&#30340;&#24102;&#22122;&#25968;&#25454;&#20013;&#39044;&#27979;&#22320;&#38754;&#23454;&#20917;&#21442;&#25968;&#25968;&#20540;&#65292;&#24182;&#23558;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#24212;&#29992;&#20110;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#33719;&#21462;&#30340;&#25104;&#20687;&#25968;&#25454;&#65292;&#29983;&#25104;&#24494;&#32467;&#26500;&#21442;&#25968;&#22270;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#34920;&#29616;&#20248;&#20110;&#29699;&#38754;&#24179;&#22343;&#25216;&#26415;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#27604;&#29699;&#38754;&#24179;&#22343;&#25216;&#26415;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#27604;&#22810;&#23618;&#24863;&#30693;&#22120;&#20855;&#26377;&#26356;&#23569;&#30340;&#26059;&#36716;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.09887v3 Announce Type: replace-cross  Abstract: Diffusion magnetic resonance imaging is sensitive to the microstructural properties of brain tissue. However, estimating clinically and scientifically relevant microstructural properties from the measured signals remains a highly challenging inverse problem that machine learning may help solve. This study investigated if recently developed rotationally invariant spherical convolutional neural networks can improve microstructural parameter estimation. We trained a spherical convolutional neural network to predict the ground-truth parameter values from efficiently simulated noisy data and applied the trained network to imaging data acquired in a clinical setting to generate microstructural parameter maps. Our network performed better than the spherical mean technique and multi-layer perceptron, achieving higher prediction accuracy than the spherical mean technique with less rotational variance than the multi-layer perceptron. Alt
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26465;&#20214;&#22810;&#27169;&#24577;&#21028;&#21035;&#65288;CMMD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#39044;&#27979;&#30772;&#20135;&#39118;&#38505;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#30772;&#20135;&#27169;&#22411;&#20013;&#32570;&#23569;MDA&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2211.08405</link><description>&lt;p&gt;
&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#30772;&#20135;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multimodal Generative Models for Bankruptcy Prediction Using Textual Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.08405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26465;&#20214;&#22810;&#27169;&#24577;&#21028;&#21035;&#65288;CMMD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#39044;&#27979;&#30772;&#20135;&#39118;&#38505;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#30772;&#20135;&#27169;&#22411;&#20013;&#32570;&#23569;MDA&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#36130;&#21153;&#25253;&#21578;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#20363;&#22914;10-K&#34920;&#20013;&#30340;&#8220;&#31649;&#29702;&#35752;&#35770;&#19982;&#20998;&#26512;&#8221;&#65288;MDA&#65289;&#37096;&#20998;&#65292;&#24050;&#34987;&#29992;&#20110;&#25552;&#39640;&#30772;&#20135;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#19978;&#24066;&#20844;&#21496;&#33719;&#21462;MDA&#37096;&#20998;&#65292;&#36825;&#38480;&#21046;&#20102;MDA&#25968;&#25454;&#22312;&#20256;&#32479;&#30772;&#20135;&#27169;&#22411;&#20013;&#30340;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23436;&#25972;&#25968;&#25454;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#32570;&#23569;MDA&#25968;&#25454;&#30340;&#20004;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#65306;&#65288;i&#65289;&#24182;&#38750;&#25152;&#26377;&#20844;&#21496;&#37117;&#26377;&#20041;&#21153;&#25552;&#20132;MDA&#65292;&#65288;ii&#65289;&#24403;&#29228;&#21462;&#21644;&#25235;&#21462;MDA&#37096;&#20998;&#26102;&#20250;&#20986;&#29616;&#25216;&#26415;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#26465;&#20214;&#22810;&#27169;&#24577;&#21028;&#21035;&#65288;CMMD&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#23884;&#20837;&#20102;&#20250;&#35745;&#12289;&#24066;&#22330;&#21644;&#25991;&#26412;&#25968;&#25454;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;CMMD&#27169;&#22411;&#38656;&#35201;&#19968;&#32452;&#25152;&#26377;&#25968;&#25454;&#27169;&#24577;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;CMMD&#27169;&#22411;&#21482;&#38656;&#35201;&#35775;&#38382;&#20250;&#35745;&#21644;&#24066;&#22330;&#27169;&#24577;&#26469;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.08405v5 Announce Type: replace-cross  Abstract: Textual data from financial filings, e.g., the Management's Discussion &amp; Analysis (MDA) section in Form 10-K, has been used to improve the prediction accuracy of bankruptcy models. In practice, however, we cannot obtain the MDA section for all public companies, which limits the use of MDA data in traditional bankruptcy models, as they need complete data to make predictions. The two main reasons for the lack of MDA are: (i) not all companies are obliged to submit the MDA and (ii) technical problems arise when crawling and scrapping the MDA section. To solve this limitation, this research introduces the Conditional Multimodal Discriminative (CMMD) model that learns multimodal representations that embed information from accounting, market, and textual data modalities. The CMMD model needs a sample with all data modalities for model training. At test time, the CMMD model only needs access to accounting and market modalities to gene
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#25152;&#26377;&#31867;&#22411;&#27169;&#22411;&#30340;&#38598;&#20307;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#36866;&#29992;&#20110;&#36719;&#23616;&#37096;&#27169;&#22411;&#65307;&#35777;&#20070;&#22522;&#20110;&#26032;&#39062;&#30340;&#23616;&#37096;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2210.16140</link><description>&lt;p&gt;
&#38754;&#21521;&#38598;&#20307;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#26412;&#22320;&#21270;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Localized Randomized Smoothing for Collective Robustness Certification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.16140
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#25152;&#26377;&#31867;&#22411;&#27169;&#22411;&#30340;&#38598;&#20307;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#36866;&#29992;&#20110;&#36719;&#23616;&#37096;&#27169;&#22411;&#65307;&#35777;&#20070;&#22522;&#20110;&#26032;&#39062;&#30340;&#23616;&#37096;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#21106;&#12289;&#33410;&#28857;&#20998;&#31867;&#31561;&#20219;&#21153;&#30340;&#27169;&#22411;&#23558;&#21333;&#20010;&#36755;&#20837;&#26144;&#23556;&#21040;&#22810;&#20010;&#26631;&#31614;&#12290;&#36890;&#36807;&#25200;&#21160;&#36825;&#20010;&#21333;&#19968;&#20849;&#20139;&#30340;&#36755;&#20837;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#23545;&#25163;&#21487;&#20197;&#25805;&#32437;&#22810;&#20010;&#39044;&#27979;&#65288;&#20363;&#22914;&#38169;&#35823;&#20998;&#31867;&#22810;&#20010;&#20687;&#32032;&#65289;&#12290;&#38598;&#20307;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#22312;&#36825;&#31181;&#23041;&#32961;&#27169;&#22411;&#19979;&#65292;&#30528;&#30524;&#20110;&#35777;&#26126;&#30028;&#23450;&#40065;&#26834;&#39044;&#27979;&#25968;&#37327;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#25152;&#26377;&#31867;&#22411;&#27169;&#22411;&#30340;&#38598;&#20307;&#40065;&#26834;&#24615;&#35777;&#20070;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#36739;&#22823;&#30340;&#19968;&#31867;&#36719;&#23616;&#37096;&#27169;&#22411;&#26159;&#26377;&#30410;&#30340;&#65292;&#20854;&#20013;&#27599;&#20010;&#36755;&#20986;&#20381;&#36182;&#20110;&#25972;&#20010;&#36755;&#20837;&#65292;&#20294;&#23545;&#19981;&#21516;&#30340;&#36755;&#20837;&#21306;&#22495;&#36171;&#20104;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#27700;&#24179;&#65288;&#20363;&#22914;&#22522;&#20110;&#23427;&#20204;&#22312;&#22270;&#20687;&#20013;&#30340;&#25509;&#36817;&#31243;&#24230;&#65289;&#12290;&#35813;&#35777;&#20070;&#22522;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#23616;&#37096;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.16140v3 Announce Type: replace  Abstract: Models for image segmentation, node classification and many other tasks map a single input to multiple labels. By perturbing this single shared input (e.g. the image) an adversary can manipulate several predictions (e.g. misclassify several pixels). Collective robustness certification is the task of provably bounding the number of robust predictions under this threat model. The only dedicated method that goes beyond certifying each output independently is limited to strictly local models, where each prediction is associated with a small receptive field. We propose a more general collective robustness certificate for all types of models. We further show that this approach is beneficial for the larger class of softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their proximity in the image). The certificate is based on our novel loc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24179;&#31283;&#26465;&#20214;&#20998;&#24067;&#24314;&#27169;&#19982;&#38750;&#24179;&#31283;&#21160;&#24577;&#24314;&#27169;&#35299;&#32806;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#19978;&#30340;&#38750;&#24179;&#31283;&#26465;&#20214;&#20998;&#24067;&#65292;&#33021;&#26356;&#22909;&#22320;&#36866;&#24212;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2209.08411</link><description>&lt;p&gt;
DynaConF&#65306;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DynaConF: Dynamic Forecasting of Non-Stationary Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24179;&#31283;&#26465;&#20214;&#20998;&#24067;&#24314;&#27169;&#19982;&#38750;&#24179;&#31283;&#21160;&#24577;&#24314;&#27169;&#35299;&#32806;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#19978;&#30340;&#38750;&#24179;&#31283;&#26465;&#20214;&#20998;&#24067;&#65292;&#33021;&#26356;&#22909;&#22320;&#36866;&#24212;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#24314;&#27169;&#26410;&#26469;&#32473;&#23450;&#36807;&#21435;&#30340;&#26465;&#20214;&#20998;&#24067;&#26159;&#20854;&#26680;&#24515;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#31181;&#26465;&#20214;&#20998;&#24067;&#26159;&#38750;&#24179;&#31283;&#30340;&#26102;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#26469;&#35828;&#65292;&#35201;&#19968;&#33268;&#23398;&#20064;&#21644;&#20934;&#30830;&#39044;&#27979;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#22320;&#23558;&#24179;&#31283;&#26465;&#20214;&#20998;&#24067;&#24314;&#27169;&#19982;&#38750;&#24179;&#31283;&#21160;&#24577;&#24314;&#27169;&#35299;&#32806;&#65292;&#26469;&#23545;&#26102;&#38388;&#19978;&#30340;&#38750;&#24179;&#31283;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36125;&#21494;&#26031;&#21160;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#36866;&#24212;&#26465;&#20214;&#20998;&#24067;&#21464;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;&#20998;&#35299;&#30340;&#36755;&#20986;&#31354;&#38388;&#22788;&#29702;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#28145;&#24230;&#26465;&#20214;&#20998;&#24067;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26356;&#22909;&#22320;&#36866;&#24212;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.08411v3 Announce Type: replace  Abstract: Deep learning has shown impressive results in a variety of time series forecasting tasks, where modeling the conditional distribution of the future given the past is the essence. However, when this conditional distribution is non-stationary, it poses challenges for these models to learn consistently and to predict accurately. In this work, we propose a new method to model non-stationary conditional distributions over time by clearly decoupling stationary conditional distribution modeling from non-stationary dynamics modeling. Our method is based on a Bayesian dynamic model that can adapt to conditional distribution changes and a deep conditional distribution model that handles multivariate time series using a factorized output space. Our experimental results on synthetic and real-world datasets show that our model can adapt to non-stationary time series better than state-of-the-art deep learning solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24754;&#35266;&#31639;&#27861; VI-LCB-Game&#65292;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#25214;&#21040;&#20102;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#21152;&#24378;&#20102;&#20808;&#21069;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2206.04044</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Based Reinforcement Learning for Offline Zero-Sum Markov Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.04044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24754;&#35266;&#31639;&#27861; VI-LCB-Game&#65292;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#25214;&#21040;&#20102;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#21152;&#24378;&#20102;&#20808;&#21069;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24754;&#35266;&#31639;&#27861;&#65292;&#21363;&#20855;&#26377;Bernstein&#39118;&#26684;&#30340;&#19979;&#38480;&#32622;&#20449;&#30028;&#30340;VI-LCB-Game&#31639;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#20197;&#26679;&#26412;&#22797;&#26434;&#24230;&#19981;&#22823;&#20110;$\frac{C_{\mathsf{clipped}}^{\star}S(A+B)}{(1-\gamma)^{3}\varepsilon^{2}}$&#65288;&#24102;&#26377;&#19968;&#20123;&#23545;&#25968;&#22240;&#23376;&#65289;&#25214;&#21040;&#19968;&#20010;$\varepsilon$-&#36817;&#20284;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#22312;&#36825;&#37324;&#65292;$C_{\mathsf{clipped}}^{\star}$ &#26159;&#21453;&#26144;&#21487;&#29992;&#25968;&#25454;&#65288;&#20851;&#20110;&#30446;&#26631;&#25968;&#25454;&#65289;&#30340;&#35206;&#30422;&#29575;&#21644;&#20998;&#24067;&#36716;&#21464;&#30340;&#26576;&#31181;&#21333;&#20391;&#21098;&#20999;&#30340;&#38598;&#20013;&#24230;&#31995;&#25968;&#65292;&#30446;&#26631;&#31934;&#24230;$\varepsilon$ &#21487;&#20197;&#26159;$\big(0,\frac{1}{1-\gamma}\big]$&#33539;&#22260;&#20869;&#30340;&#20219;&#20309;&#20540;&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#21152;&#24378;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.04044v2 Announce Type: replace  Abstract: This paper makes progress towards learning Nash equilibria in two-player zero-sum Markov games from offline data. Specifically, consider a $\gamma$-discounted infinite-horizon Markov game with $S$ states, where the max-player has $A$ actions and the min-player has $B$ actions. We propose a pessimistic model-based algorithm with Bernstein-style lower confidence bounds -- called VI-LCB-Game -- that provably finds an $\varepsilon$-approximate Nash equilibrium with a sample complexity no larger than $\frac{C_{\mathsf{clipped}}^{\star}S(A+B)}{(1-\gamma)^{3}\varepsilon^{2}}$ (up to some log factor). Here, $C_{\mathsf{clipped}}^{\star}$ is some unilateral clipped concentrability coefficient that reflects the coverage and distribution shift of the available data (vis-\`a-vis the target data), and the target accuracy $\varepsilon$ can be any value within $\big(0,\frac{1}{1-\gamma}\big]$. Our sample complexity bound strengthens prior art by a 
&lt;/p&gt;</description></item><item><title>&#34892;&#20026;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26410;&#26469;&#34892;&#21160;&#21644;&#29366;&#24577;&#36335;&#24452;&#30340;&#21344;&#29992;&#65292;&#26681;&#25454;&#26368;&#22823;&#21344;&#29992;&#21407;&#21017;&#65292;&#22870;&#21169;&#26159;&#21344;&#29992;&#36335;&#24452;&#31354;&#38388;&#30340;&#25163;&#27573;&#65292;&#32780;&#19981;&#26159;&#30446;&#26631;&#26412;&#36523;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#26368;&#20248;&#31574;&#30053;&#21644;&#29366;&#24577;&#20540;&#20989;&#25968;&#30456;&#20851;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#35777;&#26126;&#20102;&#20540;&#36845;&#20195;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;</title><link>https://arxiv.org/abs/2205.10316</link><description>&lt;p&gt;
&#20174;&#20869;&#22312;&#21160;&#26426;&#21040;&#21344;&#25454;&#34892;&#21160;-&#29366;&#24577;&#36335;&#24452;&#31354;&#38388;&#30340;&#22797;&#26434;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Complex behavior from intrinsic motivation to occupy action-state path space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.10316
&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26410;&#26469;&#34892;&#21160;&#21644;&#29366;&#24577;&#36335;&#24452;&#30340;&#21344;&#29992;&#65292;&#26681;&#25454;&#26368;&#22823;&#21344;&#29992;&#21407;&#21017;&#65292;&#22870;&#21169;&#26159;&#21344;&#29992;&#36335;&#24452;&#31354;&#38388;&#30340;&#25163;&#27573;&#65292;&#32780;&#19981;&#26159;&#30446;&#26631;&#26412;&#36523;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#26368;&#20248;&#31574;&#30053;&#21644;&#29366;&#24577;&#20540;&#20989;&#25968;&#30456;&#20851;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#35777;&#26126;&#20102;&#20540;&#36845;&#20195;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#34892;&#20026;&#29702;&#35770;&#35748;&#20026;&#65292;&#20195;&#29702;&#20154;&#20542;&#21521;&#20110;&#26368;&#22823;&#21270;&#26576;&#31181;&#24418;&#24335;&#30340;&#22870;&#21169;&#25110;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#21160;&#29289;&#32463;&#24120;&#20986;&#20110;&#22909;&#22855;&#24515;&#31227;&#21160;&#65292;&#24182;&#19988;&#20284;&#20046;&#22312;&#27809;&#26377;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#21463;&#21040;&#28608;&#21169;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25918;&#24323;&#20102;&#22870;&#21169;&#26368;&#22823;&#21270;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#34892;&#20026;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26410;&#26469;&#34892;&#21160;&#21644;&#29366;&#24577;&#36335;&#24452;&#30340;&#21344;&#29992;&#12290;&#26681;&#25454;&#26368;&#22823;&#21344;&#29992;&#21407;&#21017;&#65292;&#22870;&#21169;&#26159;&#21344;&#29992;&#36335;&#24452;&#31354;&#38388;&#30340;&#25163;&#27573;&#65292;&#32780;&#19981;&#26159;&#30446;&#26631;&#26412;&#36523;&#65307;&#30446;&#26631;&#23548;&#21521;&#24615;&#31616;&#21333;&#22320;&#20316;&#20026;&#25628;&#32034;&#36164;&#28304;&#30340;&#29702;&#24615;&#26041;&#24335;&#32780;&#20986;&#29616;&#65292;&#20197;&#20351;&#36816;&#21160;&#20174;&#24191;&#20041;&#19978;&#29702;&#35299;&#27704;&#19981;&#32467;&#26463;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34892;&#21160;-&#29366;&#24577;&#36335;&#24452;&#29109;&#26159;&#21807;&#19968;&#19982;&#21487;&#21152;&#24615;&#21644;&#20854;&#20182;&#30452;&#35266;&#30340;&#39044;&#26399;&#26410;&#26469;&#34892;&#21160;-&#29366;&#24577;&#36335;&#24452;&#21344;&#29992;&#24615;&#36136;&#19968;&#33268;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#20248;&#31574;&#30053;&#21644;&#29366;&#24577;&#20540;&#20989;&#25968;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20540;&#36845;&#20195;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#20351;&#29992;&#31163;&#25955;&#21644;&#36830;&#32493;&#29366;&#24577;&#20219;&#21153;&#65292;&#21253;&#25324;&#19968;&#20010;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.10316v2 Announce Type: replace  Abstract: Most theories of behavior posit that agents tend to maximize some form of reward or utility. However, animals very often move with curiosity and seem to be motivated in a reward-free manner. Here we abandon the idea of reward maximization, and propose that the goal of behavior is maximizing occupancy of future paths of actions and states. According to this maximum occupancy principle, rewards are the means to occupy path space, not the goal per se; goal-directedness simply emerges as rational ways of searching for resources so that movement, understood amply, never ends. We find that action-state path entropy is the only measure consistent with additivity and other intuitive properties of expected future action-state path occupancy. We provide analytical expressions that relate the optimal policy and state-value function, and prove convergence of our value iteration algorithm. Using discrete and continuous state tasks, including a hi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#31283;&#20581;&#32479;&#35745;&#22871;&#21033;&#31574;&#30053;&#65292;&#26080;&#38656;&#20381;&#36182;&#36164;&#20135;&#21327;&#25972;&#23545;&#30340;&#35782;&#21035;&#65292;&#21487;&#22312;&#39640;&#32500;&#37329;&#34701;&#24066;&#22330;&#20013;&#23454;&#29616;&#30408;&#21033;&#20132;&#26131;&#12290;</title><link>https://arxiv.org/abs/2203.03179</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#25968;&#25454;&#39537;&#21160;&#30340;&#31283;&#20581;&#32479;&#35745;&#22871;&#21033;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Detecting data-driven robust statistical arbitrage strategies with deep neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.03179
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#31283;&#20581;&#32479;&#35745;&#22871;&#21033;&#31574;&#30053;&#65292;&#26080;&#38656;&#20381;&#36182;&#36164;&#20135;&#21327;&#25972;&#23545;&#30340;&#35782;&#21035;&#65292;&#21487;&#22312;&#39640;&#32500;&#37329;&#34701;&#24066;&#22330;&#20013;&#23454;&#29616;&#30408;&#21033;&#20132;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#35782;&#21035;&#20986;&#31283;&#20581;&#30340;&#32479;&#35745;&#22871;&#21033;&#31574;&#30053;&#12290;&#31283;&#20581;&#30340;&#32479;&#35745;&#22871;&#21033;&#31574;&#30053;&#25351;&#30340;&#26159;&#22312;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19979;&#23454;&#29616;&#30408;&#21033;&#20132;&#26131;&#30340;&#20132;&#26131;&#31574;&#30053;&#12290;&#36825;&#19968;&#21019;&#26032;&#26041;&#27861;&#20801;&#35768;&#21516;&#26102;&#32771;&#34385;&#22823;&#37327;&#22522;&#30784;&#35777;&#21048;&#65292;&#24182;&#19981;&#20381;&#36182;&#20110;&#36164;&#20135;&#21327;&#25972;&#23545;&#30340;&#35782;&#21035;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#39640;&#32500;&#37329;&#34701;&#24066;&#22330;&#25110;&#22312;&#20256;&#32479;&#23545;&#20914;&#20132;&#26131;&#26041;&#27861;&#22833;&#36133;&#30340;&#24066;&#22330;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#35266;&#23519;&#24066;&#22330;&#25968;&#25454;&#20013;&#24471;&#21040;&#30340;&#21487;&#23481;&#35768;&#27010;&#29575;&#27979;&#24230;&#30340;&#27169;&#31946;&#38598;&#26500;&#24314;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#21487;&#34987;&#35748;&#20026;&#26159;&#26080;&#27169;&#22411;&#19988;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20855;&#26377;&#39640;&#30408;&#21033;&#20132;&#26131;&#34920;&#29616;&#30340;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#29978;&#33267;&#22312;50&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.03179v4 Announce Type: replace-cross  Abstract: We present an approach, based on deep neural networks, that allows identifying robust statistical arbitrage strategies in financial markets. Robust statistical arbitrage strategies refer to trading strategies that enable profitable trading under model ambiguity. The presented novel methodology allows to consider a large amount of underlying securities simultaneously and does not depend on the identification of cointegrated pairs of assets, hence it is applicable on high-dimensional financial markets or in markets where classical pairs trading approaches fail. Moreover, we provide a method to build an ambiguity set of admissible probability measures that can be derived from observed market data. Thus, the approach can be considered as being model-free and entirely data-driven. We showcase the applicability of our method by providing empirical investigations with highly profitable trading performances even in 50 dimensions, durin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#26080;&#22870;&#21169;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#21040;&#22312;&#32447;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23398;&#20064;&#21160;&#24577;VCG&#26426;&#21046;&#19988;&#20855;&#26377;&#19978;&#30028;&#20026;$\tilde{\mathcal{O}}(T^{2/3})$&#30340;&#36951;&#25022;&#20445;&#35777;&#30340;&#26032;&#39062;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2202.12797</link><description>&lt;p&gt;
&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23398;&#20064;&#21160;&#24577;&#26426;&#21046;&#65306;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamic Mechanisms in Unknown Environments: A Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.12797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26080;&#22870;&#21169;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#21040;&#22312;&#32447;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23398;&#20064;&#21160;&#24577;VCG&#26426;&#21046;&#19988;&#20855;&#26377;&#19978;&#30028;&#20026;$\tilde{\mathcal{O}}(T^{2/3})$&#30340;&#36951;&#25022;&#20445;&#35777;&#30340;&#26032;&#39062;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26426;&#21046;&#35774;&#35745;&#30740;&#31350;&#20102;&#26426;&#21046;&#35774;&#35745;&#32773;&#22312;&#26102;&#21464;&#29615;&#22659;&#20013;&#24212;&#35813;&#22914;&#20309;&#22312;&#20195;&#29702;&#20043;&#38388;&#20998;&#37197;&#36164;&#28304;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#38382;&#39064;&#65292;&#21363;&#20195;&#29702;&#26681;&#25454;&#26410;&#30693;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#19982;&#26426;&#21046;&#35774;&#35745;&#32773;&#20114;&#21160;&#65292;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20195;&#29702;&#30340;&#22870;&#21169;&#21644;&#26426;&#21046;&#35774;&#35745;&#32773;&#30340;&#29366;&#24577;&#26681;&#25454;&#19968;&#20010;&#24102;&#26377;&#26410;&#30693;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#26680;&#30340;&#24773;&#33410;MDP&#28436;&#21270;&#12290;&#25105;&#20204;&#20851;&#27880;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#32447;&#24615;&#20989;&#25968;&#36817;&#20284;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#22810;&#36718;&#20114;&#21160;&#20013;&#24674;&#22797;&#21160;&#24577;Vickrey-Clarke-Grove(VCG)&#26426;&#21046;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#23558;&#26080;&#22870;&#21169;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;(RL)&#32467;&#21512;&#36827;&#26469;&#65292;&#20197;&#24110;&#21161;&#22312;&#20016;&#23500;&#30340;&#31574;&#30053;&#31354;&#38388;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#20174;&#32780;&#20272;&#35745;&#21160;&#24577;VCG&#26426;&#21046;&#20013;&#30340;&#20215;&#26684;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#36951;&#25022;&#19978;&#30028;&#20026;$\tilde{\mathcal{O}}(T^{2/3})$&#65292;&#24182;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.12797v2 Announce Type: replace  Abstract: Dynamic mechanism design studies how mechanism designers should allocate resources among agents in a time-varying environment. We consider the problem where the agents interact with the mechanism designer according to an unknown Markov Decision Process (MDP), where agent rewards and the mechanism designer's state evolve according to an episodic MDP with unknown reward functions and transition kernels. We focus on the online setting with linear function approximation and propose novel learning algorithms to recover the dynamic Vickrey-Clarke-Grove (VCG) mechanism over multiple rounds of interaction. A key contribution of our approach is incorporating reward-free online Reinforcement Learning (RL) to aid exploration over a rich policy space to estimate prices in the dynamic VCG mechanism. We show that the regret of our proposed method is upper bounded by $\tilde{\mathcal{O}}(T^{2/3})$ and further devise a lower bound to show that our a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#31995;&#32479;&#22238;&#39038;&#65292;&#24182;&#25552;&#20986;&#20102;&#31995;&#32479;&#24615;&#20998;&#31867;&#27861;&#20197;&#25351;&#23548;&#29616;&#26377;&#24322;&#36136;&#24615;GNN&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2202.07082</link><description>&lt;p&gt;
&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Graphs with Heterophily: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.07082
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#31995;&#32479;&#22238;&#39038;&#65292;&#24182;&#25552;&#20986;&#20102;&#31995;&#32479;&#24615;&#20998;&#31867;&#27861;&#20197;&#25351;&#23548;&#29616;&#26377;&#24322;&#36136;&#24615;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#35768;&#22810;&#22270;&#20998;&#26512;&#20219;&#21153;&#21644;&#24212;&#29992;&#21463;&#30410;&#12290;&#22823;&#22810;&#25968;GNNs&#36890;&#24120;&#20381;&#36182;&#20110;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#21363;&#23646;&#20110;&#21516;&#19968;&#31867;&#21035;&#30340;&#33410;&#28857;&#26356;&#21487;&#33021;&#30456;&#36830;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#30340;&#22270;&#23646;&#24615;&#65292;&#21363;&#19981;&#21516;&#26631;&#31614;&#30340;&#33410;&#28857;&#24448;&#24448;&#30456;&#36830;&#65292;&#36825;&#26174;&#33879;&#38480;&#21046;&#20102;&#23450;&#21046;&#30340;&#21516;&#36136;&#24615;GNNs&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#24322;&#36136;&#24615;&#22270;&#30340;GNNs&#27491;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#20197;&#22686;&#24378;&#23545;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#23398;&#20064;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;GNNs&#22238;&#39038;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#20998;&#31867;&#27861;&#65292;&#26412;&#36136;&#19978;&#25351;&#23548;&#30528;&#29616;&#26377;&#30340;&#24322;&#36136;&#24615;GNN&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#27010;&#25324;&#24615;&#25688;&#35201;&#21644;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.07082v2 Announce Type: replace  Abstract: Recent years have witnessed fast developments of graph neural networks (GNNs) that have benefited myriads of graph analytic tasks and applications. In general, most GNNs depend on the homophily assumption that nodes belonging to the same class are more likely to be connected. However, as a ubiquitous graph property in numerous real-world scenarios, heterophily, i.e., nodes with different labels tend to be linked, significantly limits the performance of tailor-made homophilic GNNs. Hence, GNNs for heterophilic graphs are gaining increasing research attention to enhance graph learning with heterophily. In this paper, we provide a comprehensive review of GNNs for heterophilic graphs. Specifically, we propose a systematic taxonomy that essentially governs existing heterophilic GNN models, along with a general summary and detailed analysis. %Furthermore, we summarize the mainstream heterophilic graph benchmarks to facilitate robust and fa
&lt;/p&gt;</description></item><item><title>Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20840;&#38754;&#22810;&#26679;&#30340;&#32593;&#39029;&#38598;&#21512;&#65292;&#20026;&#35299;&#20915;&#32593;&#32476;&#33258;&#21160;&#21270;&#31639;&#27861;&#35774;&#35745;&#20013;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2111.02168</link><description>&lt;p&gt;
Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#65306;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#20803;&#32032;&#25552;&#21517;
&lt;/p&gt;
&lt;p&gt;
The Klarna Product Page Dataset: Web Element Nomination with Graph Neural Networks and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.02168
&lt;/p&gt;
&lt;p&gt;
Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20840;&#38754;&#22810;&#26679;&#30340;&#32593;&#39029;&#38598;&#21512;&#65292;&#20026;&#35299;&#20915;&#32593;&#32476;&#33258;&#21160;&#21270;&#31639;&#27861;&#35774;&#35745;&#20013;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Web&#33258;&#21160;&#21270;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#29992;&#25143;&#19982;&#25968;&#23383;&#19990;&#30028;&#30340;&#20114;&#21160;&#26041;&#24335;&#65292;&#36890;&#36807;&#22797;&#26434;&#30340;&#35745;&#31639;&#26041;&#27861;&#25552;&#20379;&#26080;&#19982;&#20262;&#27604;&#30340;&#24110;&#21161;&#65292;&#31616;&#21270;&#20219;&#21153;&#12290;&#22312;&#36825;&#19968;&#28436;&#36827;&#36807;&#31243;&#20013;&#65292;&#32593;&#32476;&#20803;&#32032;&#25552;&#21517;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#28041;&#21450;&#35782;&#21035;&#32593;&#39029;&#19978;&#30340;&#29420;&#29305;&#20803;&#32032;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#32593;&#32476;&#33258;&#21160;&#21270;&#31639;&#27861;&#35774;&#35745;&#30340;&#21457;&#23637;&#21463;&#21040;&#20840;&#38754;&#21644;&#30495;&#23454;&#21453;&#26144;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#22797;&#26434;&#24615;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#22810;&#26679;&#30340;&#32593;&#39029;&#38598;&#21512;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;8,175&#20010;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#30340;51,701&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#20135;&#21697;&#39029;&#38754;&#65292;&#35206;&#30422;&#20102;&#20843;&#20010;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#38468;&#24102;&#20102;&#19968;&#32452;&#28210;&#26579;&#39029;&#38754;&#25130;&#22270;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#24320;&#22987;&#30740;&#31350;Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.02168v4 Announce Type: replace-cross  Abstract: Web automation holds the potential to revolutionize how users interact with the digital world, offering unparalleled assistance and simplifying tasks via sophisticated computational methods. Central to this evolution is the web element nomination task, which entails identifying unique elements on webpages. Unfortunately, the development of algorithmic designs for web automation is hampered by the scarcity of comprehensive and realistic datasets that reflect the complexity faced by real-world applications on the Web. To address this, we introduce the Klarna Product Page Dataset, a comprehensive and diverse collection of webpages that surpasses existing datasets in richness and variety. The dataset features 51,701 manually labeled product pages from 8,175 e-commerce websites across eight geographic regions, accompanied by a dataset of rendered page screenshots. To initiate research on the Klarna Product Page Dataset, we empirical
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#22266;&#23450;&#32622;&#20449;&#24230;&#19979;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22312;&#35782;&#21035;&#26368;&#20339;&#33218;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#8220;&#36319;&#36394;&#20572;&#27490;&#8221;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#39640;&#25928;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2106.14077</link><description>&lt;p&gt;
&#22312;&#26368;&#20339;&#33218;&#35782;&#21035;&#20013;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Contextual Information in Best Arm Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.14077
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#22266;&#23450;&#32622;&#20449;&#24230;&#19979;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22312;&#35782;&#21035;&#26368;&#20339;&#33218;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#8220;&#36319;&#36394;&#20572;&#27490;&#8221;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#39640;&#25928;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#24403;&#26377;&#19978;&#19979;&#25991;&#65288;&#21327;&#21464;&#37327;&#65289;&#20449;&#24687;&#21487;&#29992;&#26102;&#30340;&#22266;&#23450;&#32622;&#20449;&#24230;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#34429;&#28982;&#25105;&#20204;&#21487;&#20197;&#22312;&#27599;&#19968;&#36718;&#20013;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20294;&#25105;&#20204;&#23545;&#19978;&#19979;&#25991;&#20998;&#24067;&#30340;&#36793;&#38469;&#21270;&#22343;&#20540;&#37325;&#35270;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#38169;&#35823;&#29575;&#20540;&#30340;&#24773;&#20917;&#19979;&#20197;&#26368;&#23567;&#25968;&#37327;&#30340;&#25277;&#26679;&#35782;&#21035;&#26368;&#20339;&#33218;&#12290;&#25105;&#20204;&#20026;&#35813;&#38382;&#39064;&#23637;&#31034;&#20102;&#29305;&#23450;&#23454;&#20363;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#19979;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#36319;&#36394;&#20572;&#27490;&#8221;&#31574;&#30053;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#29256;&#26412;&#65292;&#20854;&#20013;&#33218;&#25277;&#21462;&#30340;&#27604;&#20363;&#36319;&#36394;&#26368;&#20248;&#20998;&#37197;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#39044;&#26399;&#30340;&#33218;&#25277;&#21462;&#27425;&#25968;&#28176;&#36817;&#22320;&#19982;&#19979;&#30028;&#21305;&#37197;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;Garivier &amp; Kaufmann&#65288;2016&#65289;&#30340;&#32467;&#26524;&#65292;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#25913;&#21892;&#23545;&#26368;&#20339;&#36793;&#38469;&#21270;&#22343;&#20540;&#22870;&#21169;&#30340;&#35782;&#21035;&#25928;&#29575;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#23454;&#20102; cont
&lt;/p&gt;
&lt;p&gt;
arXiv:2106.14077v3 Announce Type: replace  Abstract: We study the best-arm identification problem with fixed confidence when contextual (covariate) information is available in stochastic bandits. Although we can use contextual information in each round, we are interested in the marginalized mean reward over the contextual distribution. Our goal is to identify the best arm with a minimal number of samplings under a given value of the error rate. We show the instance-specific sample complexity lower bounds for the problem. Then, we propose a context-aware version of the "Track-and-Stop" strategy, wherein the proportion of the arm draws tracks the set of optimal allocations and prove that the expected number of arm draws matches the lower bound asymptotically. We demonstrate that contextual information can be used to improve the efficiency of the identification of the best marginalized mean reward compared with the results of Garivier &amp; Kaufmann (2016). We experimentally confirm that cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#23450;&#22238;&#24402;&#65288;AR&#65289;&#30340;&#21487;&#25193;&#23637;&#20984;&#35268;&#21010;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#22823;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2103.07020</link><description>&lt;p&gt;
&#21033;&#29992;&#20984;&#35268;&#21010;&#30340;&#26368;&#22823;&#32447;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Max-Linear Regression by Convex Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2103.07020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#23450;&#22238;&#24402;&#65288;AR&#65289;&#30340;&#21487;&#25193;&#23637;&#20984;&#35268;&#21010;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#22823;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22810;&#20803;&#26368;&#22823;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#38656;&#35201;&#20174;$n$&#20010;&#29420;&#31435;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;$\boldsymbol{\beta}_{1},\dotsc,\boldsymbol{\beta}_{k}\in\mathbb{R}^{p}$&#65292;&#36825;&#20123;&#26679;&#26412;&#26159;&#65288;&#22122;&#22768;&#30340;&#65289;&#35266;&#27979;$y = \max_{1\leq j \leq k} \boldsymbol{\beta}_{j}^{\mathsf{T}} \boldsymbol{x} + \mathrm{noise}$&#12290;&#26368;&#22823;&#32447;&#24615;&#27169;&#22411;&#24191;&#27867;&#22320;&#25512;&#24191;&#20102;&#20256;&#32479;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#24403;&#32447;&#24615;&#27169;&#22411;&#30340;&#25968;&#37327;$k$&#36275;&#22815;&#22823;&#26102;&#65292;&#23427;&#21487;&#20197;&#20197;&#20219;&#24847;&#31934;&#24230;&#36924;&#36817;&#20219;&#20309;&#20984;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#26368;&#22823;&#32447;&#24615;&#27169;&#22411;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#20351;&#24471;&#22238;&#24402;&#21442;&#25968;&#30340;&#20272;&#35745;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25991;&#29486;&#20013;&#27809;&#26377;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20984;&#35268;&#21010;&#31243;&#24207;&#65292;&#21363;&#38170;&#23450;&#22238;&#24402;&#65288;AR&#65289;&#65292;&#20316;&#20026;&#26368;&#22823;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#30340;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2103.07020v2 Announce Type: replace-cross  Abstract: We consider the multivariate max-linear regression problem where the model parameters $\boldsymbol{\beta}_{1},\dotsc,\boldsymbol{\beta}_{k}\in\mathbb{R}^{p}$ need to be estimated from $n$ independent samples of the (noisy) observations $y = \max_{1\leq j \leq k} \boldsymbol{\beta}_{j}^{\mathsf{T}} \boldsymbol{x} + \mathrm{noise}$. The max-linear model vastly generalizes the conventional linear model, and it can approximate any convex function to an arbitrary accuracy when the number of linear models $k$ is large enough. However, the inherent nonlinearity of the max-linear model renders the estimation of the regression parameters computationally challenging. Particularly, no estimator based on convex programming is known in the literature. We formulate and analyze a scalable convex program given by anchored regression (AR) as the estimator for the max-linear regression problem. Under the standard Gaussian observation setting, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28151;&#21512;&#31574;&#30053;&#26799;&#24230;&#65288;MPG&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#31574;&#30053;&#26799;&#24230;&#65292;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2102.11513</link><description>&lt;p&gt;
&#28151;&#21512;&#31574;&#30053;&#26799;&#24230;&#65306;&#25968;&#25454;&#19982;&#27169;&#22411;&#20849;&#21516;&#39537;&#21160;&#30340;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixed Policy Gradient: off-policy reinforcement learning driven jointly by data and model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.11513
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28151;&#21512;&#31574;&#30053;&#26799;&#24230;&#65288;MPG&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#31574;&#30053;&#26799;&#24230;&#65292;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#30446;&#21069;&#65292;&#20027;&#27969;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#36890;&#24120;&#22312;&#28176;&#36817;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#19982;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#30456;&#27604;&#25910;&#25947;&#36895;&#24230;&#35201;&#24930;&#24471;&#22810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#28151;&#21512;&#31574;&#30053;&#26799;&#24230;&#65288;MPG&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#32463;&#39564;&#25968;&#25454;&#21644;&#31574;&#30053;&#26799;&#24230;&#65288;PG&#65289;&#20013;&#30340;&#36716;&#31227;&#27169;&#22411;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#21152;&#36895;&#25910;&#25947;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#24418;&#24335;&#19978;&#65292;MPG&#34987;&#26500;&#24314;&#20026;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;PG&#30340;&#21152;&#26435;&#24179;&#22343;&#65292;&#20854;&#20013;&#21069;&#32773;&#26159;&#23398;&#20064;&#30340;Q&#20540;&#20989;&#25968;&#30340;&#23548;&#25968;&#65292;&#32780;&#21518;&#32773;&#26159;&#27169;&#22411;&#39044;&#27979;&#22238;&#25253;&#30340;&#23548;&#25968;&#12290;&#20026;&#25351;&#23548;&#26435;&#37325;&#35774;&#35745;&#65292;&#25105;&#20204;&#20998;&#26512;&#24182;&#27604;&#36739;&#27599;&#20010;PG&#35823;&#24046;&#30340;&#19978;&#38480;&#12290;&#20381;&#38752;&#36825;&#19968;&#28857;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26469;&#21551;&#21457;&#24615;&#22320;&#35843;&#25972;&#26435;&#37325;&#12290;&#29305;&#21035;&#22320;&#65292;&#35201;&#33719;&#24471;&#26356;&#22909;&#30340;PG&#65292;&#25968;&#25454;&#39537;&#21160;PG&#30340;&#26435;&#37325;&#34987;&#35774;&#35745;&#25104;&#27839;&#30528;&#23398;&#20064;&#36807;&#31243;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.11513v2 Announce Type: replace  Abstract: Reinforcement learning (RL) shows great potential in sequential decision-making. At present, mainstream RL algorithms are data-driven, which usually yield better asymptotic performance but much slower convergence compared with model-driven methods. This paper proposes mixed policy gradient (MPG) algorithm, which fuses the empirical data and the transition model in policy gradient (PG) to accelerate convergence without performance degradation. Formally, MPG is constructed as a weighted average of the data-driven and model-driven PGs, where the former is the derivative of the learned Q-value function, and the latter is that of the model-predictive return. To guide the weight design, we analyze and compare the upper bound of each PG error. Relying on that, a rule-based method is employed to heuristically adjust the weights. In particular, to get a better PG, the weight of the data-driven PG is designed to grow along the learning process
&lt;/p&gt;</description></item><item><title>&#28165;&#27927;&#20102;MSR-VTT&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#35270;&#39057;&#23383;&#24149;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2102.06448</link><description>&lt;p&gt;
&#20855;&#26377;&#24178;&#20928;&#26631;&#27880;&#30340;MSR-Video to Text&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The MSR-Video to Text Dataset with Clean Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.06448
&lt;/p&gt;
&lt;p&gt;
&#28165;&#27927;&#20102;MSR-VTT&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#35270;&#39057;&#23383;&#24149;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#23383;&#24149;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#20869;&#23481;&#30340;&#31616;&#30701;&#25551;&#36848;&#65292;&#36890;&#24120;&#26159;&#19968;&#21477;&#35805;&#30340;&#24418;&#24335;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#21517;&#20026;MSR Video to Text&#65288;MSR-VTT&#65289;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#32463;&#24120;&#34987;&#29992;&#20316;&#27979;&#35797;&#36825;&#20123;&#26041;&#27861;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#30340;&#20154;&#24037;&#26631;&#27880;&#65292;&#21363;&#35270;&#39057;&#20869;&#23481;&#25551;&#36848;&#65292;&#23384;&#22312;&#30456;&#24403;&#22810;&#30340;&#22122;&#38899;&#65292;&#20363;&#22914;&#35768;&#22810;&#37325;&#22797;&#23383;&#24149;&#21644;&#35768;&#22810;&#23383;&#24149;&#21253;&#21547;&#35821;&#27861;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#20250;&#32473;&#35270;&#39057;&#23383;&#24149;&#27169;&#22411;&#23398;&#20064;&#24213;&#23618;&#27169;&#24335;&#24102;&#26469;&#22256;&#38590;&#12290;&#25105;&#20204;&#36890;&#36807;&#28040;&#38500;&#36825;&#20123;&#38382;&#39064;&#26469;&#28165;&#29702;MSR-VTT&#30340;&#26631;&#27880;&#65292;&#28982;&#21518;&#22312;&#28165;&#29702;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#20960;&#31181;&#20856;&#22411;&#30340;&#35270;&#39057;&#23383;&#24149;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25968;&#25454;&#28165;&#27927;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#27969;&#34892;&#30340;&#23450;&#37327;&#25351;&#26631;&#34913;&#37327;&#12290;&#25105;&#20204;&#25307;&#21215;&#20102;&#21463;&#35797;&#32773;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.06448v4 Announce Type: replace-cross  Abstract: Video captioning automatically generates short descriptions of the video content, usually in form of a single sentence. Many methods have been proposed for solving this task. A large dataset called MSR Video to Text (MSR-VTT) is often used as the benchmark dataset for testing the performance of the methods. However, we found that the human annotations, i.e., the descriptions of video contents in the dataset are quite noisy, e.g., there are many duplicate captions and many captions contain grammatical problems. These problems may pose difficulties to video captioning models for learning underlying patterns. We cleaned the MSR-VTT annotations by removing these problems, then tested several typical video captioning models on the cleaned dataset. Experimental results showed that data cleaning boosted the performances of the models measured by popular quantitative metrics. We recruited subjects to evaluate the results of a model tra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#25805;&#20316;&#23567;&#30340;&#23545;&#20598;&#21464;&#37327;&#27963;&#36291;&#38598;&#19978;&#30340;&#27425;&#26799;&#24230;&#26041;&#27861;&#21644;&#21033;&#29992;Frank-Wolfe&#31867;&#22411;&#20248;&#21270;&#22120;&#30340;&#31232;&#30095;&#24615;&#36136;&#65292;&#26469;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#36793;&#30028;&#39564;&#35777;&#31995;&#32479;&#20013;&#24120;&#35265;&#30340;&#26494;&#24347;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2101.05844</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#23545;&#20598;&#31639;&#27861;&#25193;&#23637;&#20984;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Scaling the Convex Barrier with Sparse Dual Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2101.05844
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#25805;&#20316;&#23567;&#30340;&#23545;&#20598;&#21464;&#37327;&#27963;&#36291;&#38598;&#19978;&#30340;&#27425;&#26799;&#24230;&#26041;&#27861;&#21644;&#21033;&#29992;Frank-Wolfe&#31867;&#22411;&#20248;&#21270;&#22120;&#30340;&#31232;&#30095;&#24615;&#36136;&#65292;&#26469;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#36793;&#30028;&#39564;&#35777;&#31995;&#32479;&#20013;&#24120;&#35265;&#30340;&#26494;&#24347;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#20945;&#32780;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#36793;&#30028;&#23545;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31995;&#32479;&#30340;&#25193;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#39640;&#25928;&#30340;&#36793;&#30028;&#31639;&#27861;&#65292;&#20294;&#36890;&#24120;&#22826;&#26494;&#20197;&#39564;&#35777;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#23646;&#24615;&#12290;&#36825;&#26159;&#22240;&#20026;&#25152;&#20351;&#29992;&#30340;&#26494;&#24347;&#24615;&#36136;&#36739;&#24369;&#65292;&#36890;&#24120;&#26159;&#19982;&#31070;&#32463;&#20803;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#30340;&#32447;&#24615;&#35268;&#21010;&#12290;&#34429;&#28982;&#23384;&#22312;&#19968;&#31181;&#26356;&#32039;&#33268;&#30340;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#26494;&#24347;&#65292;&#20294;&#20195;&#20215;&#26159;&#25351;&#25968;&#32423;&#30340;&#32422;&#26463;&#65292;&#24403;&#21069;&#32570;&#20047;&#39640;&#25928;&#30340;&#23450;&#21046;&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#26032;&#39062;&#30340;&#23545;&#20598;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#65306;&#19968;&#31181;&#22312;&#19968;&#20010;&#23567;&#30340;&#23545;&#20598;&#21464;&#37327;&#27963;&#36291;&#38598;&#19978;&#36816;&#34892;&#27425;&#26799;&#24230;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;Frank-Wolfe&#31867;&#22411;&#20248;&#21270;&#22120;&#30340;&#31232;&#30095;&#24615;&#36136;&#20165;&#20135;&#29983;&#32447;&#24615;&#20869;&#23384;&#24320;&#38144;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#24674;&#22797;&#20102;&#26032;&#26494;&#24347;&#30340;&#20248;&#21183;&#65306;&#32039;&#20945;&#24615;&#21644;&#32447;&#24615;&#20998;&#31163;&#39044;&#35328;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#20063;&#20998;&#20139;&#20102;pr&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
arXiv:2101.05844v3 Announce Type: replace  Abstract: Tight and efficient neural network bounding is crucial to the scaling of neural network verification systems. Many efficient bounding algorithms have been presented recently, but they are often too loose to verify more challenging properties. This is due to the weakness of the employed relaxation, which is usually a linear program of size linear in the number of neurons. While a tighter linear relaxation for piecewise-linear activations exists, it comes at the cost of exponentially many constraints and currently lacks an efficient customized solver. We alleviate this deficiency by presenting two novel dual algorithms: one operates a subgradient method on a small active set of dual variables, the other exploits the sparsity of Frank-Wolfe type optimizers to incur only a linear memory cost. Both methods recover the strengths of the new relaxation: tightness and a linear separation oracle. At the same time, they share the benefits of pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#20809;&#28369;&#20989;&#25968;&#30340;&#21487;&#36870;&#21464;&#25442;&#34920;&#31034;&#21333;&#35843;&#19977;&#35282;&#24418;&#26144;&#23556;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#24471;&#30456;&#20851;&#30340;&#26080;&#31351;&#32500;&#26368;&#23567;&#21270;&#38382;&#39064;&#20855;&#26377;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;</title><link>https://arxiv.org/abs/2009.10303</link><description>&lt;p&gt;
&#20851;&#20110;&#21333;&#35843;&#19977;&#35282;&#24418;&#36755;&#36816;&#26144;&#23556;&#30340;&#34920;&#31034;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On the representation and learning of monotone triangular transport maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2009.10303
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#20809;&#28369;&#20989;&#25968;&#30340;&#21487;&#36870;&#21464;&#25442;&#34920;&#31034;&#21333;&#35843;&#19977;&#35282;&#24418;&#26144;&#23556;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#24471;&#30456;&#20851;&#30340;&#26080;&#31351;&#32500;&#26368;&#23567;&#21270;&#38382;&#39064;&#20855;&#26377;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#24230;&#30340;&#36755;&#36816;&#25552;&#20379;&#20102;&#23545;&#24314;&#27169;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#21151;&#33021;&#26041;&#27861;&#65292;&#22312;&#23494;&#24230;&#20272;&#35745;&#12289;&#36125;&#21494;&#26031;&#25512;&#26029;&#12289;&#29983;&#25104;&#24314;&#27169;&#31561;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#21333;&#35843;&#19977;&#35282;&#24418;&#36755;&#36816;&#26144;&#23556;&#8212;&#8212;Knothe-Rosenblatt (KR)&#25490;&#21015;&#30340;&#36817;&#20284;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#26159;&#19968;&#20010;&#32463;&#20856;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26144;&#23556;&#30340;&#34920;&#31034;&#21644;&#21442;&#25968;&#21270;&#23545;&#20854;&#36890;&#29992;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#20197;&#21450;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26144;&#23556;&#25152;&#24341;&#36215;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#24615;&#36136;&#65288;&#20363;&#22914;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65289;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#21333;&#35843;&#19977;&#35282;&#24418;&#26144;&#23556;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20809;&#28369;&#20989;&#25968;&#30340;&#21487;&#36870;&#21464;&#25442;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#21464;&#25442;&#30340;&#26465;&#20214;&#65292;&#20351;&#24471;&#30456;&#20851;&#30340;&#26080;&#31351;&#32500;&#26368;&#23567;&#21270;&#38382;&#39064;&#27809;&#26377;&#34394;&#20551;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#21363;&#25152;&#26377;&#23616;&#37096;&#26497;&#23567;&#20540;&#37117;&#26159;&#20840;&#23616;&#26497;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2009.10303v3 Announce Type: replace-cross  Abstract: Transportation of measure provides a versatile approach for modeling complex probability distributions, with applications in density estimation, Bayesian inference, generative modeling, and beyond. Monotone triangular transport maps$\unicode{x2014}$approximations of the Knothe$\unicode{x2013}$Rosenblatt (KR) rearrangement$\unicode{x2014}$are a canonical choice for these tasks. Yet the representation and parameterization of such maps have a significant impact on their generality and expressiveness, and on properties of the optimization problem that arises in learning a map from data (e.g., via maximum likelihood estimation). We present a general framework for representing monotone triangular maps via invertible transformations of smooth functions. We establish conditions on the transformation such that the associated infinite-dimensional minimization problem has no spurious local minima, i.e., all local minima are global minima;
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#30340;Tanimoto&#26680;&#20844;&#24335;&#65292;&#20801;&#35768;&#34913;&#37327;&#20219;&#24847;&#23454;&#20540;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20809;&#28369;&#36924;&#36817;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2007.05943</link><description>&lt;p&gt;
&#23558;Tanimoto&#31867;&#22411;&#26680;&#27867;&#21270;&#21040;&#23454;&#20540;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
On the generalization of Tanimoto-type kernels to real valued functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2007.05943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#30340;Tanimoto&#26680;&#20844;&#24335;&#65292;&#20801;&#35768;&#34913;&#37327;&#20219;&#24847;&#23454;&#20540;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20809;&#28369;&#36924;&#36817;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tanimoto&#26680;&#65288;Jaccard&#25351;&#25968;&#65289;&#26159;&#25551;&#36848;&#20108;&#20540;&#23646;&#24615;&#38598;&#30456;&#20284;&#24615;&#30340;&#30693;&#21517;&#24037;&#20855;&#12290;&#24050;&#23558;&#20854;&#25193;&#23637;&#21040;&#23646;&#24615;&#20026;&#38750;&#36127;&#23454;&#25968;&#20540;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#30340;Tanimoto&#26680;&#20844;&#24335;&#65292;&#20801;&#35768;&#34913;&#37327;&#20219;&#24847;&#23454;&#20540;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#30340;&#38598;&#21512;&#32479;&#19968;&#23646;&#24615;&#34920;&#31034;&#26469;&#26500;&#24314;&#27492;&#25193;&#23637;&#12290;&#22312;&#25512;&#23548;&#26680;&#30340;&#19968;&#33324;&#24418;&#24335;&#21518;&#65292;&#20174;&#26680;&#20989;&#25968;&#20013;&#25552;&#21462;&#20102;&#26174;&#24335;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#19968;&#33324;&#26680;&#21253;&#21547;&#21040;Tanimoto&#26680;&#20013;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#26680;&#20063;&#34920;&#31034;&#20026;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#21830;&#65292;&#24182;&#25552;&#20379;&#20102;&#20809;&#28369;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2007.05943v2 Announce Type: replace  Abstract: The Tanimoto kernel (Jaccard index) is a well known tool to describe the similarity between sets of binary attributes. It has been extended to the case when the attributes are nonnegative real values. This paper introduces a more general Tanimoto kernel formulation which allows to measure the similarity of arbitrary real-valued functions. This extension is constructed by unifying the representation of the attributes via properly chosen sets. After deriving the general form of the kernel, explicit feature representation is extracted from the kernel function, and a simply way of including general kernels into the Tanimoto kernel is shown. Finally, the kernel is also expressed as a quotient of piecewise linear functions, and a smooth approximation is provided.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30005;&#32908;&#32905;&#21050;&#28608;&#38035;&#40060;&#39285;&#21644;&#38598;&#25104;&#32593;&#32476;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#22402;&#38035;&#25439;&#23475;&#21644;&#37322;&#25918;&#27515;&#20129;&#29575;&#36739;&#39640;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#21487;&#25345;&#32493;&#24320;&#23637;&#20241;&#38386;&#38035;&#40060;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2006.10125</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#22411;&#30005;&#32908;&#32905;&#21050;&#28608;&#65288;EMS&#65289;&#40060;&#39285;&#21644;&#38598;&#25104;&#32593;&#32476;&#31639;&#27861;&#36827;&#34892;&#21487;&#25345;&#32493;&#20241;&#38386;&#38035;&#40060;&#20197;&#26368;&#22823;&#38480;&#24230;&#25552;&#39640;&#25429;&#33719;&#21644;&#37322;&#25918;&#30340;&#21487;&#29983;&#23384;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sustainable Recreational Fishing Using a Novel Electrical Muscle Stimulation (EMS) Lure and Ensemble Network Algorithm to Maximize Catch and Release Survivability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2006.10125
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30005;&#32908;&#32905;&#21050;&#28608;&#38035;&#40060;&#39285;&#21644;&#38598;&#25104;&#32593;&#32476;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#22402;&#38035;&#25439;&#23475;&#21644;&#37322;&#25918;&#27515;&#20129;&#29575;&#36739;&#39640;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#21487;&#25345;&#32493;&#24320;&#23637;&#20241;&#38386;&#38035;&#40060;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;20&#20159;&#33267;7&#20159;&#22402;&#38035;&#32773;&#65292;&#22402;&#38035;&#27604;&#21830;&#19994;&#25302;&#32593;&#28180;&#19994;&#26222;&#36941;&#22810;&#20986;&#36817;&#20116;&#20493;&#12290;&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#25968;&#21313;&#19975;&#20010;&#24037;&#20316;&#19982;&#22402;&#38035;&#20135;&#19994;&#30456;&#20851;&#65292;&#20026;&#27700;&#36793;&#31038;&#21306;&#21644;&#28180;&#19994;&#20445;&#25252;&#21306;&#24102;&#26469;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#25910;&#20837;&#12290;&#28982;&#32780;&#65292;&#22402;&#38035;&#27963;&#21160;&#30340;&#24191;&#27867;&#27969;&#34892;&#23545;&#27700;&#29983;&#29983;&#29289;&#22810;&#26679;&#24615;&#26500;&#25104;&#38590;&#20197;&#35268;&#33539;&#30340;&#23041;&#32961;&#12290;&#20363;&#22914;&#65292;&#26368;&#22810;25%&#30340;&#36807;&#24230;&#25429;&#25438;&#31181;&#32676;&#21487;&#20197;&#36861;&#28335;&#21040;&#22402;&#38035;&#32773;&#12290;&#36825;&#19968;&#20196;&#20154;&#25285;&#24551;&#30340;&#32479;&#35745;&#25968;&#25454;&#21487;&#36890;&#36807;43%&#30340;&#24179;&#22343;&#25429;&#25438;&#21644;&#37322;&#25918;&#27515;&#20129;&#29575;&#26469;&#35299;&#37322;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38057;&#30456;&#20851;&#30340;&#20260;&#23475;&#21644;&#25163;&#27861;&#19981;&#24403;&#22320;&#22788;&#29702;&#24102;&#26469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#20020;&#26102;&#19987;&#21033;&#35774;&#35745;&#20998;&#21035;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#30005;&#32908;&#32905;&#21050;&#28608;&#30340;&#38035;&#40060;&#39285;&#65292;&#20316;&#20026;&#23574;&#38057;&#30340;&#26080;&#23475;&#21644;&#20302;&#25104;&#26412;&#26367;&#20195;&#26041;&#26696;&#12290;&#26089;&#26399;&#21407;&#22411;&#26174;&#31034;&#20102;&#19968;&#31181;&#25345;&#24658;&#30340;&#30005;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2006.10125v3 Announce Type: replace-cross  Abstract: With 200-700 million anglers in the world, sportfishing is nearly five times more common than commercial trawling. Worldwide, hundreds of thousands of jobs are linked to the sportfishing industry, which generates billions of dollars for water-side communities and fisheries conservatories alike. However, the sheer popularity of recreational fishing poses threats to aquatic biodiversity that are hard to regulate. For example, as much as 25% of overfished populations can be traced to anglers. This alarming statistic is explained by the average catch and release mortality rate of 43%, which primarily results from hook-related injuries and careless out-of-water handling. The provisional-patented design proposed in this paper addresses both these problems separately First, a novel, electrical muscle stimulation based fishing lure is proposed as a harmless and low cost alternative to sharp hooks. Early prototypes show a constant elect
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#24212;&#28857;&#36827;&#34892;&#31232;&#30095;&#27491;&#20132;&#21464;&#20998;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#32039;&#30340;&#36793;&#32536;&#20284;&#28982;&#19979;&#30028;&#21644;&#26032;&#30340;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;</title><link>https://arxiv.org/abs/1910.10596</link><description>&lt;p&gt;
&#31232;&#30095;&#27491;&#20132;&#21464;&#20998;&#25512;&#26029;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Sparse Orthogonal Variational Inference for Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1910.10596
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#24212;&#28857;&#36827;&#34892;&#31232;&#30095;&#27491;&#20132;&#21464;&#20998;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#32039;&#30340;&#36793;&#32536;&#20284;&#28982;&#19979;&#30028;&#21644;&#26032;&#30340;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#21464;&#20998;&#36924;&#36817;&#39640;&#26031;&#36807;&#31243;&#30340;&#35299;&#37322;&#65292;&#20351;&#29992;&#24863;&#24212;&#28857;&#65292;&#36825;&#21487;&#20197;&#23548;&#33268;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#30340;&#31639;&#27861;&#12290;&#23427;&#22522;&#20110;&#23558;&#39640;&#26031;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#29420;&#31435;&#36807;&#31243;&#20043;&#21644;&#65306;&#19968;&#20010;&#30001;&#26377;&#38480;&#22522;&#24863;&#24212;&#28857;&#23637;&#24320;&#65292;&#21478;&#19968;&#20010;&#25429;&#33719;&#21097;&#20313;&#21464;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24418;&#24335;&#21487;&#24674;&#22797;&#29616;&#26377;&#36924;&#36817;&#65292;&#24182;&#21516;&#26102;&#20801;&#35768;&#33719;&#24471;&#26356;&#32039;&#30340;&#36793;&#32536;&#20284;&#28982;&#19979;&#30028;&#21644;&#26032;&#30340;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#20960;&#31181;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#25928;&#29575;&#65292;&#20174;&#26631;&#20934;&#22238;&#24402;&#21040;&#22810;&#31867;&#20998;&#31867;&#65292;&#20351;&#29992;(&#28145;&#24230;)&#21367;&#31215;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182;&#22312;CIFAR-10&#19978;&#25253;&#21578;&#20102;&#32431;GP&#27169;&#22411;&#20013;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1910.10596v5 Announce Type: replace-cross  Abstract: We introduce a new interpretation of sparse variational approximations for Gaussian processes using inducing points, which can lead to more scalable algorithms than previous methods. It is based on decomposing a Gaussian process as a sum of two independent processes: one spanned by a finite basis of inducing points and the other capturing the remaining variation. We show that this formulation recovers existing approximations and at the same time allows to obtain tighter lower bounds on the marginal likelihood and new stochastic variational inference algorithms. We demonstrate the efficiency of these algorithms in several Gaussian process models ranging from standard regression to multi-class classification using (deep) convolutional Gaussian processes and report state-of-the-art results on CIFAR-10 among purely GP-based models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#20215;&#21644;&#20108;&#20215;&#25293;&#21334;&#30340;&#32463;&#27982;&#32467;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#26469;&#26377;&#25928;&#35782;&#21035;&#23454;&#26102;&#31454;&#20215;&#24191;&#21578;&#30340;&#25928;&#26524;&#65292;&#26368;&#23567;&#21270;&#23454;&#39564;&#25104;&#26412;&#65292;&#24182;&#33719;&#24471;&#20102;&#39034;&#24207;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/1908.08600</link><description>&lt;p&gt;
&#23454;&#26102;&#31454;&#20215;&#24191;&#21578;&#20013;&#30340;&#22312;&#32447;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Online Causal Inference for Advertising in Real-Time Bidding Auctions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1908.08600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#20215;&#21644;&#20108;&#20215;&#25293;&#21334;&#30340;&#32463;&#27982;&#32467;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#26469;&#26377;&#25928;&#35782;&#21035;&#23454;&#26102;&#31454;&#20215;&#24191;&#21578;&#30340;&#25928;&#26524;&#65292;&#26368;&#23567;&#21270;&#23454;&#39564;&#25104;&#26412;&#65292;&#24182;&#33719;&#24471;&#20102;&#39034;&#24207;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#31454;&#20215;&#65288;RTB&#65289;&#31995;&#32479;&#36890;&#36807;&#25293;&#21334;&#23558;&#29992;&#25143;&#26333;&#20809;&#20998;&#37197;&#32473;&#31454;&#20105;&#23545;&#25163;&#30340;&#24191;&#21578;&#21830;&#65292;&#32487;&#32493;&#22312;&#25968;&#23383;&#24191;&#21578;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#12290;&#35780;&#20272;&#36825;&#31181;&#24191;&#21578;&#30340;&#26377;&#25928;&#24615;&#22312;&#30740;&#31350;&#21644;&#23454;&#36341;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#36890;&#36807;&#36825;&#31181;&#26426;&#21046;&#36141;&#20080;&#30340;&#24191;&#21578;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#21033;&#29992;&#19968;&#20215;&#21644;&#20108;&#20215;&#25293;&#21334;&#30340;&#32463;&#27982;&#32467;&#26500;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#24191;&#21578;&#25928;&#26524;&#30001;&#26368;&#20339;&#20986;&#20215;&#30830;&#23450;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#36825;&#20123;&#26368;&#20339;&#20986;&#20215;&#26159;&#21807;&#19968;&#38656;&#35201;&#24674;&#22797;&#30340;&#23545;&#35937;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;TS&#65289;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#19968;&#20010;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#25104;&#21151;&#24674;&#22797;&#36825;&#20123;&#20986;&#20215;&#21644;&#22240;&#27492;&#24191;&#21578;&#25928;&#26524;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23454;&#39564;&#25104;&#26412;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#31639;&#27861;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#26159;&#39034;&#24207;&#26368;&#20248;&#30340;&#65292;&#24182;&#21033;&#29992;RTB&#25293;&#21334;&#25968;&#25454;&#23637;&#31034;&#20102;&#23427;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1908.08600v4 Announce Type: replace  Abstract: Real-time bidding (RTB) systems, which utilize auctions to allocate user impressions to competing advertisers, continue to enjoy success in digital advertising. Assessing the effectiveness of such advertising remains a challenge in research and practice. This paper proposes a new approach to perform causal inference on advertising bought through such mechanisms. Leveraging the economic structure of first- and second-price auctions, we first show that the effects of advertising are identified by the optimal bids. Hence, since these optimal bids are the only objects that need to be recovered, we introduce an adapted Thompson sampling (TS) algorithm to solve a multi-armed bandit problem that succeeds in recovering such bids and, consequently, the effects of advertising while minimizing the costs of experimentation. We derive a regret bound for our algorithm which is order optimal and use data from RTB auctions to show that it outperform
&lt;/p&gt;</description></item><item><title>HiFT&#26159;&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#23454;&#29616;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15207</link><description>&lt;p&gt;
HiFT:&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy. (arXiv:2401.15207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15207
&lt;/p&gt;
&lt;p&gt;
HiFT&#26159;&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#23454;&#29616;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#38271;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21442;&#25968;&#38656;&#35201;&#21344;&#29992;&#22823;&#37327;GPU&#20869;&#23384;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#22120;&#20197;&#33410;&#30465;GPU&#20869;&#23384;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#38750;&#38646;&#38454;&#20248;&#21270;&#22120;&#22312;&#22823;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#19978;&#26356;&#23481;&#26131;&#25910;&#25947;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29420;&#31435;&#20110;&#20248;&#21270;&#22120;&#30340;&#31471;&#21040;&#31471;&#20998;&#23618;&#24494;&#35843;&#31574;&#30053;HiFT&#65292;&#23427;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#12290; HiFT&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#23384;&#20648;&#22312;GPU&#20869;&#23384;&#20013;&#30340;&#26799;&#24230;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#21442;&#25968;&#30340;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#20869;&#23384;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;HiFT&#23454;&#29616;&#20102;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#65288;2&#65289;HiFT&#25903;&#25345;&#21253;&#25324;&#22312;&#20869;&#30340;&#21508;&#31181;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Full-parameter fine-tuning has become the go-to choice for adapting language models (LMs) to downstream tasks due to its excellent performance. As LMs grow in size, fine-tuning the full parameters of LMs requires a prohibitively large amount of GPU memory. Existing approaches utilize zeroth-order optimizer to conserve GPU memory, which can potentially compromise the performance of LMs as non-zero order optimizers tend to converge more readily on most downstream tasks. In this paper, we propose a novel optimizer-independent end-to-end hierarchical fine-tuning strategy, HiFT, which only updates a subset of parameters at each training step. HiFT can significantly reduce the amount of gradients and optimizer state parameters residing in GPU memory at the same time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT achieves comparable performance to parameter-efficient fine-tuning and standard full parameter fine-tuning. (2) HiFT supports various optimizers including
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#27969;&#24418;&#65292;&#24182;&#20351;&#29992;Ricci&#27969;&#20351;&#27969;&#24418;&#28508;&#31354;&#38388;&#36880;&#27493;&#36866;&#24212;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;PDE&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.14591</link><description>&lt;p&gt;
&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#26102;&#21464;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Ricci flow-guided autoencoders in learning time-dependent dynamics. (arXiv:2401.14591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14591
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Ricci&#27969;&#24341;&#23548;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23398;&#20064;&#27969;&#24418;&#65292;&#24182;&#20351;&#29992;Ricci&#27969;&#20351;&#27969;&#24418;&#28508;&#31354;&#38388;&#36880;&#27493;&#36866;&#24212;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;PDE&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#23588;&#20854;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#20854;&#20013;&#27969;&#24418;&#28508;&#31354;&#38388;&#26681;&#25454;Ricci&#27969;&#21457;&#23637;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#22312;&#29289;&#29702;&#20449;&#24687;&#35774;&#32622;&#20013;&#27169;&#25311;Ricci&#27969;&#26469;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#21305;&#37197;&#27969;&#24418;&#37327;&#65292;&#20197;&#20415;&#23454;&#29616;Ricci&#27969;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#27969;&#24418;&#26159;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#23398;&#20064;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#35782;&#21035;&#20986;&#29702;&#24819;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#21516;&#26102;&#28436;&#21464;&#20063;&#33021;&#22312;&#38745;&#24577;&#26041;&#27861;&#19978;&#24341;&#36215;&#26356;&#23485;&#23481;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20855;&#26377;&#21608;&#26399;&#24615;&#21644;&#38543;&#26426;&#24615;&#31561;&#29702;&#24819;&#29305;&#24449;&#30340;PDE&#65292;&#24182;&#22312;&#20998;&#24067;&#20869;&#21644;&#22806;&#25512;&#22330;&#26223;&#20013;&#36827;&#34892;&#35823;&#24046;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a manifold-based autoencoder method for learning nonlinear dynamics in time, notably partial differential equations (PDEs), in which the manifold latent space evolves according to Ricci flow. This can be accomplished by simulating Ricci flow in a physics-informed setting, and manifold quantities can be matched so that Ricci flow is empirically achieved. With our methodology, the manifold is learned as part of the training procedure, so ideal geometries may be discerned, while the evolution simultaneously induces a more accommodating latent representation over static methods. We present our method on a range of numerical experiments consisting of PDEs that encompass desirable characteristics such as periodicity and randomness, remarking error on in-distribution and extrapolation scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCompress&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#26435;&#37325;&#32858;&#31867;&#21644;&#26381;&#21153;&#22120;&#31471;&#30693;&#35782;&#33976;&#39311;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22312;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#39640;&#24230;&#21487;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14211</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#26435;&#37325;&#32858;&#31867;&#21644;&#26381;&#21153;&#22120;&#31471;&#33976;&#39311;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation. (arXiv:2401.14211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCompress&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#26435;&#37325;&#32858;&#31867;&#21644;&#26381;&#21153;&#22120;&#31471;&#30693;&#35782;&#33976;&#39311;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22312;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#39640;&#24230;&#21487;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#26395;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#22810;&#20010;&#35774;&#22791;&#20849;&#21516;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#22797;&#30340;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#36890;&#20449;&#23548;&#33268;&#20102;&#36807;&#22810;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#32473;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#20363;&#22914;&#31232;&#30095;&#21270;&#21644;&#26435;&#37325;&#32858;&#31867;&#65292;&#28982;&#32780;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#20462;&#25913;&#24213;&#23618;&#30340;&#27169;&#22411;&#32858;&#21512;&#26041;&#26696;&#25110;&#32773;&#28041;&#21450;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#21518;&#32773;&#19981;&#20165;&#35843;&#25972;&#20102;&#27169;&#22411;&#30340;&#21387;&#32553;&#29575;&#65292;&#36824;&#38480;&#21046;&#20102;&#27169;&#22411;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;FedCompress&#65292;&#23427;&#32467;&#21512;&#20102;&#21160;&#24577;&#26435;&#37325;&#32858;&#31867;&#21644;&#26381;&#21153;&#22120;&#31471;&#30693;&#35782;&#33976;&#39311;&#65292;&#20197;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21516;&#26102;&#23398;&#20064;&#39640;&#24230;&#21487;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a promising technique for the collaborative training of deep neural networks across multiple devices while preserving data privacy. Despite its potential benefits, FL is hindered by excessive communication costs due to repeated server-client communication during training. To address this challenge, model compression techniques, such as sparsification and weight clustering are applied, which often require modifying the underlying model aggregation schemes or involve cumbersome hyperparameter tuning, with the latter not only adjusts the model's compression rate but also limits model's potential for continuous improvement over growing data. In this paper, we propose FedCompress, a novel approach that combines dynamic weight clustering and server-side knowledge distillation to reduce communication costs while learning highly generalizable models. Through a comprehensive evaluation on diverse public datasets, we demonstrate the efficacy of our approach compared to
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.14142</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#32479;&#19968;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14142
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411; (CBM)&#65292;&#22312;&#20026;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26368;&#32456;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#32416;&#27491;&#19968;&#20010;&#39044;&#27979;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33016;&#37096;&#8221;&#65289;&#26080;&#27861;&#24110;&#21161;&#32416;&#27491;&#39640;&#24230;&#30456;&#20851;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33145;&#37096;&#8221;&#65289;&#65292;&#23548;&#33268;&#26368;&#32456;&#20934;&#30830;&#29575;&#19981;&#29702;&#24819;&#65307;&#23427;&#20204;&#26080;&#27861;&#33258;&#28982;&#22320;&#37327;&#21270;&#19981;&#21516;&#27010;&#24565;&#21644;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#23545;&#20110;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#26631;&#31614;&#8220;Kentucky Warbler&#8221;&#21644;&#27010;&#24565;&#8220;&#40657;&#33394;&#22068;&#24052;&#8221;&#30340;&#22270;&#20687;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#21478;&#19968;&#20010;&#27010;&#24565;&#8220;&#40657;&#33394;&#20896;&#8221;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#26356;&#28145;&#23618;&#27425;&#30340;&#27934;&#23519;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;Energy-based Concept Bottleneck Models&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11648</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#20998;&#23618;&#27491;&#21017;&#21270;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#30340;&#35786;&#26029;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#21046;&#23450;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20027;&#21160;&#26410;&#26469;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35768;&#22810;&#30740;&#31350;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;EHR&#25968;&#25454;&#22266;&#26377;&#30340;&#24322;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#65292;&#24517;&#28982;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NECHO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20998;&#23618;&#27491;&#21017;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#19968;&#23545;&#21452;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#34701;&#21512;&#28085;&#30422;&#21307;&#23398;&#20195;&#30721;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#26041;&#38754;&#20449;&#24687;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22260;&#32469;&#30528;&#21307;&#23398;&#20195;&#30721;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21307;&#23398;&#26412;&#20307;&#20013;&#30340;&#29238;&#32423;&#20449;&#24687;&#26469;&#35268;&#33539;&#29305;&#23450;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;EHR&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>QuasiNet&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#20056;&#31215;&#23618;&#35299;&#20915;&#20102;&#23567;&#35268;&#27169;&#38544;&#34255;&#31070;&#32463;&#20803;&#19979;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22312;&#38590;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#25910;&#25947;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06137</link><description>&lt;p&gt;
QuasiNet: &#19968;&#31181;&#20855;&#26377;&#21487;&#35757;&#32451;&#20056;&#31215;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QuasiNet: a neural network with trainable product layers. (arXiv:2401.06137v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06137
&lt;/p&gt;
&lt;p&gt;
QuasiNet&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#20056;&#31215;&#23618;&#35299;&#20915;&#20102;&#23567;&#35268;&#27169;&#38544;&#34255;&#31070;&#32463;&#20803;&#19979;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22312;&#38590;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#25910;&#25947;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#20284;XOR&#25110;&#22855;&#20598;&#26657;&#39564;&#31561;&#38590;&#39064;&#30340;&#23567;&#35268;&#27169;&#38544;&#34255;&#31070;&#32463;&#20803;&#19979;&#21482;&#33021;&#23454;&#29616;&#26377;&#38480;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#25104;&#21151;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21463;&#29616;&#26377;&#20855;&#26377;&#25152;&#35859;&#20056;&#31215;&#31070;&#32463;&#20803;&#21644;&#30001;&#32463;&#20856;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#25512;&#23548;&#20986;&#30340;&#23398;&#20064;&#35268;&#21017;&#21551;&#21457;&#65292;&#20248;&#38597;&#22320;&#35299;&#20915;&#20102;&#20114;&#26021;&#24773;&#20917;&#30340;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#30340;&#20855;&#26377;&#39044;&#35774;&#19988;&#19981;&#21487;&#35843;&#33410;&#26435;&#37325;&#30340;&#20056;&#31215;&#31070;&#32463;&#20803;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#20803;&#20056;&#31215;&#23618;&#20063;&#33021;&#22815;&#23398;&#20064;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#35813;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#25104;&#21151;&#29575;&#19982;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#22312;&#21069;&#36848;&#38382;&#39064;&#21644;&#20854;&#20182;&#38590;&#39064;&#65288;&#22914;&#20004;&#20010;&#34746;&#26059;&#65289;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#26356;&#25104;&#21151;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical neural networks achieve only limited convergence in hard problems such as XOR or parity when the number of hidden neurons is small. With the motivation to improve the success rate of neural networks in these problems, we propose a new neural network model inspired by existing neural network models with so called product neurons and a learning rule derived from classical error backpropagation, which elegantly solves the problem of mutually exclusive situations. Unlike existing product neurons, which have weights that are preset and not adaptable, our product layers of neurons also do learn. We tested the model and compared its success rate to a classical multilayer perceptron in the aforementioned problems as well as in other hard problems such as the two spirals. Our results indicate that our model is clearly more successful than the classical MLP and has the potential to be used in many tasks and applications.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;AUROC&#21644;AUPRC&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#21487;&#20197;&#20197;&#27010;&#29575;&#26415;&#35821;&#31616;&#27905;&#22320;&#30456;&#20851;&#32852;&#12290;&#30456;&#27604;&#20110;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30340;AUPRC&#20248;&#36234;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;AUPRC&#24182;&#19981;&#22914;&#20154;&#20204;&#39044;&#26399;&#30340;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#23475;&#30340;&#25351;&#26631;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25991;&#29486;&#39564;&#35777;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.06091</link><description>&lt;p&gt;
AUROC&#21644;AUPRC&#22312;&#31867;&#19981;&#24179;&#34913;&#19979;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at AUROC and AUPRC under Class Imbalance. (arXiv:2401.06091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06091
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;AUROC&#21644;AUPRC&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#21487;&#20197;&#20197;&#27010;&#29575;&#26415;&#35821;&#31616;&#27905;&#22320;&#30456;&#20851;&#32852;&#12290;&#30456;&#27604;&#20110;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30340;AUPRC&#20248;&#36234;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;AUPRC&#24182;&#19981;&#22914;&#20154;&#20204;&#39044;&#26399;&#30340;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#23475;&#30340;&#25351;&#26631;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25991;&#29486;&#39564;&#35777;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#24191;&#27867;&#30340;&#35266;&#28857;&#26159;&#65292;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#38754;&#31215;&#21463;&#38480;&#21046;&#30340;&#20934;&#30830;&#29575;&#26354;&#32447;&#65288;AUPRC&#65289;&#27604;&#21463;&#35797;&#32773;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65288;AUROC&#65289;&#26356;&#22909;&#22320;&#29992;&#20110;&#27169;&#22411;&#27604;&#36739;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#36890;&#36807;&#26032;&#39062;&#30340;&#25968;&#23398;&#20998;&#26512;&#25361;&#25112;&#20102;&#36825;&#19968;&#35266;&#28857;&#65292;&#24182;&#35828;&#26126;&#20102;AUROC&#21644;AUPRC&#21487;&#20197;&#20197;&#27010;&#29575;&#26415;&#35821;&#31616;&#27905;&#22320;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AUPRC&#24182;&#19981;&#22914;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30340;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#26356;&#20248;&#65292;&#29978;&#33267;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#23475;&#30340;&#25351;&#26631;&#65292;&#22240;&#20026;&#23427;&#20542;&#21521;&#20110;&#36807;&#20998;&#20559;&#21521;&#20110;&#22312;&#27491;&#26679;&#26412;&#36739;&#20026;&#39057;&#32321;&#30340;&#23376;&#32676;&#20013;&#25913;&#21892;&#27169;&#22411;&#12290;&#36825;&#31181;&#20559;&#24046;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#22686;&#21152;&#31639;&#27861;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#20123;&#27934;&#35265;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#22238;&#39038;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;arXiv&#19978;&#30340;150&#22810;&#19975;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#37325;&#28857;&#26159;&#39564;&#35777;&#21644;&#35777;&#26126;&#22768;&#31216;&#30340;AUPRC&#20248;&#36234;&#24615;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning (ML), a widespread adage is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for binary classification tasks with class imbalance. This paper challenges this notion through novel mathematical analysis, illustrating that AUROC and AUPRC can be concisely related in probabilistic terms. We demonstrate that AUPRC, contrary to popular belief, is not superior in cases of class imbalance and might even be a harmful metric, given its inclination to unduly favor model improvements in subpopulations with more frequent positive labels. This bias can inadvertently heighten algorithmic disparities. Prompted by these insights, a thorough review of existing ML literature was conducted, utilizing large language models to analyze over 1.5 million papers from arXiv. Our investigation focused on the prevalence and substantiation of the purported AUPRC superiority. The result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Shapley&#20540;&#22238;&#24402;&#23545;&#28192;&#36947;&#21512;&#20316;&#20249;&#20276;&#23618;&#38754;&#30340;&#33829;&#38144;&#32489;&#25928;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#19982;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;Shapley&#20540;&#22238;&#24402;&#30340;&#23454;&#29992;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#35843;&#25972;&#31995;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.05653</link><description>&lt;p&gt;
&#20351;&#29992;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#65288;MMM&#65289;&#21644;Shapley&#20540;&#22238;&#24402;&#37327;&#21270;&#28192;&#36947;&#21512;&#20316;&#20249;&#20276;&#23618;&#38754;&#30340;&#33829;&#38144;&#32489;&#25928;
&lt;/p&gt;
&lt;p&gt;
Quantifying Marketing Performance at Channel-Partner Level by Using Marketing Mix Modeling (MMM) and Shapley Value Regression. (arXiv:2401.05653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Shapley&#20540;&#22238;&#24402;&#23545;&#28192;&#36947;&#21512;&#20316;&#20249;&#20276;&#23618;&#38754;&#30340;&#33829;&#38144;&#32489;&#25928;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#19982;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;Shapley&#20540;&#22238;&#24402;&#30340;&#23454;&#29992;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#35843;&#25972;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#28192;&#36947;&#21512;&#20316;&#20249;&#20276;&#23618;&#38754;&#21033;&#29992;Shapley&#20540;&#22238;&#24402;&#26469;&#35299;&#26512;&#33829;&#38144;&#32489;&#25928;&#30340;&#24212;&#29992;&#65292;&#34917;&#20805;&#20102;&#28192;&#36947;&#23618;&#38754;&#30340;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#65288;MMM&#65289;&#12290;&#21033;&#29992;&#26469;&#33258;&#37329;&#34701;&#26381;&#21153;&#34892;&#19994;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Shapley&#20540;&#22238;&#24402;&#22312;&#35780;&#20272;&#20010;&#21035;&#21512;&#20316;&#20249;&#20276;&#36129;&#29486;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#32467;&#26500;&#21270;&#30340;&#29616;&#22330;&#27979;&#35797;&#20197;&#21450;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#26368;&#20026;&#20934;&#30830;&#65292;&#20294;&#32463;&#24120;&#20250;&#38750;&#24120;&#22797;&#26434;&#21644;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;Shapley&#20540;&#22238;&#24402;&#26159;&#19968;&#31181;&#26356;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#31163;&#33829;&#38144;&#28192;&#36947;&#20013;&#27599;&#20010;&#33829;&#38144;&#21512;&#20316;&#20249;&#20276;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25512;&#23548;&#35843;&#25972;&#31995;&#25968;&#65292;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the application of Shapley Value Regression in dissecting marketing performance at channel-partner level, complementing channel-level Marketing Mix Modeling (MMM). Utilizing real-world data from the financial services industry, we demonstrate the practicality of Shapley Value Regression in evaluating individual partner contributions. Although structured in-field testing along with cooperative game theory is most accurate, it can often be highly complex and expensive to conduct. Shapley Value Regression is thus a more feasible approach to disentangle the influence of each marketing partner within a marketing channel. We also propose a simple method to derive adjusted coefficients of Shapley Value Regression and compares it with alternative approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.00110</link><description>&lt;p&gt;
&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20381;&#38752;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26469;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#28982;&#32780;&#20854;&#24778;&#20154;&#30340;&#25928;&#26524;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#28304;&#33258;&#20854;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#24863;&#30693;&#25351;&#23548;&#30340;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;&#25193;&#25955;&#35757;&#32451;&#20013;&#21152;&#20837;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;&#30001;&#20110;&#25193;&#25955;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#20998;&#25968;&#21305;&#37197;&#30446;&#26631;&#19982;&#26080;&#30417;&#30563;&#35757;&#32451;&#24863;&#30693;&#32593;&#32476;&#26102;&#20351;&#29992;&#30340;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#30446;&#26631;&#38750;&#24120;&#30456;&#20284;&#65292;&#22240;&#27492;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#24863;&#30693;&#32593;&#32476;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#24863;&#30693;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#24863;&#30693;&#30446;&#26631;&#65292;&#20854;&#32467;&#26524;&#26159;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#26465;&#20214;&#29983;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#32780;&#19981;&#19982;&#26465;&#20214;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;M3D&#30340;&#26032;&#22411;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#25552;&#39640;&#21387;&#32553;&#25928;&#29575;&#65292;&#20811;&#26381;&#20102;&#20248;&#21270;&#36807;&#31243;&#22312;&#23454;&#38469;&#21644;&#36739;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.15927</link><description>&lt;p&gt;
M3D&#65306;&#36890;&#36807;&#26368;&#23567;&#21270;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#36827;&#34892;&#25968;&#25454;&#38598;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy. (arXiv:2312.15927v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;M3D&#30340;&#26032;&#22411;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#25552;&#39640;&#21387;&#32553;&#25928;&#29575;&#65292;&#20811;&#26381;&#20102;&#20248;&#21270;&#36807;&#31243;&#22312;&#23454;&#38469;&#21644;&#36739;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#23384;&#20648;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#21512;&#25104;&#38598;&#21512;&#26469;&#20445;&#30041;&#21407;&#22987;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#30446;&#21069;&#65292;&#20197;&#20248;&#21270;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#26159;&#25968;&#25454;&#38598;&#21387;&#32553;&#39046;&#22495;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21452;&#23618;&#20248;&#21270;&#36807;&#31243;&#38459;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#21644;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#25552;&#39640;&#21387;&#32553;&#25928;&#29575;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20998;&#24067;&#21305;&#37197;&#65288;DM&#65289;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#21387;&#32553;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19987;&#27880;&#20110;&#23545;&#40784;&#20998;&#24067;&#30340;&#19968;&#38454;&#30697;&#65292;&#30446;&#21069;&#30340;&#22522;&#20110;DM&#30340;&#26041;&#27861;&#19982;&#20197;&#20248;&#21270;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#32467;&#26524;&#19981;&#22826;&#21487;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;M3D&#30340;&#26032;&#22411;&#22522;&#20110;DM&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training state-of-the-art (SOTA) deep models often requires extensive data, resulting in substantial training and storage costs. To address these challenges, dataset condensation has been developed to learn a small synthetic set that preserves essential information from the original large-scale dataset. Nowadays, optimization-oriented methods have been the primary method in the field of dataset condensation for achieving SOTA results. However, the bi-level optimization process hinders the practical application of such methods to realistic and larger datasets. To enhance condensation efficiency, previous works proposed Distribution-Matching (DM) as an alternative, which significantly reduces the condensation cost. Nonetheless, current DM-based methods have yielded less comparable results to optimization-oriented methods due to their focus on aligning only the first moment of the distributions. In this paper, we present a novel DM-based method named M3D for dataset condensation by Minimi
&lt;/p&gt;</description></item><item><title>&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#32467;&#21512;&#20102;&#22270;&#25968;&#25454;&#24211;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#39640;&#25928;&#23384;&#20648;&#12289;&#26816;&#32034;&#21644;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2312.15591</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Neural Graph Databases. (arXiv:2312.15591v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15591
&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#32467;&#21512;&#20102;&#22270;&#25968;&#25454;&#24211;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#39640;&#25928;&#23384;&#20648;&#12289;&#26816;&#32034;&#21644;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#20449;&#24687;&#31995;&#32479;&#26102;&#20195;&#65292;&#39640;&#25928;&#20934;&#30830;&#22320;&#26816;&#32034;&#25968;&#25454;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#65288;NGDB&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#23558;&#22270;&#25968;&#25454;&#24211;&#65288;&#22270;&#24418;&#25968;&#25454;&#24211;&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#39640;&#25928;&#23384;&#20648;&#12289;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;&#31070;&#32463;&#23884;&#20837;&#23384;&#20648;&#21644;&#22797;&#26434;&#31070;&#32463;&#36923;&#36753;&#26597;&#35810;&#22238;&#31572;&#20026;NGDB&#25552;&#20379;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#24403;&#22270;&#24418;&#19981;&#23436;&#25972;&#26102;&#65292;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#21487;&#20197;&#36890;&#36807;&#25552;&#21462;&#28508;&#22312;&#27169;&#24335;&#21644;&#34920;&#31034;&#26469;&#22635;&#34917;&#22270;&#32467;&#26500;&#20013;&#30340;&#31354;&#32570;&#65292;&#25581;&#31034;&#38544;&#34255;&#30340;&#20851;&#31995;&#24182;&#23454;&#29616;&#20934;&#30830;&#30340;&#26597;&#35810;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#24694;&#24847;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#32452;&#21512;&#26597;&#35810;&#25512;&#26029;&#20986;&#26356;&#22810;&#25935;&#24863;&#20449;&#24687;&#65292;&#20363;&#22914;&#36890;&#36807;&#27604;&#36739;&#22270;&#25968;&#25454;&#24211;&#20013;Turing&#22870;&#24471;&#20027;&#30340;&#31572;&#26696;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data and rapidly evolving information systems, efficient and accurate data retrieval has become increasingly crucial. Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (graph DBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data. The usage of neural embedding storage and complex neural logical query answering provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the database. Malicious attackers can infer more sensitive information in the database using well-designed combinatorial queries, such as by comparing the answer sets of where Turing Award winner
&lt;/p&gt;</description></item><item><title>TiMix&#26159;&#19968;&#31181;&#23558;&#25991;&#26412;&#24863;&#30693;&#30340;&#22270;&#20687;&#28151;&#21512;&#25216;&#26415;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.08846</link><description>&lt;p&gt;
TiMix: &#25991;&#26412;&#24863;&#30693;&#22270;&#20687;&#28151;&#21512;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08846
&lt;/p&gt;
&lt;p&gt;
TiMix&#26159;&#19968;&#31181;&#23558;&#25991;&#26412;&#24863;&#30693;&#30340;&#22270;&#20687;&#28151;&#21512;&#25216;&#26415;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#65288;SMCL&#65289;&#36890;&#36807;&#23545;&#40784;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#65292;&#26174;&#33879;&#25512;&#36827;&#20102;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#32476;&#25910;&#38598;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#25193;&#22823;SMCL&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#25552;&#39640;VLP&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#24863;&#30693;&#22270;&#20687;&#28151;&#21512;&#65288;TiMix&#65289;&#65292;&#23558;&#22522;&#20110;&#28151;&#21512;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#38598;&#25104;&#21040;SMCL&#20013;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#20174;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#30340;&#35282;&#24230;&#23545;TiMix&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#34920;&#26126;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#30340;&#28151;&#21512;&#25968;&#25454;&#26679;&#26412;&#38544;&#24335;&#22320;&#20316;&#20026;&#23545;&#27604;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;TiMix&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMixfrom a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when be
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20998;&#25968;&#23548;&#25968;&#19982;&#25972;&#25968;&#23548;&#25968;&#20043;&#38388;&#30340;&#26032;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#24179;&#28369;&#19988;&#20984;&#12289;&#24179;&#28369;&#19988;&#24378;&#20984;&#20197;&#21450;&#24179;&#28369;&#19988;&#38750;&#20984;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#20026;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2311.18426</link><description>&lt;p&gt;
&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Fractional Gradient Descent. (arXiv:2311.18426v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20998;&#25968;&#23548;&#25968;&#19982;&#25972;&#25968;&#23548;&#25968;&#20043;&#38388;&#30340;&#26032;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#24179;&#28369;&#19988;&#20984;&#12289;&#24179;&#28369;&#19988;&#24378;&#20984;&#20197;&#21450;&#24179;&#28369;&#19988;&#38750;&#20984;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#20026;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25968;&#23548;&#25968;&#26159;&#25972;&#25968;&#38454;&#23548;&#25968;&#30340;&#19968;&#31181;&#24191;&#20041;&#25512;&#24191;&#12290;&#23545;&#20110;&#20248;&#21270;&#38382;&#39064;&#65292;&#30740;&#31350;&#20351;&#29992;&#20998;&#25968;&#23548;&#25968;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#26159;&#38750;&#24120;&#26377;&#24847;&#20041;&#30340;&#12290;&#30446;&#21069;&#65292;&#23545;&#20110;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#22312;&#30740;&#31350;&#26041;&#27861;&#21644;&#30740;&#31350;&#29615;&#22659;&#26041;&#38754;&#37117;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#20998;&#26512;&#20102;&#24179;&#28369;&#19988;&#20984;&#12289;&#24179;&#28369;&#19988;&#24378;&#20984;&#20197;&#21450;&#24179;&#28369;&#19988;&#38750;&#20984;&#29615;&#22659;&#19979;&#30340;&#20998;&#25968;&#26799;&#24230;&#19979;&#38477;&#30340;&#21464;&#31181;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24314;&#31435;&#23558;&#20998;&#25968;&#23548;&#25968;&#19982;&#25972;&#25968;&#23548;&#25968;&#32852;&#31995;&#36215;&#26469;&#30340;&#26032;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#30028;&#38480;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#19988;&#24378;&#20984;&#20989;&#25968;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#21644;&#23545;&#20110;&#24179;&#28369;&#19988;&#20984;&#20989;&#25968;&#30340;O(1/T)&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26356;&#36866;&#21512;&#20998;&#25968;&#23548;&#25968;&#30340;&#25193;&#23637;&#24179;&#28369;&#24230;&#27010;&#24565;-Holder&#24179;&#28369;&#24230;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#19988;&#38750;&#20984;&#20989;&#25968;&#30340;O(1/T)&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fractional derivatives are a well-studied generalization of integer order derivatives. Naturally, for optimization, it is of interest to understand the convergence properties of gradient descent using fractional derivatives. Convergence analysis of fractional gradient descent is currently limited both in the methods analyzed and the settings analyzed. This paper aims to fill in these gaps by analyzing variations of fractional gradient descent in smooth and convex, smooth and strongly convex, and smooth and non-convex settings. First, novel bounds will be established bridging fractional and integer derivatives. Then, these bounds will be applied to the aforementioned settings to prove linear convergence for smooth and strongly convex functions and $O(1/T)$ convergence for smooth and convex functions. Additionally, we prove $O(1/T)$ convergence for smooth and non-convex functions using an extended notion of smoothness - H\"older smoothness - that is more natural for fractional derivative
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.13541</link><description>&lt;p&gt;
&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#19982;&#26080;&#20559;&#38598;&#20013;&#21147;
&lt;/p&gt;
&lt;p&gt;
Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#30340;&#20108;&#27425;&#20851;&#31995;&#65292;&#20854;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#24403;&#22788;&#29702;&#38271;&#25991;&#26723;&#25110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26102;&#65292;&#36825;&#19968;&#38480;&#21046;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#65292;&#23545;&#33258;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34913;&#37327;&#36825;&#20123;&#25968;&#37327;&#30340;&#24037;&#20855;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#65292;&#26088;&#22312;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24120;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#20248;&#20110;&#20854;&#20182;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26367;&#20195;&#26041;&#27861;&#65292;&#20026;&#22686;&#24378;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#38468;&#22312;&#34917;&#20805;&#26448;&#26009;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models. Our code is available in supplementary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.01200</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#20986;&#29616;&#26032;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#26032;&#27169;&#22411;&#32780;&#19981;&#26159;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#25910;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26032;&#35821;&#35328;&#20986;&#29616;&#26102;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#22909;&#22788;&#21644;&#24330;&#31471;&#65292;&#21363;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#20174;&#21333;&#35821;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20986;&#21457;&#65292;&#25105;&#20204;&#36880;&#27493;&#28155;&#21152;&#20102;&#26469;&#33258;&#25386;&#23041;&#35821;&#21644;&#20912;&#23707;&#35821;&#30340;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#21069;&#21521;&#21644;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#22914;&#20309;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#39034;&#24207;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#21069;&#21521;&#36716;&#31227;&#20027;&#35201;&#26159;&#27491;&#21521;&#30340;&#65292;&#19981;&#21463;&#35821;&#35328;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#21017;&#21487;&#33021;&#26159;&#27491;&#21521;&#30340;&#25110;&#36127;&#21521;&#30340;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#24335;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#31181;&#35821;&#35328;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#29256;&#26412;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29255;&#27573;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#26469;&#20943;&#23569;&#23398;&#20064;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#25552;&#39640;&#20102;2&#20493;&#65292;&#24182;&#19988;&#23545;&#20110;&#38543;&#26426;&#29615;&#22659;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20141</link><description>&lt;p&gt;
&#23545;&#27604;&#24046;&#24322;&#24615;&#39044;&#27979;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Contrastive Difference Predictive Coding. (arXiv:2310.20141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#29256;&#26412;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29255;&#27573;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#26469;&#20943;&#23569;&#23398;&#20064;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#25552;&#39640;&#20102;2&#20493;&#65292;&#24182;&#19988;&#23545;&#20110;&#38543;&#26426;&#29615;&#22659;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21644;&#25512;&#29702;&#26410;&#26469;&#26159;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#30340;&#26680;&#24515;&#12290;&#20363;&#22914;&#65292;&#30446;&#26631;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#23398;&#20064;&#34920;&#31034;&#20197;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#35775;&#38382;&#30340;&#29366;&#24577;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#20351;&#29992;&#23545;&#27604;&#24615;&#39044;&#27979;&#32534;&#30721;&#26469;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20294;&#23398;&#20064;&#32534;&#30721;&#38271;&#26399;&#20381;&#36182;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#29256;&#26412;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65292;&#23558;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29255;&#27573;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#20943;&#23569;&#23398;&#20064;&#26410;&#26469;&#20107;&#20214;&#39044;&#27979;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#23548;&#20986;&#30446;&#26631;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25104;&#21151;&#29575;&#19978;&#23454;&#29616;&#20102;&#20013;&#20301;&#25968;&#25552;&#39640;2&#20493;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#24212;&#23545;&#38543;&#26426;&#29615;&#22659;&#12290;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#32422;&#20026;20&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FlowDRO&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#27969;&#27169;&#22411;&#21644;Wasserstein&#36817;&#31471;&#26799;&#24230;&#27969;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#26356;&#22823;&#26679;&#26412;&#22823;&#23567;&#30340;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19253</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Flow-based Distributionally Robust Optimization. (arXiv:2310.19253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FlowDRO&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#27969;&#27169;&#22411;&#21644;Wasserstein&#36817;&#31471;&#26799;&#24230;&#27969;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#26356;&#22823;&#26679;&#26412;&#22823;&#23567;&#30340;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FlowDRO&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#35201;&#27714;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#65288;&#20063;&#31216;&#20026;&#26368;&#19981;&#21033;&#20998;&#24067;&#65292;LFD&#65289;&#26159;&#36830;&#32493;&#30340;&#65292;&#20174;&#32780;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#21487;&#25193;&#23637;&#21040;&#20855;&#26377;&#26356;&#22823;&#26679;&#26412;&#22823;&#23567;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#23545;&#35825;&#23548;&#30340;&#40065;&#26834;&#31639;&#27861;&#30340;&#26356;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26080;&#38480;&#32500;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#65292;&#22312;&#25968;&#25454;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36830;&#32493;&#26102;&#38388;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;Wasserstein&#36817;&#31471;&#26799;&#24230;&#27969;&#31867;&#22411;&#30340;&#31639;&#27861;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36880;&#27493;&#35757;&#32451;&#22359;&#20869;&#30340;&#31070;&#32463;&#32593;&#32476;&#24207;&#21015;&#26469;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#26694;&#26550;&#36890;&#29992;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#21644;&#22823;&#26679;&#26412;&#22823;&#23567;&#65292;&#24182;&#21487;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a computationally efficient framework, called \texttt{FlowDRO}, for solving flow-based distributionally robust optimization (DRO) problems with Wasserstein uncertainty sets, when requiring the worst-case distribution (also called the Least Favorable Distribution, LFD) to be continuous so that the algorithm can be scalable to problems with larger sample sizes and achieve better generalization capability for the induced robust algorithms. To tackle the computationally challenging infinitely dimensional optimization problem, we leverage flow-based models, continuous-time invertible transport maps between the data distribution and the target distribution, and develop a Wasserstein proximal gradient flow type of algorithm. In practice, we parameterize the transport maps by a sequence of neural networks progressively trained in blocks by gradient descent. Our computational framework is general, can handle high-dimensional data with large sample sizes, and can be useful for various
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#24378;&#30340;&#31616;&#21333;&#38750;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;GraphACL&#65292;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#33410;&#28857;&#30340;&#38750;&#23545;&#31216;&#35270;&#22270;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#21516;&#31867;&#21644;&#24322;&#31867;&#22270;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#23545;&#20110;&#24314;&#27169;&#24322;&#31867;&#22270;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.18884</link><description>&lt;p&gt;
&#26080;&#38656;&#22686;&#24378;&#30340;&#31616;&#21333;&#38750;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple and Asymmetric Graph Contrastive Learning without Augmentations. (arXiv:2310.18884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#24378;&#30340;&#31616;&#21333;&#38750;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;GraphACL&#65292;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#33410;&#28857;&#30340;&#38750;&#23545;&#31216;&#35270;&#22270;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#21516;&#31867;&#21644;&#24322;&#31867;&#22270;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#23545;&#20110;&#24314;&#27169;&#24322;&#31867;&#22270;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#26174;&#31034;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#21046;&#30340;&#22270;&#22686;&#24378;&#21644;&#21516;&#31867;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#36830;&#36890;&#33410;&#28857;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#31867;&#26631;&#31614;&#21644;&#19981;&#30456;&#20284;&#29305;&#24449;&#30340;&#24322;&#31867;&#22270;&#19978;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21516;&#31867;&#21644;&#24322;&#31867;&#22270;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#33410;&#28857;&#30340;&#38750;&#23545;&#31216;&#35270;&#22270;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#31216;&#20026;&#22270;&#30340;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;(GraphACL)&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19981;&#20381;&#36182;&#20110;&#22270;&#22686;&#24378;&#21644;&#21516;&#31867;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;GraphACL&#33021;&#22815;&#25429;&#25417;&#21333;&#36339;&#26412;&#22320;&#37051;&#22495;&#20449;&#24687;&#21644;&#21452;&#36339;&#21333;&#19968;&#30456;&#20284;&#24615;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#24314;&#27169;&#24322;&#31867;&#22270;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning (GCL) has shown superior performance in representation learning in graph-structured data. Despite their success, most existing GCL methods rely on prefabricated graph augmentation and homophily assumptions. Thus, they fail to generalize well to heterophilic graphs where connected nodes may have different class labels and dissimilar features. In this paper, we study the problem of conducting contrastive learning on homophilic and heterophilic graphs. We find that we can achieve promising performance simply by considering an asymmetric view of the neighboring nodes. The resulting simple algorithm, Asymmetric Contrastive Learning for Graphs (GraphACL), is easy to implement and does not rely on graph augmentations and homophily assumptions. We provide theoretical and empirical evidence that GraphACL can capture one-hop local neighborhood information and two-hop monophily similarity, which are both important for modeling heterophilic graphs. Experimental results s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#26469;&#25913;&#21892;&#20809;&#35889;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.18306</link><description>&lt;p&gt;
&#30417;&#30563;&#21644;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Supervised and Penalized Baseline Correction. (arXiv:2310.18306v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#26469;&#25913;&#21892;&#20809;&#35889;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#27979;&#37327;&#21487;&#20197;&#26174;&#31034;&#30001;&#21560;&#25910;&#21644;&#25955;&#23556;&#25104;&#20998;&#28151;&#21512;&#24341;&#36215;&#30340;&#25197;&#26354;&#20809;&#35889;&#24418;&#29366;&#12290;&#36825;&#20123;&#25197;&#26354;&#65288;&#25110;&#22522;&#32447;&#65289;&#36890;&#24120;&#34920;&#29616;&#20026;&#38750;&#24658;&#23450;&#20559;&#31227;&#25110;&#20302;&#39057;&#25391;&#33633;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22522;&#32447;&#21487;&#33021;&#23545;&#20998;&#26512;&#21644;&#23450;&#37327;&#32467;&#26524;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22522;&#32447;&#26657;&#27491;&#26159;&#19968;&#20010;&#28085;&#30422;&#20102;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#24635;&#31216;&#65292;&#36890;&#36807;&#33719;&#21462;&#22522;&#32447;&#20809;&#35889;&#65288;&#19981;&#38656;&#35201;&#30340;&#25197;&#26354;&#65289;&#24182;&#36890;&#36807;&#24046;&#24322;&#21270;&#21435;&#38500;&#25197;&#26354;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26657;&#27491;&#26041;&#27861;&#21363;&#20351;&#21487;&#29992;&#20998;&#26512;&#29289;&#27987;&#24230;&#25110;&#32773;&#23427;&#20204;&#23545;&#35266;&#23519;&#21040;&#30340;&#20809;&#35889;&#21464;&#24322;&#26377;&#37325;&#35201;&#36129;&#29486;&#65292;&#20063;&#27809;&#26377;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#21463;&#32602;&#22522;&#32447;&#26657;&#27491;&#65289;&#24182;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20808;&#39564;&#20998;&#26512;&#29289;&#27987;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#23558;&#22312;&#20004;&#20010;&#36817;&#32418;&#22806;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#24615;&#33021;&#65292;&#21253;&#25324;&#32463;&#20856;&#21463;&#32602;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectroscopic measurements can show distorted spectra shapes arising from a mixture of absorbing and scattering contributions. These distortions (or baselines) often manifest themselves as non-constant offsets or low-frequency oscillations. As a result, these baselines can adversely affect analytical and quantitative results. Baseline correction is an umbrella term where one applies pre-processing methods to obtain baseline spectra (the unwanted distortions) and then remove the distortions by differencing. However, current state-of-the art baseline correction methods do not utilize analyte concentrations even if they are available, or even if they contribute significantly to the observed spectral variability. We examine a class of state-of-the-art methods (penalized baseline correction) and modify them such that they can accommodate a priori analyte concentration such that prediction can be enhanced. Performance will be access on two near infra-red data sets across both classical penal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#32676;&#20307;&#25552;&#31034;&#23454;&#29616;&#33719;&#21462;&#36890;&#29992;&#30693;&#35782;&#21644;&#20010;&#24615;&#21270;&#30693;&#35782;&#65292;&#20197;&#35757;&#32451;&#36866;&#24212;&#19981;&#21516;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.18285</link><description>&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#19982;&#32676;&#20307;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Federated Learning with Group-Aware Prompt Tuning. (arXiv:2310.18285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#32676;&#20307;&#25552;&#31034;&#23454;&#29616;&#33719;&#21462;&#36890;&#29992;&#30693;&#35782;&#21644;&#20010;&#24615;&#21270;&#30693;&#35782;&#65292;&#20197;&#35757;&#32451;&#36866;&#24212;&#19981;&#21516;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20419;&#20351;&#23427;&#20204;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#20855;&#26377;&#19981;&#21516;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#22330;&#26223;&#12290;&#20026;&#20102;&#28385;&#36275;FL&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#38656;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#24341;&#20837;&#20102;&#21516;&#26102;&#23398;&#20064;&#20849;&#20139;&#21644;&#32676;&#20307;&#25552;&#31034;&#30340;&#27010;&#24565;&#65292;&#33021;&#22815;&#21516;&#26102;&#33719;&#21462;&#36890;&#29992;&#30693;&#35782;&#21644;&#32676;&#20307;&#29305;&#23450;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25552;&#31034;&#36873;&#25321;&#27169;&#22359;&#20026;&#27599;&#20010;&#36755;&#20837;&#20998;&#37197;&#20010;&#24615;&#21270;&#30340;&#32676;&#20307;&#25552;&#31034;&#65292;&#20351;&#20840;&#23616;&#27169;&#22411;&#19982;&#27599;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#19981;&#21516;&#30340;&#26412;&#22320;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25645;&#24314;&#20102;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved remarkable success in various machine-learning tasks, prompting their widespread adoption. In this paper, we explore their application in the context of federated learning (FL), with a particular focus on heterogeneous scenarios where individual clients possess diverse local datasets. To meet the computational and communication demands of FL, we leverage pre-trained Transformers and use an efficient prompt-tuning strategy. Our strategy introduces the concept of learning both shared and group prompts, enabling the acquisition of universal knowledge and group-specific knowledge simultaneously. Additionally, a prompt selection module assigns personalized group prompts to each input, aligning the global model with the data distribution of each client. This approach allows us to train a single global model that can automatically adapt to various local client data distributions without requiring local fine-tuning. In this way, our proposed method effectively bridge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#31354;&#20013;&#32852;&#21512;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#24191;&#25773;&#25658;&#24102;&#26412;&#22320;&#20449;&#24687;&#30340;&#27169;&#25311;&#20449;&#21495;&#23454;&#29616;&#26356;&#26032;&#31574;&#30053;&#21442;&#25968;&#65292;&#30740;&#31350;&#20102;&#22122;&#22768;&#21644;&#20449;&#36947;&#22833;&#30495;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#32467;&#26524;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16592</link><description>&lt;p&gt;
&#26080;&#32447;&#32852;&#21512;&#31574;&#30053;&#26799;&#24230;&#30340;&#36807;&#31354;&#20013;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Over-the-air Federated Policy Gradient. (arXiv:2310.16592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#31354;&#20013;&#32852;&#21512;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#24191;&#25773;&#25658;&#24102;&#26412;&#22320;&#20449;&#24687;&#30340;&#27169;&#25311;&#20449;&#21495;&#23454;&#29616;&#26356;&#26032;&#31574;&#30053;&#21442;&#25968;&#65292;&#30740;&#31350;&#20102;&#22122;&#22768;&#21644;&#20449;&#36947;&#22833;&#30495;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#32467;&#26524;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36807;&#31354;&#20013;&#32858;&#21512;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#23398;&#20064;&#12289;&#20248;&#21270;&#21644;&#24863;&#30693;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#31354;&#20013;&#32852;&#21512;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20854;&#20013;&#25152;&#26377;&#30340;&#26234;&#33021;&#20307;&#21516;&#26102;&#21521;&#20849;&#20139;&#30340;&#26080;&#32447;&#20449;&#36947;&#24191;&#25773;&#25658;&#24102;&#26412;&#22320;&#20449;&#24687;&#30340;&#27169;&#25311;&#20449;&#21495;&#65292;&#20013;&#22830;&#25511;&#21046;&#22120;&#20351;&#29992;&#25509;&#25910;&#21040;&#30340;&#27719;&#24635;&#27874;&#24418;&#26469;&#26356;&#26032;&#31574;&#30053;&#21442;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22122;&#22768;&#21644;&#20449;&#36947;&#22833;&#30495;&#23545;&#25152;&#25552;&#20986;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#36890;&#20449;&#21644;&#37319;&#26679;&#30340;&#22797;&#26434;&#24230;&#26469;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#36817;&#20284;&#30340;&#31283;&#23450;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#20223;&#30495;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, over-the-air aggregation has been widely considered in large-scale distributed learning, optimization, and sensing. In this paper, we propose the over-the-air federated policy gradient algorithm, where all agents simultaneously broadcast an analog signal carrying local information to a common wireless channel, and a central controller uses the received aggregated waveform to update the policy parameters. We investigate the effect of noise and channel distortion on the convergence of the proposed algorithm, and establish the complexities of communication and sampling for finding an $\epsilon$-approximate stationary point. Finally, we present some simulation results to show the effectiveness of the algorithm.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#31181;&#31616;&#21333;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#34920;&#31034;&#20026;&#21521;&#37327;&#12290;&#36825;&#20123;&#20989;&#25968;&#21521;&#37327;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#36824;&#20855;&#26377;&#23558;&#35821;&#20041;&#21521;&#37327;&#36827;&#34892;&#32452;&#21512;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20989;&#25968;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Function Vectors in Large Language Models. (arXiv:2310.15213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15213
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#31181;&#31616;&#21333;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#34920;&#31034;&#20026;&#21521;&#37327;&#12290;&#36825;&#20123;&#20989;&#25968;&#21521;&#37327;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#36824;&#20855;&#26377;&#23558;&#35821;&#20041;&#21521;&#37327;&#36827;&#34892;&#32452;&#21512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#34920;&#31034;&#20026;&#33258;&#22238;&#24402;&#21464;&#25442;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#21521;&#37327;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20219;&#21153;&#19978;&#20351;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#23569;&#25968;&#27880;&#24847;&#21147;&#22836;&#20256;&#36755;&#20102;&#23637;&#31034;&#20219;&#21153;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20989;&#25968;&#21521;&#37327;&#65288;FV&#65289;&#12290;FV&#23545;&#19978;&#19979;&#25991;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21363;&#23427;&#20204;&#22312;&#19981;&#31867;&#20284;&#20110;&#20854;&#25910;&#38598;&#26102;&#30340;ICL&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#35302;&#21457;&#23545;&#36755;&#20837;&#30340;&#20219;&#21153;&#25191;&#34892;&#65292;&#20363;&#22914;&#38646;&#26679;&#26412;&#21644;&#33258;&#28982;&#25991;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#23618;&#19978;&#27979;&#35797;&#20102;FV&#65292;&#24182;&#22312;&#20013;&#23618;&#21457;&#29616;&#24378;&#22823;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;FV&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#24182;&#21457;&#29616;&#34429;&#28982;&#23427;&#20204;&#36890;&#24120;&#21253;&#21547;&#32534;&#30721;&#20989;&#25968;&#30340;&#36755;&#20986;&#31354;&#38388;&#30340;&#20449;&#24687;&#65292;&#20294;&#20165;&#27492;&#20449;&#24687;&#26080;&#27861;&#37325;&#26500;FV&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;FV&#20013;&#30340;&#35821;&#20041;&#21521;&#37327;&#32452;&#21512;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#23384;&#22312;&#32452;&#21512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some exte
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#33258;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#33258;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.14053</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#65306;&#29992;IdentityChain&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain. (arXiv:2310.14053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#33258;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#33258;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;(Code LLMs)&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#22240;&#27492;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20934;&#30830;&#24615;&#35780;&#20272;&#26041;&#27861;&#35780;&#20272;Code LLMs&#22312;&#19968;&#31995;&#21015;&#29420;&#31435;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#24573;&#35270;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#30452;&#35266;&#26469;&#35762;&#65292;&#19968;&#20010;&#21487;&#20449;&#36182;&#30340;&#27169;&#22411;&#22312;&#20026;&#20854;&#33258;&#36523;&#30340;&#20195;&#30721;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20197;&#21450;&#20026;&#20854;&#33258;&#36523;&#30340;&#35268;&#33539;&#29983;&#25104;&#20195;&#30721;&#26102;&#24212;&#35813;&#26159;&#33258;&#19968;&#33268;&#30340;&#12290;&#26410;&#33021;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#25581;&#31034;&#20102;&#23545;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20849;&#20139;&#35821;&#20041;&#30340;&#29702;&#35299;&#30340;&#19981;&#36275;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#39318;&#20808;&#27491;&#24335;&#23450;&#20041;&#20102;Code LLMs&#30340;&#33258;&#19968;&#33268;&#24615;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;IdentityChain&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#33258;&#19968;&#33268;&#24615;&#21644;&#20256;&#32479;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;11&#20010;Code LLMs&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#26410;&#33021;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indee
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26641;&#24230;&#37327;&#26377;&#22122;&#22768;&#30340;&#20248;&#21270;&#20256;&#36755;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#26641;&#32467;&#26500;&#25200;&#21160;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13653</link><description>&lt;p&gt;
&#37319;&#29992;&#26377;&#22122;&#22768;&#26641;&#24230;&#37327;&#30340;&#20248;&#21270;&#20256;&#36755;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Measures with Noisy Tree Metric. (arXiv:2310.13653v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26641;&#24230;&#37327;&#26377;&#22122;&#22768;&#30340;&#20248;&#21270;&#20256;&#36755;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#26641;&#32467;&#26500;&#25200;&#21160;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26641;&#24230;&#37327;&#31354;&#38388;&#19978;&#25903;&#25345;&#30340;&#27010;&#29575;&#27979;&#24230;&#30340;&#20248;&#21270;&#20256;&#36755;&#65288;OT&#65289;&#38382;&#39064;&#12290;&#24050;&#30693;&#36825;&#31181;OT&#38382;&#39064;&#65288;&#21363;&#26641;-&#29926;&#29926;&#26031;&#22374;&#65288;TW&#65289;&#65289;&#20855;&#26377;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#20294;&#22522;&#26412;&#19978;&#21462;&#20915;&#20110;&#36755;&#20837;&#27979;&#24230;&#25903;&#25345;&#19978;&#30340;&#24213;&#23618;&#26641;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#25805;&#20316;&#20013;&#65292;&#30001;&#20110;&#22122;&#22768;&#25110;&#23545;&#25239;&#24615;&#27979;&#37327;&#65292;&#32473;&#23450;&#30340;&#26641;&#32467;&#26500;&#21487;&#33021;&#20250;&#34987;&#25200;&#21160;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#26368;&#22823;-&#26368;&#23567;&#40065;&#26834;OT&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#22312;&#19968;&#20010;&#26641;&#24230;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#19978;&#20004;&#20010;&#36755;&#20837;&#27979;&#24230;&#20043;&#38388;&#30340;&#26368;&#22823;&#21487;&#33021;&#36317;&#31163;&#12290;&#24635;&#20307;&#19978;&#35828;&#65292;&#30001;&#20110;&#20854;&#38750;&#20984;&#24615;&#21644;&#38750;&#20809;&#28369;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#35745;&#31639;&#65292;&#21363;&#20415;&#26159;&#22312;&#25903;&#25345;&#20026;1&#32500;&#31354;&#38388;&#30340;&#27979;&#24230;&#24773;&#20917;&#19979;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#35268;&#27169;&#24773;&#26223;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#36793;&#32536;&#21024;&#38500;/&#28155;&#21152;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26641;&#24230;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36825;&#20010;&#38598;&#21512;&#22312;&#19968;&#20010;&#20248;&#38597;&#30340;&#26694;&#26550;&#19979;&#28085;&#30422;&#20102;&#22810;&#26679;&#30340;&#26641;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study optimal transport (OT) problem for probability measures supported on a tree metric space. It is known that such OT problem (i.e., tree-Wasserstein (TW)) admits a closed-form expression, but depends fundamentally on the underlying tree structure over supports of input measures. In practice, the given tree structure may be, however, perturbed due to noisy or adversarial measurements. In order to mitigate this issue, we follow the max-min robust OT approach which considers the maximal possible distances between two input measures over an uncertainty set of tree metrics. In general, this approach is hard to compute, even for measures supported in $1$-dimensional space, due to its non-convexity and non-smoothness which hinders its practical applications, especially for large-scale settings. In this work, we propose \emph{novel uncertainty sets of tree metrics} from the lens of edge deletion/addition which covers a diversity of tree structures in an elegant framework. Consequently, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#23398;&#20064;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20855;&#26377;&#29305;&#23450;&#22870;&#21169;&#21644;&#27491;&#21017;&#21270;&#22120;&#32467;&#26500;&#30340;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29983;&#25104;&#27969;&#32593;&#32476;&#35757;&#32451;&#20013;&#20855;&#26377;&#23454;&#38469;&#25928;&#29575;&#21644;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12934</link><description>&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#20316;&#20026;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks as Entropy-Regularized RL. (arXiv:2310.12934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#23398;&#20064;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20855;&#26377;&#29305;&#23450;&#22870;&#21169;&#21644;&#27491;&#21017;&#21270;&#22120;&#32467;&#26500;&#30340;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29983;&#25104;&#27969;&#32593;&#32476;&#35757;&#32451;&#20013;&#20855;&#26377;&#23454;&#38469;&#25928;&#29575;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;(GFlowNets)&#26159;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#20197;&#20415;&#26679;&#26412;&#20855;&#26377;&#19982;&#32473;&#23450;&#22870;&#21169;&#25104;&#27604;&#20363;&#30340;&#32452;&#21512;&#31163;&#25955;&#23545;&#35937;&#30340;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#21160;&#20316;&#12290; GFlowNets&#21033;&#29992;&#38382;&#39064;&#30340;&#24207;&#21015;&#24615;&#36136;&#65292;&#19982;&#24378;&#21270;&#23398;&#20064;(RL)&#36827;&#34892;&#31867;&#27604;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;RL&#21644;GFlowNets&#20043;&#38388;&#30340;&#32852;&#31995;&#25193;&#23637;&#21040;&#20102;&#19968;&#33324;&#24773;&#20917;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#23398;&#20064;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#20219;&#21153;&#39640;&#25928;&#22320;&#37325;&#26032;&#23450;&#20041;&#20026;&#20855;&#26377;&#29305;&#23450;&#22870;&#21169;&#21644;&#27491;&#21017;&#21270;&#22120;&#32467;&#26500;&#30340;&#29109;&#27491;&#21017;&#21270;RL&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#26631;&#20934;&#30340;&#36719;RL&#31639;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#27010;&#29575;&#24314;&#27169;&#20219;&#21153;&#30340;GFlowNet&#35757;&#32451;&#65292;&#26469;&#35828;&#26126;&#36825;&#31181;&#37325;&#23450;&#20041;&#30340;&#23454;&#38469;&#25928;&#29575;&#12290;&#19982;&#20808;&#21069;&#25253;&#36947;&#30340;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#34920;&#26126;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#19982;&#24050;&#26377;&#30340;GFlowNet&#35757;&#32451;&#26041;&#27861;&#31454;&#20105;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20010;&#35266;&#28857;&#20026;&#23558;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#34701;&#20837;&#23454;&#38469;&#38382;&#39064;&#25552;&#20379;&#20102;&#30452;&#25509;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed generative flow networks (GFlowNets) are a method of training a policy to sample compositional discrete objects with probabilities proportional to a given reward via a sequence of actions. GFlowNets exploit the sequential nature of the problem, drawing parallels with reinforcement learning (RL). Our work extends the connection between RL and GFlowNets to a general case. We demonstrate how the task of learning a generative flow network can be efficiently redefined as an entropy-regularized RL problem with a specific reward and regularizer structure. Furthermore, we illustrate the practical efficiency of this reformulation by applying standard soft RL algorithms to GFlowNet training across several probabilistic modeling tasks. Contrary to previously reported results, we show that entropic RL approaches can be competitive against established GFlowNet training methods. This perspective opens a direct path for integrating reinforcement learning principles into the real
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#23618;&#27425;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31232;&#30095;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#23618;&#27425;&#20135;&#21697;&#21644;/&#25110;&#26102;&#38388;&#32467;&#26500;&#65292;&#20174;&#32780;&#20026;&#25968;&#30334;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#25552;&#20379;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;M5&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;10%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.12809</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#23618;&#27425;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Forecasting at Scale. (arXiv:2310.12809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#23618;&#27425;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31232;&#30095;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#23618;&#27425;&#20135;&#21697;&#21644;/&#25110;&#26102;&#38388;&#32467;&#26500;&#65292;&#20174;&#32780;&#20026;&#25968;&#30334;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#25552;&#20379;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;M5&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;10%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#29616;&#26377;&#30340;&#23618;&#27425;&#39044;&#27979;&#25216;&#26415;&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31232;&#30095;&#25439;&#22833;&#20989;&#25968;&#26469;&#23398;&#20064;&#21315;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#33268;&#39044;&#27979;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#23618;&#27425;&#20135;&#21697;&#21644;/&#25110;&#26102;&#38388;&#32467;&#26500;&#12290;&#25105;&#20204;&#31232;&#30095;&#23618;&#27425;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#28857;&#26159;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#23454;&#36341;&#32773;&#33021;&#22815;&#20135;&#29983;&#19982;&#20219;&#20309;&#36873;&#25321;&#30340;&#27178;&#21521;&#25110;&#26102;&#38388;&#23618;&#27425;&#19968;&#33268;&#30340;&#24213;&#23618;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#28040;&#38500;&#20256;&#32479;&#23618;&#27425;&#39044;&#27979;&#25216;&#26415;&#20013;&#38656;&#35201;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#20943;&#23569;&#20102;&#39044;&#27979;&#27969;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#20844;&#24320;&#30340;M5&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#22522;&#20934;&#25439;&#22833;&#20989;&#25968;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31232;&#30095;&#23618;&#27425;&#25439;&#22833;&#20989;&#25968;&#24615;&#33021;&#25552;&#39640;&#20102;10%&#65288;RMSE&#65289;&#12290;&#25105;&#20204;&#23558;&#31232;&#30095;&#23618;&#27425;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#22312;bol&#36825;&#20010;&#22823;&#22411;&#27431;&#27954;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#29616;&#23384;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing hierarchical forecasting techniques scale poorly when the number of time series increases. We propose to learn a coherent forecast for millions of time series with a single bottom-level forecast model by using a sparse loss function that directly optimizes the hierarchical product and/or temporal structure. The benefit of our sparse hierarchical loss function is that it provides practitioners a method of producing bottom-level forecasts that are coherent to any chosen cross-sectional or temporal hierarchy. In addition, removing the need for a post-processing step as required in traditional hierarchical forecasting techniques reduces the computational cost of the prediction phase in the forecasting pipeline. On the public M5 dataset, our sparse hierarchical loss function performs up to 10% (RMSE) better compared to the baseline loss function. We implement our sparse hierarchical loss function within an existing forecasting model at bol, a large European e-commerce platform, res
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#19978;&#30340;&#26368;&#20248;&#20256;&#36755;&#26469;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11762</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
A Quasi-Wasserstein Loss for Learning Graph Neural Networks. (arXiv:2310.11762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11762
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#19978;&#30340;&#26368;&#20248;&#20256;&#36755;&#26469;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20219;&#21153;&#20013;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#29420;&#31435;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#33410;&#28857;&#30340;&#65292;&#21363;&#20351;&#33410;&#28857;&#23884;&#20837;&#21644;&#23427;&#20204;&#30340;&#26631;&#31614;&#30001;&#20110;&#22270;&#32467;&#26500;&#30340;&#23384;&#22312;&#32780;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#65288;QW&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#20511;&#21161;&#20110;&#22312;&#22270;&#19978;&#23450;&#20041;&#30340;&#26368;&#20248;&#20256;&#36755;&#65292;&#20174;&#32780;&#24341;&#23548;GNN&#30340;&#26032;&#23398;&#20064;&#21644;&#39044;&#27979;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#8220;&#20934;&#29926;&#29380;&#26031;&#22374;&#8221;&#36317;&#31163;&#65292;&#29992;&#20110;&#35266;&#27979;&#21040;&#30340;&#22810;&#32500;&#33410;&#28857;&#26631;&#31614;&#21644;&#23427;&#20204;&#30340;&#20272;&#35745;&#20043;&#38388;&#65292;&#36890;&#36807;&#20248;&#21270;&#22312;&#22270;&#36793;&#19978;&#23450;&#20041;&#30340;&#26631;&#31614;&#20256;&#36755;&#12290;&#36825;&#20123;&#20272;&#35745;&#26159;&#30001;&#19968;&#20010;GNN&#21442;&#25968;&#21270;&#30340;&#65292;&#20854;&#20013;&#26368;&#20248;&#26631;&#31614;&#20256;&#36755;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#30830;&#23450;&#22270;&#36793;&#30340;&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#26631;&#31614;&#20256;&#36755;&#30340;&#20005;&#26684;&#32422;&#26463;&#37325;&#26032;&#34920;&#36798;&#20026;&#22522;&#20110;Bregman&#25955;&#24230;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25152;&#25552;&#20986;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#65292;&#20851;&#32852;&#20004;&#20010;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#23398;&#20064;GNN&#20197;&#21450;&#26368;&#20248;&#26631;&#31614;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;&#65292;&#21457;&#29616;LLMs&#23545;&#20110;&#21508;&#31181;&#24494;&#35843;&#25968;&#25454;&#38598;&#37117;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31163;&#32676;&#29992;&#25143;&#21644;&#36129;&#29486;&#22823;&#37327;&#25968;&#25454;&#30340;&#29992;&#25143;&#12290;&#36825;&#23545;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.09266</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
User Inference Attacks on Large Language Models. (arXiv:2310.09266v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;&#65292;&#21457;&#29616;LLMs&#23545;&#20110;&#21508;&#31181;&#24494;&#35843;&#25968;&#25454;&#38598;&#37117;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31163;&#32676;&#29992;&#25143;&#21644;&#36129;&#29486;&#22823;&#37327;&#25968;&#25454;&#30340;&#29992;&#25143;&#12290;&#36825;&#23545;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23450;&#21046;&#20026;&#19987;&#19994;&#20219;&#21153;&#21644;&#24212;&#29992;&#30340;&#24120;&#35265;&#26377;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#24494;&#35843;LLMs&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#31216;&#20026;&#29992;&#25143;&#25512;&#29702;&#30340;&#29616;&#23454;&#23041;&#32961;&#27169;&#22411;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#25512;&#26029;&#20986;&#29992;&#25143;&#30340;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#20110;&#24494;&#35843;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#31181;&#23041;&#32961;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#21482;&#38656;&#35201;&#20174;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#19968;&#23567;&#32452;&#26679;&#26412;&#65288;&#21487;&#33021;&#19982;&#29992;&#20110;&#35757;&#32451;&#30340;&#26679;&#26412;&#19981;&#21516;&#65289;&#21644;&#23545;&#24494;&#35843;LLM&#30340;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#22312;&#21508;&#31181;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#26131;&#21463;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#26377;&#26102;&#25915;&#20987;&#25104;&#21151;&#29575;&#25509;&#36817;&#23436;&#32654;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21738;&#20123;&#29305;&#24615;&#20351;&#29992;&#25143;&#23481;&#26131;&#21463;&#21040;&#29992;&#25143;&#25512;&#29702;&#30340;&#25915;&#20987;&#65292;&#21457;&#29616;&#31163;&#32676;&#29992;&#25143;&#65288;&#21363;&#25968;&#25454;&#20998;&#24067;&#19982;&#20854;&#20182;&#29992;&#25143;&#26126;&#26174;&#19981;&#21516;&#65289;&#21644;&#36129;&#29486;&#22823;&#37327;&#25968;&#25454;&#30340;&#29992;&#25143;&#26356;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35299;&#20915;&#36825;&#31181;&#25915;&#20987;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is a common and effective method for tailoring large language models (LLMs) to specialized tasks and applications. In this paper, we study the privacy implications of fine-tuning LLMs on user data. To this end, we define a realistic threat model, called user inference, wherein an attacker infers whether or not a user's data was used for fine-tuning. We implement attacks for this threat model that require only a small set of samples from a user (possibly different from the samples used for training) and black-box access to the fine-tuned LLM. We find that LLMs are susceptible to user inference attacks across a variety of fine-tuning datasets, at times with near perfect attack success rates. Further, we investigate which properties make users vulnerable to user inference, finding that outlier users (i.e. those with data distributions sufficiently different from other users) and users who contribute large quantities of data are most susceptible to attack. Finally, we explore s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;&#65288;NDMs&#65289;&#65292;&#23427;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23398;&#20064;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#26080;&#38656;&#27169;&#25311;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#21464;&#20998;&#30028;&#23545;NDMs&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21487;&#23398;&#20064;&#21464;&#25442;&#30340;NDMs&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08337</link><description>&lt;p&gt;
&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Diffusion Models. (arXiv:2310.08337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;&#65288;NDMs&#65289;&#65292;&#23427;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23398;&#20064;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#26080;&#38656;&#27169;&#25311;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#21464;&#20998;&#30028;&#23545;NDMs&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21487;&#23398;&#20064;&#21464;&#25442;&#30340;NDMs&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35768;&#22810;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#21482;&#20801;&#35768;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#32447;&#24615;&#36716;&#25442;&#65292;&#21463;&#21040;&#20102;&#19968;&#23450;&#30340;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26356;&#24191;&#27867;&#30340;&#21464;&#25442;&#23478;&#26063;&#21487;&#33021;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;&#29983;&#25104;&#20998;&#24067;&#65292;&#31616;&#21270;&#36870;&#36807;&#31243;&#24182;&#32553;&#23567;&#30495;&#23454;&#36127;&#23545;&#25968;&#20284;&#28982;&#21644;&#21464;&#20998;&#36817;&#20284;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;&#65288;NDMs&#65289;&#65292;&#23427;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23398;&#20064;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#26080;&#38656;&#27169;&#25311;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#21464;&#20998;&#30028;&#23545;NDMs&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;NDMs&#30340;&#26102;&#38388;&#36830;&#32493;&#24418;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#25104;&#30340;&#25968;&#20540;ODE&#21644;SDE&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#24555;&#36895;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#21487;&#23398;&#20064;&#21464;&#25442;&#30340;NDMs&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#26377;&#25928;&#35843;&#33410;&#20102;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06286</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#34892;&#20026;&#25233;&#21046;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Suppressing Overestimation in Q-Learning through Adversarial Behaviors. (arXiv:2310.06286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#26377;&#25928;&#35843;&#33410;&#20102;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#31216;&#20026;&#34394;&#25311;&#23545;&#25239;&#24615;Q&#23398;&#20064;&#65288;DAQ&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#35843;&#33410;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;&#36890;&#36807;&#34394;&#25311;&#29609;&#23478;&#65292;&#23398;&#20064;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#19968;&#20010;&#21452;&#20154;&#38646;&#21644;&#21338;&#24328;&#12290;&#25152;&#25552;&#20986;&#30340;DAQ&#23558;&#20960;&#31181;Q&#23398;&#20064;&#30340;&#21464;&#20307;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#25511;&#21046;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#20363;&#22914;maxmin Q&#23398;&#20064;&#21644;minmax Q&#23398;&#20064;&#65288;&#26412;&#25991;&#25552;&#20986;&#65289;&#12290;&#36890;&#36807;&#34394;&#25311;&#23545;&#25239;&#24615;&#34892;&#20026;&#65292;&#25152;&#25552;&#20986;&#30340;DAQ&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#29616;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#35843;&#25972;&#23545;&#25239;&#24615;Q&#23398;&#20064;&#65292;&#20174;&#32508;&#21512;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;DAQ&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#23454;&#35777;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;DAQ&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias thourgh dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#21435;&#38500;LLMs&#20013;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#30340;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#26469;&#20256;&#36798;&#20854;&#20449;&#24565;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.06272</link><description>&lt;p&gt;
&#35753;&#27169;&#22411;&#35828;&#23494;&#25991;: &#36890;&#36807;&#23884;&#20837;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;
&lt;/p&gt;
&lt;p&gt;
Let Models Speak Ciphers: Multiagent Debate through Embeddings. (arXiv:2310.06272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#21435;&#38500;LLMs&#20013;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#30340;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#26469;&#20256;&#36798;&#20854;&#20449;&#24565;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#30340;&#35752;&#35770;&#21644;&#36777;&#35770;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#28508;&#21147;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#30001;&#20110;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#32780;&#25104;&#20026;&#26126;&#26174;&#30340;&#20132;&#27969;&#36873;&#25321;&#65292;&#20294;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26102;&#38656;&#35201;&#36827;&#34892;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#21487;&#33021;&#23384;&#22312;&#20449;&#24687;&#20002;&#22833;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#22240;&#20026;&#23427;&#20165;&#20351;&#29992;&#19968;&#20010;&#26631;&#35760;&#26469;&#20195;&#34920;&#27169;&#22411;&#22312;&#25972;&#20010;&#35789;&#27719;&#34920;&#20013;&#30340;&#20449;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#65288;&#36890;&#36807;&#23884;&#20837;&#34920;&#31034;&#36827;&#34892;&#20132;&#27969;&#30340;&#32593;&#32476;&#27169;&#22411;&#21327;&#35758;&#65289;&#30340;&#36890;&#20449;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;LLMs&#20013;&#21435;&#38500;&#20102;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#23427;&#20204;&#36890;&#36807;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#30340;&#26399;&#26395;&#26469;&#20256;&#36798;&#23427;&#20204;&#30340;&#20449;&#24565;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#20559;&#31163;&#33258;&#28982;&#35821;&#35328;&#65292;CIPHER&#22312;&#19981;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights. While the state-of-the-a
&lt;/p&gt;</description></item><item><title>GeoLLM&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#26469;&#33258;OpenStreetMap&#30340;&#36741;&#21161;&#22320;&#22270;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#27979;&#37327;&#20154;&#21475;&#23494;&#24230;&#21644;&#32463;&#27982;&#29983;&#35745;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.06213</link><description>&lt;p&gt;
GeoLLM: &#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
GeoLLM: Extracting Geospatial Knowledge from Large Language Models. (arXiv:2310.06213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06213
&lt;/p&gt;
&lt;p&gt;
GeoLLM&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#26469;&#33258;OpenStreetMap&#30340;&#36741;&#21161;&#22320;&#22270;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#27979;&#37327;&#20154;&#21475;&#23494;&#24230;&#21644;&#32463;&#27982;&#29983;&#35745;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#31181;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#24120;&#24120;&#20381;&#36182;&#20110;&#20840;&#29699;&#33539;&#22260;&#21487;&#29992;&#30340;&#21355;&#26143;&#22270;&#20687;&#31561;&#39044;&#27979;&#21464;&#37327;&#65292;&#36825;&#21487;&#33021;&#35201;&#20040;&#24456;&#26114;&#36149;&#65292;&#35201;&#20040;&#32570;&#20047;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#20114;&#32852;&#32593;&#35821;&#35328;&#35821;&#26009;&#24211;&#20013;&#21253;&#21547;&#30340;&#22823;&#37327;&#30693;&#35782;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;LLM&#20013;&#23884;&#20837;&#20102;&#26377;&#20851;&#20301;&#32622;&#30340;&#26174;&#33879;&#31354;&#38388;&#20449;&#24687;&#65292;&#20294;&#20165;&#20351;&#29992;&#22320;&#29702;&#22352;&#26631;&#26469;&#26597;&#35810;LLM&#23545;&#20110;&#39044;&#27979;&#20154;&#21475;&#23494;&#24230;&#31561;&#20851;&#38190;&#25351;&#26631;&#26159;&#26080;&#25928;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;LLM&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#26469;&#33258;OpenStreetMap&#30340;&#36741;&#21161;&#22320;&#22270;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22269;&#38469;&#31038;&#21306;&#20851;&#24515;&#30340;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#20154;&#21475;&#23494;&#24230;&#21644;&#32463;&#27982;&#29983;&#35745;&#30340;&#27979;&#37327;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-GNN&#12290;&#23427;&#21033;&#29992;LLMs&#23545;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;GNN&#33021;&#22815;&#23545;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04668</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMS&#65289;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Label-free Node Classification on Graphs with Large Language Models (LLMS). (arXiv:2310.04668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-GNN&#12290;&#23427;&#21033;&#29992;LLMs&#23545;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;GNN&#33021;&#22815;&#23545;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Graph Neural Networks&#65292;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#30830;&#20445;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#22312;&#25991;&#26412;&#23646;&#24615;&#22270;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#39640;&#25928;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#25512;&#29702;&#25104;&#26412;&#36739;&#39640;&#12290;&#37492;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26080;&#26631;&#31614;&#22270;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;LLM-GNN&#12290;&#23427;&#38598;&#25104;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLMs&#34987;&#29992;&#26469;&#27880;&#37322;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;GNNs&#33021;&#22815;&#39044;&#27979;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#12290;LLM-GNN&#30340;&#23454;&#29616;&#38754;&#20020;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#25105;&#20204;&#22914;&#20309;&#20027;&#21160;&#36873;&#25321;&#35201;&#30001;LLMs&#27880;&#37322;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#22686;&#24378;GNN&#30340;&#35757;&#32451;&#65311;&#25105;&#20204;&#22914;&#20309;&#21033;&#29992;LLMs&#26469;&#20248;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22788;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to ob
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.04395</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25552;&#39640;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;$\theta$&#21644;&#25968;&#25454;$y$&#30340;&#27010;&#29575;&#32852;&#21512;&#27169;&#22411;$p(\theta, y)$&#20013;&#30340;&#36890;&#29992;&#23545;&#31216;&#24615;&#65292;&#25913;&#36827;&#20102;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#65288;ABI&#65289;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#31616;&#35328;&#20043;&#65292;&#25105;&#20204;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#65292;&#24182;&#22522;&#20110;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#12290;&#22312;&#23436;&#32654;&#36817;&#20284;&#24773;&#20917;&#19979;&#65292;&#36793;&#38469;&#20284;&#28982;&#22312;&#25152;&#26377;&#21442;&#25968;&#20540;&#19978;&#37117;&#26159;&#24120;&#25968;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#35823;&#24046;&#23548;&#33268;&#19981;&#21516;&#21442;&#25968;&#20540;&#30340;&#36793;&#38469;&#20284;&#28982;&#20272;&#35745;&#20013;&#23384;&#22312;&#19981;&#21487;&#21462;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23545;&#31216;&#24615;&#30340;&#36829;&#21453;&#24418;&#24335;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#26174;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#20284;&#28982;&#65289;&#30340;&#21452;&#23792;&#29609;&#20855;&#38382;&#39064;&#21644;&#20855;&#26377;&#38544;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#27169;&#25311;&#65289;&#30340;&#29616;&#23454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSTI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#24615;&#24182;&#23558;Transformer&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#29256;&#26412;&#26469;&#26497;&#22823;&#22320;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;Transformer&#27169;&#22411;&#65292;&#24182;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;</title><link>http://arxiv.org/abs/2310.04361</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#25512;&#29702;&#26469;&#21033;&#29992;Transformer&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploiting Transformer Activation Sparsity with Dynamic Inference. (arXiv:2310.04361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSTI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#24615;&#24182;&#23558;Transformer&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#29256;&#26412;&#26469;&#26497;&#22823;&#22320;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;Transformer&#27169;&#22411;&#65292;&#24182;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#23613;&#31649;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#24120;&#38754;&#20020;&#23454;&#38469;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#26174;&#33879;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#34920;&#26126;&#23384;&#22312;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21160;&#24577;&#31232;&#30095;&#21270;Transformer&#25512;&#29702;&#65288;DSTI&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#24615;&#24182;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#20854;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#29256;&#26412;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#38477;&#20302;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#35757;&#32451;&#20986;&#25104;&#21151;&#39044;&#27979;&#27599;&#20010;&#19987;&#23478;&#30456;&#23545;&#36129;&#29486;&#30340;&#23567;&#22411;&#38376;&#25511;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#30830;&#23450;&#27599;&#20010;&#20196;&#29260;&#25191;&#34892;&#30340;&#19987;&#23478;&#25968;&#37327;&#30340;&#26426;&#21046;&#12290;DSTI&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;&#38024;&#23545;BERT-base&#20998;&#31867;&#27169;&#22411;&#65292;&#25105;&#20204;&#38477;&#20302;&#20102;&#25512;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models, despite their impressive performance, often face practical limitations due to their high computational requirements. At the same time, previous studies have revealed significant activation sparsity in these models, indicating the presence of redundant computations. In this paper, we propose Dynamic Sparsified Transformer Inference (DSTI), a method that radically reduces the inference cost of Transformer models by enforcing activation sparsity and subsequently transforming a dense model into its sparse Mixture of Experts (MoE) version. We demonstrate that it is possible to train small gating networks that successfully predict the relative contribution of each expert during inference. Furthermore, we introduce a mechanism that dynamically determines the number of executed experts individually for each token. DSTI can be applied to any Transformer-based architecture and has negligible impact on the accuracy. For the BERT-base classification model, we reduce inference c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2310.03986</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#65292;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation. (arXiv:2310.03986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03986
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#26469;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#19968;&#20123;&#30456;&#20851;&#30340;&#27169;&#24577;&#20013;&#35266;&#23519;&#21040;&#65292;&#22914;&#26524;&#22312;&#27979;&#35797;&#26102;&#38388;&#32570;&#23569;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#31616;&#21333;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#31243;&#24207;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#26469;&#34917;&#20607;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#36866;&#24212;&#21487;&#20197;&#37096;&#20998;&#24357;&#34917;&#30001;&#20110;&#32570;&#22833;&#27169;&#24577;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#38024;&#23545;&#21487;&#29992;&#27169;&#24577;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#29420;&#31435;&#30340;&#12289;&#19987;&#38376;&#30340;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#36866;&#24212;&#25152;&#38656;&#30340;&#21442;&#25968;&#38750;&#24120;&#23569;&#65288;&#20363;&#22914;&#65292;&#23569;&#20110;&#65289;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities. However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose simple and parameter-efficient adaptation procedures for pretrained multimodal networks. In particular, we exploit low-rank adaptation and modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Ground-A-Video &#30340;&#22522;&#20110;&#24341;&#23548;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#23646;&#24615;&#35270;&#39057;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36755;&#20837;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#30340;&#22810;&#23646;&#24615;&#32534;&#36753;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#20854;&#20182;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01107</link><description>&lt;p&gt;
Ground-A-Video: &#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models. (arXiv:2310.01107v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Ground-A-Video &#30340;&#22522;&#20110;&#24341;&#23548;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#23646;&#24615;&#35270;&#39057;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36755;&#20837;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#30340;&#22810;&#23646;&#24615;&#32534;&#36753;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#20854;&#20182;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#39057;&#32534;&#36753;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#26524;&#65292;&#23454;&#29616;&#20102;&#21333;&#23646;&#24615;&#32534;&#36753;&#25110;&#39118;&#26684;&#20256;&#36882;&#30340;&#20219;&#21153;&#65292;&#19981;&#35770;&#36890;&#36807;&#22312;&#25991;&#26412;-&#35270;&#39057;&#25968;&#25454;&#19978;&#35757;&#32451;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#27169;&#22411;&#36824;&#26159;&#37319;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#22810;&#23646;&#24615;&#32534;&#36753;&#24773;&#26223;&#30340;&#22797;&#26434;&#24615;&#26102;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#27604;&#22914;&#24573;&#30053;&#25110;&#24573;&#35270;&#25152;&#26399;&#26395;&#30340;&#23646;&#24615;&#21464;&#21270;&#65292;&#20462;&#25913;&#36755;&#20837;&#35270;&#39057;&#30340;&#38169;&#35823;&#20803;&#32032;&#65292;&#20197;&#21450;&#26080;&#27861;&#20445;&#30041;&#24212;&#35813;&#20445;&#25345;&#21407;&#26679;&#30340;&#36755;&#20837;&#35270;&#39057;&#21306;&#22495;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24341;&#23548;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26694;&#26550;&#65292;&#21517;&#20026; Ground-A-Video&#65292;&#29992;&#20110;&#22810;&#23646;&#24615;&#35270;&#39057;&#32534;&#36753;&#12290;Ground-A-Video&#20197;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#36755;&#20837;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#30340;&#22810;&#23646;&#24615;&#32534;&#36753;&#65292;&#24182;&#19988;&#27809;&#26377;&#19978;&#36848;&#32570;&#28857;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#24341;&#20837;&#20102;&#20132;&#21449;&#24103;&#38376;&#25511;&#27880;&#24847;&#21147;&#65292;&#20197;&#19968;&#31181;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#26041;&#24335;&#23558;&#23450;&#20301;&#20449;&#24687;&#34701;&#20837;&#21040;&#28508;&#22312;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent endeavors in video editing have showcased promising results in single-attribute editing or style transfer tasks, either by training text-to-video (T2V) models on text-video data or adopting training-free methods. However, when confronted with the complexities of multi-attribute editing scenarios, they exhibit shortcomings such as omitting or overlooking intended attribute changes, modifying the wrong elements of the input video, and failing to preserve regions of the input video that should remain intact. To address this, here we present a novel grounding-guided video-to-video translation framework called Ground-A-Video for multi-attribute video editing. Ground-A-Video attains temporally consistent multi-attribute editing of input videos in a training-free manner without aforementioned shortcomings. Central to our method is the introduction of Cross-Frame Gated Attention which incorporates groundings information into the latent representations in a temporally consistent fashion,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#29087;&#24713;&#29305;&#24449;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25429;&#25417;&#26032;&#39062;&#29305;&#24449;&#26469;&#36991;&#20813;&#20551;&#38452;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#24322;&#24120;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#32972;&#26223;&#27169;&#22411;&#21644;&#23494;&#38598;&#21305;&#37197;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.00797</link><description>&lt;p&gt;
&#36229;&#36234;&#29087;&#24713;&#29305;&#24449;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Going Beyond Familiar Features for Deep Anomaly Detection. (arXiv:2310.00797v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#29087;&#24713;&#29305;&#24449;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25429;&#25417;&#26032;&#39062;&#29305;&#24449;&#26469;&#36991;&#20813;&#20551;&#38452;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#24322;&#24120;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#32972;&#26223;&#27169;&#22411;&#21644;&#23494;&#38598;&#21305;&#37197;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#28041;&#21450;&#35782;&#21035;&#19981;&#31526;&#21512;&#24050;&#23398;&#20064;&#30340;&#27491;&#24120;&#27169;&#22411;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#20043;&#21069;&#30340;&#28145;&#24230;AD&#24037;&#20316;&#20027;&#35201;&#22522;&#20110;&#29087;&#24713;&#24615;&#20551;&#35774;&#65292;&#22312;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;&#29087;&#24713;&#29305;&#24449;&#20316;&#20026;&#21442;&#32771;&#12290;&#34429;&#28982;&#36825;&#31181;&#31574;&#30053;&#24050;&#34987;&#35777;&#26126;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#23454;&#38469;&#19978;&#65292;&#22312;&#24322;&#24120;&#21253;&#21547;&#26410;&#34987;&#39044;&#35757;&#32451;&#32534;&#30721;&#24456;&#22909;&#25429;&#25417;&#21040;&#30340;&#20840;&#26032;&#29305;&#24449;&#26102;&#65292;&#23427;&#20250;&#23548;&#33268;&#25345;&#32493;&#30340;&#20551;&#38452;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;AD&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25429;&#25417;&#26032;&#39062;&#29305;&#24449;&#20316;&#20026;&#26410;&#35299;&#37322;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#36890;&#36807;&#22312;&#28151;&#21512;&#26041;&#27861;&#20013;&#32467;&#21512;&#30456;&#20284;&#24615;&#21644;&#26032;&#39062;&#24615;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#24322;&#24120;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#24322;&#24120;&#31867;&#22411;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#32972;&#26223;&#27169;&#22411;&#21644;&#23494;&#38598;&#21305;&#37197;&#30340;&#38656;&#27714;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#32771;&#34385;&#26032;&#39062;&#29305;&#24449;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#30340;&#20551;&#38452;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection (AD) is a critical task that involves identifying observations that do not conform to a learned model of normality. Prior work in deep AD is predominantly based on a familiarity hypothesis, where familiar features serve as the reference in a pre-trained embedding space. While this strategy has proven highly successful, it turns out that it causes consistent false negatives when anomalies consist of truly novel features that are not well captured by the pre-trained encoding. We propose a novel approach to AD using explainability to capture novel features as unexplained observations in the input space. We achieve strong performance across a wide range of anomaly benchmarks by combining similarity and novelty in a hybrid approach. Our approach establishes a new state-of-the-art across multiple benchmarks, handling diverse anomaly types while eliminating the need for expensive background models and dense matching. In particular, we show that by taking account of novel fea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19981;&#19968;&#23450;&#38656;&#35201;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.00771</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pre-training with Synthetic Data Helps Offline Reinforcement Learning. (arXiv:2310.00771v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19981;&#19968;&#23450;&#38656;&#35201;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;Decision Transformer&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#26159;&#21542;&#21482;&#33021;&#36890;&#36807;&#35821;&#35328;&#39044;&#35757;&#32451;&#23454;&#29616;&#65292;&#36824;&#26159;&#21487;&#20197;&#36890;&#36807;&#19981;&#28041;&#21450;&#35821;&#35328;&#30340;&#26356;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#35821;&#35328;&#23545;&#20110;&#25913;&#21892;&#24615;&#33021;&#24182;&#19981;&#26159;&#24517;&#35201;&#30340;&#65292;&#23454;&#38469;&#19978;&#65292;&#20351;&#29992;&#21512;&#25104;&#30340;IID&#25968;&#25454;&#36827;&#34892;&#23569;&#37327;&#26356;&#26032;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#19982;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#25552;&#21319;&#65307;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#23454;&#39564;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#39044;&#35757;&#32451;Conservative Q-Learning(CQL)&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;Q-learning&#65292;&#24182;&#36890;&#24120;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#39592;&#24178;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#22312;CQL&#31639;&#27861;&#20013;&#21462;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with sim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20445;&#24207;GFlowNets&#65288;OP-GFNs&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#19982;&#20505;&#36873;&#32773;&#30340;&#25490;&#24207;&#30456;&#19968;&#33268;&#30340;&#27010;&#29575;&#36827;&#34892;&#37319;&#26679;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#39044;&#23450;&#20041;&#26631;&#37327;&#22870;&#21169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#35777;&#26126;&#35757;&#32451;&#36807;&#31243;&#31232;&#30095;&#22870;&#21169;&#26223;&#35266;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.00386</link><description>&lt;p&gt;
&#20445;&#24207;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Order-Preserving GFlowNets. (arXiv:2310.00386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20445;&#24207;GFlowNets&#65288;OP-GFNs&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#19982;&#20505;&#36873;&#32773;&#30340;&#25490;&#24207;&#30456;&#19968;&#33268;&#30340;&#27010;&#29575;&#36827;&#34892;&#37319;&#26679;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#39044;&#23450;&#20041;&#26631;&#37327;&#22870;&#21169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#35777;&#26126;&#35757;&#32451;&#36807;&#31243;&#31232;&#30095;&#22870;&#21169;&#26223;&#35266;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#34987;&#24341;&#20837;&#20316;&#20026;&#19968;&#31181;&#26681;&#25454;&#32473;&#23450;&#22870;&#21169;&#27010;&#29575;&#37319;&#26679;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#38598;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;GFlowNets&#21482;&#33021;&#19982;&#39044;&#23450;&#20041;&#30340;&#26631;&#37327;&#22870;&#21169;&#19968;&#36215;&#20351;&#29992;&#65292;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20219;&#21153;&#20013;&#65292;&#36825;&#21487;&#33021;&#26159;&#35745;&#31639;&#26114;&#36149;&#30340;&#25110;&#32773;&#30452;&#25509;&#19981;&#21487;&#35775;&#38382;&#30340;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20248;&#20808;&#35782;&#21035;&#39640;&#22870;&#21169;&#20505;&#36873;&#32773;&#65292;&#20256;&#32479;&#20570;&#27861;&#26159;&#23558;&#22870;&#21169;&#25552;&#39640;&#21040;&#26356;&#39640;&#30340;&#25351;&#25968;&#65292;&#32780;&#36825;&#20010;&#26368;&#20248;&#36873;&#25321;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#24207;GFlowNets&#65288;OP-GFNs&#65289;&#65292;&#23427;&#20204;&#20197;&#19982;&#25552;&#20379;&#30340;&#65288;&#37096;&#20998;&#65289;&#20505;&#36873;&#32773;&#25490;&#24207;&#19968;&#33268;&#30340;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#27010;&#29575;&#36827;&#34892;&#37319;&#26679;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#22870;&#21169;&#20989;&#25968;&#30340;&#26174;&#24335;&#34920;&#36798;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;OP-GFNs&#30340;&#35757;&#32451;&#36807;&#31243;&#36880;&#28176;&#31232;&#30095;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#26223;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates with probabilities proportional to a given reward. However, GFlowNets can only be used with a predefined scalar reward, which can be either computationally expensive or not directly accessible, in the case of multi-objective optimization (MOO) tasks for example. Moreover, to prioritize identifying high-reward candidates, the conventional practice is to raise the reward to a higher exponent, the optimal choice of which may vary across different environments. To address these issues, we propose Order-Preserving GFlowNets (OP-GFNs), which sample with probabilities in proportion to a learned reward function that is consistent with a provided (partial) order on the candidates, thus eliminating the need for an explicit formulation of the reward function. We theoretically prove that the training process of OP-GFNs gradually sparsifies the learned reward landscape in single-objective max
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;</title><link>http://arxiv.org/abs/2309.17264</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#19968;&#33324;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#25551;&#32472;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#25110;&#30149;&#29702;&#32467;&#26500;&#65292;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26500;&#24314;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#20998;&#21106;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#27880;&#37322;&#38750;&#24120;&#32321;&#29712;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#23398;&#35270;&#39057;&#25110;3D&#20307;&#31215;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#24046;&#30340;&#24103;&#38388;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#65292;&#19968;&#20010;&#21517;&#20026;Moving Object Segmentation (MOS)&#30340;&#22522;&#26412;&#20219;&#21153;&#22312;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23427;&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#20174;&#32972;&#26223;&#20013;&#25551;&#32472;&#31227;&#21160;&#29289;&#20307;&#65292;&#21482;&#38656;&#35201;&#26368;&#23567;&#30340;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;MOS&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21517;&#20026;iMOS&#12290;&#23545;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;iMOS&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#38656;&#23545;&#24207;&#21015;&#20013;&#23569;&#37327;&#30340;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;iMOS&#23601;&#21487;&#20197;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
&lt;/p&gt;</description></item><item><title>Transformer-VQ&#26159;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;Transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#35745;&#31639;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.16354</link><description>&lt;p&gt;
Transformer-VQ: &#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Transformer-VQ: Linear-Time Transformers via Vector Quantization. (arXiv:2309.16354v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16354
&lt;/p&gt;
&lt;p&gt;
Transformer-VQ&#26159;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;Transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#35745;&#31639;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Transformer-VQ&#65292;&#19968;&#31181;&#20165;&#32534;&#30721;&#22120;&#30340;Transformer&#65292;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#35745;&#31639;&#22522;&#20110;softmax&#30340;&#23494;&#38598;&#33258;&#27880;&#24847;&#21147;&#12290;Transformer-VQ&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#26159;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#38190;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#32531;&#23384;&#26426;&#21046;&#23454;&#29616;&#30340;&#12290;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#65292;Transformer-VQ&#22312;&#36136;&#37327;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;Enwik8(0.99 bpb)&#65292;PG-19(26.6 ppl)&#21644;ImageNet64(3.16 bpb)&#37117;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#21069;K&#31232;&#30095;softmax&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#22312;&#23494;&#24230;&#21644;&#21442;&#25968;&#20272;&#35745;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#23450;&#20041;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25506;&#35752;&#20102;&#36755;&#20837;&#21306;&#22495;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#30495;&#23454;&#19987;&#23478;&#25968;&#37327;&#24050;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#23494;&#24230;&#21644;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#26679;&#26412;&#37327;&#25104;&#27491;&#27604;&#65292;&#20294;&#24403;&#30495;&#23454;&#27169;&#24335;&#26410;&#30693;&#26102;</title><link>http://arxiv.org/abs/2309.13850</link><description>&lt;p&gt;
&#32479;&#35745;&#35282;&#24230;&#19979;&#30340;&#21069;K&#31232;&#30095;Softmax&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts. (arXiv:2309.13850v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#21069;K&#31232;&#30095;softmax&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#22312;&#23494;&#24230;&#21644;&#21442;&#25968;&#20272;&#35745;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#23450;&#20041;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25506;&#35752;&#20102;&#36755;&#20837;&#21306;&#22495;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#30495;&#23454;&#19987;&#23478;&#25968;&#37327;&#24050;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#23494;&#24230;&#21644;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#26679;&#26412;&#37327;&#25104;&#27491;&#27604;&#65292;&#20294;&#24403;&#30495;&#23454;&#27169;&#24335;&#26410;&#30693;&#26102;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;K&#31232;&#30095;softmax&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#34987;&#24191;&#27867;&#29992;&#20110;&#22312;&#19981;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#23613;&#31649;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#23545;&#35813;&#38376;&#25511;&#20989;&#25968;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#20110;&#21069;K&#31232;&#30095;softmax&#38376;&#25511;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#23427;&#23558;&#36755;&#20837;&#31354;&#38388;&#21010;&#20998;&#20026;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#30340;&#22810;&#20010;&#21306;&#22495;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#65292;&#25105;&#20204;&#23545;&#21069;K&#31232;&#30095;softmax&#38376;&#25511;&#20989;&#25968;&#23545;&#23494;&#24230;&#21644;&#21442;&#25968;&#20272;&#35745;&#30340;&#24433;&#21709;&#24314;&#31435;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#36182;&#20110;&#23450;&#20041;&#21442;&#25968;&#20043;&#38388;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25429;&#25417;&#36755;&#20837;&#21306;&#22495;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;&#24403;&#30495;&#23454;&#19987;&#23478;&#25968;&#37327;$k_{\ast}$&#24050;&#30693;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23494;&#24230;&#21644;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#37117;&#19982;&#26679;&#26412;&#37327;&#25104;&#27491;&#27604;&#12290;&#28982;&#32780;&#65292;&#24403;$k_{\ast}$&#21464;&#20026;&#26410;&#30693;&#19988;&#30495;&#23454;&#27169;&#24335;&#26102;
&lt;/p&gt;
&lt;p&gt;
Top-K sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. Despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem. The main challenge comes from the structure of the top-K sparse softmax gating function, which partitions the input space into multiple regions with distinct behaviors. By focusing on a Gaussian mixture of experts, we establish theoretical results on the effects of the top-K sparse softmax gating function on both density and parameter estimations. Our results hinge upon defining novel loss functions among parameters to capture different behaviors of the input regions. When the true number of experts $k_{\ast}$ is known, we demonstrate that the convergence rates of density and parameter estimations are both parametric on the sample size. However, when $k_{\ast}$ becomes unknown and the true mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#21307;&#30103;&#20915;&#31574;&#20013;&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#35774;&#35745;&#35201;&#27714;&#65292;&#20197;&#33043;&#27602;&#30151;&#35786;&#26029;&#20026;&#20363;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#25903;&#25345;&#20020;&#24202;&#19987;&#23478;&#22312;&#20915;&#31574;&#36807;&#31243;&#30340;&#20013;&#38388;&#38454;&#27573;&#21457;&#25381;&#20316;&#29992;&#65288;&#22914;&#29983;&#25104;&#20551;&#35774;&#25110;&#25910;&#38598;&#25968;&#25454;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.12368</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#21307;&#30103;&#20915;&#31574;&#20013;&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#65306;&#20197;&#33043;&#27602;&#30151;&#35786;&#26029;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis. (arXiv:2309.12368v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#21307;&#30103;&#20915;&#31574;&#20013;&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#35774;&#35745;&#35201;&#27714;&#65292;&#20197;&#33043;&#27602;&#30151;&#35786;&#26029;&#20026;&#20363;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#25903;&#25345;&#20020;&#24202;&#19987;&#23478;&#22312;&#20915;&#31574;&#36807;&#31243;&#30340;&#20013;&#38388;&#38454;&#27573;&#21457;&#25381;&#20316;&#29992;&#65288;&#22914;&#29983;&#25104;&#20551;&#35774;&#25110;&#25910;&#38598;&#25968;&#25454;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#30740;&#31350;&#35770;&#25991;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#21364;&#38754;&#20020;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#33043;&#27602;&#30151;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#26089;&#26399;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#35786;&#26029;&#30340;&#24613;&#24615;&#33268;&#21629;&#20840;&#36523;&#24615;&#24863;&#26579;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#33021;&#22815;&#25903;&#25345;&#20020;&#24202;&#19987;&#23478;&#20570;&#20986;&#26356;&#22909;&#33043;&#27602;&#30151;&#26089;&#26399;&#35786;&#26029;&#20915;&#31574;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;&#30740;&#31350;&#20174;&#19968;&#20010;&#24418;&#25104;&#24615;&#30740;&#31350;&#24320;&#22987;&#65292;&#35843;&#26597;&#20026;&#20160;&#20040;&#20020;&#24202;&#19987;&#23478;&#22312;&#30005;&#23376;&#30149;&#21382;&#31995;&#32479;&#20013;&#25918;&#24323;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#33043;&#27602;&#30151;&#39044;&#27979;&#27169;&#22359;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#22312;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#30340;&#20013;&#38388;&#38454;&#27573;&#65288;&#22914;&#29983;&#25104;&#20551;&#35774;&#25110;&#25910;&#38598;&#25968;&#25454;&#65289;&#25903;&#25345;&#20154;&#31867;&#19987;&#23478;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26500;&#24314;&#20102;SepsisLab&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today's AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21367;&#31215;&#28145;&#24230;&#26680;&#26426;&#22120;&#30340;&#26032;&#22411;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32431;&#31929;&#20351;&#29992;&#26680;&#32780;&#19981;&#20351;&#29992;&#29305;&#24449;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#36328;&#22495;&#35825;&#23548;&#28857;&#36817;&#20284;&#26041;&#26696;&#21644;&#22810;&#31181;&#27169;&#22411;&#21464;&#20307;&#30340;&#35774;&#35745;&#65292;&#36798;&#21040;&#20102;&#22312;MNIST&#12289;CIFAR-10&#21644;CIFAR-100&#19978;&#25509;&#36817;&#29978;&#33267;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.09814</link><description>&lt;p&gt;
&#21367;&#31215;&#28145;&#24230;&#26680;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Convolutional Deep Kernel Machines. (arXiv:2309.09814v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09814
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21367;&#31215;&#28145;&#24230;&#26680;&#26426;&#22120;&#30340;&#26032;&#22411;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32431;&#31929;&#20351;&#29992;&#26680;&#32780;&#19981;&#20351;&#29992;&#29305;&#24449;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#36328;&#22495;&#35825;&#23548;&#28857;&#36817;&#20284;&#26041;&#26696;&#21644;&#22810;&#31181;&#27169;&#22411;&#21464;&#20307;&#30340;&#35774;&#35745;&#65292;&#36798;&#21040;&#20102;&#22312;MNIST&#12289;CIFAR-10&#21644;CIFAR-100&#19978;&#25509;&#36817;&#29978;&#33267;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26680;&#26426;&#22120;(DKMs)&#26159;&#19968;&#31181;&#26368;&#36817;&#24341;&#20837;&#30340;&#20855;&#26377;&#20854;&#20182;&#28145;&#24230;&#27169;&#22411;&#28789;&#27963;&#24615;&#30340;&#26680;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#12290;DKMs&#32431;&#31929;&#20351;&#29992;&#26680;&#65292;&#32780;&#19981;&#20351;&#29992;&#29305;&#24449;&#65292;&#22240;&#27492;&#19982;&#20854;&#20182;&#26041;&#27861;&#65288;&#20174;&#31070;&#32463;&#32593;&#32476;&#21040;&#28145;&#24230;&#26680;&#23398;&#20064;&#29978;&#33267;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#65289;&#19981;&#21516;&#65292;&#21518;&#32773;&#37117;&#20351;&#29992;&#29305;&#24449;&#20316;&#20026;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21367;&#31215;DKMs&#65292;&#24182;&#37197;&#20197;&#19968;&#31181;&#39640;&#25928;&#30340;&#36328;&#22495;&#35825;&#23548;&#28857;&#36817;&#20284;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#24182;&#23454;&#39564;&#35780;&#20272;&#20102;&#35768;&#22810;&#27169;&#22411;&#21464;&#20307;&#65292;&#21253;&#25324;9&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20026;&#21367;&#31215;DKMs&#35774;&#35745;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#20004;&#31181;&#20284;&#28982;&#20989;&#25968;&#21644;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#39030;&#23618;&#12290;&#23613;&#31649;&#21482;&#22312;&#32422;28&#20010;GPU&#23567;&#26102;&#20869;&#35757;&#32451;&#65288;&#27604;&#23436;&#20840;&#30340;NNGP / NTK / Myrtle kernel&#24555;1-2&#20010;&#25968;&#37327;&#32423;&#65289;&#65292;&#20294;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;MNIST&#19978;&#23454;&#29616;&#20102;&#32422;99&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#65292;&#22312;CIFAR-10&#19978;&#20026;92&#65285;&#65292;&#22312;CIFAR-100&#19978;&#20026;71&#65285;&#65292;&#21516;&#26102;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep kernel machines (DKMs) are a recently introduced kernel method with the flexibility of other deep models including deep NNs and deep Gaussian processes. DKMs work purely with kernels, never with features, and are therefore different from other methods ranging from NNs to deep kernel learning and even deep Gaussian processes, which all use features as a fundamental component. Here, we introduce convolutional DKMs, along with an efficient inter-domain inducing point approximation scheme. Further, we develop and experimentally assess a number of model variants, including 9 different types of normalisation designed for the convolutional DKMs, two likelihoods, and two different types of top-layer. The resulting models achieve around 99% test accuracy on MNIST, 92% on CIFAR-10 and 71% on CIFAR-100, despite training in only around 28 GPU hours, 1-2 orders of magnitude faster than full NNGP / NTK / Myrtle kernels, whilst achieving comparable performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#27604;&#21021;&#22987;&#29366;&#24577;&#32531;&#20914;&#21306;&#30340;&#27010;&#24565;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#30340;&#29366;&#24577;&#26469;&#21021;&#22987;&#21270;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#20197;&#24341;&#23548;&#20854;&#36827;&#20837;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#29366;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.09752</link><description>&lt;p&gt;
&#23545;&#27604;&#21021;&#22987;&#29366;&#24577;&#32531;&#20914;&#21306;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contrastive Initial State Buffer for Reinforcement Learning. (arXiv:2309.09752v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#27604;&#21021;&#22987;&#29366;&#24577;&#32531;&#20914;&#21306;&#30340;&#27010;&#24565;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#30340;&#29366;&#24577;&#26469;&#21021;&#22987;&#21270;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#20197;&#24341;&#23548;&#20854;&#36827;&#20837;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#29366;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21208;&#25506;&#19982;&#21033;&#29992;&#20043;&#38388;&#30340;&#24179;&#34913;&#32473;&#20174;&#26377;&#38480;&#30340;&#26679;&#26412;&#20013;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#24102;&#26469;&#20102;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#24037;&#20316;&#22312;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#36827;&#34892;&#31574;&#30053;&#26356;&#26032;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#20102;&#37325;&#26032;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#30340;&#28508;&#21147;&#12290;&#29420;&#31435;&#20110;&#22522;&#26412;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#21021;&#22987;&#29366;&#24577;&#32531;&#20914;&#21306;&#30340;&#27010;&#24565;&#65292;&#23427;&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#36873;&#25321;&#29366;&#24577;&#65292;&#24182;&#29992;&#36825;&#20123;&#29366;&#24577;&#21021;&#22987;&#21270;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#20197;&#24341;&#23548;&#23427;&#36208;&#21521;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#20381;&#36182;&#20219;&#20309;&#20851;&#20110;&#29615;&#22659;&#30340;&#20808;&#39564;&#20449;&#24687;&#65306;&#65288;i&#65289;&#22235;&#36275;&#26426;&#22120;&#20154;&#31359;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#21644;&#65288;ii&#65289;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#22312;&#36187;&#36947;&#19978;&#39134;&#34892;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21021;&#22987;&#29366;&#24577;&#32531;&#20914;&#21306;&#22312;&#20219;&#21153;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#21516;&#26102;&#36824;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning, the trade-off between exploration and exploitation poses a complex challenge for achieving efficient learning from limited samples. While recent works have been effective in leveraging past experiences for policy updates, they often overlook the potential of reusing past experiences for data collection. Independent of the underlying RL algorithm, we introduce the concept of a Contrastive Initial State Buffer, which strategically selects states from past experiences and uses them to initialize the agent in the environment in order to guide it toward more informative states. We validate our approach on two complex robotic tasks without relying on any prior information about the environment: (i) locomotion of a quadruped robot traversing challenging terrains and (ii) a quadcopter drone racing through a track. The experimental results show that our initial state buffer achieves higher task performance than the nominal baseline while also speeding up training conv
&lt;/p&gt;</description></item><item><title>RaTrack&#26159;&#19968;&#31181;&#38024;&#23545;&#38647;&#36798;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#20197;&#21450;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#31227;&#21160;&#29289;&#20307;&#30340;&#31934;&#30830;&#36319;&#36394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09737</link><description>&lt;p&gt;
RaTrack: &#24102;&#26377;4D&#38647;&#36798;&#28857;&#20113;&#30340;&#36816;&#21160;&#29289;&#20307;&#26816;&#27979;&#19982;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09737
&lt;/p&gt;
&lt;p&gt;
RaTrack&#26159;&#19968;&#31181;&#38024;&#23545;&#38647;&#36798;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#20197;&#21450;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#31227;&#21160;&#29289;&#20307;&#30340;&#31934;&#30830;&#36319;&#36394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#33258;&#20027;&#24615;&#20381;&#36182;&#20110;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#31934;&#30830;&#24863;&#30693;&#12290;&#22312;3D&#19990;&#30028;&#20013;&#31283;&#23450;&#22320;&#36319;&#36394;&#31227;&#21160;&#29289;&#20307;&#22240;&#27492;&#23545;&#20110;&#36712;&#36857;&#39044;&#27979;&#12289;&#36991;&#38556;&#21644;&#36335;&#24452;&#35268;&#21010;&#31561;&#24212;&#29992;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;LiDAR&#25110;&#30456;&#26426;&#36827;&#34892;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#65292;&#20294;4D&#25104;&#20687;&#38647;&#36798;&#30340;&#33021;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#35748;&#35782;&#21040;4D&#38647;&#36798;&#25968;&#25454;&#20013;&#30340;&#38647;&#36798;&#22122;&#22768;&#21644;&#28857;&#31232;&#30095;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RaTrack&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22522;&#20110;&#38647;&#36798;&#30340;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25682;&#24323;&#20102;&#23545;&#29305;&#23450;&#23545;&#35937;&#31867;&#22411;&#21644;3D&#36793;&#30028;&#26694;&#30340;&#20381;&#36182;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#65292;&#24182;&#37197;&#20197;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#12290;&#22312;View-of-Delft&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;RaTrack&#23637;&#31034;&#20986;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#36816;&#21160;&#29289;&#20307;&#36319;&#36394;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO&#65289;&#38024;&#23545;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26102;&#38388;&#21644;&#27745;&#26579;&#26469;&#32531;&#35299;&#20132;&#36890;&#38459;&#22622;&#38382;&#39064;&#65292;&#32463;&#23454;&#35777;&#20998;&#26512;&#21644;&#23450;&#24615;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08254</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Quantitative and Qualitative Evaluation of Reinforcement Learning Policies for Autonomous Vehicles. (arXiv:2309.08254v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO&#65289;&#38024;&#23545;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26102;&#38388;&#21644;&#27745;&#26579;&#26469;&#32531;&#35299;&#20132;&#36890;&#38459;&#22622;&#38382;&#39064;&#65292;&#32463;&#23454;&#35777;&#20998;&#26512;&#21644;&#23450;&#24615;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#20132;&#36890;&#29615;&#22659;&#20013;&#20248;&#21270;&#20132;&#36890;&#21160;&#21147;&#23398;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#19982;&#20154;&#39550;&#39542;&#36710;&#36742;&#24182;&#23384;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;AVs&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31574;&#30053;&#26469;&#26368;&#23567;&#21270;&#20132;&#36890;&#38459;&#22622;&#65288;&#21363;&#26368;&#23567;&#21270;&#27178;&#36807;&#31859;&#20848;&#30340;&#29615;&#24418;&#36947;&#30340;&#26102;&#38388;&#65289;&#24182;&#20943;&#23569;&#27745;&#26579;&#12290;&#36890;&#36807;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#27745;&#26579;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20808;&#36827;&#30340;&#39550;&#39542;&#33329;&#23450;&#24615;&#35780;&#20272;&#20102;&#23398;&#21040;&#30340;&#31574;&#30053;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35780;&#20272;&#31574;&#30053;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25509;&#21463;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22120;&#36827;&#34892;&#20102;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#20132;&#36890;&#24179;&#31283;&#24615;&#21644;&#23433;&#20840;&#24863;&#31561;&#19968;&#31995;&#21015;&#25351;&#26631;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#39550;&#39542;&#36710;&#36742;&#30340;&#24863;&#30693;&#21644;&#34892;&#36710;&#24179;&#28369;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing traffic dynamics in an evolving transportation landscape is crucial, particularly in scenarios where autonomous vehicles (AVs) with varying levels of autonomy coexist with human-driven cars. This paper presents a novel approach to optimizing choices of AVs using Proximal Policy Optimization (PPO), a reinforcement learning algorithm. We learned a policy to minimize traffic jams (i.e., minimize the time to cross the scenario) and to minimize pollution in a roundabout in Milan, Italy. Through empirical analysis, we demonstrate that our approach can reduce time and pollution levels. Furthermore, we qualitatively evaluate the learned policy using a cutting-edge cockpit to assess its performance in near-real-world conditions. To gauge the practicality and acceptability of the policy, we conducted evaluations with human participants using the simulator, focusing on a range of metrics like traffic smoothness and safety perception. In general, our findings show that human-driven vehi
&lt;/p&gt;</description></item><item><title>CaloClouds II&#26159;&#19968;&#20010;&#36229;&#24555;&#36895;&#20960;&#20309;&#29420;&#31435;&#30340;&#39640;&#20998;&#36776;&#29575;&#37327;&#33021;&#22120;&#27169;&#25311;&#65292;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#24471;&#20998;&#24314;&#27169;&#65292;&#30456;&#27604;&#20256;&#32479;&#27169;&#25311;&#26356;&#24555;&#19988;&#20855;&#26377;&#21487;&#27604;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.05704</link><description>&lt;p&gt;
CaloClouds II: &#36229;&#24555;&#36895;&#20960;&#20309;&#29420;&#31435;&#30340;&#39640;&#20998;&#36776;&#29575;&#37327;&#33021;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation. (arXiv:2309.05704v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05704
&lt;/p&gt;
&lt;p&gt;
CaloClouds II&#26159;&#19968;&#20010;&#36229;&#24555;&#36895;&#20960;&#20309;&#29420;&#31435;&#30340;&#39640;&#20998;&#36776;&#29575;&#37327;&#33021;&#22120;&#27169;&#25311;&#65292;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#24471;&#20998;&#24314;&#27169;&#65292;&#30456;&#27604;&#20256;&#32479;&#27169;&#25311;&#26356;&#24555;&#19988;&#20855;&#26377;&#21487;&#27604;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#30340;&#33021;&#37327;&#27785;&#31215;&#30340;&#24555;&#36895;&#27169;&#25311;&#23545;&#20110;&#26410;&#26469;&#20855;&#26377;&#19981;&#26029;&#22686;&#21152;&#20142;&#24230;&#30340;&#23545;&#25758;&#26426;&#23454;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#21152;&#24555;&#21644;&#22686;&#24378;&#29289;&#29702;&#20998;&#26512;&#20013;&#30340;&#20256;&#32479;&#27169;&#25311;&#38142;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#21162;&#21147;&#23616;&#38480;&#20110;&#20381;&#36182;&#20110;&#22266;&#23450;&#12289;&#35268;&#21017;&#30340;&#25506;&#27979;&#22120;&#35835;&#20986;&#20960;&#20309;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;CaloClouds&#27169;&#22411;&#26159;&#19968;&#20010;&#20960;&#20309;&#29420;&#31435;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20026;&#35774;&#24819;&#20013;&#30340;&#22269;&#38469;&#22823;&#22411;&#25506;&#27979;&#22120;&#65288;ILD&#65289;&#30340;&#30005;&#30913;&#37327;&#33021;&#22120;&#29983;&#25104;&#22359;&#29366;&#28857;&#20113;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CaloClouds II&#65292;&#23427;&#20855;&#26377;&#19968;&#20123;&#20851;&#38190;&#30340;&#25913;&#36827;&#12290;&#36825;&#21253;&#25324;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#24471;&#20998;&#30340;&#24314;&#27169;&#65292;&#23427;&#20801;&#35768;&#36827;&#34892;25&#27493;&#37319;&#26679;&#65292;&#19982;CaloClouds&#30340;&#20445;&#30495;&#24230;&#30456;&#24403;&#65292;&#21516;&#26102;&#22312;&#21333;&#20010;CPU&#19978;&#27604;Geant4&#24555;6&#20493;&#65288;&#27604;CaloClouds&#24555;5&#20493;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25193;&#25955;&#27169;&#22411;&#25552;&#28860;&#25104;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast simulation of the energy depositions in high-granular detectors is needed for future collider experiments with ever increasing luminosities. Generative machine learning (ML) models have been shown to speed up and augment the traditional simulation chain in physics analysis. However, the majority of previous efforts were limited to models relying on fixed, regular detector readout geometries. A major advancement is the recently introduced CaloClouds model, a geometry-independent diffusion model, which generates calorimeter showers as point clouds for the electromagnetic calorimeter of the envisioned International Large Detector (ILD).  In this work, we introduce CaloClouds II which features a number of key improvements. This includes continuous time score-based modelling, which allows for a 25 step sampling with comparable fidelity to CaloClouds while yielding a $6\times$ speed-up over Geant4 on a single CPU ($5\times$ over CaloClouds). We further distill the diffusion model into a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#28176;&#21464;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#20174;&#27169;&#24335;&#35782;&#21035;&#21040;&#20915;&#31574;&#21644;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#28041;&#21450;&#22343;&#34913;&#21644;&#21338;&#24328;&#35770;&#30340;&#25968;&#23398;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#20110;&#25552;&#20379;&#21160;&#26426;&#21644;&#30452;&#35266;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.04877</link><description>&lt;p&gt;
&#28176;&#21464;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28201;&#21644;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning. (arXiv:2309.04877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04877
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#28176;&#21464;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#20174;&#27169;&#24335;&#35782;&#21035;&#21040;&#20915;&#31574;&#21644;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#28041;&#21450;&#22343;&#34913;&#21644;&#21338;&#24328;&#35770;&#30340;&#25968;&#23398;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#20110;&#25552;&#20379;&#21160;&#26426;&#21644;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#22522;&#20110;&#19982;&#28176;&#21464;&#20248;&#21270;&#30340;&#32039;&#23494;&#32852;&#31995;&#12290;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#37096;&#20998;&#21462;&#20915;&#20110;&#20174;&#27169;&#24335;&#35782;&#21035;&#21040;&#20915;&#31574;&#21644;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#36716;&#21464;&#12290;&#22312;&#36825;&#20123;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#65292;&#28041;&#21450;&#22343;&#34913;&#21644;&#21338;&#24328;&#35770;&#32780;&#19981;&#26159;&#26497;&#20540;&#30340;&#26032;&#30340;&#25968;&#23398;&#25361;&#25112;&#20986;&#29616;&#20102;&#12290;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;--&#32771;&#34385;&#21040;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#39640;&#32500;&#24230;&#21644;&#22823;&#35268;&#27169;--&#20294;&#31616;&#21333;&#30340;&#26799;&#24230;&#19979;&#38477;&#19981;&#20877;&#26159;&#31639;&#27861;&#35774;&#35745;&#30340;&#20986;&#21457;&#28857;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#30340;&#26356;&#24191;&#27867;&#26694;&#26550;&#30340;&#28201;&#21644;&#20171;&#32461;&#65292;&#20174;&#38797;&#28857;&#21644;&#21333;&#35843;&#21338;&#24328;&#24320;&#22987;&#65292;&#28982;&#21518;&#21040;&#19968;&#33324;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#12290;&#34429;&#28982;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#20960;&#20010;&#31639;&#27861;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#20294;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#25552;&#20379;&#21160;&#26426;&#21644;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress in machine learning in recent years has been based on a highly productive connection to gradient-based optimization. Further progress hinges in part on a shift in focus from pattern recognition to decision-making and multi-agent problems. In these broader settings, new mathematical challenges emerge that involve equilibria and game theory instead of optima. Gradient-based methods remain essential -- given the high dimensionality and large scale of machine-learning problems -- but simple gradient descent is no longer the point of departure for algorithm design. We provide a gentle introduction to a broader framework for gradient-based algorithms in machine learning, beginning with saddle points and monotone games, and proceeding to general variational inequalities. While we provide convergence proofs for several of the algorithms that we present, our main focus is that of providing motivation and intuition.
&lt;/p&gt;</description></item><item><title>&#22312;&#22270;&#24418;&#39044;&#27979;&#38382;&#39064;&#20013;&#65292;GNNs&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#22270;&#32467;&#26500;&#65292;&#21363;&#20351;&#22312;&#24573;&#30053;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24120;&#35268;&#22270;&#23545;&#20110;&#36825;&#31181;&#36807;&#25311;&#21512;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04332</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#38656;&#35201;&#30340;&#26102;&#20505;&#20173;&#28982;&#20351;&#29992;&#22270;&#24418;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks Use Graphs When They Shouldn't. (arXiv:2309.04332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04332
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#24418;&#39044;&#27979;&#38382;&#39064;&#20013;&#65292;GNNs&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#22270;&#32467;&#26500;&#65292;&#21363;&#20351;&#22312;&#24573;&#30053;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24120;&#35268;&#22270;&#23545;&#20110;&#36825;&#31181;&#36807;&#25311;&#21512;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#21253;&#25324;&#31038;&#20132;&#32593;&#32476;&#12289;&#20998;&#23376;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#31561;&#65292;&#23545;&#22270;&#24418;&#36827;&#34892;&#39044;&#27979;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#25104;&#20026;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22270;&#24418;&#26631;&#27880;&#38382;&#39064;&#30340;&#23454;&#20363;&#21253;&#25324;&#22270;&#32467;&#26500;(&#21363;&#37051;&#25509;&#30697;&#38453;)&#21644;&#33410;&#28857;&#29305;&#23450;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#22270;&#32467;&#26500;&#23545;&#20110;&#39044;&#27979;&#20219;&#21153;&#26469;&#35828;&#24182;&#19981;&#20855;&#26377;&#20449;&#24687;&#37327;&#12290;&#20363;&#22914;&#65292;&#20998;&#23376;&#24615;&#36136;&#22914;&#25705;&#23572;&#36136;&#37327;&#20165;&#20381;&#36182;&#20110;&#32452;&#25104;&#21407;&#23376;(&#33410;&#28857;&#29305;&#24449;)&#65292;&#32780;&#19982;&#20998;&#23376;&#32467;&#26500;&#26080;&#20851;&#12290;&#23613;&#31649;GNNs&#26377;&#33021;&#21147;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24573;&#30053;&#22270;&#32467;&#26500;&#65292;&#20294;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#20250;&#36825;&#26679;&#20570;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GNNs&#23454;&#38469;&#19978;&#20542;&#21521;&#20110;&#22312;&#36807;&#25311;&#21512;&#22270;&#32467;&#26500;&#65292;&#21363;&#22312;&#24573;&#30053;&#23427;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#20173;&#22312;&#20351;&#29992;&#12290;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#22270;&#20998;&#24067;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#21457;&#29616;&#24120;&#35268;&#22270;&#23545;&#36825;&#31181;&#36807;&#25311;&#21512;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictions over graphs play a crucial role in various domains, including social networks, molecular biology, medicine, and more. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Instances of graph labeling problems consist of the graph-structure (i.e., the adjacency matrix), along with node-specific feature vectors. In some cases, this graph-structure is non-informative for the predictive task. For instance, molecular properties such as molar mass depend solely on the constituent atoms (node features), and not on the molecular structure. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#30340;&#37327;&#23376;&#26550;&#26500;&#23558;3D&#21644;&#31354;&#38388;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#20114;&#25972;&#21512;&#65292;&#25552;&#39640;&#20102;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03919</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#20197;&#25552;&#39640;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
A hybrid quantum-classical fusion neural network to improve protein-ligand binding affinity predictions for drug discovery. (arXiv:2309.03919v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#30340;&#37327;&#23376;&#26550;&#26500;&#23558;3D&#21644;&#31354;&#38388;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#20114;&#25972;&#21512;&#65292;&#25552;&#39640;&#20102;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#20851;&#38190;&#22312;&#20110;&#20934;&#30830;&#39044;&#27979;&#28508;&#22312;&#33647;&#29289;&#20998;&#23376;&#19982;&#38774;&#34507;&#30333;&#20043;&#38388;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#34507;&#30333;&#30452;&#25509;&#24433;&#21709;&#30142;&#30149;&#30340;&#36827;&#23637;&#26102;&#12290;&#28982;&#32780;&#65292;&#20272;&#35745;&#32467;&#21512;&#20146;&#21644;&#21147;&#38656;&#35201;&#26174;&#33879;&#30340;&#36130;&#21153;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26032;&#20852;&#30340;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#24402;&#21151;&#20110;&#23427;&#20204;&#22266;&#26377;&#30340;&#24182;&#34892;&#24615;&#21644;&#31649;&#29702;&#25968;&#25454;&#32500;&#24230;&#25351;&#25968;&#32423;&#22686;&#21152;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#36827;&#23637;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#25910;&#25947;&#31283;&#23450;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of drug discovery hinges on the accurate prediction of binding affinity between prospective drug molecules and target proteins, especially when such proteins directly influence disease progression. However, estimating binding affinity demands significant financial and computational resources. While state-of-the-art methodologies employ classical machine learning (ML) techniques, emerging hybrid quantum machine learning (QML) models have shown promise for enhanced performance, owing to their inherent parallelism and capacity to manage exponential increases in data dimensionality. Despite these advances, existing models encounter issues related to convergence stability and prediction accuracy. This paper introduces a novel hybrid quantum-classical deep learning model tailored for binding affinity prediction in drug discovery. Specifically, the proposed model synergistically integrates 3D and spatial graph convolutional neural networks within an optimized quantum architecture. S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#24046;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#31163;&#25955;&#20998;&#24067;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#31163;&#25955;&#19988;&#20445;&#25345;&#24230;&#37327;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#38656;&#35201;&#36830;&#32493;&#23884;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#36830;&#32493;&#23884;&#20837;&#27969;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2308.15613</link><description>&lt;p&gt;
&#28151;&#21512;&#26041;&#24046;&#27969;&#29992;&#20110;&#31163;&#25955;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Mixed Variational Flows for Discrete Variables. (arXiv:2308.15613v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#24046;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#31163;&#25955;&#20998;&#24067;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#31163;&#25955;&#19988;&#20445;&#25345;&#24230;&#37327;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#38656;&#35201;&#36830;&#32493;&#23884;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#36830;&#32493;&#23884;&#20837;&#27969;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#27969;&#20801;&#35768;&#20174;&#20107;&#32773;&#23398;&#20064;&#22797;&#26434;&#30340;&#36830;&#32493;&#20998;&#24067;&#65292;&#20294;&#26159;&#36817;&#20284;&#31163;&#25955;&#20998;&#24067;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#31163;&#25955;&#30446;&#26631;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#20013;-&#36890;&#24120;&#26159;&#36890;&#36807;&#36830;&#32493;&#26494;&#24347;&#25110;&#21435;&#37327;&#21270;-&#28982;&#21518;&#24212;&#29992;&#36830;&#32493;&#27969;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#19968;&#20010;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;&#21407;&#22987;&#31163;&#25955;&#30446;&#26631;&#30340;&#26367;&#20195;&#30446;&#26631;&#65292;&#21487;&#33021;&#20855;&#26377;&#20559;&#20506;&#25110;&#19981;&#31283;&#23450;&#30340;&#26799;&#24230;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#21019;&#24314;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#20998;&#24067;&#30340;&#21464;&#20998;&#27969;&#26063;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#36830;&#32493;&#23884;&#20837;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20445;&#25345;&#24230;&#37327;&#30340;&#31163;&#25955;&#21487;&#36870;&#26144;&#23556;&#65292;&#20351;&#31163;&#25955;&#30446;&#26631;&#20445;&#25345;&#19981;&#21464;&#65292;&#28982;&#21518;&#22522;&#20110;&#35813;&#26144;&#23556;&#21019;&#24314;&#20102;&#19968;&#20010;&#28151;&#21512;&#21464;&#20998;&#27969;(MAD Mix)&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#25193;&#23637;&#65292;&#29992;&#20110;&#22788;&#29702;&#32852;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MAD Mix&#20135;&#29983;&#20102;&#27604;&#36830;&#32493;&#23884;&#20837;&#27969;&#26356;&#21487;&#38752;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational flows allow practitioners to learn complex continuous distributions, but approximating discrete distributions remains a challenge. Current methodologies typically embed the discrete target in a continuous space - usually via continuous relaxation or dequantization - and then apply a continuous flow. These approaches involve a surrogate target that may not capture the original discrete target, might have biased or unstable gradients, and can create a difficult optimization problem. In this work, we develop a variational flow family for discrete distributions without any continuous embedding. First, we develop a measure-preserving and discrete (MAD) invertible map that leaves the discrete target invariant, and then create a mixed variational flow (MAD Mix) based on that map. We also develop an extension to MAD Mix that handles joint discrete and continuous models. Our experiments suggest that MAD Mix produces more reliable approximations than continuous-embedding flows while 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11838</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#26657;&#20934;&#38382;&#39064;&#65292;&#23613;&#31649;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20351;&#29992;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26694;&#26550;&#26469;&#25913;&#21892;&#26657;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#26657;&#20934;&#23646;&#24615;&#30340;&#30740;&#31350;&#26377;&#28857;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20840;&#38754;&#25506;&#32034;&#26657;&#20934;&#23646;&#24615;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#23613;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#12290;&#25105;&#20204;&#29305;&#21035;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;NATS-Bench&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#20102;90&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#26657;&#20934;&#24230;&#37327;&#21644;12&#20010;&#20854;&#20182;&#26657;&#20934;&#24230;&#37327;&#65292;&#28085;&#30422;&#20102;117,702&#20010;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#22238;&#31572;&#35813;&#39046;&#22495;&#19968;&#20123;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26657;&#20934;&#33021;&#21542;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65311;&#65288;ii&#65289;&#33021;&#21542;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#20154;&#24037;&#21644;&#31639;&#27861;&#21512;&#20316;&#65292;&#23545;&#20110;&#22810;&#20010;&#22122;&#38899;&#27169;&#22411;&#26469;&#35828;&#65292;&#23558;&#36873;&#25321;&#39033;&#30446;&#30340;&#23376;&#38598;&#22823;&#23567;$k$&#35774;&#32622;&#22312;$[2, n-1]$&#33539;&#22260;&#20869;&#33021;&#22815;&#26368;&#22823;&#21270;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#39033;&#30446;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11721</link><description>&lt;p&gt;
&#20004;&#20010;&#21015;&#34920;&#20160;&#20040;&#26102;&#20505;&#27604;&#19968;&#20010;&#21015;&#34920;&#26356;&#22909;&#65311;&#21512;&#20316;&#20915;&#31574;&#20013;&#30340;&#30410;&#22788;&#21644;&#20260;&#23475;
&lt;/p&gt;
&lt;p&gt;
When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making. (arXiv:2308.11721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11721
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#20154;&#24037;&#21644;&#31639;&#27861;&#21512;&#20316;&#65292;&#23545;&#20110;&#22810;&#20010;&#22122;&#38899;&#27169;&#22411;&#26469;&#35828;&#65292;&#23558;&#36873;&#25321;&#39033;&#30446;&#30340;&#23376;&#38598;&#22823;&#23567;$k$&#35774;&#32622;&#22312;$[2, n-1]$&#33539;&#22260;&#20869;&#33021;&#22815;&#26368;&#22823;&#21270;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#39033;&#30446;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#24456;&#22823;&#19968;&#37096;&#20998;&#20851;&#27880;&#30340;&#26159;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20294;&#26368;&#36817;&#26356;&#22810;&#22320;&#20851;&#27880;&#20110;&#20248;&#21270;&#20154;&#24037;&#21644;&#31639;&#27861;&#30340;&#32852;&#21512;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#20154;&#24037;&#21644;&#31639;&#27861;&#21512;&#20316;&#65292;&#22312;&#36825;&#31181;&#21512;&#20316;&#20013;&#65292;&#31639;&#27861;&#21487;&#20197;&#35775;&#38382;&#19968;&#32452;n&#20010;&#39033;&#30446;&#65292;&#24182;&#23558;&#22823;&#23567;&#20026;k&#30340;&#19968;&#20010;&#23376;&#38598;&#21576;&#29616;&#32473;&#20154;&#31867;&#65292;&#28982;&#21518;&#20154;&#31867;&#20174;&#36825;&#20123;k&#20010;&#39033;&#30446;&#20013;&#36873;&#25321;&#19968;&#20010;&#26368;&#32456;&#39033;&#30446;&#12290;&#36825;&#31181;&#24773;&#20917;&#21487;&#20197;&#27169;&#25311;&#20869;&#23481;&#25512;&#33616;&#12289;&#36335;&#24452;&#35268;&#21010;&#25110;&#20219;&#20309;&#31867;&#22411;&#30340;&#26631;&#27880;&#20219;&#21153;&#12290;&#30001;&#20110;&#20154;&#31867;&#21644;&#31639;&#27861;&#37117;&#23545;&#39033;&#30446;&#30340;&#30495;&#23454;&#25490;&#24207;&#26377;&#30528;&#19981;&#23436;&#32654;&#12289;&#26377;&#22122;&#38899;&#30340;&#20449;&#24687;&#65292;&#20851;&#38190;&#38382;&#39064;&#26159;&#65306;&#21738;&#20010;$k$&#20540;&#33021;&#26368;&#22823;&#21270;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#39033;&#30446;&#30340;&#27010;&#29575;&#65311;&#23545;&#20110;$k=1$&#65292;&#31639;&#27861;&#21333;&#29420;&#34892;&#21160;&#26102;&#24615;&#33021;&#26368;&#20248;&#65292;&#32780;&#23545;&#20110;$k=n$&#65292;&#20154;&#31867;&#21333;&#29420;&#34892;&#21160;&#26102;&#24615;&#33021;&#26368;&#20248;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#22810;&#20010;&#22122;&#38899;&#27169;&#22411;&#65292;&#23558;$k$&#35774;&#32622;&#22312;$[2, n-1]$&#33539;&#22260;&#20869;&#26159;&#26368;&#20248;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#21512;&#20316;&#26377;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to collaborating,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#26799;&#24230;&#65292;&#22686;&#24378;&#20102;&#28145;&#23618;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#22312;&#22788;&#29702;&#31995;&#32479;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11464</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21516;&#36136;&#24615;&#21040;&#24322;&#36136;&#24615;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning. (arXiv:2308.11464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11464
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#26799;&#24230;&#65292;&#22686;&#24378;&#20102;&#28145;&#23618;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#22312;&#22788;&#29702;&#31995;&#32479;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#31995;&#32479;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#22686;&#24378;&#22823;&#22810;&#25968;&#27169;&#22411;&#21516;&#36136;&#24615;FL&#26041;&#27861;&#22788;&#29702;&#31995;&#32479;&#24322;&#36136;&#24615;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#25193;&#23637;&#23427;&#20204;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20174;&#35814;&#32454;&#25506;&#32034;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;FL&#35774;&#32622;&#24320;&#22987;&#65292;&#21457;&#29616;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#65306;&#65288;1&#65289;&#23458;&#25143;&#31471;&#24615;&#33021;&#19982;&#23618;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21576;&#27491;&#30456;&#20851;&#65292;&#65288;2&#65289;&#27973;&#23618;&#27604;&#28145;&#23618;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#65292;&#65288;3&#65289;&#36739;&#20026;&#24179;&#28369;&#30340;&#26799;&#24230;&#20998;&#24067;&#25351;&#31034;&#20102;&#26356;&#39640;&#30340;&#23618;&#30456;&#20284;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InCo Aggregation&#26041;&#27861;&#65292;&#21033;&#29992;&#20869;&#37096;&#36328;&#23618;&#26799;&#24230;&#65292;&#21363;&#26381;&#21153;&#22120;&#27169;&#22411;&#20013;&#26469;&#33258;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#26799;&#24230;&#28151;&#21512;&#65292;&#20197;&#22686;&#24378;&#28145;&#23618;&#30340;&#30456;&#20284;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#23458;&#25143;&#31471;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverags internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods 
&lt;/p&gt;</description></item><item><title>ALI-DPFL&#26159;&#19968;&#31181;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#26469;&#20248;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.10457</link><description>&lt;p&gt;
ALI-DPFL: &#20855;&#26377;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations. (arXiv:2308.10457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10457
&lt;/p&gt;
&lt;p&gt;
ALI-DPFL&#26159;&#19968;&#31181;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#26469;&#20248;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#20849;&#20139;&#35757;&#32451;&#21442;&#25968;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#65292;&#20801;&#35768;&#22810;&#20010;&#35774;&#22791;&#25110;&#32452;&#32455;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#23545;&#36825;&#20123;&#35757;&#32451;&#21442;&#25968;&#30340;&#25512;&#29702;&#25915;&#20987;&#65288;&#20363;&#22914;&#24046;&#20998;&#25915;&#20987;&#65289;&#26469;&#25512;&#26029;&#20010;&#20307;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#24046;&#20998;&#38544;&#31169;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#20197;&#38450;&#27490;&#27492;&#31867;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#22330;&#26223;&#20013;&#32771;&#34385;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#65292;&#20854;&#20013;&#26082;&#26377;&#38544;&#31169;&#39044;&#31639;&#21463;&#38480;&#65292;&#21448;&#26377;&#36890;&#20449;&#36718;&#27425;&#21463;&#38480;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25910;&#25947;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#25214;&#21040;&#22312;&#20219;&#24847;&#20004;&#20010;&#39034;&#24207;&#20840;&#23616;&#26356;&#26032;&#20043;&#38388;&#30340;&#23458;&#25143;&#26426;&#20043;&#38388;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#26412;&#22320;&#36845;&#20195;&#27425;&#25968;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ALI-DPFL&#65289;&#12290;&#25105;&#20204;&#22312;FashionMNIST&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning technique that allows model training among multiple devices or organizations by sharing training parameters instead of raw data. However, adversaries can still infer individual information through inference attacks (e.g. differential attacks) on these training parameters. As a result, Differential Privacy (DP) has been widely used in FL to prevent such attacks. We consider differentially private federated learning in a resource-constrained scenario, where both privacy budget and communication round are constrained. By theoretically analyzing the convergence, we can find the optimal number of differentially private local iterations for clients between any two sequential global updates. Based on this, we design an algorithm of differentially private federated learning with adaptive local iterations (ALI-DPFL). We experiment our algorithm on the FashionMNIST and CIFAR10 datasets, and demonstrate significantly better performances th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22320;&#19979;&#25968;&#25454;&#38598;&#30340;&#31283;&#23450;&#38477;&#32500;&#26041;&#27861;&#65292;&#36890;&#36807;&#21018;&#24615;&#21464;&#25442;&#23454;&#29616;&#20102;&#27431;&#20960;&#37324;&#24503;&#19981;&#21464;&#34920;&#31034;&#65292;&#33021;&#22815;&#37327;&#21270;&#22320;&#19979;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#19988;&#36866;&#24212;&#22806;&#26679;&#26412;&#28857;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.08079</link><description>&lt;p&gt;
&#29992;&#20110;&#25903;&#25345;&#22320;&#19979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#35299;&#37322;&#30340;&#31283;&#23450;&#20302;&#32500;&#31354;&#38388;&#21018;&#24615;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation. (arXiv:2308.08079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08079
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22320;&#19979;&#25968;&#25454;&#38598;&#30340;&#31283;&#23450;&#38477;&#32500;&#26041;&#27861;&#65292;&#36890;&#36807;&#21018;&#24615;&#21464;&#25442;&#23454;&#29616;&#20102;&#27431;&#20960;&#37324;&#24503;&#19981;&#21464;&#34920;&#31034;&#65292;&#33021;&#22815;&#37327;&#21270;&#22320;&#19979;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#19988;&#36866;&#24212;&#22806;&#26679;&#26412;&#28857;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#19979;&#25968;&#25454;&#38598;&#22825;&#28982;&#22320;&#20855;&#26377;&#22823;&#25968;&#25454;&#29305;&#24449;&#65292;&#22914;&#24222;&#22823;&#30340;&#20307;&#31215;&#12289;&#22810;&#26679;&#30340;&#29305;&#24449;&#21644;&#39640;&#36895;&#37319;&#26679;&#36895;&#24230;&#65292;&#21463;&#21040;&#21508;&#31181;&#29289;&#29702;&#12289;&#24037;&#31243;&#21644;&#22320;&#36136;&#36755;&#20837;&#24341;&#36215;&#30340;&#32500;&#25968;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#22312;&#29616;&#26377;&#30340;&#38477;&#32500;&#26041;&#27861;&#20013;&#65292;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#24230;&#37327;&#22810;&#32500;&#32553;&#25918;&#65288;MDS&#65289;&#65292;&#26159;&#22320;&#19979;&#25968;&#25454;&#38598;&#20013;&#39318;&#36873;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#34429;&#28982;MDS&#20445;&#30041;&#20102;&#20869;&#22312;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21253;&#25324;&#19981;&#31283;&#23450;&#30340;&#21807;&#19968;&#35299;&#65292;&#19981;&#21464;&#20110;&#27431;&#20960;&#37324;&#24503;&#21464;&#25442;&#65292;&#24182;&#19988;&#27809;&#26377;&#22806;&#26679;&#26412;&#28857;&#65288;OOSP&#65289;&#25193;&#23637;&#12290;&#20026;&#20102;&#22686;&#24378;&#22320;&#19979;&#25512;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#65292;&#24517;&#39035;&#23558;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#31283;&#23450;&#30340;&#12289;&#38477;&#32500;&#30340;&#34920;&#31034;&#65292;&#20197;&#23481;&#32435;OOSP&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#21018;&#24615;&#21464;&#25442;&#23454;&#29616;&#20102;LDS&#30340;&#31283;&#23450;&#27431;&#20960;&#37324;&#24503;&#19981;&#21464;&#34920;&#31034;&#12290;&#36890;&#36807;&#35745;&#31639;MDS&#36755;&#20837;&#30340;&#19981;&#30456;&#20284;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
Subsurface datasets inherently possess big data characteristics such as vast volume, diverse features, and high sampling speeds, further compounded by the curse of dimensionality from various physical, engineering, and geological inputs. Among the existing dimensionality reduction (DR) methods, nonlinear dimensionality reduction (NDR) methods, especially Metric-multidimensional scaling (MDS), are preferred for subsurface datasets due to their inherent complexity. While MDS retains intrinsic data structure and quantifies uncertainty, its limitations include unstabilized unique solutions invariant to Euclidean transformations and an absence of out-of-sample points (OOSP) extension. To enhance subsurface inferential and machine learning workflows, datasets must be transformed into stable, reduced-dimension representations that accommodate OOSP.  Our solution employs rigid transformations for a stabilized Euclidean invariant representation for LDS. By computing an MDS input dissimilarity m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#21644;&#32447;&#24615;&#25506;&#27979;&#19982;&#24494;&#35843;&#30340;&#38598;&#25104;&#65292;&#26377;&#25928;&#20943;&#36731;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07728</link><description>&lt;p&gt;
&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65306;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#24212;&#24615;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#21644;&#32447;&#24615;&#25506;&#27979;&#19982;&#24494;&#35843;&#30340;&#38598;&#25104;&#65292;&#26377;&#25928;&#20943;&#36731;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#24050;&#20855;&#22791;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#21457;&#29983;&#30072;&#21464;&#12290;&#22312;&#36866;&#24212;&#26032;&#30446;&#26631;&#39046;&#22495;&#26102;&#20943;&#36731;&#29305;&#24449;&#30072;&#21464;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#23545;&#22836;&#23618;&#36827;&#34892;&#23545;&#40784;&#22788;&#29702;&#21487;&#20197;&#22788;&#29702;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25209;&#24402;&#19968;&#21270;&#23618;&#30340;&#22788;&#29702;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#24615;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#12289;&#32447;&#24615;&#25506;&#27979;&#21644;&#24494;&#35843;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#25913;&#26469;&#26377;&#25928;&#20943;&#36731;&#29305;&#24449;&#30072;&#21464;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#32447;&#24615;&#25506;&#27979;&#21644;&#24494;&#35843;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integrati
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#34892;&#19994;&#23558;&#20135;&#29983;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#65292;&#31616;&#21270;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#20351;&#29992;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.06013</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#34892;&#19994;&#30340;&#26410;&#26469;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Telecom: Forthcoming Impact on the Industry. (arXiv:2308.06013v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06013
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#34892;&#19994;&#23558;&#20135;&#29983;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#65292;&#31616;&#21270;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#20351;&#29992;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#32929;&#21464;&#38761;&#30340;&#21147;&#37327;&#65292;&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20256;&#32479;&#39046;&#22495;&#20043;&#22806;&#65292;&#36824;&#22312;&#35768;&#22810;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#20851;&#27880;&#12290;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#30005;&#20449;&#34892;&#19994;&#38754;&#20020;&#30528;&#28508;&#22312;&#24433;&#21709;&#30340;&#21069;&#26223;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#20123;&#24433;&#21709;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;LLMs&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#23427;&#20204;&#30446;&#21069;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#30005;&#20449;&#34892;&#19994;&#21487;&#20197;&#26041;&#20415;&#23454;&#26045;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#31616;&#21270;&#20102;&#30446;&#21069;&#22952;&#30861;&#36816;&#33829;&#25928;&#29575;&#24182;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#24037;&#31243;&#19987;&#19994;&#30693;&#35782;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#22312;&#30005;&#20449;&#39046;&#22495;&#21033;&#29992;LLMs&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#26159;&#20805;&#20998;&#21033;&#29992;LLMs&#28508;&#21147;&#21644;&#21457;&#25381;&#20854;&#33021;&#21147;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as a transformative force, revolutionizing numerous fields well beyond the conventional domain of Natural Language Processing (NLP) and garnering unprecedented attention. As LLM technology continues to progress, the telecom industry is facing the prospect of its potential impact on its landscape. To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations. We also examine the use cases that can be readily implemented in the telecom industry, streamlining numerous tasks that currently hinder operational efficiency and demand significant manpower and engineering expertise. Furthermore, we uncover essential research directions that deal with the distinctive challenges of utilizing the LLMs within the telecom domain. Addressing these challenges represents a significant stride towards fully harnessing the potential of LLMs and unlocking their capabilities to the fulles
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#27714;&#35299;&#20027;&#26041;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#32593;&#32476;&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#32452;&#21644;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#31934;&#24230;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.02514</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#26041;&#31243;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language models as master equation solvers. (arXiv:2308.02514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#27714;&#35299;&#20027;&#26041;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#32593;&#32476;&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#32452;&#21644;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#31934;&#24230;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#26041;&#31243;&#22312;&#24314;&#27169;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#20013;&#20855;&#26377;&#22522;&#26412;&#37325;&#35201;&#24615;&#65292;&#28982;&#32780;&#30001;&#20110;&#29366;&#24577;&#31354;&#38388;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#35299;&#20915;&#20027;&#26041;&#31243;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#23558;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#24212;&#29992;&#20026;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20027;&#26041;&#31243;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#36895;&#29575;&#21442;&#25968;&#12289;&#21021;&#22987;&#26465;&#20214;&#21644;&#26102;&#38388;&#20540;&#30452;&#25509;&#26144;&#23556;&#21040;&#19982;&#36755;&#20837;&#19978;&#19979;&#25991;&#23436;&#20840;&#21305;&#37197;&#30340;&#29366;&#24577;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#36817;&#20284;&#22320;&#27714;&#35299;&#20102;&#20027;&#26041;&#31243;&#30340;&#26368;&#19968;&#33324;&#24418;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23545;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#21453;&#39304;&#22870;&#21169;&#30001;&#19968;&#32452;&#21464;&#20998;&#33258;&#22238;&#24402;&#27169;&#22411;&#25552;&#20379;&#12290;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20195;&#34920;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23545;&#20110;&#22810;&#27169;&#32452;&#21644;&#39640;&#32500;&#31995;&#32479;&#65292;&#20934;&#30830;&#24615;&#24456;&#39640;&#12290;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Master equations are of fundamental importance in modeling stochastic dynamical systems.However, solving master equations is challenging due to the exponential increase in the number of possible states or trajectories with the dimension of the state space. In this study, we propose repurposing language models as a machine learning approach to solve master equations. We design a prompt-based neural network to map rate parameters, initial conditions, and time values directly to the state joint probability distribution that exactly matches the input contexts. In this way, we approximate the solution of the master equation in its most general form. We train the network using the policy gradient algorithm within the reinforcement learning framework, with feedback rewards provided by a set of variational autoregressive models. By applying this approach to representative examples, we observe high accuracy for both multi-module and high-dimensional systems. The trained network also exhibits ex
&lt;/p&gt;</description></item><item><title>PELICAN&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#38382;&#39064;&#20013;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;PELICAN&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#23545;&#31216;&#32676;&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#38477;&#20302;&#22797;&#26434;&#24615;&#12289;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#21644;&#25552;&#39640;&#24615;&#33021;&#30340;&#29305;&#28857;&#12290;&#23427;&#22312;&#26631;&#35760;&#21644;&#37325;&#26500;&#21160;&#37327;&#22686;&#24378;&#30340;&#39030;&#22840;&#20811;&#65292;&#24182;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#29305;&#21035;&#35782;&#21035;&#21644;&#27979;&#37327;W&#29627;&#33394;&#23376;&#65292;&#20197;&#21450;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#21943;&#27880;&#31561;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.16506</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#22312;&#31890;&#23376;&#29289;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;PELICAN
&lt;/p&gt;
&lt;p&gt;
Explainable Equivariant Neural Networks for Particle Physics: PELICAN. (arXiv:2307.16506v2 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16506
&lt;/p&gt;
&lt;p&gt;
PELICAN&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#38382;&#39064;&#20013;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;PELICAN&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#23545;&#31216;&#32676;&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#38477;&#20302;&#22797;&#26434;&#24615;&#12289;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#21644;&#25552;&#39640;&#24615;&#33021;&#30340;&#29305;&#28857;&#12290;&#23427;&#22312;&#26631;&#35760;&#21644;&#37325;&#26500;&#21160;&#37327;&#22686;&#24378;&#30340;&#39030;&#22840;&#20811;&#65292;&#24182;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#29305;&#21035;&#35782;&#21035;&#21644;&#27979;&#37327;W&#29627;&#33394;&#23376;&#65292;&#20197;&#21450;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#21943;&#27880;&#31561;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PELICAN&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32622;&#25442;&#31561;&#21464;&#19988;&#27931;&#20262;&#20857;&#19981;&#21464;&#25110;&#21327;&#21464;&#30340;&#32858;&#21512;&#32593;&#32476;&#65292;&#26088;&#22312;&#20811;&#26381;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#38382;&#39064;&#30340;&#24120;&#35265;&#38480;&#21046;&#12290;&#19982;&#35768;&#22810;&#20351;&#29992;&#38750;&#19987;&#29992;&#26550;&#26500;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;PELICAN&#37319;&#29992;&#22522;&#20110;&#23545;&#31216;&#32676;&#30340;&#26550;&#26500;&#65292;&#20307;&#29616;&#20102;&#22797;&#26434;&#24230;&#38477;&#20302;&#12289;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#21644;&#24615;&#33021;&#25552;&#21319;&#31561;&#20248;&#21183;&#65292;&#32780;&#38750;&#20197;&#24222;&#22823;&#30340;&#21442;&#25968;&#20026;&#20195;&#20215;&#12290;&#25105;&#20204;&#22312;&#26631;&#35760;&#65288;&#20998;&#31867;&#65289;&#21644;&#37325;&#26500;&#65288;&#22238;&#24402;&#65289;&#21160;&#37327;&#22686;&#24378;&#30340;&#39030;&#22840;&#20811;&#30340;&#32972;&#26223;&#19979;&#23545;PELICAN&#31639;&#27861;&#26550;&#26500;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#22312;&#27931;&#20262;&#20857;&#22686;&#24378;&#30340;&#39030;&#22840;&#20811;&#24378;&#23376;&#26411;&#24577;&#30340;&#23494;&#38598;&#29615;&#22659;&#20013;&#29305;&#21035;&#35782;&#21035;&#21644;&#27979;&#37327;W&#29627;&#33394;&#23376;&#30340;&#22256;&#38590;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#23558;PELICAN&#24212;&#29992;&#20110;&#35782;&#21035;&#22840;&#20811;-&#24341;&#21457;&#19982;&#33014;&#23376;-&#24341;&#21457;&#21943;&#27880;&#20197;&#21450;&#22810;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
PELICAN is a novel permutation equivariant and Lorentz invariant or covariant aggregator network designed to overcome common limitations found in architectures applied to particle physics problems. Compared to many approaches that use non-specialized architectures that neglect underlying physics principles and require very large numbers of parameters, PELICAN employs a fundamentally symmetry group-based architecture that demonstrates benefits in terms of reduced complexity, increased interpretability, and raw performance. We present a comprehensive study of the PELICAN algorithm architecture in the context of both tagging (classification) and reconstructing (regression) Lorentz-boosted top quarks, including the difficult task of specifically identifying and measuring the $W$-boson inside the dense environment of the Lorentz-boosted top-quark hadronic final state. We also extend the application of PELICAN to the tasks of identifying quark-initiated vs.~gluon-initiated jets, and a multi-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.12856</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#35268;&#21010;&#12289;&#38271;&#26399;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#31243;&#24207;&#21512;&#25104;&#33021;&#21147;&#30340;&#29616;&#23454;&#19990;&#30028;WebAgent
&lt;/p&gt;
&lt;p&gt;
A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#20027;Web&#33258;&#21160;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#32593;&#31449;&#19978;&#65292;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#19977;&#20010;&#26041;&#38754;&#30340;&#38480;&#21046;&#65306;&#24320;&#25918;&#39046;&#22495;&#24615;&#12289;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#23545;HTML&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;WebAgent&#36890;&#36807;&#23558;&#25351;&#20196;&#20998;&#35299;&#20026;&#35268;&#33539;&#30340;&#23376;&#25351;&#20196;&#65292;&#23558;&#38271;HTML&#25991;&#26723;&#24635;&#32467;&#20026;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#20174;&#20013;&#29983;&#25104;&#30340;Python&#31243;&#24207;&#23545;&#32593;&#31449;&#36827;&#34892;&#25805;&#20316;&#26469;&#25552;&#21069;&#36827;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#20351;&#29992;Flan-U-PaLM&#35774;&#35745;&#20102;WebAgent&#65292;&#29992;&#20110;&#29983;&#25104;&#26377;&#26681;&#20195;&#30721;&#65292;&#24182;&#20351;&#29992;HTML-T5&#36827;&#34892;&#39044;&#35757;&#32451;LLMs&#65292;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#20197;&#21450;&#28151;&#21512;&#38271;&#36328;&#24230;&#21435;&#22122;&#30446;&#26631;&#26469;&#36827;&#34892;&#35268;&#21010;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22826;&#36203;&#20857;&#27969;&#23548;&#21521;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23450;&#20301;&#31934;&#24230;&#20302;&#21644;&#26080;&#27861;&#20840;&#23616;&#23450;&#20301;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05551</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22826;&#36203;&#20857;&#27969;&#23548;&#21521;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization. (arXiv:2307.05551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22826;&#36203;&#20857;&#27969;&#23548;&#21521;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23450;&#20301;&#31934;&#24230;&#20302;&#21644;&#26080;&#27861;&#20840;&#23616;&#23450;&#20301;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#31859;&#25216;&#26415;&#21644;&#20808;&#36827;&#26448;&#26009;&#30340;&#31185;&#23398;&#36827;&#23637;&#20026;&#20307;&#20869;&#31934;&#20934;&#21307;&#23398;&#30340;&#32435;&#31859;&#23610;&#24230;&#35013;&#32622;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20854;&#20013;&#21253;&#25324;&#38598;&#25104;&#24863;&#24212;&#12289;&#35745;&#31639;&#12289;&#36890;&#20449;&#12289;&#25968;&#25454;&#21644;&#33021;&#37327;&#23384;&#20648;&#33021;&#21147;&#12290;&#22312;&#20154;&#20307;&#24515;&#34880;&#31649;&#31995;&#32479;&#20013;&#65292;&#36825;&#20123;&#35013;&#32622;&#34987;&#35774;&#24819;&#20026;&#34987;&#21160;&#27969;&#21160;&#24182;&#25345;&#32493;&#24863;&#30693;&#20197;&#20415;&#26816;&#27979;&#35786;&#26029;&#24863;&#20852;&#36259;&#30340;&#20107;&#20214;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20107;&#20214;&#30340;&#29289;&#29702;&#20301;&#32622;&#65288;&#22914;&#36523;&#20307;&#21306;&#22495;&#65289;&#20998;&#37197;&#32473;&#23427;&#20204;&#65292;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#21040;&#36825;&#20123;&#20107;&#20214;&#30340;&#35786;&#26029;&#20215;&#20540;&#65292;&#36825;&#26159;&#27969;&#23548;&#21521;&#23450;&#20301;&#30340;&#20027;&#35201;&#21629;&#39064;&#12290;&#24403;&#21069;&#30340;&#27969;&#23548;&#21521;&#23450;&#20301;&#26041;&#27861;&#23384;&#22312;&#23450;&#20301;&#31934;&#24230;&#20302;&#21644;&#26080;&#27861;&#22312;&#25972;&#20010;&#24515;&#34880;&#31649;&#31995;&#32479;&#20869;&#26412;&#22320;&#21270;&#20107;&#20214;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#36827;&#34892;&#23450;&#20301;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#20301;&#31934;&#24230;&#21644;&#35206;&#30422;&#33539;&#22260;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific advancements in nanotechnology and advanced materials are paving the way toward nanoscale devices for in-body precision medicine; comprising integrated sensing, computing, communication, data and energy storage capabilities. In the human cardiovascular system, such devices are envisioned to be passively flowing and continuously sensing for detecting events of diagnostic interest. The diagnostic value of detecting such events can be enhanced by assigning to them their physical locations (e.g., body region), which is the main proposition of flow-guided localization. Current flow-guided localization approaches suffer from low localization accuracy and they are by-design unable to localize events within the entire cardiovascular system. Toward addressing this issue, we propose the utilization of Graph Neural Networks (GNNs) for this purpose, and demonstrate localization accuracy and coverage enhancements of our proposal over the existing State of the Art (SotA) approaches. Based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#31216;&#22810;&#39033;&#24335;&#20195;&#25968;&#30340;&#24037;&#20855;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#19988;&#20307;&#31995;&#32467;&#26500;&#22823;&#23567;&#19981;&#21464;&#30340;GNNs&#65292;&#23384;&#22312;&#19968;&#23545;&#38750;&#21516;&#26500;&#26681;&#26641;&#22312;&#20219;&#24847;&#36845;&#20195;&#27425;&#25968;&#20869;&#26080;&#27861;&#34987;&#21306;&#20998;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#30340;GNNs&#21482;&#38656;&#20004;&#27425;&#36845;&#20195;&#21363;&#21487;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22914;&#26524;&#20801;&#35768;&#38750;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#21017;&#22312;&#20004;&#27425;&#36845;&#20195;&#20869;&#65292;&#21333;&#20010;&#31070;&#32463;&#20803;&#24863;&#30693;&#22120;&#21487;&#20197;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#26641;&#30340;&#26681;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.04661</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the power of graph neural networks and the role of the activation function. (arXiv:2307.04661v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#31216;&#22810;&#39033;&#24335;&#20195;&#25968;&#30340;&#24037;&#20855;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#19988;&#20307;&#31995;&#32467;&#26500;&#22823;&#23567;&#19981;&#21464;&#30340;GNNs&#65292;&#23384;&#22312;&#19968;&#23545;&#38750;&#21516;&#26500;&#26681;&#26641;&#22312;&#20219;&#24847;&#36845;&#20195;&#27425;&#25968;&#20869;&#26080;&#27861;&#34987;&#21306;&#20998;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#30340;GNNs&#21482;&#38656;&#20004;&#27425;&#36845;&#20195;&#21363;&#21487;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22914;&#26524;&#20801;&#35768;&#38750;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#21017;&#22312;&#20004;&#27425;&#36845;&#20195;&#20869;&#65292;&#21333;&#20010;&#31070;&#32463;&#20803;&#24863;&#30693;&#22120;&#21487;&#20197;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#26641;&#30340;&#26681;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34920;&#36798;&#33021;&#21147;&#30340;&#26032;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#12289;&#20854;&#20307;&#31995;&#32467;&#26500;&#22823;&#23567;&#19981;&#38543;&#22270;&#36755;&#20837;&#22823;&#23567;&#22686;&#38271;&#30340;GNNs&#65292;&#23384;&#22312;&#19968;&#23545;&#28145;&#24230;&#20026;&#20108;&#30340;&#38750;&#21516;&#26500;&#26681;&#26641;&#65292;&#20351;&#24471;GNNs&#22312;&#20219;&#24847;&#36845;&#20195;&#27425;&#25968;&#20869;&#26080;&#27861;&#21306;&#20998;&#23427;&#20204;&#30340;&#26681;&#33410;&#28857;&#12290;&#35777;&#26126;&#20381;&#36182;&#20110;&#23545;&#31216;&#22810;&#39033;&#24335;&#20195;&#25968;&#30340;&#24037;&#20855;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24050;&#32463;&#30693;&#36947;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#26080;&#30028;GNNs&#65288;&#20854;&#22823;&#23567;&#20801;&#35768;&#38543;&#22270;&#22823;&#23567;&#25913;&#21464;&#65289;&#21482;&#38656;&#20004;&#27425;&#36845;&#20195;&#21363;&#21487;&#21306;&#20998;&#36825;&#20123;&#39030;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#26377;&#30028;&#22823;&#23567;&#21644;&#26080;&#30028;&#22823;&#23567;&#30340;GNNs&#20043;&#38388;&#23384;&#22312;&#20005;&#26684;&#30340;&#20998;&#31163;&#65292;&#22238;&#31572;&#20102; [Grohe, 2021] &#25552;&#20986;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#20801;&#35768;&#38750;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#21017;&#22312;&#20004;&#27425;&#36845;&#20195;&#20013;&#65292;&#21333;&#20010;&#31070;&#32463;&#20803;&#24863;&#30693;&#22120;&#21487;&#20197;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#26641;&#30340;&#26681;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we present new results about the expressivity of Graph Neural Networks (GNNs). We prove that for any GNN with piecewise polynomial activations, whose architecture size does not grow with the graph input sizes, there exists a pair of non-isomorphic rooted trees of depth two such that the GNN cannot distinguish their root vertex up to an arbitrary number of iterations. The proof relies on tools from the algebra of symmetric polynomials. In contrast, it was already known that unbounded GNNs (those whose size is allowed to change with the graph sizes) with piecewise polynomial activations can distinguish these vertices in only two iterations. Our results imply a strict separation between bounded and unbounded size GNNs, answering an open question formulated by [Grohe, 2021]. We next prove that if one allows activations that are not piecewise polynomial, then in two iterations a single neuron perceptron can distinguish the root vertices of any pair of nonisomorphic trees of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#26469;&#24809;&#32602;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#20998;&#21106;&#32467;&#26524;&#19982;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#20855;&#26377;&#22266;&#26377;&#20960;&#20309;&#29305;&#24449;&#30340;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.03137</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#20998;&#21106;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images. (arXiv:2307.03137v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#26469;&#24809;&#32602;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#20998;&#21106;&#32467;&#26524;&#19982;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#20855;&#26377;&#22266;&#26377;&#20960;&#20309;&#29305;&#24449;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#20998;&#21106;&#32593;&#32476;&#26102;&#65292;&#32593;&#32476;&#24182;&#27809;&#26377;&#26126;&#30830;&#34987;&#35201;&#27714;&#23398;&#20064;&#22270;&#20687;&#30340;&#20840;&#23616;&#19981;&#21464;&#24615;&#65292;&#22914;&#23545;&#35937;&#30340;&#24418;&#29366;&#21644;&#22810;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#19981;&#21464;&#24615;&#32435;&#20837;&#32593;&#32476;&#35757;&#32451;&#20013;&#21487;&#33021;&#26377;&#21161;&#20110;&#25913;&#21892;&#21508;&#31181;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#24403;&#23427;&#20204;&#26159;&#38656;&#35201;&#20998;&#21106;&#30340;&#23545;&#35937;&#30340;&#22266;&#26377;&#29305;&#24615;&#26102;&#12290;&#26412;&#25991;&#20197;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#30340;&#20998;&#21106;&#20026;&#20363;&#65292;&#36825;&#20123;&#34880;&#31649;&#30001;&#20110;&#20154;&#20307;&#35299;&#21078;&#23398;&#65292;&#36890;&#24120;&#22312;&#36523;&#20307;&#20013;&#20197;&#29305;&#23450;&#30340;&#20960;&#20309;&#24418;&#29366;&#20986;&#29616;&#65292;&#24182;&#22312;2D CT&#22270;&#20687;&#19978;&#20027;&#35201;&#21576;&#29616;&#20026;&#22278;&#24418;&#23545;&#35937;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#24809;&#32602;&#22320;&#38754;&#30495;&#23454;&#20540;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#24322;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#20998;&#21106;&#32593;&#32476;&#35774;&#35745;&#19981;&#21516;&#65292;&#20808;&#21069;&#30340;&#35774;&#35745;&#26159;&#23558;&#38408;&#20540;&#28388;&#27874;&#24212;&#29992;&#20110;&#39044;&#27979;&#22270;&#20687;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation networks are not explicitly imposed to learn global invariants of an image, such as the shape of an object and the geometry between multiple objects, when they are trained with a standard loss function. On the other hand, incorporating such invariants into network training may help improve performance for various segmentation tasks when they are the intrinsic characteristics of the objects to be segmented. One example is segmentation of aorta and great vessels in computed tomography (CT) images where vessels are found in a particular geometry in the body due to the human anatomy and they mostly seem as round objects on a 2D CT image. This paper addresses this issue by introducing a new topology-aware loss function that penalizes topology dissimilarities between the ground truth and prediction through persistent homology. Different from the previously suggested segmentation network designs, which apply the threshold filtration on a likelihood function of the prediction map 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#21644;&#35823;&#24046;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#27668;&#27873;&#24341;&#36215;&#30340;&#30913;&#22330;&#27874;&#21160;&#65292;&#37325;&#24314;&#30005;&#35299;&#36807;&#31243;&#20013;&#30340;&#27668;&#27873;&#20998;&#24067;&#21644;&#30005;&#23548;&#29575;&#22270;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02496</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#21644;&#35823;&#24046;&#25193;&#25955;&#23398;&#20064;&#21033;&#29992;&#23548;&#30005;&#22270;&#37325;&#24314;&#27668;&#27873;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion. (arXiv:2307.02496v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#21644;&#35823;&#24046;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#27668;&#27873;&#24341;&#36215;&#30340;&#30913;&#22330;&#27874;&#21160;&#65292;&#37325;&#24314;&#30005;&#35299;&#36807;&#31243;&#20013;&#30340;&#27668;&#27873;&#20998;&#24067;&#21644;&#30005;&#23548;&#29575;&#22270;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#35299;&#26159;&#29615;&#20445;&#30340;&#27682;&#27668;&#29983;&#20135;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#65292;&#20294;&#26159;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#27668;&#27873;&#20250;&#38459;&#30861;&#21453;&#24212;&#65292;&#38477;&#20302;&#30005;&#27744;&#25928;&#29575;&#65292;&#22686;&#21152;&#33021;&#37327;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27668;&#27873;&#20250;&#23548;&#33268;&#30005;&#27744;&#20869;&#37096;&#30340;&#30005;&#23548;&#29575;&#21457;&#29983;&#21464;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#21608;&#22260;&#20135;&#29983;&#24863;&#24212;&#30913;&#22330;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#22806;&#37096;&#30913;&#20256;&#24863;&#22120;&#27979;&#37327;&#36825;&#20123;&#27668;&#27873;&#24341;&#36215;&#30340;&#30913;&#22330;&#27874;&#21160;&#65292;&#24182;&#27714;&#35299;Biot-Savart&#23450;&#24459;&#30340;&#21453;&#38382;&#39064;&#65292;&#21487;&#20197;&#20272;&#35745;&#30005;&#27744;&#20869;&#30340;&#30005;&#23548;&#29575;&#65292;&#20174;&#32780;&#24471;&#21040;&#27668;&#27873;&#30340;&#22823;&#23567;&#21644;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#20165;&#20973;&#20960;&#20010;&#24863;&#24212;&#30913;&#22330;&#27979;&#37327;&#20540;&#30830;&#23450;&#39640;&#20998;&#36776;&#29575;&#30005;&#23548;&#29575;&#22270;&#26159;&#19968;&#20010;&#30149;&#24577;&#21453;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#37325;&#24314;&#30005;&#23548;&#29575;&#22330;&#12290;&#25105;&#20204;&#30340;&#23450;&#24615;&#32467;&#26524;&#21644;&#20351;&#29992;&#38543;&#26426;&#35823;&#24046;&#25193;&#25955;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;Tikhonov&#27491;&#21017;&#21270;&#30456;&#27604;&#65292;INN&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles generated during the process hinder reactions, reduce cell efficiency, and increase energy consumption. Additionally, these gas bubbles cause changes in the conductivity inside the cell, resulting in corresponding variations in the induced magnetic field around the cell. Therefore, measuring these gas bubble-induced magnetic field fluctuations using external magnetic sensors and solving the inverse problem of Biot-Savart Law allows for estimating the conductivity in the cell and, thus, bubble size and location. However, determining high-resolution conductivity maps from only a few induced magnetic field measurements is an ill-posed inverse problem. To overcome this, we exploit Invertible Neural Networks (INNs) to reconstruct the conductivity field. Our qualitative results and quantitative evaluation using random error diffusion show that INN achieves far superior performance compared to Tikhonov regularizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24815;&#24615;&#23548;&#33322;&#39046;&#22495;&#20013;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#19981;&#21516;&#36710;&#36742;&#25805;&#20316;&#39046;&#22495;&#30340;&#30740;&#31350;&#12289;&#28388;&#27874;&#21442;&#25968;&#23398;&#20064;&#30340;&#25913;&#36827;&#20197;&#21450;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#21644;&#21435;&#22122;&#26041;&#27861;&#12290;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;: &#24815;&#24615;&#23548;&#33322;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;</title><link>http://arxiv.org/abs/2307.00014</link><description>&lt;p&gt;
&#24815;&#24615;&#23548;&#33322;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Inertial Navigation Meets Deep Learning: A Survey of Current Trends and Future Directions. (arXiv:2307.00014v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24815;&#24615;&#23548;&#33322;&#39046;&#22495;&#20013;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#19981;&#21516;&#36710;&#36742;&#25805;&#20316;&#39046;&#22495;&#30340;&#30740;&#31350;&#12289;&#28388;&#27874;&#21442;&#25968;&#23398;&#20064;&#30340;&#25913;&#36827;&#20197;&#21450;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#21644;&#21435;&#22122;&#26041;&#27861;&#12290;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;: &#24815;&#24615;&#23548;&#33322;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24815;&#24615;&#20256;&#24863;&#22312;&#35768;&#22810;&#24212;&#29992;&#21644;&#24179;&#21488;&#20013;&#34987;&#20351;&#29992;&#65292;&#20174;&#26234;&#33021;&#25163;&#26426;&#31561;&#26085;&#24120;&#35774;&#22791;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31561;&#22797;&#26434;&#35774;&#22791;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24815;&#24615;&#20256;&#24863;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#36825;&#26159;&#30001;&#20110;&#39640;&#25928;&#30340;&#35745;&#31639;&#30828;&#20214;&#30340;&#21457;&#23637;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21487;&#33719;&#24471;&#24615;&#12290;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#34987;&#29992;&#20110;&#24378;&#21270;&#22522;&#20110;&#27169;&#22411;&#30340;&#23548;&#33322;&#21644;&#20256;&#24863;&#22120;&#34701;&#21512;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20837;&#32508;&#36848;&#12290;&#25105;&#20204;&#20998;&#21035;&#32771;&#23519;&#20102;&#27599;&#20010;&#36710;&#36742;&#25805;&#20316;&#39046;&#22495;&#65292;&#21253;&#25324;&#38470;&#22320;&#12289;&#31354;&#20013;&#21644;&#28023;&#27915;&#12290;&#27599;&#20010;&#39046;&#22495;&#20998;&#20026;&#32431;&#24815;&#24615;&#36827;&#23637;&#21644;&#22522;&#20110;&#28388;&#27874;&#21442;&#25968;&#23398;&#20064;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#29992;&#20110;&#26657;&#20934;&#21644;&#21435;&#22122;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#25972;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#24120;&#29992;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inertial sensing is used in many applications and platforms, ranging from day-to-day devices such as smartphones to very complex ones such as autonomous vehicles. In recent years, the development of machine learning and deep learning techniques has increased significantly in the field of inertial sensing. This is due to the development of efficient computing hardware and the accessibility of publicly available sensor data. These data-driven approaches are used to empower model-based navigation and sensor fusion algorithms. This paper provides an in-depth review of those deep learning methods. We examine separately, each vehicle operation domain including land, air, and sea. Each domain is divided into pure inertial advances and improvements based on filter parameters learning. In addition, we review deep learning approaches for calibrating and denoising inertial sensors. Throughout the paper, we discuss these trends and future directions. We also provide statistics on the commonly used
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16838</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Kernel Ridge Regression with Gradient-Based Optimization Methods. (arXiv:2306.16838v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#26159;&#32447;&#24615;&#23725;&#22238;&#24402;&#30340;&#38750;&#32447;&#24615;&#25512;&#24191;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KRR&#30446;&#26631;&#20989;&#25968;&#30340;&#31561;&#20215;&#24418;&#24335;&#65292;&#20026;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#21644;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#25171;&#24320;&#20102;&#21487;&#33021;&#12290;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#8212;&#8212;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#35753;&#25105;&#20204;&#33021;&#22815;&#22312;KGF&#21644;KRR&#20043;&#38388;&#29702;&#35770;&#19978;&#30028;&#23450;&#24046;&#24322;&#12290;&#25105;&#20204;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#23558;KRR&#27867;&#21270;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;KGF&#21644;KRR&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#29992;&#36825;&#20123;&#24809;&#32602;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#20351;&#29992;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#65288;&#20063;&#31216;&#20026;&#22352;&#26631;&#19979;&#38477;&#65289;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel ridge regression, KRR, is a non-linear generalization of linear ridge regression. Here, we introduce an equivalent formulation of the objective function of KRR, opening up both for using other penalties than the ridge penalty and for studying kernel ridge regression from the perspective of gradient descent. Using a continuous-time perspective, we derive a closed-form solution, kernel gradient flow, KGF, with regularization through early stopping, which allows us to theoretically bound the differences between KGF and KRR. We generalize KRR by replacing the ridge penalty with the $\ell_1$ and $\ell_\infty$ penalties and utilize the fact that analogously to the similarities between KGF and KRR, the solutions obtained when using these penalties are very similar to those obtained from forward stagewise regression (also known as coordinate descent) and sign gradient descent in combination with early stopping. Thus the need for computationally heavy proximal gradient descent algorithms
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#21160;&#37327;&#26469;&#25552;&#21319;FedAvg&#21644;SCAFFOLD&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#24341;&#20837;&#21160;&#37327;&#21487;&#20197;&#20351;FedAvg&#22312;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20551;&#35774;&#19979;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2306.16504</link><description>&lt;p&gt;
&#21160;&#37327;&#31616;&#21333;&#32780;&#21487;&#35777;&#23454;&#22320;&#22686;&#24378;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Momentum Benefits Non-IID Federated Learning Simply and Provably. (arXiv:2306.16504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#21160;&#37327;&#26469;&#25552;&#21319;FedAvg&#21644;SCAFFOLD&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#24341;&#20837;&#21160;&#37327;&#21487;&#20197;&#20351;FedAvg&#22312;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20551;&#35774;&#19979;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#30340;&#24378;&#22823;&#33539;&#20363;&#65292;&#20294;&#30001;&#20110;&#19981;&#21487;&#38752;&#30340;&#32593;&#32476;&#36830;&#25509;&#12289;&#32531;&#24930;&#30340;&#36890;&#20449;&#20197;&#21450;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#23427;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;FedAvg&#21644;SCAFFOLD&#26159;&#20004;&#31181;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#22522;&#26412;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;FedAvg&#22312;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#36890;&#20449;&#20043;&#21069;&#37319;&#29992;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#65292;&#32780;SCAFFOLD&#22312;&#20854;&#26412;&#22320;&#26356;&#26032;&#20013;&#32500;&#25252;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#25511;&#21046;&#21464;&#37327;&#20197;&#34917;&#20607;&#8220;&#23458;&#25143;&#31471;&#28418;&#31227;&#8221;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#23545;&#31639;&#27861;&#32467;&#26500;&#36827;&#34892;&#19981;&#20999;&#23454;&#38469;&#30340;&#35843;&#25972;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#26377;&#30028;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21160;&#37327;&#26469;&#22686;&#24378;FedAvg&#21644;SCAFFOLD&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#24403;&#25152;&#26377;&#23458;&#25143;&#31471;&#21442;&#19982;&#35757;&#32451;&#36807;&#31243;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24341;&#20837;&#21160;&#37327;&#21487;&#20197;&#20351;FedAvg&#22312;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20551;&#35774;&#19979;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a powerful paradigm for large-scale machine learning, but it faces significant challenges due to unreliable network connections, slow communication, and substantial data heterogeneity across clients. FedAvg and SCAFFOLD are two fundamental algorithms to address these challenges. In particular, FedAvg employs multiple local updates before communicating with a central server, while SCAFFOLD maintains a control variable on each client to compensate for "client drift" in its local updates. Various methods have been proposed in literature to enhance the convergence of these two algorithms, but they either make impractical adjustments to algorithmic structure, or rely on the assumption of bounded data heterogeneity.  This paper explores the utilization of momentum to enhance the performance of FedAvg and SCAFFOLD. When all clients participate in the training process, we demonstrate that incorporating momentum allows FedAvg to converge without relying on the assumption o
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10698</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#35760;&#24518;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10698
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21463;&#21040;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38480;&#21046;&#65292;&#20005;&#37325;&#20381;&#36182;&#19982;&#29615;&#22659;&#30340;&#22810;&#27425;&#20132;&#20114;&#25165;&#33021;&#33719;&#24471;&#20934;&#30830;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20284;&#20046;&#20381;&#36182;&#28023;&#39532;&#20307;&#20174;&#36807;&#21435;&#26377;&#20851;&#20219;&#21153;&#30340;&#32463;&#21382;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#25351;&#23548;&#20854;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20026;&#20195;&#29702;&#35774;&#35745;&#31867;&#20284;&#28023;&#39532;&#20307;&#30340;&#27169;&#22359;&#20197;&#23558;&#36807;&#21435;&#30340;&#32463;&#21382;&#34701;&#20837;&#26082;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#28041;&#21450;&#36873;&#25321;&#24403;&#21069;&#20219;&#21153;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#36825;&#20123;&#32463;&#39564;&#19982;&#20915;&#31574;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#26816;&#32034;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09222</link><description>&lt;p&gt;
&#38543;&#26426;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization. (arXiv:2306.09222v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09222
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;f-&#25955;&#24230;&#30340;&#21551;&#21457;&#65292;&#24050;&#30693;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#25913;&#36827;&#30340;&#27867;&#21270;&#20445;&#35777;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21152;&#26435;&#26041;&#26696;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#21487;&#20197;&#19982;&#35768;&#22810;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;SGD&#21644;Adam&#65289;&#32467;&#21512;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#21644;&#39046;&#22495;&#36866;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;DomainBed&#21644;Tabular&#20998;&#31867;&#22522;&#20934;&#19978;&#20998;&#21035;&#27604;&#29616;&#26377;&#26368;&#20339;&#32467;&#26524;&#25552;&#21319;&#20102;0.7%&#21644;1.44%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;BERT&#22312;GLUE&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.94%&#65292;&#23558;ViT&#22312;ImageNet-1K&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.01%&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#39044;&#31034;&#30528;&#23427;&#22312;&#25913;&#21892;&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a re-weighted gradient descent technique for boosting the performance of deep neural networks, which involves importance weighting of data points during each optimization step. Our approach is inspired by distributionally robust optimization with f-divergences, which has been known to result in models with improved generalization guarantees. Our re-weighting scheme is simple, computationally efficient, and can be combined with many popular optimization algorithms such as SGD and Adam. Empirically, we demonstrate the superiority of our approach on various tasks, including supervised learning, domain adaptation. Notably, we obtain improvements of +0.7% and +1.44% over SOTA on DomainBed and Tabular classification benchmarks, respectively. Moreover, our algorithm boosts the performance of BERT on GLUE benchmarks by +1.94%, and ViT on ImageNet-1K by +1.01%. These results demonstrate the effectiveness of the proposed approach, indicating its potential for improving performance in 
&lt;/p&gt;</description></item><item><title>BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.03530</link><description>&lt;p&gt;
BackpropTools: &#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control. (arXiv:2306.03530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03530
&lt;/p&gt;
&lt;p&gt;
BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20135;&#29983;&#20986;&#20855;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#35757;&#32451;&#26102;&#38388;&#36807;&#38271;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#23454;&#26102;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#30340;&#32570;&#20047;&#38480;&#21046;&#20102;&#23398;&#20064;&#31574;&#30053;&#22312;&#23454;&#38469;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BackpropTools&#65292;&#19968;&#31181;&#20381;&#36182;&#24615;-free&#12289;header-only&#12289;pure C++&#30340;&#28145;&#24230;&#30417;&#30563;&#21644;&#24378;&#21270;&#23398;&#20064;&#24211;&#12290;&#21033;&#29992;&#26368;&#36817;C++&#26631;&#20934;&#30340;&#27169;&#26495;&#20803;&#32534;&#31243;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20197;&#30001;&#32534;&#35793;&#22120;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#12290;&#20854;&#26032;&#39062;&#30340;&#26550;&#26500;&#20801;&#35768;BackpropTools&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#20174;HPC&#38598;&#32676;&#12289;&#24037;&#20316;&#31449;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#21040;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#25163;&#34920;&#21644;&#24494;&#25511;&#21046;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30001;&#20110;RL&#31639;&#27861;&#19982;&#27169;&#25311;&#29615;&#22659;&#30340;&#32039;&#23494;&#38598;&#25104;&#65292;BackpropTools&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#20351;&#20854;&#25104;&#20026;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (RL) has been demonstrated to yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing deep learning libraries. To address these challenges, we present BackpropTools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Leveraging the template meta-programming capabilities of recent C++ standards, we provide composable components that can be tightly integrated by the compiler. Its novel architecture allows BackpropTools to be used seamlessly on a heterogeneous set of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#23454;&#39564;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#28436;&#31034;&#20102;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03346</link><description>&lt;p&gt;
&#31283;&#23450;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;: &#31163;&#32447;&#30446;&#26631;&#36798;&#25104;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Contrastive RL: Techniques for Offline Goal Reaching. (arXiv:2306.03346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#23454;&#39564;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#28436;&#31034;&#20102;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24050;&#32463;&#24320;&#21457;&#20102;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24378;&#21270;&#23398;&#20064;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#33258;&#30417;&#30563;&#38382;&#39064;&#65306;&#23398;&#20064;&#36798;&#21040;&#20219;&#20309;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#25351;&#23450;&#30340;&#22870;&#21169;&#25110;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#24314;&#31435;&#33258;&#30417;&#30563;&#22522;&#30784;&#23454;&#38469;&#19978;&#38754;&#20020;&#30528;&#19968;&#20123;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#27492;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#21078;&#26512;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#19982;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36890;&#36807;&#36825;&#20123;&#35774;&#35745;&#20915;&#31574;&#65292;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#20854;&#20013;&#20219;&#21153;&#30001;&#35757;&#32451;&#21518;&#25552;&#20379;&#30340;&#21333;&#20010;&#30446;&#26631;&#22270;&#20687;&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the same way that the computer vision (CV) and natural language processing (NLP) communities have developed self-supervised methods, reinforcement learning (RL) can be cast as a self-supervised problem: learning to reach any goal, without requiring human-specified rewards or labels. However, actually building a self-supervised foundation for RL faces some important challenges. Building on prior contrastive approaches to this RL problem, we conduct careful ablation experiments and discover that a shallow and wide architecture, combined with careful weight initialization and data augmentation, can significantly boost the performance of these contrastive RL approaches on challenging simulated benchmarks. Additionally, we demonstrate that, with these design decisions, contrastive approaches can solve real-world robotic manipulation tasks, with tasks being specified by a single goal image provided after training.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22343;&#21248;&#30340;$k$-dag&#24191;&#25773;&#27169;&#22411;&#65292;&#30830;&#23450;&#20102;&#19982;$p$&#21644;$k$&#26377;&#20851;&#30340;&#38408;&#20540;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#22810;&#25968;&#35268;&#21017;&#30340;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01727</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#36882;&#24402;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#30340;&#24191;&#25773;
&lt;/p&gt;
&lt;p&gt;
Broadcasting in random recursive dags. (arXiv:2306.01727v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01727
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22343;&#21248;&#30340;$k$-dag&#24191;&#25773;&#27169;&#22411;&#65292;&#30830;&#23450;&#20102;&#19982;$p$&#21644;$k$&#26377;&#20851;&#30340;&#38408;&#20540;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#22810;&#25968;&#35268;&#21017;&#30340;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#22343;&#21248;&#30340;$k$-dag&#36890;&#36807;&#20174;&#29616;&#26377;&#33410;&#28857;&#20013;&#22343;&#21248;&#38543;&#26426;&#36873;&#25321;$k$&#20010;&#29238;&#33410;&#28857;&#26469;&#25512;&#24191;&#22343;&#21248;&#30340;&#38543;&#26426;&#36882;&#24402;&#26641;&#12290;&#23427;&#20197;$k$&#20010;&#8220;&#26681;&#8221;&#24320;&#22987;&#12290;&#27599;&#20010;$k$&#20010;&#26681;&#33410;&#28857;&#37117;&#34987;&#20998;&#37197;&#19968;&#20010;&#20301;&#12290;&#36825;&#20123;&#20301;&#36890;&#36807;&#19968;&#20010;&#22024;&#26434;&#30340;&#20449;&#36947;&#20256;&#25773;&#12290;&#27599;&#20010;&#29238;&#33410;&#28857;&#30340;&#20301;&#37117;&#20197;&#27010;&#29575;$p$&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#36827;&#34892;&#22823;&#22810;&#25968;&#34920;&#20915;&#12290;&#24403;&#25152;&#26377;&#33410;&#28857;&#37117;&#25509;&#25910;&#21040;&#23427;&#20204;&#30340;&#20301;&#21518;&#65292;$k$-dag&#34987;&#26174;&#31034;&#65292;&#19981;&#35782;&#21035;&#26681;&#33410;&#28857;&#12290;&#30446;&#26631;&#26159;&#20272;&#35745;&#25152;&#26377;&#26681;&#33410;&#28857;&#20013;&#30340;&#22823;&#22810;&#25968;&#20301;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;$p$&#30340;&#38408;&#20540;&#65292;&#20316;&#20026;&#19968;&#20010;&#20851;&#20110;$k$&#30340;&#20989;&#25968;&#65292;&#20351;&#24471;&#25152;&#26377;&#33410;&#28857;&#30340;&#22823;&#22810;&#25968;&#35268;&#21017;&#20135;&#29983;&#38169;&#35823;$c+o(1)$&#30340;&#27010;&#29575;&#23567;&#20110;$1/2$&#12290;&#22312;&#38408;&#20540;&#20197;&#19978;&#65292;&#22823;&#22810;&#25968;&#35268;&#21017;&#30340;&#38169;&#35823;&#27010;&#29575;&#20026;$1/2+o(1)$&#12290;
&lt;/p&gt;
&lt;p&gt;
A uniform $k$-{\sc dag} generalizes the uniform random recursive tree by picking $k$ parents uniformly at random from the existing nodes. It starts with $k$ ''roots''. Each of the $k$ roots is assigned a bit. These bits are propagated by a noisy channel. The parents' bits are flipped with probability $p$, and a majority vote is taken. When all nodes have received their bits, the $k$-{\sc dag} is shown without identifying the roots. The goal is to estimate the majority bit among the roots. We identify the threshold for $p$ as a function of $k$ below which the majority rule among all nodes yields an error $c+o(1)$ with $c&lt;1/2$. Above the threshold the majority rule errs with probability $1/2+o(1)$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#20248;&#21270;&#25439;&#22833;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00742</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Going Deeper with Spectral Embeddings. (arXiv:2306.00742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#20248;&#21270;&#25439;&#22833;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#22320;&#22788;&#29702;&#28023;&#37327;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#65292;&#31185;&#23398;&#23478;&#20204;&#37319;&#29992;&#34920;&#31034;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#19968;&#20123;&#24213;&#23618;&#36816;&#31639;&#30340;&#35889;&#20998;&#35299;&#20043;&#38388;&#23637;&#29616;&#20986;&#26126;&#26174;&#30340;&#32852;&#31995;&#12290;&#22312;&#21382;&#21490;&#19978;&#65292;&#26159;&#36890;&#36807;&#22312;&#25968;&#25454;&#30340;&#39030;&#37096;&#26500;&#24314;&#22270;&#24418;&#26469;&#24314;&#31435;&#26126;&#30830;&#30340;&#35889;&#23884;&#20837;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#26500;&#24314;&#30340;&#65292;&#36825;&#23558;&#23548;&#33268;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#20197;&#20248;&#21270;&#22522;&#26412;&#21464;&#20998;&#25439;&#22833;&#30340;&#31639;&#27861;&#65292;&#23427;&#20204;&#20135;&#29983;&#20102;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#26469;&#22312;&#19968;&#27493;&#20013;&#29983;&#25104;&#26032;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make sense of millions of raw data and represent them efficiently, practitioners rely on representation learning. Recently, deep connections have been shown between these approaches and the spectral decompositions of some underlying operators. Historically, explicit spectral embeddings were built from graphs constructed on top of the data. In contrast, we propose two new methods to build spectral embeddings: one based on functional analysis principles and kernel methods, which leads to algorithms with theoretical guarantees, and the other based on deep networks trained to optimize principled variational losses, which yield practically efficient algorithms. Furthermore, we provide a new sampling algorithm that leverages learned representations to generate new samples in a single step.
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#40784;&#22534;&#26632;&#20013;&#30340;&#36793;&#38469;&#29305;&#24449;&#27010;&#29575;&#27979;&#24230;&#26469;&#36827;&#34892;&#39046;&#22495;&#24191;&#20041;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;N-BEATS&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.15196</link><description>&lt;p&gt;
&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS
&lt;/p&gt;
&lt;p&gt;
Feature-aligned N-BEATS with Sinkhorn divergence. (arXiv:2305.15196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#40784;&#22534;&#26632;&#20013;&#30340;&#36793;&#38469;&#29305;&#24449;&#27010;&#29575;&#27979;&#24230;&#26469;&#36827;&#34892;&#39046;&#22495;&#24191;&#20041;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;N-BEATS&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#24191;&#20041;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#26159;N-BEATS&#30340;&#38750;&#24179;&#20961;&#25193;&#23637;&#65292;&#37319;&#29992;&#20102;&#21452;&#37325;&#27531;&#24046;&#21472;&#21152;&#21407;&#21017;&#65288;Oreshkin&#31561;&#20154;[42]&#65289;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#22260;&#32469;&#30528;&#30001;N-BEATS&#27599;&#20010;&#22534;&#26632;&#30340;&#27531;&#24046;&#21644;&#29305;&#24449;&#25552;&#21462;&#31639;&#23376;&#30340;&#22797;&#26434;&#32452;&#21512;&#20135;&#29983;&#30340;&#36793;&#38469;&#29305;&#24449;&#27010;&#29575;&#27979;&#24230;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#36817;&#20284;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#65288;Sinkhorn&#36317;&#31163;&#65289;&#23558;&#23427;&#20204;&#22534;&#21472;&#22320;&#23545;&#40784;&#12290;&#35757;&#32451;&#25439;&#22833;&#30001;&#26469;&#33258;&#22810;&#20010;&#28304;&#22495;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;&#21363;&#39044;&#27979;&#25439;&#22833;&#65289;&#21644;Sinkhorn&#36317;&#31163;&#35745;&#31639;&#30340;&#23545;&#40784;&#25439;&#22833;&#32452;&#25104;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#20010;&#28304;&#25968;&#25454;&#24207;&#21015;&#20013;&#22534;&#21472;&#22320;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;N-BEATS&#30340;&#21487;&#35299;&#37322;&#35774;&#35745;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Feature-aligned N-BEATS as a domain-generalized time series forecasting model. It is a nontrivial extension of N-BEATS with doubly residual stacking principle (Oreshkin et al.[42]) into a representation learning framework. In particular, it revolves around marginal feature probability measures induced by the intricate composition of residual and feature extracting operators of N-BEATS in each stack and aligns them stack-wisely via an approximate of an optimal transport distance referred to as the Sinkhorn divergence. The training loss consists of an empirical risk minimization from multiple source domains, i.e., forecasting loss, and an alignment loss calculated with the Sinkhorn divergence, which allows the model to learn invariant features stack-wisely across multiple source data sequences while retaining N-BEATS's interpretable design and forecasting power. Comprehensive experimental evaluations with ablation studies are provided and the corresponding results demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#38382;&#39064;&#65292;&#24341;&#20837;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;EEA&#26041;&#27861;&#21450;&#25552;&#20986;&#30340;&#29983;&#25104;&#30340;EEA&#65288;GEEA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#23454;&#29616;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#19988;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#30340;&#23454;&#20307;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14651</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#23454;&#20307;&#23545;&#40784;&#21450;&#36229;&#36234;&#65306;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Revisit and Outstrip Entity Alignment: A Perspective of Generative Models. (arXiv:2305.14651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#38382;&#39064;&#65292;&#24341;&#20837;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;EEA&#26041;&#27861;&#21450;&#25552;&#20986;&#30340;&#29983;&#25104;&#30340;EEA&#65288;GEEA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#23454;&#29616;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#19988;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#30340;&#23454;&#20307;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#12290;&#25105;&#20204;&#34920;&#26126;EEA&#26159;&#19968;&#20010;&#29305;&#27530;&#30340;&#38382;&#39064;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#31867;&#20284;&#20110;&#20856;&#22411;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#30446;&#26631;&#65292;&#22522;&#20110;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;EEA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20182;&#20204;&#19981;&#23436;&#25972;&#30340;&#30446;&#26631;&#38480;&#21046;&#20102;&#23454;&#20307;&#23545;&#40784;&#21644;&#23454;&#20307;&#21512;&#25104;&#65288;&#21363;&#29983;&#25104;&#26032;&#23454;&#20307;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#30340;EEA&#65288;abbr.&#65292;GEEA&#65289;&#26694;&#26550;&#21644;&#25552;&#20986;&#30340;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;M-VAE&#21487;&#20197;&#23558;&#19968;&#20010;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#23454;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;GEEA&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent embedding-based methods have achieved great successes on exploiting entity alignment from knowledge graph (KG) embeddings of multiple modals. In this paper, we study embedding-based entity alignment (EEA) from a perspective of generative models. We show that EEA is a special problem where the main objective is analogous to that in a typical generative model, based on which we theoretically prove the effectiveness of the recently developed generative adversarial network (GAN)-based EEA methods. We then reveal that their incomplete objective limits the capacity on both entity alignment and entity synthesis (i.e., generating new entities). We mitigate this problem by introducing a generative EEA (abbr., GEEA) framework with the proposed mutual variational autoencoder (M-VAE) as the generative model. M-VAE can convert an entity from one KG to another and generate new entities from random noise vectors. We demonstrate the power of GEEA with theoretical analysis and empirical experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11854</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577; Web &#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027; Web &#23548;&#33322;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20381;&#36182;&#25968;&#21313;&#20159;&#27425;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#24615;&#20132;&#20114;&#21644;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#21033;&#29992;&#26469;&#33258;&#20016;&#23500;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33073;&#26426;&#35757;&#32451;&#65292;&#29992;&#20110;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340; Web &#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;&#65292; WebGUM&#65292;&#23427;&#35266;&#23519;&#20102;&#32593;&#39029;&#25130;&#22270;&#21644; HTML &#39029;&#38754;&#65292;&#24182;&#36755;&#20986; Web &#23548;&#33322;&#25805;&#20316;&#65292;&#22914;&#21333;&#20987;&#21644;&#36755;&#20837;&#12290;WebGUM &#26159;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#22823;&#37327;&#30340;&#28436;&#31034;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312; MiniWoB &#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#36229;&#36807;&#20043;&#21069;&#26368;&#20339;&#33073;&#26426;&#26041;&#27861; 31.9% &#20197;&#19978;&#65292;&#25509;&#36817;&#23454;&#29616;&#22312;&#32447;&#20132;&#20114;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09859</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27969;&#30021;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#38750;&#24120;&#30456;&#20284;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#35805;&#35821;&#65292;&#22240;&#27492;&#21306;&#20998;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#20154;&#31867;&#20889;&#20316;&#30340;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#35780;&#35770;&#24182;&#27169;&#20223;&#26576;&#20123;&#20316;&#32773;&#21644;&#20154;&#29289;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340; logits&#65292;&#25110;&#38656;&#35201;&#21487;&#20197;&#20174;&#30446;&#26631;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#31181;&#40657;&#21283;&#23376;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#35266;&#23519;&#21040;&#29983;&#25104;&#25991;&#26412;&#22312;&#29983;&#25104;&#22120;&#30340;&#20284;&#28982;&#20989;&#25968;&#19979;&#26159;&#23616;&#37096;&#26368;&#20248;&#30340;&#65292;&#32780;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#21017;&#19981;&#26159;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#36739;&#23567;&#19988;&#37096;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65306;&#23427;&#20204;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#26816;&#27979;&#26469;&#33258;&#23567;&#22411;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#30456;&#21516;&#30340;&#35821;&#26009;&#24211;&#23545;&#26816;&#27979;&#24615;&#33021;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#27809;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20170;&#21518;&#21487;&#20197;&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#20351;&#29992;&#65292;</title><link>http://arxiv.org/abs/2305.03514</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#25913;&#21464;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Transform Computational Social Science?. (arXiv:2305.03514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#27809;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20170;&#21518;&#21487;&#20197;&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#20351;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#25104;&#21151;&#22320;&#22312;&#35768;&#22810;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#36827;&#34892;&#38646;-shot&#25805;&#20316;&#65288;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65289;&#12290;&#22914;&#26524;&#36825;&#31181;&#33021;&#21147;&#20063;&#36866;&#29992;&#20110;&#23545;&#35828;&#26381;&#21147;&#21644;&#25919;&#27835;&#24847;&#35782;&#24418;&#24577;&#31561;&#31038;&#20250;&#29616;&#35937;&#30340;&#32534;&#30721;&#65292;&#37027;&#20040;LLMs&#23601;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21464;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;(CSS)&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;CSS&#24037;&#20855;&#30340;&#36335;&#32447;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#20248;&#31168;&#30340;&#25552;&#31034;&#23454;&#36341;&#20197;&#21450;&#19968;&#20010;&#24191;&#27867;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#20197;&#27979;&#37327;13&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;24&#20010;&#20195;&#34920;&#24615;&#30340;CSS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;LLMs&#26080;&#27861;&#36229;&#36234;&#26368;&#20339;&#24494;&#35843;&#27169;&#22411;&#65292;&#20294;&#20173;&#28982;&#19982;&#20154;&#31867;&#36798;&#25104;&#20102;&#20844;&#24179;&#30340;&#21327;&#35758;&#27700;&#24179;&#12290;&#22312;&#33258;&#30001;&#24418;&#24335;&#30340;&#32534;&#30721;&#20219;&#21153;&#65288;&#29983;&#25104;&#65289;&#19978;&#65292;LLMs&#29983;&#25104;&#30340;&#35299;&#37322;&#24120;&#24120;&#36229;&#36807;&#20102;&#24037;&#20316;&#32773;&#30340;&#40644;&#37329;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#20170;&#22825;&#30340;LLMs&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#20174;&#26681;&#26412;&#19978;&#22686;&#24378;CSS&#30740;&#31350;&#27969;&#31243;&#65306;(1)&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#26080;&#32541;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SVD-QCQP&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#30340;&#39640;&#25928;&#20869;&#26680;&#23398;&#20064;&#23454;&#29616;&#65292;&#29992;&#20110;&#23398;&#20064;&#21322;&#20998;&#31163;&#26680;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07472</link><description>&lt;p&gt;
&#36890;&#29992;&#26680;&#23398;&#20064;&#30340;&#39640;&#25928;&#20984;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Convex Algorithms for Universal Kernel Learning. (arXiv:2304.07472v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SVD-QCQP&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#30340;&#39640;&#25928;&#20869;&#26680;&#23398;&#20064;&#23454;&#29616;&#65292;&#29992;&#20110;&#23398;&#20064;&#21322;&#20998;&#31163;&#26680;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26680;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#21462;&#20915;&#20110;&#23427;&#20204;&#33021;&#22815;&#20248;&#21270;&#30340;&#26680;&#38598;&#12290;&#29702;&#24819;&#30340;&#26680;&#38598;&#24212;&#35813;&#65306;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#65288;&#20197;&#20415;&#20110;&#21487;&#22788;&#29702;&#24615;&#65289;&#65307;&#22312;&#25152;&#26377;&#26680;&#38598;&#20013;&#23494;&#38598;&#65288;&#20197;&#20415;&#20110;&#40065;&#26834;&#24615;&#65289;&#65307;&#26159;&#36890;&#29992;&#30340;&#65288;&#20197;&#20415;&#20110;&#20934;&#30830;&#24615;&#65289;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#20351;&#29992;&#27491;&#23450;&#30697;&#38453;&#26469;&#21442;&#25968;&#21270;&#19968;&#31867;&#27491;&#21322;&#20998;&#31163;&#26680;&#12290;&#23613;&#31649;&#27492;&#31867;&#26680;&#33021;&#22815;&#28385;&#36275;&#25152;&#26377;&#19977;&#20010;&#26631;&#20934;&#65292;&#20294;&#20043;&#21069;&#29992;&#20110;&#20248;&#21270;&#27492;&#31867;&#26680;&#30340;&#31639;&#27861;&#20165;&#38480;&#20110;&#20998;&#31867;&#65292;&#24182;&#19988;&#36824;&#20381;&#36182;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#21322;&#20998;&#31163;&#26680;&#30340;&#38382;&#39064;&#20316;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;SVD-QCQP&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#20854;&#19982;&#20043;&#21069;&#22522;&#20110;SDP&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20869;&#26680;&#23398;&#20064;&#23454;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy and complexity of machine learning algorithms based on kernel optimization are determined by the set of kernels over which they are able to optimize. An ideal set of kernels should: admit a linear parameterization (for tractability); be dense in the set of all kernels (for robustness); be universal (for accuracy). Recently, a framework was proposed for using positive matrices to parameterize a class of positive semi-separable kernels. Although this class can be shown to meet all three criteria, previous algorithms for optimization of such kernels were limited to classification and furthermore relied on computationally complex Semidefinite Programming (SDP) algorithms. In this paper, we pose the problem of learning semiseparable kernels as a minimax optimization problem and propose a SVD-QCQP primal-dual algorithm which dramatically reduces the computational complexity as compared with previous SDP-based approaches. Furthermore, we provide an efficient implementation of thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#24067;&#20013;&#30693;&#35782;&#33976;&#39311;&#65288;IDKD&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33410;&#28857;&#38388;&#21516;&#36136;&#21270;&#25968;&#25454;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#26469;&#20174;&#27599;&#20010;&#33410;&#28857;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#30340;&#26631;&#31614;&#23558;&#20854;&#20256;&#36882;&#32473;&#20854;&#37051;&#23621;&#65292;&#20197;&#23454;&#29616;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#32422;&#26463;&#12290;&#22312;&#38750;i.i.d.&#25968;&#25454;&#38598;&#19978;&#65292;IDKD&#30340;&#24615;&#33021;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.04326</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#38750;IID&#25968;&#25454;&#21516;&#36136;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;&#20998;&#25955;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation for Decentralized Learning. (arXiv:2304.04326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#24067;&#20013;&#30693;&#35782;&#33976;&#39311;&#65288;IDKD&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33410;&#28857;&#38388;&#21516;&#36136;&#21270;&#25968;&#25454;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#26469;&#20174;&#27599;&#20010;&#33410;&#28857;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#30340;&#26631;&#31614;&#23558;&#20854;&#20256;&#36882;&#32473;&#20854;&#37051;&#23621;&#65292;&#20197;&#23454;&#29616;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#32422;&#26463;&#12290;&#22312;&#38750;i.i.d.&#25968;&#25454;&#38598;&#19978;&#65292;IDKD&#30340;&#24615;&#33021;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#23398;&#20064;&#20801;&#35768;&#22312;&#22810;&#20010;&#33410;&#28857;&#19978;&#20197;&#20998;&#25955;&#24335;&#26041;&#24335;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#25968;&#25454;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#33410;&#28857;&#38388;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#24067;&#20013;&#30693;&#35782;&#33976;&#39311;&#65288;IDKD&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#27492;&#25361;&#25112;&#12290;IDKD&#30340;&#30446;&#26631;&#26159;&#21516;&#36136;&#21270;&#33410;&#28857;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#34429;&#28982;&#36825;&#31181;&#25968;&#25454;&#21516;&#36136;&#21270;&#21487;&#20197;&#36890;&#36807;&#22312;&#33410;&#28857;&#20043;&#38388;&#20132;&#25442;&#25968;&#25454;&#26469;&#23454;&#29616;&#65292;&#20294;&#36825;&#26679;&#20250;&#29306;&#29298;&#38544;&#31169;&#12290;IDKD&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#26469;&#20174;&#27599;&#20010;&#33410;&#28857;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#30340;&#26631;&#31614;&#23558;&#20854;&#20256;&#36882;&#32473;&#20854;&#37051;&#23621;&#65292;&#20197;&#23454;&#29616;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#32422;&#26463;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;IDKD&#22312;&#38750;i.i.d.&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#23427;&#22312;&#21516;&#36136;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized learning enables serverless training of deep neural networks (DNNs) in a distributed manner on multiple nodes. This allows for the use of large datasets, as well as the ability to train with a wide variety of data sources. However, one of the key challenges with decentralized learning is heterogeneity in the data distribution across the nodes. In this paper, we propose In-Distribution Knowledge Distillation (IDKD) to address the challenge of heterogeneous data distribution. The goal of IDKD is to homogenize the data distribution across the nodes. While such data homogenization can be achieved by exchanging data among the nodes sacrificing privacy, IDKD achieves the same objective using a common public dataset across nodes without breaking the privacy constraint. This public dataset is different from the training dataset and is used to distill the knowledge from each node and communicate it to its neighbors through the generated labels. With traditional knowledge distillat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35777;&#26126;&#20855;&#26377;&#19981;&#21464;&#34920;&#31034;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#20351;&#19981;&#21464;&#24615;&#25104;&#20026;&#22495;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.03431</link><description>&lt;p&gt;
&#40065;&#26834;&#19981;&#21464;&#34920;&#31034;&#20013;&#30340;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization In Robust Invariant Representation. (arXiv:2304.03431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35777;&#26126;&#20855;&#26377;&#19981;&#21464;&#34920;&#31034;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#20351;&#19981;&#21464;&#24615;&#25104;&#20026;&#22495;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#24120;&#35265;&#21464;&#25442;&#30340;&#19981;&#21464;&#34920;&#31034;&#26041;&#27861;&#24120;&#29992;&#20110;&#30446;&#26631;&#35782;&#21035;&#12290;&#23398;&#20064;&#19981;&#21464;&#24615;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#26356;&#23481;&#26131;&#24212;&#29992;&#12290;&#30001;&#20110;&#19981;&#25913;&#21464;&#23545;&#35937;&#22266;&#26377;&#23646;&#24615;&#30340;&#25968;&#25454;&#21464;&#25442;&#26159;&#35782;&#21035;&#20219;&#21153;&#20013;&#20027;&#35201;&#30340;&#22797;&#26434;&#24615;&#26469;&#28304;&#65292;&#23545;&#36825;&#20123;&#21464;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#24182;&#31616;&#21270;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#35797;&#22270;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#20855;&#26377;&#26576;&#20123;&#21464;&#25442;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#22312;&#20808;&#21069;&#26410;&#35265;&#22495;&#20013;&#26159;&#21542;&#20173;&#20855;&#26377;&#19981;&#21464;&#24615;&#65311;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#19981;&#21464;&#34920;&#31034;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#20351;&#19981;&#21464;&#24615;&#25104;&#20026;&#22495;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised approaches for learning representations invariant to common transformations are used quite often for object recognition. Learning invariances makes models more robust and practical to use in real-world scenarios. Since data transformations that do not change the intrinsic properties of the object cause the majority of the complexity in recognition tasks, models that are invariant to these transformations help reduce the amount of training data required. This further increases the model's efficiency and simplifies training. In this paper, we investigate the generalization of invariant representations on out-of-distribution data and try to answer the question: Do model representations invariant to some transformations in a particular seen domain also remain invariant in previously unseen domains? Through extensive experiments, we demonstrate that the invariant model learns unstructured latent representations that are robust to distribution shifts, thus making invariance a de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DistGER&#30340;&#20998;&#24067;&#24335;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#22522;&#20110;&#20449;&#24687;&#23548;&#21521;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#65292;&#21033;&#29992;&#22810;&#31181;&#20248;&#21270;&#25216;&#26415;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#21313;&#20159;&#32423;&#21035;&#22270;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2303.15702</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#23548;&#21521;&#38543;&#26426;&#28216;&#36208;&#30340;&#20998;&#24067;&#24335;&#22270;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributed Graph Embedding with Information-Oriented Random Walks. (arXiv:2303.15702v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DistGER&#30340;&#20998;&#24067;&#24335;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#22522;&#20110;&#20449;&#24687;&#23548;&#21521;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#65292;&#21033;&#29992;&#22810;&#31181;&#20248;&#21270;&#25216;&#26415;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#21313;&#20159;&#32423;&#21035;&#22270;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23884;&#20837;&#23558;&#22270;&#33410;&#28857;&#26144;&#23556;&#21040;&#20302;&#32500;&#21521;&#37327;&#20013;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;&#12289;&#22522;&#20110;&#20449;&#24687;&#23548;&#21521;&#38543;&#26426;&#28216;&#36208;&#30340;&#36890;&#29992;&#22270;&#23884;&#20837;&#26694;&#26550;DistGER&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#21313;&#20159;&#32423;&#21035;&#30340;&#22270;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#37327;&#24335;&#22320;&#35745;&#31639;&#20449;&#24687;&#23548;&#21521;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#24182;&#21033;&#29992;&#20102;&#22810;&#31181;&#37051;&#36817;&#24863;&#30693;&#12289;&#27969;&#24335;&#12289;&#24182;&#34892;&#30340;&#22270;&#21010;&#20998;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#26426;&#22120;&#19978;&#26082;&#39640;&#36136;&#37327;&#30340;&#26412;&#22320;&#21010;&#20998;&#65292;&#21448;&#20986;&#33394;&#30340;&#36127;&#36733;&#22343;&#34913;&#12290;&#21516;&#26102;DistGER&#25913;&#36827;&#20102;&#20998;&#24067;&#24335;Skip-Gram&#23398;&#20064;&#27169;&#22411;&#20197;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#65292;&#24182;&#20248;&#21270;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#30340;&#35775;&#38382;&#23616;&#37096;&#24615;&#12289;CPU&#21534;&#21520;&#37327;&#21644;&#21516;&#27493;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph embedding maps graph nodes to low-dimensional vectors, and is widely adopted in machine learning tasks. The increasing availability of billion-edge graphs underscores the importance of learning efficient and effective embeddings on large graphs, such as link prediction on Twitter with over one billion edges. Most existing graph embedding methods fall short of reaching high data scalability. In this paper, we present a general-purpose, distributed, information-centric random walk-based graph embedding framework, DistGER, which can scale to embed billion-edge graphs. DistGER incrementally computes information-centric random walks. It further leverages a multi-proximity-aware, streaming, parallel graph partitioning strategy, simultaneously achieving high local partition quality and excellent workload balancing across machines. DistGER also improves the distributed Skip-Gram learning model to generate node embeddings by optimizing the access locality, CPU throughput, and synchronizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Uni-RXN&#26694;&#26550;&#65292;&#22312;&#21270;&#23398;&#21453;&#24212;Pretraining&#21644;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20855;&#22791;&#21270;&#23398;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20165;&#20381;&#36182;&#23569;&#37327;&#21453;&#24212;&#27169;&#26495;&#30340;&#38480;&#21046;&#65292;&#29983;&#25104;&#36136;&#37327;&#39640;&#12289;&#21487;&#21512;&#25104;&#30340;&#33647;&#29289;&#31867;&#20998;&#23376;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.06965</link><description>&lt;p&gt;
Uni-RXN: &#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24357;&#21512;&#21270;&#23398;&#21453;&#24212;Pretraining&#21644;&#26465;&#20214;&#20998;&#23376;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation. (arXiv:2303.06965v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Uni-RXN&#26694;&#26550;&#65292;&#22312;&#21270;&#23398;&#21453;&#24212;Pretraining&#21644;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20855;&#22791;&#21270;&#23398;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20165;&#20381;&#36182;&#23569;&#37327;&#21453;&#24212;&#27169;&#26495;&#30340;&#38480;&#21046;&#65292;&#29983;&#25104;&#36136;&#37327;&#39640;&#12289;&#21487;&#21512;&#25104;&#30340;&#33647;&#29289;&#31867;&#20998;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21453;&#24212;&#26159;&#33647;&#29289;&#35774;&#35745;&#21644;&#26377;&#26426;&#21270;&#23398;&#30740;&#31350;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#19968;&#20010;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#21270;&#23398;&#21453;&#24212;&#22522;&#26412;&#35268;&#21017;&#30340;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#21453;&#24212;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#65292;&#20801;&#35768;&#26356;&#25972;&#20307;&#30340;&#26041;&#27861;&#12290;&#21463;&#26377;&#26426;&#21270;&#23398;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#24402;&#32435;&#20559;&#35265;&#32435;&#20837;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20855;&#22791;&#21270;&#23398;&#30693;&#35782;&#65292;&#35813;&#26694;&#26550;&#21487;&#24212;&#29992;&#20110;&#22522;&#20110;&#21453;&#24212;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20165;&#20381;&#36182;&#23569;&#37327;&#21453;&#24212;&#27169;&#26495;&#30340;&#38480;&#21046;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#30340;&#21487;&#21512;&#25104;&#33647;&#29289;&#31867;&#20998;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemical reactions are the fundamental building blocks of drug design and organic chemistry research. In recent years, there has been a growing need for a large-scale deep-learning framework that can efficiently capture the basic rules of chemical reactions. In this paper, we have proposed a unified framework that addresses both the reaction representation learning and molecule generation tasks, which allows for a more holistic approach. Inspired by the organic chemistry mechanism, we develop a novel pretraining framework that enables us to incorporate inductive biases into the model. Our framework achieves state-of-the-art results on challenging downstream tasks. By possessing chemical knowledge, this framework can be applied to reaction-based generative models, overcoming the limitations of current molecule generation models that rely on a small number of reaction templates. In the extensive experiments, our model generates synthesizable drug-like structures of high quality. Overall,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65288;FwA&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#32593;&#32476;&#19978;&#30340;&#24322;&#26500;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21327;&#35758;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05445</link><description>&lt;p&gt;
&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65306;&#22797;&#26434;&#32593;&#32476;&#19978;&#24322;&#26500;&#36172;&#21338;&#26426;&#30340;&#39640;&#25928;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks. (arXiv:2303.05445v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65288;FwA&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#32593;&#32476;&#19978;&#30340;&#24322;&#26500;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21327;&#35758;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#39034;&#24207;&#20915;&#31574;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#22914;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#21644;&#26080;&#32447;&#32593;&#32476;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22810;&#20195;&#29702;&#30340;&#22330;&#26223;&#65292;&#27599;&#20010;&#20195;&#29702;&#35299;&#20915;&#33258;&#24049;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#36172;&#21338;&#26426;&#25317;&#26377;&#19981;&#21516;&#30340;&#33218;&#12290;&#20182;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#36890;&#36807;&#32473;&#23450;&#32593;&#32476;&#30340;&#36890;&#20449;&#21327;&#35758;&#21327;&#20316;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#20182;&#20204;&#30340;&#38598;&#20307;&#36951;&#25022;&#12290;&#20808;&#21069;&#20851;&#20110;&#27492;&#38382;&#39064;&#30340;&#25991;&#29486;&#21482;&#32771;&#34385;&#20102;&#33218;&#30340;&#24322;&#36136;&#24615;&#21644;&#32593;&#32476;&#21270;&#20195;&#29702;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21516;&#26102;&#21253;&#21547;&#36825;&#20004;&#20010;&#29305;&#24615;&#30340;&#35774;&#32622;&#12290;&#38024;&#23545;&#36825;&#19968;&#26032;&#39062;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#26631;&#20934;&#27867;&#27946;&#21327;&#35758;&#32467;&#21512;&#32463;&#20856;&#30340;&#19978;&#32622;&#20449;&#30028;&#31574;&#30053;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20943;&#36731;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#27867;&#27946;&#36896;&#25104;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#35758;&#65292;&#31216;&#20026;&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65288;FwA&#65289;&#12290;&#25105;&#20204;&#23545;&#30001;&#27492;&#20135;&#29983;&#30340;&#36951;&#25022;&#19978;&#30028;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#21327;&#35758;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-armed bandits are extensively used to model sequential decision-making, making them ubiquitous in many real-life applications such as online recommender systems and wireless networking. We consider a multi-agent setting where each agent solves their own bandit instance endowed with a different set of arms. Their goal is to minimize their group regret while collaborating via some communication protocol over a given network. Previous literature on this problem only considered arm heterogeneity and networked agents separately. In this work, we introduce a setting that encompasses both features. For this novel setting, we first provide a rigorous regret analysis for a standard flooding protocol combined with the classic UCB policy. Then, to mitigate the issue of high communication costs incurred by flooding in complex networks, we propose a new protocol called Flooding with Absorption (FwA). We provide a theoretical analysis of the resulting regret bound and discuss the advantages of
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13991</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#23398;&#20064;&#23545;&#26410;&#30693;&#39046;&#22495;&#36827;&#34892;&#27867;&#21270;&#65306;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#30340;&#32763;&#35793;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13991
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#30001;&#20110;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65288;&#22914;&#23545;&#25239;&#35757;&#32451;&#65292;&#22810;&#39046;&#22495;&#28151;&#21512;&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32423;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#35268;&#33539;&#25552;&#21462;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#23545;&#39118;&#26684;&#65288;&#20363;&#22914;&#65292;&#26080;&#20449;&#24687;&#30340;&#32441;&#29702;&#65289;&#26377;&#24456;&#24378;&#30340;&#20559;&#22909;&#65292;&#32780;&#19981;&#26159;&#23545;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#24418;&#29366;&#65289;&#30340;&#20559;&#22909;&#65292;&#36825;&#19982;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25918;&#23556;&#31185;&#21307;&#24072;&#20542;&#21521;&#20110;&#20174;CXR&#22270;&#20687;&#20013;&#23398;&#20064;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#22240;&#27492;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#20174;CXR&#22270;&#20687;&#36827;&#34892;&#30149;&#29702;&#35786;&#26029;&#30340;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#27169;&#22411;&#24212;&#35813;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#65288;SRMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance degradation due to source domain mismatch is a longstanding challenge in deep learning-based medical image analysis, particularly for chest X-rays (CXRs). Several methods (e.g., adversarial training, multi-domain mixups) have been proposed to extract domain-invariant high-level features to address this domain shift. However, these methods do not explicitly regularize the content and style characteristics of the extracted domain-invariant features. Recent studies have demonstrated that CNN models exhibit a strong bias toward styles (e.g., uninformative textures) rather than content (e.g., shape), in stark contrast to the human-vision system. Radiologists tend to learn visual cues from CXRs and thus perform well across multiple domains. Therefore, in medical imaging for pathology diagnosis from CXR images, models should extract domain-invariant features that are style-invariant and content-biased. Motivated by this, we employ the novel style randomization modules (SRMs) at bo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#20869;&#29983;&#24615;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Two-Stage Least Squares&#26041;&#27861;&#30340;&#22312;&#32447;&#21464;&#20307;O2SLS&#26469;&#22788;&#29702;&#20869;&#29983;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#35782;&#21035;&#29575;&#21644;&#39044;&#27979;&#36951;&#25022;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.09357</link><description>&lt;p&gt;
&#22312;&#32447;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;: &#36951;&#25022;&#20998;&#26512;&#21644;Bandit&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Online Instrumental Variable Regression: Regret Analysis and Bandit Feedback. (arXiv:2302.09357v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09357
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#20869;&#29983;&#24615;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Two-Stage Least Squares&#26041;&#27861;&#30340;&#22312;&#32447;&#21464;&#20307;O2SLS&#26469;&#22788;&#29702;&#20869;&#29983;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#35782;&#21035;&#29575;&#21644;&#39044;&#27979;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#29983;&#24615;&#26159;&#23454;&#38469;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#29616;&#35937;&#65292;&#22240;&#20026;&#36951;&#28431;&#21464;&#37327;&#12289;&#25112;&#30053;&#34892;&#20026;&#12289;&#27979;&#37327;&#35823;&#24046;&#31561;&#21407;&#22240;&#23548;&#33268;&#22122;&#22768;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#29616;&#26377;&#30340;&#26080;&#30028;&#22122;&#22768;&#21644;&#32447;&#24615;Bandit&#38543;&#26426;&#22312;&#32447;&#32447;&#24615;&#22238;&#24402;&#20998;&#26512;&#20005;&#37325;&#20381;&#36182;&#22806;&#29983;&#24615;&#65292;&#21363;&#22122;&#22768;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#12290;&#37492;&#20110;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#22238;&#24402;&#22312;&#38543;&#26426;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#36229;&#35782;&#21035;&#21644;&#24688;&#22909;&#35782;&#21035;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Two-Stage Least Squares&#26041;&#27861;&#30340;&#22312;&#32447;&#21464;&#20307;&#65288;&#21363;O2SLS&#65289;&#26469;&#22788;&#29702;&#20869;&#29983;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;O2SLS&#23454;&#29616;&#20102;$ \mathcal{O} \left(d_x d_z \log ^ 2 T \right)$&#30340;&#35782;&#21035;&#29575;&#21644;$ \tilde {\mathcal {O}} \left(\gamma \sqrt {d_x T} \right)$&#30340;&#39044;&#27979;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Endogeneity, i.e. the dependence between noise and covariates, is a common phenomenon in real data due to omitted variables, strategic behaviours, measurement errors etc. In contrast, the existing analyses of stochastic online linear regression with unbounded noise and linear bandits depend heavily on exogeneity, i.e. the independence between noise and covariates. Motivated by this gap, we study the over-and just-identified Instrumental Variable (IV) regression for stochastic online learning. IV regression and the Two-Stage Least Squares approach to it are widely deployed in economics and causal inference to identify the underlying model from an endogenous dataset. Thus, we propose to use an online variant of Two-Stage Least Squares approach, namely O2SLS, to tackle endogeneity in stochastic online learning. Our analysis shows that O2SLS achieves $\mathcal{O}\left(d_x d_z \log ^2 T\right)$ identification and $\tilde{\mathcal{O}}\left(\gamma \sqrt{d_x T}\right)$ oracle regret after $T$ 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#31995;&#32479;&#26469;&#24402;&#32435;&#36825;&#20123;&#26041;&#27861;&#12290;&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.06433</link><description>&lt;p&gt;
&#26631;&#31614;&#25928;&#29575;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Label-efficient Time Series Representation Learning: A Review. (arXiv:2302.06433v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06433
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#31995;&#32479;&#26469;&#24402;&#32435;&#36825;&#20123;&#26041;&#27861;&#12290;&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26102;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20363;&#22914;&#36801;&#31227;&#23398;&#20064;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#20419;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#26377;&#38480;&#30340;&#26102;&#38388;&#24207;&#21015;&#26631;&#31614;&#20013;&#33719;&#21462;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#26469;&#26681;&#25454;&#23427;&#20204;&#23545;&#22806;&#37096;&#25968;&#25454;&#28304;&#30340;&#20381;&#36182;&#65292;&#23545;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#24182;&#24635;&#32467;&#20102;&#24403;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#33021;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#26356;&#22909;&#36827;&#23637;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scarcity of labeled data is one of the main challenges of applying deep learning models on time series data in the real world. Therefore, several approaches, e.g., transfer learning, self-supervised learning, and semi-supervised learning, have been recently developed to promote the learning capability of deep learning models from the limited time series labels. In this survey, for the first time, we provide a novel taxonomy to categorize existing approaches that address the scarcity of labeled data problem in time series data based on their dependency on external data sources. Moreover, we present a review of the recent advances in each approach and conclude the limitations of the current works and provide future directions that could yield better progress in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#26377;&#30028;f-&#25955;&#24230;&#32422;&#26463;&#19979;&#65292;&#36817;&#20284;&#25298;&#32477;&#37319;&#26679;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#36890;&#36807;&#920;(~(D/f'(n)))&#20989;&#25968;&#26469;&#34920;&#31034;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#30456;&#20851;&#31639;&#27861;&#30340;&#24615;&#33021;&#20381;&#28982;&#25104;&#31435;&#12290;</title><link>http://arxiv.org/abs/2302.04658</link><description>&lt;p&gt;
&#36817;&#20284;&#25298;&#32477;&#37319;&#26679;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21450;&#20854;&#22312;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Sample Complexity of Approximate Rejection Sampling with Applications to Smoothed Online Learning. (arXiv:2302.04658v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#26377;&#30028;f-&#25955;&#24230;&#32422;&#26463;&#19979;&#65292;&#36817;&#20284;&#25298;&#32477;&#37319;&#26679;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#36890;&#36807;&#920;(~(D/f'(n)))&#20989;&#25968;&#26469;&#34920;&#31034;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#30456;&#20851;&#31639;&#27861;&#30340;&#24615;&#33021;&#20381;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#26469;&#33258;&#20998;&#24067;&#956;&#30340;n&#20010;&#29420;&#31435;&#26679;&#26412;&#65292;&#24182;&#19988;&#25105;&#20204;&#24076;&#26395;&#36755;&#20986;&#20854;&#20013;&#19968;&#20010;&#26679;&#26412;&#65292;&#20351;&#24471;&#36755;&#20986;&#30340;&#20998;&#24067;&#23613;&#21487;&#33021;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#957;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25152;&#26377;&#20855;&#26377;&#26377;&#30028;f-&#25955;&#24230;Df(&#957;|&#956;)&#8804;D&#30340;&#957;,&#956;&#23545;&#20013;&#65292;&#20851;&#20110;n&#30340;&#26368;&#20248;&#24635;&#21464;&#24046;&#36317;&#31163;&#30001;&#920;(~(D/f'(n)))&#32473;&#20986;&#12290;&#20043;&#21069;&#65292;&#36825;&#20010;&#38382;&#39064;&#21482;&#30740;&#31350;&#20102;&#957;&#30456;&#23545;&#20110;&#956;&#30340;Radon-Nikodym&#23548;&#25968;&#19968;&#33268;&#26377;&#30028;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#20284;&#20046;&#38750;&#24120;&#19981;&#21516;&#30340;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#23567;&#21270;&#36951;&#25022;&#21644;&#20855;&#26377;oracle&#25928;&#29575;&#30340;&#31639;&#27861;&#30340;&#36951;&#25022;&#21363;&#20351;&#22312;&#23545;&#25163;&#26377;&#36793;&#30028;f-&#25955;&#24230;&#65288;&#32780;&#19981;&#26159;&#26377;&#30028;Radon-Nikodym&#23548;&#25968;&#65289;&#30340;&#26494;&#24347;&#32422;&#26463;&#19979;&#65292;&#20173;&#28982;&#25104;&#31435;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#22343;&#21248;&#20272;&#35745;&#20013;&#29992;&#20110;&#24179;&#22343;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Suppose we are given access to $n$ independent samples from distribution $\mu$ and we wish to output one of them with the goal of making the output distributed as close as possible to a target distribution $\nu$. In this work we show that the optimal total variation distance as a function of $n$ is given by $\tilde\Theta(\frac{D}{f'(n)})$ over the class of all pairs $\nu,\mu$ with a bounded $f$-divergence $D_f(\nu\|\mu)\leq D$. Previously, this question was studied only for the case when the Radon-Nikodym derivative of $\nu$ with respect to $\mu$ is uniformly bounded. We then consider an application in the seemingly very different field of smoothed online learning, where we show that recent results on the minimax regret and the regret of oracle-efficient algorithms still hold even under relaxed constraints on the adversary (to have bounded $f$-divergence, as opposed to bounded Radon-Nikodym derivative). Finally, we also study efficacy of importance sampling for mean estimates uniform o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#20102;&#19968;&#20010;"&#35748;&#35777;&#24726;&#35770;"&#65292;&#35748;&#35777;&#34429;&#28982;&#21487;&#20197;&#23637;&#31034;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#39069;&#22806;&#25581;&#31034;&#20102;&#26377;&#20851;&#35748;&#35777;&#27169;&#22411;&#30340;&#20449;&#24687;&#20063;&#25104;&#20026;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#23548;&#33268;&#26356;&#22909;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04379</link><description>&lt;p&gt;
&#35748;&#35777;&#24726;&#35770;: &#35748;&#35777;&#20250;&#25581;&#31034;&#26356;&#22909;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The Certification Paradox: Certifications Admit Better Attacks. (arXiv:2302.04379v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#20102;&#19968;&#20010;"&#35748;&#35777;&#24726;&#35770;"&#65292;&#35748;&#35777;&#34429;&#28982;&#21487;&#20197;&#23637;&#31034;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#39069;&#22806;&#25581;&#31034;&#20102;&#26377;&#20851;&#35748;&#35777;&#27169;&#22411;&#30340;&#20449;&#24687;&#20063;&#25104;&#20026;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#23548;&#33268;&#26356;&#22909;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20445;&#35777;&#26377;&#19968;&#20010;&#26377;&#30028;&#21306;&#22495;&#20869;&#19981;&#23384;&#22312;&#23545;&#25239;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#35748;&#35777;&#26426;&#21046;&#22312;&#23637;&#31034;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;: &#35748;&#35777;&#26159;&#21542;&#20250;&#26377;&#20219;&#20309;&#24847;&#24819;&#19981;&#21040;&#30340;&#21518;&#26524;&#65292;&#36890;&#36807;&#25581;&#31034;&#26377;&#20851;&#35748;&#35777;&#27169;&#22411;&#30340;&#39069;&#22806;&#20449;&#24687;&#65311;&#25105;&#20204;&#20197;&#32943;&#23450;&#30340;&#31572;&#26696;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#35748;&#35777;&#19981;&#20165;&#27979;&#37327;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#23637;&#29616;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"&#35748;&#35777;&#24863;&#30693;&#25915;&#20987;"&#65292;&#22312;&#38024;&#23545;&#32463;&#36807;&#35748;&#35777;&#30340;&#27169;&#22411;&#36827;&#34892;&#25915;&#20987;&#26102;&#65292;&#36825;&#31181;&#25915;&#20987;&#20250;&#27604;&#20197;&#21069;&#30340;&#20219;&#20309;&#26041;&#27861;&#26356;&#39057;&#32321;&#22320;&#20135;&#29983;&#26356;&#23567;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#23454;&#29616;&#20102;&#26368;&#22810;34%&#30340;&#25200;&#21160;&#35268;&#33539;&#20013;&#20301;&#25968;&#30340;&#20943;&#23567;(&#27604;&#36739;&#30446;&#26631;&#21644;&#25915;&#20987;&#23454;&#20363;)&#65292;&#21516;&#26102;&#38656;&#35201;&#30340;&#35745;&#31639;&#26102;&#38388;&#27604;PGD&#31561;&#26041;&#27861;&#23569;&#20102;90%&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#23454;&#29616;&#20102;&#22914;&#27492;&#26174;&#30528;&#30340;&#25200;&#21160;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38477;&#20302;&#65292;&#31361;&#26174;&#20102;&#20197;&#35748;&#35777;&#20316;&#20026;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#30340;&#19968;&#31181;&#24726;&#35770;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35748;&#35777;&#19981;&#20165;&#25581;&#31034;&#20102;&#31283;&#20581;&#27169;&#22411;&#30340;&#23646;&#24615;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#29992;&#26469;&#21457;&#36215;&#26356;&#26377;&#25928;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
In guaranteeing that no adversarial examples exist within a bounded region, certification mechanisms play an important role in demonstrating the robustness of neural networks. In this work we ask: Could certifications have any unintended consequences, through exposing additional information about certified models? We answer this question in the affirmative, demonstrating that certifications not only measure model robustness but also present a new attack surface. We propose \emph{Certification Aware Attacks}, that produce smaller adversarial perturbations more than twice as frequently as any prior approach, when launched against certified models. Our attacks achieve an up to $34\%$ reduction in the median perturbation norm (comparing target and attack instances), while requiring $90 \%$ less computational time than approaches like PGD. That our attacks achieve such significant reductions in perturbation size and computational cost highlights an apparent paradox in deploying certificatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20960;&#20309;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65292;&#24182;&#22312;&#39640;&#32500;&#24230;&#25968;&#25454;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.03660</link><description>&lt;p&gt;
&#19968;&#33324;&#20960;&#20309;&#19978;&#30340;&#40654;&#26364;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Riemannian Flow Matching on General Geometries. (arXiv:2302.03660v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20960;&#20309;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65292;&#24182;&#22312;&#39640;&#32500;&#24230;&#25968;&#25454;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#65288;RFM&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27969;&#24418;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#12290;&#29616;&#26377;&#30340;&#27969;&#24418;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#27169;&#25311;&#65292;&#35201;&#20040;&#26080;&#27861;&#26412;&#36136;&#19978;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#35201;&#20040;&#20351;&#29992;&#38480;&#21046;&#37327;&#30340;&#36817;&#20284;&#26469;&#20135;&#29983;&#26377;&#20559;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#40654;&#26364;&#27969;&#21305;&#37197;&#32469;&#36807;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#22810;&#30340;&#20248;&#21183;&#65306;&#23427;&#22312;&#31616;&#21333;&#20960;&#20309;&#19978;&#26080;&#38656;&#27169;&#25311;&#65292;&#19981;&#38656;&#35201;&#25955;&#24230;&#35745;&#31639;&#65292;&#24182;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;&#20854;&#30446;&#26631;&#21521;&#37327;&#22330;&#12290; RFM&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#26500;&#24314;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#21069;&#24230;&#37327;&#65292;&#20197;&#23450;&#20041;&#30446;&#26631;&#21521;&#37327;&#22330;&#65292;&#20854;&#20013;&#21253;&#25324;&#29616;&#26377;&#30340;&#27431;&#20960;&#37324;&#24471;&#24773;&#20917;&#12290;&#20026;&#20102;&#25193;&#23637;&#21040;&#19968;&#33324;&#20960;&#20309;&#65292;&#25105;&#20204;&#20381;&#38752;&#20351;&#29992;&#35889;&#20998;&#35299;&#26469;&#26377;&#25928;&#22320;&#21363;&#20852;&#35745;&#31639;&#21069;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;3D&#32593;&#26684;&#21644;&#21452;&#26354;&#31354;&#38388;&#19978;&#35757;&#32451;&#26631;&#20934;&#21270;&#27969;&#26469;&#35777;&#26126;&#20854;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on real-world non-Euclidean datasets, and we demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#36136;&#37327;&#23384;&#22312;&#27874;&#21160;&#65292;&#24341;&#20837;&#20102;&#8220;&#23614;&#37096;&#36136;&#37327;&#8221;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2212.13925</link><description>&lt;p&gt;
&#36136;&#37327;&#30340;&#23614;&#37096;
&lt;/p&gt;
&lt;p&gt;
Quality at the Tail. (arXiv:2212.13925v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#36136;&#37327;&#23384;&#22312;&#27874;&#21160;&#65292;&#24341;&#20837;&#20102;&#8220;&#23614;&#37096;&#36136;&#37327;&#8221;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#31995;&#32479;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#38656;&#35201;&#19968;&#31181;&#32454;&#33268;&#20837;&#24494;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#32771;&#34385;&#21040;&#25512;&#29702;&#36136;&#37327;&#21644;&#25512;&#29702;&#26102;&#38388;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20005;&#33499;&#30340;&#29615;&#22659;&#19979;&#65292;&#35201;&#27714;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#25351;&#26631;&#30340;&#35201;&#27714;&#12290;&#24573;&#35270;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#26041;&#38754;&#37117;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21644;&#19981;&#21487;&#36870;&#30340;&#21518;&#26524;&#65292;&#21253;&#25324;&#20154;&#21592;&#20260;&#20129;&#21644;&#36130;&#20135;&#25439;&#22833;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#30740;&#31350;&#32570;&#20047;&#23545;&#36825;&#20123;&#25351;&#26631;&#30340;&#20840;&#38754;&#32771;&#34385;&#65292;&#36890;&#24120;&#22312;&#29702;&#24819;&#25110;&#23485;&#26494;&#26465;&#20214;&#19979;&#36827;&#34892;&#65292;&#20174;&#32780;&#23548;&#33268;&#35780;&#20272;&#26041;&#27861;&#19981;&#23436;&#25972;&#25110;&#19981;&#30452;&#35266;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#36136;&#37327;&#30340;&#27874;&#21160;&#65292;&#36827;&#19968;&#27493;&#32473;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#21644;&#25361;&#25112;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#65292;&#24341;&#20837;&#20102;&#8220;&#23614;&#37096;&#36136;&#37327;&#8221;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#20998;&#24067;&#23614;&#37096;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and evaluating deep learning models and systems necessitate a meticulous approach to ensure comprehensive assessment. In practical applications, it is paramount to consider both the inference quality and the inference time, particularly within critical contexts, where stringent requirements demand the simultaneous satisfaction of both metrics. Neglecting either aspect can result in severe and irreversible consequences, including loss of human life and property damage. Unfortunately, many studies lack a comprehensive consideration of these metrics, often conducted under ideal or permissive conditions, thereby leading to incomplete or non-intuitive evaluation methodologies.  This study reveals that deep learning inference quality exhibits fluctuations, which further introduces complications and challenges to the benchmarking and evaluation. To better characterize the phenomenon, the concept of "tail quality" is introduced, which indicates the quality at the tail of distribut
&lt;/p&gt;</description></item><item><title>TREE-G&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#20462;&#25913;&#20915;&#31574;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#20998;&#35010;&#20989;&#25968;&#21644;&#25351;&#38024;&#26426;&#21046;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#20110;&#24102;&#26377;&#25299;&#25169;&#20449;&#24687;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2207.02760</link><description>&lt;p&gt;
TREE-G:&#20915;&#31574;&#26641;&#23545;&#25239;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TREE-G: Decision Trees Contesting Graph Neural Networks. (arXiv:2207.02760v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02760
&lt;/p&gt;
&lt;p&gt;
TREE-G&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#20462;&#25913;&#20915;&#31574;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#20998;&#35010;&#20989;&#25968;&#21644;&#25351;&#38024;&#26426;&#21046;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#20110;&#24102;&#26377;&#25299;&#25169;&#20449;&#24687;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#19978;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12289;&#26131;&#20110;&#24212;&#29992;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#24212;&#29992;&#20915;&#31574;&#26641;&#24182;&#23558;&#25299;&#25169;&#20449;&#24687;&#19982;&#22270;&#30340;&#39030;&#28857;&#19978;&#30340;&#34920;&#26684;&#25968;&#25454;&#30456;&#32467;&#21512;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TREE-G&#12290;TREE-G&#20462;&#25913;&#20102;&#26631;&#20934;&#20915;&#31574;&#26641;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#26032;&#22411;&#20998;&#35010;&#20989;&#25968;&#12290;&#36825;&#20010;&#20998;&#35010;&#20989;&#25968;&#19981;&#20165;&#21253;&#25324;&#33410;&#28857;&#29305;&#24449;&#21644;&#25299;&#25169;&#20449;&#24687;&#65292;&#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25351;&#38024;&#26426;&#21046;&#65292;&#20801;&#35768;&#20998;&#35010;&#33410;&#28857;&#20351;&#29992;&#22312;&#20808;&#21069;&#20998;&#35010;&#20013;&#35745;&#31639;&#24471;&#21040;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20998;&#35010;&#20989;&#25968;&#33021;&#22815;&#36866;&#24212;&#39044;&#27979;&#20219;&#21153;&#21644;&#24403;&#21069;&#30340;&#22270;&#12290;&#25105;&#20204;&#23545;TREE-G&#30340;&#29702;&#35770;&#24615;&#36136;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#22312;&#22810;&#20010;&#22270;&#21644;&#39030;&#28857;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;&#20102;&#23427;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
When dealing with tabular data, models based on decision trees are a popular choice due to their high accuracy on these data types, their ease of application, and explainability properties. However, when it comes to graph-structured data, it is not clear how to apply them effectively, in a way that incorporates the topological information with the tabular data available on the vertices of the graph. To address this challenge, we introduce TREE-G. TREE-G modifies standard decision trees, by introducing a novel split function that is specialized for graph data. Not only does this split function incorporate the node features and the topological information, but it also uses a novel pointer mechanism that allows split nodes to use information computed in previous splits. Therefore, the split function adapts to the predictive task and the graph at hand. We analyze the theoretical properties of TREE-G and demonstrate its benefits empirically on multiple graph and vertex prediction benchmarks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#21363;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;ICA&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.07751</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#30340;&#21487;&#36776;&#35782;&#24615;&#65306;&#31232;&#30095;&#24615;&#21450;&#20854;&#23427;
&lt;/p&gt;
&lt;p&gt;
On the Identifiability of Nonlinear ICA: Sparsity and Beyond. (arXiv:2206.07751v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#21363;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;ICA&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#26088;&#22312;&#20174;&#20854;&#21487;&#35266;&#27979;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#24674;&#22797;&#20986;&#28508;&#22312;&#29420;&#31435;&#20998;&#37327;&#12290;&#22914;&#20309;&#20351;&#38750;&#32447;&#24615;ICA&#27169;&#22411;&#21487;&#36776;&#35782;&#30452;&#21040;&#26576;&#20123;&#24179;&#20961;&#19981;&#30830;&#23450;&#24615;&#26159;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#31361;&#30772;&#26159;&#23558;&#28304;&#30340;&#26631;&#20934;&#29420;&#31435;&#24615;&#20551;&#35774;&#37325;&#26032;&#23450;&#20041;&#20026;&#22312;&#26576;&#20123;&#36741;&#21161;&#21464;&#37327;&#65288;&#20363;&#22914;&#31867;&#26631;&#31614;&#21644;/&#25110;&#22495;/&#26102;&#38388;&#32034;&#24341;&#65289;&#32473;&#23450;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#20316;&#20026;&#24369;&#30417;&#30563;&#25110;&#24402;&#32435;&#20559;&#32622;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26080;&#26465;&#20214;&#20808;&#39564;&#30340;&#38750;&#32447;&#24615;ICA&#26080;&#27861;&#20174;&#36825;&#20123;&#21457;&#23637;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#26465;&#26367;&#20195;&#36335;&#24452;&#65292;&#24182;&#20165;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#20363;&#22914;&#32467;&#26500;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#32422;&#26463;&#30340;&#20855;&#20307;&#23454;&#20363;&#19979;&#65292;&#29420;&#31435;&#30340;&#28508;&#22312;&#20998;&#37327;&#21487;&#20197;&#20174;&#20854;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#36776;&#35782;&#20986;&#26469;&#65292;&#36798;&#21040;&#38750;&#24179;&#20961;&#30340;&#38750;&#32447;&#24615;ICA&#21487;&#35782;&#21035;&#24615;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear independent component analysis (ICA) aims to recover the underlying independent latent sources from their observable nonlinear mixtures. How to make the nonlinear ICA model identifiable up to certain trivial indeterminacies is a long-standing problem in unsupervised learning. Recent breakthroughs reformulate the standard independence assumption of sources as conditional independence given some auxiliary variables (e.g., class labels and/or domain/time indexes) as weak supervision or inductive bias. However, nonlinear ICA with unconditional priors cannot benefit from such developments. We explore an alternative path and consider only assumptions on the mixing process, such as Structural Sparsity. We show that under specific instantiations of such constraints, the independent latent sources can be identified from their nonlinear mixtures up to a permutation and a component-wise transformation, thus achieving nontrivial identifiability of nonlinear ICA without auxiliary variable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;&#26799;&#24230;&#24456;&#26114;&#36149;&#25110;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#20123;&#36741;&#21161;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#21463;&#30410;&#20110;&#30446;&#26631;&#21644;&#36741;&#21161;&#20449;&#24687;&#20043;&#38388;&#30340;Hessian&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2206.00395</link><description>&lt;p&gt;
&#20855;&#22791;&#36741;&#21161;&#20449;&#24687;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimization with access to auxiliary information. (arXiv:2206.00395v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;&#26799;&#24230;&#24456;&#26114;&#36149;&#25110;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#20123;&#36741;&#21161;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#21463;&#30410;&#20110;&#30446;&#26631;&#21644;&#36741;&#21161;&#20449;&#24687;&#20043;&#38388;&#30340;Hessian&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the fundamental optimization question of minimizing a target function with expensive or limited gradient computation, given access to some auxiliary side function with cheaper or more available gradients. The authors propose two generic new algorithms and prove that this framework can benefit from the Hessian similarity assumption between the target and side information.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#26412;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#22312;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;$f(x)$&#30340;&#26799;&#24230;&#24456;&#26114;&#36149;&#25110;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#20123;&#36741;&#21161;&#20989;&#25968;$h(x)$&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#36825;&#20010;&#20844;&#24335;&#28085;&#30422;&#20102;&#35768;&#22810;&#23454;&#38469;&#30456;&#20851;&#30340;&#35774;&#32622;&#65292;&#22914;i&#65289;&#22312;SGD&#20013;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#65292;ii&#65289;&#36801;&#31227;&#23398;&#20064;&#65292;iii&#65289;&#32852;&#37030;&#23398;&#20064;&#65292;iv&#65289;&#20351;&#29992;&#21387;&#32553;&#27169;&#22411;/&#20002;&#24323;&#31561;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#36825;&#20123;&#35774;&#32622;&#65292;&#24182;&#35777;&#26126;&#20165;&#20351;&#29992;&#30446;&#26631;&#21644;&#36741;&#21161;&#20449;&#24687;&#20043;&#38388;&#30340;Hessian&#30456;&#20284;&#24615;&#20551;&#35774;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#36825;&#20010;&#26694;&#26550;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the fundamental optimization question of minimizing a target function $f(x)$ whose gradients are expensive to compute or have limited availability, given access to some auxiliary side function $h(x)$ whose gradients are cheap or more available. This formulation captures many settings of practical relevance such as i) re-using batches in SGD, ii) transfer learning, iii) federated learning, iv) training with compressed models/dropout, etc. We propose two generic new algorithms which are applicable in all these settings and prove using only an assumption on the Hessian similarity between the target and side information that we can benefit from this framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23485;&#27867;&#25512;&#33616;&#31995;&#32479;(BroadCF)&#65292;&#20351;&#29992;&#23485;&#27867;&#23398;&#20064;&#31995;&#32479;(BLS)&#20316;&#20026;&#26144;&#23556;&#20989;&#25968;&#26469;&#23398;&#20064;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#29992;&#25143;-&#39033;&#35780;&#32423;&#21327;&#21516;&#21521;&#37327;&#39044;&#22788;&#29702;&#31243;&#24207;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;BLS&#23398;&#20064;&#30340;&#26684;&#24335;&#12290;BroadCF&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29992;&#25143;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;CF&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.11602</link><description>&lt;p&gt;
&#23485;&#27867;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Broad Recommender System: An Efficient Nonlinear Collaborative Filtering Approach. (arXiv:2204.11602v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23485;&#27867;&#25512;&#33616;&#31995;&#32479;(BroadCF)&#65292;&#20351;&#29992;&#23485;&#27867;&#23398;&#20064;&#31995;&#32479;(BLS)&#20316;&#20026;&#26144;&#23556;&#20989;&#25968;&#26469;&#23398;&#20064;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#29992;&#25143;-&#39033;&#35780;&#32423;&#21327;&#21516;&#21521;&#37327;&#39044;&#22788;&#29702;&#31243;&#24207;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;BLS&#23398;&#20064;&#30340;&#26684;&#24335;&#12290;BroadCF&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29992;&#25143;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;CF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#24191;&#27867;&#24341;&#20837;&#21040;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#20013;&#65292;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#25512;&#33616;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25429;&#33719;&#39033;&#30446;&#21644;&#29992;&#25143;&#20043;&#38388;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;DNN&#30340;&#27169;&#22411;&#36890;&#24120;&#36973;&#21463;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#65292;&#21363;&#28040;&#32791;&#38750;&#24120;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#24182;&#23384;&#20648;&#22823;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23485;&#27867;&#25512;&#33616;&#31995;&#32479;&#65292;&#31216;&#20026;&#23485;&#27867;&#21327;&#21516;&#36807;&#28388;&#65288;BroadCF&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#12290;&#23485;&#27867;&#23398;&#20064;&#31995;&#32479;&#65288;BLS&#65289;&#34987;&#29992;&#20316;&#26144;&#23556;&#20989;&#25968;&#65292;&#20197;&#23398;&#20064;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21487;&#20197;&#36991;&#20813;&#19978;&#36848;&#38382;&#39064;&#65292;&#21516;&#26102;&#23454;&#29616;&#38750;&#24120;&#20196;&#20154;&#28385;&#24847;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#23558;&#21407;&#22987;&#35780;&#20998;&#25968;&#25454;&#30452;&#25509;&#39304;&#36865;&#21040;BLS&#20013;&#24182;&#19981;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;-&#39033;&#35780;&#32423;&#21327;&#21516;&#21521;&#37327;&#39044;&#22788;&#29702;&#31243;&#24207;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;BLS&#23398;&#20064;&#30340;&#26684;&#24335;&#12290;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;BroadCF&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;CF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Deep Neural Networks (DNNs) have been widely introduced into Collaborative Filtering (CF) to produce more accurate recommendation results due to their capability of capturing the complex nonlinear relationships between items and users.However, the DNNs-based models usually suffer from high computational complexity, i.e., consuming very long training time and storing huge amount of trainable parameters. To address these problems, we propose a new broad recommender system called Broad Collaborative Filtering (BroadCF), which is an efficient nonlinear collaborative filtering approach. Instead of DNNs, Broad Learning System (BLS) is used as a mapping function to learn the complex nonlinear relationships between users and items, which can avoid the above issues while achieving very satisfactory recommendation performance. However, it is not feasible to directly feed the original rating data into BLS. To this end, we propose a user-item rating collaborative vector preprocessing pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#20851;&#30340;&#22522;&#20110;&#26680;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#32479;&#35745;&#37327;&#22312;&#27979;&#37327;&#27969;&#24418;&#25968;&#25454;&#26102;&#30340;&#24212;&#29992;&#12290;&#25991;&#31456;&#23637;&#31034;&#20102;&#26816;&#39564;&#27700;&#24179;&#21644;&#21151;&#29575;&#19982;&#26680;&#24102;&#23485;&#12289;&#26679;&#26412;&#25968;&#37327;&#21644;&#27969;&#24418;&#20869;&#22312;&#32500;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#24314;&#31435;&#20102;&#27979;&#35797;&#21151;&#29575;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2105.03425</link><description>&lt;p&gt;
&#27979;&#37327;&#27969;&#24418;&#25968;&#25454;&#30340;&#26680;&#21452;&#26679;&#26412;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Kernel Two-Sample Tests for Manifold Data. (arXiv:2105.03425v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.03425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#20851;&#30340;&#22522;&#20110;&#26680;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#32479;&#35745;&#37327;&#22312;&#27979;&#37327;&#27969;&#24418;&#25968;&#25454;&#26102;&#30340;&#24212;&#29992;&#12290;&#25991;&#31456;&#23637;&#31034;&#20102;&#26816;&#39564;&#27700;&#24179;&#21644;&#21151;&#29575;&#19982;&#26680;&#24102;&#23485;&#12289;&#26679;&#26412;&#25968;&#37327;&#21644;&#27969;&#24418;&#20869;&#22312;&#32500;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#24314;&#31435;&#20102;&#27979;&#35797;&#21151;&#29575;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#27969;&#24418;&#25968;&#25454;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#20851;&#30340;&#22522;&#20110;&#26680;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#32479;&#35745;&#37327;&#65292;&#20551;&#35774;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#25509;&#36817;&#20110;&#20302;&#32500;&#27969;&#24418;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#27979;&#35797;&#27700;&#24179;&#21644;&#21151;&#29575;&#19982;&#26680;&#24102;&#23485;&#12289;&#26679;&#26412;&#25968;&#37327;&#21644;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#25968;&#25454;&#23494;&#24230;&#25903;&#25345;&#22312;&#19968;&#20010;&#23884;&#20837;&#21040;$m$&#32500;&#31354;&#38388;&#20013;&#30340;$d$&#32500;&#23376;&#27969;&#24418;$\mathcal{M}$&#19978;&#26102;&#65292;&#20174;&#26381;&#20174;&#20110;&#19968;&#23545;&#20998;&#24067;$p$&#21644;$q$&#25277;&#21462;&#30340;&#25968;&#25454;&#36827;&#34892;&#26680;&#21452;&#26679;&#26412;&#26816;&#39564;&#65292;&#36825;&#23545;&#20998;&#24067;$ p $&#21644;$q$&#26159;&#20855;&#26377;H\"older&#38454;$\beta$&#65288;&#26368;&#39640;2&#65289;&#65292;&#26679;&#26412;&#25968;&#37327;$n$&#36275;&#22815;&#22823;&#65292;&#20351;&#24471;$\Delta_2\gtrsim n^{- {2\beta/(d+4\beta)}}$&#65292;&#20854;&#20013;$\Delta_2$&#26159;&#27969;&#24418;&#19978;$p$&#21644;$q$&#20043;&#38388;&#30340;&#24179;&#26041;$L^2$-&#24046;&#24322;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36275;&#22815;&#22823;&#19988;&#26377;&#38480;$n$&#30340;&#27979;&#35797;&#21151;&#29575;&#19979;&#30028;&#65292;&#20854;&#20013;&#26680;&#24102;&#23485;&#21442;&#25968;$\gamma$&#30340;&#27604;&#20363;&#23610;&#24230;&#20026;$n^ {-1/(d+4\beta)}$&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a study of a kernel-based two-sample test statistic related to the Maximum Mean Discrepancy (MMD) in the manifold data setting, assuming that high-dimensional observations are close to a low-dimensional manifold. We characterize the test level and power in relation to the kernel bandwidth, the number of samples, and the intrinsic dimensionality of the manifold. Specifically, we show that when data densities are supported on a $d$-dimensional sub-manifold $\mathcal{M}$ embedded in an $m$-dimensional space, the kernel two-sample test for data sampled from a pair of distributions $p$ and $q$ that are H\"older with order $\beta$ (up to 2) is powerful when the number of samples $n$ is large such that $\Delta_2 \gtrsim n^{- { 2 \beta/( d + 4 \beta ) }}$, where $\Delta_2$ is the squared $L^2$-divergence between $p$ and $q$ on manifold. We establish a lower bound on the test power for finite $n$ that is sufficiently large, where the kernel bandwidth parameter $\gamma$ scales as $n^{
&lt;/p&gt;</description></item></channel></rss>